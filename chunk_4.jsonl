{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconfigure and sets up all of the logging modules.", "response": "def setup_logging(verbosity, no_color, user_log_file):\n    \"\"\"Configures and sets up all of the logging\n\n    Returns the requested logging level, as its integer value.\n    \"\"\"\n\n    # Determine the level to be logging at.\n    if verbosity >= 1:\n        level = \"DEBUG\"\n    elif verbosity == -1:\n        level = \"WARNING\"\n    elif verbosity == -2:\n        level = \"ERROR\"\n    elif verbosity <= -3:\n        level = \"CRITICAL\"\n    else:\n        level = \"INFO\"\n\n    level_number = getattr(logging, level)\n\n    # The \"root\" logger should match the \"console\" level *unless* we also need\n    # to log to a user log file.\n    include_user_log = user_log_file is not None\n    if include_user_log:\n        additional_log_file = user_log_file\n        root_level = \"DEBUG\"\n    else:\n        additional_log_file = \"/dev/null\"\n        root_level = level\n\n    # Disable any logging besides WARNING unless we have DEBUG level logging\n    # enabled for vendored libraries.\n    vendored_log_level = \"WARNING\" if level in [\"INFO\", \"ERROR\"] else \"DEBUG\"\n\n    # Shorthands for clarity\n    log_streams = {\n        \"stdout\": \"ext://sys.stdout\",\n        \"stderr\": \"ext://sys.stderr\",\n    }\n    handler_classes = {\n        \"stream\": \"pip._internal.utils.logging.ColorizedStreamHandler\",\n        \"file\": \"pip._internal.utils.logging.BetterRotatingFileHandler\",\n    }\n\n    logging.config.dictConfig({\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"filters\": {\n            \"exclude_warnings\": {\n                \"()\": \"pip._internal.utils.logging.MaxLevelFilter\",\n                \"level\": logging.WARNING,\n            },\n        },\n        \"formatters\": {\n            \"indent\": {\n                \"()\": IndentingFormatter,\n                \"format\": \"%(message)s\",\n            },\n            \"indent_with_timestamp\": {\n                \"()\": IndentingFormatter,\n                \"format\": \"%(message)s\",\n                \"add_timestamp\": True,\n            },\n        },\n        \"handlers\": {\n            \"console\": {\n                \"level\": level,\n                \"class\": handler_classes[\"stream\"],\n                \"no_color\": no_color,\n                \"stream\": log_streams[\"stdout\"],\n                \"filters\": [\"exclude_warnings\"],\n                \"formatter\": \"indent\",\n            },\n            \"console_errors\": {\n                \"level\": \"WARNING\",\n                \"class\": handler_classes[\"stream\"],\n                \"no_color\": no_color,\n                \"stream\": log_streams[\"stderr\"],\n                \"formatter\": \"indent\",\n            },\n            \"user_log\": {\n                \"level\": \"DEBUG\",\n                \"class\": handler_classes[\"file\"],\n                \"filename\": additional_log_file,\n                \"delay\": True,\n                \"formatter\": \"indent_with_timestamp\",\n            },\n        },\n        \"root\": {\n            \"level\": root_level,\n            \"handlers\": [\"console\", \"console_errors\"] + (\n                [\"user_log\"] if include_user_log else []\n            ),\n        },\n        \"loggers\": {\n            \"pip._vendor\": {\n                \"level\": vendored_log_level\n            }\n        },\n    })\n\n    return level_number"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format(self, record):\n        formatted = super(IndentingFormatter, self).format(record)\n        prefix = ''\n        if self.add_timestamp:\n            prefix = self.formatTime(record, \"%Y-%m-%dT%H:%M:%S \")\n        prefix += \" \" * get_indentation()\n        formatted = \"\".join([\n            prefix + line\n            for line in formatted.splitlines(True)\n        ])\n        return formatted", "response": "Formats the log record by our current indentation level."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn whether the handler is using sys. stdout.", "response": "def _using_stdout(self):\n        \"\"\"\n        Return whether the handler is using sys.stdout.\n        \"\"\"\n        if WINDOWS and colorama:\n            # Then self.stream is an AnsiToWin32 object.\n            return self.stream.wrapped is sys.stdout\n\n        return self.stream is sys.stdout"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _cast_boolean(value):\n    _BOOLEANS = {'1': True, 'yes': True, 'true': True, 'on': True,\n                 '0': False, 'no': False, 'false': False, 'off': False, '': False}\n    value = str(value)\n    if value.lower() not in _BOOLEANS:\n        raise ValueError('Not a boolean: %s' % value)\n\n    return _BOOLEANS[value.lower()]", "response": "Helper to convert config values to boolean as ConfigParser do.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getenv(option, default=undefined, cast=undefined):\n\n    # We can't avoid __contains__ because value may be empty.\n    if option in os.environ:\n        value = os.environ[option]\n    else:\n        if isinstance(default, Undefined):\n            raise UndefinedValueError('{} not found. Declare it as envvar or define a default value.'.format(option))\n\n        value = default\n\n    if isinstance(cast, Undefined):\n        return value\n\n    if cast is bool:\n        value = _cast_boolean(value)\n    elif cast is list:\n        value = [x for x in value.split(',') if x]\n    else:\n        value = cast(value)\n\n    return value", "response": "Get the value for an option from os. environ or default if not defined."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef join_options(options):\n    rv = []\n    any_prefix_is_slash = False\n    for opt in options:\n        prefix = split_opt(opt)[0]\n        if prefix == '/':\n            any_prefix_is_slash = True\n        rv.append((len(prefix), opt))\n\n    rv.sort(key=lambda x: x[0])\n\n    rv = ', '.join(x[1] for x in rv)\n    return rv, any_prefix_is_slash", "response": "Given a list of option strings this joins them in the most appropriate\n    way and returns them in the form ( formatted_string any_prefix_is_slash ) where the second item in the tuple is a flag that otherwise it indicates if any of the option prefixes was a slash."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_usage(self, prog, args='', prefix='Usage: '):\n        usage_prefix = '%*s%s ' % (self.current_indent, prefix, prog)\n        text_width = self.width - self.current_indent\n\n        if text_width >= (term_len(usage_prefix) + 20):\n            # The arguments will fit to the right of the prefix.\n            indent = ' ' * term_len(usage_prefix)\n            self.write(wrap_text(args, text_width,\n                                 initial_indent=usage_prefix,\n                                 subsequent_indent=indent))\n        else:\n            # The prefix is too long, put the arguments on the next line.\n            self.write(usage_prefix)\n            self.write('\\n')\n            indent = ' ' * (max(self.current_indent, term_len(prefix)) + 4)\n            self.write(wrap_text(args, text_width,\n                                 initial_indent=indent,\n                                 subsequent_indent=indent))\n\n        self.write('\\n')", "response": "Writes a usage line into the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites re - indented text into the buffer.", "response": "def write_text(self, text):\n        \"\"\"Writes re-indented text into the buffer.  This rewraps and\n        preserves paragraphs.\n        \"\"\"\n        text_width = max(self.width - self.current_indent, 11)\n        indent = ' ' * self.current_indent\n        self.write(wrap_text(text, text_width,\n                             initial_indent=indent,\n                             subsequent_indent=indent,\n                             preserve_paragraphs=True))\n        self.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_dl(self, rows, col_max=30, col_spacing=2):\n        rows = list(rows)\n        widths = measure_table(rows)\n        if len(widths) != 2:\n            raise TypeError('Expected two columns for definition list')\n\n        first_col = min(widths[0], col_max) + col_spacing\n\n        for first, second in iter_rows(rows, len(widths)):\n            self.write('%*s%s' % (self.current_indent, '', first))\n            if not second:\n                self.write('\\n')\n                continue\n            if term_len(first) <= first_col - col_spacing:\n                self.write(' ' * (first_col - term_len(first)))\n            else:\n                self.write('\\n')\n                self.write(' ' * (first_col + self.current_indent))\n\n            text_width = max(self.width - first_col - 2, 10)\n            lines = iter(wrap_text(second, text_width).splitlines())\n            if lines:\n                self.write(next(lines) + '\\n')\n                for line in lines:\n                    self.write('%*s%s\\n' % (\n                        first_col + self.current_indent, '', line))\n            else:\n                self.write('\\n')", "response": "Writes a definition list into the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef section(self, name):\n        self.write_paragraph()\n        self.write_heading(name)\n        self.indent()\n        try:\n            yield\n        finally:\n            self.dedent()", "response": "Helpful context manager that writes a paragraph a heading and the indents and then dewrites the section."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef invalid_config_error_message(action, key, val):\n    if action in ('store_true', 'store_false'):\n        return (\"{0} is not a valid value for {1} option, \"\n                \"please specify a boolean value like yes/no, \"\n                \"true/false or 1/0 instead.\").format(val, key)\n\n    return (\"{0} is not a valid value for {1} option, \"\n            \"please specify a numerical value like 1/0 \"\n            \"instead.\").format(val, key)", "response": "Returns a better error message when invalid configuration option\n    is provided."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _format_option_strings(self, option, mvarfmt=' <%s>', optsep=', '):\n        opts = []\n\n        if option._short_opts:\n            opts.append(option._short_opts[0])\n        if option._long_opts:\n            opts.append(option._long_opts[0])\n        if len(opts) > 1:\n            opts.insert(1, optsep)\n\n        if option.takes_value():\n            metavar = option.metavar or option.dest.lower()\n            opts.append(mvarfmt % metavar.lower())\n\n        return ''.join(opts)", "response": "Format the option strings and return a comma - separated list of option strings and metavars."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_usage(self, usage):\n        msg = '\\nUsage: %s\\n' % self.indent_lines(textwrap.dedent(usage), \"  \")\n        return msg", "response": "Format the usage string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninsert an OptionGroup at a given position.", "response": "def insert_option_group(self, idx, *args, **kwargs):\n        \"\"\"Insert an OptionGroup at a given position.\"\"\"\n        group = self.add_option_group(*args, **kwargs)\n\n        self.option_groups.pop()\n        self.option_groups.insert(idx, group)\n\n        return group"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a list of all options including those in option groups.", "response": "def option_list_all(self):\n        \"\"\"Get a list of all options, including those in option groups.\"\"\"\n        res = self.option_list[:]\n        for i in self.option_groups:\n            res.extend(i.option_list)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update_defaults(self, defaults):\n\n        # Accumulate complex default state.\n        self.values = optparse.Values(self.defaults)\n        late_eval = set()\n        # Then set the options with those values\n        for key, val in self._get_ordered_configuration_items():\n            # '--' because configuration supports only long names\n            option = self.get_option('--' + key)\n\n            # Ignore options not present in this parser. E.g. non-globals put\n            # in [global] by users that want them to apply to all applicable\n            # commands.\n            if option is None:\n                continue\n\n            if option.action in ('store_true', 'store_false', 'count'):\n                try:\n                    val = strtobool(val)\n                except ValueError:\n                    error_msg = invalid_config_error_message(\n                        option.action, key, val\n                    )\n                    self.error(error_msg)\n\n            elif option.action == 'append':\n                val = val.split()\n                val = [self.check_default(option, key, v) for v in val]\n            elif option.action == 'callback':\n                late_eval.add(option.dest)\n                opt_str = option.get_opt_string()\n                val = option.convert_value(opt_str, val)\n                # From take_action\n                args = option.callback_args or ()\n                kwargs = option.callback_kwargs or {}\n                option.callback(option, opt_str, val, self, *args, **kwargs)\n            else:\n                val = self.check_default(option, key, val)\n\n            defaults[option.dest] = val\n\n        for key in late_eval:\n            defaults[key] = getattr(self.values, key)\n        self.values = None\n        return defaults", "response": "Updates the given defaults with values from the config files and the environ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride to make updating the defaults after instantiation of the option parser possible.", "response": "def get_default_values(self):\n        \"\"\"Overriding to make updating the defaults after instantiation of\n        the option parser possible, _update_defaults() does the dirty work.\"\"\"\n        if not self.process_default_values:\n            # Old, pre-Optik 1.5 behaviour.\n            return optparse.Values(self.defaults)\n\n        # Load the configuration, or error out in case of an error\n        try:\n            self.config.load()\n        except ConfigurationError as err:\n            self.exit(UNKNOWN_ERROR, str(err))\n\n        defaults = self._update_defaults(self.defaults.copy())  # ours\n        for option in self._get_all_options():\n            default = defaults.get(option.dest)\n            if isinstance(default, string_types):\n                opt_str = option.get_opt_string()\n                defaults[option.dest] = option.check_value(opt_str, default)\n        return optparse.Values(defaults)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves a single distribution and returns all the dependencies of that distribution.", "response": "def resolve_dist(cls, dist, working_set):\n        \"\"\"Given a local distribution and a working set, returns all dependencies from the set.\n\n        :param dist: A single distribution to find the dependencies of\n        :type dist: :class:`pkg_resources.Distribution`\n        :param working_set: A working set to search for all packages\n        :type working_set: :class:`pkg_resources.WorkingSet`\n        :return: A set of distributions which the package depends on, including the package\n        :rtype: set(:class:`pkg_resources.Distribution`)\n        \"\"\"\n\n        deps = set()\n        deps.add(dist)\n        try:\n            reqs = dist.requires()\n        except (AttributeError, OSError, IOError):  # The METADATA file can't be found\n            return deps\n        for req in reqs:\n            dist = working_set.find(req)\n            deps |= cls.resolve_dist(dist, working_set)\n        return deps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef base_paths(self):\n\n        prefix = make_posix(self.prefix.as_posix())\n        install_scheme = 'nt' if (os.name == 'nt') else 'posix_prefix'\n        paths = get_paths(install_scheme, vars={\n            'base': prefix,\n            'platbase': prefix,\n        })\n        paths[\"PATH\"] = paths[\"scripts\"] + os.pathsep + os.defpath\n        if \"prefix\" not in paths:\n            paths[\"prefix\"] = prefix\n        purelib = make_posix(get_python_lib(plat_specific=0, prefix=prefix))\n        platlib = make_posix(get_python_lib(plat_specific=1, prefix=prefix))\n        if purelib == platlib:\n            lib_dirs = purelib\n        else:\n            lib_dirs = purelib + os.pathsep + platlib\n        paths[\"libdir\"] = purelib\n        paths[\"purelib\"] = purelib\n        paths[\"platlib\"] = platlib\n        paths['PYTHONPATH'] = os.pathsep.join([\"\", \".\", lib_dirs])\n        paths[\"libdirs\"] = lib_dirs\n        return paths", "response": "Returns the base paths for the context appropriate for the environment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the system path inside the environment.", "response": "def sys_path(self):\n        \"\"\"\n        The system path inside the environment\n\n        :return: The :data:`sys.path` from the environment\n        :rtype: list\n        \"\"\"\n\n        from .vendor.vistir.compat import JSONDecodeError\n        current_executable = vistir.compat.Path(sys.executable).as_posix()\n        if not self.python or self.python == current_executable:\n            return sys.path\n        elif any([sys.prefix == self.prefix, not self.is_venv]):\n            return sys.path\n        cmd_args = [self.python, \"-c\", \"import json, sys; print(json.dumps(sys.path))\"]\n        path, _ = vistir.misc.run(cmd_args, return_object=False, nospin=True, block=True, combine_stderr=False, write_to_stdout=False)\n        try:\n            path = json.loads(path.strip())\n        except JSONDecodeError:\n            path = sys.path\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the python prefix run inside the context of the environment", "response": "def sys_prefix(self):\n        \"\"\"\n        The prefix run inside the context of the environment\n\n        :return: The python prefix inside the environment\n        :rtype: :data:`sys.prefix`\n        \"\"\"\n\n        command = [self.python, \"-c\" \"import sys; print(sys.prefix)\"]\n        c = vistir.misc.run(command, return_object=True, block=True, nospin=True, write_to_stdout=False)\n        sys_prefix = vistir.compat.Path(vistir.misc.to_text(c.out).strip()).as_posix()\n        return sys_prefix"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the pip version in the environment. Useful for knowing which args we can use when installing.", "response": "def pip_version(self):\n        \"\"\"\n        Get the pip version in the environment.  Useful for knowing which args we can use\n        when installing.\n        \"\"\"\n        from .vendor.packaging.version import parse as parse_version\n        pip = next(iter(\n            pkg for pkg in self.get_installed_packages() if pkg.key == \"pip\"\n        ), None)\n        if pip is not None:\n            pip_version = parse_version(pip.version)\n        return parse_version(\"18.0\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_distributions(self):\n\n        pkg_resources = self.safe_import(\"pkg_resources\")\n        libdirs = self.base_paths[\"libdirs\"].split(os.pathsep)\n        dists = (pkg_resources.find_distributions(libdir) for libdir in libdirs)\n        for dist in itertools.chain.from_iterable(dists):\n            yield dist", "response": "Returns an iterator over the distributions installed on the library path of the environment."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_egg(self, egg_dist):\n        site_packages = self.libdir[1]\n        search_filename = \"{0}.egg-link\".format(egg_dist.project_name)\n        try:\n            user_site = site.getusersitepackages()\n        except AttributeError:\n            user_site = site.USER_SITE\n        search_locations = [site_packages, user_site]\n        for site_directory in search_locations:\n            egg = os.path.join(site_directory, search_filename)\n            if os.path.isfile(egg):\n                return egg", "response": "Find an egg by name in the given environment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining whether the supplied distribution is in the project.", "response": "def dist_is_in_project(self, dist):\n        \"\"\"Determine whether the supplied distribution is in the environment.\"\"\"\n        from .project import _normalized\n        prefixes = [\n            _normalized(prefix) for prefix in self.base_paths[\"libdirs\"].split(os.pathsep)\n            if _normalized(prefix).startswith(_normalized(self.prefix.as_posix()))\n        ]\n        location = self.locate_dist(dist)\n        if not location:\n            return False\n        location = _normalized(make_posix(location))\n        return any(location.startswith(prefix) for prefix in prefixes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_installed(self, pkgname):\n\n        return any(d for d in self.get_distributions() if d.project_name == pkgname)", "response": "Given a package name returns whether it is installed in the environment\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun a python command in the enviornment context.", "response": "def run_py(self, cmd, cwd=os.curdir):\n        \"\"\"Run a python command in the enviornment context.\n\n        :param cmd: A command to run in the environment - runs with `python -c`\n        :type cmd: str or list\n        :param str cwd: The working directory in which to execute the command, defaults to :data:`os.curdir`\n        :return: A finished command object\n        :rtype: :class:`~subprocess.Popen`\n        \"\"\"\n\n        c = None\n        if isinstance(cmd, six.string_types):\n            script = vistir.cmdparse.Script.parse(\"{0} -c {1}\".format(self.python, cmd))\n        else:\n            script = vistir.cmdparse.Script.parse([self.python, \"-c\"] + list(cmd))\n        with self.activated():\n            c = vistir.misc.run(script._parts, return_object=True, nospin=True, cwd=cwd, write_to_stdout=False)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_activate_this(self):\n        if self.is_venv:\n            activate_this = os.path.join(self.scripts_dir, \"activate_this.py\")\n            if not os.path.isfile(activate_this):\n                raise OSError(\"No such file: {0!s}\".format(activate_this))\n            with open(activate_this, \"r\") as f:\n                code = compile(f.read(), activate_this, \"exec\")\n                exec(code, dict(__file__=activate_this))", "response": "Runs the environment s inline activation script"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uninstall(self, pkgname, *args, **kwargs):\n\n        auto_confirm = kwargs.pop(\"auto_confirm\", True)\n        verbose = kwargs.pop(\"verbose\", False)\n        with self.activated():\n            monkey_patch = next(iter(\n                dist for dist in self.base_working_set\n                if dist.project_name == \"recursive-monkey-patch\"\n            ), None)\n            if monkey_patch:\n                monkey_patch.activate()\n            pip_shims = self.safe_import(\"pip_shims\")\n            pathset_base = pip_shims.UninstallPathSet\n            pathset_base._permitted = PatchedUninstaller._permitted\n            dist = next(\n                iter(filter(lambda d: d.project_name == pkgname, self.get_working_set())),\n                None\n            )\n            pathset = pathset_base.from_dist(dist)\n            if pathset is not None:\n                pathset.remove(auto_confirm=auto_confirm, verbose=verbose)\n            try:\n                yield pathset\n            except Exception as e:\n                if pathset is not None:\n                    pathset.rollback()\n            else:\n                if pathset is not None:\n                    pathset.commit()\n            if pathset is None:\n                return", "response": "A context manager which allows uninstallation of packages from the environment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stn(s, length, encoding, errors):\n    s = s.encode(encoding, errors)\n    return s[:length] + (length - len(s)) * NUL", "response": "Convert a string to a null - terminated bytes object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nts(s, encoding, errors):\n    p = s.find(b\"\\0\")\n    if p != -1:\n        s = s[:p]\n    return s.decode(encoding, errors)", "response": "Convert a null - terminated bytes object to a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a number field to a python number.", "response": "def nti(s):\n    \"\"\"Convert a number field to a python number.\n    \"\"\"\n    # There are two possible encodings for a number field, see\n    # itn() below.\n    if s[0] != chr(0o200):\n        try:\n            n = int(nts(s, \"ascii\", \"strict\") or \"0\", 8)\n        except ValueError:\n            raise InvalidHeaderError(\"invalid header\")\n    else:\n        n = 0\n        for i in range(len(s) - 1):\n            n <<= 8\n            n += ord(s[i + 1])\n    return n"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a python number to a number field.", "response": "def itn(n, digits=8, format=DEFAULT_FORMAT):\n    \"\"\"Convert a python number to a number field.\n    \"\"\"\n    # POSIX 1003.1-1988 requires numbers to be encoded as a string of\n    # octal digits followed by a null-byte, this allows values up to\n    # (8**(digits-1))-1. GNU tar allows storing numbers greater than\n    # that if necessary. A leading 0o200 byte indicates this particular\n    # encoding, the following digits-1 bytes are a big-endian\n    # representation. This allows values up to (256**(digits-1))-1.\n    if 0 <= n < 8 ** (digits - 1):\n        s = (\"%0*o\" % (digits - 1, n)).encode(\"ascii\") + NUL\n    else:\n        if format != GNU_FORMAT or n >= 256 ** (digits - 1):\n            raise ValueError(\"overflow in number field\")\n\n        if n < 0:\n            # XXX We mimic GNU tar's behaviour with negative numbers,\n            # this could raise OverflowError.\n            n = struct.unpack(\"L\", struct.pack(\"l\", n))[0]\n\n        s = bytearray()\n        for i in range(digits - 1):\n            s.insert(0, n & 0o377)\n            n >>= 8\n        s.insert(0, 0o200)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the checksum for a member s header by summing up all characters except for the chksum field.", "response": "def calc_chksums(buf):\n    \"\"\"Calculate the checksum for a member's header by summing up all\n       characters except for the chksum field which is treated as if\n       it was filled with spaces. According to the GNU tar sources,\n       some tars (Sun and NeXT) calculate chksum with signed char,\n       which will be different if there are chars in the buffer with\n       the high bit set. So we calculate two checksums, unsigned and\n       signed.\n    \"\"\"\n    unsigned_chksum = 256 + sum(struct.unpack(\"148B\", buf[:148]) + struct.unpack(\"356B\", buf[156:512]))\n    signed_chksum = 256 + sum(struct.unpack(\"148b\", buf[:148]) + struct.unpack(\"356b\", buf[156:512]))\n    return unsigned_chksum, signed_chksum"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copyfileobj(src, dst, length=None):\n    if length == 0:\n        return\n    if length is None:\n        while True:\n            buf = src.read(16*1024)\n            if not buf:\n                break\n            dst.write(buf)\n        return\n\n    BUFSIZE = 16 * 1024\n    blocks, remainder = divmod(length, BUFSIZE)\n    for b in range(blocks):\n        buf = src.read(BUFSIZE)\n        if len(buf) < BUFSIZE:\n            raise IOError(\"end of file reached\")\n        dst.write(buf)\n\n    if remainder != 0:\n        buf = src.read(remainder)\n        if len(buf) < remainder:\n            raise IOError(\"end of file reached\")\n        dst.write(buf)\n    return", "response": "Copy length bytes from fileobj src to fileobj dst."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a file s mode to a string of the form - rwxrwxrwx.", "response": "def filemode(mode):\n    \"\"\"Convert a file's mode to a string of the form\n       -rwxrwxrwx.\n       Used by TarFile.list()\n    \"\"\"\n    perm = []\n    for table in filemode_table:\n        for bit, char in table:\n            if mode & bit == bit:\n                perm.append(char)\n                break\n        else:\n            perm.append(\"-\")\n    return \"\".join(perm)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if name points to a tar archive that we can handle otherwise return False.", "response": "def is_tarfile(name):\n    \"\"\"Return True if name points to a tar archive that we\n       are able to handle, else return False.\n    \"\"\"\n    try:\n        t = open(name)\n        t.close()\n        return True\n    except TarError:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes for writing with gzip compression.", "response": "def _init_write_gz(self):\n        \"\"\"Initialize for writing with gzip compression.\n        \"\"\"\n        self.cmp = self.zlib.compressobj(9, self.zlib.DEFLATED,\n                                            -self.zlib.MAX_WBITS,\n                                            self.zlib.DEF_MEM_LEVEL,\n                                            0)\n        timestamp = struct.pack(\"<L\", int(time.time()))\n        self.__write(b\"\\037\\213\\010\\010\" + timestamp + b\"\\002\\377\")\n        if self.name.endswith(\".gz\"):\n            self.name = self.name[:-3]\n        # RFC1952 says we must use ISO-8859-1 for the FNAME field.\n        self.__write(self.name.encode(\"iso-8859-1\", \"replace\") + NUL)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, s):\n        if self.comptype == \"gz\":\n            self.crc = self.zlib.crc32(s, self.crc)\n        self.pos += len(s)\n        if self.comptype != \"tar\":\n            s = self.cmp.compress(s)\n        self.__write(s)", "response": "Write string s to the stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites string s to the stream if a whole new block is ready to be written.", "response": "def __write(self, s):\n        \"\"\"Write string s to the stream if a whole new block\n           is ready to be written.\n        \"\"\"\n        self.buf += s\n        while len(self.buf) > self.bufsize:\n            self.fileobj.write(self.buf[:self.bufsize])\n            self.buf = self.buf[self.bufsize:]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self):\n        if self.closed:\n            return\n\n        if self.mode == \"w\" and self.comptype != \"tar\":\n            self.buf += self.cmp.flush()\n\n        if self.mode == \"w\" and self.buf:\n            self.fileobj.write(self.buf)\n            self.buf = b\"\"\n            if self.comptype == \"gz\":\n                # The native zlib crc is an unsigned 32-bit integer, but\n                # the Python wrapper implicitly casts that to a signed C\n                # long.  So, on a 32-bit box self.crc may \"look negative\",\n                # while the same crc on a 64-bit box may \"look positive\".\n                # To avoid irksome warnings from the `struct` module, force\n                # it to look positive on all boxes.\n                self.fileobj.write(struct.pack(\"<L\", self.crc & 0xffffffff))\n                self.fileobj.write(struct.pack(\"<L\", self.pos & 0xffffFFFF))\n\n        if not self._extfileobj:\n            self.fileobj.close()\n\n        self.closed = True", "response": "Close the _Stream object. No operation should bebe\n           done on it afterwards."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_read_gz(self):\n        self.cmp = self.zlib.decompressobj(-self.zlib.MAX_WBITS)\n        self.dbuf = b\"\"\n\n        # taken from gzip.GzipFile with some alterations\n        if self.__read(2) != b\"\\037\\213\":\n            raise ReadError(\"not a gzip file\")\n        if self.__read(1) != b\"\\010\":\n            raise CompressionError(\"unsupported compression method\")\n\n        flag = ord(self.__read(1))\n        self.__read(6)\n\n        if flag & 4:\n            xlen = ord(self.__read(1)) + 256 * ord(self.__read(1))\n            self.read(xlen)\n        if flag & 8:\n            while True:\n                s = self.__read(1)\n                if not s or s == NUL:\n                    break\n        if flag & 16:\n            while True:\n                s = self.__read(1)\n                if not s or s == NUL:\n                    break\n        if flag & 2:\n            self.__read(2)", "response": "Initialize for reading a gzip compressed fileobj."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the stream s file pointer to pos. Negative seeking Integrity is forbidden.", "response": "def seek(self, pos=0):\n        \"\"\"Set the stream's file pointer to pos. Negative seeking\n           is forbidden.\n        \"\"\"\n        if pos - self.pos >= 0:\n            blocks, remainder = divmod(pos - self.pos, self.bufsize)\n            for i in range(blocks):\n                self.read(self.bufsize)\n            self.read(remainder)\n        else:\n            raise StreamError(\"seeking backwards is not allowed\")\n        return self.pos"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(self, size=None):\n        if size is None:\n            t = []\n            while True:\n                buf = self._read(self.bufsize)\n                if not buf:\n                    break\n                t.append(buf)\n            buf = \"\".join(t)\n        else:\n            buf = self._read(size)\n        self.pos += len(buf)\n        return buf", "response": "Return the next size number of bytes from the stream."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read(self, size):\n        if self.comptype == \"tar\":\n            return self.__read(size)\n\n        c = len(self.dbuf)\n        while c < size:\n            buf = self.__read(self.bufsize)\n            if not buf:\n                break\n            try:\n                buf = self.cmp.decompress(buf)\n            except IOError:\n                raise ReadError(\"invalid compressed data\")\n            self.dbuf += buf\n            c += len(buf)\n        buf = self.dbuf[:size]\n        self.dbuf = self.dbuf[size:]\n        return buf", "response": "Read size bytes from the stream."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __read(self, size):\n        c = len(self.buf)\n        while c < size:\n            buf = self.fileobj.read(self.bufsize)\n            if not buf:\n                break\n            self.buf += buf\n            c += len(buf)\n        buf = self.buf[:size]\n        self.buf = self.buf[size:]\n        return buf", "response": "Read size bytes from the internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading data from the file.", "response": "def read(self, size=None):\n        \"\"\"Read data from the file.\n        \"\"\"\n        if size is None:\n            size = self.size - self.position\n        else:\n            size = min(size, self.size - self.position)\n\n        buf = b\"\"\n        while size > 0:\n            while True:\n                data, start, stop, offset = self.map[self.map_index]\n                if start <= self.position < stop:\n                    break\n                else:\n                    self.map_index += 1\n                    if self.map_index == len(self.map):\n                        self.map_index = 0\n            length = min(size, stop - self.position)\n            if data:\n                self.fileobj.seek(offset + (self.position - start))\n                buf += self.fileobj.read(length)\n            else:\n                buf += NUL * length\n            size -= length\n            self.position += length\n        return buf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read(self, size=None):\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file\")\n\n        buf = b\"\"\n        if self.buffer:\n            if size is None:\n                buf = self.buffer\n                self.buffer = b\"\"\n            else:\n                buf = self.buffer[:size]\n                self.buffer = self.buffer[size:]\n\n        if size is None:\n            buf += self.fileobj.read()\n        else:\n            buf += self.fileobj.read(size - len(buf))\n\n        self.position += len(buf)\n        return buf", "response": "Read at most size bytes from the file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readline(self, size=-1):\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file\")\n\n        pos = self.buffer.find(b\"\\n\") + 1\n        if pos == 0:\n            # no newline found.\n            while True:\n                buf = self.fileobj.read(self.blocksize)\n                self.buffer += buf\n                if not buf or b\"\\n\" in buf:\n                    pos = self.buffer.find(b\"\\n\") + 1\n                    if pos == 0:\n                        # no newline found.\n                        pos = len(self.buffer)\n                    break\n\n        if size != -1:\n            pos = min(size, pos)\n\n        buf = self.buffer[:pos]\n        self.buffer = self.buffer[pos:]\n        self.position += len(buf)\n        return buf", "response": "Read one entire line from the file. If size is present return a string with at most size. If size is non - negative return a string with at most size."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef seek(self, pos, whence=os.SEEK_SET):\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file\")\n\n        if whence == os.SEEK_SET:\n            self.position = min(max(pos, 0), self.size)\n        elif whence == os.SEEK_CUR:\n            if pos < 0:\n                self.position = max(self.position + pos, 0)\n            else:\n                self.position = min(self.position + pos, self.size)\n        elif whence == os.SEEK_END:\n            self.position = max(min(self.size + pos, self.size), 0)\n        else:\n            raise ValueError(\"Invalid argument\")\n\n        self.buffer = b\"\"\n        self.fileobj.seek(self.position)", "response": "Seek to a position in the file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_info(self):\n        info = {\n            \"name\":     self.name,\n            \"mode\":     self.mode & 0o7777,\n            \"uid\":      self.uid,\n            \"gid\":      self.gid,\n            \"size\":     self.size,\n            \"mtime\":    self.mtime,\n            \"chksum\":   self.chksum,\n            \"type\":     self.type,\n            \"linkname\": self.linkname,\n            \"uname\":    self.uname,\n            \"gname\":    self.gname,\n            \"devmajor\": self.devmajor,\n            \"devminor\": self.devminor\n        }\n\n        if info[\"type\"] == DIRTYPE and not info[\"name\"].endswith(\"/\"):\n            info[\"name\"] += \"/\"\n\n        return info", "response": "Return the TarInfo s attributes as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a tar header as a string of 512 byte blocks.", "response": "def tobuf(self, format=DEFAULT_FORMAT, encoding=ENCODING, errors=\"surrogateescape\"):\n        \"\"\"Return a tar header as a string of 512 byte blocks.\n        \"\"\"\n        info = self.get_info()\n\n        if format == USTAR_FORMAT:\n            return self.create_ustar_header(info, encoding, errors)\n        elif format == GNU_FORMAT:\n            return self.create_gnu_header(info, encoding, errors)\n        elif format == PAX_FORMAT:\n            return self.create_pax_header(info, encoding)\n        else:\n            raise ValueError(\"invalid format\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_ustar_header(self, info, encoding, errors):\n        info[\"magic\"] = POSIX_MAGIC\n\n        if len(info[\"linkname\"]) > LENGTH_LINK:\n            raise ValueError(\"linkname is too long\")\n\n        if len(info[\"name\"]) > LENGTH_NAME:\n            info[\"prefix\"], info[\"name\"] = self._posix_split_name(info[\"name\"])\n\n        return self._create_header(info, USTAR_FORMAT, encoding, errors)", "response": "Create a ustar header block."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_gnu_header(self, info, encoding, errors):\n        info[\"magic\"] = GNU_MAGIC\n\n        buf = b\"\"\n        if len(info[\"linkname\"]) > LENGTH_LINK:\n            buf += self._create_gnu_long_header(info[\"linkname\"], GNUTYPE_LONGLINK, encoding, errors)\n\n        if len(info[\"name\"]) > LENGTH_NAME:\n            buf += self._create_gnu_long_header(info[\"name\"], GNUTYPE_LONGNAME, encoding, errors)\n\n        return buf + self._create_header(info, GNU_FORMAT, encoding, errors)", "response": "Create a GNU header block."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_pax_header(self, info, encoding):\n        info[\"magic\"] = POSIX_MAGIC\n        pax_headers = self.pax_headers.copy()\n\n        # Test string fields for values that exceed the field length or cannot\n        # be represented in ASCII encoding.\n        for name, hname, length in (\n                (\"name\", \"path\", LENGTH_NAME), (\"linkname\", \"linkpath\", LENGTH_LINK),\n                (\"uname\", \"uname\", 32), (\"gname\", \"gname\", 32)):\n\n            if hname in pax_headers:\n                # The pax header has priority.\n                continue\n\n            # Try to encode the string as ASCII.\n            try:\n                info[name].encode(\"ascii\", \"strict\")\n            except UnicodeEncodeError:\n                pax_headers[hname] = info[name]\n                continue\n\n            if len(info[name]) > length:\n                pax_headers[hname] = info[name]\n\n        # Test number fields for values that exceed the field limit or values\n        # that like to be stored as float.\n        for name, digits in ((\"uid\", 8), (\"gid\", 8), (\"size\", 12), (\"mtime\", 12)):\n            if name in pax_headers:\n                # The pax header has priority. Avoid overflow.\n                info[name] = 0\n                continue\n\n            val = info[name]\n            if not 0 <= val < 8 ** (digits - 1) or isinstance(val, float):\n                pax_headers[name] = str(val)\n                info[name] = 0\n\n        # Create a pax extended header if necessary.\n        if pax_headers:\n            buf = self._create_pax_generic_header(pax_headers, XHDTYPE, encoding)\n        else:\n            buf = b\"\"\n\n        return buf + self._create_header(info, USTAR_FORMAT, \"ascii\", \"replace\")", "response": "Create a pax extended header block. If the object cannot be represented this way prepend a pax extended header sequence\n           with supplement information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting a name longer than 100 chars into a prefix and a name part.", "response": "def _posix_split_name(self, name):\n        \"\"\"Split a name longer than 100 chars into a prefix\n           and a name part.\n        \"\"\"\n        prefix = name[:LENGTH_PREFIX + 1]\n        while prefix and prefix[-1] != \"/\":\n            prefix = prefix[:-1]\n\n        name = name[len(prefix):]\n        prefix = prefix[:-1]\n\n        if not prefix or len(name) > LENGTH_NAME:\n            raise ValueError(\"name is too long\")\n        return prefix, name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_header(info, format, encoding, errors):\n        parts = [\n            stn(info.get(\"name\", \"\"), 100, encoding, errors),\n            itn(info.get(\"mode\", 0) & 0o7777, 8, format),\n            itn(info.get(\"uid\", 0), 8, format),\n            itn(info.get(\"gid\", 0), 8, format),\n            itn(info.get(\"size\", 0), 12, format),\n            itn(info.get(\"mtime\", 0), 12, format),\n            b\"        \", # checksum field\n            info.get(\"type\", REGTYPE),\n            stn(info.get(\"linkname\", \"\"), 100, encoding, errors),\n            info.get(\"magic\", POSIX_MAGIC),\n            stn(info.get(\"uname\", \"\"), 32, encoding, errors),\n            stn(info.get(\"gname\", \"\"), 32, encoding, errors),\n            itn(info.get(\"devmajor\", 0), 8, format),\n            itn(info.get(\"devminor\", 0), 8, format),\n            stn(info.get(\"prefix\", \"\"), 155, encoding, errors)\n        ]\n\n        buf = struct.pack(\"%ds\" % BLOCKSIZE, b\"\".join(parts))\n        chksum = calc_chksums(buf[-BLOCKSIZE:])[0]\n        buf = buf[:-364] + (\"%06o\\0\" % chksum).encode(\"ascii\") + buf[-357:]\n        return buf", "response": "Create a header block."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_payload(payload):\n        blocks, remainder = divmod(len(payload), BLOCKSIZE)\n        if remainder > 0:\n            payload += (BLOCKSIZE - remainder) * NUL\n        return payload", "response": "Return the string payload filled with zero bytes\n           up to the next 512 byte border."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_gnu_long_header(cls, name, type, encoding, errors):\n        name = name.encode(encoding, errors) + NUL\n\n        info = {}\n        info[\"name\"] = \"././@LongLink\"\n        info[\"type\"] = type\n        info[\"size\"] = len(name)\n        info[\"magic\"] = GNU_MAGIC\n\n        # create extended header + name blocks.\n        return cls._create_header(info, USTAR_FORMAT, encoding, errors) + \\\n                cls._create_payload(name)", "response": "Create a GNUTYPE_LONGNAME or GNUTYPE_LONGLINK header for a name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef frombuf(cls, buf, encoding, errors):\n        if len(buf) == 0:\n            raise EmptyHeaderError(\"empty header\")\n        if len(buf) != BLOCKSIZE:\n            raise TruncatedHeaderError(\"truncated header\")\n        if buf.count(NUL) == BLOCKSIZE:\n            raise EOFHeaderError(\"end of file header\")\n\n        chksum = nti(buf[148:156])\n        if chksum not in calc_chksums(buf):\n            raise InvalidHeaderError(\"bad checksum\")\n\n        obj = cls()\n        obj.name = nts(buf[0:100], encoding, errors)\n        obj.mode = nti(buf[100:108])\n        obj.uid = nti(buf[108:116])\n        obj.gid = nti(buf[116:124])\n        obj.size = nti(buf[124:136])\n        obj.mtime = nti(buf[136:148])\n        obj.chksum = chksum\n        obj.type = buf[156:157]\n        obj.linkname = nts(buf[157:257], encoding, errors)\n        obj.uname = nts(buf[265:297], encoding, errors)\n        obj.gname = nts(buf[297:329], encoding, errors)\n        obj.devmajor = nti(buf[329:337])\n        obj.devminor = nti(buf[337:345])\n        prefix = nts(buf[345:500], encoding, errors)\n\n        # Old V7 tar format represents a directory as a regular\n        # file with a trailing slash.\n        if obj.type == AREGTYPE and obj.name.endswith(\"/\"):\n            obj.type = DIRTYPE\n\n        # The old GNU sparse format occupies some of the unused\n        # space in the buffer for up to 4 sparse structures.\n        # Save the them for later processing in _proc_sparse().\n        if obj.type == GNUTYPE_SPARSE:\n            pos = 386\n            structs = []\n            for i in range(4):\n                try:\n                    offset = nti(buf[pos:pos + 12])\n                    numbytes = nti(buf[pos + 12:pos + 24])\n                except ValueError:\n                    break\n                structs.append((offset, numbytes))\n                pos += 24\n            isextended = bool(buf[482])\n            origsize = nti(buf[483:495])\n            obj._sparse_structs = (structs, isextended, origsize)\n\n        # Remove redundant slashes from directories.\n        if obj.isdir():\n            obj.name = obj.name.rstrip(\"/\")\n\n        # Reconstruct a ustar longname.\n        if prefix and obj.type not in GNU_TYPES:\n            obj.name = prefix + \"/\" + obj.name\n        return obj", "response": "Construct a TarInfo object from a 512 byte string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fromtarfile(cls, tarfile):\n        buf = tarfile.fileobj.read(BLOCKSIZE)\n        obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)\n        obj.offset = tarfile.fileobj.tell() - BLOCKSIZE\n        return obj._proc_member(tarfile)", "response": "Return the next TarInfo object from tarfile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _proc_member(self, tarfile):\n        if self.type in (GNUTYPE_LONGNAME, GNUTYPE_LONGLINK):\n            return self._proc_gnulong(tarfile)\n        elif self.type == GNUTYPE_SPARSE:\n            return self._proc_sparse(tarfile)\n        elif self.type in (XHDTYPE, XGLTYPE, SOLARIS_XHDTYPE):\n            return self._proc_pax(tarfile)\n        else:\n            return self._proc_builtin(tarfile)", "response": "Choose the right processing method depending on the type and call it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess a builtin type or an unknown type which will be treated as a regular file.", "response": "def _proc_builtin(self, tarfile):\n        \"\"\"Process a builtin type or an unknown type which\n           will be treated as a regular file.\n        \"\"\"\n        self.offset_data = tarfile.fileobj.tell()\n        offset = self.offset_data\n        if self.isreg() or self.type not in SUPPORTED_TYPES:\n            # Skip the following data blocks.\n            offset += self._block(self.size)\n        tarfile.offset = offset\n\n        # Patch the TarInfo object with saved global\n        # header information.\n        self._apply_pax_info(tarfile.pax_headers, tarfile.encoding, tarfile.errors)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing the blocks that hold a GNU longname or longlink member.", "response": "def _proc_gnulong(self, tarfile):\n        \"\"\"Process the blocks that hold a GNU longname\n           or longlink member.\n        \"\"\"\n        buf = tarfile.fileobj.read(self._block(self.size))\n\n        # Fetch the next header and process it.\n        try:\n            next = self.fromtarfile(tarfile)\n        except HeaderError:\n            raise SubsequentHeaderError(\"missing or bad subsequent header\")\n\n        # Patch the TarInfo object from the next header with\n        # the longname information.\n        next.offset = self.offset\n        if self.type == GNUTYPE_LONGNAME:\n            next.name = nts(buf, tarfile.encoding, tarfile.errors)\n        elif self.type == GNUTYPE_LONGLINK:\n            next.linkname = nts(buf, tarfile.encoding, tarfile.errors)\n\n        return next"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses a GNU sparse header plus extra headers.", "response": "def _proc_sparse(self, tarfile):\n        \"\"\"Process a GNU sparse header plus extra headers.\n        \"\"\"\n        # We already collected some sparse structures in frombuf().\n        structs, isextended, origsize = self._sparse_structs\n        del self._sparse_structs\n\n        # Collect sparse structures from extended header blocks.\n        while isextended:\n            buf = tarfile.fileobj.read(BLOCKSIZE)\n            pos = 0\n            for i in range(21):\n                try:\n                    offset = nti(buf[pos:pos + 12])\n                    numbytes = nti(buf[pos + 12:pos + 24])\n                except ValueError:\n                    break\n                if offset and numbytes:\n                    structs.append((offset, numbytes))\n                pos += 24\n            isextended = bool(buf[504])\n        self.sparse = structs\n\n        self.offset_data = tarfile.fileobj.tell()\n        tarfile.offset = self.offset_data + self._block(self.size)\n        self.size = origsize\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _proc_pax(self, tarfile):\n        # Read the header information.\n        buf = tarfile.fileobj.read(self._block(self.size))\n\n        # A pax header stores supplemental information for either\n        # the following file (extended) or all following files\n        # (global).\n        if self.type == XGLTYPE:\n            pax_headers = tarfile.pax_headers\n        else:\n            pax_headers = tarfile.pax_headers.copy()\n\n        # Check if the pax header contains a hdrcharset field. This tells us\n        # the encoding of the path, linkpath, uname and gname fields. Normally,\n        # these fields are UTF-8 encoded but since POSIX.1-2008 tar\n        # implementations are allowed to store them as raw binary strings if\n        # the translation to UTF-8 fails.\n        match = re.search(br\"\\d+ hdrcharset=([^\\n]+)\\n\", buf)\n        if match is not None:\n            pax_headers[\"hdrcharset\"] = match.group(1).decode(\"utf8\")\n\n        # For the time being, we don't care about anything other than \"BINARY\".\n        # The only other value that is currently allowed by the standard is\n        # \"ISO-IR 10646 2000 UTF-8\" in other words UTF-8.\n        hdrcharset = pax_headers.get(\"hdrcharset\")\n        if hdrcharset == \"BINARY\":\n            encoding = tarfile.encoding\n        else:\n            encoding = \"utf8\"\n\n        # Parse pax header information. A record looks like that:\n        # \"%d %s=%s\\n\" % (length, keyword, value). length is the size\n        # of the complete record including the length field itself and\n        # the newline. keyword and value are both UTF-8 encoded strings.\n        regex = re.compile(br\"(\\d+) ([^=]+)=\")\n        pos = 0\n        while True:\n            match = regex.match(buf, pos)\n            if not match:\n                break\n\n            length, keyword = match.groups()\n            length = int(length)\n            value = buf[match.end(2) + 1:match.start(1) + length - 1]\n\n            # Normally, we could just use \"utf8\" as the encoding and \"strict\"\n            # as the error handler, but we better not take the risk. For\n            # example, GNU tar <= 1.23 is known to store filenames it cannot\n            # translate to UTF-8 as raw strings (unfortunately without a\n            # hdrcharset=BINARY header).\n            # We first try the strict standard encoding, and if that fails we\n            # fall back on the user's encoding and error handler.\n            keyword = self._decode_pax_field(keyword, \"utf8\", \"utf8\",\n                    tarfile.errors)\n            if keyword in PAX_NAME_FIELDS:\n                value = self._decode_pax_field(value, encoding, tarfile.encoding,\n                        tarfile.errors)\n            else:\n                value = self._decode_pax_field(value, \"utf8\", \"utf8\",\n                        tarfile.errors)\n\n            pax_headers[keyword] = value\n            pos += length\n\n        # Fetch the next header.\n        try:\n            next = self.fromtarfile(tarfile)\n        except HeaderError:\n            raise SubsequentHeaderError(\"missing or bad subsequent header\")\n\n        # Process GNU sparse information.\n        if \"GNU.sparse.map\" in pax_headers:\n            # GNU extended sparse format version 0.1.\n            self._proc_gnusparse_01(next, pax_headers)\n\n        elif \"GNU.sparse.size\" in pax_headers:\n            # GNU extended sparse format version 0.0.\n            self._proc_gnusparse_00(next, pax_headers, buf)\n\n        elif pax_headers.get(\"GNU.sparse.major\") == \"1\" and pax_headers.get(\"GNU.sparse.minor\") == \"0\":\n            # GNU extended sparse format version 1.0.\n            self._proc_gnusparse_10(next, pax_headers, tarfile)\n\n        if self.type in (XHDTYPE, SOLARIS_XHDTYPE):\n            # Patch the TarInfo object with the extended header info.\n            next._apply_pax_info(pax_headers, tarfile.encoding, tarfile.errors)\n            next.offset = self.offset\n\n            if \"size\" in pax_headers:\n                # If the extended header replaces the size field,\n                # we need to recalculate the offset where the next\n                # header starts.\n                offset = next.offset_data\n                if next.isreg() or next.type not in SUPPORTED_TYPES:\n                    offset += next._block(next.size)\n                tarfile.offset = offset\n\n        return next", "response": "Process an extended or global header as described in\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a GNU tar extended sparse header version 0. 0.", "response": "def _proc_gnusparse_00(self, next, pax_headers, buf):\n        \"\"\"Process a GNU tar extended sparse header, version 0.0.\n        \"\"\"\n        offsets = []\n        for match in re.finditer(br\"\\d+ GNU.sparse.offset=(\\d+)\\n\", buf):\n            offsets.append(int(match.group(1)))\n        numbytes = []\n        for match in re.finditer(br\"\\d+ GNU.sparse.numbytes=(\\d+)\\n\", buf):\n            numbytes.append(int(match.group(1)))\n        next.sparse = list(zip(offsets, numbytes))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _proc_gnusparse_01(self, next, pax_headers):\n        sparse = [int(x) for x in pax_headers[\"GNU.sparse.map\"].split(\",\")]\n        next.sparse = list(zip(sparse[::2], sparse[1::2]))", "response": "Process a GNU tar extended sparse header version 0. 1.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _proc_gnusparse_10(self, next, pax_headers, tarfile):\n        fields = None\n        sparse = []\n        buf = tarfile.fileobj.read(BLOCKSIZE)\n        fields, buf = buf.split(b\"\\n\", 1)\n        fields = int(fields)\n        while len(sparse) < fields * 2:\n            if b\"\\n\" not in buf:\n                buf += tarfile.fileobj.read(BLOCKSIZE)\n            number, buf = buf.split(b\"\\n\", 1)\n            sparse.append(int(number))\n        next.offset_data = tarfile.fileobj.tell()\n        next.sparse = list(zip(sparse[::2], sparse[1::2]))", "response": "Process a GNU tar extended sparse header version 1. 0."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreplace fields with supplemental information from a previous pax extended or global header.", "response": "def _apply_pax_info(self, pax_headers, encoding, errors):\n        \"\"\"Replace fields with supplemental information from a previous\n           pax extended or global header.\n        \"\"\"\n        for keyword, value in pax_headers.items():\n            if keyword == \"GNU.sparse.name\":\n                setattr(self, \"path\", value)\n            elif keyword == \"GNU.sparse.size\":\n                setattr(self, \"size\", int(value))\n            elif keyword == \"GNU.sparse.realsize\":\n                setattr(self, \"size\", int(value))\n            elif keyword in PAX_FIELDS:\n                if keyword in PAX_NUMBER_FIELDS:\n                    try:\n                        value = PAX_NUMBER_FIELDS[keyword](value)\n                    except ValueError:\n                        value = 0\n                if keyword == \"path\":\n                    value = value.rstrip(\"/\")\n                setattr(self, keyword, value)\n\n        self.pax_headers = pax_headers.copy()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _decode_pax_field(self, value, encoding, fallback_encoding, fallback_errors):\n        try:\n            return value.decode(encoding, \"strict\")\n        except UnicodeDecodeError:\n            return value.decode(fallback_encoding, fallback_errors)", "response": "Decode a single field from a pax record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _block(self, count):\n        blocks, remainder = divmod(count, BLOCKSIZE)\n        if remainder:\n            blocks += 1\n        return blocks * BLOCKSIZE", "response": "Round up a byte count by BLOCKSIZE and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens a tar archive for reading writing or appending.", "response": "def open(cls, name=None, mode=\"r\", fileobj=None, bufsize=RECORDSIZE, **kwargs):\n        \"\"\"Open a tar archive for reading, writing or appending. Return\n           an appropriate TarFile class.\n\n           mode:\n           'r' or 'r:*' open for reading with transparent compression\n           'r:'         open for reading exclusively uncompressed\n           'r:gz'       open for reading with gzip compression\n           'r:bz2'      open for reading with bzip2 compression\n           'a' or 'a:'  open for appending, creating the file if necessary\n           'w' or 'w:'  open for writing without compression\n           'w:gz'       open for writing with gzip compression\n           'w:bz2'      open for writing with bzip2 compression\n\n           'r|*'        open a stream of tar blocks with transparent compression\n           'r|'         open an uncompressed stream of tar blocks for reading\n           'r|gz'       open a gzip compressed stream of tar blocks\n           'r|bz2'      open a bzip2 compressed stream of tar blocks\n           'w|'         open an uncompressed stream for writing\n           'w|gz'       open a gzip compressed stream for writing\n           'w|bz2'      open a bzip2 compressed stream for writing\n        \"\"\"\n\n        if not name and not fileobj:\n            raise ValueError(\"nothing to open\")\n\n        if mode in (\"r\", \"r:*\"):\n            # Find out which *open() is appropriate for opening the file.\n            for comptype in cls.OPEN_METH:\n                func = getattr(cls, cls.OPEN_METH[comptype])\n                if fileobj is not None:\n                    saved_pos = fileobj.tell()\n                try:\n                    return func(name, \"r\", fileobj, **kwargs)\n                except (ReadError, CompressionError) as e:\n                    if fileobj is not None:\n                        fileobj.seek(saved_pos)\n                    continue\n            raise ReadError(\"file could not be opened successfully\")\n\n        elif \":\" in mode:\n            filemode, comptype = mode.split(\":\", 1)\n            filemode = filemode or \"r\"\n            comptype = comptype or \"tar\"\n\n            # Select the *open() function according to\n            # given compression.\n            if comptype in cls.OPEN_METH:\n                func = getattr(cls, cls.OPEN_METH[comptype])\n            else:\n                raise CompressionError(\"unknown compression type %r\" % comptype)\n            return func(name, filemode, fileobj, **kwargs)\n\n        elif \"|\" in mode:\n            filemode, comptype = mode.split(\"|\", 1)\n            filemode = filemode or \"r\"\n            comptype = comptype or \"tar\"\n\n            if filemode not in \"rw\":\n                raise ValueError(\"mode must be 'r' or 'w'\")\n\n            stream = _Stream(name, filemode, comptype, fileobj, bufsize)\n            try:\n                t = cls(name, filemode, stream, **kwargs)\n            except:\n                stream.close()\n                raise\n            t._extfileobj = False\n            return t\n\n        elif mode in \"aw\":\n            return cls.taropen(name, mode, fileobj, **kwargs)\n\n        raise ValueError(\"undiscernible mode\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef taropen(cls, name, mode=\"r\", fileobj=None, **kwargs):\n        if len(mode) > 1 or mode not in \"raw\":\n            raise ValueError(\"mode must be 'r', 'a' or 'w'\")\n        return cls(name, mode, fileobj, **kwargs)", "response": "Open uncompressed tar archive name for reading or writing."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gzopen(cls, name, mode=\"r\", fileobj=None, compresslevel=9, **kwargs):\n        if len(mode) > 1 or mode not in \"rw\":\n            raise ValueError(\"mode must be 'r' or 'w'\")\n\n        try:\n            import gzip\n            gzip.GzipFile\n        except (ImportError, AttributeError):\n            raise CompressionError(\"gzip module is not available\")\n\n        extfileobj = fileobj is not None\n        try:\n            fileobj = gzip.GzipFile(name, mode + \"b\", compresslevel, fileobj)\n            t = cls.taropen(name, mode, fileobj, **kwargs)\n        except IOError:\n            if not extfileobj and fileobj is not None:\n                fileobj.close()\n            if fileobj is None:\n                raise\n            raise ReadError(\"not a gzip file\")\n        except:\n            if not extfileobj and fileobj is not None:\n                fileobj.close()\n            raise\n        t._extfileobj = extfileobj\n        return t", "response": "Open a gzip compressed tar archive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nopening a compressed tar archive with bz2.", "response": "def bz2open(cls, name, mode=\"r\", fileobj=None, compresslevel=9, **kwargs):\n        \"\"\"Open bzip2 compressed tar archive name for reading or writing.\n           Appending is not allowed.\n        \"\"\"\n        if len(mode) > 1 or mode not in \"rw\":\n            raise ValueError(\"mode must be 'r' or 'w'.\")\n\n        try:\n            import bz2\n        except ImportError:\n            raise CompressionError(\"bz2 module is not available\")\n\n        if fileobj is not None:\n            fileobj = _BZ2Proxy(fileobj, mode)\n        else:\n            fileobj = bz2.BZ2File(name, mode, compresslevel=compresslevel)\n\n        try:\n            t = cls.taropen(name, mode, fileobj, **kwargs)\n        except (IOError, EOFError):\n            fileobj.close()\n            raise ReadError(\"not a bzip2 file\")\n        t._extfileobj = False\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getmember(self, name):\n        tarinfo = self._getmember(name)\n        if tarinfo is None:\n            raise KeyError(\"filename %r not found\" % name)\n        return tarinfo", "response": "Return a TarInfo object for the named member."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getmembers(self):\n        self._check()\n        if not self._loaded:    # if we want to obtain a list of\n            self._load()        # all members, we first have to\n                                # scan the whole archive.\n        return self.members", "response": "Return the members of the archive as a list of TarInfo objects. The members are returned in the same order as the members in the archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gettarinfo(self, name=None, arcname=None, fileobj=None):\n        self._check(\"aw\")\n\n        # When fileobj is given, replace name by\n        # fileobj's real name.\n        if fileobj is not None:\n            name = fileobj.name\n\n        # Building the name of the member in the archive.\n        # Backward slashes are converted to forward slashes,\n        # Absolute paths are turned to relative paths.\n        if arcname is None:\n            arcname = name\n        drv, arcname = os.path.splitdrive(arcname)\n        arcname = arcname.replace(os.sep, \"/\")\n        arcname = arcname.lstrip(\"/\")\n\n        # Now, fill the TarInfo object with\n        # information specific for the file.\n        tarinfo = self.tarinfo()\n        tarinfo.tarfile = self\n\n        # Use os.stat or os.lstat, depending on platform\n        # and if symlinks shall be resolved.\n        if fileobj is None:\n            if hasattr(os, \"lstat\") and not self.dereference:\n                statres = os.lstat(name)\n            else:\n                statres = os.stat(name)\n        else:\n            statres = os.fstat(fileobj.fileno())\n        linkname = \"\"\n\n        stmd = statres.st_mode\n        if stat.S_ISREG(stmd):\n            inode = (statres.st_ino, statres.st_dev)\n            if not self.dereference and statres.st_nlink > 1 and \\\n                    inode in self.inodes and arcname != self.inodes[inode]:\n                # Is it a hardlink to an already\n                # archived file?\n                type = LNKTYPE\n                linkname = self.inodes[inode]\n            else:\n                # The inode is added only if its valid.\n                # For win32 it is always 0.\n                type = REGTYPE\n                if inode[0]:\n                    self.inodes[inode] = arcname\n        elif stat.S_ISDIR(stmd):\n            type = DIRTYPE\n        elif stat.S_ISFIFO(stmd):\n            type = FIFOTYPE\n        elif stat.S_ISLNK(stmd):\n            type = SYMTYPE\n            linkname = os.readlink(name)\n        elif stat.S_ISCHR(stmd):\n            type = CHRTYPE\n        elif stat.S_ISBLK(stmd):\n            type = BLKTYPE\n        else:\n            return None\n\n        # Fill the TarInfo object with all\n        # information we can get.\n        tarinfo.name = arcname\n        tarinfo.mode = stmd\n        tarinfo.uid = statres.st_uid\n        tarinfo.gid = statres.st_gid\n        if type == REGTYPE:\n            tarinfo.size = statres.st_size\n        else:\n            tarinfo.size = 0\n        tarinfo.mtime = statres.st_mtime\n        tarinfo.type = type\n        tarinfo.linkname = linkname\n        if pwd:\n            try:\n                tarinfo.uname = pwd.getpwuid(tarinfo.uid)[0]\n            except KeyError:\n                pass\n        if grp:\n            try:\n                tarinfo.gname = grp.getgrgid(tarinfo.gid)[0]\n            except KeyError:\n                pass\n\n        if type in (CHRTYPE, BLKTYPE):\n            if hasattr(os, \"major\") and hasattr(os, \"minor\"):\n                tarinfo.devmajor = os.major(statres.st_rdev)\n                tarinfo.devminor = os.minor(statres.st_rdev)\n        return tarinfo", "response": "Create a TarInfo object for either the file name arcname or fileobj."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list(self, verbose=True):\n        self._check()\n\n        for tarinfo in self:\n            if verbose:\n                print(filemode(tarinfo.mode), end=' ')\n                print(\"%s/%s\" % (tarinfo.uname or tarinfo.uid,\n                                 tarinfo.gname or tarinfo.gid), end=' ')\n                if tarinfo.ischr() or tarinfo.isblk():\n                    print(\"%10s\" % (\"%d,%d\" \\\n                                    % (tarinfo.devmajor, tarinfo.devminor)), end=' ')\n                else:\n                    print(\"%10d\" % tarinfo.size, end=' ')\n                print(\"%d-%02d-%02d %02d:%02d:%02d\" \\\n                      % time.localtime(tarinfo.mtime)[:6], end=' ')\n\n            print(tarinfo.name + (\"/\" if tarinfo.isdir() else \"\"), end=' ')\n\n            if verbose:\n                if tarinfo.issym():\n                    print(\"->\", tarinfo.linkname, end=' ')\n                if tarinfo.islnk():\n                    print(\"link to\", tarinfo.linkname, end=' ')\n            print()", "response": "Print a table of contents of the archive."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the TarInfo object tarinfo to the archive. fileobj is optional.", "response": "def addfile(self, tarinfo, fileobj=None):\n        \"\"\"Add the TarInfo object `tarinfo' to the archive. If `fileobj' is\n           given, tarinfo.size bytes are read from it and added to the archive.\n           You can create TarInfo objects using gettarinfo().\n           On Windows platforms, `fileobj' should always be opened with mode\n           'rb' to avoid irritation about the file size.\n        \"\"\"\n        self._check(\"aw\")\n\n        tarinfo = copy.copy(tarinfo)\n\n        buf = tarinfo.tobuf(self.format, self.encoding, self.errors)\n        self.fileobj.write(buf)\n        self.offset += len(buf)\n\n        # If there's data to follow, append it.\n        if fileobj is not None:\n            copyfileobj(fileobj, self.fileobj, tarinfo.size)\n            blocks, remainder = divmod(tarinfo.size, BLOCKSIZE)\n            if remainder > 0:\n                self.fileobj.write(NUL * (BLOCKSIZE - remainder))\n                blocks += 1\n            self.offset += blocks * BLOCKSIZE\n\n        self.members.append(tarinfo)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extractall(self, path=\".\", members=None):\n        directories = []\n\n        if members is None:\n            members = self\n\n        for tarinfo in members:\n            if tarinfo.isdir():\n                # Extract directories with a safe mode.\n                directories.append(tarinfo)\n                tarinfo = copy.copy(tarinfo)\n                tarinfo.mode = 0o700\n            # Do not set_attrs directories, as we will do that further down\n            self.extract(tarinfo, path, set_attrs=not tarinfo.isdir())\n\n        # Reverse sort directories.\n        directories.sort(key=lambda a: a.name)\n        directories.reverse()\n\n        # Set correct owner, mtime and filemode on directories.\n        for tarinfo in directories:\n            dirpath = os.path.join(path, tarinfo.name)\n            try:\n                self.chown(tarinfo, dirpath)\n                self.utime(tarinfo, dirpath)\n                self.chmod(tarinfo, dirpath)\n            except ExtractError as e:\n                if self.errorlevel > 1:\n                    raise\n                else:\n                    self._dbg(1, \"tarfile: %s\" % e)", "response": "Extract all members from the archive to the current working directory and set owner modification time and permissions on directories afterwards."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract(self, member, path=\"\", set_attrs=True):\n        self._check(\"r\")\n\n        if isinstance(member, str):\n            tarinfo = self.getmember(member)\n        else:\n            tarinfo = member\n\n        # Prepare the link target for makelink().\n        if tarinfo.islnk():\n            tarinfo._link_target = os.path.join(path, tarinfo.linkname)\n\n        try:\n            self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n                                 set_attrs=set_attrs)\n        except EnvironmentError as e:\n            if self.errorlevel > 0:\n                raise\n            else:\n                if e.filename is None:\n                    self._dbg(1, \"tarfile: %s\" % e.strerror)\n                else:\n                    self._dbg(1, \"tarfile: %s %r\" % (e.strerror, e.filename))\n        except ExtractError as e:\n            if self.errorlevel > 1:\n                raise\n            else:\n                self._dbg(1, \"tarfile: %s\" % e)", "response": "Extract a member from the archive to the current working directory using its full name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extractfile(self, member):\n        self._check(\"r\")\n\n        if isinstance(member, str):\n            tarinfo = self.getmember(member)\n        else:\n            tarinfo = member\n\n        if tarinfo.isreg():\n            return self.fileobject(self, tarinfo)\n\n        elif tarinfo.type not in SUPPORTED_TYPES:\n            # If a member's type is unknown, it is treated as a\n            # regular file.\n            return self.fileobject(self, tarinfo)\n\n        elif tarinfo.islnk() or tarinfo.issym():\n            if isinstance(self.fileobj, _Stream):\n                # A small but ugly workaround for the case that someone tries\n                # to extract a (sym)link as a file-object from a non-seekable\n                # stream of tar blocks.\n                raise StreamError(\"cannot extract (sym)link as file object\")\n            else:\n                # A (sym)link's file object is its target's file object.\n                return self.extractfile(self._find_link_target(tarinfo))\n        else:\n            # If there's no data associated with the member (directory, chrdev,\n            # blkdev, etc.), return None instead of a file object.\n            return None", "response": "Extracts a member from the archive as a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _extract_member(self, tarinfo, targetpath, set_attrs=True):\n        # Fetch the TarInfo object for the given name\n        # and build the destination pathname, replacing\n        # forward slashes to platform specific separators.\n        targetpath = targetpath.rstrip(\"/\")\n        targetpath = targetpath.replace(\"/\", os.sep)\n\n        # Create all upper directories.\n        upperdirs = os.path.dirname(targetpath)\n        if upperdirs and not os.path.exists(upperdirs):\n            # Create directories that are not part of the archive with\n            # default permissions.\n            os.makedirs(upperdirs)\n\n        if tarinfo.islnk() or tarinfo.issym():\n            self._dbg(1, \"%s -> %s\" % (tarinfo.name, tarinfo.linkname))\n        else:\n            self._dbg(1, tarinfo.name)\n\n        if tarinfo.isreg():\n            self.makefile(tarinfo, targetpath)\n        elif tarinfo.isdir():\n            self.makedir(tarinfo, targetpath)\n        elif tarinfo.isfifo():\n            self.makefifo(tarinfo, targetpath)\n        elif tarinfo.ischr() or tarinfo.isblk():\n            self.makedev(tarinfo, targetpath)\n        elif tarinfo.islnk() or tarinfo.issym():\n            self.makelink(tarinfo, targetpath)\n        elif tarinfo.type not in SUPPORTED_TYPES:\n            self.makeunknown(tarinfo, targetpath)\n        else:\n            self.makefile(tarinfo, targetpath)\n\n        if set_attrs:\n            self.chown(tarinfo, targetpath)\n            if not tarinfo.issym():\n                self.chmod(tarinfo, targetpath)\n                self.utime(tarinfo, targetpath)", "response": "Extract the TarInfo object tarinfo to a physical physical\n           file called targetpath."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makedir(self, tarinfo, targetpath):\n        try:\n            # Use a safe mode for the directory, the real mode is set\n            # later in _extract_member().\n            os.mkdir(targetpath, 0o700)\n        except EnvironmentError as e:\n            if e.errno != errno.EEXIST:\n                raise", "response": "Make a directory called targetpath."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makefile(self, tarinfo, targetpath):\n        source = self.fileobj\n        source.seek(tarinfo.offset_data)\n        target = bltn_open(targetpath, \"wb\")\n        if tarinfo.sparse is not None:\n            for offset, size in tarinfo.sparse:\n                target.seek(offset)\n                copyfileobj(source, target, size)\n        else:\n            copyfileobj(source, target, tarinfo.size)\n        target.seek(tarinfo.size)\n        target.truncate()\n        target.close()", "response": "Make a file called targetpath."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a file from a TarInfo object with an unknown type at targetpath.", "response": "def makeunknown(self, tarinfo, targetpath):\n        \"\"\"Make a file from a TarInfo object with an unknown type\n           at targetpath.\n        \"\"\"\n        self.makefile(tarinfo, targetpath)\n        self._dbg(1, \"tarfile: Unknown file type %r, \" \\\n                     \"extracted as regular file.\" % tarinfo.type)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makefifo(self, tarinfo, targetpath):\n        if hasattr(os, \"mkfifo\"):\n            os.mkfifo(targetpath)\n        else:\n            raise ExtractError(\"fifo not supported by system\")", "response": "Make a fifo called targetpath."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a character or block device called targetpath.", "response": "def makedev(self, tarinfo, targetpath):\n        \"\"\"Make a character or block device called targetpath.\n        \"\"\"\n        if not hasattr(os, \"mknod\") or not hasattr(os, \"makedev\"):\n            raise ExtractError(\"special devices not supported by system\")\n\n        mode = tarinfo.mode\n        if tarinfo.isblk():\n            mode |= stat.S_IFBLK\n        else:\n            mode |= stat.S_IFCHR\n\n        os.mknod(targetpath, mode,\n                 os.makedev(tarinfo.devmajor, tarinfo.devminor))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a symbolic link called targetpath.", "response": "def makelink(self, tarinfo, targetpath):\n        \"\"\"Make a (symbolic) link called targetpath. If it cannot be created\n          (platform limitation), we try to make a copy of the referenced file\n          instead of a link.\n        \"\"\"\n        try:\n            # For systems that support symbolic and hard links.\n            if tarinfo.issym():\n                os.symlink(tarinfo.linkname, targetpath)\n            else:\n                # See extract().\n                if os.path.exists(tarinfo._link_target):\n                    os.link(tarinfo._link_target, targetpath)\n                else:\n                    self._extract_member(self._find_link_target(tarinfo),\n                                         targetpath)\n        except symlink_exception:\n            if tarinfo.issym():\n                linkpath = os.path.join(os.path.dirname(tarinfo.name),\n                                        tarinfo.linkname)\n            else:\n                linkpath = tarinfo.linkname\n        else:\n            try:\n                self._extract_member(self._find_link_target(tarinfo),\n                                     targetpath)\n            except KeyError:\n                raise ExtractError(\"unable to resolve link inside archive\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nchange the owner of targetpath according to tarinfo.", "response": "def chown(self, tarinfo, targetpath):\n        \"\"\"Set owner of targetpath according to tarinfo.\n        \"\"\"\n        if pwd and hasattr(os, \"geteuid\") and os.geteuid() == 0:\n            # We have to be root to do so.\n            try:\n                g = grp.getgrnam(tarinfo.gname)[2]\n            except KeyError:\n                g = tarinfo.gid\n            try:\n                u = pwd.getpwnam(tarinfo.uname)[2]\n            except KeyError:\n                u = tarinfo.uid\n            try:\n                if tarinfo.issym() and hasattr(os, \"lchown\"):\n                    os.lchown(targetpath, u, g)\n                else:\n                    if sys.platform != \"os2emx\":\n                        os.chown(targetpath, u, g)\n            except EnvironmentError as e:\n                raise ExtractError(\"could not change owner\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef chmod(self, tarinfo, targetpath):\n        if hasattr(os, 'chmod'):\n            try:\n                os.chmod(targetpath, tarinfo.mode)\n            except EnvironmentError as e:\n                raise ExtractError(\"could not change mode\")", "response": "Change the file permissions of targetpath according to tarinfo."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef utime(self, tarinfo, targetpath):\n        if not hasattr(os, 'utime'):\n            return\n        try:\n            os.utime(targetpath, (tarinfo.mtime, tarinfo.mtime))\n        except EnvironmentError as e:\n            raise ExtractError(\"could not change modification time\")", "response": "Set modification time of targetpath according to tarinfo."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef next(self):\n        self._check(\"ra\")\n        if self.firstmember is not None:\n            m = self.firstmember\n            self.firstmember = None\n            return m\n\n        # Read the next block.\n        self.fileobj.seek(self.offset)\n        tarinfo = None\n        while True:\n            try:\n                tarinfo = self.tarinfo.fromtarfile(self)\n            except EOFHeaderError as e:\n                if self.ignore_zeros:\n                    self._dbg(2, \"0x%X: %s\" % (self.offset, e))\n                    self.offset += BLOCKSIZE\n                    continue\n            except InvalidHeaderError as e:\n                if self.ignore_zeros:\n                    self._dbg(2, \"0x%X: %s\" % (self.offset, e))\n                    self.offset += BLOCKSIZE\n                    continue\n                elif self.offset == 0:\n                    raise ReadError(str(e))\n            except EmptyHeaderError:\n                if self.offset == 0:\n                    raise ReadError(\"empty file\")\n            except TruncatedHeaderError as e:\n                if self.offset == 0:\n                    raise ReadError(str(e))\n            except SubsequentHeaderError as e:\n                raise ReadError(str(e))\n            break\n\n        if tarinfo is not None:\n            self.members.append(tarinfo)\n        else:\n            self._loaded = True\n\n        return tarinfo", "response": "Return the next member of the archive as a TarInfo object when there is no more available. Return None if there is no more member."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getmember(self, name, tarinfo=None, normalize=False):\n        # Ensure that all members have been loaded.\n        members = self.getmembers()\n\n        # Limit the member search list up to tarinfo.\n        if tarinfo is not None:\n            members = members[:members.index(tarinfo)]\n\n        if normalize:\n            name = os.path.normpath(name)\n\n        for member in reversed(members):\n            if normalize:\n                member_name = os.path.normpath(member.name)\n            else:\n                member_name = member.name\n\n            if name == member_name:\n                return member", "response": "Find an archive member by name from bottom to top."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load(self):\n        while True:\n            tarinfo = self.next()\n            if tarinfo is None:\n                break\n        self._loaded = True", "response": "Read through the entire archive file and look for readable\n           members."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if TarFile is still open and if the operation s mode corresponds to TarFile s mode.", "response": "def _check(self, mode=None):\n        \"\"\"Check if TarFile is still open, and if the operation's mode\n           corresponds to TarFile's mode.\n        \"\"\"\n        if self.closed:\n            raise IOError(\"%s is closed\" % self.__class__.__name__)\n        if mode is not None and self.mode not in mode:\n            raise IOError(\"bad operation for mode %r\" % self.mode)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_link_target(self, tarinfo):\n        if tarinfo.issym():\n            # Always search the entire archive.\n            linkname = os.path.dirname(tarinfo.name) + \"/\" + tarinfo.linkname\n            limit = None\n        else:\n            # Search the archive before the link, because a hard link is\n            # just a reference to an already archived file.\n            linkname = tarinfo.linkname\n            limit = tarinfo\n\n        member = self._getmember(linkname, tarinfo=limit, normalize=True)\n        if member is None:\n            raise KeyError(\"linkname %r not found\" % linkname)\n        return member", "response": "Find the target member of a symlink or hardlink member in the archive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting debugging output to sys. stderr.", "response": "def _dbg(self, level, msg):\n        \"\"\"Write debugging output to sys.stderr.\n        \"\"\"\n        if level <= self.debug:\n            print(msg, file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the absolute path to a given relative path.", "response": "def path_to(self, p):\n        \"\"\"Returns the absolute path to a given relative path.\"\"\"\n        if os.path.isabs(p):\n            return p\n\n        return os.sep.join([self._original_dir, p])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_package_list(self, package_section):\n        from pipenv.vendor.requirementslib.utils import is_vcs\n        ps = {}\n        # TODO: Separate the logic for showing packages from the filters for supplying pip-tools\n        for k, v in self.parsed_pipfile.get(package_section, {}).items():\n            # Skip editable VCS deps.\n            if hasattr(v, \"keys\"):\n                # When a vcs url is gven without editable it only appears as a key\n                # Eliminate any vcs, path, or url entries which are not editable\n                # Since pip-tools can't do deep resolution on them, even setuptools-installable ones\n                if (\n                    is_vcs(v)\n                    or is_vcs(k)\n                    or (is_installable_file(k) or is_installable_file(v))\n                    or any(\n                        (\n                            prefix in v\n                            and (os.path.isfile(v[prefix]) or is_valid_url(v[prefix]))\n                        )\n                        for prefix in [\"path\", \"file\"]\n                    )\n                ):\n                    # If they are editable, do resolve them\n                    if \"editable\" not in v:\n                        # allow wheels to be passed through\n                        if not (\n                            hasattr(v, \"keys\")\n                            and v.get(\"path\", v.get(\"file\", \"\")).endswith(\".whl\")\n                        ):\n                            continue\n                        ps.update({k: v})\n\n                    else:\n                        ps.update({k: v})\n                else:\n                    ps.update({k: v})\n            else:\n                # Since these entries have no attributes we know they are not editable\n                # So we can safely exclude things that need to be editable in order to be resolved\n                # First exclude anything that is a vcs entry either in the key or value\n                if not (\n                    any(is_vcs(i) for i in [k, v])\n                    or\n                    # Then exclude any installable files that are not directories\n                    # Because pip-tools can resolve setup.py for example\n                    any(is_installable_file(i) for i in [k, v])\n                    or\n                    # Then exclude any URLs because they need to be editable also\n                    # Things that are excluded can only be 'shallow resolved'\n                    any(is_valid_url(i) for i in [k, v])\n                ):\n                    ps.update({k: v})\n        return ps", "response": "Builds a list of packages for pip - tools to consume."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_virtualenv_hash(self, name):\n\n        def get_name(name, location):\n            name = self._sanitize(name)\n            hash = hashlib.sha256(location.encode()).digest()[:6]\n            encoded_hash = base64.urlsafe_b64encode(hash).decode()\n            return name, encoded_hash[:8]\n\n        clean_name, encoded_hash = get_name(name, self.pipfile_location)\n        venv_name = \"{0}-{1}\".format(clean_name, encoded_hash)\n\n        # This should work most of the time for\n        #   Case-sensitive filesystems,\n        #   In-project venv\n        #   \"Proper\" path casing (on non-case-sensitive filesystems).\n        if (\n            not fnmatch.fnmatch(\"A\", \"a\")\n            or self.is_venv_in_project()\n            or get_workon_home().joinpath(venv_name).exists()\n        ):\n            return clean_name, encoded_hash\n\n        # Check for different capitalization of the same project.\n        for path in get_workon_home().iterdir():\n            if not is_virtual_environment(path):\n                continue\n            try:\n                env_name, hash_ = path.name.rsplit(\"-\", 1)\n            except ValueError:\n                continue\n            if len(hash_) != 8 or env_name.lower() != name.lower():\n                continue\n            return get_name(env_name, self.pipfile_location.replace(name, env_name))\n\n        # Use the default if no matching env exists.\n        return clean_name, encoded_hash", "response": "Get the name of the virtualenv adjusted for windows if needed\n            = > base64 - encoded - base64 - encoded - hash"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a proper name to the database.", "response": "def register_proper_name(self, name):\n        \"\"\"Registers a proper name to the database.\"\"\"\n        with self.proper_names_db_path.open(\"a\") as f:\n            f.write(u\"{0}\\n\".format(name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parsed_pipfile(self):\n        contents = self.read_pipfile()\n        # use full contents to get around str/bytes 2/3 issues\n        cache_key = (self.pipfile_location, contents)\n        if cache_key not in _pipfile_cache:\n            parsed = self._parse_pipfile(contents)\n            _pipfile_cache[cache_key] = parsed\n        return _pipfile_cache[cache_key]", "response": "Parse Pipfile into a TOMLFile and cache it"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _lockfile(self):\n        pfile = pipfile.load(self.pipfile_location, inject_env=False)\n        lockfile = json.loads(pfile.lock())\n        for section in (\"default\", \"develop\"):\n            lock_section = lockfile.get(section, {})\n            for key in list(lock_section.keys()):\n                norm_key = pep423_name(key)\n                lockfile[section][norm_key] = lock_section.pop(key)\n        return lockfile", "response": "Pipfile. lock divided by PyPI and external dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef all_packages(self):\n        p = dict(self.parsed_pipfile.get(\"dev-packages\", {}))\n        p.update(self.parsed_pipfile.get(\"packages\", {}))\n        return p", "response": "Returns a list of all packages."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the Pipfile filled with juicy defaults.", "response": "def create_pipfile(self, python=None):\n        \"\"\"Creates the Pipfile, filled with juicy defaults.\"\"\"\n        from .vendor.pip_shims.shims import (\n            ConfigOptionParser, make_option_group, index_group\n        )\n\n        config_parser = ConfigOptionParser(name=self.name)\n        config_parser.add_option_group(make_option_group(index_group, config_parser))\n        install = config_parser.option_groups[0]\n        indexes = (\n            \" \".join(install.get_option(\"--extra-index-url\").default)\n            .lstrip(\"\\n\")\n            .split(\"\\n\")\n        )\n        sources = [DEFAULT_SOURCE,]\n        for i, index in enumerate(indexes):\n            if not index:\n                continue\n\n            source_name = \"pip_index_{}\".format(i)\n            verify_ssl = index.startswith(\"https\")\n            sources.append(\n                {u\"url\": index, u\"verify_ssl\": verify_ssl, u\"name\": source_name}\n            )\n\n        data = {\n            u\"source\": sources,\n            # Default packages.\n            u\"packages\": {},\n            u\"dev-packages\": {},\n        }\n        # Default requires.\n        required_python = python\n        if not python:\n            if self.virtualenv_location:\n                required_python = self.which(\"python\", self.virtualenv_location)\n            else:\n                required_python = self.which(\"python\")\n        version = python_version(required_python) or PIPENV_DEFAULT_PYTHON_VERSION\n        if version and len(version) >= 3:\n            data[u\"requires\"] = {\"python_version\": version[: len(\"2.7\")]}\n        self.write_toml(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_toml(self, data, path=None):\n        if path is None:\n            path = self.pipfile_location\n        data = convert_toml_outline_tables(data)\n        try:\n            formatted_data = tomlkit.dumps(data).rstrip()\n        except Exception:\n            document = tomlkit.document()\n            for section in (\"packages\", \"dev-packages\"):\n                document[section] = tomlkit.container.Table()\n                # Convert things to inline tables \u2014 fancy :)\n                for package in data.get(section, {}):\n                    if hasattr(data[section][package], \"keys\"):\n                        table = tomlkit.inline_table()\n                        table.update(data[section][package])\n                        document[section][package] = table\n                    else:\n                        document[section][package] = tomlkit.string(data[section][package])\n            formatted_data = tomlkit.dumps(document).rstrip()\n\n        if (\n            vistir.compat.Path(path).absolute()\n            == vistir.compat.Path(self.pipfile_location).absolute()\n        ):\n            newlines = self._pipfile_newlines\n        else:\n            newlines = DEFAULT_NEWLINES\n        formatted_data = cleanup_toml(formatted_data)\n        with io.open(path, \"w\", newline=newlines) as f:\n            f.write(formatted_data)\n        # pipfile is mutated!\n        self.clear_pipfile_cache()", "response": "Writes the given data structure out as TOML."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite out the lockfile.", "response": "def write_lockfile(self, content):\n        \"\"\"Write out the lockfile.\n        \"\"\"\n        s = self._lockfile_encoder.encode(content)\n        open_kwargs = {\"newline\": self._lockfile_newlines, \"encoding\": \"utf-8\"}\n        with vistir.contextmanagers.atomic_open_for_write(\n            self.lockfile_location, **open_kwargs\n        ) as f:\n            f.write(s)\n            # Write newline at end of document. GH-319.\n            # Only need '\\n' here; the file object handles the rest.\n            if not s.endswith(u\"\\n\"):\n                f.write(u\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_source(self, source):\n        if not is_valid_url(source):\n            try:\n                source = self.get_source(name=source)\n            except SourceNotFound:\n                source = self.get_source(url=source)\n        else:\n            source = self.get_source(url=source)\n        return source", "response": "Given a source find it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the equivalent package name in pipfile", "response": "def get_package_name_in_pipfile(self, package_name, dev=False):\n        \"\"\"Get the equivalent package name in pipfile\"\"\"\n        key = \"dev-packages\" if dev else \"packages\"\n        section = self.parsed_pipfile.get(key, {})\n        package_name = pep423_name(package_name)\n        for name in section.keys():\n            if pep423_name(name) == package_name:\n                return name\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a given index to the Pipfile.", "response": "def add_index_to_pipfile(self, index, verify_ssl=True):\n        \"\"\"Adds a given index to the Pipfile.\"\"\"\n        # Read and append Pipfile.\n        p = self.parsed_pipfile\n        try:\n            self.get_source(url=index)\n        except SourceNotFound:\n            source = {\"url\": index, \"verify_ssl\": verify_ssl}\n        else:\n            return\n        source[\"name\"] = self.src_name_from_url(index)\n        # Add the package to the group.\n        if \"source\" not in p:\n            p[\"source\"] = [source]\n        else:\n            p[\"source\"].append(source)\n        # Write Pipfile.\n        self.write_toml(p)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure proper casing of Pipfile packages", "response": "def ensure_proper_casing(self):\n        \"\"\"Ensures proper casing of Pipfile packages\"\"\"\n        pfile = self.parsed_pipfile\n        casing_changed = self.proper_case_section(pfile.get(\"packages\", {}))\n        casing_changed |= self.proper_case_section(pfile.get(\"dev-packages\", {}))\n        return casing_changed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef proper_case_section(self, section):\n        # Casing for section.\n        changed_values = False\n        unknown_names = [k for k in section.keys() if k not in set(self.proper_names)]\n        # Replace each package with proper casing.\n        for dep in unknown_names:\n            try:\n                # Get new casing for package name.\n                new_casing = proper_case(dep)\n            except IOError:\n                # Unable to normalize package name.\n                continue\n\n            if new_casing != dep:\n                changed_values = True\n                self.register_proper_name(new_casing)\n                # Replace old value with new value.\n                old_value = section[dep]\n                section[new_casing] = old_value\n                del section[dep]\n        # Return whether or not values have been changed.\n        return changed_values", "response": "Verify proper casing is retrieved when available for each dependency in the section."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a native Python type from the list of compiled nodes.", "response": "def native_concat(nodes):\n    \"\"\"Return a native Python type from the list of compiled nodes. If the\n    result is a single node, its value is returned. Otherwise, the nodes are\n    concatenated as strings. If the result can be parsed with\n    :func:`ast.literal_eval`, the parsed value is returned. Otherwise, the\n    string is returned.\n    \"\"\"\n    head = list(islice(nodes, 2))\n\n    if not head:\n        return None\n\n    if len(head) == 1:\n        out = head[0]\n    else:\n        out = u''.join([text_type(v) for v in chain(head, nodes)])\n\n    try:\n        return literal_eval(out)\n    except (ValueError, SyntaxError, MemoryError):\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_Output(self, node, frame):\n        if self.has_known_extends and frame.require_output_check:\n            return\n\n        finalize = self.environment.finalize\n        finalize_context = getattr(finalize, 'contextfunction', False)\n        finalize_eval = getattr(finalize, 'evalcontextfunction', False)\n        finalize_env = getattr(finalize, 'environmentfunction', False)\n\n        if finalize is not None:\n            if finalize_context or finalize_eval:\n                const_finalize = None\n            elif finalize_env:\n                def const_finalize(x):\n                    return finalize(self.environment, x)\n            else:\n                const_finalize = finalize\n        else:\n            def const_finalize(x):\n                return x\n\n        # If we are inside a frame that requires output checking, we do so.\n        outdent_later = False\n\n        if frame.require_output_check:\n            self.writeline('if parent_template is None:')\n            self.indent()\n            outdent_later = True\n\n        # Try to evaluate as many chunks as possible into a static string at\n        # compile time.\n        body = []\n\n        for child in node.nodes:\n            try:\n                if const_finalize is None:\n                    raise nodes.Impossible()\n\n                const = child.as_const(frame.eval_ctx)\n                if not has_safe_repr(const):\n                    raise nodes.Impossible()\n            except nodes.Impossible:\n                body.append(child)\n                continue\n\n            # the frame can't be volatile here, because otherwise the as_const\n            # function would raise an Impossible exception at that point\n            try:\n                if frame.eval_ctx.autoescape:\n                    if hasattr(const, '__html__'):\n                        const = const.__html__()\n                    else:\n                        const = escape(const)\n\n                const = const_finalize(const)\n            except Exception:\n                # if something goes wrong here we evaluate the node at runtime\n                # for easier debugging\n                body.append(child)\n                continue\n\n            if body and isinstance(body[-1], list):\n                body[-1].append(const)\n            else:\n                body.append([const])\n\n        # if we have less than 3 nodes or a buffer we yield or extend/append\n        if len(body) < 3 or frame.buffer is not None:\n            if frame.buffer is not None:\n                # for one item we append, for more we extend\n                if len(body) == 1:\n                    self.writeline('%s.append(' % frame.buffer)\n                else:\n                    self.writeline('%s.extend((' % frame.buffer)\n\n                self.indent()\n\n            for item in body:\n                if isinstance(item, list):\n                    val = repr(native_concat(item))\n\n                    if frame.buffer is None:\n                        self.writeline('yield ' + val)\n                    else:\n                        self.writeline(val + ',')\n                else:\n                    if frame.buffer is None:\n                        self.writeline('yield ', item)\n                    else:\n                        self.newline(item)\n\n                    close = 0\n\n                    if finalize is not None:\n                        self.write('environment.finalize(')\n\n                        if finalize_context:\n                            self.write('context, ')\n\n                        close += 1\n\n                    self.visit(item, frame)\n\n                    if close > 0:\n                        self.write(')' * close)\n\n                    if frame.buffer is not None:\n                        self.write(',')\n\n            if frame.buffer is not None:\n                # close the open parentheses\n                self.outdent()\n                self.writeline(len(body) == 1 and ')' or '))')\n\n        # otherwise we create a format string as this is faster in that case\n        else:\n            format = []\n            arguments = []\n\n            for item in body:\n                if isinstance(item, list):\n                    format.append(native_concat(item).replace('%', '%%'))\n                else:\n                    format.append('%s')\n                    arguments.append(item)\n\n            self.writeline('yield ')\n            self.write(repr(concat(format)) + ' % (')\n            self.indent()\n\n            for argument in arguments:\n                self.newline(argument)\n                close = 0\n\n                if finalize is not None:\n                    self.write('environment.finalize(')\n\n                    if finalize_context:\n                        self.write('context, ')\n                    elif finalize_eval:\n                        self.write('context.eval_ctx, ')\n                    elif finalize_env:\n                        self.write('environment, ')\n\n                    close += 1\n\n                self.visit(argument, frame)\n                self.write(')' * close + ', ')\n\n            self.outdent()\n            self.writeline(')')\n\n        if outdent_later:\n            self.outdent()", "response": "This method is called by the code generator when the output of a function is not yet generated."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(self, *args, **kwargs):\n        self.clear()\n        self.updateall(*args, **kwargs)\n        return self", "response": "Load all existing key - value items and import all key - value items from\n        <mapping > Returns a new mapping object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating this dictionary with the items from <mapping > replacing them with shared keys before adding new key - value items.", "response": "def updateall(self, *args, **kwargs):\n        \"\"\"\n        Update this dictionary with the items from <mapping>, replacing\n        existing key:value items with shared keys before adding new key:value\n        items.\n\n        Example:\n          omd = omdict([(1,1), (2,2)])\n          omd.updateall([(2,'two'), (1,'one'), (2,222), (1,111)])\n          omd.allitems() == [(1, 'one'), (2, 'two'), (2, 222), (1, 111)]\n\n        Returns: <self>.\n        \"\"\"\n        self._update_updateall(False, *args, **kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bin_update_items(self, items, replace_at_most_one,\n                          replacements, leftovers):\n        \"\"\"\n        <replacements and <leftovers> are modified directly, ala pass by\n        reference.\n        \"\"\"\n        for key, value in items:\n            # If there are existing items with key <key> that have yet to be\n            # marked for replacement, mark that item's value to be replaced by\n            # <value> by appending it to <replacements>.\n            if key in self and key not in replacements:\n                replacements[key] = [value]\n            elif (key in self and not replace_at_most_one and\n                  len(replacements[key]) < len(self.values(key))):\n                replacements[key].append(value)\n            else:\n                if replace_at_most_one:\n                    replacements[key] = [value]\n                else:\n                    leftovers.append((key, value))", "response": "Bin update items with new values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the list of values for the given key. If no key is in the dictionary returns default.", "response": "def getlist(self, key, default=[]):\n        \"\"\"\n        Returns: The list of values for <key> if <key> is in the dictionary,\n        else <default>. If <default> is not provided, an empty list is\n        returned.\n        \"\"\"\n        if key in self:\n            return [node.value for node in self._map[key]]\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addlist(self, key, valuelist=[]):\n        for value in valuelist:\n            self.add(key, value)\n        return self", "response": "Add the values in valuelist to the list of values for key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the list of values to values.", "response": "def setlist(self, key, values):\n        \"\"\"\n        Sets <key>'s list of values to <values>. Existing items with key <key>\n        are first replaced with new values from <values>. Any remaining old\n        items that haven't been replaced with new values are deleted, and any\n        new values from <values> that don't have corresponding items with <key>\n        to replace are appended to the end of the list of all items.\n\n        If values is an empty list, [], <key> is deleted, equivalent in action\n        to del self[<key>].\n\n        Example:\n          omd = omdict([(1,1), (2,2)])\n          omd.setlist(1, [11, 111])\n          omd.allitems() == [(1,11), (2,2), (1,111)]\n\n          omd = omdict([(1,1), (1,11), (2,2), (1,111)])\n          omd.setlist(1, [None])\n          omd.allitems() == [(1,None), (2,2)]\n\n          omd = omdict([(1,1), (1,11), (2,2), (1,111)])\n          omd.setlist(1, [])\n          omd.allitems() == [(2,2)]\n\n        Returns: <self>.\n        \"\"\"\n        if not values and key in self:\n            self.pop(key)\n        else:\n            it = zip_longest(\n                list(self._map.get(key, [])), values, fillvalue=_absent)\n            for node, value in it:\n                if node is not _absent and value is not _absent:\n                    node.value = value\n                elif node is _absent:\n                    self.add(key, value)\n                elif value is _absent:\n                    self._map[key].remove(node)\n                    self._items.removenode(node)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all values from the values of key.", "response": "def removevalues(self, key, values):\n        \"\"\"\n        Removes all <values> from the values of <key>. If <key> has no\n        remaining values after removevalues(), the key is popped.\n\n        Example:\n          omd = omdict([(1, 1), (1, 11), (1, 1), (1, 111)])\n          omd.removevalues(1, [1, 111])\n          omd.allitems() == [(1, 11)]\n\n        Returns: <self>.\n        \"\"\"\n        self.setlist(key, [v for v in self.getlist(key) if v not in values])\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef poplist(self, key, default=_absent):\n        if key in self:\n            values = self.getlist(key)\n            del self._map[key]\n            for node, nodekey, nodevalue in self._items:\n                if nodekey == key:\n                    self._items.removenode(node)\n            return values\n        elif key not in self._map and default is not _absent:\n            return default\n        raise KeyError(key)", "response": "Returns a list of values for the key. If the key is not in the dictionary return default."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopping and return a key - value item.", "response": "def popitem(self, fromall=False, last=True):\n        \"\"\"\n        Pop and return a key:value item.\n\n        If <fromall> is False, items()[0] is popped if <last> is False or\n        items()[-1] is popped if <last> is True. All remaining items with the\n        same key are removed.\n\n        If <fromall> is True, allitems()[0] is popped if <last> is False or\n        allitems()[-1] is popped if <last> is True. Any remaining items with\n        the same key remain.\n\n        Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.popitem() == (3,3)\n          omd.popitem(fromall=False, last=False) == (1,1)\n          omd.popitem(fromall=False, last=False) == (2,2)\n\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.popitem(fromall=True, last=False) == (1,1)\n          omd.popitem(fromall=True, last=False) == (1,11)\n          omd.popitem(fromall=True, last=True) == (3,3)\n          omd.popitem(fromall=True, last=False) == (1,111)\n\n        Params:\n          fromall: Whether to pop an item from items() (<fromall> is True) or\n            allitems() (<fromall> is False).\n          last: Boolean whether to pop the first item or last item of items()\n            or allitems().\n        Raises: KeyError if the dictionary is empty.\n        Returns: The first or last item from item() or allitem().\n        \"\"\"\n        if not self._items:\n            raise KeyError('popitem(): %s is empty' % self.__class__.__name__)\n\n        if fromall:\n            node = self._items[-1 if last else 0]\n            key = node.key\n            return key, self.popvalue(key, last=last)\n        else:\n            key = list(self._map.keys())[-1 if last else 0]\n            return key, self.pop(key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npop and return a key : valuelist item comprised of a key and that key s associated list of values.", "response": "def poplistitem(self, last=True):\n        \"\"\"\n        Pop and return a key:valuelist item comprised of a key and that key's\n        list of values. If <last> is False, a key:valuelist item comprised of\n        keys()[0] and its list of values is popped and returned. If <last> is\n        True, a key:valuelist item comprised of keys()[-1] and its list of\n        values is popped and returned.\n\n        Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.poplistitem(last=True) == (3,[3])\n          omd.poplistitem(last=False) == (1,[1,11,111])\n\n        Params:\n          last: Boolean whether to pop the first or last key and its associated\n            list of values.\n        Raises: KeyError if the dictionary is empty.\n        Returns: A two-tuple comprised of the first or last key and its\n          associated list of values.\n        \"\"\"\n        if not self._items:\n            s = 'poplistitem(): %s is empty' % self.__class__.__name__\n            raise KeyError(s)\n\n        key = self.keys()[-1 if last else 0]\n        return key, self.poplist(key)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all the values of the key. If key is not provided and is not in the dictionary returns a list of all the values of the key. If key is provided and is not a dictionary key returns a list of all the values of the key. If key is provided and is not a dictionary key returns a list of all the values of the key.", "response": "def values(self, key=_absent):\n        \"\"\"\n        Raises: KeyError if <key> is provided and not in the dictionary.\n        Returns: List created from itervalues(<key>).If <key> is provided and\n          is a dictionary key, only values of items with key <key> are\n          returned.\n        \"\"\"\n        if key is not _absent and key in self._map:\n            return self.getlist(key)\n        return list(self.itervalues())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an iterator over the values of the items in the dictionary.", "response": "def itervalues(self, key=_absent):\n        \"\"\"\n        Parity with dict.itervalues() except the optional <key> parameter has\n        been added. If <key> is provided, only values from items with the\n        provided key are iterated over. KeyError is raised if <key> is provided\n        and not in the dictionary.\n\n        Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.itervalues(1) -> 1 -> 11 -> 111\n          omd.itervalues() -> 1 -> 11 -> 111 -> 2 -> 3\n\n        Raises: KeyError if <key> is provided and isn't in the dictionary.\n        Returns: An iterator over the values() of the dictionary, or only the\n          values of key <key> if <key> is provided.\n        \"\"\"\n        if key is not _absent:\n            if key in self:\n                return iter([node.value for node in self._map[key]])\n            raise KeyError(key)\n        return iter([nodes[0].value for nodes in six.itervalues(self._map)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an iterator over every item in the dictionary.", "response": "def iterallitems(self, key=_absent):\n        '''\n        Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.iterallitems() == (1,1) -> (1,11) -> (1,111) -> (2,2) -> (3,3)\n          omd.iterallitems(1) == (1,1) -> (1,11) -> (1,111)\n\n        Raises: KeyError if <key> is provided and not in the dictionary.\n        Returns: An iterator over every item in the diciontary. If <key> is\n          provided, only items with the key <key> are iterated over.\n        '''\n        if key is not _absent:\n            # Raises KeyError if <key> is not in self._map.\n            return self.iteritems(key)\n        return self._items.iteritems()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iterallvalues(self, key=_absent):\n        '''\n        Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.iterallvalues() == 1 -> 11 -> 111 -> 2 -> 3\n\n        Returns: An iterator over the values of every item in the dictionary.\n        '''\n        if key is not _absent:\n            if key in self:\n                return iter(self.getlist(key))\n            raise KeyError(key)\n        return self._items.itervalues()", "response": "Returns an iterator over the values of every item in the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreverse the order of all items in the dictionary.", "response": "def reverse(self):\n        \"\"\"\n        Reverse the order of all items in the dictionary.\n\n        Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.reverse()\n          omd.allitems() == [(3,3), (2,2), (1,111), (1,11), (1,1)]\n\n        Returns: <self>.\n        \"\"\"\n        for key in six.iterkeys(self._map):\n            self._map[key].reverse()\n        self._items.reverse()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if there are conflicting or missing requirements.", "response": "def check_requirements(self, reqs):\n        # type: (Iterable[str]) -> Tuple[Set[Tuple[str, str]], Set[str]]\n        \"\"\"Return 2 sets:\n            - conflicting requirements: set of (installed, wanted) reqs tuples\n            - missing requirements: set of reqs\n        \"\"\"\n        missing = set()\n        conflicting = set()\n        if reqs:\n            ws = WorkingSet(self._lib_dirs)\n            for req in reqs:\n                try:\n                    if ws.find(Requirement.parse(req)) is None:\n                        missing.add(req)\n                except VersionConflict as e:\n                    conflicting.add((str(e.args[0].as_requirement()),\n                                     str(e.args[1])))\n        return conflicting, missing"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives an iterable of arguments and an iterable of nargs specifications returns a tuple with all the unpacked arguments at the first index and all remaining arguments as the second.", "response": "def _unpack_args(args, nargs_spec):\n    \"\"\"Given an iterable of arguments and an iterable of nargs specifications,\n    it returns a tuple with all the unpacked arguments at the first index\n    and all remaining arguments as the second.\n\n    The nargs specification is the number of arguments that should be consumed\n    or `-1` to indicate that this position should eat up all the remainders.\n\n    Missing items are filled with `None`.\n    \"\"\"\n    args = deque(args)\n    nargs_spec = deque(nargs_spec)\n    rv = []\n    spos = None\n\n    def _fetch(c):\n        try:\n            if spos is None:\n                return c.popleft()\n            else:\n                return c.pop()\n        except IndexError:\n            return None\n\n    while nargs_spec:\n        nargs = _fetch(nargs_spec)\n        if nargs == 1:\n            rv.append(_fetch(args))\n        elif nargs > 1:\n            x = [_fetch(args) for _ in range(nargs)]\n            # If we're reversed, we're pulling in the arguments in reverse,\n            # so we need to turn them around.\n            if spos is not None:\n                x.reverse()\n            rv.append(tuple(x))\n        elif nargs < 0:\n            if spos is not None:\n                raise TypeError('Cannot have two nargs < 0')\n            spos = len(rv)\n            rv.append(None)\n\n    # spos is the position of the wildcard (star).  If it's not `None`,\n    # we fill it with the remainder.\n    if spos is not None:\n        rv[spos] = tuple(args)\n        args = []\n        rv[spos + 1:] = reversed(rv[spos + 1:])\n\n    return tuple(rv), list(args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive an argument string this attempts to split it into small parts.", "response": "def split_arg_string(string):\n    \"\"\"Given an argument string this attempts to split it into small parts.\"\"\"\n    rv = []\n    for match in re.finditer(r\"('([^'\\\\]*(?:\\\\.[^'\\\\]*)*)'\"\n                             r'|\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\"'\n                             r'|\\S+)\\s*', string, re.S):\n        arg = match.group().strip()\n        if arg[:1] == arg[-1:] and arg[:1] in '\"\\'':\n            arg = arg[1:-1].encode('ascii', 'backslashreplace') \\\n                .decode('unicode-escape')\n        try:\n            arg = type(string)(arg)\n        except UnicodeError:\n            pass\n        rv.append(arg)\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_option(self, opts, dest, action=None, nargs=1, const=None,\n                   obj=None):\n        \"\"\"Adds a new option named `dest` to the parser.  The destination\n        is not inferred (unlike with optparse) and needs to be explicitly\n        provided.  Action can be any of ``store``, ``store_const``,\n        ``append``, ``appnd_const`` or ``count``.\n\n        The `obj` can be used to identify the option in the order list\n        that is returned from the parser.\n        \"\"\"\n        if obj is None:\n            obj = dest\n        opts = [normalize_opt(opt, self.ctx) for opt in opts]\n        option = Option(opts, dest, action=action, nargs=nargs,\n                        const=const, obj=obj)\n        self._opt_prefixes.update(option.prefixes)\n        for opt in option._short_opts:\n            self._short_opt[opt] = option\n        for opt in option._long_opts:\n            self._long_opt[opt] = option", "response": "Adds a new option to the parser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a positional argument named dest to the parser.", "response": "def add_argument(self, dest, nargs=1, obj=None):\n        \"\"\"Adds a positional argument named `dest` to the parser.\n\n        The `obj` can be used to identify the option in the order list\n        that is returned from the parser.\n        \"\"\"\n        if obj is None:\n            obj = dest\n        self._args.append(Argument(dest=dest, nargs=nargs, obj=obj))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse positional arguments and returns a tuple of options and arguments and order.", "response": "def parse_args(self, args):\n        \"\"\"Parses positional arguments and returns ``(values, args, order)``\n        for the parsed options and arguments as well as the leftover\n        arguments if there are any.  The order is a list of objects as they\n        appear on the command line.  If arguments appear multiple times they\n        will be memorized multiple times as well.\n        \"\"\"\n        state = ParsingState(args)\n        try:\n            self._process_args_for_options(state)\n            self._process_args_for_args(state)\n        except UsageError:\n            if self.ctx is None or not self.ctx.resilient_parsing:\n                raise\n        return state.opts, state.largs, state.order"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_graph(dists, scheme='default'):\n    scheme = get_scheme(scheme)\n    graph = DependencyGraph()\n    provided = {}  # maps names to lists of (version, dist) tuples\n\n    # first, build the graph and find out what's provided\n    for dist in dists:\n        graph.add_distribution(dist)\n\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Add to provided: %s, %s, %s', name, version, dist)\n            provided.setdefault(name, []).append((version, dist))\n\n    # now make the edges\n    for dist in dists:\n        requires = (dist.run_requires | dist.meta_requires |\n                    dist.build_requires | dist.dev_requires)\n        for req in requires:\n            try:\n                matcher = scheme.matcher(req)\n            except UnsupportedVersionError:\n                # XXX compat-mode if cannot read the version\n                logger.warning('could not read version %r - using name only',\n                               req)\n                name = req.split()[0]\n                matcher = scheme.matcher(name)\n\n            name = matcher.key   # case-insensitive\n\n            matched = False\n            if name in provided:\n                for version, provider in provided[name]:\n                    try:\n                        match = matcher.match(version)\n                    except UnsupportedVersionError:\n                        match = False\n\n                    if match:\n                        graph.add_edge(dist, provider, req)\n                        matched = True\n                        break\n            if not matched:\n                graph.add_missing(dist, req)\n    return graph", "response": "Makes a dependency graph from the given distributions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_required_dists(dists, dist):\n    if dist not in dists:\n        raise DistlibException('given distribution %r is not a member '\n                               'of the list' % dist.name)\n    graph = make_graph(dists)\n\n    req = []  # required distributions\n    todo = graph.adjacency_list[dist]  # list of nodes we should inspect\n\n    while todo:\n        d = todo.pop()[0]\n        req.append(d)\n        for pred in graph.adjacency_list[d]:\n            if pred not in req:\n                todo.append(pred)\n\n    return req", "response": "Recursively generate a list of distributions that are required by dist."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclearing the cache, setting it to its initial state.", "response": "def clear(self):\n        \"\"\"\n        Clear the cache, setting it to its initial state.\n        \"\"\"\n        self.name.clear()\n        self.path.clear()\n        self.generated = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a distribution to the cache.", "response": "def add(self, dist):\n        \"\"\"\n        Add a distribution to the cache.\n        :param dist: The distribution to add.\n        \"\"\"\n        if dist.path not in self.path:\n            self.path[dist.path] = dist\n            self.name.setdefault(dist.key, []).append(dist)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_cache(self):\n        gen_dist = not self._cache.generated\n        gen_egg = self._include_egg and not self._cache_egg.generated\n        if gen_dist or gen_egg:\n            for dist in self._yield_distributions():\n                if isinstance(dist, InstalledDistribution):\n                    self._cache.add(dist)\n                else:\n                    self._cache_egg.add(dist)\n\n            if gen_dist:\n                self._cache.generated = True\n            if gen_egg:\n                self._cache_egg.generated = True", "response": "Generate the cache with the contents of the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a directory name for the given name and version number.", "response": "def distinfo_dirname(cls, name, version):\n        \"\"\"\n        The *name* and *version* parameters are converted into their\n        filename-escaped form, i.e. any ``'-'`` characters are replaced\n        with ``'_'`` other than the one in ``'dist-info'`` and the one\n        separating the name from the version number.\n\n        :parameter name: is converted to a standard distribution name by replacing\n                         any runs of non- alphanumeric characters with a single\n                         ``'-'``.\n        :type name: string\n        :parameter version: is converted to a standard version string. Spaces\n                            become dots, and all other non-alphanumeric characters\n                            (except dots) become dashes, with runs of multiple\n                            dashes condensed to a single dash.\n        :type version: string\n        :returns: directory name\n        :rtype: string\"\"\"\n        name = name.replace('-', '_')\n        return '-'.join([name, version]) + DISTINFO_EXT"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_distributions(self):\n        if not self._cache_enabled:\n            for dist in self._yield_distributions():\n                yield dist\n        else:\n            self._generate_cache()\n\n            for dist in self._cache.path.values():\n                yield dist\n\n            if self._include_egg:\n                for dist in self._cache_egg.path.values():\n                    yield dist", "response": "Returns an iterator that yields the installed distributions and returns\n        instances for each one of them."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the installed distribution with the given name.", "response": "def get_distribution(self, name):\n        \"\"\"\n        Looks for a named distribution on the path.\n\n        This function only returns the first result found, as no more than one\n        value is expected. If nothing is found, ``None`` is returned.\n\n        :rtype: :class:`InstalledDistribution`, :class:`EggInfoDistribution`\n                or ``None``\n        \"\"\"\n        result = None\n        name = name.lower()\n        if not self._cache_enabled:\n            for dist in self._yield_distributions():\n                if dist.key == name:\n                    result = dist\n                    break\n        else:\n            self._generate_cache()\n\n            if name in self._cache.name:\n                result = self._cache.name[name][0]\n            elif self._include_egg and name in self._cache_egg.name:\n                result = self._cache_egg.name[name][0]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef provides_distribution(self, name, version=None):\n        matcher = None\n        if version is not None:\n            try:\n                matcher = self._scheme.matcher('%s (%s)' % (name, version))\n            except ValueError:\n                raise DistlibException('invalid name or version: %r, %r' %\n                                      (name, version))\n\n        for dist in self.get_distributions():\n            # We hit a problem on Travis where enum34 was installed and doesn't\n            # have a provides attribute ...\n            if not hasattr(dist, 'provides'):\n                logger.debug('No \"provides\": %s', dist)\n            else:\n                provided = dist.provides\n\n                for p in provided:\n                    p_name, p_ver = parse_name_and_version(p)\n                    if matcher is None:\n                        if p_name == name:\n                            yield dist\n                            break\n                    else:\n                        if p_name == name and matcher.match(p_ver):\n                            yield dist\n                            break", "response": "Yields the names of all distributions that provide the given name and version."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_file_path(self, name, relative_path):\n        dist = self.get_distribution(name)\n        if dist is None:\n            raise LookupError('no distribution named %r found' % name)\n        return dist.get_resource_path(relative_path)", "response": "Get the path to a resource file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_exported_entries(self, category, name=None):\n        for dist in self.get_distributions():\n            r = dist.exports\n            if category in r:\n                d = r[category]\n                if name is not None:\n                    if name in d:\n                        yield d[name]\n                else:\n                    for v in d.values():\n                        yield v", "response": "Return all of the exported entries in a particular category."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef provides(self):\n        plist = self.metadata.provides\n        s = '%s (%s)' % (self.name, self.version)\n        if s not in plist:\n            plist.append(s)\n        return plist", "response": "A set of names and versions provided by this distribution."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef matches_requirement(self, req):\n        # Requirement may contain extras - parse to lose those\n        # from what's passed to the matcher\n        r = parse_requirement(req)\n        scheme = get_scheme(self.metadata.scheme)\n        try:\n            matcher = scheme.matcher(r.requirement)\n        except UnsupportedVersionError:\n            # XXX compat-mode if cannot read the version\n            logger.warning('could not read version %r - using name only',\n                           req)\n            name = req.split()[0]\n            matcher = scheme.matcher(name)\n\n        name = matcher.key   # case-insensitive\n\n        result = False\n        for p in self.provides:\n            p_name, p_ver = parse_name_and_version(p)\n            if p_name != name:\n                continue\n            try:\n                result = matcher.match(p_ver)\n                break\n            except UnsupportedVersionError:\n                pass\n        return result", "response": "Say if this instance matches a requirement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the hash of some data using a particular hash algorithm.", "response": "def get_hash(self, data, hasher=None):\n        \"\"\"\n        Get the hash of some data, using a particular hash algorithm, if\n        specified.\n\n        :param data: The data to be hashed.\n        :type data: bytes\n        :param hasher: The name of a hash implementation, supported by hashlib,\n                       or ``None``. Examples of valid values are ``'sha1'``,\n                       ``'sha224'``, ``'sha384'``, '``sha256'``, ``'md5'`` and\n                       ``'sha512'``. If no hasher is specified, the ``hasher``\n                       attribute of the :class:`InstalledDistribution` instance\n                       is used. If the hasher is determined to be ``None``, MD5\n                       is used as the hashing algorithm.\n        :returns: The hash of the data. If a hasher was explicitly specified,\n                  the returned hash will be prefixed with the specified hasher\n                  followed by '='.\n        :rtype: str\n        \"\"\"\n        if hasher is None:\n            hasher = self.hasher\n        if hasher is None:\n            hasher = hashlib.md5\n            prefix = ''\n        else:\n            hasher = getattr(hashlib, hasher)\n            prefix = '%s=' % self.hasher\n        digest = hasher(data).digest()\n        digest = base64.urlsafe_b64encode(digest).rstrip(b'=').decode('ascii')\n        return '%s%s' % (prefix, digest)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the list of installed files for the distribution", "response": "def _get_records(self):\n        \"\"\"\n        Get the list of installed files for the distribution\n        :return: A list of tuples of path, hash and size. Note that hash and\n                 size might be ``None`` for some entries. The path is exactly\n                 as stored in the file (which is as in PEP 376).\n        \"\"\"\n        results = []\n        r = self.get_distinfo_resource('RECORD')\n        with contextlib.closing(r.as_stream()) as stream:\n            with CSVReader(stream=stream) as record_reader:\n                # Base location is parent dir of .dist-info dir\n                #base_location = os.path.dirname(self.path)\n                #base_location = os.path.abspath(base_location)\n                for row in record_reader:\n                    missing = [None for i in range(len(row), 3)]\n                    path, checksum, size = row + missing\n                    #if not os.path.isabs(path):\n                    #    path = path.replace('/', os.sep)\n                    #    path = os.path.join(base_location, path)\n                    results.append((path, checksum, size))\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the information exported by this distribution.", "response": "def exports(self):\n        \"\"\"\n        Return the information exported by this distribution.\n        :return: A dictionary of exports, mapping an export category to a dict\n                 of :class:`ExportEntry` instances describing the individual\n                 export entries, and keyed by name.\n        \"\"\"\n        result = {}\n        r = self.get_distinfo_resource(EXPORTS_FILENAME)\n        if r:\n            result = self.read_exports()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread exports data from a file in. ini format.", "response": "def read_exports(self):\n        \"\"\"\n        Read exports data from a file in .ini format.\n\n        :return: A dictionary of exports, mapping an export category to a list\n                 of :class:`ExportEntry` instances describing the individual\n                 export entries.\n        \"\"\"\n        result = {}\n        r = self.get_distinfo_resource(EXPORTS_FILENAME)\n        if r:\n            with contextlib.closing(r.as_stream()) as stream:\n                result = read_exports(stream)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a dictionary of exports to a file in. ini format.", "response": "def write_exports(self, exports):\n        \"\"\"\n        Write a dictionary of exports to a file in .ini format.\n        :param exports: A dictionary of exports, mapping an export category to\n                        a list of :class:`ExportEntry` instances describing the\n                        individual export entries.\n        \"\"\"\n        rf = self.get_distinfo_file(EXPORTS_FILENAME)\n        with open(rf, 'w') as f:\n            write_exports(exports, f)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the absolute path to a resource file with the given relative path.", "response": "def get_resource_path(self, relative_path):\n        \"\"\"\n        NOTE: This API may change in the future.\n\n        Return the absolute path to a resource file with the given relative\n        path.\n\n        :param relative_path: The path, relative to .dist-info, of the resource\n                              of interest.\n        :return: The absolute path where the resource is to be found.\n        \"\"\"\n        r = self.get_distinfo_resource('RESOURCES')\n        with contextlib.closing(r.as_stream()) as stream:\n            with CSVReader(stream=stream) as resources_reader:\n                for relative, destination in resources_reader:\n                    if relative == relative_path:\n                        return destination\n        raise KeyError('no resource file with relative path %r '\n                       'is installed' % relative_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_installed_files(self, paths, prefix, dry_run=False):\n        prefix = os.path.join(prefix, '')\n        base = os.path.dirname(self.path)\n        base_under_prefix = base.startswith(prefix)\n        base = os.path.join(base, '')\n        record_path = self.get_distinfo_file('RECORD')\n        logger.info('creating %s', record_path)\n        if dry_run:\n            return None\n        with CSVWriter(record_path) as writer:\n            for path in paths:\n                if os.path.isdir(path) or path.endswith(('.pyc', '.pyo')):\n                    # do not put size and hash, as in PEP-376\n                    hash_value = size = ''\n                else:\n                    size = '%d' % os.path.getsize(path)\n                    with open(path, 'rb') as fp:\n                        hash_value = self.get_hash(fp.read())\n                if path.startswith(base) or (base_under_prefix and\n                                             path.startswith(prefix)):\n                    path = os.path.relpath(path, base)\n                writer.writerow((path, hash_value, size))\n\n            # add the RECORD file itself\n            if record_path.startswith(base):\n                record_path = os.path.relpath(record_path, base)\n            writer.writerow((record_path, '', ''))\n        return record_path", "response": "Writes the RECORD file using the paths iterable passed in."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_installed_files(self):\n        mismatches = []\n        base = os.path.dirname(self.path)\n        record_path = self.get_distinfo_file('RECORD')\n        for path, hash_value, size in self.list_installed_files():\n            if not os.path.isabs(path):\n                path = os.path.join(base, path)\n            if path == record_path:\n                continue\n            if not os.path.exists(path):\n                mismatches.append((path, 'exists', True, False))\n            elif os.path.isfile(path):\n                actual_size = str(os.path.getsize(path))\n                if size and actual_size != size:\n                    mismatches.append((path, 'size', size, actual_size))\n                elif hash_value:\n                    if '=' in hash_value:\n                        hasher = hash_value.split('=', 1)[0]\n                    else:\n                        hasher = None\n\n                    with open(path, 'rb') as f:\n                        actual_hash = self.get_hash(f.read(), hasher)\n                        if actual_hash != hash_value:\n                            mismatches.append((path, 'hash', hash_value, actual_hash))\n        return mismatches", "response": "Checks that the hashes and sizes of the installed files are matched by the files in RECORD. Returns a list of tuples consisting of the path exists size hash and actual value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of shared locations whose keys are in the set prefix purelib platlib scripts headers data and namespace.", "response": "def shared_locations(self):\n        \"\"\"\n        A dictionary of shared locations whose keys are in the set 'prefix',\n        'purelib', 'platlib', 'scripts', 'headers', 'data' and 'namespace'.\n        The corresponding value is the absolute path of that category for\n        this distribution, and takes into account any paths selected by the\n        user at installation time (e.g. via command-line arguments). In the\n        case of the 'namespace' key, this would be a list of absolute paths\n        for the roots of namespace packages in this distribution.\n\n        The first time this property is accessed, the relevant information is\n        read from the SHARED file in the .dist-info directory.\n        \"\"\"\n        result = {}\n        shared_path = os.path.join(self.path, 'SHARED')\n        if os.path.isfile(shared_path):\n            with codecs.open(shared_path, 'r', encoding='utf-8') as f:\n                lines = f.read().splitlines()\n            for line in lines:\n                key, value = line.split('=', 1)\n                if key == 'namespace':\n                    result.setdefault(key, []).append(value)\n                else:\n                    result[key] = value\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_shared_locations(self, paths, dry_run=False):\n        shared_path = os.path.join(self.path, 'SHARED')\n        logger.info('creating %s', shared_path)\n        if dry_run:\n            return None\n        lines = []\n        for key in ('prefix', 'lib', 'headers', 'scripts', 'data'):\n            path = paths[key]\n            if os.path.isdir(paths[key]):\n                lines.append('%s=%s' % (key,  path))\n        for ns in paths.get('namespace', ()):\n            lines.append('namespace=%s' % ns)\n\n        with codecs.open(shared_path, 'w', encoding='utf-8') as f:\n            f.write('\\n'.join(lines))\n        return shared_path", "response": "Writes the shared locations to the SHARED file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a path located under the. dist - info directory.", "response": "def get_distinfo_file(self, path):\n        \"\"\"\n        Returns a path located under the ``.dist-info`` directory. Returns a\n        string representing the path.\n\n        :parameter path: a ``'/'``-separated path relative to the\n                         ``.dist-info`` directory or an absolute path;\n                         If *path* is an absolute path and doesn't start\n                         with the ``.dist-info`` directory path,\n                         a :class:`DistlibException` is raised\n        :type path: str\n        :rtype: str\n        \"\"\"\n        # Check if it is an absolute path  # XXX use relpath, add tests\n        if path.find(os.sep) >= 0:\n            # it's an absolute path?\n            distinfo_dirname, path = path.split(os.sep)[-2:]\n            if distinfo_dirname != self.path.split(os.sep)[-1]:\n                raise DistlibException(\n                    'dist-info file %r does not belong to the %r %s '\n                    'distribution' % (path, self.name, self.version))\n\n        # The file must be relative\n        if path not in DIST_FILES:\n            raise DistlibException('invalid path for a dist-info file: '\n                                   '%r at %r' % (path, self.path))\n\n        return os.path.join(self.path, path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_distinfo_files(self):\n        base = os.path.dirname(self.path)\n        for path, checksum, size in self._get_records():\n            # XXX add separator or use real relpath algo\n            if not os.path.isabs(path):\n                path = os.path.join(base, path)\n            if path.startswith(self.path):\n                yield path", "response": "Yields paths for each RECORD entry in the. dist - info directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that the hashes and sizes of the files in RECORD are matched by the files in the RECORD. Returns a list of tuples consisting of the path exists size hash and actual value.", "response": "def check_installed_files(self):\n        \"\"\"\n        Checks that the hashes and sizes of the files in ``RECORD`` are\n        matched by the files themselves. Returns a (possibly empty) list of\n        mismatches. Each entry in the mismatch list will be a tuple consisting\n        of the path, 'exists', 'size' or 'hash' according to what didn't match\n        (existence is checked first, then size, then hash), the expected\n        value and the actual value.\n        \"\"\"\n        mismatches = []\n        record_path = os.path.join(self.path, 'installed-files.txt')\n        if os.path.exists(record_path):\n            for path, _, _ in self.list_installed_files():\n                if path == record_path:\n                    continue\n                if not os.path.exists(path):\n                    mismatches.append((path, 'exists', True, False))\n        return mismatches"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists the installed files and returns a list of tuples.", "response": "def list_installed_files(self):\n        \"\"\"\n        Iterates over the ``installed-files.txt`` entries and returns a tuple\n        ``(path, hash, size)`` for each line.\n\n        :returns: a list of (path, hash, size)\n        \"\"\"\n\n        def _md5(path):\n            f = open(path, 'rb')\n            try:\n                content = f.read()\n            finally:\n                f.close()\n            return hashlib.md5(content).hexdigest()\n\n        def _size(path):\n            return os.stat(path).st_size\n\n        record_path = os.path.join(self.path, 'installed-files.txt')\n        result = []\n        if os.path.exists(record_path):\n            with codecs.open(record_path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    p = os.path.normpath(os.path.join(self.path, line))\n                    # \"./\" is present as a marker between installed files\n                    # and installation metadata files\n                    if not os.path.exists(p):\n                        logger.warning('Non-existent file: %s', p)\n                        if p.endswith(('.pyc', '.pyo')):\n                            continue\n                        #otherwise fall through and fail\n                    if not os.path.isdir(p):\n                        result.append((p, _md5(p), _size(p)))\n            result.append((record_path, None, None))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields the paths for the distinfo files for the current version of the current package.", "response": "def list_distinfo_files(self, absolute=False):\n        \"\"\"\n        Iterates over the ``installed-files.txt`` entries and returns paths for\n        each line if the path is pointing to a file located in the\n        ``.egg-info`` directory or one of its subdirectories.\n\n        :parameter absolute: If *absolute* is ``True``, each returned path is\n                          transformed into a local absolute path. Otherwise the\n                          raw value from ``installed-files.txt`` is returned.\n        :type absolute: boolean\n        :returns: iterator of paths\n        \"\"\"\n        record_path = os.path.join(self.path, 'installed-files.txt')\n        if os.path.exists(record_path):\n            skip = True\n            with codecs.open(record_path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line == './':\n                        skip = False\n                        continue\n                    if not skip:\n                        p = os.path.normpath(os.path.join(self.path, line))\n                        if p.startswith(self.path):\n                            if absolute:\n                                yield p\n                            else:\n                                yield line"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an edge from distribution x to distribution y with the given label*.", "response": "def add_edge(self, x, y, label=None):\n        \"\"\"Add an edge from distribution *x* to distribution *y* with the given\n        *label*.\n\n        :type x: :class:`distutils2.database.InstalledDistribution` or\n                 :class:`distutils2.database.EggInfoDistribution`\n        :type y: :class:`distutils2.database.InstalledDistribution` or\n                 :class:`distutils2.database.EggInfoDistribution`\n        :type label: ``str`` or ``None``\n        \"\"\"\n        self.adjacency_list[x].append((y, label))\n        # multiple edges are allowed, so be careful\n        if x not in self.reverse_list[y]:\n            self.reverse_list[y].append(x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a missing requirement for the given distribution.", "response": "def add_missing(self, distribution, requirement):\n        \"\"\"\n        Add a missing *requirement* for the given *distribution*.\n\n        :type distribution: :class:`distutils2.database.InstalledDistribution`\n                            or :class:`distutils2.database.EggInfoDistribution`\n        :type requirement: ``str``\n        \"\"\"\n        logger.debug('%s missing %r', distribution, requirement)\n        self.missing.setdefault(distribution, []).append(requirement)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints only a subgraph", "response": "def repr_node(self, dist, level=1):\n        \"\"\"Prints only a subgraph\"\"\"\n        output = [self._repr_dist(dist)]\n        for other, label in self.adjacency_list[dist]:\n            dist = self._repr_dist(other)\n            if label is not None:\n                dist = '%s [%s]' % (dist, label)\n            output.append('    ' * level + str(dist))\n            suboutput = self.repr_node(other, level + 1)\n            subs = suboutput.split('\\n')\n            output.extend(subs[1:])\n        return '\\n'.join(output)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a DOT output for the current state of the object.", "response": "def to_dot(self, f, skip_disconnected=True):\n        \"\"\"Writes a DOT output for the graph to the provided file *f*.\n\n        If *skip_disconnected* is set to ``True``, then all distributions\n        that are not dependent on any other distribution are skipped.\n\n        :type f: has to support ``file``-like operations\n        :type skip_disconnected: ``bool``\n        \"\"\"\n        disconnected = []\n\n        f.write(\"digraph dependencies {\\n\")\n        for dist, adjs in self.adjacency_list.items():\n            if len(adjs) == 0 and not skip_disconnected:\n                disconnected.append(dist)\n            for other, label in adjs:\n                if not label is None:\n                    f.write('\"%s\" -> \"%s\" [label=\"%s\"]\\n' %\n                            (dist.name, other.name, label))\n                else:\n                    f.write('\"%s\" -> \"%s\"\\n' % (dist.name, other.name))\n        if not skip_disconnected and len(disconnected) > 0:\n            f.write('subgraph disconnected {\\n')\n            f.write('label = \"Disconnected\"\\n')\n            f.write('bgcolor = red\\n')\n\n            for dist in disconnected:\n                f.write('\"%s\"' % dist.name)\n                f.write('\\n')\n            f.write('}\\n')\n        f.write('}\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef topological_sort(self):\n        result = []\n        # Make a shallow copy of the adjacency list\n        alist = {}\n        for k, v in self.adjacency_list.items():\n            alist[k] = v[:]\n        while True:\n            # See what we can remove in this run\n            to_remove = []\n            for k, v in list(alist.items())[:]:\n                if not v:\n                    to_remove.append(k)\n                    del alist[k]\n            if not to_remove:\n                # What's left in alist (if anything) is a cycle.\n                break\n            # Remove from the adjacency list of others\n            for k, v in alist.items():\n                alist[k] = [(d, r) for d, r in v if d not in to_remove]\n            logger.debug('Moving to result: %s',\n                         ['%s (%s)' % (d.name, d.version) for d in to_remove])\n            result.extend(to_remove)\n        return result, list(alist.keys())", "response": "Perform a topological sort of the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an error to the tree.", "response": "def add(self, error):\n        \"\"\" Add an error to the tree.\n\n        :param error: :class:`~cerberus.errors.ValidationError`\n        \"\"\"\n        if not self._path_of_(error):\n            self.errors.append(error)\n            self.errors.sort()\n        else:\n            super(ErrorTree, self).add(error)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all errors for a particular path.", "response": "def fetch_errors_from(self, path):\n        \"\"\" Returns all errors for a particular path.\n\n        :param path: :class:`tuple` of :term:`hashable` s.\n        :rtype: :class:`~cerberus.errors.ErrorList`\n        \"\"\"\n        node = self.fetch_node_from(path)\n        if node is not None:\n            return node.errors\n        else:\n            return ErrorList()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a node for a path.", "response": "def fetch_node_from(self, path):\n        \"\"\" Returns a node for a path.\n\n        :param path: Tuple of :term:`hashable` s.\n        :rtype: :class:`~cerberus.errors.ErrorTreeNode` or :obj:`None`\n        \"\"\"\n        context = self\n        for key in path:\n            context = context[key]\n            if context is None:\n                break\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _insert_error(self, path, node):\n        field = path[0]\n        if len(path) == 1:\n            if field in self.tree:\n                subtree = self.tree[field].pop()\n                self.tree[field] += [node, subtree]\n            else:\n                self.tree[field] = [node, {}]\n        elif len(path) >= 1:\n            if field not in self.tree:\n                self.tree[field] = [{}]\n            subtree = self.tree[field][-1]\n\n            if subtree:\n                new = self.__class__(tree=copy(subtree))\n            else:\n                new = self.__class__()\n            new._insert_error(path[1:], node)\n            subtree.update(new.tree)", "response": "Adds an error or sub - tree to the tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rewrite_error_path(self, error, offset=0):\n        if error.is_logic_error:\n            self._rewrite_logic_error_path(error, offset)\n        elif error.is_group_error:\n            self._rewrite_group_error_path(error, offset)", "response": "Recursively rewrites the error path to correctly represent group errors and logic errors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dispatch_hook(key, hooks, hook_data, **kwargs):\n    hooks = hooks or {}\n    hooks = hooks.get(key)\n    if hooks:\n        if hasattr(hooks, '__call__'):\n            hooks = [hooks]\n        for hook in hooks:\n            _hook_data = hook(hook_data, **kwargs)\n            if _hook_data is not None:\n                hook_data = _hook_data\n    return hook_data", "response": "Dispatches a hook dictionary on a given piece of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(ctx):\n    '''Display all the stored key/value.'''\n    file = ctx.obj['FILE']\n    dotenv_as_dict = dotenv_values(file)\n    for k, v in dotenv_as_dict.items():\n        click.echo('%s=%s' % (k, v))", "response": "Display all the stored key / value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(ctx, key, value):\n    '''Store the given key/value.'''\n    file = ctx.obj['FILE']\n    quote = ctx.obj['QUOTE']\n    success, key, value = set_key(file, key, value, quote)\n    if success:\n        click.echo('%s=%s' % (key, value))\n    else:\n        exit(1)", "response": "Store the given key / value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(ctx, key):\n    '''Retrieve the value for the given key.'''\n    file = ctx.obj['FILE']\n    stored_value = get_key(file, key)\n    if stored_value:\n        click.echo('%s=%s' % (key, stored_value))\n    else:\n        exit(1)", "response": "Retrieve the value for the given key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the given key.", "response": "def unset(ctx, key):\n    '''Removes the given key.'''\n    file = ctx.obj['FILE']\n    quote = ctx.obj['QUOTE']\n    success, key = unset_key(file, key, quote)\n    if success:\n        click.echo(\"Successfully removed %s\" % key)\n    else:\n        exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun command with environment variables present.", "response": "def run(ctx, commandline):\n    \"\"\"Run command with environment variables present.\"\"\"\n    file = ctx.obj['FILE']\n    dotenv_as_dict = dotenv_values(file)\n    if not commandline:\n        click.echo('No command given.')\n        exit(1)\n    ret = run_command(commandline, dotenv_as_dict)\n    exit(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck whether the distribution is in the current Python installation.", "response": "def _is_installation_local(name):\n    \"\"\"Check whether the distribution is in the current Python installation.\n\n    This is used to distinguish packages seen by a virtual environment. A venv\n    may be able to see global packages, but we don't want to mess with them.\n    \"\"\"\n    loc = os.path.normcase(pkg_resources.working_set.by_key[name].location)\n    pre = os.path.normcase(sys.prefix)\n    return os.path.commonprefix([loc, pre]) == pre"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _group_installed_names(packages):\n    groupcoll = GroupCollection(set(), set(), set(), set())\n\n    for distro in pkg_resources.working_set:\n        name = distro.key\n        try:\n            package = packages[name]\n        except KeyError:\n            groupcoll.unneeded.add(name)\n            continue\n\n        r = requirementslib.Requirement.from_pipfile(name, package)\n        if not r.is_named:\n            # Always mark non-named. I think pip does something similar?\n            groupcoll.outdated.add(name)\n        elif not _is_up_to_date(distro, r.get_version()):\n            groupcoll.outdated.add(name)\n        else:\n            groupcoll.uptodate.add(name)\n\n    return groupcoll", "response": "Group locally installed packages based on given specifications."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing paths for distlib. wheel. Wheel to install into.", "response": "def _build_paths():\n    \"\"\"Prepare paths for distlib.wheel.Wheel to install into.\n    \"\"\"\n    paths = sysconfig.get_paths()\n    return {\n        \"prefix\": sys.prefix,\n        \"data\": paths[\"data\"],\n        \"scripts\": paths[\"scripts\"],\n        \"headers\": paths[\"include\"],\n        \"purelib\": paths[\"purelib\"],\n        \"platlib\": paths[\"platlib\"],\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dumps(data):  # type: (_TOMLDocument) -> str\n    if not isinstance(data, _TOMLDocument) and isinstance(data, dict):\n        data = item(data)\n\n    return data.as_string()", "response": "Dumps a TOMLDocument into a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds all files under the base and set allfiles to the absolute pathnames of files found.", "response": "def findall(self):\n        \"\"\"Find all files under the base and set ``allfiles`` to the absolute\n        pathnames of files found.\n        \"\"\"\n        from stat import S_ISREG, S_ISDIR, S_ISLNK\n\n        self.allfiles = allfiles = []\n        root = self.base\n        stack = [root]\n        pop = stack.pop\n        push = stack.append\n\n        while stack:\n            root = pop()\n            names = os.listdir(root)\n\n            for name in names:\n                fullname = os.path.join(root, name)\n\n                # Avoid excess stat calls -- just one will do, thank you!\n                stat = os.stat(fullname)\n                mode = stat.st_mode\n                if S_ISREG(mode):\n                    allfiles.append(fsdecode(fullname))\n                elif S_ISDIR(mode) and not S_ISLNK(mode):\n                    push(fullname)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, item):\n        if not item.startswith(self.prefix):\n            item = os.path.join(self.base, item)\n        self.files.add(os.path.normpath(item))", "response": "Add a file to the manifest."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns sorted files in directory order", "response": "def sorted(self, wantdirs=False):\n        \"\"\"\n        Return sorted files in directory order\n        \"\"\"\n\n        def add_dir(dirs, d):\n            dirs.add(d)\n            logger.debug('add_dir added %s', d)\n            if d != self.base:\n                parent, _ = os.path.split(d)\n                assert parent not in ('', '/')\n                add_dir(dirs, parent)\n\n        result = set(self.files)    # make a copy!\n        if wantdirs:\n            dirs = set()\n            for f in result:\n                add_dir(dirs, os.path.dirname(f))\n            result |= dirs\n        return [os.path.join(*path_tuple) for path_tuple in\n                sorted(os.path.split(path) for path in result)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a single line of a file - type directive which adds some files from allfiles to the files list and removes some files from allfiles from files list.", "response": "def process_directive(self, directive):\n        \"\"\"\n        Process a directive which either adds some files from ``allfiles`` to\n        ``files``, or removes some files from ``files``.\n\n        :param directive: The directive to process. This should be in a format\n                     compatible with distutils ``MANIFEST.in`` files:\n\n                     http://docs.python.org/distutils/sourcedist.html#commands\n        \"\"\"\n        # Parse the line: split it up, make sure the right number of words\n        # is there, and return the relevant words.  'action' is always\n        # defined: it's the first word of the line.  Which of the other\n        # three are defined depends on the action; it'll be either\n        # patterns, (dir and patterns), or (dirpattern).\n        action, patterns, thedir, dirpattern = self._parse_directive(directive)\n\n        # OK, now we know that the action is valid and we have the\n        # right number of words on the line for that action -- so we\n        # can proceed with minimal error-checking.\n        if action == 'include':\n            for pattern in patterns:\n                if not self._include_pattern(pattern, anchor=True):\n                    logger.warning('no files found matching %r', pattern)\n\n        elif action == 'exclude':\n            for pattern in patterns:\n                found = self._exclude_pattern(pattern, anchor=True)\n                #if not found:\n                #    logger.warning('no previously-included files '\n                #                   'found matching %r', pattern)\n\n        elif action == 'global-include':\n            for pattern in patterns:\n                if not self._include_pattern(pattern, anchor=False):\n                    logger.warning('no files found matching %r '\n                                   'anywhere in distribution', pattern)\n\n        elif action == 'global-exclude':\n            for pattern in patterns:\n                found = self._exclude_pattern(pattern, anchor=False)\n                #if not found:\n                #    logger.warning('no previously-included files '\n                #                   'matching %r found anywhere in '\n                #                   'distribution', pattern)\n\n        elif action == 'recursive-include':\n            for pattern in patterns:\n                if not self._include_pattern(pattern, prefix=thedir):\n                    logger.warning('no files found matching %r '\n                                   'under directory %r', pattern, thedir)\n\n        elif action == 'recursive-exclude':\n            for pattern in patterns:\n                found = self._exclude_pattern(pattern, prefix=thedir)\n                #if not found:\n                #    logger.warning('no previously-included files '\n                #                   'matching %r found under directory %r',\n                #                   pattern, thedir)\n\n        elif action == 'graft':\n            if not self._include_pattern(None, prefix=dirpattern):\n                logger.warning('no directories found matching %r',\n                               dirpattern)\n\n        elif action == 'prune':\n            if not self._exclude_pattern(None, prefix=dirpattern):\n                logger.warning('no previously-included directories found '\n                               'matching %r', dirpattern)\n        else:   # pragma: no cover\n            # This should never happen, as it should be caught in\n            # _parse_template_line\n            raise DistlibException(\n                'invalid action %r' % action)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_directive(self, directive):\n        words = directive.split()\n        if len(words) == 1 and words[0] not in ('include', 'exclude',\n                                                'global-include',\n                                                'global-exclude',\n                                                'recursive-include',\n                                                'recursive-exclude',\n                                                'graft', 'prune'):\n            # no action given, let's use the default 'include'\n            words.insert(0, 'include')\n\n        action = words[0]\n        patterns = thedir = dir_pattern = None\n\n        if action in ('include', 'exclude',\n                      'global-include', 'global-exclude'):\n            if len(words) < 2:\n                raise DistlibException(\n                    '%r expects <pattern1> <pattern2> ...' % action)\n\n            patterns = [convert_path(word) for word in words[1:]]\n\n        elif action in ('recursive-include', 'recursive-exclude'):\n            if len(words) < 3:\n                raise DistlibException(\n                    '%r expects <dir> <pattern1> <pattern2> ...' % action)\n\n            thedir = convert_path(words[1])\n            patterns = [convert_path(word) for word in words[2:]]\n\n        elif action in ('graft', 'prune'):\n            if len(words) != 2:\n                raise DistlibException(\n                    '%r expects a single <dir_pattern>' % action)\n\n            dir_pattern = convert_path(words[1])\n\n        else:\n            raise DistlibException('unknown action %r' % action)\n\n        return action, patterns, thedir, dir_pattern", "response": "Parse a directive into action patterns dir_pattern"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches the file list for a pattern and adds it to the internal list of all files.", "response": "def _include_pattern(self, pattern, anchor=True, prefix=None,\n                         is_regex=False):\n        \"\"\"Select strings (presumably filenames) from 'self.files' that\n        match 'pattern', a Unix-style wildcard (glob) pattern.\n\n        Patterns are not quite the same as implemented by the 'fnmatch'\n        module: '*' and '?'  match non-special characters, where \"special\"\n        is platform-dependent: slash on Unix; colon, slash, and backslash on\n        DOS/Windows; and colon on Mac OS.\n\n        If 'anchor' is true (the default), then the pattern match is more\n        stringent: \"*.py\" will match \"foo.py\" but not \"foo/bar.py\".  If\n        'anchor' is false, both of these will match.\n\n        If 'prefix' is supplied, then only filenames starting with 'prefix'\n        (itself a pattern) and ending with 'pattern', with anything in between\n        them, will match.  'anchor' is ignored in this case.\n\n        If 'is_regex' is true, 'anchor' and 'prefix' are ignored, and\n        'pattern' is assumed to be either a string containing a regex or a\n        regex object -- no translation is done, the regex is just compiled\n        and used as-is.\n\n        Selected strings will be added to self.files.\n\n        Return True if files are found.\n        \"\"\"\n        # XXX docstring lying about what the special chars are?\n        found = False\n        pattern_re = self._translate_pattern(pattern, anchor, prefix, is_regex)\n\n        # delayed loading of allfiles list\n        if self.allfiles is None:\n            self.findall()\n\n        for name in self.allfiles:\n            if pattern_re.search(name):\n                self.files.add(name)\n                found = True\n        return found"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _exclude_pattern(self, pattern, anchor=True, prefix=None,\n                         is_regex=False):\n        \"\"\"Remove strings (presumably filenames) from 'files' that match\n        'pattern'.\n\n        Other parameters are the same as for 'include_pattern()', above.\n        The list 'self.files' is modified in place. Return True if files are\n        found.\n\n        This API is public to allow e.g. exclusion of SCM subdirs, e.g. when\n        packaging source distributions\n        \"\"\"\n        found = False\n        pattern_re = self._translate_pattern(pattern, anchor, prefix, is_regex)\n        for f in list(self.files):\n            if pattern_re.search(f):\n                self.files.remove(f)\n                found = True\n        return found", "response": "Exclude patterns from the list of files that match the pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntranslates a shell - like wildcard pattern to a compiled regular expression.", "response": "def _translate_pattern(self, pattern, anchor=True, prefix=None,\n                           is_regex=False):\n        \"\"\"Translate a shell-like wildcard pattern to a compiled regular\n        expression.\n\n        Return the compiled regex.  If 'is_regex' true,\n        then 'pattern' is directly compiled to a regex (if it's a string)\n        or just returned as-is (assumes it's a regex object).\n        \"\"\"\n        if is_regex:\n            if isinstance(pattern, str):\n                return re.compile(pattern)\n            else:\n                return pattern\n\n        if _PYTHON_VERSION > (3, 2):\n            # ditch start and end characters\n            start, _, end = self._glob_to_re('_').partition('_')\n\n        if pattern:\n            pattern_re = self._glob_to_re(pattern)\n            if _PYTHON_VERSION > (3, 2):\n                assert pattern_re.startswith(start) and pattern_re.endswith(end)\n        else:\n            pattern_re = ''\n\n        base = re.escape(os.path.join(self.base, ''))\n        if prefix is not None:\n            # ditch end of pattern character\n            if _PYTHON_VERSION <= (3, 2):\n                empty_pattern = self._glob_to_re('')\n                prefix_re = self._glob_to_re(prefix)[:-len(empty_pattern)]\n            else:\n                prefix_re = self._glob_to_re(prefix)\n                assert prefix_re.startswith(start) and prefix_re.endswith(end)\n                prefix_re = prefix_re[len(start): len(prefix_re) - len(end)]\n            sep = os.sep\n            if os.sep == '\\\\':\n                sep = r'\\\\'\n            if _PYTHON_VERSION <= (3, 2):\n                pattern_re = '^' + base + sep.join((prefix_re,\n                                                    '.*' + pattern_re))\n            else:\n                pattern_re = pattern_re[len(start): len(pattern_re) - len(end)]\n                pattern_re = r'%s%s%s%s.*%s%s' % (start, base, prefix_re, sep,\n                                                  pattern_re, end)\n        else:  # no prefix -- respect anchor flag\n            if anchor:\n                if _PYTHON_VERSION <= (3, 2):\n                    pattern_re = '^' + base + pattern_re\n                else:\n                    pattern_re = r'%s%s%s' % (start, base, pattern_re[len(start):])\n\n        return re.compile(pattern_re)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _glob_to_re(self, pattern):\n        pattern_re = fnmatch.translate(pattern)\n\n        # '?' and '*' in the glob pattern become '.' and '.*' in the RE, which\n        # IMHO is wrong -- '?' and '*' aren't supposed to match slash in Unix,\n        # and by extension they shouldn't match such \"special characters\" under\n        # any OS.  So change all non-escaped dots in the RE to match any\n        # character except the special characters (currently: just os.sep).\n        sep = os.sep\n        if os.sep == '\\\\':\n            # we're using a regex to manipulate a regex, so we need\n            # to escape the backslash twice\n            sep = r'\\\\\\\\'\n        escaped = r'\\1[^%s]' % sep\n        pattern_re = re.sub(r'((?<!\\\\)(\\\\\\\\)*)\\.', escaped, pattern_re)\n        return pattern_re", "response": "Translate a shell - like glob pattern to a regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, key, val):\n        key_lower = key.lower()\n        new_vals = [key, val]\n        # Keep the common case aka no item present as fast as possible\n        vals = self._container.setdefault(key_lower, new_vals)\n        if new_vals is not vals:\n            vals.append(val)", "response": "Adds a ( name value ) pair to the container. Doesn t overwrite the value if it already exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getlist(self, key, default=__marker):\n        try:\n            vals = self._container[key.lower()]\n        except KeyError:\n            if default is self.__marker:\n                return []\n            return default\n        else:\n            return vals[1:]", "response": "Returns a list of all the values for the named field. Returns an empty list if the key doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\niterating over all header lines including duplicate ones.", "response": "def iteritems(self):\n        \"\"\"Iterate over all header lines, including duplicate ones.\"\"\"\n        for key in self:\n            vals = self._container[key.lower()]\n            for val in vals[1:]:\n                yield vals[0], val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef itermerged(self):\n        for key in self:\n            val = self._container[key.lower()]\n            yield val[0], ', '.join(val[1:])", "response": "Iterate over all headers merging duplicate ones together."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_httplib(cls, message):  # Python 2\n        # python2.7 does not expose a proper API for exporting multiheaders\n        # efficiently. This function re-reads raw lines from the message\n        # object and extracts the multiheaders properly.\n        obs_fold_continued_leaders = (' ', '\\t')\n        headers = []\n\n        for line in message.headers:\n            if line.startswith(obs_fold_continued_leaders):\n                if not headers:\n                    # We received a header line that starts with OWS as described\n                    # in RFC-7230 S3.2.4. This indicates a multiline header, but\n                    # there exists no previous header to which we can attach it.\n                    raise InvalidHeader(\n                        'Header continuation with no previous header: %s' % line\n                    )\n                else:\n                    key, value = headers[-1]\n                    headers[-1] = (key, value + ' ' + line.strip())\n                    continue\n\n            key, value = line.split(':', 1)\n            headers.append((key, value.strip()))\n\n        return cls(headers)", "response": "Read headers from a Python 2 httplib message object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting the cookies from the response into a CookieJar.", "response": "def extract_cookies_to_jar(jar, request, response):\n    \"\"\"Extract the cookies from the response into a CookieJar.\n\n    :param jar: cookielib.CookieJar (not necessarily a RequestsCookieJar)\n    :param request: our own requests.Request object\n    :param response: urllib3.HTTPResponse object\n    \"\"\"\n    if not (hasattr(response, '_original_response') and\n            response._original_response):\n        return\n    # the _original_response field is the wrapped httplib.HTTPResponse object,\n    req = MockRequest(request)\n    # pull out the HTTPMessage with the headers and put it in the mock:\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cookie_header(jar, request):\n    r = MockRequest(request)\n    jar.add_cookie_header(r)\n    return r.get_new_headers().get('Cookie')", "response": "Produce an appropriate Cookie header string to be sent with request."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_cookie_by_name(cookiejar, name, domain=None, path=None):\n    clearables = []\n    for cookie in cookiejar:\n        if cookie.name != name:\n            continue\n        if domain is not None and domain != cookie.domain:\n            continue\n        if path is not None and path != cookie.path:\n            continue\n        clearables.append((cookie.domain, cookie.path, cookie.name))\n\n    for domain, path, name in clearables:\n        cookiejar.clear(domain, path, name)", "response": "Removes a cookie by name by default over all domains and paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_cookie(name, value, **kwargs):\n    result = {\n        'version': 0,\n        'name': name,\n        'value': value,\n        'port': None,\n        'domain': '',\n        'path': '/',\n        'secure': False,\n        'expires': None,\n        'discard': True,\n        'comment': None,\n        'comment_url': None,\n        'rest': {'HttpOnly': None},\n        'rfc2109': False,\n    }\n\n    badargs = set(kwargs) - set(result)\n    if badargs:\n        err = 'create_cookie() got unexpected keyword arguments: %s'\n        raise TypeError(err % list(badargs))\n\n    result.update(kwargs)\n    result['port_specified'] = bool(result['port'])\n    result['domain_specified'] = bool(result['domain'])\n    result['domain_initial_dot'] = result['domain'].startswith('.')\n    result['path_specified'] = bool(result['path'])\n\n    return cookielib.Cookie(**result)", "response": "Create a new Cookie object from the given name and value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef morsel_to_cookie(morsel):\n\n    expires = None\n    if morsel['max-age']:\n        try:\n            expires = int(time.time() + int(morsel['max-age']))\n        except ValueError:\n            raise TypeError('max-age: %s must be integer' % morsel['max-age'])\n    elif morsel['expires']:\n        time_template = '%a, %d-%b-%Y %H:%M:%S GMT'\n        expires = calendar.timegm(\n            time.strptime(morsel['expires'], time_template)\n        )\n    return create_cookie(\n        comment=morsel['comment'],\n        comment_url=bool(morsel['comment']),\n        discard=False,\n        domain=morsel['domain'],\n        expires=expires,\n        name=morsel.key,\n        path=morsel['path'],\n        port=None,\n        rest={'HttpOnly': morsel['httponly']},\n        rfc2109=False,\n        secure=bool(morsel['secure']),\n        value=morsel.value,\n        version=morsel['version'] or 0,\n    )", "response": "Convert a Morsel object into a Cookie containing the one k - v pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a CookieJar from a key - value dictionary.", "response": "def cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):\n    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\n    :param cookiejar: (optional) A cookiejar to add the cookies to.\n    :param overwrite: (optional) If False, will not replace cookies\n        already in the jar with new ones.\n    :rtype: CookieJar\n    \"\"\"\n    if cookiejar is None:\n        cookiejar = RequestsCookieJar()\n\n    if cookie_dict is not None:\n        names_from_jar = [cookie.name for cookie in cookiejar]\n        for name in cookie_dict:\n            if overwrite or (name not in names_from_jar):\n                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))\n\n    return cookiejar"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the given cookies to the given CookieJar and returns a new CookieJar object.", "response": "def merge_cookies(cookiejar, cookies):\n    \"\"\"Add cookies to cookiejar and returns a merged CookieJar.\n\n    :param cookiejar: CookieJar object to add the cookies to.\n    :param cookies: Dictionary or CookieJar object to be added.\n    :rtype: CookieJar\n    \"\"\"\n    if not isinstance(cookiejar, cookielib.CookieJar):\n        raise ValueError('You can only merge into CookieJar')\n\n    if isinstance(cookies, dict):\n        cookiejar = cookiejar_from_dict(\n            cookies, cookiejar=cookiejar, overwrite=False)\n    elif isinstance(cookies, cookielib.CookieJar):\n        try:\n            cookiejar.update(cookies)\n        except AttributeError:\n            for cookie_in_jar in cookies:\n                cookiejar.set_cookie(cookie_in_jar)\n\n    return cookiejar"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a key from the cache.", "response": "def get(self, name, default=None, domain=None, path=None):\n        \"\"\"Dict-like get() that also supports optional domain and path args in\n        order to resolve naming collisions from using one cookie jar over\n        multiple domains.\n\n        .. warning:: operation is O(n), not O(1).\n        \"\"\"\n        try:\n            return self._find_no_duplicates(name, domain, path)\n        except KeyError:\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_domains(self):\n        domains = []\n        for cookie in iter(self):\n            if cookie.domain not in domains:\n                domains.append(cookie.domain)\n        return domains", "response": "Utility method to list all the domains in the jar."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if there are multiple domains in the jar. Returns False otherwise.", "response": "def multiple_domains(self):\n        \"\"\"Returns True if there are multiple domains in the jar.\n        Returns False otherwise.\n\n        :rtype: bool\n        \"\"\"\n        domains = []\n        for cookie in iter(self):\n            if cookie.domain is not None and cookie.domain in domains:\n                return True\n            domains.append(cookie.domain)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, other):\n        if isinstance(other, cookielib.CookieJar):\n            for cookie in other:\n                self.set_cookie(copy.copy(cookie))\n        else:\n            super(RequestsCookieJar, self).update(other)", "response": "Updates this jar with cookies from another CookieJar or dict - like object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrequesting uses this method internally to get cookie values. If there are conflicting cookies, _find arbitrarily chooses one. See _find_no_duplicates if you want an exception thrown if there are conflicting cookies. :param name: a string containing name of cookie :param domain: (optional) string containing domain of cookie :param path: (optional) string containing path of cookie :return: cookie.value", "response": "def _find(self, name, domain=None, path=None):\n        \"\"\"Requests uses this method internally to get cookie values.\n\n        If there are conflicting cookies, _find arbitrarily chooses one.\n        See _find_no_duplicates if you want an exception thrown if there are\n        conflicting cookies.\n\n        :param name: a string containing name of cookie\n        :param domain: (optional) string containing domain of cookie\n        :param path: (optional) string containing path of cookie\n        :return: cookie.value\n        \"\"\"\n        for cookie in iter(self):\n            if cookie.name == name:\n                if domain is None or cookie.domain == domain:\n                    if path is None or cookie.path == path:\n                        return cookie.value\n\n        raise KeyError('name=%r, domain=%r, path=%r' % (name, domain, path))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_no_duplicates(self, name, domain=None, path=None):\n        toReturn = None\n        for cookie in iter(self):\n            if cookie.name == name:\n                if domain is None or cookie.domain == domain:\n                    if path is None or cookie.path == path:\n                        if toReturn is not None:  # if there are multiple cookies that meet passed in criteria\n                            raise CookieConflictError('There are multiple cookies with name, %r' % (name))\n                        toReturn = cookie.value  # we will eventually return this as long as no cookie conflict\n\n        if toReturn:\n            return toReturn\n        raise KeyError('name=%r, domain=%r, path=%r' % (name, domain, path))", "response": "This function is used by get and get_item to find no duplicates in the cookie list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a copy of this RequestsCookieJar.", "response": "def copy(self):\n        \"\"\"Return a copy of this RequestsCookieJar.\"\"\"\n        new_cj = RequestsCookieJar()\n        new_cj.set_policy(self.get_policy())\n        new_cj.update(self)\n        return new_cj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump (self):\n        '''This returns a copy of the screen as a unicode string. This is similar to\n        __str__/__unicode__ except that lines are not terminated with line\n        feeds.'''\n\n        return u''.join ([ u''.join(c) for c in self.w ])", "response": "This returns a copy of the screen as a unicode string. This is similar to\n       . __str__ except that lines are not terminated with line\n        feeds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove the cursor down with scrolling.", "response": "def lf (self):\n        '''This moves the cursor down with scrolling.\n        '''\n\n        old_r = self.cur_r\n        self.cursor_down()\n        if old_r == self.cur_r:\n            self.scroll_up ()\n            self.erase_line()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_abs (self, r, c, ch):\n        '''Screen array starts at 1 index.'''\n\n        r = constrain (r, 1, self.rows)\n        c = constrain (c, 1, self.cols)\n        if isinstance(ch, bytes):\n            ch = self._decode(ch)[0]\n        else:\n            ch = ch[0]\n        self.w[r-1][c-1] = ch", "response": "This method is used to put the absolute values of the screen array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put (self, ch):\n        '''This puts a characters at the current cursor position.\n        '''\n\n        if isinstance(ch, bytes):\n            ch = self._decode(ch)\n\n        self.put_abs (self.cur_r, self.cur_c, ch)", "response": "This puts a character at the current cursor position."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert_abs (self, r, c, ch):\n        '''This inserts a character at (r,c). Everything under\n        and to the right is shifted right one character.\n        The last character of the line is lost.\n        '''\n\n        if isinstance(ch, bytes):\n            ch = self._decode(ch)\n\n        r = constrain (r, 1, self.rows)\n        c = constrain (c, 1, self.cols)\n        for ci in range (self.cols, c, -1):\n            self.put_abs (r,ci, self.get_abs(r,ci-1))\n        self.put_abs (r,c,ch)", "response": "This inserts a character at ( r c."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_region (self, rs,cs, re,ce):\n        '''This returns a list of lines representing the region.\n        '''\n\n        rs = constrain (rs, 1, self.rows)\n        re = constrain (re, 1, self.rows)\n        cs = constrain (cs, 1, self.cols)\n        ce = constrain (ce, 1, self.cols)\n        if rs > re:\n            rs, re = re, rs\n        if cs > ce:\n            cs, ce = ce, cs\n        sc = []\n        for r in range (rs, re+1):\n            line = u''\n            for c in range (cs, ce + 1):\n                ch = self.get_abs (r,c)\n                line = line + ch\n            sc.append (line)\n        return sc", "response": "This returns a list of lines representing the region."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave current cursor position.", "response": "def cursor_save_attrs (self): # <ESC>7\n        '''Save current cursor position.'''\n\n        self.cur_saved_r = self.cur_r\n        self.cur_saved_c = self.cur_c"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling scrolling from row start to row end.", "response": "def scroll_screen_rows (self, rs, re): # <ESC>[{start};{end}r\n        '''Enable scrolling from row {start} to row {end}.'''\n\n        self.scroll_row_start = rs\n        self.scroll_row_end = re\n        self.scroll_constrain()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nscrolls display down one line.", "response": "def scroll_down (self): # <ESC>D\n        '''Scroll display down one line.'''\n\n        # Screen is indexed from 1, but arrays are indexed from 0.\n        s = self.scroll_row_start - 1\n        e = self.scroll_row_end - 1\n        self.w[s+1:e+1] = copy.deepcopy(self.w[s:e])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nerase from the current cursor position to the end of the current line.", "response": "def erase_end_of_line (self): # <ESC>[0K -or- <ESC>[K\n        '''Erases from the current cursor position to the end of the current\n        line.'''\n\n        self.fill_region (self.cur_r, self.cur_c, self.cur_r, self.cols)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef erase_start_of_line (self): # <ESC>[1K\n        '''Erases from the current cursor position to the start of the current\n        line.'''\n\n        self.fill_region (self.cur_r, 1, self.cur_r, self.cur_c)", "response": "Erases from the current cursor position to the start of the current\n        line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nerase the entire current line.", "response": "def erase_line (self): # <ESC>[2K\n        '''Erases the entire current line.'''\n\n        self.fill_region (self.cur_r, 1, self.cur_r, self.cols)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nerase the current line down to the bottom of the screen.", "response": "def erase_down (self): # <ESC>[0J -or- <ESC>[J\n        '''Erases the screen from the current line down to the bottom of the\n        screen.'''\n\n        self.erase_end_of_line ()\n        self.fill_region (self.cur_r + 1, 1, self.rows, self.cols)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef erase_up (self): # <ESC>[1J\n        '''Erases the screen from the current line up to the top of the\n        screen.'''\n\n        self.erase_start_of_line ()\n        self.fill_region (self.cur_r-1, 1, 1, self.cols)", "response": "Erases the screen from the current line up to the top of the\n        screen."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npulling a value from the dict and convert it to int", "response": "def to_int(d, key, default_to_zero=False, default=None, required=True):\n    \"\"\"Pull a value from the dict and convert to int\n\n    :param default_to_zero: If the value is None or empty, treat it as zero\n    :param default: If the value is missing in the dict use this default\n\n    \"\"\"\n    value = d.get(key) or default\n    if (value in [\"\", None]) and default_to_zero:\n        return 0\n    if value is None:\n        if required:\n            raise ParseError(\"Unable to read %s from %s\" % (key, d))\n    else:\n        return int(value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_timezone(matches, default_timezone=UTC):\n\n    if matches[\"timezone\"] == \"Z\":\n        return UTC\n    # This isn't strictly correct, but it's common to encounter dates without\n    # timezones so I'll assume the default (which defaults to UTC).\n    # Addresses issue 4.\n    if matches[\"timezone\"] is None:\n        return default_timezone\n    sign = matches[\"tz_sign\"]\n    hours = to_int(matches, \"tz_hour\")\n    minutes = to_int(matches, \"tz_minute\", default_to_zero=True)\n    description = \"%s%02d:%02d\" % (sign, hours, minutes)\n    if sign == \"-\":\n        hours = -hours\n        minutes = -minutes\n    return FixedOffset(hours, minutes, description)", "response": "Parses ISO 8601 time zone specs into tzinfo offsets"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse ISO 8601 dates into a datetime object.", "response": "def parse_date(datestring, default_timezone=UTC):\n    \"\"\"Parses ISO 8601 dates into datetime objects\n\n    The timezone is parsed from the date string. However it is quite common to\n    have dates without a timezone (not strictly correct). In this case the\n    default timezone specified in default_timezone is used. This is UTC by\n    default.\n\n    :param datestring: The date to parse as a string\n    :param default_timezone: A datetime tzinfo instance to use when no timezone\n                             is specified in the datestring. If this is set to\n                             None then a naive datetime object is returned.\n    :returns: A datetime.datetime instance\n    :raises: ParseError when there is a problem parsing the date or\n             constructing the datetime instance.\n\n    \"\"\"\n    if not isinstance(datestring, _basestring):\n        raise ParseError(\"Expecting a string %r\" % datestring)\n    m = ISO8601_REGEX.match(datestring)\n    if not m:\n        raise ParseError(\"Unable to parse date string %r\" % datestring)\n    groups = m.groupdict()\n\n    tz = parse_timezone(groups, default_timezone=default_timezone)\n\n    groups[\"second_fraction\"] = int(Decimal(\"0.%s\" % (groups[\"second_fraction\"] or 0)) * Decimal(\"1000000.0\"))\n\n    try:\n        return datetime.datetime(\n            year=to_int(groups, \"year\"),\n            month=to_int(groups, \"month\", default=to_int(groups, \"monthdash\", required=False, default=1)),\n            day=to_int(groups, \"day\", default=to_int(groups, \"daydash\", required=False, default=1)),\n            hour=to_int(groups, \"hour\", default_to_zero=True),\n            minute=to_int(groups, \"minute\", default_to_zero=True),\n            second=to_int(groups, \"second\", default_to_zero=True),\n            microsecond=groups[\"second_fraction\"],\n            tzinfo=tz,\n        )\n    except Exception as e:\n        raise ParseError(e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsignals handler, used to gracefully shut down the ``spinner`` instance when specified signal is received by the process running the ``spinner``. ``signum`` and ``frame`` are mandatory arguments. Check ``signal.signal`` function for more details.", "response": "def default_handler(signum, frame, spinner):\n    \"\"\"Signal handler, used to gracefully shut down the ``spinner`` instance\n    when specified signal is received by the process running the ``spinner``.\n\n    ``signum`` and ``frame`` are mandatory arguments. Check ``signal.signal``\n    function for more details.\n    \"\"\"\n    spinner.fail()\n    spinner.stop()\n    sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsignals handler used to gracefully shut down the spinner instance when specified signal is received by the process running the spinner.", "response": "def fancy_handler(signum, frame, spinner):\n    \"\"\"Signal handler, used to gracefully shut down the ``spinner`` instance\n    when specified signal is received by the process running the ``spinner``.\n\n    ``signum`` and ``frame`` are mandatory arguments. Check ``signal.signal``\n    function for more details.\n    \"\"\"\n    spinner.red.fail(\"\u2718\")\n    spinner.stop()\n    sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uts46_remap(domain, std3_rules=True, transitional=False):\n    from .uts46data import uts46data\n    output = u\"\"\n    try:\n        for pos, char in enumerate(domain):\n            code_point = ord(char)\n            uts46row = uts46data[code_point if code_point < 256 else\n                bisect.bisect_left(uts46data, (code_point, \"Z\")) - 1]\n            status = uts46row[1]\n            replacement = uts46row[2] if len(uts46row) == 3 else None\n            if (status == \"V\" or\n                    (status == \"D\" and not transitional) or\n                    (status == \"3\" and not std3_rules and replacement is None)):\n                output += char\n            elif replacement is not None and (status == \"M\" or\n                    (status == \"3\" and not std3_rules) or\n                    (status == \"D\" and transitional)):\n                output += replacement\n            elif status != \"I\":\n                raise IndexError()\n        return unicodedata.normalize(\"NFC\", output)\n    except IndexError:\n        raise InvalidCodepoint(\n            \"Codepoint {0} not allowed at position {1} in {2}\".format(\n            _unot(code_point), pos + 1, repr(domain)))", "response": "Re - map the characters in the string according to UTS46 processing."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _implementation():\n    implementation = platform.python_implementation()\n\n    if implementation == 'CPython':\n        implementation_version = platform.python_version()\n    elif implementation == 'PyPy':\n        implementation_version = '%s.%s.%s' % (sys.pypy_version_info.major,\n                                               sys.pypy_version_info.minor,\n                                               sys.pypy_version_info.micro)\n        if sys.pypy_version_info.releaselevel != 'final':\n            implementation_version = ''.join([\n                implementation_version, sys.pypy_version_info.releaselevel\n            ])\n    elif implementation == 'Jython':\n        implementation_version = platform.python_version()  # Complete Guess\n    elif implementation == 'IronPython':\n        implementation_version = platform.python_version()  # Complete Guess\n    else:\n        implementation_version = 'Unknown'\n\n    return {'name': implementation, 'version': implementation_version}", "response": "Return a dict with the Python implementation and version."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef info():\n    try:\n        platform_info = {\n            'system': platform.system(),\n            'release': platform.release(),\n        }\n    except IOError:\n        platform_info = {\n            'system': 'Unknown',\n            'release': 'Unknown',\n        }\n\n    implementation_info = _implementation()\n    urllib3_info = {'version': urllib3.__version__}\n    chardet_info = {'version': chardet.__version__}\n\n    pyopenssl_info = {\n        'version': None,\n        'openssl_version': '',\n    }\n    if OpenSSL:\n        pyopenssl_info = {\n            'version': OpenSSL.__version__,\n            'openssl_version': '%x' % OpenSSL.SSL.OPENSSL_VERSION_NUMBER,\n        }\n    cryptography_info = {\n        'version': getattr(cryptography, '__version__', ''),\n    }\n    idna_info = {\n        'version': getattr(idna, '__version__', ''),\n    }\n\n    system_ssl = ssl.OPENSSL_VERSION_NUMBER\n    system_ssl_info = {\n        'version': '%x' % system_ssl if system_ssl is not None else ''\n    }\n\n    return {\n        'platform': platform_info,\n        'implementation': implementation_info,\n        'system_ssl': system_ssl_info,\n        'using_pyopenssl': pyopenssl is not None,\n        'pyOpenSSL': pyopenssl_info,\n        'urllib3': urllib3_info,\n        'chardet': chardet_info,\n        'cryptography': cryptography_info,\n        'idna': idna_info,\n        'requests': {\n            'version': requests_version,\n        },\n    }", "response": "Generate information for a bug report."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef package_type(self):\n        mapping = {'bdist_egg': u'egg', 'bdist_wheel': u'wheel',\n                   'sdist': u'source'}\n        ptype = self._release['packagetype']\n        if ptype in mapping.keys():\n            return mapping[ptype]\n        return ptype", "response": "Return the type of the package."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing a single character. Called by write.", "response": "def process (self, c):\n        \"\"\"Process a single character. Called by :meth:`write`.\"\"\"\n        if isinstance(c, bytes):\n            c = self._decode(c)\n        self.state.process(c)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write (self, s):\n        if isinstance(s, bytes):\n            s = self._decode(s)\n        for c in s:\n            self.process(c)", "response": "Process text writing it to the virtual screen while handling\n        ANSI escape codes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_ch (self, ch):\n        '''This puts a character at the current cursor position. The cursor\n        position is moved forward with wrap-around, but no scrolling is done if\n        the cursor hits the lower-right corner of the screen. '''\n\n        if isinstance(ch, bytes):\n            ch = self._decode(ch)\n\n        #\\r and \\n both produce a call to cr() and lf(), respectively.\n        ch = ch[0]\n\n        if ch == u'\\r':\n            self.cr()\n            return\n        if ch == u'\\n':\n            self.crlf()\n            return\n        if ch == chr(screen.BS):\n            self.cursor_back()\n            return\n        self.put_abs(self.cur_r, self.cur_c, ch)\n        old_r = self.cur_r\n        old_c = self.cur_c\n        self.cursor_forward()\n        if old_c == self.cur_c:\n            self.cursor_down()\n            if old_r != self.cur_r:\n                self.cursor_home (self.cur_r, 1)\n            else:\n                self.scroll_up ()\n                self.cursor_home (self.cur_r, 1)\n                self.erase_line()", "response": "This writes a character at the current cursor position."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_best_encoding(stream):\n    rv = getattr(stream, 'encoding', None) or sys.getdefaultencoding()\n    if is_ascii_encoding(rv):\n        return 'utf-8'\n    return rv", "response": "Returns the default stream encoding if not found."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_terminal_size(fallback=(80, 24)):\n    # Try the environment first\n    try:\n        columns = int(os.environ[\"COLUMNS\"])\n    except (KeyError, ValueError):\n        columns = 0\n\n    try:\n        lines = int(os.environ[\"LINES\"])\n    except (KeyError, ValueError):\n        lines = 0\n\n    # Only query if necessary\n    if columns <= 0 or lines <= 0:\n        try:\n            size = _get_terminal_size(sys.__stdout__.fileno())\n        except (NameError, OSError):\n            size = terminal_size(*fallback)\n\n        if columns <= 0:\n            columns = size.columns\n        if lines <= 0:\n            lines = size.lines\n\n    return terminal_size(columns, lines)", "response": "Get the size of the terminal window."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_headers(keep_alive=None, accept_encoding=None, user_agent=None,\n                 basic_auth=None, proxy_basic_auth=None, disable_cache=None):\n    \"\"\"\n    Shortcuts for generating request headers.\n\n    :param keep_alive:\n        If ``True``, adds 'connection: keep-alive' header.\n\n    :param accept_encoding:\n        Can be a boolean, list, or string.\n        ``True`` translates to 'gzip,deflate'.\n        List will get joined by comma.\n        String will be used as provided.\n\n    :param user_agent:\n        String representing the user-agent you want, such as\n        \"python-urllib3/0.6\"\n\n    :param basic_auth:\n        Colon-separated username:password string for 'authorization: basic ...'\n        auth header.\n\n    :param proxy_basic_auth:\n        Colon-separated username:password string for 'proxy-authorization: basic ...'\n        auth header.\n\n    :param disable_cache:\n        If ``True``, adds 'cache-control: no-cache' header.\n\n    Example::\n\n        >>> make_headers(keep_alive=True, user_agent=\"Batman/1.0\")\n        {'connection': 'keep-alive', 'user-agent': 'Batman/1.0'}\n        >>> make_headers(accept_encoding=True)\n        {'accept-encoding': 'gzip,deflate'}\n    \"\"\"\n    headers = {}\n    if accept_encoding:\n        if isinstance(accept_encoding, str):\n            pass\n        elif isinstance(accept_encoding, list):\n            accept_encoding = ','.join(accept_encoding)\n        else:\n            accept_encoding = ACCEPT_ENCODING\n        headers['accept-encoding'] = accept_encoding\n\n    if user_agent:\n        headers['user-agent'] = user_agent\n\n    if keep_alive:\n        headers['connection'] = 'keep-alive'\n\n    if basic_auth:\n        headers['authorization'] = 'Basic ' + \\\n            b64encode(b(basic_auth)).decode('utf-8')\n\n    if proxy_basic_auth:\n        headers['proxy-authorization'] = 'Basic ' + \\\n            b64encode(b(proxy_basic_auth)).decode('utf-8')\n\n    if disable_cache:\n        headers['cache-control'] = 'no-cache'\n\n    return headers", "response": "Generates HTTP headers for the current request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_file_position(body, pos):\n    if pos is not None:\n        rewind_body(body, pos)\n    elif getattr(body, 'tell', None) is not None:\n        try:\n            pos = body.tell()\n        except (IOError, OSError):\n            # This differentiates from None, allowing us to catch\n            # a failed `tell()` later when trying to rewind the body.\n            pos = _FAILEDTELL\n\n    return pos", "response": "Set the file position for the next available entry in the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rewind_body(body, body_pos):\n    body_seek = getattr(body, 'seek', None)\n    if body_seek is not None and isinstance(body_pos, integer_types):\n        try:\n            body_seek(body_pos)\n        except (IOError, OSError):\n            raise UnrewindableBodyError(\"An error occurred when rewinding request \"\n                                        \"body for redirect/retry.\")\n    elif body_pos is _FAILEDTELL:\n        raise UnrewindableBodyError(\"Unable to record file position for rewinding \"\n                                    \"request body during a redirect/retry.\")\n    else:\n        raise ValueError(\"body_pos must be of type integer, \"\n                         \"instead it was %s.\" % type(body_pos))", "response": "Attempt to rewind the body to a certain position."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mapping_to_frozenset(mapping):\n    mapping = mapping.copy()\n    for key, value in mapping.items():\n        if isinstance(value, Mapping):\n            mapping[key] = mapping_to_frozenset(value)\n        elif isinstance(value, Sequence):\n            value = list(value)\n            for i, item in enumerate(value):\n                if isinstance(item, Mapping):\n                    value[i] = mapping_to_frozenset(item)\n            mapping[key] = tuple(value)\n    return frozenset(mapping.items())", "response": "Convert a mapping to a frozen set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cmdify(self):\n        return \" \".join(\n            itertools.chain(\n                [_quote_if_contains(self.command, r\"[\\s^()]\")],\n                (_quote_if_contains(arg, r\"[\\s^]\") for arg in self.args),\n            )\n        )", "response": "Encode into a cmd - executable string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _split_what(what):\n    return (\n        frozenset(cls for cls in what if isclass(cls)),\n        frozenset(cls for cls in what if isinstance(cls, Attribute)),\n    )", "response": "Splits the given set of classes and attributes into two sets."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef include(*what):\n    cls, attrs = _split_what(what)\n\n    def include_(attribute, value):\n        return value.__class__ in cls or attribute in attrs\n\n    return include_", "response": "Returns a function that returns True if the value of the attribute is in the class of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exclude(*what):\n    cls, attrs = _split_what(what)\n\n    def exclude_(attribute, value):\n        return value.__class__ not in cls and attribute not in attrs\n\n    return exclude_", "response": "Returns a function that returns True if the value of the attribute in the value is not in the class of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the attrs attribute values of an object as a dict.", "response": "def asdict(\n    inst,\n    recurse=True,\n    filter=None,\n    dict_factory=dict,\n    retain_collection_types=False,\n):\n    \"\"\"\n    Return the ``attrs`` attribute values of *inst* as a dict.\n\n    Optionally recurse into other ``attrs``-decorated classes.\n\n    :param inst: Instance of an ``attrs``-decorated class.\n    :param bool recurse: Recurse into classes that are also\n        ``attrs``-decorated.\n    :param callable filter: A callable whose return code determines whether an\n        attribute or element is included (``True``) or dropped (``False``).  Is\n        called with the :class:`attr.Attribute` as the first argument and the\n        value as the second argument.\n    :param callable dict_factory: A callable to produce dictionaries from.  For\n        example, to produce ordered dictionaries instead of normal Python\n        dictionaries, pass in ``collections.OrderedDict``.\n    :param bool retain_collection_types: Do not convert to ``list`` when\n        encountering an attribute whose type is ``tuple`` or ``set``.  Only\n        meaningful if ``recurse`` is ``True``.\n\n    :rtype: return type of *dict_factory*\n\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    ..  versionadded:: 16.0.0 *dict_factory*\n    ..  versionadded:: 16.1.0 *retain_collection_types*\n    \"\"\"\n    attrs = fields(inst.__class__)\n    rv = dict_factory()\n    for a in attrs:\n        v = getattr(inst, a.name)\n        if filter is not None and not filter(a, v):\n            continue\n        if recurse is True:\n            if has(v.__class__):\n                rv[a.name] = asdict(\n                    v, True, filter, dict_factory, retain_collection_types\n                )\n            elif isinstance(v, (tuple, list, set)):\n                cf = v.__class__ if retain_collection_types is True else list\n                rv[a.name] = cf(\n                    [\n                        _asdict_anything(\n                            i, filter, dict_factory, retain_collection_types\n                        )\n                        for i in v\n                    ]\n                )\n            elif isinstance(v, dict):\n                df = dict_factory\n                rv[a.name] = df(\n                    (\n                        _asdict_anything(\n                            kk, filter, df, retain_collection_types\n                        ),\n                        _asdict_anything(\n                            vv, filter, df, retain_collection_types\n                        ),\n                    )\n                    for kk, vv in iteritems(v)\n                )\n            else:\n                rv[a.name] = v\n        else:\n            rv[a.name] = v\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef astuple(\n    inst,\n    recurse=True,\n    filter=None,\n    tuple_factory=tuple,\n    retain_collection_types=False,\n):\n    \"\"\"\n    Return the ``attrs`` attribute values of *inst* as a tuple.\n\n    Optionally recurse into other ``attrs``-decorated classes.\n\n    :param inst: Instance of an ``attrs``-decorated class.\n    :param bool recurse: Recurse into classes that are also\n        ``attrs``-decorated.\n    :param callable filter: A callable whose return code determines whether an\n        attribute or element is included (``True``) or dropped (``False``).  Is\n        called with the :class:`attr.Attribute` as the first argument and the\n        value as the second argument.\n    :param callable tuple_factory: A callable to produce tuples from.  For\n        example, to produce lists instead of tuples.\n    :param bool retain_collection_types: Do not convert to ``list``\n        or ``dict`` when encountering an attribute which type is\n        ``tuple``, ``dict`` or ``set``.  Only meaningful if ``recurse`` is\n        ``True``.\n\n    :rtype: return type of *tuple_factory*\n\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    ..  versionadded:: 16.2.0\n    \"\"\"\n    attrs = fields(inst.__class__)\n    rv = []\n    retain = retain_collection_types  # Very long. :/\n    for a in attrs:\n        v = getattr(inst, a.name)\n        if filter is not None and not filter(a, v):\n            continue\n        if recurse is True:\n            if has(v.__class__):\n                rv.append(\n                    astuple(\n                        v,\n                        recurse=True,\n                        filter=filter,\n                        tuple_factory=tuple_factory,\n                        retain_collection_types=retain,\n                    )\n                )\n            elif isinstance(v, (tuple, list, set)):\n                cf = v.__class__ if retain is True else list\n                rv.append(\n                    cf(\n                        [\n                            astuple(\n                                j,\n                                recurse=True,\n                                filter=filter,\n                                tuple_factory=tuple_factory,\n                                retain_collection_types=retain,\n                            )\n                            if has(j.__class__)\n                            else j\n                            for j in v\n                        ]\n                    )\n                )\n            elif isinstance(v, dict):\n                df = v.__class__ if retain is True else dict\n                rv.append(\n                    df(\n                        (\n                            astuple(\n                                kk,\n                                tuple_factory=tuple_factory,\n                                retain_collection_types=retain,\n                            )\n                            if has(kk.__class__)\n                            else kk,\n                            astuple(\n                                vv,\n                                tuple_factory=tuple_factory,\n                                retain_collection_types=retain,\n                            )\n                            if has(vv.__class__)\n                            else vv,\n                        )\n                        for kk, vv in iteritems(v)\n                    )\n                )\n            else:\n                rv.append(v)\n        else:\n            rv.append(v)\n    return rv if tuple_factory is list else tuple_factory(rv)", "response": "Return the attrs attribute values of inst as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new copy of inst with the specified attributes applied.", "response": "def assoc(inst, **changes):\n    \"\"\"\n    Copy *inst* and apply *changes*.\n\n    :param inst: Instance of a class with ``attrs`` attributes.\n    :param changes: Keyword changes in the new copy.\n\n    :return: A copy of inst with *changes* incorporated.\n\n    :raise attr.exceptions.AttrsAttributeNotFoundError: If *attr_name* couldn't\n        be found on *cls*.\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    ..  deprecated:: 17.1.0\n        Use :func:`evolve` instead.\n    \"\"\"\n    import warnings\n\n    warnings.warn(\n        \"assoc is deprecated and will be removed after 2018/01.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    new = copy.copy(inst)\n    attrs = fields(inst.__class__)\n    for k, v in iteritems(changes):\n        a = getattr(attrs, k, NOTHING)\n        if a is NOTHING:\n            raise AttrsAttributeNotFoundError(\n                \"{k} is not an attrs attribute on {cl}.\".format(\n                    k=k, cl=new.__class__\n                )\n            )\n        _obj_setattr(new, k, v)\n    return new"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef evolve(inst, **changes):\n    cls = inst.__class__\n    attrs = fields(cls)\n    for a in attrs:\n        if not a.init:\n            continue\n        attr_name = a.name  # To deal with private attributes.\n        init_name = attr_name if attr_name[0] != \"_\" else attr_name[1:]\n        if init_name not in changes:\n            changes[init_name] = getattr(inst, attr_name)\n    return cls(**changes)", "response": "Creates a new instance based on inst with attributes applied."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new package object for the specified package name on the PyPI server.", "response": "def get(package_name, pypi_server=\"https://pypi.python.org/pypi/\"):\n    \"\"\"\n    Constructs a request to the PyPI server and returns a\n    :class:`yarg.package.Package`.\n\n    :param package_name: case sensitive name of the package on the PyPI server.\n    :param pypi_server: (option) URL to the PyPI server.\n\n        >>> import yarg\n        >>> package = yarg.get('yarg')\n        <Package yarg>\n    \"\"\"\n    if not pypi_server.endswith(\"/\"):\n        pypi_server = pypi_server + \"/\"\n    response = requests.get(\"{0}{1}/json\".format(pypi_server,\n                                                 package_name))\n    if response.status_code >= 300:\n        raise HTTPError(status_code=response.status_code,\n                        reason=response.reason)\n    if hasattr(response.content, 'decode'):\n        return json2package(response.content.decode())\n    else:\n        return json2package(response.content)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_ctx(cli, prog_name, args):\n    ctx = cli.make_context(prog_name, args, resilient_parsing=True)\n    args = ctx.protected_args + ctx.args\n    while args:\n        if isinstance(ctx.command, MultiCommand):\n            if not ctx.command.chain:\n                cmd_name, cmd, args = ctx.command.resolve_command(ctx, args)\n                if cmd is None:\n                    return ctx\n                ctx = cmd.make_context(cmd_name, args, parent=ctx,\n                                       resilient_parsing=True)\n                args = ctx.protected_args + ctx.args\n            else:\n                # Walk chained subcommand contexts saving the last one.\n                while args:\n                    cmd_name, cmd, args = ctx.command.resolve_command(ctx, args)\n                    if cmd is None:\n                        return ctx\n                    sub_ctx = cmd.make_context(cmd_name, args, parent=ctx,\n                                               allow_extra_args=True,\n                                               allow_interspersed_args=False,\n                                               resilient_parsing=True)\n                    args = sub_ctx.args\n                ctx = sub_ctx\n                args = sub_ctx.protected_args + sub_ctx.args\n        else:\n            break\n    return ctx", "response": "Parse into a hierarchy of contexts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_incomplete_option(all_args, cmd_param):\n    if not isinstance(cmd_param, Option):\n        return False\n    if cmd_param.is_flag:\n        return False\n    last_option = None\n    for index, arg_str in enumerate(reversed([arg for arg in all_args if arg != WORDBREAK])):\n        if index + 1 > cmd_param.nargs:\n            break\n        if start_of_option(arg_str):\n            last_option = arg_str\n\n    return True if last_option and last_option in cmd_param.opts else False", "response": "Checks whether or not the last option declaration is incomplete and if it is not the last option in the list of args."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking whether or not the last argument is incomplete.", "response": "def is_incomplete_argument(current_params, cmd_param):\n    \"\"\"\n    :param current_params: the current params and values for this argument as already entered\n    :param cmd_param: the current command parameter\n    :return: whether or not the last argument is incomplete and corresponds to this cmd_param. In\n    other words whether or not the this cmd_param argument can still accept values\n    \"\"\"\n    if not isinstance(cmd_param, Argument):\n        return False\n    current_param_values = current_params[cmd_param.name]\n    if current_param_values is None:\n        return True\n    if cmd_param.nargs == -1:\n        return True\n    if isinstance(current_param_values, abc.Iterable) \\\n            and cmd_param.nargs > 1 and len(current_param_values) < cmd_param.nargs:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of user - specified completions for the param .", "response": "def get_user_autocompletions(ctx, args, incomplete, cmd_param):\n    \"\"\"\n    :param ctx: context associated with the parsed command\n    :param args: full list of args\n    :param incomplete: the incomplete text to autocomplete\n    :param cmd_param: command definition\n    :return: all the possible user-specified completions for the param\n    \"\"\"\n    results = []\n    if isinstance(cmd_param.type, Choice):\n        # Choices don't support descriptions.\n        results = [(c, None)\n                   for c in cmd_param.type.choices if str(c).startswith(incomplete)]\n    elif cmd_param.autocompletion is not None:\n        dynamic_completions = cmd_param.autocompletion(ctx=ctx,\n                                                       args=args,\n                                                       incomplete=incomplete)\n        results = [c if isinstance(c, tuple) else (c, None)\n                   for c in dynamic_completions]\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all visible commands that start with starts_with.", "response": "def get_visible_commands_starting_with(ctx, starts_with):\n    \"\"\"\n    :param ctx: context associated with the parsed command\n    :starts_with: string that visible commands must start with.\n    :return: all visible (not hidden) commands that start with starts_with.\n    \"\"\"\n    for c in ctx.command.list_commands(ctx):\n        if c.startswith(starts_with):\n            command = ctx.command.get_command(ctx, c)\n            if not command.hidden:\n                yield command"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_choices(cli, prog_name, args, incomplete):\n    all_args = copy.deepcopy(args)\n\n    ctx = resolve_ctx(cli, prog_name, args)\n    if ctx is None:\n        return []\n\n    # In newer versions of bash long opts with '='s are partitioned, but it's easier to parse\n    # without the '='\n    if start_of_option(incomplete) and WORDBREAK in incomplete:\n        partition_incomplete = incomplete.partition(WORDBREAK)\n        all_args.append(partition_incomplete[0])\n        incomplete = partition_incomplete[2]\n    elif incomplete == WORDBREAK:\n        incomplete = ''\n\n    completions = []\n    if start_of_option(incomplete):\n        # completions for partial options\n        for param in ctx.command.params:\n            if isinstance(param, Option) and not param.hidden:\n                param_opts = [param_opt for param_opt in param.opts +\n                              param.secondary_opts if param_opt not in all_args or param.multiple]\n                completions.extend([(o, param.help) for o in param_opts if o.startswith(incomplete)])\n        return completions\n    # completion for option values from user supplied values\n    for param in ctx.command.params:\n        if is_incomplete_option(all_args, param):\n            return get_user_autocompletions(ctx, all_args, incomplete, param)\n    # completion for argument values from user supplied values\n    for param in ctx.command.params:\n        if is_incomplete_argument(ctx.params, param):\n            return get_user_autocompletions(ctx, all_args, incomplete, param)\n\n    add_subcommand_completions(ctx, incomplete, completions)\n    # Sort before returning so that proper ordering can be enforced in custom types.\n    return sorted(completions)", "response": "Get all possible completions for the incomplete command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninterpreting a marker and return a result depending on environment.", "response": "def interpret(marker, execution_context=None):\n    \"\"\"\n    Interpret a marker and return a result depending on environment.\n\n    :param marker: The marker to interpret.\n    :type marker: str\n    :param execution_context: The context used for name lookup.\n    :type execution_context: mapping\n    \"\"\"\n    try:\n        expr, rest = parse_marker(marker)\n    except Exception as e:\n        raise SyntaxError('Unable to interpret marker syntax: %s: %s' % (marker, e))\n    if rest and rest[0] != '#':\n        raise SyntaxError('unexpected trailing data in marker: %s: %s' % (marker, rest))\n    context = dict(DEFAULT_CONTEXT)\n    if execution_context:\n        context.update(execution_context)\n    return evaluator.evaluate(expr, context)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating a marker expression in the specified context.", "response": "def evaluate(self, expr, context):\n        \"\"\"\n        Evaluate a marker expression returned by the :func:`parse_requirement`\n        function in the specified context.\n        \"\"\"\n        if isinstance(expr, string_types):\n            if expr[0] in '\\'\"':\n                result = expr[1:-1]\n            else:\n                if expr not in context:\n                    raise SyntaxError('unknown variable: %s' % expr)\n                result = context[expr]\n        else:\n            assert isinstance(expr, dict)\n            op = expr['op']\n            if op not in self.operations:\n                raise NotImplementedError('op not implemented: %s' % op)\n            elhs = expr['lhs']\n            erhs = expr['rhs']\n            if _is_literal(expr['lhs']) and _is_literal(expr['rhs']):\n                raise SyntaxError('invalid comparison: %s %s %s' % (elhs, op, erhs))\n\n            lhs = self.evaluate(elhs, context)\n            rhs = self.evaluate(erhs, context)\n            result = self.operations[op](lhs, rhs)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef colored(text, color=None, on_color=None, attrs=None):\n    if os.getenv(\"ANSI_COLORS_DISABLED\") is None:\n        style = \"NORMAL\"\n        if \"bold\" in attrs:\n            style = \"BRIGHT\"\n            attrs.remove(\"bold\")\n        if color is not None:\n            color = color.upper()\n            text = to_native_string(\"%s%s%s%s%s\") % (\n                to_native_string(getattr(colorama.Fore, color)),\n                to_native_string(getattr(colorama.Style, style)),\n                to_native_string(text),\n                to_native_string(colorama.Fore.RESET),\n                to_native_string(colorama.Style.NORMAL),\n            )\n\n        if on_color is not None:\n            on_color = on_color.upper()\n            text = to_native_string(\"%s%s%s%s\") % (\n                to_native_string(getattr(colorama.Back, on_color)),\n                to_native_string(text),\n                to_native_string(colorama.Back.RESET),\n                to_native_string(colorama.Style.NORMAL),\n            )\n\n        if attrs is not None:\n            fmt_str = to_native_string(\"%s[%%dm%%s%s[9m\") % (chr(27), chr(27))\n            for attr in attrs:\n                text = fmt_str % (ATTRIBUTES[attr], text)\n\n        text += RESET\n    return text", "response": "Colorize text using a reimplementation of the colorizer from yaspin."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inc_n(self, n, exception=None):  # type: (int, Optional[ParseError]) -> bool\n        return self._src.inc_n(n=n, exception=exception)", "response": "Increments the parser by n characters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconsumes chars until min or max is satisfied.", "response": "def consume(self, chars, min=0, max=-1):\n        \"\"\"\n        Consume chars until min/max is satisfied is valid.\n        \"\"\"\n        return self._src.consume(chars=chars, min=min, max=max)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_error(self, exception=ParseError, *args):\n        return self._src.parse_error(exception, *args)", "response": "Creates a generic parse error"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmerges the given Item with the last one currently in the given Container. Returns True if the items were merged False otherwise.", "response": "def _merge_ws(self, item, container):  # type: (Item, Container) -> bool\n        \"\"\"\n        Merges the given Item with the last one currently in the given Container if\n        both are whitespace items.\n\n        Returns True if the items were merged.\n        \"\"\"\n        last = container.last_item()\n        if not last:\n            return False\n\n        if not isinstance(item, Whitespace) or not isinstance(last, Whitespace):\n            return False\n\n        start = self._idx - (len(last.s) + len(item.s))\n        container.body[-1] = (\n            container.body[-1][0],\n            Whitespace(self._src[start : self._idx]),\n        )\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_child(self, parent, child):  # type: (str, str) -> bool\n        parent_parts = tuple(self._split_table_name(parent))\n        child_parts = tuple(self._split_table_name(child))\n\n        if parent_parts == child_parts:\n            return False\n\n        return parent_parts == child_parts[: len(parent_parts)]", "response": "Returns whether a key is strictly a child of another key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the next item and returns it as a tuple.", "response": "def _parse_item(self):  # type: () -> Optional[Tuple[Optional[Key], Item]]\n        \"\"\"\n        Attempts to parse the next item and returns it, along with its key\n        if the item is value-like.\n        \"\"\"\n        self.mark()\n        with self._state as state:\n            while True:\n                c = self._current\n                if c == \"\\n\":\n                    # Found a newline; Return all whitespace found up to this point.\n                    self.inc()\n\n                    return None, Whitespace(self.extract())\n                elif c in \" \\t\\r\":\n                    # Skip whitespace.\n                    if not self.inc():\n                        return None, Whitespace(self.extract())\n                elif c == \"#\":\n                    # Found a comment, parse it\n                    indent = self.extract()\n                    cws, comment, trail = self._parse_comment_trail()\n\n                    return None, Comment(Trivia(indent, cws, comment, trail))\n                elif c == \"[\":\n                    # Found a table, delegate to the calling function.\n                    return\n                else:\n                    # Begining of a KV pair.\n                    # Return to beginning of whitespace so it gets included\n                    # as indentation for the KV about to be parsed.\n                    state.restore = True\n                    break\n\n        return self._parse_key_value(True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the comment and trail.", "response": "def _parse_comment_trail(self):  # type: () -> Tuple[str, str, str]\n        \"\"\"\n        Returns (comment_ws, comment, trail)\n        If there is no comment, comment_ws and comment will\n        simply be empty.\n        \"\"\"\n        if self.end():\n            return \"\", \"\", \"\"\n\n        comment = \"\"\n        comment_ws = \"\"\n        self.mark()\n\n        while True:\n            c = self._current\n\n            if c == \"\\n\":\n                break\n            elif c == \"#\":\n                comment_ws = self.extract()\n\n                self.mark()\n                self.inc()  # Skip #\n\n                # The comment itself\n                while not self.end() and not self._current.is_nl() and self.inc():\n                    pass\n\n                comment = self.extract()\n                self.mark()\n\n                break\n            elif c in \" \\t\\r\":\n                self.inc()\n            else:\n                raise self.parse_error(UnexpectedCharError, c)\n\n            if self.end():\n                break\n\n        while self._current.is_spaces() and self.inc():\n            pass\n\n        if self._current == \"\\r\":\n            self.inc()\n\n        if self._current == \"\\n\":\n            self.inc()\n\n        trail = \"\"\n        if self._idx != self._marker or self._current.is_ws():\n            trail = self.extract()\n\n        return comment_ws, comment, trail"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_quoted_key(self):  # type: () -> Key\n        quote_style = self._current\n        key_type = None\n        dotted = False\n        for t in KeyType:\n            if t.value == quote_style:\n                key_type = t\n                break\n\n        if key_type is None:\n            raise RuntimeError(\"Should not have entered _parse_quoted_key()\")\n\n        self.inc()\n        self.mark()\n\n        while self._current != quote_style and self.inc():\n            pass\n\n        key = self.extract()\n\n        if self._current == \".\":\n            self.inc()\n            dotted = True\n            key += \".\" + self._parse_key().as_string()\n            key_type = KeyType.Bare\n        else:\n            self.inc()\n\n        return Key(key, key_type, \"\", dotted)", "response": "Parses a key enclosed in either single or double quotes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_bare_key(self):  # type: () -> Key\n        key_type = None\n        dotted = False\n\n        self.mark()\n        while self._current.is_bare_key_char() and self.inc():\n            pass\n\n        key = self.extract()\n\n        if self._current == \".\":\n            self.inc()\n            dotted = True\n            key += \".\" + self._parse_key().as_string()\n            key_type = KeyType.Bare\n\n        return Key(key, key_type, \"\", dotted)", "response": "Parses a bare key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a value from the current position.", "response": "def _parse_value(self):  # type: () -> Item\n        \"\"\"\n        Attempts to parse a value at the current position.\n        \"\"\"\n        self.mark()\n        c = self._current\n        trivia = Trivia()\n\n        if c == StringType.SLB.value:\n            return self._parse_basic_string()\n        elif c == StringType.SLL.value:\n            return self._parse_literal_string()\n        elif c == BoolType.TRUE.value[0]:\n            return self._parse_true()\n        elif c == BoolType.FALSE.value[0]:\n            return self._parse_false()\n        elif c == \"[\":\n            return self._parse_array()\n        elif c == \"{\":\n            return self._parse_inline_table()\n        elif c in \"+-\" or self._peek(4) in {\n            \"+inf\",\n            \"-inf\",\n            \"inf\",\n            \"+nan\",\n            \"-nan\",\n            \"nan\",\n        }:\n            # Number\n            while self._current not in \" \\t\\n\\r#,]}\" and self.inc():\n                pass\n\n            raw = self.extract()\n\n            item = self._parse_number(raw, trivia)\n            if item is not None:\n                return item\n\n            raise self.parse_error(InvalidNumberError)\n        elif c in string.digits:\n            # Integer, Float, Date, Time or DateTime\n            while self._current not in \" \\t\\n\\r#,]}\" and self.inc():\n                pass\n\n            raw = self.extract()\n\n            m = RFC_3339_LOOSE.match(raw)\n            if m:\n                if m.group(1) and m.group(5):\n                    # datetime\n                    try:\n                        return DateTime(parse_rfc3339(raw), trivia, raw)\n                    except ValueError:\n                        raise self.parse_error(InvalidDateTimeError)\n\n                if m.group(1):\n                    try:\n                        return Date(parse_rfc3339(raw), trivia, raw)\n                    except ValueError:\n                        raise self.parse_error(InvalidDateError)\n\n                if m.group(5):\n                    try:\n                        return Time(parse_rfc3339(raw), trivia, raw)\n                    except ValueError:\n                        raise self.parse_error(InvalidTimeError)\n\n            item = self._parse_number(raw, trivia)\n            if item is not None:\n                return item\n\n            raise self.parse_error(InvalidNumberError)\n        else:\n            raise self.parse_error(UnexpectedCharError, c)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_table(\n        self, parent_name=None\n    ):  # type: (Optional[str]) -> Tuple[Key, Union[Table, AoT]]\n        \"\"\"\n        Parses a table element.\n        \"\"\"\n        if self._current != \"[\":\n            raise self.parse_error(\n                InternalParserError, \"_parse_table() called on non-bracket character.\"\n            )\n\n        indent = self.extract()\n        self.inc()  # Skip opening bracket\n\n        if self.end():\n            raise self.parse_error(UnexpectedEofError)\n\n        is_aot = False\n        if self._current == \"[\":\n            if not self.inc():\n                raise self.parse_error(UnexpectedEofError)\n\n            is_aot = True\n\n        # Key\n        self.mark()\n        while self._current != \"]\" and self.inc():\n            if self.end():\n                raise self.parse_error(UnexpectedEofError)\n\n            pass\n\n        name = self.extract()\n        if not name.strip():\n            raise self.parse_error(EmptyTableNameError)\n\n        key = Key(name, sep=\"\")\n        name_parts = tuple(self._split_table_name(name))\n        missing_table = False\n        if parent_name:\n            parent_name_parts = tuple(self._split_table_name(parent_name))\n        else:\n            parent_name_parts = tuple()\n\n        if len(name_parts) > len(parent_name_parts) + 1:\n            missing_table = True\n\n        name_parts = name_parts[len(parent_name_parts) :]\n\n        values = Container(True)\n\n        self.inc()  # Skip closing bracket\n        if is_aot:\n            # TODO: Verify close bracket\n            self.inc()\n\n        cws, comment, trail = self._parse_comment_trail()\n\n        result = Null()\n\n        if len(name_parts) > 1:\n            if missing_table:\n                # Missing super table\n                # i.e. a table initialized like this: [foo.bar]\n                # without initializing [foo]\n                #\n                # So we have to create the parent tables\n                table = Table(\n                    Container(True),\n                    Trivia(indent, cws, comment, trail),\n                    is_aot and name_parts[0].key in self._aot_stack,\n                    is_super_table=True,\n                    name=name_parts[0].key,\n                )\n\n                result = table\n                key = name_parts[0]\n\n                for i, _name in enumerate(name_parts[1:]):\n                    if _name in table:\n                        child = table[_name]\n                    else:\n                        child = Table(\n                            Container(True),\n                            Trivia(indent, cws, comment, trail),\n                            is_aot and i == len(name_parts[1:]) - 1,\n                            is_super_table=i < len(name_parts[1:]) - 1,\n                            name=_name.key,\n                            display_name=name if i == len(name_parts[1:]) - 1 else None,\n                        )\n\n                    if is_aot and i == len(name_parts[1:]) - 1:\n                        table.append(_name, AoT([child], name=table.name, parsed=True))\n                    else:\n                        table.append(_name, child)\n\n                    table = child\n                    values = table.value\n        else:\n            if name_parts:\n                key = name_parts[0]\n\n        while not self.end():\n            item = self._parse_item()\n            if item:\n                _key, item = item\n                if not self._merge_ws(item, values):\n                    if _key is not None and _key.is_dotted():\n                        self._handle_dotted_key(values, _key, item)\n                    else:\n                        values.append(_key, item)\n            else:\n                if self._current == \"[\":\n                    is_aot_next, name_next = self._peek_table()\n\n                    if self._is_child(name, name_next):\n                        key_next, table_next = self._parse_table(name)\n\n                        values.append(key_next, table_next)\n\n                        # Picking up any sibling\n                        while not self.end():\n                            _, name_next = self._peek_table()\n\n                            if not self._is_child(name, name_next):\n                                break\n\n                            key_next, table_next = self._parse_table(name)\n\n                            values.append(key_next, table_next)\n\n                    break\n                else:\n                    raise self.parse_error(\n                        InternalParserError,\n                        \"_parse_item() returned None on a non-bracket character.\",\n                    )\n\n        if isinstance(result, Null):\n            result = Table(\n                values,\n                Trivia(indent, cws, comment, trail),\n                is_aot,\n                name=name,\n                display_name=name,\n            )\n\n            if is_aot and (not self._aot_stack or name != self._aot_stack[-1]):\n                result = self._parse_aot(result, name)\n\n        return key, result", "response": "Parses a table element."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npeeks ahead non - intrusively by cloning then restoring the parser.", "response": "def _peek_table(self):  # type: () -> Tuple[bool, str]\n        \"\"\"\n        Peeks ahead non-intrusively by cloning then restoring the\n        initial state of the parser.\n\n        Returns the name of the table about to be parsed,\n        as well as whether it is part of an AoT.\n        \"\"\"\n        # we always want to restore after exiting this scope\n        with self._state(save_marker=True, restore=True):\n            if self._current != \"[\":\n                raise self.parse_error(\n                    InternalParserError,\n                    \"_peek_table() entered on non-bracket character\",\n                )\n\n            # AoT\n            self.inc()\n            is_aot = False\n            if self._current == \"[\":\n                self.inc()\n                is_aot = True\n\n            self.mark()\n\n            while self._current != \"]\" and self.inc():\n                table_name = self.extract()\n\n            return is_aot, table_name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_aot(self, first, name_first):  # type: (Table, str) -> AoT\n        payload = [first]\n        self._aot_stack.append(name_first)\n        while not self.end():\n            is_aot_next, name_next = self._peek_table()\n            if is_aot_next and name_next == name_first:\n                _, table = self._parse_table(name_first)\n                payload.append(table)\n            else:\n                break\n\n        self._aot_stack.pop()\n\n        return AoT(payload, parsed=True)", "response": "Parses all siblings of the provided table first and bundles them into AoT."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _peek(self, n):  # type: (int) -> str\n        # we always want to restore after exiting this scope\n        with self._state(restore=True):\n            buf = \"\"\n            for _ in range(n):\n                if self._current not in \" \\t\\n\\r#,]}\":\n                    buf += self._current\n                    self.inc()\n                    continue\n\n                break\n            return buf", "response": "Returns the next n characters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _peek_unicode(\n        self, is_long\n    ):  # type: (bool) -> Tuple[Optional[str], Optional[str]]\n        \"\"\"\n        Peeks ahead non-intrusively by cloning then restoring the\n        initial state of the parser.\n\n        Returns the unicode value is it's a valid one else None.\n        \"\"\"\n        # we always want to restore after exiting this scope\n        with self._state(save_marker=True, restore=True):\n            if self._current not in {\"u\", \"U\"}:\n                raise self.parse_error(\n                    InternalParserError, \"_peek_unicode() entered on non-unicode value\"\n                )\n\n            self.inc()  # Dropping prefix\n            self.mark()\n\n            if is_long:\n                chars = 8\n            else:\n                chars = 4\n\n            if not self.inc_n(chars):\n                value, extracted = None, None\n            else:\n                extracted = self.extract()\n\n                if extracted[0].lower() == \"d\" and extracted[1].strip(\"01234567\"):\n                    return None, None\n\n                try:\n                    value = chr(int(extracted, 16))\n                except ValueError:\n                    value = None\n\n            return value, extracted", "response": "Return the unicode value of the current state of the current locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_first(s, delims):\n    min_idx = None\n    min_delim = None\n    for d in delims:\n        idx = s.find(d)\n        if idx < 0:\n            continue\n\n        if min_idx is None or idx < min_idx:\n            min_idx = idx\n            min_delim = d\n\n    if min_idx is None or min_idx < 0:\n        return s, '', None\n\n    return s[:min_idx], s[min_idx + 1:], min_delim", "response": "Given a string and an iterable of delimiters split on the first found\n    delimiter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a url return a parsed : class :. Url namedtuple.", "response": "def parse_url(url):\n    \"\"\"\n    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\n    performed to parse incomplete urls. Fields not provided will be None.\n\n    Partly backwards-compatible with :mod:`urlparse`.\n\n    Example::\n\n        >>> parse_url('http://google.com/mail/')\n        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)\n        >>> parse_url('google.com:80')\n        Url(scheme=None, host='google.com', port=80, path=None, ...)\n        >>> parse_url('/foo?bar')\n        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)\n    \"\"\"\n\n    # While this code has overlap with stdlib's urlparse, it is much\n    # simplified for our needs and less annoying.\n    # Additionally, this implementations does silly things to be optimal\n    # on CPython.\n\n    if not url:\n        # Empty\n        return Url()\n\n    scheme = None\n    auth = None\n    host = None\n    port = None\n    path = None\n    fragment = None\n    query = None\n\n    # Scheme\n    if '://' in url:\n        scheme, url = url.split('://', 1)\n\n    # Find the earliest Authority Terminator\n    # (http://tools.ietf.org/html/rfc3986#section-3.2)\n    url, path_, delim = split_first(url, ['/', '?', '#'])\n\n    if delim:\n        # Reassemble the path\n        path = delim + path_\n\n    # Auth\n    if '@' in url:\n        # Last '@' denotes end of auth part\n        auth, url = url.rsplit('@', 1)\n\n    # IPv6\n    if url and url[0] == '[':\n        host, url = url.split(']', 1)\n        host += ']'\n\n    # Port\n    if ':' in url:\n        _host, port = url.split(':', 1)\n\n        if not host:\n            host = _host\n\n        if port:\n            # If given, ports must be integers. No whitespace, no plus or\n            # minus prefixes, no non-integer digits such as ^2 (superscript).\n            if not port.isdigit():\n                raise LocationParseError(url)\n            try:\n                port = int(port)\n            except ValueError:\n                raise LocationParseError(url)\n        else:\n            # Blank ports are cool, too. (rfc3986#section-3.2.3)\n            port = None\n\n    elif not host and url:\n        host = url\n\n    if not path:\n        return Url(scheme, auth, host, port, path, query, fragment)\n\n    # Fragment\n    if '#' in path:\n        path, fragment = path.split('#', 1)\n\n    # Query\n    if '?' in path:\n        path, query = path.split('?', 1)\n\n    return Url(scheme, auth, host, port, path, query, fragment)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the host from a URL.", "response": "def get_host(url):\n    \"\"\"\n    Deprecated. Use :func:`parse_url` instead.\n    \"\"\"\n    p = parse_url(url)\n    return p.scheme or 'http', p.hostname, p.port"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request_uri(self):\n        uri = self.path or '/'\n\n        if self.query is not None:\n            uri += '?' + self.query\n\n        return uri", "response": "Absolute path including the query string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nnetworking location including host and port", "response": "def netloc(self):\n        \"\"\"Network location including host and port\"\"\"\n        if self.port:\n            return '%s:%d' % (self.host, self.port)\n        return self.host"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting self into a url", "response": "def url(self):\n        \"\"\"\n        Convert self into a url\n\n        This function should more or less round-trip with :func:`.parse_url`. The\n        returned url may not be exactly the same as the url inputted to\n        :func:`.parse_url`, but it should be equivalent by the RFC (e.g., urls\n        with a blank port will have : removed).\n\n        Example: ::\n\n            >>> U = parse_url('http://google.com/mail/')\n            >>> U.url\n            'http://google.com/mail/'\n            >>> Url('http', 'username:password', 'host.com', 80,\n            ... '/path', 'query', 'fragment').url\n            'http://username:password@host.com:80/path?query#fragment'\n        \"\"\"\n        scheme, auth, host, port, path, query, fragment = self\n        url = ''\n\n        # We use \"is not None\" we want things to happen with empty strings (or 0 port)\n        if scheme is not None:\n            url += scheme + '://'\n        if auth is not None:\n            url += auth + '@'\n        if host is not None:\n            url += host\n        if port is not None:\n            url += ':' + str(port)\n        if path is not None:\n            url += path\n        if query is not None:\n            url += '?' + query\n        if fragment is not None:\n            url += '#' + fragment\n\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_template_path(template):\n    pieces = []\n    for piece in template.split('/'):\n        if path.sep in piece \\\n           or (path.altsep and path.altsep in piece) or \\\n           piece == path.pardir:\n            raise TemplateNotFound(template)\n        elif piece and piece != '.':\n            pieces.append(piece)\n    return pieces", "response": "Split a path into segments and perform a sanity check."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_source(self, environment, template):\n        if not self.has_source_access:\n            raise RuntimeError('%s cannot provide access to the source' %\n                               self.__class__.__name__)\n        raise TemplateNotFound(template)", "response": "Get the source of a template and reload the template if it has changed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a template from the bytecode cache.", "response": "def load(self, environment, name, globals=None):\n        \"\"\"Loads a template.  This method looks up the template in the cache\n        or loads one by calling :meth:`get_source`.  Subclasses should not\n        override this method as loaders working on collections of other\n        loaders (such as :class:`PrefixLoader` or :class:`ChoiceLoader`)\n        will not call this method but `get_source` directly.\n        \"\"\"\n        code = None\n        if globals is None:\n            globals = {}\n\n        # first we try to get the source for this template together\n        # with the filename and the uptodate function.\n        source, filename, uptodate = self.get_source(environment, name)\n\n        # try to load the code from the bytecode cache if there is a\n        # bytecode cache configured.\n        bcc = environment.bytecode_cache\n        if bcc is not None:\n            bucket = bcc.get_bucket(environment, name, filename, source)\n            code = bucket.code\n\n        # if we don't have code so far (not cached, no longer up to\n        # date) etc. we compile the template\n        if code is None:\n            code = environment.compile(source, name, filename)\n\n        # if the bytecode cache is available and the bucket doesn't\n        # have a code so far, we give the bucket the new code and put\n        # it back to the bytecode cache.\n        if bcc is not None and bucket.code is None:\n            bucket.code = code\n            bcc.set_bucket(bucket)\n\n        return environment.template_class.from_code(environment, code,\n                                                    globals, uptodate)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string describing the probable encoding of a file or a list of strings.", "response": "def description_of(lines, name='stdin'):\n    \"\"\"\n    Return a string describing the probable encoding of a file or\n    list of strings.\n\n    :param lines: The lines to get the encoding of.\n    :type lines: Iterable of bytes\n    :param name: Name of file or collection of lines\n    :type name: str\n    \"\"\"\n    u = UniversalDetector()\n    for line in lines:\n        line = bytearray(line)\n        u.feed(line)\n        # shortcut out of the loop to save reading further - particularly useful if we read a BOM.\n        if u.done:\n            break\n    u.close()\n    result = u.result\n    if PY2:\n        name = name.decode(sys.getfilesystemencoding(), 'ignore')\n    if result['encoding']:\n        return '{0}: {1} with confidence {2}'.format(name, result['encoding'],\n                                                     result['confidence'])\n    else:\n        return '{0}: no result'.format(name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_genshi(walker):\n    text = []\n    for token in walker:\n        type = token[\"type\"]\n        if type in (\"Characters\", \"SpaceCharacters\"):\n            text.append(token[\"data\"])\n        elif text:\n            yield TEXT, \"\".join(text), (None, -1, -1)\n            text = []\n\n        if type in (\"StartTag\", \"EmptyTag\"):\n            if token[\"namespace\"]:\n                name = \"{%s}%s\" % (token[\"namespace\"], token[\"name\"])\n            else:\n                name = token[\"name\"]\n            attrs = Attrs([(QName(\"{%s}%s\" % attr if attr[0] is not None else attr[1]), value)\n                           for attr, value in token[\"data\"].items()])\n            yield (START, (QName(name), attrs), (None, -1, -1))\n            if type == \"EmptyTag\":\n                type = \"EndTag\"\n\n        if type == \"EndTag\":\n            if token[\"namespace\"]:\n                name = \"{%s}%s\" % (token[\"namespace\"], token[\"name\"])\n            else:\n                name = token[\"name\"]\n\n            yield END, QName(name), (None, -1, -1)\n\n        elif type == \"Comment\":\n            yield COMMENT, token[\"data\"], (None, -1, -1)\n\n        elif type == \"Doctype\":\n            yield DOCTYPE, (token[\"name\"], token[\"publicId\"],\n                            token[\"systemId\"]), (None, -1, -1)\n\n        else:\n            pass  # FIXME: What to do?\n\n    if text:\n        yield TEXT, \"\".join(text), (None, -1, -1)", "response": "Convert a tree to a genshi tree"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a requirements file.", "response": "def parse_requirements(file_):\n    \"\"\"Parse a requirements formatted file.\n\n    Traverse a string until a delimiter is detected, then split at said\n    delimiter, get module name by element index, create a dict consisting of\n    module:version, and add dict to list of parsed modules.\n\n    Args:\n        file_: File to parse.\n\n    Raises:\n        OSerror: If there's any issues accessing the file.\n\n    Returns:\n        tuple: The contents of the file, excluding comments.\n    \"\"\"\n    modules = []\n    delim = [\"<\", \">\", \"=\", \"!\", \"~\"]  # https://www.python.org/dev/peps/pep-0508/#complete-grammar\n\n    try:\n        f = open_func(file_, \"r\")\n    except OSError:\n        logging.error(\"Failed on file: {}\".format(file_))\n        raise\n    else:\n        data = [x.strip() for x in f.readlines() if x != \"\\n\"]\n    finally:\n        f.close()\n\n    data = [x for x in data if x[0].isalpha()]\n\n    for x in data:\n        if not any([y in x for y in delim]):  # Check for modules w/o a specifier.\n            modules.append({\"name\": x, \"version\": None})\n        for y in x:\n            if y in delim:\n                module = x.split(y)\n                module_name = module[0]\n                module_version = module[-1].replace(\"=\", \"\")\n                module = {\"name\": module_name, \"version\": module_version}\n\n                if module not in modules:\n                    modules.append(module)\n\n                break\n\n    return modules"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomparing modules in a file to imported modules in a project.", "response": "def compare_modules(file_, imports):\n    \"\"\"Compare modules in a file to imported modules in a project.\n\n    Args:\n        file_ (str): File to parse for modules to be compared.\n        imports (tuple): Modules being imported in the project.\n\n    Returns:\n        tuple: The modules not imported in the project, but do exist in the\n               specified file.\n    \"\"\"\n    modules = parse_requirements(file_)\n\n    imports = [imports[i][\"name\"] for i in range(len(imports))]\n    modules = [modules[i][\"name\"] for i in range(len(modules))]\n    modules_not_imported = set(modules) - set(imports)\n\n    return modules_not_imported"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef diff(file_, imports):\n    modules_not_imported = compare_modules(file_, imports)\n\n    logging.info(\"The following modules are in {} but do not seem to be imported: \"\n                 \"{}\".format(file_, \", \".join(x for x in modules_not_imported)))", "response": "Display the difference between modules in a file and imported modules."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean(file_, imports):\n    modules_not_imported = compare_modules(file_, imports)\n    re_remove = re.compile(\"|\".join(modules_not_imported))\n    to_write = []\n\n    try:\n        f = open_func(file_, \"r+\")\n    except OSError:\n        logging.error(\"Failed on file: {}\".format(file_))\n        raise\n    else:\n        for i in f.readlines():\n            if re_remove.match(i) is None:\n                to_write.append(i)\n        f.seek(0)\n        f.truncate()\n\n        for i in to_write:\n            f.write(i)\n    finally:\n        f.close()\n\n    logging.info(\"Successfully cleaned up requirements in \" + file_)", "response": "Remove modules that aren t imported in project from file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading requirements from a file like object and yields a generator of RequirementFile objects.", "response": "def read_requirements(fh, resolve=False):\n    \"\"\"\n    Reads requirements from a file like object and (optionally) from referenced files.\n    :param fh: file like object to read from\n    :param resolve: boolean. resolves referenced files.\n    :return: generator\n    \"\"\"\n    is_temp_file = not hasattr(fh, 'name')\n    for num, line in enumerate(iter_lines(fh)):\n        line = line.strip()\n        if not line:\n            # skip empty lines\n            continue\n        if line.startswith('#') or \\\n            line.startswith('-i') or \\\n            line.startswith('--index-url') or \\\n            line.startswith('--extra-index-url') or \\\n            line.startswith('-f') or line.startswith('--find-links') or \\\n            line.startswith('--no-index') or line.startswith('--allow-external') or \\\n            line.startswith('--allow-unverified') or line.startswith('-Z') or \\\n            line.startswith('--always-unzip'):\n            # skip unsupported lines\n            continue\n        elif line.startswith('-r') or line.startswith('--requirement'):\n            # got a referenced file here, try to resolve the path\n            # if this is a tempfile, skip\n            if is_temp_file:\n                continue\n            filename = line.strip(\"-r \").strip(\"--requirement\").strip()\n            # if there is a comment, remove it\n            if \" #\" in filename:\n                filename = filename.split(\" #\")[0].strip()\n            req_file_path = os.path.join(os.path.dirname(fh.name), filename)\n            if resolve:\n                # recursively yield the resolved requirements\n                if os.path.exists(req_file_path):\n                    with open(req_file_path) as _fh:\n                        for req in read_requirements(_fh, resolve=True):\n                            yield req\n            else:\n                yield RequirementFile(path=req_file_path)\n        else:\n            try:\n                parseable_line = line\n                # multiline requirements are not parseable\n                if \"\\\\\" in line:\n                    parseable_line = line.replace(\"\\\\\", \"\")\n                    for next_line in iter_lines(fh, num + 1):\n                        parseable_line += next_line.strip().replace(\"\\\\\", \"\")\n                        line += \"\\n\" + next_line\n                        if \"\\\\\" in next_line:\n                            continue\n                        break\n                req, = parse_line(parseable_line)\n                if len(req.specifier._specs) == 1 and \\\n                        next(iter(req.specifier._specs))._spec[0] == \"==\":\n                    yield Package(key=req.name, version=next(iter(req.specifier._specs))._spec[1])\n                else:\n                    try:\n                        fname = fh.name\n                    except AttributeError:\n                        fname = line\n\n                    click.secho(\n                        \"Warning: unpinned requirement '{req}' found in {fname}, \"\n                        \"unable to check.\".format(req=req.name,\n                                                  fname=fname),\n                        fg=\"yellow\",\n                        file=sys.stderr\n                    )\n            except ValueError:\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ipaddress_match(ipname, host_ip):\n    # OpenSSL may add a trailing newline to a subjectAltName's IP address\n    # Divergence from upstream: ipaddress can't handle byte str\n    ip = ipaddress.ip_address(_to_unicode(ipname).rstrip())\n    return ip == host_ip", "response": "Exact match of IP addresses."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_metadata(candidates, traces, dependencies, pythons):\n    metasets_mapping = _calculate_metasets_mapping(\n        dependencies, pythons, copy.deepcopy(traces),\n    )\n    for key, candidate in candidates.items():\n        candidate.markers = _format_metasets(metasets_mapping[key])", "response": "Add metadata to a set of candidates based on the dependency tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _walk(top, topdown=True, onerror=None, followlinks=False):\n    dirs = []\n    nondirs = []\n\n    # We may not have read permission for top, in which case we can't\n    # get a list of the files the directory contains.  os.walk\n    # always suppressed the exception then, rather than blow up for a\n    # minor reason when (say) a thousand readable directories are still\n    # left to visit.  That logic is copied here.\n    try:\n        scandir_it = scandir(top)\n    except OSError as error:\n        if onerror is not None:\n            onerror(error)\n        return\n\n    while True:\n        try:\n            try:\n                entry = next(scandir_it)\n            except StopIteration:\n                break\n        except OSError as error:\n            if onerror is not None:\n                onerror(error)\n            return\n\n        try:\n            is_dir = entry.is_dir()\n        except OSError:\n            # If is_dir() raises an OSError, consider that the entry is not\n            # a directory, same behaviour than os.path.isdir().\n            is_dir = False\n\n        if is_dir:\n            dirs.append(entry.name)\n        else:\n            nondirs.append(entry.name)\n\n        if not topdown and is_dir:\n            # Bottom-up: recurse into sub-directory, but exclude symlinks to\n            # directories if followlinks is False\n            if followlinks:\n                walk_into = True\n            else:\n                try:\n                    is_symlink = entry.is_symlink()\n                except OSError:\n                    # If is_symlink() raises an OSError, consider that the\n                    # entry is not a symbolic link, same behaviour than\n                    # os.path.islink().\n                    is_symlink = False\n                walk_into = not is_symlink\n\n            if walk_into:\n                for entry in walk(entry.path, topdown, onerror, followlinks):\n                    yield entry\n\n    # Yield before recursion if going top down\n    if topdown:\n        yield top, dirs, nondirs\n\n        # Recurse into sub-directories\n        for name in dirs:\n            new_path = join(top, name)\n            # Issue #23605: os.path.islink() is used instead of caching\n            # entry.is_symlink() result during the loop on os.scandir() because\n            # the caller can replace the directory entry during the \"yield\"\n            # above.\n            if followlinks or not islink(new_path):\n                for entry in walk(new_path, topdown, onerror, followlinks):\n                    yield entry\n    else:\n        # Yield after recursion if going bottom up\n        yield top, dirs, nondirs", "response": "A recursive version of os. walk that returns a list of all the files in the directory top."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request_encode_url(self, method, url, fields=None, headers=None,\n                           **urlopen_kw):\n        \"\"\"\n        Make a request using :meth:`urlopen` with the ``fields`` encoded in\n        the url. This is useful for request methods like GET, HEAD, DELETE, etc.\n        \"\"\"\n        if headers is None:\n            headers = self.headers\n\n        extra_kw = {'headers': headers}\n        extra_kw.update(urlopen_kw)\n\n        if fields:\n            url += '?' + urlencode(fields)\n\n        return self.urlopen(method, url, **extra_kw)", "response": "Make a request using the fields encoded in\nialog the url."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a request using :meth:`urlopen` with the ``fields`` encoded in the body. This is useful for request methods like POST, PUT, PATCH, etc. When ``encode_multipart=True`` (default), then :meth:`urllib3.filepost.encode_multipart_formdata` is used to encode the payload with the appropriate content type. Otherwise :meth:`urllib.urlencode` is used with the 'application/x-www-form-urlencoded' content type. Multipart encoding must be used when posting files, and it's reasonably safe to use it in other times too. However, it may break request signing, such as with OAuth. Supports an optional ``fields`` parameter of key/value strings AND key/filetuple. A filetuple is a (filename, data, MIME type) tuple where the MIME type is optional. For example:: fields = { 'foo': 'bar', 'fakefile': ('foofile.txt', 'contents of foofile'), 'realfile': ('barfile.txt', open('realfile').read()), 'typedfile': ('bazfile.bin', open('bazfile').read(), 'image/jpeg'), 'nonamefile': 'contents of nonamefile field', } When uploading a file, providing a filename (the first parameter of the tuple) is optional but recommended to best mimic behavior of browsers. Note that if ``headers`` are supplied, the 'Content-Type' header will be overwritten because it depends on the dynamic random boundary string which is used to compose the body of the request. The random boundary string can be explicitly set with the ``multipart_boundary`` parameter.", "response": "def request_encode_body(self, method, url, fields=None, headers=None,\n                            encode_multipart=True, multipart_boundary=None,\n                            **urlopen_kw):\n        \"\"\"\n        Make a request using :meth:`urlopen` with the ``fields`` encoded in\n        the body. This is useful for request methods like POST, PUT, PATCH, etc.\n\n        When ``encode_multipart=True`` (default), then\n        :meth:`urllib3.filepost.encode_multipart_formdata` is used to encode\n        the payload with the appropriate content type. Otherwise\n        :meth:`urllib.urlencode` is used with the\n        'application/x-www-form-urlencoded' content type.\n\n        Multipart encoding must be used when posting files, and it's reasonably\n        safe to use it in other times too. However, it may break request\n        signing, such as with OAuth.\n\n        Supports an optional ``fields`` parameter of key/value strings AND\n        key/filetuple. A filetuple is a (filename, data, MIME type) tuple where\n        the MIME type is optional. For example::\n\n            fields = {\n                'foo': 'bar',\n                'fakefile': ('foofile.txt', 'contents of foofile'),\n                'realfile': ('barfile.txt', open('realfile').read()),\n                'typedfile': ('bazfile.bin', open('bazfile').read(),\n                              'image/jpeg'),\n                'nonamefile': 'contents of nonamefile field',\n            }\n\n        When uploading a file, providing a filename (the first parameter of the\n        tuple) is optional but recommended to best mimic behavior of browsers.\n\n        Note that if ``headers`` are supplied, the 'Content-Type' header will\n        be overwritten because it depends on the dynamic random boundary string\n        which is used to compose the body of the request. The random boundary\n        string can be explicitly set with the ``multipart_boundary`` parameter.\n        \"\"\"\n        if headers is None:\n            headers = self.headers\n\n        extra_kw = {'headers': {}}\n\n        if fields:\n            if 'body' in urlopen_kw:\n                raise TypeError(\n                    \"request got values for both 'fields' and 'body', can only specify one.\")\n\n            if encode_multipart:\n                body, content_type = encode_multipart_formdata(fields, boundary=multipart_boundary)\n            else:\n                body, content_type = urlencode(fields), 'application/x-www-form-urlencoded'\n\n            extra_kw['body'] = body\n            extra_kw['headers'] = {'Content-Type': content_type}\n\n        extra_kw['headers'].update(headers)\n        extra_kw.update(urlopen_kw)\n\n        return self.urlopen(method, url, **extra_kw)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pass_context(f):\n    def new_func(*args, **kwargs):\n        return f(get_current_context(), *args, **kwargs)\n    return update_wrapper(new_func, f)", "response": "Marks a callback as wanting to receive the current context as first argument."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pass_obj(f):\n    def new_func(*args, **kwargs):\n        return f(get_current_context().obj, *args, **kwargs)\n    return update_wrapper(new_func, f)", "response": "Decorator to pass the object on the object\n    context onwards."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive an object type this creates a decorator that will work similar to :func:`pass_obj` but instead of passing the object of the current context, it will find the innermost context of type :func:`object_type`. This generates a decorator that works roughly like this:: from functools import update_wrapper def decorator(f): @pass_context def new_func(ctx, *args, **kwargs): obj = ctx.find_object(object_type) return ctx.invoke(f, obj, *args, **kwargs) return update_wrapper(new_func, f) return decorator :param object_type: the type of the object to pass. :param ensure: if set to `True`, a new object will be created and remembered on the context if it's not there yet.", "response": "def make_pass_decorator(object_type, ensure=False):\n    \"\"\"Given an object type this creates a decorator that will work\n    similar to :func:`pass_obj` but instead of passing the object of the\n    current context, it will find the innermost context of type\n    :func:`object_type`.\n\n    This generates a decorator that works roughly like this::\n\n        from functools import update_wrapper\n\n        def decorator(f):\n            @pass_context\n            def new_func(ctx, *args, **kwargs):\n                obj = ctx.find_object(object_type)\n                return ctx.invoke(f, obj, *args, **kwargs)\n            return update_wrapper(new_func, f)\n        return decorator\n\n    :param object_type: the type of the object to pass.\n    :param ensure: if set to `True`, a new object will be created and\n                   remembered on the context if it's not there yet.\n    \"\"\"\n    def decorator(f):\n        def new_func(*args, **kwargs):\n            ctx = get_current_context()\n            if ensure:\n                obj = ctx.ensure_object(object_type)\n            else:\n                obj = ctx.find_object(object_type)\n            if obj is None:\n                raise RuntimeError('Managed to invoke callback without a '\n                                   'context object of type %r existing'\n                                   % object_type.__name__)\n            return ctx.invoke(f, obj, *args, **kwargs)\n        return update_wrapper(new_func, f)\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattaching an argument to the command. All positional arguments are passed as parameter declarations to :class:`Argument`; all keyword arguments are forwarded unchanged (except ``cls``). This is equivalent to creating an :class:`Argument` instance manually and attaching it to the :attr:`Command.params` list. :param cls: the argument class to instantiate. This defaults to :class:`Argument`.", "response": "def argument(*param_decls, **attrs):\n    \"\"\"Attaches an argument to the command.  All positional arguments are\n    passed as parameter declarations to :class:`Argument`; all keyword\n    arguments are forwarded unchanged (except ``cls``).\n    This is equivalent to creating an :class:`Argument` instance manually\n    and attaching it to the :attr:`Command.params` list.\n\n    :param cls: the argument class to instantiate.  This defaults to\n                :class:`Argument`.\n    \"\"\"\n    def decorator(f):\n        ArgumentClass = attrs.pop('cls', Argument)\n        _param_memo(f, ArgumentClass(param_decls, **attrs))\n        return f\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef option(*param_decls, **attrs):\n    def decorator(f):\n        # Issue 926, copy attrs, so pre-defined options can re-use the same cls=\n        option_attrs = attrs.copy()\n\n        if 'help' in option_attrs:\n            option_attrs['help'] = inspect.cleandoc(option_attrs['help'])\n        OptionClass = option_attrs.pop('cls', Option)\n        _param_memo(f, OptionClass(param_decls, **option_attrs))\n        return f\n    return decorator", "response": "Decorator to add an option to the command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef password_option(*param_decls, **attrs):\n    def decorator(f):\n        attrs.setdefault('prompt', True)\n        attrs.setdefault('confirmation_prompt', True)\n        attrs.setdefault('hide_input', True)\n        return option(*(param_decls or ('--password',)), **attrs)(f)\n    return decorator", "response": "Shortcut for password prompts.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef help_option(*param_decls, **attrs):\n    def decorator(f):\n        def callback(ctx, param, value):\n            if value and not ctx.resilient_parsing:\n                echo(ctx.get_help(), color=ctx.color)\n                ctx.exit()\n        attrs.setdefault('is_flag', True)\n        attrs.setdefault('expose_value', False)\n        attrs.setdefault('help', 'Show this message and exit.')\n        attrs.setdefault('is_eager', True)\n        attrs['callback'] = callback\n        return option(*(param_decls or ('--help',)), **attrs)(f)\n    return decorator", "response": "Decorator to add a help option to all commands."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef was_installed_by_pip(pkg):\n    # type: (str) -> bool\n    \"\"\"Checks whether pkg was installed by pip\n\n    This is used not to display the upgrade message when pip is in fact\n    installed by system package manager, such as dnf on Fedora.\n    \"\"\"\n    try:\n        dist = pkg_resources.get_distribution(pkg)\n        return (dist.has_metadata('INSTALLER') and\n                'pip' in dist.get_metadata_lines('INSTALLER'))\n    except pkg_resources.DistributionNotFound:\n        return False", "response": "Checks whether a given pip package was installed by pip\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pip_version_check(session, options):\n    # type: (PipSession, optparse.Values) -> None\n    \"\"\"Check for an update for pip.\n\n    Limit the frequency of checks to once per week. State is stored either in\n    the active virtualenv or in the user's USER_CACHE_DIR keyed off the prefix\n    of the pip script path.\n    \"\"\"\n    installed_version = get_installed_version(\"pip\")\n    if not installed_version:\n        return\n\n    pip_version = packaging_version.parse(installed_version)\n    pypi_version = None\n\n    try:\n        state = SelfCheckState(cache_dir=options.cache_dir)\n\n        current_time = datetime.datetime.utcnow()\n        # Determine if we need to refresh the state\n        if \"last_check\" in state.state and \"pypi_version\" in state.state:\n            last_check = datetime.datetime.strptime(\n                state.state[\"last_check\"],\n                SELFCHECK_DATE_FMT\n            )\n            if (current_time - last_check).total_seconds() < 7 * 24 * 60 * 60:\n                pypi_version = state.state[\"pypi_version\"]\n\n        # Refresh the version if we need to or just see if we need to warn\n        if pypi_version is None:\n            # Lets use PackageFinder to see what the latest pip version is\n            finder = PackageFinder(\n                find_links=options.find_links,\n                index_urls=[options.index_url] + options.extra_index_urls,\n                allow_all_prereleases=False,  # Explicitly set to False\n                trusted_hosts=options.trusted_hosts,\n                session=session,\n            )\n            all_candidates = finder.find_all_candidates(\"pip\")\n            if not all_candidates:\n                return\n            pypi_version = str(\n                max(all_candidates, key=lambda c: c.version).version\n            )\n\n            # save that we've performed a check\n            state.save(pypi_version, current_time)\n\n        remote_version = packaging_version.parse(pypi_version)\n\n        # Determine if our pypi_version is older\n        if (pip_version < remote_version and\n                pip_version.base_version != remote_version.base_version and\n                was_installed_by_pip('pip')):\n            # Advise \"python -m pip\" on Windows to avoid issues\n            # with overwriting pip.exe.\n            if WINDOWS:\n                pip_cmd = \"python -m pip\"\n            else:\n                pip_cmd = \"pip\"\n            logger.warning(\n                \"You are using pip version %s, however version %s is \"\n                \"available.\\nYou should consider upgrading via the \"\n                \"'%s install --upgrade pip' command.\",\n                pip_version, pypi_version, pip_cmd\n            )\n    except Exception:\n        logger.debug(\n            \"There was an error checking the latest version of pip\",\n            exc_info=True,\n        )", "response": "Checks if pip has installed a version of the current version and updates the state if it has changed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning abbreviated implementation name.", "response": "def get_abbr_impl():\n    # type: () -> str\n    \"\"\"Return abbreviated implementation name.\"\"\"\n    if hasattr(sys, 'pypy_version_info'):\n        pyimpl = 'pp'\n    elif sys.platform.startswith('java'):\n        pyimpl = 'jy'\n    elif sys.platform == 'cli':\n        pyimpl = 'ip'\n    else:\n        pyimpl = 'cp'\n    return pyimpl"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_impl_version_info():\n    # type: () -> Tuple[int, ...]\n    \"\"\"Return sys.version_info-like tuple for use in decrementing the minor\n    version.\"\"\"\n    if get_abbr_impl() == 'pp':\n        # as per https://github.com/pypa/pip/issues/2882\n        # attrs exist only on pypy\n        return (sys.version_info[0],\n                sys.pypy_version_info.major,  # type: ignore\n                sys.pypy_version_info.minor)  # type: ignore\n    else:\n        return sys.version_info[0], sys.version_info[1]", "response": "Return sys. version_info - like tuple for use in decrementing the minor\n    version."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_platform():\n    # type: () -> str\n    \"\"\"Return our platform name 'win32', 'linux_x86_64'\"\"\"\n    if sys.platform == 'darwin':\n        # distutils.util.get_platform() returns the release based on the value\n        # of MACOSX_DEPLOYMENT_TARGET on which Python was built, which may\n        # be significantly older than the user's current machine.\n        release, _, machine = platform.mac_ver()\n        split_ver = release.split('.')\n\n        if machine == \"x86_64\" and _is_running_32bit():\n            machine = \"i386\"\n        elif machine == \"ppc64\" and _is_running_32bit():\n            machine = \"ppc\"\n\n        return 'macosx_{}_{}_{}'.format(split_ver[0], split_ver[1], machine)\n\n    # XXX remove distutils dependency\n    result = distutils.util.get_platform().replace('.', '_').replace('-', '_')\n    if result == \"linux_x86_64\" and _is_running_32bit():\n        # 32 bit Python program (running on a 64 bit Linux): pip should only\n        # install and run 32 bit compiled extensions in that case.\n        result = \"linux_i686\"\n\n    return result", "response": "Return our platform name win32 linux_x86_64"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_supported(\n    versions=None,  # type: Optional[List[str]]\n    noarch=False,  # type: bool\n    platform=None,  # type: Optional[str]\n    impl=None,  # type: Optional[str]\n    abi=None  # type: Optional[str]\n):\n    # type: (...) -> List[Pep425Tag]\n    \"\"\"Return a list of supported tags for each version specified in\n    `versions`.\n\n    :param versions: a list of string versions, of the form [\"33\", \"32\"],\n        or None. The first version will be assumed to support our ABI.\n    :param platform: specify the exact platform you want valid\n        tags for, or None. If None, use the local system platform.\n    :param impl: specify the exact implementation you want valid\n        tags for, or None. If None, use the local interpreter impl.\n    :param abi: specify the exact abi you want valid\n        tags for, or None. If None, use the local interpreter abi.\n    \"\"\"\n    supported = []\n\n    # Versions must be given with respect to the preference\n    if versions is None:\n        version_info = get_impl_version_info()\n        versions = get_all_minor_versions_as_strings(version_info)\n\n    impl = impl or get_abbr_impl()\n\n    abis = []  # type: List[str]\n\n    abi = abi or get_abi_tag()\n    if abi:\n        abis[0:0] = [abi]\n\n    abi3s = set()\n    for suffix in get_extension_suffixes():\n        if suffix.startswith('.abi'):\n            abi3s.add(suffix.split('.', 2)[1])\n\n    abis.extend(sorted(list(abi3s)))\n\n    abis.append('none')\n\n    if not noarch:\n        arch = platform or get_platform()\n        arch_prefix, arch_sep, arch_suffix = arch.partition('_')\n        if arch.startswith('macosx'):\n            # support macosx-10.6-intel on macosx-10.9-x86_64\n            match = _osx_arch_pat.match(arch)\n            if match:\n                name, major, minor, actual_arch = match.groups()\n                tpl = '{}_{}_%i_%s'.format(name, major)\n                arches = []\n                for m in reversed(range(int(minor) + 1)):\n                    for a in get_darwin_arches(int(major), m, actual_arch):\n                        arches.append(tpl % (m, a))\n            else:\n                # arch pattern didn't match (?!)\n                arches = [arch]\n        elif arch_prefix == 'manylinux2010':\n            # manylinux1 wheels run on most manylinux2010 systems with the\n            # exception of wheels depending on ncurses. PEP 571 states\n            # manylinux1 wheels should be considered manylinux2010 wheels:\n            # https://www.python.org/dev/peps/pep-0571/#backwards-compatibility-with-manylinux1-wheels\n            arches = [arch, 'manylinux1' + arch_sep + arch_suffix]\n        elif platform is None:\n            arches = []\n            if is_manylinux2010_compatible():\n                arches.append('manylinux2010' + arch_sep + arch_suffix)\n            if is_manylinux1_compatible():\n                arches.append('manylinux1' + arch_sep + arch_suffix)\n            arches.append(arch)\n        else:\n            arches = [arch]\n\n        # Current version, current API (built specifically for our Python):\n        for abi in abis:\n            for arch in arches:\n                supported.append(('%s%s' % (impl, versions[0]), abi, arch))\n\n        # abi3 modules compatible with older version of Python\n        for version in versions[1:]:\n            # abi3 was introduced in Python 3.2\n            if version in {'31', '30'}:\n                break\n            for abi in abi3s:   # empty set if not Python 3\n                for arch in arches:\n                    supported.append((\"%s%s\" % (impl, version), abi, arch))\n\n        # Has binaries, does not use the Python API:\n        for arch in arches:\n            supported.append(('py%s' % (versions[0][0]), 'none', arch))\n\n    # No abi / arch, but requires our implementation:\n    supported.append(('%s%s' % (impl, versions[0]), 'none', 'any'))\n    # Tagged specifically as being cross-version compatible\n    # (with just the major version specified)\n    supported.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))\n\n    # No abi / arch, generic Python\n    for i, version in enumerate(versions):\n        supported.append(('py%s' % (version,), 'none', 'any'))\n        if i == 0:\n            supported.append(('py%s' % (version[0]), 'none', 'any'))\n\n    return supported", "response": "Returns a list of tags that are supported by the current version of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_netrc_auth(url, raise_errors=False):\n\n    try:\n        from netrc import netrc, NetrcParseError\n\n        netrc_path = None\n\n        for f in NETRC_FILES:\n            try:\n                loc = os.path.expanduser('~/{}'.format(f))\n            except KeyError:\n                # os.path.expanduser can fail when $HOME is undefined and\n                # getpwuid fails. See https://bugs.python.org/issue20164 &\n                # https://github.com/requests/requests/issues/1846\n                return\n\n            if os.path.exists(loc):\n                netrc_path = loc\n                break\n\n        # Abort early if there isn't one.\n        if netrc_path is None:\n            return\n\n        ri = urlparse(url)\n\n        # Strip port numbers from netloc. This weird `if...encode`` dance is\n        # used for Python 3.2, which doesn't support unicode literals.\n        splitstr = b':'\n        if isinstance(url, str):\n            splitstr = splitstr.decode('ascii')\n        host = ri.netloc.split(splitstr)[0]\n\n        try:\n            _netrc = netrc(netrc_path).authenticators(host)\n            if _netrc:\n                # Return with login / password\n                login_i = (0 if _netrc[0] else 1)\n                return (_netrc[login_i], _netrc[2])\n        except (NetrcParseError, IOError):\n            # If there was a parsing error or a permissions issue reading the file,\n            # we'll just skip netrc auth unless explicitly asked to raise errors.\n            if raise_errors:\n                raise\n\n    # AppEngine hackiness.\n    except (ImportError, AttributeError):\n        pass", "response": "Returns the Requests tuple auth for a given url from netrc."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to guess the filename of the given object.", "response": "def guess_filename(obj):\n    \"\"\"Tries to guess the filename of the given object.\"\"\"\n    name = getattr(obj, 'name', None)\n    if (name and isinstance(name, basestring) and name[0] != '<' and\n            name[-1] != '>'):\n        return os.path.basename(name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_zipped_paths(path):\n    if os.path.exists(path):\n        # this is already a valid path, no need to do anything further\n        return path\n\n    # find the first valid part of the provided path and treat that as a zip archive\n    # assume the rest of the path is the name of a member in the archive\n    archive, member = os.path.split(path)\n    while archive and not os.path.exists(archive):\n        archive, prefix = os.path.split(archive)\n        member = '/'.join([prefix, member])\n\n    if not zipfile.is_zipfile(archive):\n        return path\n\n    zip_file = zipfile.ZipFile(archive)\n    if member not in zip_file.namelist():\n        return path\n\n    # we have a valid zip archive and a valid member of that archive\n    tmp = tempfile.gettempdir()\n    extracted_path = os.path.join(tmp, *member.split('/'))\n    if not os.path.exists(extracted_path):\n        extracted_path = zip_file.extract(member, path=tmp)\n\n    return extracted_path", "response": "Replace nonexistent paths that look like they refer to a member of a zip archive with the location of an extracted copy of the target."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake an object and test to see if it can be represented as a dictionary.", "response": "def from_key_val_list(value):\n    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. Unless it can not be represented as such, return an\n    OrderedDict, e.g.,\n\n    ::\n\n        >>> from_key_val_list([('key', 'val')])\n        OrderedDict([('key', 'val')])\n        >>> from_key_val_list('string')\n        ValueError: cannot encode objects that are not 2-tuples\n        >>> from_key_val_list({'key': 'val'})\n        OrderedDict([('key', 'val')])\n\n    :rtype: OrderedDict\n    \"\"\"\n    if value is None:\n        return None\n\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError('cannot encode objects that are not 2-tuples')\n\n    return OrderedDict(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a list of items from a header value.", "response": "def parse_list_header(value):\n    \"\"\"Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Quotes are removed automatically after parsing.\n\n    It basically works like :func:`parse_set_header` just that items\n    may appear multiple times and case sensitivity is preserved.\n\n    The return value is a standard :class:`list`:\n\n    >>> parse_list_header('token, \"quoted value\"')\n    ['token', 'quoted value']\n\n    To create a header from the :class:`list` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a list header.\n    :return: :class:`list`\n    :rtype: list\n    \"\"\"\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a string with a dict header.", "response": "def parse_dict_header(value):\n    \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n    convert them into a python dict:\n\n    >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n    >>> type(d) is dict\n    True\n    >>> sorted(d.items())\n    [('bar', 'as well'), ('foo', 'is a fish')]\n\n    If there is no value for a key it will be `None`:\n\n    >>> parse_dict_header('key_without_value')\n    {'key_without_value': None}\n\n    To create a header from the :class:`dict` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a dict header.\n    :return: :class:`dict`\n    :rtype: dict\n    \"\"\"\n    result = {}\n    for item in _parse_list_header(value):\n        if '=' not in item:\n            result[item] = None\n            continue\n        name, value = item.split('=', 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a key - value dictionary from a CookieJar object.", "response": "def dict_from_cookiejar(cj):\n    \"\"\"Returns a key/value dictionary from a CookieJar.\n\n    :param cj: CookieJar object to extract cookies from.\n    :rtype: dict\n    \"\"\"\n\n    cookie_dict = {}\n\n    for cookie in cj:\n        cookie_dict[cookie.name] = cookie.value\n\n    return cookie_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_content_type_header(header):\n\n    tokens = header.split(';')\n    content_type, params = tokens[0].strip(), tokens[1:]\n    params_dict = {}\n    items_to_strip = \"\\\"' \"\n\n    for param in params:\n        param = param.strip()\n        if param:\n            key, value = param, True\n            index_of_equals = param.find(\"=\")\n            if index_of_equals != -1:\n                key = param[:index_of_equals].strip(items_to_strip)\n                value = param[index_of_equals + 1:].strip(items_to_strip)\n            params_dict[key.lower()] = value\n    return content_type, params_dict", "response": "Returns content type and parameters from given header"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_encoding_from_headers(headers):\n\n    content_type = headers.get('content-type')\n\n    if not content_type:\n        return None\n\n    content_type, params = _parse_content_type_header(content_type)\n\n    if 'charset' in params:\n        return params['charset'].strip(\"'\\\"\")\n\n    if 'text' in content_type:\n        return 'ISO-8859-1'", "response": "Returns the encoding from the given HTTP Header Dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate over slices of a string.", "response": "def iter_slices(string, slice_length):\n    \"\"\"Iterate over slices of a string.\"\"\"\n    pos = 0\n    if slice_length is None or slice_length <= 0:\n        slice_length = len(string)\n    while pos < len(string):\n        yield string[pos:pos + slice_length]\n        pos += slice_length"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_unicode_from_response(r):\n    warnings.warn((\n        'In requests 3.0, get_unicode_from_response will be removed. For '\n        'more information, please see the discussion on issue #2266. (This'\n        ' warning should only appear once.)'),\n        DeprecationWarning)\n\n    tried_encodings = []\n\n    # Try charset from content-type\n    encoding = get_encoding_from_headers(r.headers)\n\n    if encoding:\n        try:\n            return str(r.content, encoding)\n        except UnicodeError:\n            tried_encodings.append(encoding)\n\n    # Fall back:\n    try:\n        return str(r.content, encoding, errors='replace')\n    except TypeError:\n        return r.content", "response": "Returns the requested content back in unicode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef address_in_network(ip, net):\n    ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]\n    netaddr, bits = net.split('/')\n    netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]\n    network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask\n    return (ipaddr & netmask) == (network & netmask)", "response": "This function allows you to check if an IP belongs to a network subnet"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the cidr format is valid.", "response": "def is_valid_cidr(string_network):\n    \"\"\"\n    Very simple check of the cidr format in no_proxy variable.\n\n    :rtype: bool\n    \"\"\"\n    if string_network.count('/') == 1:\n        try:\n            mask = int(string_network.split('/')[1])\n        except ValueError:\n            return False\n\n        if mask < 1 or mask > 32:\n            return False\n\n        try:\n            socket.inet_aton(string_network.split('/')[0])\n        except socket.error:\n            return False\n    else:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the environment variable env_name to value", "response": "def set_environ(env_name, value):\n    \"\"\"Set the environment variable 'env_name' to 'value'\n\n    Save previous value, yield, and then restore the previous value stored in\n    the environment variable 'env_name'.\n\n    If 'value' is None, do nothing\"\"\"\n    value_changed = value is not None\n    if value_changed:\n        old_value = os.environ.get(env_name)\n        os.environ[env_name] = value\n    try:\n        yield\n    finally:\n        if value_changed:\n            if old_value is None:\n                del os.environ[env_name]\n            else:\n                os.environ[env_name] = old_value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef should_bypass_proxies(url, no_proxy):\n    # Prioritize lowercase environment variables over uppercase\n    # to keep a consistent behaviour with other http projects (curl, wget).\n    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())\n\n    # First check whether no_proxy is defined. If it is, check that the URL\n    # we're getting isn't in the no_proxy list.\n    no_proxy_arg = no_proxy\n    if no_proxy is None:\n        no_proxy = get_proxy('no_proxy')\n    parsed = urlparse(url)\n\n    if parsed.hostname is None:\n        # URLs don't always have hostnames, e.g. file:/// urls.\n        return True\n\n    if no_proxy:\n        # We need to check whether we match here. We need to see if we match\n        # the end of the hostname, both with and without the port.\n        no_proxy = (\n            host for host in no_proxy.replace(' ', '').split(',') if host\n        )\n\n        if is_ipv4_address(parsed.hostname):\n            for proxy_ip in no_proxy:\n                if is_valid_cidr(proxy_ip):\n                    if address_in_network(parsed.hostname, proxy_ip):\n                        return True\n                elif parsed.hostname == proxy_ip:\n                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &\n                    # matches the IP of the index\n                    return True\n        else:\n            host_with_port = parsed.hostname\n            if parsed.port:\n                host_with_port += ':{}'.format(parsed.port)\n\n            for host in no_proxy:\n                if parsed.hostname.endswith(host) or host_with_port.endswith(host):\n                    # The URL does match something in no_proxy, so we don't want\n                    # to apply the proxies on this URL.\n                    return True\n\n    with set_environ('no_proxy', no_proxy_arg):\n        # parsed.hostname can be `None` in cases such as a file URI.\n        try:\n            bypass = proxy_bypass(parsed.hostname)\n        except (TypeError, socket.gaierror):\n            bypass = False\n\n    if bypass:\n        return True\n\n    return False", "response": "Returns whether we should bypass proxies on a URL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef select_proxy(url, proxies):\n    proxies = proxies or {}\n    urlparts = urlparse(url)\n    if urlparts.hostname is None:\n        return proxies.get(urlparts.scheme, proxies.get('all'))\n\n    proxy_keys = [\n        urlparts.scheme + '://' + urlparts.hostname,\n        urlparts.scheme,\n        'all://' + urlparts.hostname,\n        'all',\n    ]\n    proxy = None\n    for proxy_key in proxy_keys:\n        if proxy_key in proxies:\n            proxy = proxies[proxy_key]\n            break\n\n    return proxy", "response": "Select a proxy for the url if applicable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a URL that may or may not have a scheme prepends the given scheme.", "response": "def prepend_scheme_if_needed(url, new_scheme):\n    \"\"\"Given a URL that may or may not have a scheme, prepend the given scheme.\n    Does not replace a present scheme with the one provided as an argument.\n\n    :rtype: str\n    \"\"\"\n    scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)\n\n    # urlparse is a finicky beast, and sometimes decides that there isn't a\n    # netloc present. Assume that it's being over-cautious, and switch netloc\n    # and path if urlparse decided there was no netloc.\n    if not netloc:\n        netloc, path = path, netloc\n\n    return urlunparse((scheme, netloc, path, params, query, fragment))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_auth_from_url(url):\n    parsed = urlparse(url)\n\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = ('', '')\n\n    return auth", "response": "Given a url with authentication components extract them into a tuple of username and password."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nverifies that header value is a string which doesn t contain leading whitespace or return characters. This prevents unintended header injection.", "response": "def check_header_validity(header):\n    \"\"\"Verifies that header value is a string which doesn't contain\n    leading whitespace or return characters. This prevents unintended\n    header injection.\n\n    :param header: tuple, in the format (name, value).\n    \"\"\"\n    name, value = header\n\n    if isinstance(value, bytes):\n        pat = _CLEAN_HEADER_REGEX_BYTE\n    else:\n        pat = _CLEAN_HEADER_REGEX_STR\n    try:\n        if not pat.match(value):\n            raise InvalidHeader(\"Invalid return character or leading space in header: %s\" % name)\n    except TypeError:\n        raise InvalidHeader(\"Value for header {%s: %s} must be of type str or \"\n                            \"bytes, not %s\" % (name, value, type(value)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a url remove the fragment and the authentication part.", "response": "def urldefragauth(url):\n    \"\"\"\n    Given a url remove the fragment and the authentication part.\n\n    :rtype: str\n    \"\"\"\n    scheme, netloc, path, params, query, fragment = urlparse(url)\n\n    # see func:`prepend_scheme_if_needed`\n    if not netloc:\n        netloc, path = path, netloc\n\n    netloc = netloc.rsplit('@', 1)[-1]\n\n    return urlunparse((scheme, netloc, path, params, query, ''))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rewind_body(prepared_request):\n    body_seek = getattr(prepared_request.body, 'seek', None)\n    if body_seek is not None and isinstance(prepared_request._body_position, integer_types):\n        try:\n            body_seek(prepared_request._body_position)\n        except (IOError, OSError):\n            raise UnrewindableBodyError(\"An error occurred when rewinding request \"\n                                        \"body for redirect.\")\n    else:\n        raise UnrewindableBodyError(\"Unable to rewind request body for redirect.\")", "response": "Move file pointer back to its recorded starting position\n                                ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a canonical version string.", "response": "def canonicalize_version(version):\n    \"\"\"\n    This is very similar to Version.__str__, but has one subtle differences\n    with the way it handles the release segment.\n    \"\"\"\n\n    try:\n        version = Version(version)\n    except InvalidVersion:\n        # Legacy versions cannot be normalized\n        return version\n\n    parts = []\n\n    # Epoch\n    if version.epoch != 0:\n        parts.append(\"{0}!\".format(version.epoch))\n\n    # Release segment\n    # NB: This strips trailing '.0's to normalize\n    parts.append(re.sub(r\"(\\.0)+$\", \"\", \".\".join(str(x) for x in version.release)))\n\n    # Pre-release\n    if version.pre is not None:\n        parts.append(\"\".join(str(x) for x in version.pre))\n\n    # Post-release\n    if version.post is not None:\n        parts.append(\".post{0}\".format(version.post))\n\n    # Development release\n    if version.dev is not None:\n        parts.append(\".dev{0}\".format(version.dev))\n\n    # Local version segment\n    if version.local is not None:\n        parts.append(\"+{0}\".format(version.local))\n\n    return \"\".join(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the python source for a node tree.", "response": "def generate(node, environment, name, filename, stream=None,\n             defer_init=False, optimized=True):\n    \"\"\"Generate the python source for a node tree.\"\"\"\n    if not isinstance(node, nodes.Template):\n        raise TypeError('Can\\'t compile non template nodes')\n    generator = environment.code_generator_class(environment, name, filename,\n                                                 stream, defer_init,\n                                                 optimized)\n    generator.visit(node)\n    if stream is None:\n        return generator.stream.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_safe_repr(value):\n    if value is None or value is NotImplemented or value is Ellipsis:\n        return True\n    if type(value) in (bool, int, float, complex, range_type, Markup) + string_types:\n        return True\n    if type(value) in (tuple, list, set, frozenset):\n        for item in value:\n            if not has_safe_repr(item):\n                return False\n        return True\n    elif type(value) is dict:\n        for key, value in iteritems(value):\n            if not has_safe_repr(key):\n                return False\n            if not has_safe_repr(value):\n                return False\n        return True\n    return False", "response": "Does the node have a safe representation?"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_undeclared(nodes, names):\n    visitor = UndeclaredNameVisitor(names)\n    try:\n        for node in nodes:\n            visitor.visit(node)\n    except VisitorExit:\n        pass\n    return visitor.undeclared", "response": "Check if the names passed are accessed undeclared."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a copy of the current one.", "response": "def copy(self):\n        \"\"\"Create a copy of the current one.\"\"\"\n        rv = object.__new__(self.__class__)\n        rv.__dict__.update(self.__dict__)\n        rv.symbols = self.symbols.copy()\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an inner frame.", "response": "def inner(self, isolated=False):\n        \"\"\"Return an inner frame.\"\"\"\n        if isolated:\n            return Frame(self.eval_ctx, level=self.symbols.level + 1)\n        return Frame(self.eval_ctx, self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfail with a TemplateAssertionError.", "response": "def fail(self, msg, lineno):\n        \"\"\"Fail with a :exc:`TemplateAssertionError`.\"\"\"\n        raise TemplateAssertionError(msg, lineno, self.name, self.filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef buffer(self, frame):\n        frame.buffer = self.temporary_identifier()\n        self.writeline('%s = []' % frame.buffer)", "response": "Enable buffering for the frame from that point onwards."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_buffer_contents(self, frame, force_unescaped=False):\n        if not force_unescaped:\n            if frame.eval_ctx.volatile:\n                self.writeline('if context.eval_ctx.autoescape:')\n                self.indent()\n                self.writeline('return Markup(concat(%s))' % frame.buffer)\n                self.outdent()\n                self.writeline('else:')\n                self.indent()\n                self.writeline('return concat(%s)' % frame.buffer)\n                self.outdent()\n                return\n            elif frame.eval_ctx.autoescape:\n                self.writeline('return Markup(concat(%s))' % frame.buffer)\n                return\n        self.writeline('return concat(%s)' % frame.buffer)", "response": "Return the buffer contents of the frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start_write(self, frame, node=None):\n        if frame.buffer is None:\n            self.writeline('yield ', node)\n        else:\n            self.writeline('%s.append(' % frame.buffer, node)", "response": "Yield or write into the frame buffer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simple_write(self, s, frame, node=None):\n        self.start_write(frame, node)\n        self.write(s)\n        self.end_write(frame)", "response": "Simple shortcut for start_write + write + end_write."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, x):\n        if self._new_lines:\n            if not self._first_write:\n                self.stream.write('\\n' * self._new_lines)\n                self.code_lineno += self._new_lines\n                if self._write_debug_info is not None:\n                    self.debug_info.append((self._write_debug_info,\n                                            self.code_lineno))\n                    self._write_debug_info = None\n            self._first_write = False\n            self.stream.write('    ' * self._indentation)\n            self._new_lines = 0\n        self.stream.write(x)", "response": "Write a string into the output stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef writeline(self, x, node=None, extra=0):\n        self.newline(node, extra)\n        self.write(x)", "response": "Combination of newline and write."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef newline(self, node=None, extra=0):\n        self._new_lines = max(self._new_lines, 1 + extra)\n        if node is not None and node.lineno != self._last_line:\n            self._write_debug_info = node.lineno\n            self._last_line = node.lineno", "response": "Add one or more newlines before the next write."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef signature(self, node, frame, extra_kwargs=None):\n        # if any of the given keyword arguments is a python keyword\n        # we have to make sure that no invalid call is created.\n        kwarg_workaround = False\n        for kwarg in chain((x.key for x in node.kwargs), extra_kwargs or ()):\n            if is_python_keyword(kwarg):\n                kwarg_workaround = True\n                break\n\n        for arg in node.args:\n            self.write(', ')\n            self.visit(arg, frame)\n\n        if not kwarg_workaround:\n            for kwarg in node.kwargs:\n                self.write(', ')\n                self.visit(kwarg, frame)\n            if extra_kwargs is not None:\n                for key, value in iteritems(extra_kwargs):\n                    self.write(', %s=%s' % (key, value))\n        if node.dyn_args:\n            self.write(', *')\n            self.visit(node.dyn_args, frame)\n\n        if kwarg_workaround:\n            if node.dyn_kwargs is not None:\n                self.write(', **dict({')\n            else:\n                self.write(', **{')\n            for kwarg in node.kwargs:\n                self.write('%r: ' % kwarg.key)\n                self.visit(kwarg.value, frame)\n                self.write(', ')\n            if extra_kwargs is not None:\n                for key, value in iteritems(extra_kwargs):\n                    self.write('%r: %s, ' % (key, value))\n            if node.dyn_kwargs is not None:\n                self.write('}, **')\n                self.visit(node.dyn_kwargs, frame)\n                self.write(')')\n            else:\n                self.write('}')\n\n        elif node.dyn_kwargs is not None:\n            self.write(', **')\n            self.visit(node.dyn_kwargs, frame)", "response": "Writes a function call to the stream for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npull all the dependencies.", "response": "def pull_dependencies(self, nodes):\n        \"\"\"Pull all the dependencies.\"\"\"\n        visitor = DependencyFinderVisitor()\n        for node in nodes:\n            visitor.visit(node)\n        for dependency in 'filters', 'tests':\n            mapping = getattr(self, dependency)\n            for name in getattr(visitor, dependency):\n                if name not in mapping:\n                    mapping[name] = self.temporary_identifier()\n                self.writeline('%s = environment.%s[%r]' %\n                               (mapping[name], dependency, name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndump the function def of a macro or call block.", "response": "def macro_body(self, node, frame):\n        \"\"\"Dump the function def of a macro or call block.\"\"\"\n        frame = frame.inner()\n        frame.symbols.analyze_node(node)\n        macro_ref = MacroRef(node)\n\n        explicit_caller = None\n        skip_special_params = set()\n        args = []\n        for idx, arg in enumerate(node.args):\n            if arg.name == 'caller':\n                explicit_caller = idx\n            if arg.name in ('kwargs', 'varargs'):\n                skip_special_params.add(arg.name)\n            args.append(frame.symbols.ref(arg.name))\n\n        undeclared = find_undeclared(node.body, ('caller', 'kwargs', 'varargs'))\n\n        if 'caller' in undeclared:\n            # In older Jinja2 versions there was a bug that allowed caller\n            # to retain the special behavior even if it was mentioned in\n            # the argument list.  However thankfully this was only really\n            # working if it was the last argument.  So we are explicitly\n            # checking this now and error out if it is anywhere else in\n            # the argument list.\n            if explicit_caller is not None:\n                try:\n                    node.defaults[explicit_caller - len(node.args)]\n                except IndexError:\n                    self.fail('When defining macros or call blocks the '\n                              'special \"caller\" argument must be omitted '\n                              'or be given a default.', node.lineno)\n            else:\n                args.append(frame.symbols.declare_parameter('caller'))\n            macro_ref.accesses_caller = True\n        if 'kwargs' in undeclared and not 'kwargs' in skip_special_params:\n            args.append(frame.symbols.declare_parameter('kwargs'))\n            macro_ref.accesses_kwargs = True\n        if 'varargs' in undeclared and not 'varargs' in skip_special_params:\n            args.append(frame.symbols.declare_parameter('varargs'))\n            macro_ref.accesses_varargs = True\n\n        # macros are delayed, they never require output checks\n        frame.require_output_check = False\n        frame.symbols.analyze_node(node)\n        self.writeline('%s(%s):' % (self.func('macro'), ', '.join(args)), node)\n        self.indent()\n\n        self.buffer(frame)\n        self.enter_frame(frame)\n\n        self.push_parameter_definitions(frame)\n        for idx, arg in enumerate(node.args):\n            ref = frame.symbols.ref(arg.name)\n            self.writeline('if %s is missing:' % ref)\n            self.indent()\n            try:\n                default = node.defaults[idx - len(node.args)]\n            except IndexError:\n                self.writeline('%s = undefined(%r, name=%r)' % (\n                    ref,\n                    'parameter %r was not provided' % arg.name,\n                    arg.name))\n            else:\n                self.writeline('%s = ' % ref)\n                self.visit(default, frame)\n            self.mark_parameter_stored(ref)\n            self.outdent()\n        self.pop_parameter_definitions()\n\n        self.blockvisit(node.body, frame)\n        self.return_buffer_contents(frame, force_unescaped=True)\n        self.leave_frame(frame, with_python_scope=True)\n        self.outdent()\n\n        return frame, macro_ref"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef macro_def(self, macro_ref, frame):\n        arg_tuple = ', '.join(repr(x.name) for x in macro_ref.node.args)\n        name = getattr(macro_ref.node, 'name', None)\n        if len(macro_ref.node.args) == 1:\n            arg_tuple += ','\n        self.write('Macro(environment, macro, %r, (%s), %r, %r, %r, '\n                   'context.eval_ctx.autoescape)' %\n                   (name, arg_tuple, macro_ref.accesses_kwargs,\n                    macro_ref.accesses_varargs, macro_ref.accesses_caller))", "response": "Dump the macro definition for the def created by macro_body."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a human readable position for the node.", "response": "def position(self, node):\n        \"\"\"Return a human readable position for the node.\"\"\"\n        rv = 'line %d' % node.lineno\n        if self.name is not None:\n            rv += ' in ' + repr(self.name)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npop the topmost level for assignment tracking and updates the context. exported_vars list if necessary.", "response": "def pop_assign_tracking(self, frame):\n        \"\"\"Pops the topmost level for assignment tracking and updates the\n        context variables if necessary.\n        \"\"\"\n        vars = self._assign_stack.pop()\n        if not frame.toplevel or not vars:\n            return\n        public_names = [x for x in vars if x[:1] != '_']\n        if len(vars) == 1:\n            name = next(iter(vars))\n            ref = frame.symbols.ref(name)\n            self.writeline('context.vars[%r] = %s' % (name, ref))\n        else:\n            self.writeline('context.vars.update({')\n            for idx, name in enumerate(vars):\n                if idx:\n                    self.write(', ')\n                ref = frame.symbols.ref(name)\n                self.write('%r: %s' % (name, ref))\n            self.write('})')\n        if public_names:\n            if len(public_names) == 1:\n                self.writeline('context.exported_vars.add(%r)' %\n                               public_names[0])\n            else:\n                self.writeline('context.exported_vars.update((%s))' %\n                               ', '.join(imap(repr, public_names)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall a block and register it for the template.", "response": "def visit_Block(self, node, frame):\n        \"\"\"Call a block and register it for the template.\"\"\"\n        level = 0\n        if frame.toplevel:\n            # if we know that we are a child template, there is no need to\n            # check if we are one\n            if self.has_known_extends:\n                return\n            if self.extends_so_far > 0:\n                self.writeline('if parent_template is None:')\n                self.indent()\n                level += 1\n\n        if node.scoped:\n            context = self.derive_context(frame)\n        else:\n            context = self.get_context_ref()\n\n        if supports_yield_from and not self.environment.is_async and \\\n           frame.buffer is None:\n            self.writeline('yield from context.blocks[%r][0](%s)' % (\n                           node.name, context), node)\n        else:\n            loop = self.environment.is_async and 'async for' or 'for'\n            self.writeline('%s event in context.blocks[%r][0](%s):' % (\n                           loop, node.name, context), node)\n            self.indent()\n            self.simple_write('event', frame)\n            self.outdent()\n\n        self.outdent(level)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetach from the current object and returns None if no object is available.", "response": "def detach(self):\n        \"\"\"If alive then mark as dead and return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None and self._registry.pop(self, None):\n            return (obj, info.func, info.args, info.kwargs or {})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npeeks the weakref of the object and returns the first tuple of the object the function is called with the args and kwargs.", "response": "def peek(self):\n        \"\"\"If alive then return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None:\n            return (obj, info.func, info.args, info.kwargs or {})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef atexit(self):\n        info = self._registry.get(self)\n        return bool(info) and info.atexit", "response": "Whether finalizer should be called at exit"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tostring(element):\n    rv = []\n\n    def serializeElement(element):\n        if not hasattr(element, \"tag\"):\n            if element.docinfo.internalDTD:\n                if element.docinfo.doctype:\n                    dtd_str = element.docinfo.doctype\n                else:\n                    dtd_str = \"<!DOCTYPE %s>\" % element.docinfo.root_name\n                rv.append(dtd_str)\n            serializeElement(element.getroot())\n\n        elif element.tag == comment_type:\n            rv.append(\"<!--%s-->\" % (element.text,))\n\n        else:\n            # This is assumed to be an ordinary element\n            if not element.attrib:\n                rv.append(\"<%s>\" % (element.tag,))\n            else:\n                attr = \" \".join([\"%s=\\\"%s\\\"\" % (name, value)\n                                 for name, value in element.attrib.items()])\n                rv.append(\"<%s %s>\" % (element.tag, attr))\n            if element.text:\n                rv.append(element.text)\n\n            for child in element:\n                serializeElement(child)\n\n            rv.append(\"</%s>\" % (element.tag,))\n\n        if hasattr(element, \"tail\") and element.tail:\n            rv.append(element.tail)\n\n    serializeElement(element)\n\n    return \"\".join(rv)", "response": "Serialize an element and its child nodes to a string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_visitor(self, node):\n        method = 'visit_' + node.__class__.__name__\n        return getattr(self, method, None)", "response": "Return the visitor function for this node or None if no visitor function for this node exists for this node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall by the visitor function to visit a node.", "response": "def generic_visit(self, node, *args, **kwargs):\n        \"\"\"Called if no explicit visitor function exists for a node.\"\"\"\n        for node in node.iter_child_nodes():\n            self.visit(node, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvisit a node and return a list of the available resource names.", "response": "def visit_list(self, node, *args, **kwargs):\n        \"\"\"As transformers may return lists in some places this method\n        can be used to enforce a list as return value.\n        \"\"\"\n        rv = self.visit(node, *args, **kwargs)\n        if not isinstance(rv, list):\n            rv = [rv]\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_wheel(\n            self, wheel_directory, config_settings=None,\n            metadata_directory=None):\n        \"\"\"Build a wheel from this project.\n\n        Returns the name of the newly created file.\n\n        In general, this will call the 'build_wheel' hook in the backend.\n        However, if that was previously called by\n        'prepare_metadata_for_build_wheel', and the same metadata_directory is\n        used, the previously built wheel will be copied to wheel_directory.\n        \"\"\"\n        if metadata_directory is not None:\n            metadata_directory = abspath(metadata_directory)\n        return self._call_hook('build_wheel', {\n            'wheel_directory': abspath(wheel_directory),\n            'config_settings': config_settings,\n            'metadata_directory': metadata_directory,\n        })", "response": "Build a wheel from this project."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct a ResNet - 18 model.", "response": "def resnet18(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resnet50(pretrained=False, **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n    return model", "response": "Constructs a ResNet - 50 model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs a ResNet - 101 model.", "response": "def cafferesnet101(num_classes=1000, pretrained='imagenet'):\n    \"\"\"Constructs a ResNet-101 model.\n    Args:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['cafferesnet101'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing a FBResNet - 153 model.", "response": "def fbresnet152(num_classes=1000, pretrained='imagenet'):\n    \"\"\"Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = FBResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['fbresnet152'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef alexnet(num_classes=1000, pretrained='imagenet'):\n    # https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py\n    model = models.alexnet(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['alexnet'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_alexnet(model)\n    return model", "response": "rAlexNet model architecture from the\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef densenet121(num_classes=1000, pretrained='imagenet'):\n    model = models.densenet121(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['densenet121'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_densenets(model)\n    return model", "response": "r Densenet -121 model from base64 encoded text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inceptionv3(num_classes=1000, pretrained='imagenet'):\n    model = models.inception_v3(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['inceptionv3'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n\n    # Modify attributs\n    model.last_linear = model.fc\n    del model.fc\n\n    def features(self, input):\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(input) # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x) # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x) # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2) # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x) # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x) # 71 x 71 x 192\n        x = F.max_pool2d(x, kernel_size=3, stride=2) # 35 x 35 x 192\n        x = self.Mixed_5b(x) # 35 x 35 x 256\n        x = self.Mixed_5c(x) # 35 x 35 x 288\n        x = self.Mixed_5d(x) # 35 x 35 x 288\n        x = self.Mixed_6a(x) # 17 x 17 x 768\n        x = self.Mixed_6b(x) # 17 x 17 x 768\n        x = self.Mixed_6c(x) # 17 x 17 x 768\n        x = self.Mixed_6d(x) # 17 x 17 x 768\n        x = self.Mixed_6e(x) # 17 x 17 x 768\n        if self.training and self.aux_logits:\n            self._out_aux = self.AuxLogits(x) # 17 x 17 x 768\n        x = self.Mixed_7a(x) # 8 x 8 x 1280\n        x = self.Mixed_7b(x) # 8 x 8 x 2048\n        x = self.Mixed_7c(x) # 8 x 8 x 2048\n        return x\n\n    def logits(self, features):\n        x = F.avg_pool2d(features, kernel_size=8) # 1 x 1 x 2048\n        x = F.dropout(x, training=self.training) # 1 x 1 x 2048\n        x = x.view(x.size(0), -1) # 2048\n        x = self.last_linear(x) # 1000 (num_classes)\n        if self.training and self.aux_logits:\n            aux = self._out_aux\n            self._out_aux = None\n            return x, aux\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.features = types.MethodType(features, model)\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model", "response": "rInception v3 model architecture from\n    Rethinking the Inception Architecture for Computer Vision <http://arxiv. org / abs / 1512. 00567 > _ _"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a ResNet - 50 model.", "response": "def resnet50(num_classes=1000, pretrained='imagenet'):\n    \"\"\"Constructs a ResNet-50 model.\n    \"\"\"\n    model = models.resnet50(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['resnet50'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef squeezenet1_0(num_classes=1000, pretrained='imagenet'):\n    model = models.squeezenet1_0(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['squeezenet1_0'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_squeezenets(model)\n    return model", "response": "rSqueezeNet model architecture from the SqueezeNet - level\n    accuracy with 50x fewer parameters and < 0. 5MB model size"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef vgg11(num_classes=1000, pretrained='imagenet'):\n    model = models.vgg11(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['vgg11'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model", "response": "VGG 11 - layer model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef adjust_learning_rate(optimizer, epoch):\n    lr = args.lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr", "response": "Adjusts the learning rate for the given optimizer to the initial LR decayed by 10 every 30 epochs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nasnetalarge(num_classes=1001, pretrained='imagenet'):\n    if pretrained:\n        settings = pretrained_settings['nasnetalarge'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n\n        # both 'imagenet'&'imagenet+background' are loaded from same parameters\n        model = NASNetALarge(num_classes=1001)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n\n        if pretrained == 'imagenet':\n            new_last_linear = nn.Linear(model.last_linear.in_features, 1000)\n            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n            model.last_linear = new_last_linear\n\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n\n        model.mean = settings['mean']\n        model.std = settings['std']\n    else:\n        model = NASNetALarge(num_classes=num_classes)\n    return model", "response": "r NASNetALarge model architecture from the NASNet <https://arxiv. org / abs / 1707.07012 > _ paper."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef adaptive_avgmax_pool2d(x, pool_type='avg', padding=0, count_include_pad=False):\n    if pool_type == 'avgmaxc':\n        x = torch.cat([\n            F.avg_pool2d(\n                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad),\n            F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n        ], dim=1)\n    elif pool_type == 'avgmax':\n        x_avg = F.avg_pool2d(\n                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n        x_max = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n        x = 0.5 * (x_avg + x_max)\n    elif pool_type == 'max':\n        x = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n    else:\n        if pool_type != 'avg':\n            print('Invalid pool type %s specified. Defaulting to average pooling.' % pool_type)\n        x = F.avg_pool2d(\n            x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n    return x", "response": "Selectable global pooling function with dynamic input kernel size\nFormula"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads a URL to a local file.", "response": "def download_url(url, destination=None, progress_bar=True):\n    \"\"\"Download a URL to a local file.\n\n    Parameters\n    ----------\n    url : str\n        The URL to download.\n    destination : str, None\n        The destination of the file. If None is given the file is saved to a temporary directory.\n    progress_bar : bool\n        Whether to show a command-line progress bar while downloading.\n\n    Returns\n    -------\n    filename : str\n        The location of the downloaded file.\n\n    Notes\n    -----\n    Progress bar use/example adapted from tqdm documentation: https://github.com/tqdm/tqdm\n    \"\"\"\n\n    def my_hook(t):\n        last_b = [0]\n\n        def inner(b=1, bsize=1, tsize=None):\n            if tsize is not None:\n                t.total = tsize\n            if b > 0:\n                t.update((b - last_b[0]) * bsize)\n            last_b[0] = b\n\n        return inner\n\n    if progress_bar:\n        with tqdm(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n            filename, _ = urlretrieve(url, filename=destination, reporthook=my_hook(t))\n    else:\n        filename, _ = urlretrieve(url, filename=destination)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new entry to the internal state.", "response": "def add(self, output, target):\n        \"\"\"\n        Args:\n            output (Tensor): NxK tensor that for each of the N examples\n                indicates the probability of the example belonging to each of\n                the K classes, according to the model. The probabilities should\n                sum to one over all classes\n            target (Tensor): binary NxK tensort that encodes which of the K\n                classes are associated with the N-th input\n                    (eg: a row [0, 1, 0, 1] indicates that the example is\n                         associated with classes 2 and 4)\n            weight (optional, Tensor): Nx1 tensor representing the weight for\n                each example (each weight > 0)\n        \"\"\"\n        if not torch.is_tensor(output):\n            output = torch.from_numpy(output)\n        if not torch.is_tensor(target):\n            target = torch.from_numpy(target)\n\n        if output.dim() == 1:\n            output = output.view(-1, 1)\n        else:\n            assert output.dim() == 2, \\\n                'wrong output size (should be 1D or 2D with one column \\\n                per class)'\n        if target.dim() == 1:\n            target = target.view(-1, 1)\n        else:\n            assert target.dim() == 2, \\\n                'wrong target size (should be 1D or 2D with one column \\\n                per class)'\n        if self.scores.numel() > 0:\n            assert target.size(1) == self.targets.size(1), \\\n                'dimensions for output should match previously added examples.'\n\n        # make sure storage is of sufficient size\n        if self.scores.storage().size() < self.scores.numel() + output.numel():\n            new_size = math.ceil(self.scores.storage().size() * 1.5)\n            self.scores.storage().resize_(int(new_size + output.numel()))\n            self.targets.storage().resize_(int(new_size + output.numel()))\n\n        # store scores and targets\n        offset = self.scores.size(0) if self.scores.dim() > 0 else 0\n        self.scores.resize_(offset + output.size(0), output.size(1))\n        self.targets.resize_(offset + target.size(0), target.size(1))\n        self.scores.narrow(0, offset, output.size(0)).copy_(output)\n        self.targets.narrow(0, offset, target.size(0)).copy_(target)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef value(self):\n\n        if self.scores.numel() == 0:\n            return 0\n        ap = torch.zeros(self.scores.size(1))\n        rg = torch.arange(1, self.scores.size(0)).float()\n\n        # compute average precision for each class\n        for k in range(self.scores.size(1)):\n            # sort scores\n            scores = self.scores[:, k]\n            targets = self.targets[:, k]\n\n            # compute average precision\n            ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)\n        return ap", "response": "Returns the model s average precision for each class k\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new version of the CNN model with the specified number of classes.", "response": "def polynet(num_classes=1000, pretrained='imagenet'):\n    \"\"\"PolyNet architecture from the paper\n    'PolyNet: A Pursuit of Structural Diversity in Very Deep Networks'\n    https://arxiv.org/abs/1611.05725\n    \"\"\"\n    if pretrained:\n        settings = pretrained_settings['polynet'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            'num_classes should be {}, but is {}'.format(\n                settings['num_classes'], num_classes)\n        model = PolyNet(num_classes=num_classes)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    else:\n        model = PolyNet(num_classes=num_classes)\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unwrap(self, dt):\n        expires = self._expires\n        if expires is AlwaysExpired or expires < dt:\n            raise Expired(self._expires)\n        return self._value", "response": "Get the cached value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the value of a cached object.", "response": "def get(self, key, dt):\n        \"\"\"Get the value of a cached object.\n\n        Parameters\n        ----------\n        key : any\n            The key to lookup.\n        dt : datetime\n            The time of the lookup.\n\n        Returns\n        -------\n        result : any\n            The value for ``key``.\n\n        Raises\n        ------\n        KeyError\n            Raised if the key is not in the cache or the value for the key\n            has expired.\n        \"\"\"\n        try:\n            return self._cache[key].unwrap(dt)\n        except Expired:\n            self.cleanup(self._cache[key]._unsafe_get_value())\n            del self._cache[key]\n            raise KeyError(key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a new key value pair to the cache.", "response": "def set(self, key, value, expiration_dt):\n        \"\"\"Adds a new key value pair to the cache.\n\n        Parameters\n        ----------\n        key : any\n            The key to use for the pair.\n        value : any\n            The value to store under the name ``key``.\n        expiration_dt : datetime\n            When should this mapping expire? The cache is considered invalid\n            for dates **strictly greater** than ``expiration_dt``.\n        \"\"\"\n        self._cache[key] = CachedObject(value, expiration_dt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure a subdirectory of the working directory.", "response": "def ensure_dir(self, *path_parts):\n        \"\"\"Ensures a subdirectory of the working directory.\n\n        Parameters\n        ----------\n        path_parts : iterable[str]\n            The parts of the path after the working directory.\n        \"\"\"\n        path = self.getpath(*path_parts)\n        ensure_directory(path)\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify_frames_aligned(frames, calendar):\n    indexes = [f.index for f in frames]\n    check_indexes_all_same(indexes, message=\"DataFrame indexes don't match:\")\n\n    columns = [f.columns for f in frames]\n    check_indexes_all_same(columns, message=\"DataFrame columns don't match:\")\n\n    start, end = indexes[0][[0, -1]]\n    cal_sessions = calendar.sessions_in_range(start, end)\n\n    check_indexes_all_same(\n        [indexes[0], cal_sessions],\n        \"DataFrame index doesn't match {} calendar:\".format(calendar.name),\n    )", "response": "Verify that all DataFrames in frames are aligned to calendar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_value(self, sid, dt, field):\n        return self.frames[field].loc[dt, sid]", "response": "Returns the value of the asset at the given sid on the given day."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the last trade datetime for the given asset and dt.", "response": "def get_last_traded_dt(self, asset, dt):\n        \"\"\"\n        Parameters\n        ----------\n        asset : zipline.asset.Asset\n            The asset identifier.\n        dt : datetime64-like\n            Midnight of the day for which data is requested.\n\n        Returns\n        -------\n        pd.Timestamp : The last know dt for the asset and dt;\n                       NaT if no trade is found before the given dt.\n        \"\"\"\n        try:\n            return self.frames['close'].loc[:, asset.sid].last_valid_index()\n        except IndexError:\n            return NaT"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef same(*values):\n    if not values:\n        return True\n    first, rest = values[0], values[1:]\n    return all(value == first for value in rest)", "response": "Check if all values in a sequence are equal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dzip_exact(*dicts):\n    if not same(*map(viewkeys, dicts)):\n        raise ValueError(\n            \"dict keys not all equal:\\n\\n%s\" % _format_unequal_keys(dicts)\n        )\n    return {k: tuple(d[k] for d in dicts) for k in dicts[0]}", "response": "Returns a new dict where the keys are the union of all keys in dicts and whose values are the union of all keys in dicts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _gen_unzip(it, elem_len):\n    elem = next(it)\n    first_elem_len = len(elem)\n\n    if elem_len is not None and elem_len != first_elem_len:\n        raise ValueError(\n            'element at index 0 was length %d, expected %d' % (\n                first_elem_len,\n                elem_len,\n            )\n        )\n    else:\n        elem_len = first_elem_len\n\n    yield elem\n    for n, elem in enumerate(it, 1):\n        if len(elem) != elem_len:\n            raise ValueError(\n                'element at index %d was length %d, expected %d' % (\n                    n,\n                    len(elem),\n                    elem_len,\n                ),\n            )\n        yield elem", "response": "Generator for unzipping the elements of the sequence in it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nunzip a length n sequence of length m seperate length n sequences.", "response": "def unzip(seq, elem_len=None):\n    \"\"\"Unzip a length n sequence of length m sequences into m seperate length\n    n sequences.\n    Parameters\n    ----------\n    seq : iterable[iterable]\n        The sequence to unzip.\n    elem_len : int, optional\n        The expected length of each element of ``seq``. If not provided this\n        will be infered from the length of the first element of ``seq``. This\n        can be used to ensure that code like: ``a, b = unzip(seq)`` does not\n        fail even when ``seq`` is empty.\n    Returns\n    -------\n    seqs : iterable[iterable]\n        The new sequences pulled out of the first iterable.\n    Raises\n    ------\n    ValueError\n        Raised when ``seq`` is empty and ``elem_len`` is not provided.\n        Raised when elements of ``seq`` do not match the given ``elem_len`` or\n        the length of the first element of ``seq``.\n    Examples\n    --------\n    >>> seq = [('a', 1), ('b', 2), ('c', 3)]\n    >>> cs, ns = unzip(seq)\n    >>> cs\n    ('a', 'b', 'c')\n    >>> ns\n    (1, 2, 3)\n\n    # checks that the elements are the same length\n    >>> seq = [('a', 1), ('b', 2), ('c', 3, 'extra')]\n    >>> cs, ns = unzip(seq)\n    Traceback (most recent call last):\n       ...\n    ValueError: element at index 2 was length 3, expected 2\n\n    # allows an explicit element length instead of infering\n    >>> seq = [('a', 1, 'extra'), ('b', 2), ('c', 3)]\n    >>> cs, ns = unzip(seq, 2)\n    Traceback (most recent call last):\n      ...\n    ValueError: element at index 0 was length 3, expected 2\n\n    # handles empty sequences when a length is given\n    >>> cs, ns = unzip([], elem_len=2)\n    >>> cs == ns == ()\n    True\n\n    Notes\n    -----\n    This function will force ``seq`` to completion.\n    \"\"\"\n    ret = tuple(zip(*_gen_unzip(map(tuple, seq), elem_len)))\n    if ret:\n        return ret\n\n    if elem_len is None:\n        raise ValueError(\"cannot unzip empty sequence without 'elem_len'\")\n    return ((),) * elem_len"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getattrs(value, attrs, default=_no_default):\n    try:\n        for attr in attrs:\n            value = getattr(value, attr)\n    except AttributeError:\n        if default is _no_default:\n            raise\n        value = default\n    return value", "response": "Perform a chained getattr on value with the values\n    in attrs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_attribute(name, value):\n    def decorator(f):\n        setattr(f, name, value)\n        return f\n    return decorator", "response": "Decorator for setting attributes on a single object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef foldr(f, seq, default=_no_default):\n    return reduce(\n        flip(f),\n        reversed(seq),\n        *(default,) if default is not _no_default else ()\n    )", "response": "Fold a function over a sequence with right associativity."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef invert(d):\n    out = {}\n    for k, v in iteritems(d):\n        try:\n            out[v].add(k)\n        except KeyError:\n            out[v] = {k}\n    return out", "response": "Invert a dictionary into a dictionary of sets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simplex_projection(v, b=1):\n\n    v = np.asarray(v)\n    p = len(v)\n\n    # Sort v into u in descending order\n    v = (v > 0) * v\n    u = np.sort(v)[::-1]\n    sv = np.cumsum(u)\n\n    rho = np.where(u > (sv - b) / np.arange(1, p + 1))[0][-1]\n    theta = np.max([0, (sv[rho] - b) / (rho + 1)])\n    w = (v - theta)\n    w[w < 0] = 0\n    return w", "response": "r Projections a vector v onto the simplex domain."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun an example module from zipline. examples.", "response": "def run_example(example_name, environ):\n    \"\"\"\n    Run an example module from zipline.examples.\n    \"\"\"\n    mod = EXAMPLE_MODULES[example_name]\n\n    register_calendar(\"YAHOO\", get_calendar(\"NYSE\"), force=True)\n\n    return run_algorithm(\n        initialize=getattr(mod, 'initialize', None),\n        handle_data=getattr(mod, 'handle_data', None),\n        before_trading_start=getattr(mod, 'before_trading_start', None),\n        analyze=getattr(mod, 'analyze', None),\n        bundle='test',\n        environ=environ,\n        # Provide a default capital base, but allow the test to override.\n        **merge({'capital_base': 1e7}, mod._test_args())\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the beta of a set of variables for each column of dependents and independent.", "response": "def vectorized_beta(dependents, independent, allowed_missing, out=None):\n    \"\"\"\n    Compute slopes of linear regressions between columns of ``dependents`` and\n    ``independent``.\n\n    Parameters\n    ----------\n    dependents : np.array[N, M]\n        Array with columns of data to be regressed against ``independent``.\n    independent : np.array[N, 1]\n        Independent variable of the regression\n    allowed_missing : int\n        Number of allowed missing (NaN) observations per column. Columns with\n        more than this many non-nan observations in both ``dependents`` and\n        ``independents`` will output NaN as the regression coefficient.\n\n    Returns\n    -------\n    slopes : np.array[M]\n        Linear regression coefficients for each column of ``dependents``.\n    \"\"\"\n    # Cache these as locals since we're going to call them multiple times.\n    nan = np.nan\n    isnan = np.isnan\n    N, M = dependents.shape\n\n    if out is None:\n        out = np.full(M, nan)\n\n    # Copy N times as a column vector and fill with nans to have the same\n    # missing value pattern as the dependent variable.\n    #\n    # PERF_TODO: We could probably avoid the space blowup by doing this in\n    # Cython.\n\n    # shape: (N, M)\n    independent = np.where(\n        isnan(dependents),\n        nan,\n        independent,\n    )\n\n    # Calculate beta as Cov(X, Y) / Cov(X, X).\n    # https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line  # noqa\n    #\n    # NOTE: The usual formula for covariance is::\n    #\n    #    mean((X - mean(X)) * (Y - mean(Y)))\n    #\n    # However, we don't actually need to take the mean of both sides of the\n    # product, because of the folllowing equivalence::\n    #\n    # Let X_res = (X - mean(X)).\n    # We have:\n    #\n    #     mean(X_res * (Y - mean(Y))) = mean(X_res * (Y - mean(Y)))\n    #                             (1) = mean((X_res * Y) - (X_res * mean(Y)))\n    #                             (2) = mean(X_res * Y) - mean(X_res * mean(Y))\n    #                             (3) = mean(X_res * Y) - mean(X_res) * mean(Y)\n    #                             (4) = mean(X_res * Y) - 0 * mean(Y)\n    #                             (5) = mean(X_res * Y)\n    #\n    #\n    # The tricky step in the above derivation is step (4). We know that\n    # mean(X_res) is zero because, for any X:\n    #\n    #     mean(X - mean(X)) = mean(X) - mean(X) = 0.\n    #\n    # The upshot of this is that we only have to center one of `independent`\n    # and `dependent` when calculating covariances. Since we need the centered\n    # `independent` to calculate its variance in the next step, we choose to\n    # center `independent`.\n\n    # shape: (N, M)\n    ind_residual = independent - nanmean(independent, axis=0)\n\n    # shape: (M,)\n    covariances = nanmean(ind_residual * dependents, axis=0)\n\n    # We end up with different variances in each column here because each\n    # column may have a different subset of the data dropped due to missing\n    # data in the corresponding dependent column.\n    # shape: (M,)\n    independent_variances = nanmean(ind_residual ** 2, axis=0)\n\n    # shape: (M,)\n    np.divide(covariances, independent_variances, out=out)\n\n    # Write nans back to locations where we have more then allowed number of\n    # missing entries.\n    nanlocs = isnan(independent).sum(axis=0) > allowed_missing\n    out[nanlocs] = nan\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat a URL for loading data from Bank of Canada.", "response": "def _format_url(instrument_type,\n                instrument_ids,\n                start_date,\n                end_date,\n                earliest_allowed_date):\n    \"\"\"\n    Format a URL for loading data from Bank of Canada.\n    \"\"\"\n    return (\n        \"http://www.bankofcanada.ca/stats/results/csv\"\n        \"?lP=lookup_{instrument_type}_yields.php\"\n        \"&sR={restrict}\"\n        \"&se={instrument_ids}\"\n        \"&dF={start}\"\n        \"&dT={end}\".format(\n            instrument_type=instrument_type,\n            instrument_ids='-'.join(map(prepend(\"L_\"), instrument_ids)),\n            restrict=earliest_allowed_date.strftime(\"%Y-%m-%d\"),\n            start=start_date.strftime(\"%Y-%m-%d\"),\n            end=end_date.strftime(\"%Y-%m-%d\"),\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a DataFrame of data from a Bank of Canada site.", "response": "def load_frame(url, skiprows):\n    \"\"\"\n    Load a DataFrame of data from a Bank of Canada site.\n    \"\"\"\n    return pd.read_csv(\n        url,\n        skiprows=skiprows,\n        skipinitialspace=True,\n        na_values=[\"Bank holiday\", \"Not available\"],\n        parse_dates=[\"Date\"],\n        index_col=\"Date\",\n    ).dropna(how='all') \\\n     .tz_localize('UTC') \\\n     .rename(columns=COLUMN_NAMES)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_known_inconsistencies(bill_data, bond_data):\n    inconsistent_dates = bill_data.index.sym_diff(bond_data.index)\n    known_inconsistencies = [\n        # bill_data has an entry for 2010-02-15, which bond_data doesn't.\n        # bond_data has an entry for 2006-09-04, which bill_data doesn't.\n        # Both of these dates are bank holidays (Flag Day and Labor Day,\n        # respectively).\n        pd.Timestamp('2006-09-04', tz='UTC'),\n        pd.Timestamp('2010-02-15', tz='UTC'),\n        # 2013-07-25 comes back as \"Not available\" from the bills endpoint.\n        # This date doesn't seem to be a bank holiday, but the previous\n        # calendar implementation dropped this entry, so we drop it as well.\n        # If someone cares deeply about the integrity of the Canadian trading\n        # calendar, they may want to consider forward-filling here rather than\n        # dropping the row.\n        pd.Timestamp('2013-07-25', tz='UTC'),\n    ]\n    unexpected_inconsistences = inconsistent_dates.drop(known_inconsistencies)\n    if len(unexpected_inconsistences):\n        in_bills = bill_data.index.difference(bond_data.index).difference(\n            known_inconsistencies\n        )\n        in_bonds = bond_data.index.difference(bill_data.index).difference(\n            known_inconsistencies\n        )\n        raise ValueError(\n            \"Inconsistent dates for Canadian treasury bills vs bonds. \\n\"\n            \"Dates with bills but not bonds: {in_bills}.\\n\"\n            \"Dates with bonds but not bills: {in_bonds}.\".format(\n                in_bills=in_bills,\n                in_bonds=in_bonds,\n            )\n        )", "response": "Check that there are no unexpected inconsistencies in the data provided by Bank of Canada."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the earliest possible date for which we can load data from this module.", "response": "def earliest_possible_date():\n    \"\"\"\n    The earliest date for which we can load data from this module.\n    \"\"\"\n    today = pd.Timestamp('now', tz='UTC').normalize()\n    # Bank of Canada only has the last 10 years of data at any given time.\n    return today.replace(year=today.year - 10)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck whether the fill price is worse than the order s limit price.", "response": "def fill_price_worse_than_limit_price(fill_price, order):\n    \"\"\"\n    Checks whether the fill price is worse than the order's limit price.\n\n    Parameters\n    ----------\n    fill_price: float\n        The price to check.\n\n    order: zipline.finance.order.Order\n        The order whose limit price to check.\n\n    Returns\n    -------\n    bool: Whether the fill price is above the limit price (for a buy) or below\n    the limit price (for a sell).\n    \"\"\"\n    if order.limit:\n        # this is tricky! if an order with a limit price has reached\n        # the limit price, we will try to fill the order. do not fill\n        # these shares if the impacted price is worse than the limit\n        # price. return early to avoid creating the transaction.\n\n        # buy order is worse if the impacted price is greater than\n        # the limit price. sell order is worse if the impacted price\n        # is less than the limit price\n        if (order.direction > 0 and fill_price > order.limit) or \\\n                (order.direction < 0 and fill_price < order.limit):\n            return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates a dtype and a missing_value passed to Term. __new__.", "response": "def validate_dtype(termname, dtype, missing_value):\n    \"\"\"\n    Validate a `dtype` and `missing_value` passed to Term.__new__.\n\n    Ensures that we know how to represent ``dtype``, and that missing_value\n    is specified for types without default missing values.\n\n    Returns\n    -------\n    validated_dtype, validated_missing_value : np.dtype, any\n        The dtype and missing_value to use for the new term.\n\n    Raises\n    ------\n    DTypeNotSpecified\n        When no dtype was passed to the instance, and the class doesn't\n        provide a default.\n    NotDType\n        When either the class or the instance provides a value not\n        coercible to a numpy dtype.\n    NoDefaultMissingValue\n        When dtype requires an explicit missing_value, but\n        ``missing_value`` is NotSpecified.\n    \"\"\"\n    if dtype is NotSpecified:\n        raise DTypeNotSpecified(termname=termname)\n\n    try:\n        dtype = dtype_class(dtype)\n    except TypeError:\n        raise NotDType(dtype=dtype, termname=termname)\n\n    if not can_represent_dtype(dtype):\n        raise UnsupportedDType(dtype=dtype, termname=termname)\n\n    if missing_value is NotSpecified:\n        missing_value = default_missing_value_for_dtype(dtype)\n\n    try:\n        if (dtype == categorical_dtype):\n            # This check is necessary because we use object dtype for\n            # categoricals, and numpy will allow us to promote numerical\n            # values to object even though we don't support them.\n            _assert_valid_categorical_missing_value(missing_value)\n\n        # For any other type, we can check if the missing_value is safe by\n        # making an array of that value and trying to safely convert it to\n        # the desired type.\n        # 'same_kind' allows casting between things like float32 and\n        # float64, but not str and int.\n        array([missing_value]).astype(dtype=dtype, casting='same_kind')\n    except TypeError as e:\n        raise TypeError(\n            \"Missing value {value!r} is not a valid choice \"\n            \"for term {termname} with dtype {dtype}.\\n\\n\"\n            \"Coercion attempt failed with: {error}\".format(\n                termname=termname,\n                value=missing_value,\n                dtype=dtype,\n                error=e,\n            )\n        )\n\n    return dtype, missing_value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that value is a valid categorical missing_value.", "response": "def _assert_valid_categorical_missing_value(value):\n    \"\"\"\n    Check that value is a valid categorical missing_value.\n\n    Raises a TypeError if the value is cannot be used as the missing_value for\n    a categorical_dtype Term.\n    \"\"\"\n    label_types = LabelArray.SUPPORTED_SCALAR_TYPES\n    if not isinstance(value, label_types):\n        raise TypeError(\n            \"Categorical terms must have missing values of type \"\n            \"{types}.\".format(\n                types=' or '.join([t.__name__ for t in label_types]),\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npops entries from the kwargs dict of the class. params.", "response": "def _pop_params(cls, kwargs):\n        \"\"\"\n        Pop entries from the `kwargs` passed to cls.__new__ based on the values\n        in `cls.params`.\n\n        Parameters\n        ----------\n        kwargs : dict\n            The kwargs passed to cls.__new__.\n\n        Returns\n        -------\n        params : list[(str, object)]\n            A list of string, value pairs containing the entries in cls.params.\n\n        Raises\n        ------\n        TypeError\n            Raised if any parameter values are not passed or not hashable.\n        \"\"\"\n        params = cls.params\n        if not isinstance(params, Mapping):\n            params = {k: NotSpecified for k in params}\n        param_values = []\n        for key, default_value in params.items():\n            try:\n                value = kwargs.pop(key, default_value)\n                if value is NotSpecified:\n                    raise KeyError(key)\n\n                # Check here that the value is hashable so that we fail here\n                # instead of trying to hash the param values tuple later.\n                hash(value)\n            except KeyError:\n                raise TypeError(\n                    \"{typename} expected a keyword parameter {name!r}.\".format(\n                        typename=cls.__name__,\n                        name=key\n                    )\n                )\n            except TypeError:\n                # Value wasn't hashable.\n                raise TypeError(\n                    \"{typename} expected a hashable value for parameter \"\n                    \"{name!r}, but got {value!r} instead.\".format(\n                        typename=cls.__name__,\n                        name=key,\n                        value=value,\n                    )\n                )\n\n            param_values.append((key, value))\n        return tuple(param_values)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the identity of the Term that would be constructed from the given arguments. Identities that compare equal will cause us to return a cached instance rather than constructing a new one. We do this primarily because it makes dependency resolution easier. This is a classmethod so that it can be called from Term.__new__ to determine whether to produce a new instance.", "response": "def _static_identity(cls,\n                         domain,\n                         dtype,\n                         missing_value,\n                         window_safe,\n                         ndim,\n                         params):\n        \"\"\"\n        Return the identity of the Term that would be constructed from the\n        given arguments.\n\n        Identities that compare equal will cause us to return a cached instance\n        rather than constructing a new one.  We do this primarily because it\n        makes dependency resolution easier.\n\n        This is a classmethod so that it can be called from Term.__new__ to\n        determine whether to produce a new instance.\n        \"\"\"\n        return (cls, domain, dtype, missing_value, window_safe, ndim, params)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init(self, domain, dtype, missing_value, window_safe, ndim, params):\n        self.domain = domain\n        self.dtype = dtype\n        self.missing_value = missing_value\n        self.window_safe = window_safe\n        self.ndim = ndim\n\n        for name, value in params:\n            if hasattr(self, name):\n                raise TypeError(\n                    \"Parameter {name!r} conflicts with already-present\"\n                    \" attribute with value {value!r}.\".format(\n                        name=name,\n                        value=getattr(self, name),\n                    )\n                )\n            # TODO: Consider setting these values as attributes and replacing\n            # the boilerplate in NumericalExpression, Rank, and\n            # PercentileFilter.\n\n        self.params = dict(params)\n\n        # Make sure that subclasses call super() in their _validate() methods\n        # by setting this flag.  The base class implementation of _validate\n        # should set this flag to True.\n        self._subclass_called_super_validate = False\n        self._validate()\n        assert self._subclass_called_super_validate, (\n            \"Term._validate() was not called.\\n\"\n            \"This probably means that you overrode _validate\"\n            \" without calling super().\"\n        )\n        del self._subclass_called_super_validate\n\n        return self", "response": "Initializes the object with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dependencies(self):\n        extra_input_rows = max(0, self.window_length - 1)\n        out = {}\n        for term in self.inputs:\n            out[term] = extra_input_rows\n        out[self.mask] = 0\n        return out", "response": "A dictionary of all the term - level dependencies for this term."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls with a column of the result of a pipeline. This needs to put the data into a format that can be used in a workspace to continue doing computations. Parameters ---------- result : pd.Series A multiindexed series with (dates, assets) whose values are the results of running this pipeline term over the dates. assets : pd.Index All of the assets being requested. This allows us to correctly shape the workspace value. Returns ------- workspace_value : array-like An array like value that the engine can consume.", "response": "def to_workspace_value(self, result, assets):\n        \"\"\"\n        Called with a column of the result of a pipeline. This needs to put\n        the data into a format that can be used in a workspace to continue\n        doing computations.\n\n        Parameters\n        ----------\n        result : pd.Series\n            A multiindexed series with (dates, assets) whose values are the\n            results of running this pipeline term over the dates.\n        assets : pd.Index\n            All of the assets being requested. This allows us to correctly\n            shape the workspace value.\n\n        Returns\n        -------\n        workspace_value : array-like\n            An array like value that the engine can consume.\n        \"\"\"\n        return result.unstack().fillna(self.missing_value).reindex(\n            columns=assets,\n            fill_value=self.missing_value,\n        ).values"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef earn_stock_dividend(self, stock_dividend):\n        return {\n            'payment_asset': stock_dividend.payment_asset,\n            'share_count': np.floor(\n                self.amount * float(stock_dividend.ratio)\n            )\n        }", "response": "Return the earned stock dividend."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the position by the split ratio and return the resulting cash. Returns the unused cash.", "response": "def handle_split(self, asset, ratio):\n        \"\"\"\n        Update the position by the split ratio, and return the resulting\n        fractional share that will be converted into cash.\n\n        Returns the unused cash.\n        \"\"\"\n        if self.asset != asset:\n            raise Exception(\"updating split with the wrong asset!\")\n\n        # adjust the # of shares by the ratio\n        # (if we had 100 shares, and the ratio is 3,\n        #  we now have 33 shares)\n        # (old_share_count / ratio = new_share_count)\n        # (old_price * ratio = new_price)\n\n        # e.g., 33.333\n        raw_share_count = self.amount / float(ratio)\n\n        # e.g., 33\n        full_share_count = np.floor(raw_share_count)\n\n        # e.g., 0.333\n        fractional_share_count = raw_share_count - full_share_count\n\n        # adjust the cost basis to the nearest cent, e.g., 60.0\n        new_cost_basis = round(self.cost_basis * ratio, 2)\n\n        self.cost_basis = new_cost_basis\n        self.amount = full_share_count\n\n        return_cash = round(float(fractional_share_count * new_cost_basis), 2)\n\n        log.info(\"after split: \" + str(self))\n        log.info(\"returning cash: \" + str(return_cash))\n\n        # return the leftover cash, which will be converted into cash\n        # (rounded to the nearest cent)\n        return return_cash"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadjusts the commission cost basis for this asset.", "response": "def adjust_commission_cost_basis(self, asset, cost):\n        \"\"\"\n        A note about cost-basis in zipline: all positions are considered\n        to share a cost basis, even if they were executed in different\n        transactions with different commission costs, different prices, etc.\n\n        Due to limitations about how zipline handles positions, zipline will\n        currently spread an externally-delivered commission charge across\n        all shares in a position.\n        \"\"\"\n\n        if asset != self.asset:\n            raise Exception('Updating a commission for a different asset?')\n        if cost == 0.0:\n            return\n\n        # If we no longer hold this position, there is no cost basis to\n        # adjust.\n        if self.amount == 0:\n            return\n\n        # We treat cost basis as the share price where we have broken even.\n        # For longs, commissions cause a relatively straight forward increase\n        # in the cost basis.\n        #\n        # For shorts, you actually want to decrease the cost basis because you\n        # break even and earn a profit when the share price decreases.\n        #\n        # Shorts are represented as having a negative `amount`.\n        #\n        # The multiplication and division by `amount` cancel out leaving the\n        # cost_basis positive, while subtracting the commission.\n\n        prev_cost = self.cost_basis * self.amount\n        if isinstance(asset, Future):\n            cost_to_use = cost / asset.price_multiplier\n        else:\n            cost_to_use = cost\n        new_cost = prev_cost + cost_to_use\n        self.cost_basis = new_cost / self.amount"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary representing the state of this position.", "response": "def to_dict(self):\n        \"\"\"\n        Creates a dictionary representing the state of this position.\n        Returns a dict object of the form:\n        \"\"\"\n        return {\n            'sid': self.asset,\n            'amount': self.amount,\n            'cost_basis': self.cost_basis,\n            'last_sale_price': self.last_sale_price\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_bundle_core():\n    _bundles = {}  # the registered bundles\n    # Expose _bundles through a proxy so that users cannot mutate this\n    # accidentally. Users may go through `register` to update this which will\n    # warn when trampling another bundle.\n    bundles = mappingproxy(_bundles)\n\n    @curry\n    def register(name,\n                 f,\n                 calendar_name='NYSE',\n                 start_session=None,\n                 end_session=None,\n                 minutes_per_day=390,\n                 create_writers=True):\n        \"\"\"Register a data bundle ingest function.\n\n        Parameters\n        ----------\n        name : str\n            The name of the bundle.\n        f : callable\n            The ingest function. This function will be passed:\n\n              environ : mapping\n                  The environment this is being run with.\n              asset_db_writer : AssetDBWriter\n                  The asset db writer to write into.\n              minute_bar_writer : BcolzMinuteBarWriter\n                  The minute bar writer to write into.\n              daily_bar_writer : BcolzDailyBarWriter\n                  The daily bar writer to write into.\n              adjustment_writer : SQLiteAdjustmentWriter\n                  The adjustment db writer to write into.\n              calendar : trading_calendars.TradingCalendar\n                  The trading calendar to ingest for.\n              start_session : pd.Timestamp\n                  The first session of data to ingest.\n              end_session : pd.Timestamp\n                  The last session of data to ingest.\n              cache : DataFrameCache\n                  A mapping object to temporarily store dataframes.\n                  This should be used to cache intermediates in case the load\n                  fails. This will be automatically cleaned up after a\n                  successful load.\n              show_progress : bool\n                  Show the progress for the current load where possible.\n        calendar_name : str, optional\n            The name of a calendar used to align bundle data.\n            Default is 'NYSE'.\n        start_session : pd.Timestamp, optional\n            The first session for which we want data. If not provided,\n            or if the date lies outside the range supported by the\n            calendar, the first_session of the calendar is used.\n        end_session : pd.Timestamp, optional\n            The last session for which we want data. If not provided,\n            or if the date lies outside the range supported by the\n            calendar, the last_session of the calendar is used.\n        minutes_per_day : int, optional\n            The number of minutes in each normal trading day.\n        create_writers : bool, optional\n            Should the ingest machinery create the writers for the ingest\n            function. This can be disabled as an optimization for cases where\n            they are not needed, like the ``quantopian-quandl`` bundle.\n\n        Notes\n        -----\n        This function my be used as a decorator, for example:\n\n        .. code-block:: python\n\n           @register('quandl')\n           def quandl_ingest_function(...):\n               ...\n\n        See Also\n        --------\n        zipline.data.bundles.bundles\n        \"\"\"\n        if name in bundles:\n            warnings.warn(\n                'Overwriting bundle with name %r' % name,\n                stacklevel=3,\n            )\n\n        # NOTE: We don't eagerly compute calendar values here because\n        # `register` is called at module scope in zipline, and creating a\n        # calendar currently takes between 0.5 and 1 seconds, which causes a\n        # noticeable delay on the zipline CLI.\n        _bundles[name] = RegisteredBundle(\n            calendar_name=calendar_name,\n            start_session=start_session,\n            end_session=end_session,\n            minutes_per_day=minutes_per_day,\n            ingest=f,\n            create_writers=create_writers,\n        )\n        return f\n\n    def unregister(name):\n        \"\"\"Unregister a bundle.\n\n        Parameters\n        ----------\n        name : str\n            The name of the bundle to unregister.\n\n        Raises\n        ------\n        UnknownBundle\n            Raised when no bundle has been registered with the given name.\n\n        See Also\n        --------\n        zipline.data.bundles.bundles\n        \"\"\"\n        try:\n            del _bundles[name]\n        except KeyError:\n            raise UnknownBundle(name)\n\n    def ingest(name,\n               environ=os.environ,\n               timestamp=None,\n               assets_versions=(),\n               show_progress=False):\n        \"\"\"Ingest data for a given bundle.\n\n        Parameters\n        ----------\n        name : str\n            The name of the bundle.\n        environ : mapping, optional\n            The environment variables. By default this is os.environ.\n        timestamp : datetime, optional\n            The timestamp to use for the load.\n            By default this is the current time.\n        assets_versions : Iterable[int], optional\n            Versions of the assets db to which to downgrade.\n        show_progress : bool, optional\n            Tell the ingest function to display the progress where possible.\n        \"\"\"\n        try:\n            bundle = bundles[name]\n        except KeyError:\n            raise UnknownBundle(name)\n\n        calendar = get_calendar(bundle.calendar_name)\n\n        start_session = bundle.start_session\n        end_session = bundle.end_session\n\n        if start_session is None or start_session < calendar.first_session:\n            start_session = calendar.first_session\n\n        if end_session is None or end_session > calendar.last_session:\n            end_session = calendar.last_session\n\n        if timestamp is None:\n            timestamp = pd.Timestamp.utcnow()\n        timestamp = timestamp.tz_convert('utc').tz_localize(None)\n\n        timestr = to_bundle_ingest_dirname(timestamp)\n        cachepath = cache_path(name, environ=environ)\n        pth.ensure_directory(pth.data_path([name, timestr], environ=environ))\n        pth.ensure_directory(cachepath)\n        with dataframe_cache(cachepath, clean_on_failure=False) as cache, \\\n                ExitStack() as stack:\n            # we use `cleanup_on_failure=False` so that we don't purge the\n            # cache directory if the load fails in the middle\n            if bundle.create_writers:\n                wd = stack.enter_context(working_dir(\n                    pth.data_path([], environ=environ))\n                )\n                daily_bars_path = wd.ensure_dir(\n                    *daily_equity_relative(\n                        name, timestr, environ=environ,\n                    )\n                )\n                daily_bar_writer = BcolzDailyBarWriter(\n                    daily_bars_path,\n                    calendar,\n                    start_session,\n                    end_session,\n                )\n                # Do an empty write to ensure that the daily ctables exist\n                # when we create the SQLiteAdjustmentWriter below. The\n                # SQLiteAdjustmentWriter needs to open the daily ctables so\n                # that it can compute the adjustment ratios for the dividends.\n\n                daily_bar_writer.write(())\n                minute_bar_writer = BcolzMinuteBarWriter(\n                    wd.ensure_dir(*minute_equity_relative(\n                        name, timestr, environ=environ)\n                    ),\n                    calendar,\n                    start_session,\n                    end_session,\n                    minutes_per_day=bundle.minutes_per_day,\n                )\n                assets_db_path = wd.getpath(*asset_db_relative(\n                    name, timestr, environ=environ,\n                ))\n                asset_db_writer = AssetDBWriter(assets_db_path)\n\n                adjustment_db_writer = stack.enter_context(\n                    SQLiteAdjustmentWriter(\n                        wd.getpath(*adjustment_db_relative(\n                            name, timestr, environ=environ)),\n                        BcolzDailyBarReader(daily_bars_path),\n                        overwrite=True,\n                    )\n                )\n            else:\n                daily_bar_writer = None\n                minute_bar_writer = None\n                asset_db_writer = None\n                adjustment_db_writer = None\n                if assets_versions:\n                    raise ValueError('Need to ingest a bundle that creates '\n                                     'writers in order to downgrade the assets'\n                                     ' db.')\n            bundle.ingest(\n                environ,\n                asset_db_writer,\n                minute_bar_writer,\n                daily_bar_writer,\n                adjustment_db_writer,\n                calendar,\n                start_session,\n                end_session,\n                cache,\n                show_progress,\n                pth.data_path([name, timestr], environ=environ),\n            )\n\n            for version in sorted(set(assets_versions), reverse=True):\n                version_path = wd.getpath(*asset_db_relative(\n                    name, timestr, environ=environ, db_version=version,\n                ))\n                with working_file(version_path) as wf:\n                    shutil.copy2(assets_db_path, wf.path)\n                    downgrade(wf.path, version)\n\n    def most_recent_data(bundle_name, timestamp, environ=None):\n        \"\"\"Get the path to the most recent data after ``date``for the\n        given bundle.\n\n        Parameters\n        ----------\n        bundle_name : str\n            The name of the bundle to lookup.\n        timestamp : datetime\n            The timestamp to begin searching on or before.\n        environ : dict, optional\n            An environment dict to forward to zipline_root.\n        \"\"\"\n        if bundle_name not in bundles:\n            raise UnknownBundle(bundle_name)\n\n        try:\n            candidates = os.listdir(\n                pth.data_path([bundle_name], environ=environ),\n            )\n            return pth.data_path(\n                [bundle_name,\n                 max(\n                     filter(complement(pth.hidden), candidates),\n                     key=from_bundle_ingest_dirname,\n                 )],\n                environ=environ,\n            )\n        except (ValueError, OSError) as e:\n            if getattr(e, 'errno', errno.ENOENT) != errno.ENOENT:\n                raise\n            raise ValueError(\n                'no data for bundle {bundle!r} on or before {timestamp}\\n'\n                'maybe you need to run: $ zipline ingest -b {bundle}'.format(\n                    bundle=bundle_name,\n                    timestamp=timestamp,\n                ),\n            )\n\n    def load(name, environ=os.environ, timestamp=None):\n        \"\"\"Loads a previously ingested bundle.\n\n        Parameters\n        ----------\n        name : str\n            The name of the bundle.\n        environ : mapping, optional\n            The environment variables. Defaults of os.environ.\n        timestamp : datetime, optional\n            The timestamp of the data to lookup.\n            Defaults to the current time.\n\n        Returns\n        -------\n        bundle_data : BundleData\n            The raw data readers for this bundle.\n        \"\"\"\n        if timestamp is None:\n            timestamp = pd.Timestamp.utcnow()\n        timestr = most_recent_data(name, timestamp, environ=environ)\n        return BundleData(\n            asset_finder=AssetFinder(\n                asset_db_path(name, timestr, environ=environ),\n            ),\n            equity_minute_bar_reader=BcolzMinuteBarReader(\n                minute_equity_path(name, timestr, environ=environ),\n            ),\n            equity_daily_bar_reader=BcolzDailyBarReader(\n                daily_equity_path(name, timestr, environ=environ),\n            ),\n            adjustment_reader=SQLiteAdjustmentReader(\n                adjustment_db_path(name, timestr, environ=environ),\n            ),\n        )\n\n    @preprocess(\n        before=optionally(ensure_timestamp),\n        after=optionally(ensure_timestamp),\n    )\n    def clean(name,\n              before=None,\n              after=None,\n              keep_last=None,\n              environ=os.environ):\n        \"\"\"Clean up data that was created with ``ingest`` or\n        ``$ python -m zipline ingest``\n\n        Parameters\n        ----------\n        name : str\n            The name of the bundle to remove data for.\n        before : datetime, optional\n            Remove data ingested before this date.\n            This argument is mutually exclusive with: keep_last\n        after : datetime, optional\n            Remove data ingested after this date.\n            This argument is mutually exclusive with: keep_last\n        keep_last : int, optional\n            Remove all but the last ``keep_last`` ingestions.\n            This argument is mutually exclusive with:\n              before\n              after\n        environ : mapping, optional\n            The environment variables. Defaults of os.environ.\n\n        Returns\n        -------\n        cleaned : set[str]\n            The names of the runs that were removed.\n\n        Raises\n        ------\n        BadClean\n            Raised when ``before`` and or ``after`` are passed with\n            ``keep_last``. This is a subclass of ``ValueError``.\n        \"\"\"\n        try:\n            all_runs = sorted(\n                filter(\n                    complement(pth.hidden),\n                    os.listdir(pth.data_path([name], environ=environ)),\n                ),\n                key=from_bundle_ingest_dirname,\n            )\n        except OSError as e:\n            if e.errno != errno.ENOENT:\n                raise\n            raise UnknownBundle(name)\n        if ((before is not None or after is not None) and\n                keep_last is not None):\n            raise BadClean(before, after, keep_last)\n\n        if keep_last is None:\n            def should_clean(name):\n                dt = from_bundle_ingest_dirname(name)\n                return (\n                    (before is not None and dt < before) or\n                    (after is not None and dt > after)\n                )\n\n        elif keep_last >= 0:\n            last_n_dts = set(take(keep_last, reversed(all_runs)))\n\n            def should_clean(name):\n                return name not in last_n_dts\n        else:\n            raise BadClean(before, after, keep_last)\n\n        cleaned = set()\n        for run in all_runs:\n            if should_clean(run):\n                path = pth.data_path([name, run], environ=environ)\n                shutil.rmtree(path)\n                cleaned.add(path)\n\n        return cleaned\n\n    return BundleCore(bundles, register, unregister, ingest, load, clean)", "response": "Create a family of data bundle functions that read from the same\n    bundle mapping."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nuse to mark a function as deprecated. Parameters ---------- msg : str The message to display in the deprecation warning. stacklevel : int How far up the stack the warning needs to go, before showing the relevant calling lines. Examples -------- @deprecated(msg='function_a is deprecated! Use function_b instead.') def function_a(*args, **kwargs):", "response": "def deprecated(msg=None, stacklevel=2):\n    \"\"\"\n    Used to mark a function as deprecated.\n\n    Parameters\n    ----------\n    msg : str\n        The message to display in the deprecation warning.\n    stacklevel : int\n        How far up the stack the warning needs to go, before\n        showing the relevant calling lines.\n\n    Examples\n    --------\n    @deprecated(msg='function_a is deprecated! Use function_b instead.')\n    def function_a(*args, **kwargs):\n    \"\"\"\n    def deprecated_dec(fn):\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                msg or \"Function %s is deprecated.\" % fn.__name__,\n                category=DeprecationWarning,\n                stacklevel=stacklevel\n            )\n            return fn(*args, **kwargs)\n        return wrapper\n    return deprecated_dec"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_pricing_adjustments(self, columns, dts, assets):\n        out = [None] * len(columns)\n        for i, column in enumerate(columns):\n            adjs = {}\n            for asset in assets:\n                adjs.update(self._get_adjustments_in_range(\n                    asset, dts, column))\n            out[i] = adjs\n        return out", "response": "Returns a list of adjustments for the given columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_adjustments_in_range(self, asset, dts, field):\n        sid = int(asset)\n        start = normalize_date(dts[0])\n        end = normalize_date(dts[-1])\n        adjs = {}\n        if field != 'volume':\n            mergers = self._adjustments_reader.get_adjustments_for_sid(\n                'mergers', sid)\n            for m in mergers:\n                dt = m[0]\n                if start < dt <= end:\n                    end_loc = dts.searchsorted(dt)\n                    adj_loc = end_loc\n                    mult = Float64Multiply(0,\n                                           end_loc - 1,\n                                           0,\n                                           0,\n                                           m[1])\n                    try:\n                        adjs[adj_loc].append(mult)\n                    except KeyError:\n                        adjs[adj_loc] = [mult]\n            divs = self._adjustments_reader.get_adjustments_for_sid(\n                'dividends', sid)\n            for d in divs:\n                dt = d[0]\n                if start < dt <= end:\n                    end_loc = dts.searchsorted(dt)\n                    adj_loc = end_loc\n                    mult = Float64Multiply(0,\n                                           end_loc - 1,\n                                           0,\n                                           0,\n                                           d[1])\n                    try:\n                        adjs[adj_loc].append(mult)\n                    except KeyError:\n                        adjs[adj_loc] = [mult]\n        splits = self._adjustments_reader.get_adjustments_for_sid(\n            'splits', sid)\n        for s in splits:\n            dt = s[0]\n            if start < dt <= end:\n                if field == 'volume':\n                    ratio = 1.0 / s[1]\n                else:\n                    ratio = s[1]\n                end_loc = dts.searchsorted(dt)\n                adj_loc = end_loc\n                mult = Float64Multiply(0,\n                                       end_loc - 1,\n                                       0,\n                                       0,\n                                       ratio)\n                try:\n                    adjs[adj_loc].append(mult)\n                except KeyError:\n                    adjs[adj_loc] = [mult]\n        return adjs", "response": "Get the adjustments for the given asset and dts."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the current set of equity entries up to end_ix.", "response": "def get(self, end_ix):\n        \"\"\"\n        Returns\n        -------\n        out : A np.ndarray of the equity pricing up to end_ix after adjustments\n              and rounding have been applied.\n        \"\"\"\n        if self.most_recent_ix == end_ix:\n            return self.current\n\n        target = end_ix - self.cal_start - self.offset + 1\n        self.current = self.window.seek(target)\n\n        self.most_recent_ix = end_ix\n        return self.current"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that the window for each asset in assets and dts are not already in the same time range.", "response": "def _ensure_sliding_windows(self, assets, dts, field,\n                                is_perspective_after):\n        \"\"\"\n        Ensure that there is a Float64Multiply window for each asset that can\n        provide data for the given parameters.\n        If the corresponding window for the (assets, len(dts), field) does not\n        exist, then create a new one.\n        If a corresponding window does exist for (assets, len(dts), field), but\n        can not provide data for the current dts range, then create a new\n        one and replace the expired window.\n\n        Parameters\n        ----------\n        assets : iterable of Assets\n            The assets in the window\n        dts : iterable of datetime64-like\n            The datetimes for which to fetch data.\n            Makes an assumption that all dts are present and contiguous,\n            in the calendar.\n        field : str\n            The OHLCV field for which to retrieve data.\n        is_perspective_after : bool\n            see: `PricingHistoryLoader.history`\n\n        Returns\n        -------\n        out : list of Float64Window with sufficient data so that each asset's\n        window can provide `get` for the index corresponding with the last\n        value in `dts`\n        \"\"\"\n        end = dts[-1]\n        size = len(dts)\n        asset_windows = {}\n        needed_assets = []\n        cal = self._calendar\n\n        assets = self._asset_finder.retrieve_all(assets)\n        end_ix = find_in_sorted_index(cal, end)\n\n        for asset in assets:\n            try:\n                window = self._window_blocks[field].get(\n                    (asset, size, is_perspective_after), end)\n            except KeyError:\n                needed_assets.append(asset)\n            else:\n                if end_ix < window.most_recent_ix:\n                    # Window needs reset. Requested end index occurs before the\n                    # end index from the previous history call for this window.\n                    # Grab new window instead of rewinding adjustments.\n                    needed_assets.append(asset)\n                else:\n                    asset_windows[asset] = window\n\n        if needed_assets:\n            offset = 0\n            start_ix = find_in_sorted_index(cal, dts[0])\n\n            prefetch_end_ix = min(end_ix + self._prefetch_length, len(cal) - 1)\n            prefetch_end = cal[prefetch_end_ix]\n            prefetch_dts = cal[start_ix:prefetch_end_ix + 1]\n            if is_perspective_after:\n                adj_end_ix = min(prefetch_end_ix + 1, len(cal) - 1)\n                adj_dts = cal[start_ix:adj_end_ix + 1]\n            else:\n                adj_dts = prefetch_dts\n            prefetch_len = len(prefetch_dts)\n            array = self._array(prefetch_dts, needed_assets, field)\n\n            if field == 'sid':\n                window_type = Int64Window\n            else:\n                window_type = Float64Window\n\n            view_kwargs = {}\n            if field == 'volume':\n                array = array.astype(float64_dtype)\n\n            for i, asset in enumerate(needed_assets):\n                adj_reader = None\n                try:\n                    adj_reader = self._adjustment_readers[type(asset)]\n                except KeyError:\n                    adj_reader = None\n                if adj_reader is not None:\n                    adjs = adj_reader.load_pricing_adjustments(\n                        [field], adj_dts, [asset])[0]\n                else:\n                    adjs = {}\n                window = window_type(\n                    array[:, i].reshape(prefetch_len, 1),\n                    view_kwargs,\n                    adjs,\n                    offset,\n                    size,\n                    int(is_perspective_after),\n                    self._decimal_places_for_asset(asset, dts[-1]),\n                )\n                sliding_window = SlidingWindow(window, size, start_ix, offset)\n                asset_windows[asset] = sliding_window\n                self._window_blocks[field].set(\n                    (asset, size, is_perspective_after),\n                    sliding_window,\n                    prefetch_end)\n\n        return [asset_windows[asset] for asset in assets]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef history(self, assets, dts, field, is_perspective_after):\n        block = self._ensure_sliding_windows(assets,\n                                             dts,\n                                             field,\n                                             is_perspective_after)\n        end_ix = self._calendar.searchsorted(dts[-1])\n\n        return concatenate(\n            [window.get(end_ix) for window in block],\n            axis=1,\n        )", "response": "This function returns the history of the specified assets and dts."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a date string into a Pandas datetime object.", "response": "def parse_date_str_series(format_str, tz, date_str_series, data_frequency,\n                              trading_day):\n        \"\"\"\n        Efficient parsing for a 1d Pandas/numpy object containing string\n        representations of dates.\n\n        Note: pd.to_datetime is significantly faster when no format string is\n        passed, and in pandas 0.12.0 the %p strptime directive is not correctly\n        handled if a format string is explicitly passed, but AM/PM is handled\n        properly if format=None.\n\n        Moreover, we were previously ignoring this parameter unintentionally\n        because we were incorrectly passing it as a positional.  For all these\n        reasons, we ignore the format_str parameter when parsing datetimes.\n        \"\"\"\n\n        # Explicitly ignoring this parameter.  See note above.\n        if format_str is not None:\n            logger.warn(\n                \"The 'format_str' parameter to fetch_csv is deprecated. \"\n                \"Ignoring and defaulting to pandas default date parsing.\"\n            )\n            format_str = None\n\n        tz_str = str(tz)\n        if tz_str == pytz.utc.zone:\n            parsed = pd.to_datetime(\n                date_str_series.values,\n                format=format_str,\n                utc=True,\n                errors='coerce',\n            )\n        else:\n            parsed = pd.to_datetime(\n                date_str_series.values,\n                format=format_str,\n                errors='coerce',\n            ).tz_localize(tz_str).tz_convert('UTC')\n\n        if data_frequency == 'daily':\n            parsed = roll_dts_to_midnight(parsed, trading_day)\n        return parsed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _lookup_unconflicted_symbol(self, symbol):\n        try:\n            uppered = symbol.upper()\n        except AttributeError:\n            # The mapping fails because symbol was a non-string\n            return numpy.nan\n\n        try:\n            return self.finder.lookup_symbol(\n                uppered,\n                as_of_date=None,\n                country_code=self.country_code,\n            )\n        except MultipleSymbolsFound:\n            # Fill conflicted entries with zeros to mark that they need to be\n            # resolved by date.\n            return 0\n        except SymbolNotFound:\n            # Fill not found entries with nans.\n            return numpy.nan", "response": "Attempts to find an unconflicted asset whose symbol is the given string. If multiple assets have held the given symbol return a 0. Otherwise return a NaN."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _cleanup_expired_assets(self, dt, position_assets):\n        algo = self.algo\n\n        def past_auto_close_date(asset):\n            acd = asset.auto_close_date\n            return acd is not None and acd <= dt\n\n        # Remove positions in any sids that have reached their auto_close date.\n        assets_to_clear = \\\n            [asset for asset in position_assets if past_auto_close_date(asset)]\n        metrics_tracker = algo.metrics_tracker\n        data_portal = self.data_portal\n        for asset in assets_to_clear:\n            metrics_tracker.process_close_position(asset, dt, data_portal)\n\n        # Remove open orders for any sids that have reached their auto close\n        # date. These orders get processed immediately because otherwise they\n        # would not be processed until the first bar of the next day.\n        blotter = algo.blotter\n        assets_to_cancel = [\n            asset for asset in blotter.open_orders\n            if past_auto_close_date(asset)\n        ]\n        for asset in assets_to_cancel:\n            blotter.cancel_all_orders_for_asset(asset)\n\n        # Make a copy here so that we are not modifying the list that is being\n        # iterated over.\n        for order in copy(blotter.new_orders):\n            if order.status == ORDER_STATUS.CANCELLED:\n                metrics_tracker.process_order(order)\n                blotter.new_orders.remove(order)", "response": "Clean up any assets that have expired before starting a new simulation day."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_daily_message(self, dt, algo, metrics_tracker):\n        perf_message = metrics_tracker.handle_market_close(\n            dt,\n            self.data_portal,\n        )\n        perf_message['daily_perf']['recorded_vars'] = algo.recorded_vars\n        return perf_message", "response": "Get a daily perf message for the given datetime."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_minute_message(self, dt, algo, metrics_tracker):\n        rvars = algo.recorded_vars\n\n        minute_message = metrics_tracker.handle_minute_close(\n            dt,\n            self.data_portal,\n        )\n\n        minute_message['minute_perf']['recorded_vars'] = rvars\n        return minute_message", "response": "Get a perf message for the given datetime."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_adjustments(self,\n                         dates,\n                         assets,\n                         should_include_splits,\n                         should_include_mergers,\n                         should_include_dividends,\n                         adjustment_type):\n        \"\"\"\n        Load collection of Adjustment objects from underlying adjustments db.\n\n        Parameters\n        ----------\n        dates : pd.DatetimeIndex\n            Dates for which adjustments are needed.\n        assets : pd.Int64Index\n            Assets for which adjustments are needed.\n        should_include_splits : bool\n            Whether split adjustments should be included.\n        should_include_mergers : bool\n            Whether merger adjustments should be included.\n        should_include_dividends : bool\n            Whether dividend adjustments should be included.\n        adjustment_type : str\n            Whether price adjustments, volume adjustments, or both, should be\n            included in the output.\n\n        Returns\n        -------\n        adjustments : dict[str -> dict[int -> Adjustment]]\n            A dictionary containing price and/or volume adjustment mappings\n            from index to adjustment objects to apply at that index.\n        \"\"\"\n        return load_adjustments_from_sqlite(\n            self.conn,\n            dates,\n            assets,\n            should_include_splits,\n            should_include_mergers,\n            should_include_dividends,\n            adjustment_type,\n        )", "response": "Loads a collection of Adjustment objects from the underlying adjustments db."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unpack_db_to_component_dfs(self, convert_dates=False):\n        return {\n            t_name: self.get_df_from_table(t_name, convert_dates)\n            for t_name in self._datetime_int_cols\n        }", "response": "Unpacks the database file into a dictionary of DataFrame objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _df_dtypes(self, table_name, convert_dates):\n        out = self._raw_table_dtypes[table_name]\n        if convert_dates:\n            out = out.copy()\n            for date_column in self._datetime_int_cols[table_name]:\n                out[date_column] = datetime64ns_dtype\n\n        return out", "response": "Get dtypes to use when unpacking sqlite tables as dataframes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the ratios to apply to the dividend when looking back at the market and merger.", "response": "def calc_dividend_ratios(self, dividends):\n        \"\"\"\n        Calculate the ratios to apply to equities when looking back at pricing\n        history so that the price is smoothed over the ex_date, when the market\n        adjusts to the change in equity value due to upcoming dividend.\n\n        Returns\n        -------\n        DataFrame\n            A frame in the same format as splits and mergers, with keys\n            - sid, the id of the equity\n            - effective_date, the date in seconds on which to apply the ratio.\n            - ratio, the ratio to apply to backwards looking pricing data.\n        \"\"\"\n        if dividends is None or dividends.empty:\n            return pd.DataFrame(np.array(\n                [],\n                dtype=[\n                    ('sid', uint64_dtype),\n                    ('effective_date', uint32_dtype),\n                    ('ratio', float64_dtype),\n                ],\n            ))\n\n        pricing_reader = self._equity_daily_bar_reader\n        input_sids = dividends.sid.values\n        unique_sids, sids_ix = np.unique(input_sids, return_inverse=True)\n        dates = pricing_reader.sessions.values\n\n        close, = pricing_reader.load_raw_arrays(\n            ['close'],\n            pd.Timestamp(dates[0], tz='UTC'),\n            pd.Timestamp(dates[-1], tz='UTC'),\n            unique_sids,\n        )\n        date_ix = np.searchsorted(dates, dividends.ex_date.values)\n        mask = date_ix > 0\n\n        date_ix = date_ix[mask]\n        sids_ix = sids_ix[mask]\n        input_dates = dividends.ex_date.values[mask]\n\n        # subtract one day to get the close on the day prior to the merger\n        previous_close = close[date_ix - 1, sids_ix]\n        input_sids = input_sids[mask]\n\n        amount = dividends.amount.values[mask]\n        ratio = 1.0 - amount / previous_close\n\n        non_nan_ratio_mask = ~np.isnan(ratio)\n        for ix in np.flatnonzero(~non_nan_ratio_mask):\n            log.warn(\n                \"Couldn't compute ratio for dividend\"\n                \" sid={sid}, ex_date={ex_date:%Y-%m-%d}, amount={amount:.3f}\",\n                sid=input_sids[ix],\n                ex_date=pd.Timestamp(input_dates[ix]),\n                amount=amount[ix],\n            )\n\n        positive_ratio_mask = ratio > 0\n        for ix in np.flatnonzero(~positive_ratio_mask & non_nan_ratio_mask):\n            log.warn(\n                \"Dividend ratio <= 0 for dividend\"\n                \" sid={sid}, ex_date={ex_date:%Y-%m-%d}, amount={amount:.3f}\",\n                sid=input_sids[ix],\n                ex_date=pd.Timestamp(input_dates[ix]),\n                amount=amount[ix],\n            )\n\n        valid_ratio_mask = non_nan_ratio_mask & positive_ratio_mask\n        return pd.DataFrame({\n            'sid': input_sids[valid_ratio_mask],\n            'effective_date': input_dates[valid_ratio_mask],\n            'ratio': ratio[valid_ratio_mask],\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the dividend payouts and the derived price adjustment ratios.", "response": "def write_dividend_data(self, dividends, stock_dividends=None):\n        \"\"\"\n        Write both dividend payouts and the derived price adjustment ratios.\n        \"\"\"\n\n        # First write the dividend payouts.\n        self._write_dividends(dividends)\n        self._write_stock_dividends(stock_dividends)\n\n        # Second from the dividend payouts, calculate ratios.\n        dividend_ratios = self.calc_dividend_ratios(dividends)\n        self.write_frame('dividends', dividend_ratios)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self,\n              splits=None,\n              mergers=None,\n              dividends=None,\n              stock_dividends=None):\n        \"\"\"\n        Writes data to a SQLite file to be read by SQLiteAdjustmentReader.\n\n        Parameters\n        ----------\n        splits : pandas.DataFrame, optional\n            Dataframe containing split data. The format of this dataframe is:\n              effective_date : int\n                  The date, represented as seconds since Unix epoch, on which\n                  the adjustment should be applied.\n              ratio : float\n                  A value to apply to all data earlier than the effective date.\n                  For open, high, low, and close those values are multiplied by\n                  the ratio. Volume is divided by this value.\n              sid : int\n                  The asset id associated with this adjustment.\n        mergers : pandas.DataFrame, optional\n            DataFrame containing merger data. The format of this dataframe is:\n              effective_date : int\n                  The date, represented as seconds since Unix epoch, on which\n                  the adjustment should be applied.\n              ratio : float\n                  A value to apply to all data earlier than the effective date.\n                  For open, high, low, and close those values are multiplied by\n                  the ratio. Volume is unaffected.\n              sid : int\n                  The asset id associated with this adjustment.\n        dividends : pandas.DataFrame, optional\n            DataFrame containing dividend data. The format of the dataframe is:\n              sid : int\n                  The asset id associated with this adjustment.\n              ex_date : datetime64\n                  The date on which an equity must be held to be eligible to\n                  receive payment.\n              declared_date : datetime64\n                  The date on which the dividend is announced to the public.\n              pay_date : datetime64\n                  The date on which the dividend is distributed.\n              record_date : datetime64\n                  The date on which the stock ownership is checked to determine\n                  distribution of dividends.\n              amount : float\n                  The cash amount paid for each share.\n\n            Dividend ratios are calculated as:\n            ``1.0 - (dividend_value / \"close on day prior to ex_date\")``\n        stock_dividends : pandas.DataFrame, optional\n            DataFrame containing stock dividend data. The format of the\n            dataframe is:\n              sid : int\n                  The asset id associated with this adjustment.\n              ex_date : datetime64\n                  The date on which an equity must be held to be eligible to\n                  receive payment.\n              declared_date : datetime64\n                  The date on which the dividend is announced to the public.\n              pay_date : datetime64\n                  The date on which the dividend is distributed.\n              record_date : datetime64\n                  The date on which the stock ownership is checked to determine\n                  distribution of dividends.\n              payment_sid : int\n                  The asset id of the shares that should be paid instead of\n                  cash.\n              ratio : float\n                  The ratio of currently held shares in the held sid that\n                  should be paid with new shares of the payment_sid.\n\n        See Also\n        --------\n        zipline.data.adjustments.SQLiteAdjustmentReader\n        \"\"\"\n        self.write_frame('splits', splits)\n        self.write_frame('mergers', mergers)\n        self.write_dividend_data(dividends, stock_dividends)\n        # Use IF NOT EXISTS here to allow multiple writes if desired.\n        self.conn.execute(\n            \"CREATE INDEX IF NOT EXISTS splits_sids \"\n            \"ON splits(sid)\"\n        )\n        self.conn.execute(\n            \"CREATE INDEX IF NOT EXISTS splits_effective_date \"\n            \"ON splits(effective_date)\"\n        )\n        self.conn.execute(\n            \"CREATE INDEX IF NOT EXISTS mergers_sids \"\n            \"ON mergers(sid)\"\n        )\n        self.conn.execute(\n            \"CREATE INDEX IF NOT EXISTS mergers_effective_date \"\n            \"ON mergers(effective_date)\"\n        )\n        self.conn.execute(\n            \"CREATE INDEX IF NOT EXISTS dividends_sid \"\n            \"ON dividends(sid)\"\n        )\n        self.conn.execute(\n            \"CREATE INDEX IF NOT EXISTS dividends_effective_date \"\n            \"ON dividends(effective_date)\"\n        )\n        self.conn.execute(\n            \"CREATE INDEX IF NOT EXISTS dividend_payouts_sid \"\n            \"ON dividend_payouts(sid)\"\n        )\n        self.conn.execute(\n            \"CREATE INDEX IF NOT EXISTS dividends_payouts_ex_date \"\n            \"ON dividend_payouts(ex_date)\"\n        )\n        self.conn.execute(\n            \"CREATE INDEX IF NOT EXISTS stock_dividend_payouts_sid \"\n            \"ON stock_dividend_payouts(sid)\"\n        )\n        self.conn.execute(\n            \"CREATE INDEX IF NOT EXISTS stock_dividends_payouts_ex_date \"\n            \"ON stock_dividend_payouts(ex_date)\"\n        )", "response": "Writes data to a SQLite file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute(self, today, assets, out, *arrays):\n        raise NotImplementedError(\n            \"{name} must define a compute method\".format(\n                name=type(self).__name__\n            )\n        )", "response": "Override this method with a function that writes a value into out."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _allocate_output(self, windows, shape):\n        missing_value = self.missing_value\n        outputs = self.outputs\n        if outputs is not NotSpecified:\n            out = recarray(\n                shape,\n                formats=[self.dtype.str] * len(outputs),\n                names=outputs,\n            )\n            out[:] = missing_value\n        else:\n            out = full(shape, missing_value, dtype=self.dtype)\n        return out", "response": "Allocate an output array whose rows should be passed to self. compute."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the user s compute function on each window with a pre - built array.", "response": "def _compute(self, windows, dates, assets, mask):\n        \"\"\"\n        Call the user's `compute` function on each window with a pre-built\n        output array.\n        \"\"\"\n        format_inputs = self._format_inputs\n        compute = self.compute\n        params = self.params\n        ndim = self.ndim\n\n        shape = (len(mask), 1) if ndim == 1 else mask.shape\n        out = self._allocate_output(windows, shape)\n\n        with self.ctx:\n            for idx, date in enumerate(dates):\n                # Never apply a mask to 1D outputs.\n                out_mask = array([True]) if ndim == 1 else mask[idx]\n\n                # Mask our inputs as usual.\n                inputs_mask = mask[idx]\n\n                masked_assets = assets[inputs_mask]\n                out_row = out[idx][out_mask]\n                inputs = format_inputs(windows, inputs_mask)\n\n                compute(date, masked_assets, out_row, *inputs, **params)\n                out[idx][out_mask] = out_row\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a type that can be used to alias another base.", "response": "def make_aliased_type(cls, other_base):\n        \"\"\"\n        Factory for making Aliased{Filter,Factor,Classifier}.\n        \"\"\"\n        docstring = dedent(\n            \"\"\"\n            A {t} that names another {t}.\n\n            Parameters\n            ----------\n            term : {t}\n            {{name}}\n            \"\"\"\n        ).format(t=other_base.__name__)\n\n        doc = format_docstring(\n            owner_name=other_base.__name__,\n            docstring=docstring,\n            formatters={'name': PIPELINE_ALIAS_NAME_DOC},\n        )\n\n        return type(\n            'Aliased' + other_base.__name__,\n            (cls, other_base),\n            {'__doc__': doc,\n             '__module__': other_base.__module__},\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_extra_rows(self,\n                           all_dates,\n                           start_date,\n                           end_date,\n                           min_extra_rows):\n        \"\"\"\n        Ensure that min_extra_rows pushes us back to a computation date.\n\n        Parameters\n        ----------\n        all_dates : pd.DatetimeIndex\n            The trading sessions against which ``self`` will be computed.\n        start_date : pd.Timestamp\n            The first date for which final output is requested.\n        end_date : pd.Timestamp\n            The last date for which final output is requested.\n        min_extra_rows : int\n            The minimum number of extra rows required of ``self``, as\n            determined by other terms that depend on ``self``.\n\n        Returns\n        -------\n        extra_rows : int\n            The number of extra rows to compute.  This will be the minimum\n            number of rows required to make our computed start_date fall on a\n            recomputation date.\n        \"\"\"\n        try:\n            current_start_pos = all_dates.get_loc(start_date) - min_extra_rows\n            if current_start_pos < 0:\n                raise NoFurtherDataError.from_lookback_window(\n                    initial_message=\"Insufficient data to compute Pipeline:\",\n                    first_date=all_dates[0],\n                    lookback_start=start_date,\n                    lookback_length=min_extra_rows,\n                )\n        except KeyError:\n            before, after = nearest_unequal_elements(all_dates, start_date)\n            raise ValueError(\n                \"Pipeline start_date {start_date} is not in calendar.\\n\"\n                \"Latest date before start_date is {before}.\\n\"\n                \"Earliest date after start_date is {after}.\".format(\n                    start_date=start_date,\n                    before=before,\n                    after=after,\n                )\n            )\n\n        # Our possible target dates are all the dates on or before the current\n        # starting position.\n        # TODO: Consider bounding this below by self.window_length\n        candidates = all_dates[:current_start_pos + 1]\n\n        # Choose the latest date in the candidates that is the start of a new\n        # period at our frequency.\n        choices = select_sampling_indices(candidates, self._frequency)\n\n        # If we have choices, the last choice is the first date if the\n        # period containing current_start_date.  Choose it.\n        new_start_date = candidates[choices[-1]]\n\n        # Add the difference between the new and old start dates to get the\n        # number of rows for the new start_date.\n        new_start_pos = all_dates.get_loc(new_start_date)\n        assert new_start_pos <= current_start_pos, \\\n            \"Computed negative extra rows!\"\n\n        return min_extra_rows + (current_start_pos - new_start_pos)", "response": "This method computes the number of extra rows required to make a new simulation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute(self, inputs, dates, assets, mask):\n        to_sample = dates[select_sampling_indices(dates, self._frequency)]\n        assert to_sample[0] == dates[0], \\\n            \"Misaligned sampling dates in %s.\" % type(self).__name__\n\n        real_compute = self._wrapped_term._compute\n\n        # Inputs will contain different kinds of values depending on whether or\n        # not we're a windowed computation.\n\n        # If we're windowed, then `inputs` is a list of iterators of ndarrays.\n        # If we're not windowed, then `inputs` is just a list of ndarrays.\n        # There are two things we care about doing with the input:\n        #   1. Preparing an input to be passed to our wrapped term.\n        #   2. Skipping an input if we're going to use an already-computed row.\n        # We perform these actions differently based on the expected kind of\n        # input, and we encapsulate these actions with closures so that we\n        # don't clutter the code below with lots of branching.\n        if self.windowed:\n            # If we're windowed, inputs are stateful AdjustedArrays.  We don't\n            # need to do any preparation before forwarding to real_compute, but\n            # we need to call `next` on them if we want to skip an iteration.\n            def prepare_inputs():\n                return inputs\n\n            def skip_this_input():\n                for w in inputs:\n                    next(w)\n        else:\n            # If we're not windowed, inputs are just ndarrays.  We need to\n            # slice out a single row when forwarding to real_compute, but we\n            # don't need to do anything to skip an input.\n            def prepare_inputs():\n                # i is the loop iteration variable below.\n                return [a[[i]] for a in inputs]\n\n            def skip_this_input():\n                pass\n\n        results = []\n        samples = iter(to_sample)\n        next_sample = next(samples)\n        for i, compute_date in enumerate(dates):\n            if next_sample == compute_date:\n                results.append(\n                    real_compute(\n                        prepare_inputs(),\n                        dates[i:i + 1],\n                        assets,\n                        mask[i:i + 1],\n                    )\n                )\n                try:\n                    next_sample = next(samples)\n                except StopIteration:\n                    # No more samples to take. Set next_sample to Nat, which\n                    # compares False with any other datetime.\n                    next_sample = pd_NaT\n            else:\n                skip_this_input()\n                # Copy results from previous sample period.\n                results.append(results[-1])\n\n        # We should have exhausted our sample dates.\n        try:\n            next_sample = next(samples)\n        except StopIteration:\n            pass\n        else:\n            raise AssertionError(\"Unconsumed sample date: %s\" % next_sample)\n\n        # Concatenate stored results.\n        return vstack(results)", "response": "Compute the term for the given dates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_downsampled_type(cls, other_base):\n        docstring = dedent(\n            \"\"\"\n            A {t} that defers to another {t} at lower-than-daily frequency.\n\n            Parameters\n            ----------\n            term : {t}\n            {{frequency}}\n            \"\"\"\n        ).format(t=other_base.__name__)\n\n        doc = format_docstring(\n            owner_name=other_base.__name__,\n            docstring=docstring,\n            formatters={'frequency': PIPELINE_DOWNSAMPLING_FREQUENCY_DOC},\n        )\n\n        return type(\n            'Downsampled' + other_base.__name__,\n            (cls, other_base,),\n            {'__doc__': doc,\n             '__module__': other_base.__module__},\n        )", "response": "Returns a type that can be used to create a Downsampled object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap a function in a processor that calls `f` on the argument before passing it along. Useful for creating simple arguments to the `@preprocess` decorator. Parameters ---------- f : function Function accepting a single argument and returning a replacement. Examples -------- >>> @preprocess(x=call(lambda x: x + 1)) ... def foo(x): ... return x ... >>> foo(1) 2", "response": "def call(f):\n    \"\"\"\n    Wrap a function in a processor that calls `f` on the argument before\n    passing it along.\n\n    Useful for creating simple arguments to the `@preprocess` decorator.\n\n    Parameters\n    ----------\n    f : function\n        Function accepting a single argument and returning a replacement.\n\n    Examples\n    --------\n    >>> @preprocess(x=call(lambda x: x + 1))\n    ... def foo(x):\n    ...     return x\n    ...\n    >>> foo(1)\n    2\n    \"\"\"\n    @wraps(f)\n    def processor(func, argname, arg):\n        return f(arg)\n    return processor"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a preprocessed function that has the same signature as func.", "response": "def _build_preprocessed_function(func,\n                                 processors,\n                                 args_defaults,\n                                 varargs,\n                                 varkw):\n    \"\"\"\n    Build a preprocessed function with the same signature as `func`.\n\n    Uses `exec` internally to build a function that actually has the same\n    signature as `func.\n    \"\"\"\n    format_kwargs = {'func_name': func.__name__}\n\n    def mangle(name):\n        return 'a' + uuid4().hex + name\n\n    format_kwargs['mangled_func'] = mangled_funcname = mangle(func.__name__)\n\n    def make_processor_assignment(arg, processor_name):\n        template = \"{arg} = {processor}({func}, '{arg}', {arg})\"\n        return template.format(\n            arg=arg,\n            processor=processor_name,\n            func=mangled_funcname,\n        )\n\n    exec_globals = {mangled_funcname: func, 'wraps': wraps}\n    defaults_seen = 0\n    default_name_template = 'a' + uuid4().hex + '_%d'\n    signature = []\n    call_args = []\n    assignments = []\n    star_map = {\n        varargs: '*',\n        varkw: '**',\n    }\n\n    def name_as_arg(arg):\n        return star_map.get(arg, '') + arg\n\n    for arg, default in args_defaults:\n        if default is NO_DEFAULT:\n            signature.append(name_as_arg(arg))\n        else:\n            default_name = default_name_template % defaults_seen\n            exec_globals[default_name] = default\n            signature.append('='.join([name_as_arg(arg), default_name]))\n            defaults_seen += 1\n\n        if arg in processors:\n            procname = mangle('_processor_' + arg)\n            exec_globals[procname] = processors[arg]\n            assignments.append(make_processor_assignment(arg, procname))\n\n        call_args.append(name_as_arg(arg))\n\n    exec_str = dedent(\n        \"\"\"\\\n        @wraps({wrapped_funcname})\n        def {func_name}({signature}):\n            {assignments}\n            return {wrapped_funcname}({call_args})\n        \"\"\"\n    ).format(\n        func_name=func.__name__,\n        signature=', '.join(signature),\n        assignments='\\n    '.join(assignments),\n        wrapped_funcname=mangled_funcname,\n        call_args=', '.join(call_args),\n    )\n    compiled = compile(\n        exec_str,\n        func.__code__.co_filename,\n        mode='exec',\n    )\n\n    exec_locals = {}\n    exec_(compiled, exec_globals, exec_locals)\n    new_func = exec_locals[func.__name__]\n\n    code = new_func.__code__\n    args = {\n        attr: getattr(code, attr)\n        for attr in dir(code)\n        if attr.startswith('co_')\n    }\n    # Copy the firstlineno out of the underlying function so that exceptions\n    # get raised with the correct traceback.\n    # This also makes dynamic source inspection (like IPython `??` operator)\n    # work as intended.\n    try:\n        # Try to get the pycode object from the underlying function.\n        original_code = func.__code__\n    except AttributeError:\n        try:\n            # The underlying callable was not a function, try to grab the\n            # `__func__.__code__` which exists on method objects.\n            original_code = func.__func__.__code__\n        except AttributeError:\n            # The underlying callable does not have a `__code__`. There is\n            # nothing for us to correct.\n            return new_func\n\n    args['co_firstlineno'] = original_code.co_firstlineno\n    new_func.__code__ = CodeType(*map(getitem(args), _code_argorder))\n    return new_func"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a Series of benchmark returns from IEX associated with symbol.", "response": "def get_benchmark_returns(symbol):\n    \"\"\"\n    Get a Series of benchmark returns from IEX associated with `symbol`.\n    Default is `SPY`.\n\n    Parameters\n    ----------\n    symbol : str\n        Benchmark symbol for which we're getting the returns.\n\n    The data is provided by IEX (https://iextrading.com/), and we can\n    get up to 5 years worth of data.\n    \"\"\"\n    r = requests.get(\n        'https://api.iextrading.com/1.0/stock/{}/chart/5y'.format(symbol)\n    )\n    data = r.json()\n\n    df = pd.DataFrame(data)\n\n    df.index = pd.DatetimeIndex(df['date'])\n    df = df['close']\n\n    return df.sort_index().tz_localize('UTC').pct_change(1).iloc[1:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delimit(delimiters, content):\n    if len(delimiters) != 2:\n        raise ValueError(\n            \"`delimiters` must be of length 2. Got %r\" % delimiters\n        )\n    return ''.join([delimiters[0], content, delimiters[1]])", "response": "Returns a new string with the first and last characters of delimiters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting nodes from graph G with indegree 0", "response": "def roots(g):\n    \"Get nodes from graph G with indegree 0\"\n    return set(n for n, d in iteritems(g.in_degree()) if d == 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _render(g, out, format_, include_asset_exists=False):\n    graph_attrs = {'rankdir': 'TB', 'splines': 'ortho'}\n    cluster_attrs = {'style': 'filled', 'color': 'lightgoldenrod1'}\n\n    in_nodes = g.loadable_terms\n    out_nodes = list(g.outputs.values())\n\n    f = BytesIO()\n    with graph(f, \"G\", **graph_attrs):\n\n        # Write outputs cluster.\n        with cluster(f, 'Output', labelloc='b', **cluster_attrs):\n            for term in filter_nodes(include_asset_exists, out_nodes):\n                add_term_node(f, term)\n\n        # Write inputs cluster.\n        with cluster(f, 'Input', **cluster_attrs):\n            for term in filter_nodes(include_asset_exists, in_nodes):\n                add_term_node(f, term)\n\n        # Write intermediate results.\n        for term in filter_nodes(include_asset_exists,\n                                 topological_sort(g.graph)):\n            if term in in_nodes or term in out_nodes:\n                continue\n            add_term_node(f, term)\n\n        # Write edges\n        for source, dest in g.graph.edges():\n            if source is AssetExists() and not include_asset_exists:\n                continue\n            add_edge(f, id(source), id(dest))\n\n    cmd = ['dot', '-T', format_]\n    try:\n        proc = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            raise RuntimeError(\n                \"Couldn't find `dot` graph layout program. \"\n                \"Make sure Graphviz is installed and `dot` is on your path.\"\n            )\n        else:\n            raise\n\n    f.seek(0)\n    proc_stdout, proc_stderr = proc.communicate(f.read())\n    if proc_stderr:\n        raise RuntimeError(\n            \"Error(s) while rendering graph: %s\" % proc_stderr.decode('utf-8')\n        )\n\n    out.write(proc_stdout)", "response": "Draw g as a graph to out in format format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay a TermGraph interactively from within IPython.", "response": "def display_graph(g, format='svg', include_asset_exists=False):\n    \"\"\"\n    Display a TermGraph interactively from within IPython.\n    \"\"\"\n    try:\n        import IPython.display as display\n    except ImportError:\n        raise NoIPython(\"IPython is not installed.  Can't display graph.\")\n\n    if format == 'svg':\n        display_cls = display.SVG\n    elif format in (\"jpeg\", \"png\"):\n        display_cls = partial(display.Image, format=format, embed=True)\n\n    out = BytesIO()\n    _render(g, out, format, include_asset_exists=include_asset_exists)\n    return display_cls(data=out.getvalue())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_attrs(attrs):\n    if not attrs:\n        return ''\n    entries = ['='.join((key, value)) for key, value in iteritems(attrs)]\n    return '[' + ', '.join(entries) + ']'", "response": "Format key value pairs from attrs into graphviz attrs format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying a function but emulate the API of an asynchronous call.", "response": "def apply_async(f, args=(), kwargs=None, callback=None):\n        \"\"\"Apply a function but emulate the API of an asynchronous call.\n\n        Parameters\n        ----------\n        f : callable\n            The function to call.\n        args : tuple, optional\n            The positional arguments.\n        kwargs : dict, optional\n            The keyword arguments.\n\n        Returns\n        -------\n        future : ApplyAsyncResult\n            The result of calling the function boxed in a future-like api.\n\n        Notes\n        -----\n        This calls the function eagerly but wraps it so that ``SequentialPool``\n        can be used where a :class:`multiprocessing.Pool` or\n        :class:`gevent.pool.Pool` would be used.\n        \"\"\"\n        try:\n            value = (identity if callback is None else callback)(\n                f(*args, **kwargs or {}),\n            )\n            successful = True\n        except Exception as e:\n            value = e\n            successful = False\n\n        return ApplyAsyncResult(value, successful)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main(extension, strict_extensions, default_extension, x):\n\n    # install a logbook handler before performing any other operations\n    logbook.StderrHandler().push_application()\n    create_args(x, zipline.extension_args)\n    load_extensions(\n        default_extension,\n        extension,\n        strict_extensions,\n        os.environ,\n    )", "response": "Top level zipline entry point."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmark that an option should only be exposed in IPython mode.", "response": "def ipython_only(option):\n    \"\"\"Mark that an option should only be exposed in IPython.\n\n    Parameters\n    ----------\n    option : decorator\n        A click.option decorator.\n\n    Returns\n    -------\n    ipython_only_dec : decorator\n        A decorator that correctly applies the argument even when not\n        using IPython mode.\n    \"\"\"\n    if __IPYTHON__:\n        return option\n\n    argname = extract_option_object(option).name\n\n    def d(f):\n        @wraps(f)\n        def _(*args, **kwargs):\n            kwargs[argname] = None\n            return f(*args, **kwargs)\n        return _\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(ctx,\n        algofile,\n        algotext,\n        define,\n        data_frequency,\n        capital_base,\n        bundle,\n        bundle_timestamp,\n        start,\n        end,\n        output,\n        trading_calendar,\n        print_algo,\n        metrics_set,\n        local_namespace,\n        blotter):\n    \"\"\"Run a backtest for the given algorithm.\n    \"\"\"\n    # check that the start and end dates are passed correctly\n    if start is None and end is None:\n        # check both at the same time to avoid the case where a user\n        # does not pass either of these and then passes the first only\n        # to be told they need to pass the second argument also\n        ctx.fail(\n            \"must specify dates with '-s' / '--start' and '-e' / '--end'\",\n        )\n    if start is None:\n        ctx.fail(\"must specify a start date with '-s' / '--start'\")\n    if end is None:\n        ctx.fail(\"must specify an end date with '-e' / '--end'\")\n\n    if (algotext is not None) == (algofile is not None):\n        ctx.fail(\n            \"must specify exactly one of '-f' / '--algofile' or\"\n            \" '-t' / '--algotext'\",\n        )\n\n    trading_calendar = get_calendar(trading_calendar)\n\n    perf = _run(\n        initialize=None,\n        handle_data=None,\n        before_trading_start=None,\n        analyze=None,\n        algofile=algofile,\n        algotext=algotext,\n        defines=define,\n        data_frequency=data_frequency,\n        capital_base=capital_base,\n        bundle=bundle,\n        bundle_timestamp=bundle_timestamp,\n        start=start,\n        end=end,\n        output=output,\n        trading_calendar=trading_calendar,\n        print_algo=print_algo,\n        metrics_set=metrics_set,\n        local_namespace=local_namespace,\n        environ=os.environ,\n        blotter=blotter,\n        benchmark_returns=None,\n    )\n\n    if output == '-':\n        click.echo(str(perf))\n    elif output != os.devnull:  # make the zipline magic not write any data\n        perf.to_pickle(output)\n\n    return perf", "response": "Run a backtest for the given algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zipline_magic(line, cell=None):\n    load_extensions(\n        default=True,\n        extensions=[],\n        strict=True,\n        environ=os.environ,\n    )\n    try:\n        return run.main(\n            # put our overrides at the start of the parameter list so that\n            # users may pass values with higher precedence\n            [\n                '--algotext', cell,\n                '--output', os.devnull,  # don't write the results by default\n            ] + ([\n                # these options are set when running in line magic mode\n                # set a non None algo text to use the ipython user_ns\n                '--algotext', '',\n                '--local-namespace',\n            ] if cell is None else []) + line.split(),\n            '%s%%zipline' % ((cell or '') and '%'),\n            # don't use system exit and propogate errors to the caller\n            standalone_mode=False,\n        )\n    except SystemExit as e:\n        # https://github.com/mitsuhiko/click/pull/533\n        # even in standalone_mode=False `--help` really wants to kill us ;_;\n        if e.code:\n            raise ValueError('main returned non-zero status code: %d' % e.code)", "response": "The zipline IPython cell magic."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ningest the data for the given bundle.", "response": "def ingest(bundle, assets_version, show_progress):\n    \"\"\"Ingest the data for the given bundle.\n    \"\"\"\n    bundles_module.ingest(\n        bundle,\n        os.environ,\n        pd.Timestamp.utcnow(),\n        assets_version,\n        show_progress,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clean(bundle, before, after, keep_last):\n    bundles_module.clean(\n        bundle,\n        before,\n        after,\n        keep_last,\n    )", "response": "Clean up data downloaded with the ingest command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bundles():\n    for bundle in sorted(bundles_module.bundles.keys()):\n        if bundle.startswith('.'):\n            # hide the test data\n            continue\n        try:\n            ingestions = list(\n                map(text_type, bundles_module.ingestions_for_bundle(bundle))\n            )\n        except OSError as e:\n            if e.errno != errno.ENOENT:\n                raise\n            ingestions = []\n\n        # If we got no ingestions, either because the directory didn't exist or\n        # because there were no entries, print a single message indicating that\n        # no ingestions have yet been made.\n        for timestamp in ingestions or [\"<no ingestions>\"]:\n            click.echo(\"%s %s\" % (bundle, timestamp))", "response": "List all of the available data bundles."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a function that can be used to make a binary operator on a filter subclass.", "response": "def binary_operator(op):\n    \"\"\"\n    Factory function for making binary operator methods on a Filter subclass.\n\n    Returns a function \"binary_operator\" suitable for implementing functions\n    like __and__ or __or__.\n    \"\"\"\n    # When combining a Filter with a NumericalExpression, we use this\n    # attrgetter instance to defer to the commuted interpretation of the\n    # NumericalExpression operator.\n    commuted_method_getter = attrgetter(method_name_for_op(op, commute=True))\n\n    def binary_operator(self, other):\n        if isinstance(self, NumericalExpression):\n            self_expr, other_expr, new_inputs = self.build_binary_op(\n                op, other,\n            )\n            return NumExprFilter.create(\n                \"({left}) {op} ({right})\".format(\n                    left=self_expr,\n                    op=op,\n                    right=other_expr,\n                ),\n                new_inputs,\n            )\n        elif isinstance(other, NumericalExpression):\n            # NumericalExpression overrides numerical ops to correctly handle\n            # merging of inputs.  Look up and call the appropriate\n            # right-binding operator with ourself as the input.\n            return commuted_method_getter(other)(self)\n        elif isinstance(other, Term):\n            if other.dtype != bool_dtype:\n                raise BadBinaryOperator(op, self, other)\n            if self is other:\n                return NumExprFilter.create(\n                    \"x_0 {op} x_0\".format(op=op),\n                    (self,),\n                )\n            return NumExprFilter.create(\n                \"x_0 {op} x_1\".format(op=op),\n                (self, other),\n            )\n        elif isinstance(other, int):  # Note that this is true for bool as well\n            return NumExprFilter.create(\n                \"x_0 {op} {constant}\".format(op=op, constant=int(other)),\n                binds=(self,),\n            )\n        raise BadBinaryOperator(op, self, other)\n\n    binary_operator.__doc__ = \"Binary Operator: '%s'\" % op\n    return binary_operator"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a factory function for making unary operator methods for Filters.", "response": "def unary_operator(op):\n    \"\"\"\n    Factory function for making unary operator methods for Filters.\n    \"\"\"\n    valid_ops = {'~'}\n    if op not in valid_ops:\n        raise ValueError(\"Invalid unary operator %s.\" % op)\n\n    def unary_operator(self):\n        # This can't be hoisted up a scope because the types returned by\n        # unary_op_return_type aren't defined when the top-level function is\n        # invoked.\n        if isinstance(self, NumericalExpression):\n            return NumExprFilter.create(\n                \"{op}({expr})\".format(op=op, expr=self._expr),\n                self.inputs,\n            )\n        else:\n            return NumExprFilter.create(\"{op}x_0\".format(op=op), (self,))\n\n    unary_operator.__doc__ = \"Unary Operator: '%s'\" % op\n    return unary_operator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(cls, expr, binds):\n        return cls(expr=expr, binds=binds, dtype=bool_dtype)", "response": "Create a new instance of the correct class based on expr and binds."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes our result with numexpr then re - apply mask.", "response": "def _compute(self, arrays, dates, assets, mask):\n        \"\"\"\n        Compute our result with numexpr, then re-apply `mask`.\n        \"\"\"\n        return super(NumExprFilter, self)._compute(\n            arrays,\n            dates,\n            assets,\n            mask,\n        ) & mask"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nensures that our percentile bounds are well - formed.", "response": "def _validate(self):\n        \"\"\"\n        Ensure that our percentile bounds are well-formed.\n        \"\"\"\n        if not 0.0 <= self._min_percentile < self._max_percentile <= 100.0:\n            raise BadPercentileBounds(\n                min_percentile=self._min_percentile,\n                max_percentile=self._max_percentile,\n                upper_bound=100.0\n            )\n        return super(PercentileFilter, self)._validate()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute(self, arrays, dates, assets, mask):\n        # TODO: Review whether there's a better way of handling small numbers\n        # of columns.\n        data = arrays[0].copy().astype(float64)\n        data[~mask] = nan\n\n        # FIXME: np.nanpercentile **should** support computing multiple bounds\n        # at once, but there's a bug in the logic for multiple bounds in numpy\n        # 1.9.2.  It will be fixed in 1.10.\n        # c.f. https://github.com/numpy/numpy/pull/5981\n        lower_bounds = nanpercentile(\n            data,\n            self._min_percentile,\n            axis=1,\n            keepdims=True,\n        )\n        upper_bounds = nanpercentile(\n            data,\n            self._max_percentile,\n            axis=1,\n            keepdims=True,\n        )\n        return (lower_bounds <= data) & (data <= upper_bounds)", "response": "Compute the mask of all values falling between the given percentiles."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_treasury_csv_column(column):\n    column_re = re.compile(\n        r\"^(?P<prefix>RIFLGFC)\"\n        \"(?P<unit>[YM])\"\n        \"(?P<periods>[0-9]{2})\"\n        \"(?P<suffix>_N.B)$\"\n    )\n\n    match = column_re.match(column)\n    if match is None:\n        raise ValueError(\"Couldn't parse CSV column %r.\" % column)\n    unit, periods = get_unit_and_periods(match.groupdict())\n\n    # Roundtrip through int to coerce '06' into '6'.\n    return str(int(periods)) + ('year' if unit == 'Y' else 'month')", "response": "Parses a treasury CSV column into a more human - readable format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload daily 10 year treasury rates from the Federal Reserve and return a pandas. Series.", "response": "def get_daily_10yr_treasury_data():\n    \"\"\"Download daily 10 year treasury rates from the Federal Reserve and\n    return a pandas.Series.\"\"\"\n    url = \"https://www.federalreserve.gov/datadownload/Output.aspx?rel=H15\" \\\n          \"&series=bcb44e57fb57efbe90002369321bfb3f&lastObs=&from=&to=\" \\\n          \"&filetype=csv&label=include&layout=seriescolumn\"\n    return pd.read_csv(url, header=5, index_col=0, names=['DATE', 'BC_10YEAR'],\n                       parse_dates=True, converters={1: dataconverter},\n                       squeeze=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sid_subdir_path(sid):\n    padded_sid = format(sid, '06')\n    return os.path.join(\n        # subdir 1 00/XX\n        padded_sid[0:2],\n        # subdir 2 XX/00\n        padded_sid[2:4],\n        \"{0}.bcolz\".format(str(padded_sid))\n    )", "response": "Return a path to the bcolz subdirectory for the given sid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_cols(cols, scale_factor, sid, invalid_data_behavior):\n    scaled_opens = (np.nan_to_num(cols['open']) * scale_factor).round()\n    scaled_highs = (np.nan_to_num(cols['high']) * scale_factor).round()\n    scaled_lows = (np.nan_to_num(cols['low']) * scale_factor).round()\n    scaled_closes = (np.nan_to_num(cols['close']) * scale_factor).round()\n\n    exclude_mask = np.zeros_like(scaled_opens, dtype=bool)\n\n    for col_name, scaled_col in [\n        ('open', scaled_opens),\n        ('high', scaled_highs),\n        ('low', scaled_lows),\n        ('close', scaled_closes),\n    ]:\n        max_val = scaled_col.max()\n\n        try:\n            check_uint32_safe(max_val, col_name)\n        except ValueError:\n            if invalid_data_behavior == 'raise':\n                raise\n\n            if invalid_data_behavior == 'warn':\n                logger.warn(\n                    'Values for sid={}, col={} contain some too large for '\n                    'uint32 (max={}), filtering them out',\n                    sid, col_name, max_val,\n                )\n\n            # We want to exclude all rows that have an unsafe value in\n            # this column.\n            exclude_mask &= (scaled_col >= np.iinfo(np.uint32).max)\n\n    # Convert all cols to uint32.\n    opens = scaled_opens.astype(np.uint32)\n    highs = scaled_highs.astype(np.uint32)\n    lows = scaled_lows.astype(np.uint32)\n    closes = scaled_closes.astype(np.uint32)\n    volumes = cols['volume'].astype(np.uint32)\n\n    # Exclude rows with unsafe values by setting to zero.\n    opens[exclude_mask] = 0\n    highs[exclude_mask] = 0\n    lows[exclude_mask] = 0\n    closes[exclude_mask] = 0\n    volumes[exclude_mask] = 0\n\n    return opens, highs, lows, closes, volumes", "response": "Adapt OHLCV columns into uint32 columns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the metadata to a JSON file in the rootdir.", "response": "def write(self, rootdir):\n        \"\"\"\n        Write the metadata to a JSON file in the rootdir.\n\n        Values contained in the metadata are:\n\n        version : int\n            The value of FORMAT_VERSION of this class.\n        ohlc_ratio : int\n            The default ratio by which to multiply the pricing data to\n            convert the floats from floats to an integer to fit within\n            the np.uint32. If ohlc_ratios_per_sid is None or does not\n            contain a mapping for a given sid, this ratio is used.\n        ohlc_ratios_per_sid : dict\n             A dict mapping each sid in the output to the factor by\n             which the pricing data is multiplied so that the float data\n             can be stored as an integer.\n        minutes_per_day : int\n            The number of minutes per each period.\n        calendar_name : str\n            The name of the TradingCalendar on which the minute bars are\n            based.\n        start_session : datetime\n            'YYYY-MM-DD' formatted representation of the first trading\n            session in the data set.\n        end_session : datetime\n            'YYYY-MM-DD' formatted representation of the last trading\n            session in the data set.\n\n        Deprecated, but included for backwards compatibility:\n\n        first_trading_day : string\n            'YYYY-MM-DD' formatted representation of the first trading day\n             available in the dataset.\n        market_opens : list\n            List of int64 values representing UTC market opens as\n            minutes since epoch.\n        market_closes : list\n            List of int64 values representing UTC market closes as\n            minutes since epoch.\n        \"\"\"\n\n        calendar = self.calendar\n        slicer = calendar.schedule.index.slice_indexer(\n            self.start_session,\n            self.end_session,\n        )\n        schedule = calendar.schedule[slicer]\n        market_opens = schedule.market_open\n        market_closes = schedule.market_close\n\n        metadata = {\n            'version': self.version,\n            'ohlc_ratio': self.default_ohlc_ratio,\n            'ohlc_ratios_per_sid': self.ohlc_ratios_per_sid,\n            'minutes_per_day': self.minutes_per_day,\n            'calendar_name': self.calendar.name,\n            'start_session': str(self.start_session.date()),\n            'end_session': str(self.end_session.date()),\n            # Write these values for backwards compatibility\n            'first_trading_day': str(self.start_session.date()),\n            'market_opens': (\n                market_opens.values.astype('datetime64[m]').\n                astype(np.int64).tolist()),\n            'market_closes': (\n                market_closes.values.astype('datetime64[m]').\n                astype(np.int64).tolist()),\n        }\n        with open(self.metadata_path(rootdir), 'w+') as fp:\n            json.dump(metadata, fp)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen an existing BcolzMinuteBarWriter for writing.", "response": "def open(cls, rootdir, end_session=None):\n        \"\"\"\n        Open an existing ``rootdir`` for writing.\n\n        Parameters\n        ----------\n        end_session : Timestamp (optional)\n            When appending, the intended new ``end_session``.\n        \"\"\"\n        metadata = BcolzMinuteBarMetadata.read(rootdir)\n        return BcolzMinuteBarWriter(\n            rootdir,\n            metadata.calendar,\n            metadata.start_session,\n            end_session if end_session is not None else metadata.end_session,\n            metadata.minutes_per_day,\n            metadata.default_ohlc_ratio,\n            metadata.ohlc_ratios_per_sid,\n            write_metadata=end_session is not None\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the full path to the bcolz rootdir for the given asset identifier.", "response": "def sidpath(self, sid):\n        \"\"\"\n        Parameters\n        ----------\n        sid : int\n            Asset identifier.\n\n        Returns\n        -------\n        out : string\n            Full path to the bcolz rootdir for the given sid.\n        \"\"\"\n        sid_subdir = _sid_subdir_path(sid)\n        return join(self._rootdir, sid_subdir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef last_date_in_output_for_sid(self, sid):\n        sizes_path = \"{0}/close/meta/sizes\".format(self.sidpath(sid))\n        if not os.path.exists(sizes_path):\n            return pd.NaT\n        with open(sizes_path, mode='r') as f:\n            sizes = f.read()\n        data = json.loads(sizes)\n        # use integer division so that the result is an int\n        # for pandas index later https://github.com/pandas-dev/pandas/blob/master/pandas/tseries/base.py#L247 # noqa\n        num_days = data['shape'][0] // self._minutes_per_day\n        if num_days == 0:\n            # empty container\n            return pd.NaT\n        return self._session_labels[num_days - 1]", "response": "Returns the midnight of the last date written in to the output for the given sid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate empty ctable for given path.", "response": "def _init_ctable(self, path):\n        \"\"\"\n        Create empty ctable for given path.\n\n        Parameters\n        ----------\n        path : string\n            The path to rootdir of the new ctable.\n        \"\"\"\n        # Only create the containing subdir on creation.\n        # This is not to be confused with the `.bcolz` directory, but is the\n        # directory up one level from the `.bcolz` directories.\n        sid_containing_dirname = os.path.dirname(path)\n        if not os.path.exists(sid_containing_dirname):\n            # Other sids may have already created the containing directory.\n            os.makedirs(sid_containing_dirname)\n        initial_array = np.empty(0, np.uint32)\n        table = ctable(\n            rootdir=path,\n            columns=[\n                initial_array,\n                initial_array,\n                initial_array,\n                initial_array,\n                initial_array,\n            ],\n            names=[\n                'open',\n                'high',\n                'low',\n                'close',\n                'volume'\n            ],\n            expectedlen=self._expectedlen,\n            mode='w',\n        )\n        table.flush()\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that a ctable exists for sid then return it.", "response": "def _ensure_ctable(self, sid):\n        \"\"\"Ensure that a ctable exists for ``sid``, then return it.\"\"\"\n        sidpath = self.sidpath(sid)\n        if not os.path.exists(sidpath):\n            return self._init_ctable(sidpath)\n        return bcolz.ctable(rootdir=sidpath, mode='a')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pad(self, sid, date):\n        table = self._ensure_ctable(sid)\n\n        last_date = self.last_date_in_output_for_sid(sid)\n\n        tds = self._session_labels\n\n        if date <= last_date or date < tds[0]:\n            # No need to pad.\n            return\n\n        if last_date == pd.NaT:\n            # If there is no data, determine how many days to add so that\n            # desired days are written to the correct slots.\n            days_to_zerofill = tds[tds.slice_indexer(end=date)]\n        else:\n            days_to_zerofill = tds[tds.slice_indexer(\n                start=last_date + tds.freq,\n                end=date)]\n\n        self._zerofill(table, len(days_to_zerofill))\n\n        new_last_date = self.last_date_in_output_for_sid(sid)\n        assert new_last_date == date, \"new_last_date={0} != date={1}\".format(\n            new_last_date, date)", "response": "Fills sid container with empty data through the specified date."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites all the supplied kwargs as attributes of the sid s file.", "response": "def set_sid_attrs(self, sid, **kwargs):\n        \"\"\"Write all the supplied kwargs as attributes of the sid's file.\n        \"\"\"\n        table = self._ensure_ctable(sid)\n        for k, v in kwargs.items():\n            table.attrs[k] = v"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, data, show_progress=False, invalid_data_behavior='warn'):\n        ctx = maybe_show_progress(\n            data,\n            show_progress=show_progress,\n            item_show_func=lambda e: e if e is None else str(e[0]),\n            label=\"Merging minute equity files:\",\n        )\n        write_sid = self.write_sid\n        with ctx as it:\n            for e in it:\n                write_sid(*e, invalid_data_behavior=invalid_data_behavior)", "response": "Write a series of market minutes data to the log file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_sid(self, sid, df, invalid_data_behavior='warn'):\n        cols = {\n            'open': df.open.values,\n            'high': df.high.values,\n            'low': df.low.values,\n            'close': df.close.values,\n            'volume': df.volume.values,\n        }\n        dts = df.index.values\n        # Call internal method, since DataFrame has already ensured matching\n        # index and value lengths.\n        self._write_cols(sid, dts, cols, invalid_data_behavior)", "response": "Writes the OHLCV data for the given sid to the bcolz ctable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the columns of the given attributes to the bcolz ctable.", "response": "def write_cols(self, sid, dts, cols, invalid_data_behavior='warn'):\n        \"\"\"\n        Write the OHLCV data for the given sid.\n        If there is no bcolz ctable yet created for the sid, create it.\n        If the length of the bcolz ctable is not exactly to the date before\n        the first day provided, fill the ctable with 0s up to that date.\n\n        Parameters\n        ----------\n        sid : int\n            The asset identifier for the data being written.\n        dts : datetime64 array\n            The dts corresponding to values in cols.\n        cols : dict of str -> np.array\n            dict of market data with the following characteristics.\n            keys are ('open', 'high', 'low', 'close', 'volume')\n            open : float64\n            high : float64\n            low  : float64\n            close : float64\n            volume : float64|int64\n        \"\"\"\n        if not all(len(dts) == len(cols[name]) for name in self.COL_NAMES):\n            raise BcolzMinuteWriterColumnMismatch(\n                \"Length of dts={0} should match cols: {1}\".format(\n                    len(dts),\n                    \" \".join(\"{0}={1}\".format(name, len(cols[name]))\n                             for name in self.COL_NAMES)))\n        self._write_cols(sid, dts, cols, invalid_data_behavior)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write_cols(self, sid, dts, cols, invalid_data_behavior):\n        table = self._ensure_ctable(sid)\n\n        tds = self._session_labels\n        input_first_day = self._calendar.minute_to_session_label(\n            pd.Timestamp(dts[0]), direction='previous')\n\n        last_date = self.last_date_in_output_for_sid(sid)\n\n        day_before_input = input_first_day - tds.freq\n\n        self.pad(sid, day_before_input)\n        table = self._ensure_ctable(sid)\n\n        # Get the number of minutes already recorded in this sid's ctable\n        num_rec_mins = table.size\n\n        all_minutes = self._minute_index\n        # Get the latest minute we wish to write to the ctable\n        last_minute_to_write = pd.Timestamp(dts[-1], tz='UTC')\n\n        # In the event that we've already written some minutely data to the\n        # ctable, guard against overwriting that data.\n        if num_rec_mins > 0:\n            last_recorded_minute = all_minutes[num_rec_mins - 1]\n            if last_minute_to_write <= last_recorded_minute:\n                raise BcolzMinuteOverlappingData(dedent(\"\"\"\n                Data with last_date={0} already includes input start={1} for\n                sid={2}\"\"\".strip()).format(last_date, input_first_day, sid))\n\n        latest_min_count = all_minutes.get_loc(last_minute_to_write)\n\n        # Get all the minutes we wish to write (all market minutes after the\n        # latest currently written, up to and including last_minute_to_write)\n        all_minutes_in_window = all_minutes[num_rec_mins:latest_min_count + 1]\n\n        minutes_count = all_minutes_in_window.size\n\n        open_col = np.zeros(minutes_count, dtype=np.uint32)\n        high_col = np.zeros(minutes_count, dtype=np.uint32)\n        low_col = np.zeros(minutes_count, dtype=np.uint32)\n        close_col = np.zeros(minutes_count, dtype=np.uint32)\n        vol_col = np.zeros(minutes_count, dtype=np.uint32)\n\n        dt_ixs = np.searchsorted(all_minutes_in_window.values,\n                                 dts.astype('datetime64[ns]'))\n\n        ohlc_ratio = self.ohlc_ratio_for_sid(sid)\n\n        (\n            open_col[dt_ixs],\n            high_col[dt_ixs],\n            low_col[dt_ixs],\n            close_col[dt_ixs],\n            vol_col[dt_ixs],\n        ) = convert_cols(cols, ohlc_ratio, sid, invalid_data_behavior)\n\n        table.append([\n            open_col,\n            high_col,\n            low_col,\n            close_col,\n            vol_col\n        ])\n        table.flush()", "response": "Internal method for writing columns to the ctable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data_len_for_day(self, day):\n        day_ix = self._session_labels.get_loc(day)\n        # Add one to the 0-indexed day_ix to get the number of days.\n        num_days = day_ix + 1\n        return num_days * self._minutes_per_day", "response": "Return the number of data points up to and including the provided day."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntruncates data beyond this date in all ctables.", "response": "def truncate(self, date):\n        \"\"\"Truncate data beyond this date in all ctables.\"\"\"\n        truncate_slice_end = self.data_len_for_day(date)\n\n        glob_path = os.path.join(self._rootdir, \"*\", \"*\", \"*.bcolz\")\n        sid_paths = sorted(glob(glob_path))\n\n        for sid_path in sid_paths:\n            file_name = os.path.basename(sid_path)\n\n            try:\n                table = bcolz.open(rootdir=sid_path)\n            except IOError:\n                continue\n            if table.len <= truncate_slice_end:\n                logger.info(\"{0} not past truncate date={1}.\", file_name, date)\n                continue\n\n            logger.info(\n                \"Truncating {0} at end_date={1}\", file_name, date.date()\n            )\n\n            table.resize(truncate_slice_end)\n\n        # Update end session in metadata.\n        metadata = BcolzMinuteBarMetadata.read(self._rootdir)\n        metadata.end_session = date\n        metadata.write(self._rootdir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _minutes_to_exclude(self):\n        market_opens = self._market_opens.values.astype('datetime64[m]')\n        market_closes = self._market_closes.values.astype('datetime64[m]')\n        minutes_per_day = (market_closes - market_opens).astype(np.int64)\n        early_indices = np.where(\n            minutes_per_day != self._minutes_per_day - 1)[0]\n        early_opens = self._market_opens[early_indices]\n        early_closes = self._market_closes[early_indices]\n        minutes = [(market_open, early_close)\n                   for market_open, early_close\n                   in zip(early_opens, early_closes)]\n        return minutes", "response": "Calculate the minutes which should be excluded when a window\n        occurs on days which had an early close."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild an interval tree that contains the nodes which are not in the minutes to exclude.", "response": "def _minute_exclusion_tree(self):\n        \"\"\"\n        Build an interval tree keyed by the start and end of each range\n        of positions should be dropped from windows. (These are the minutes\n        between an early close and the minute which would be the close based\n        on the regular period if there were no early close.)\n        The value of each node is the same start and end position stored as\n        a tuple.\n\n        The data is stored as such in support of a fast answer to the question,\n        does a given start and end position overlap any of the exclusion spans?\n\n        Returns\n        -------\n        IntervalTree containing nodes which represent the minutes to exclude\n        because of early closes.\n        \"\"\"\n        itree = IntervalTree()\n        for market_open, early_close in self._minutes_to_exclude():\n            start_pos = self._find_position_of_minute(early_close) + 1\n            end_pos = (\n                self._find_position_of_minute(market_open)\n                +\n                self._minutes_per_day\n                -\n                1\n            )\n            data = (start_pos, end_pos)\n            itree[start_pos:end_pos + 1] = data\n        return itree"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _exclusion_indices_for_range(self, start_idx, end_idx):\n        itree = self._minute_exclusion_tree\n        if itree.overlaps(start_idx, end_idx):\n            ranges = []\n            intervals = itree[start_idx:end_idx]\n            for interval in intervals:\n                ranges.append(interval.data)\n            return sorted(ranges)\n        else:\n            return None", "response": "Returns a list of tuples of start stop indices which represent the ranges of minutes\n        which should be excluded when a market minute window is requested."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_value(self, sid, dt, field):\n        if self._last_get_value_dt_value == dt.value:\n            minute_pos = self._last_get_value_dt_position\n        else:\n            try:\n                minute_pos = self._find_position_of_minute(dt)\n            except ValueError:\n                raise NoDataOnDate()\n\n            self._last_get_value_dt_value = dt.value\n            self._last_get_value_dt_position = minute_pos\n\n        try:\n            value = self._open_minute_file(field, sid)[minute_pos]\n        except IndexError:\n            value = 0\n        if value == 0:\n            if field == 'volume':\n                return 0\n            else:\n                return np.nan\n\n        if field != 'volume':\n            value *= self._ohlc_ratio_inverse_for_sid(sid)\n        return value", "response": "Retrieves the value of the given asset for the given sid dt and field."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_position_of_minute(self, minute_dt):\n        return find_position_of_minute(\n            self._market_open_values,\n            self._market_close_values,\n            minute_dt.value / NANOS_IN_MINUTE,\n            self._minutes_per_day,\n            False,\n        )", "response": "Internal method that returns the position of the given minute in the list of market open and market close values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the raw arrays for the given fields and sids.", "response": "def load_raw_arrays(self, fields, start_dt, end_dt, sids):\n        \"\"\"\n        Parameters\n        ----------\n        fields : list of str\n           'open', 'high', 'low', 'close', or 'volume'\n        start_dt: Timestamp\n           Beginning of the window range.\n        end_dt: Timestamp\n           End of the window range.\n        sids : list of int\n           The asset identifiers in the window.\n\n        Returns\n        -------\n        list of np.ndarray\n            A list with an entry per field of ndarrays with shape\n            (minutes in range, sids) with a dtype of float64, containing the\n            values for the respective field over start and end dt range.\n        \"\"\"\n        start_idx = self._find_position_of_minute(start_dt)\n        end_idx = self._find_position_of_minute(end_dt)\n\n        num_minutes = (end_idx - start_idx + 1)\n\n        results = []\n\n        indices_to_exclude = self._exclusion_indices_for_range(\n            start_idx, end_idx)\n        if indices_to_exclude is not None:\n            for excl_start, excl_stop in indices_to_exclude:\n                length = excl_stop - excl_start + 1\n                num_minutes -= length\n\n        shape = num_minutes, len(sids)\n\n        for field in fields:\n            if field != 'volume':\n                out = np.full(shape, np.nan)\n            else:\n                out = np.zeros(shape, dtype=np.uint32)\n\n            for i, sid in enumerate(sids):\n                carray = self._open_minute_file(field, sid)\n                values = carray[start_idx:end_idx + 1]\n                if indices_to_exclude is not None:\n                    for excl_start, excl_stop in indices_to_exclude[::-1]:\n                        excl_slice = np.s_[\n                            excl_start - start_idx:excl_stop - start_idx + 1]\n                        values = np.delete(values, excl_slice)\n\n                where = values != 0\n                # first slice down to len(where) because we might not have\n                # written data for all the minutes requested\n                if field != 'volume':\n                    out[:len(where), i][where] = (\n                        values[where] * self._ohlc_ratio_inverse_for_sid(sid))\n                else:\n                    out[:len(where), i][where] = values[where]\n\n            results.append(out)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, frames):\n        with HDFStore(self._path, 'w',\n                      complevel=self._complevel, complib=self._complib) \\\n                as store:\n            panel = pd.Panel.from_dict(dict(frames))\n            panel.to_hdf(store, 'updates')\n        with tables.open_file(self._path, mode='r+') as h5file:\n            h5file.set_node_attr('/', 'version', 0)", "response": "Write the frames to the target HDF5 file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef next_event_indexer(all_dates,\n                       data_query_cutoff,\n                       all_sids,\n                       event_dates,\n                       event_timestamps,\n                       event_sids):\n    \"\"\"\n    Construct an index array that, when applied to an array of values, produces\n    a 2D array containing the values associated with the next event for each\n    sid at each moment in time.\n\n    Locations where no next event was known will be filled with -1.\n\n    Parameters\n    ----------\n    all_dates : ndarray[datetime64[ns], ndim=1]\n        Row labels for the target output.\n    data_query_cutoff : pd.DatetimeIndex\n        The boundaries for the given trading sessions in ``all_dates``.\n    all_sids : ndarray[int, ndim=1]\n        Column labels for the target output.\n    event_dates : ndarray[datetime64[ns], ndim=1]\n        Dates on which each input events occurred/will occur.  ``event_dates``\n        must be in sorted order, and may not contain any NaT values.\n    event_timestamps : ndarray[datetime64[ns], ndim=1]\n        Dates on which we learned about each input event.\n    event_sids : ndarray[int, ndim=1]\n        Sids assocated with each input event.\n\n    Returns\n    -------\n    indexer : ndarray[int, ndim=2]\n        An array of shape (len(all_dates), len(all_sids)) of indices into\n        ``event_{dates,timestamps,sids}``.\n    \"\"\"\n    validate_event_metadata(event_dates, event_timestamps, event_sids)\n    out = np.full((len(all_dates), len(all_sids)), -1, dtype=np.int64)\n\n    sid_ixs = all_sids.searchsorted(event_sids)\n    # side='right' here ensures that we include the event date itself\n    # if it's in all_dates.\n    dt_ixs = all_dates.searchsorted(event_dates, side='right')\n    ts_ixs = data_query_cutoff.searchsorted(event_timestamps, side='right')\n\n    # Walk backward through the events, writing the index of the event into\n    # slots ranging from the event's timestamp to its asof.  This depends for\n    # correctness on the fact that event_dates is sorted in ascending order,\n    # because we need to overwrite later events with earlier ones if their\n    # eligible windows overlap.\n    for i in range(len(event_sids) - 1, -1, -1):\n        start_ix = ts_ixs[i]\n        end_ix = dt_ixs[i]\n        out[start_ix:end_ix, sid_ixs[i]] = i\n\n    return out", "response": "Construct an index array that when applied to an array of values produces a 2D array containing the values associated with the next event for each trading session."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef previous_event_indexer(data_query_cutoff_times,\n                           all_sids,\n                           event_dates,\n                           event_timestamps,\n                           event_sids):\n    \"\"\"\n    Construct an index array that, when applied to an array of values, produces\n    a 2D array containing the values associated with the previous event for\n    each sid at each moment in time.\n\n    Locations where no previous event was known will be filled with -1.\n\n    Parameters\n    ----------\n    data_query_cutoff : pd.DatetimeIndex\n        The boundaries for the given trading sessions.\n    all_dates : ndarray[datetime64[ns], ndim=1]\n        Row labels for the target output.\n    all_sids : ndarray[int, ndim=1]\n        Column labels for the target output.\n    event_dates : ndarray[datetime64[ns], ndim=1]\n        Dates on which each input events occurred/will occur.  ``event_dates``\n        must be in sorted order, and may not contain any NaT values.\n    event_timestamps : ndarray[datetime64[ns], ndim=1]\n        Dates on which we learned about each input event.\n    event_sids : ndarray[int, ndim=1]\n        Sids assocated with each input event.\n\n    Returns\n    -------\n    indexer : ndarray[int, ndim=2]\n        An array of shape (len(all_dates), len(all_sids)) of indices into\n        ``event_{dates,timestamps,sids}``.\n    \"\"\"\n    validate_event_metadata(event_dates, event_timestamps, event_sids)\n    out = np.full(\n        (len(data_query_cutoff_times), len(all_sids)),\n        -1,\n        dtype=np.int64,\n    )\n\n    eff_dts = np.maximum(event_dates, event_timestamps)\n    sid_ixs = all_sids.searchsorted(event_sids)\n    dt_ixs = data_query_cutoff_times.searchsorted(eff_dts, side='right')\n\n    # Walk backwards through the events, writing the index of the event into\n    # slots ranging from max(event_date, event_timestamp) to the start of the\n    # previously-written event.  This depends for correctness on the fact that\n    # event_dates is sorted in ascending order, because we need to have written\n    # later events so we know where to stop forward-filling earlier events.\n    last_written = {}\n    for i in range(len(event_dates) - 1, -1, -1):\n        sid_ix = sid_ixs[i]\n        dt_ix = dt_ixs[i]\n        out[dt_ix:last_written.get(sid_ix, None), sid_ix] = i\n        last_written[sid_ix] = dt_ix\n    return out", "response": "Construct an index array that when applied to an array of values produces a 2D array containing the values associated with the previous event for the given trading session."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the last piece of information known on each date in the date index for each group. Input df MUST be sorted such that the correct last item is chosen from each group. Parameters ---------- df : pd.DataFrame The DataFrame containing the data to be grouped. Must be sorted so that the correct last item is chosen from each group. data_query_cutoff_times : pd.DatetimeIndex The dates to use for grouping and reindexing. assets : pd.Int64Index The assets that should be included in the column multiindex. reindex : bool Whether or not the DataFrame should be reindexed against the date index. This will add back any dates to the index that were grouped away. have_sids : bool Whether or not the DataFrame has sids. If it does, they will be used in the groupby. extra_groupers : list of str Any extra field names that should be included in the groupby. Returns ------- last_in_group : pd.DataFrame A DataFrame with dates as the index and fields used in the groupby as levels of a multiindex of columns.", "response": "def last_in_date_group(df,\n                       data_query_cutoff_times,\n                       assets,\n                       reindex=True,\n                       have_sids=True,\n                       extra_groupers=None):\n    \"\"\"\n    Determine the last piece of information known on each date in the date\n\n    index for each group. Input df MUST be sorted such that the correct last\n    item is chosen from each group.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The DataFrame containing the data to be grouped. Must be sorted so that\n        the correct last item is chosen from each group.\n    data_query_cutoff_times : pd.DatetimeIndex\n        The dates to use for grouping and reindexing.\n    assets : pd.Int64Index\n        The assets that should be included in the column multiindex.\n    reindex : bool\n        Whether or not the DataFrame should be reindexed against the date\n        index. This will add back any dates to the index that were grouped\n        away.\n    have_sids : bool\n        Whether or not the DataFrame has sids. If it does, they will be used\n        in the groupby.\n    extra_groupers : list of str\n        Any extra field names that should be included in the groupby.\n\n    Returns\n    -------\n    last_in_group : pd.DataFrame\n        A DataFrame with dates as the index and fields used in the groupby as\n        levels of a multiindex of columns.\n\n    \"\"\"\n    idx = [data_query_cutoff_times[data_query_cutoff_times.searchsorted(\n        df[TS_FIELD_NAME].values,\n    )]]\n    if have_sids:\n        idx += [SID_FIELD_NAME]\n    if extra_groupers is None:\n        extra_groupers = []\n    idx += extra_groupers\n\n    last_in_group = df.drop(TS_FIELD_NAME, axis=1).groupby(\n        idx,\n        sort=False,\n    ).last()\n\n    # For the number of things that we're grouping by (except TS), unstack\n    # the df. Done this way because of an unresolved pandas bug whereby\n    # passing a list of levels with mixed dtypes to unstack causes the\n    # resulting DataFrame to have all object-type columns.\n    for _ in range(len(idx) - 1):\n        last_in_group = last_in_group.unstack(-1)\n\n    if reindex:\n        if have_sids:\n            cols = last_in_group.columns\n            last_in_group = last_in_group.reindex(\n                index=data_query_cutoff_times,\n                columns=pd.MultiIndex.from_product(\n                    tuple(cols.levels[0:len(extra_groupers) + 1]) + (assets,),\n                    names=cols.names,\n                ),\n            )\n        else:\n            last_in_group = last_in_group.reindex(data_query_cutoff_times)\n\n    return last_in_group"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nforward fill values in a DataFrame with special logic to handle cases that pd. DataFrame. ffill cannot and cast columns to appropriate types.", "response": "def ffill_across_cols(df, columns, name_map):\n    \"\"\"\n    Forward fill values in a DataFrame with special logic to handle cases\n    that pd.DataFrame.ffill cannot and cast columns to appropriate types.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The DataFrame to do forward-filling on.\n    columns : list of BoundColumn\n        The BoundColumns that correspond to columns in the DataFrame to which\n        special filling and/or casting logic should be applied.\n    name_map: map of string -> string\n        Mapping from the name of each BoundColumn to the associated column\n        name in `df`.\n    \"\"\"\n    df.ffill(inplace=True)\n\n    # Fill in missing values specified by each column. This is made\n    # significantly more complex by the fact that we need to work around\n    # two pandas issues:\n\n    # 1) When we have sids, if there are no records for a given sid for any\n    #    dates, pandas will generate a column full of NaNs for that sid.\n    #    This means that some of the columns in `dense_output` are now\n    #    float instead of the intended dtype, so we have to coerce back to\n    #    our expected type and convert NaNs into the desired missing value.\n\n    # 2) DataFrame.ffill assumes that receiving None as a fill-value means\n    #    that no value was passed.  Consequently, there's no way to tell\n    #    pandas to replace NaNs in an object column with None using fillna,\n    #    so we have to roll our own instead using df.where.\n    for column in columns:\n        column_name = name_map[column.name]\n        # Special logic for strings since `fillna` doesn't work if the\n        # missing value is `None`.\n        if column.dtype == categorical_dtype:\n            df[column_name] = df[\n                column.name\n            ].where(pd.notnull(df[column_name]),\n                    column.missing_value)\n        else:\n            # We need to execute `fillna` before `astype` in case the\n            # column contains NaNs and needs to be cast to bool or int.\n            # This is so that the NaNs are replaced first, since pandas\n            # can't convert NaNs for those types.\n            df[column_name] = df[\n                column_name\n            ].fillna(column.missing_value).astype(column.dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shift_dates(dates, start_date, end_date, shift):\n    try:\n        start = dates.get_loc(start_date)\n    except KeyError:\n        if start_date < dates[0]:\n            raise NoFurtherDataError(\n                msg=(\n                    \"Pipeline Query requested data starting on {query_start}, \"\n                    \"but first known date is {calendar_start}\"\n                ).format(\n                    query_start=str(start_date),\n                    calendar_start=str(dates[0]),\n                )\n            )\n        else:\n            raise ValueError(\"Query start %s not in calendar\" % start_date)\n\n    # Make sure that shifting doesn't push us out of the calendar.\n    if start < shift:\n        raise NoFurtherDataError(\n            msg=(\n                \"Pipeline Query requested data from {shift}\"\n                \" days before {query_start}, but first known date is only \"\n                \"{start} days earlier.\"\n            ).format(shift=shift, query_start=start_date, start=start),\n        )\n\n    try:\n        end = dates.get_loc(end_date)\n    except KeyError:\n        if end_date > dates[-1]:\n            raise NoFurtherDataError(\n                msg=(\n                    \"Pipeline Query requesting data up to {query_end}, \"\n                    \"but last known date is {calendar_end}\"\n                ).format(\n                    query_end=end_date,\n                    calendar_end=dates[-1],\n                )\n            )\n        else:\n            raise ValueError(\"Query end %s not in calendar\" % end_date)\n    return dates[start - shift], dates[end - shift]", "response": "This function takes a list of dates and shifts them back to the specified number of days."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats the docstring for a specific function or class.", "response": "def format_docstring(owner_name, docstring, formatters):\n    \"\"\"\n    Template ``formatters`` into ``docstring``.\n\n    Parameters\n    ----------\n    owner_name : str\n        The name of the function or class whose docstring is being templated.\n        Only used for error messages.\n    docstring : str\n        The docstring to template.\n    formatters : dict[str -> str]\n        Parameters for a a str.format() call on ``docstring``.\n\n        Multi-line values in ``formatters`` will have leading whitespace padded\n        to match the leading whitespace of the substitution string.\n    \"\"\"\n    # Build a dict of parameters to a vanilla format() call by searching for\n    # each entry in **formatters and applying any leading whitespace to each\n    # line in the desired substitution.\n    format_params = {}\n    for target, doc_for_target in iteritems(formatters):\n        # Search for '{name}', with optional leading whitespace.\n        regex = re.compile(r'^(\\s*)' + '({' + target + '})$', re.MULTILINE)\n        matches = regex.findall(docstring)\n        if not matches:\n            raise ValueError(\n                \"Couldn't find template for parameter {!r} in docstring \"\n                \"for {}.\"\n                \"\\nParameter name must be alone on a line surrounded by \"\n                \"braces.\".format(target, owner_name),\n            )\n        elif len(matches) > 1:\n            raise ValueError(\n                \"Couldn't found multiple templates for parameter {!r}\"\n                \"in docstring for {}.\"\n                \"\\nParameter should only appear once.\".format(\n                    target, owner_name\n                )\n            )\n\n        (leading_whitespace, _) = matches[0]\n        format_params[target] = pad_lines_after_first(\n            leading_whitespace,\n            doc_for_target,\n        )\n\n    return docstring.format(**format_params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef templated_docstring(**docs):\n    def decorator(f):\n        f.__doc__ = format_docstring(f.__name__, f.__doc__, docs)\n        return f\n    return decorator", "response": "Decorator allowing the use of templated docstrings."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a column to the DataFrame.", "response": "def add(self, term, name, overwrite=False):\n        \"\"\"\n        Add a column.\n\n        The results of computing `term` will show up as a column in the\n        DataFrame produced by running this pipeline.\n\n        Parameters\n        ----------\n        column : zipline.pipeline.Term\n            A Filter, Factor, or Classifier to add to the pipeline.\n        name : str\n            Name of the column to add.\n        overwrite : bool\n            Whether to overwrite the existing entry if we already have a column\n            named `name`.\n        \"\"\"\n        self.validate_column(name, term)\n\n        columns = self.columns\n        if name in columns:\n            if overwrite:\n                self.remove(name)\n            else:\n                raise KeyError(\"Column '{}' already exists.\".format(name))\n\n        if not isinstance(term, ComputableTerm):\n            raise TypeError(\n                \"{term} is not a valid pipeline column. Did you mean to \"\n                \"append '.latest'?\".format(term=term)\n            )\n\n        self._columns[name] = term"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_screen(self, screen, overwrite=False):\n        if self._screen is not None and not overwrite:\n            raise ValueError(\n                \"set_screen() called with overwrite=False and screen already \"\n                \"set.\\n\"\n                \"If you want to apply multiple filters as a screen use \"\n                \"set_screen(filter1 & filter2 & ...).\\n\"\n                \"If you want to replace the previous screen with a new one, \"\n                \"use set_screen(new_filter, overwrite=True).\"\n            )\n        self._screen = screen", "response": "Sets the screen on this Pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_execution_plan(self,\n                          domain,\n                          default_screen,\n                          start_date,\n                          end_date):\n        \"\"\"\n        Compile into an ExecutionPlan.\n\n        Parameters\n        ----------\n        domain : zipline.pipeline.domain.Domain\n            Domain on which the pipeline will be executed.\n        default_screen : zipline.pipeline.term.Term\n            Term to use as a screen if self.screen is None.\n        all_dates : pd.DatetimeIndex\n            A calendar of dates to use to calculate starts and ends for each\n            term.\n        start_date : pd.Timestamp\n            The first date of requested output.\n        end_date : pd.Timestamp\n            The last date of requested output.\n\n        Returns\n        -------\n        graph : zipline.pipeline.graph.ExecutionPlan\n            Graph encoding term dependencies, including metadata about extra\n            row requirements.\n        \"\"\"\n        if self._domain is not GENERIC and self._domain is not domain:\n            raise AssertionError(\n                \"Attempted to compile Pipeline with domain {} to execution \"\n                \"plan with different domain {}.\".format(self._domain, domain)\n            )\n\n        return ExecutionPlan(\n            domain=domain,\n            terms=self._prepare_graph_terms(default_screen),\n            start_date=start_date,\n            end_date=end_date,\n        )", "response": "Compile into an ExecutionPlan object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prepare_graph_terms(self, default_screen):\n        columns = self.columns.copy()\n        screen = self.screen\n        if screen is None:\n            screen = default_screen\n        columns[SCREEN_NAME] = screen\n        return columns", "response": "Helper for to_graph and to_execution_plan."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrender this Pipeline as a DAG.", "response": "def show_graph(self, format='svg'):\n        \"\"\"\n        Render this Pipeline as a DAG.\n\n        Parameters\n        ----------\n        format : {'svg', 'png', 'jpeg'}\n            Image format to render with.  Default is 'svg'.\n        \"\"\"\n        g = self.to_simple_graph(AssetExists())\n        if format == 'svg':\n            return g.svg\n        elif format == 'png':\n            return g.png\n        elif format == 'jpeg':\n            return g.jpeg\n        else:\n            # We should never get here because of the expect_element decorator\n            # above.\n            raise AssertionError(\"Unknown graph format %r.\" % format)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _output_terms(self):\n        terms = list(six.itervalues(self._columns))\n        screen = self.screen\n        if screen is not None:\n            terms.append(screen)\n        return terms", "response": "Returns a list of terms that are outputs of this pipeline plus all screen terms that are outputs of this pipeline plus the screen terms that are outputs of this pipeline plus the\n            screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the domain for this pipeline.", "response": "def domain(self, default):\n        \"\"\"\n        Get the domain for this pipeline.\n\n        - If an explicit domain was provided at construction time, use it.\n        - Otherwise, infer a domain from the registered columns.\n        - If no domain can be inferred, return ``default``.\n\n        Parameters\n        ----------\n        default : zipline.pipeline.Domain\n            Domain to use if no domain can be inferred from this pipeline by\n            itself.\n\n        Returns\n        -------\n        domain : zipline.pipeline.Domain\n            The domain for the pipeline.\n\n        Raises\n        ------\n        AmbiguousDomain\n        ValueError\n            If the terms in ``self`` conflict with self._domain.\n        \"\"\"\n        # Always compute our inferred domain to ensure that it's compatible\n        # with our explicit domain.\n        inferred = infer_domain(self._output_terms)\n\n        if inferred is GENERIC and self._domain is GENERIC:\n            # Both generic. Fall back to default.\n            return default\n        elif inferred is GENERIC and self._domain is not GENERIC:\n            # Use the non-generic domain.\n            return self._domain\n        elif inferred is not GENERIC and self._domain is GENERIC:\n            # Use the non-generic domain.\n            return inferred\n        else:\n            # Both non-generic. They have to match.\n            if inferred is not self._domain:\n                raise ValueError(\n                    \"Conflicting domains in Pipeline. Inferred {}, but {} was \"\n                    \"passed at construction.\".format(inferred, self._domain)\n                )\n            return inferred"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure that the element is present in the tuple.", "response": "def _ensure_element(tup, elem):\n    \"\"\"\n    Create a tuple containing all elements of tup, plus elem.\n\n    Returns the new tuple and the index of elem in the new tuple.\n    \"\"\"\n    try:\n        return tup, tup.index(elem)\n    except ValueError:\n        return tuple(chain(tup, (elem,))), len(tup)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that our expression string is well - formed and that all of our variables are in the correct order.", "response": "def _validate(self):\n        \"\"\"\n        Ensure that our expression string has variables of the form x_0, x_1,\n        ... x_(N - 1), where N is the length of our inputs.\n        \"\"\"\n        variable_names, _unused = getExprNames(self._expr, {})\n        expr_indices = []\n        for name in variable_names:\n            if name == 'inf':\n                continue\n            match = _VARIABLE_NAME_RE.match(name)\n            if not match:\n                raise ValueError(\"%r is not a valid variable name\" % name)\n            expr_indices.append(int(match.group(2)))\n\n        expr_indices.sort()\n        expected_indices = list(range(len(self.inputs)))\n        if expr_indices != expected_indices:\n            raise ValueError(\n                \"Expected %s for variable indices, but got %s\" % (\n                    expected_indices, expr_indices,\n                )\n            )\n        super(NumericalExpression, self)._validate()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute(self, arrays, dates, assets, mask):\n        out = full(mask.shape, self.missing_value, dtype=self.dtype)\n        # This writes directly into our output buffer.\n        numexpr.evaluate(\n            self._expr,\n            local_dict={\n                \"x_%d\" % idx: array\n                for idx, array in enumerate(arrays)\n            },\n            global_dict={'inf': inf},\n            out=out,\n        )\n        return out", "response": "Compute our stored expression string with numexpr."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns self. _expr with all variables rebound to the indices implied by new_inputs.", "response": "def _rebind_variables(self, new_inputs):\n        \"\"\"\n        Return self._expr with all variables rebound to the indices implied by\n        new_inputs.\n        \"\"\"\n        expr = self._expr\n\n        # If we have 11+ variables, some of our variable names may be\n        # substrings of other variable names. For example, we might have x_1,\n        # x_10, and x_100. By enumerating in reverse order, we ensure that\n        # every variable name which is a substring of another variable name is\n        # processed after the variable of which it is a substring. This\n        # guarantees that the substitution of any given variable index only\n        # ever affects exactly its own index. For example, if we have variables\n        # with indices going up to 100, we will process all of the x_1xx names\n        # before x_1x, which will be before x_1, so the substitution of x_1\n        # will not affect x_1x, which will not affect x_1xx.\n        for idx, input_ in reversed(list(enumerate(self.inputs))):\n            old_varname = \"x_%d\" % idx\n            # Temporarily rebind to x_temp_N so that we don't overwrite the\n            # same value multiple times.\n            temp_new_varname = \"x_temp_%d\" % new_inputs.index(input_)\n            expr = expr.replace(old_varname, temp_new_varname)\n        # Clear out the temp variables now that we've finished iteration.\n        return expr.replace(\"_temp_\", \"_\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmerges the inputs of two NumericalExpressions into a single input tuple and rewriting their respective string expressions to make input names resolve correctly.", "response": "def _merge_expressions(self, other):\n        \"\"\"\n        Merge the inputs of two NumericalExpressions into a single input tuple,\n        rewriting their respective string expressions to make input names\n        resolve correctly.\n\n        Returns a tuple of (new_self_expr, new_other_expr, new_inputs)\n        \"\"\"\n        new_inputs = tuple(set(self.inputs).union(other.inputs))\n        new_self_expr = self._rebind_variables(new_inputs)\n        new_other_expr = other._rebind_variables(new_inputs)\n        return new_self_expr, new_other_expr, new_inputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_binary_op(self, op, other):\n        if isinstance(other, NumericalExpression):\n            self_expr, other_expr, new_inputs = self._merge_expressions(other)\n        elif isinstance(other, Term):\n            self_expr = self._expr\n            new_inputs, other_idx = _ensure_element(self.inputs, other)\n            other_expr = \"x_%d\" % other_idx\n        elif isinstance(other, Number):\n            self_expr = self._expr\n            other_expr = str(other)\n            new_inputs = self.inputs\n        else:\n            raise BadBinaryOperator(op, other)\n        return self_expr, other_expr, new_inputs", "response": "Builds the expression string and a new inputs tuple for combining\n            self and other with a binary operator."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef graph_repr(self):\n\n        # Replace any floating point numbers in the expression\n        # with their scientific notation\n        final = re.sub(r\"[-+]?\\d*\\.\\d+\",\n                       lambda x: format(float(x.group(0)), '.2E'),\n                       self._expr)\n        # Graphviz interprets `\\l` as \"divide label into lines, left-justified\"\n        return \"Expression:\\\\l  {}\\\\l\".format(\n            final,\n        )", "response": "Short repr to use when rendering Pipeline graphs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the last modified time of a file.", "response": "def last_modified_time(path):\n    \"\"\"\n    Get the last modified time of path as a Timestamp.\n    \"\"\"\n    return pd.Timestamp(os.path.getmtime(path), unit='s', tz='UTC')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef zipline_root(environ=None):\n    if environ is None:\n        environ = os.environ\n\n    root = environ.get('ZIPLINE_ROOT', None)\n    if root is None:\n        root = expanduser('~/.zipline')\n\n    return root", "response": "Returns the path to the zipline root directory for all zipline - managed files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_adjustments(self, dates, assets):\n        make_adjustment = partial(make_adjustment_from_labels, dates, assets)\n\n        min_date, max_date = dates[[0, -1]]\n        # TODO: Consider porting this to Cython.\n        if len(self.adjustments) == 0:\n            return {}\n\n        # Mask for adjustments whose apply_dates are in the requested window of\n        # dates.\n        date_bounds = self.adjustment_apply_dates.slice_indexer(\n            min_date,\n            max_date,\n        )\n        dates_filter = zeros(len(self.adjustments), dtype='bool')\n        dates_filter[date_bounds] = True\n        # Ignore adjustments whose apply_date is in range, but whose end_date\n        # is out of range.\n        dates_filter &= (self.adjustment_end_dates >= min_date)\n\n        # Mask for adjustments whose sids are in the requested assets.\n        sids_filter = self.adjustment_sids.isin(assets.values)\n\n        adjustments_to_use = self.adjustments.loc[\n            dates_filter & sids_filter\n        ].set_index('apply_date')\n\n        # For each apply_date on which we have an adjustment, compute\n        # the integer index of that adjustment's apply_date in `dates`.\n        # Then build a list of Adjustment objects for that apply_date.\n        # This logic relies on the sorting applied on the previous line.\n        out = {}\n        previous_apply_date = object()\n        for row in adjustments_to_use.itertuples():\n            # This expansion depends on the ordering of the DataFrame columns,\n            # defined above.\n            apply_date, sid, value, kind, start_date, end_date = row\n            if apply_date != previous_apply_date:\n                # Get the next apply date if no exact match.\n                row_loc = dates.get_loc(apply_date, method='bfill')\n                current_date_adjustments = out[row_loc] = []\n                previous_apply_date = apply_date\n\n            # Look up the approprate Adjustment constructor based on the value\n            # of `kind`.\n            current_date_adjustments.append(\n                make_adjustment(start_date, end_date, sid, kind, value)\n            )\n        return out", "response": "Return a dict of Adjustment objects in the format expected by AdjustedArray."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads data from our stored baseline.", "response": "def load_adjusted_array(self, domain, columns, dates, sids, mask):\n        \"\"\"\n        Load data from our stored baseline.\n        \"\"\"\n        if len(columns) != 1:\n            raise ValueError(\n                \"Can't load multiple columns with DataFrameLoader\"\n            )\n\n        column = columns[0]\n        self._validate_input_column(column)\n\n        date_indexer = self.dates.get_indexer(dates)\n        assets_indexer = self.assets.get_indexer(sids)\n\n        # Boolean arrays with True on matched entries\n        good_dates = (date_indexer != -1)\n        good_assets = (assets_indexer != -1)\n\n        data = self.baseline[ix_(date_indexer, assets_indexer)]\n        mask = (good_assets & as_column(good_dates)) & mask\n\n        # Mask out requested columns/rows that didn't match.\n        data[~mask] = column.missing_value\n\n        return {\n            column: AdjustedArray(\n                # Pull out requested columns/rows from our baseline data.\n                data=data,\n                adjustments=self.format_adjustments(dates, sids),\n                missing_value=column.missing_value,\n            ),\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake sure a passed column is our column.", "response": "def _validate_input_column(self, column):\n        \"\"\"Make sure a passed column is our column.\n        \"\"\"\n        if column != self.column and column.unspecialize() != self.column:\n            raise ValueError(\"Can't load unknown column %s\" % column)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_from_directory(list_name):\n    data = {}\n    dir_path = os.path.join(SECURITY_LISTS_DIR, list_name)\n    for kd_name in listdir(dir_path):\n        kd = datetime.strptime(kd_name, DATE_FORMAT).replace(\n            tzinfo=pytz.utc)\n        data[kd] = {}\n        kd_path = os.path.join(dir_path, kd_name)\n        for ld_name in listdir(kd_path):\n            ld = datetime.strptime(ld_name, DATE_FORMAT).replace(\n                tzinfo=pytz.utc)\n            data[kd][ld] = {}\n            ld_path = os.path.join(kd_path, ld_name)\n            for fname in listdir(ld_path):\n                fpath = os.path.join(ld_path, fname)\n                with open(fpath) as f:\n                    symbols = f.read().splitlines()\n                    data[kd][ld][fname] = symbols\n\n    return data", "response": "Load the data from a directory structure of the LEVERAGED_ETF list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _weak_lru_cache(maxsize=100):\n    def decorating_function(\n            user_function, tuple=tuple, sorted=sorted, len=len,\n            KeyError=KeyError):\n\n        hits, misses = [0], [0]\n        kwd_mark = (object(),)    # separates positional and keyword args\n        lock = Lock()             # needed because OrderedDict isn't threadsafe\n\n        if maxsize is None:\n            cache = _WeakArgsDict()  # cache without ordering or size limit\n\n            @wraps(user_function)\n            def wrapper(*args, **kwds):\n                key = args\n                if kwds:\n                    key += kwd_mark + tuple(sorted(kwds.items()))\n                try:\n                    result = cache[key]\n                    hits[0] += 1\n                    return result\n                except KeyError:\n                    pass\n                result = user_function(*args, **kwds)\n                cache[key] = result\n                misses[0] += 1\n                return result\n        else:\n            # ordered least recent to most recent\n            cache = _WeakArgsOrderedDict()\n            cache_popitem = cache.popitem\n            cache_renew = cache.move_to_end\n\n            @wraps(user_function)\n            def wrapper(*args, **kwds):\n                key = args\n                if kwds:\n                    key += kwd_mark + tuple(sorted(kwds.items()))\n                with lock:\n                    try:\n                        result = cache[key]\n                        cache_renew(key)    # record recent use of this key\n                        hits[0] += 1\n                        return result\n                    except KeyError:\n                        pass\n                result = user_function(*args, **kwds)\n                with lock:\n                    cache[key] = result     # record recent use of this key\n                    misses[0] += 1\n                    if len(cache) > maxsize:\n                        # purge least recently used cache entry\n                        cache_popitem(False)\n                return result\n\n        def cache_info():\n            \"\"\"Report cache statistics\"\"\"\n            with lock:\n                return hits[0], misses[0], maxsize, len(cache)\n\n        def cache_clear():\n            \"\"\"Clear the cache and cache statistics\"\"\"\n            with lock:\n                cache.clear()\n                hits[0] = misses[0] = 0\n\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        return wrapper\n\n    return decorating_function", "response": "A weak version of the lru_cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef weak_lru_cache(maxsize=100):\n    class desc(lazyval):\n        def __get__(self, instance, owner):\n            if instance is None:\n                return self\n            try:\n                return self._cache[instance]\n            except KeyError:\n                inst = ref(instance)\n\n                @_weak_lru_cache(maxsize)\n                @wraps(self._get)\n                def wrapper(*args, **kwargs):\n                    return self._get(inst(), *args, **kwargs)\n\n                self._cache[instance] = wrapper\n                return wrapper\n\n        @_weak_lru_cache(maxsize)\n        def __call__(self, *args, **kwargs):\n            return self._get(*args, **kwargs)\n\n    return desc", "response": "Weak least - recently - used cache decorator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if name is a final object in the given mro.", "response": "def is_final(name, mro):\n    \"\"\"\n    Checks if `name` is a `final` object in the given `mro`.\n    We need to check the mro because we need to directly go into the __dict__\n    of the classes. Because `final` objects are descriptor, we need to grab\n    them _BEFORE_ the `__call__` is invoked.\n    \"\"\"\n    return any(isinstance(getattr(c, '__dict__', {}).get(name), final)\n               for c in bases_mro(mro))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bind(self, name):\n        return _BoundColumnDescr(\n            dtype=self.dtype,\n            missing_value=self.missing_value,\n            name=name,\n            doc=self.doc,\n            metadata=self.metadata,\n        )", "response": "Bind a Column object to its name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nspecializing self to a concrete domain.", "response": "def specialize(self, domain):\n        \"\"\"Specialize ``self`` to a concrete domain.\n        \"\"\"\n        if domain == self.domain:\n            return self\n\n        return type(self)(\n            dtype=self.dtype,\n            missing_value=self.missing_value,\n            dataset=self._dataset.specialize(domain),\n            name=self._name,\n            doc=self.__doc__,\n            metadata=self._metadata,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_column(cls, name):\n        clsdict = vars(cls)\n        try:\n            maybe_column = clsdict[name]\n            if not isinstance(maybe_column, _BoundColumnDescr):\n                raise KeyError(name)\n        except KeyError:\n            raise AttributeError(\n                \"{dset} has no column {colname!r}:\\n\\n\"\n                \"Possible choices are:\\n\"\n                \"{choices}\".format(\n                    dset=cls.qualname,\n                    colname=name,\n                    choices=bulleted_list(\n                        sorted(cls._column_names),\n                        max_count=10,\n                    ),\n                )\n            )\n\n        # Resolve column descriptor into a BoundColumn.\n        return maybe_column.__get__(None, cls)", "response": "Look up a column by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_dataset(cls, coords):\n        class Slice(cls._SliceType):\n            extra_coords = coords\n\n        Slice.__name__ = '%s.slice(%s)' % (\n            cls.__name__,\n            ', '.join('%s=%r' % item for item in coords.items()),\n        )\n        return Slice", "response": "Construct a new dataset given the coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef slice(cls, *args, **kwargs):\n        coords, hash_key = cls._canonical_key(args, kwargs)\n        try:\n            return cls._slice_cache[hash_key]\n        except KeyError:\n            pass\n\n        Slice = cls._make_dataset(coords)\n        cls._slice_cache[hash_key] = Slice\n        return Slice", "response": "Take a slice of a DataSetFamily to produce a dataset with the given coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the expected value for an asset and date.", "response": "def expected_bar_value(asset_id, date, colname):\n    \"\"\"\n    Check that the raw value for an asset/date/column triple is as\n    expected.\n\n    Used by tests to verify data written by a writer.\n    \"\"\"\n    from_asset = asset_id * 100000\n    from_colname = OHLCV.index(colname) * 1000\n    from_date = (date - PSEUDO_EPOCH).days\n    return from_asset + from_colname + from_date"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an array containing cls. expected_value for each date and asset pair in the inputs.", "response": "def expected_bar_values_2d(dates,\n                           assets,\n                           asset_info,\n                           colname,\n                           holes=None):\n    \"\"\"\n    Return an 2D array containing cls.expected_value(asset_id, date,\n    colname) for each date/asset pair in the inputs.\n\n    Missing locs are filled with 0 for volume and NaN for price columns:\n\n        - Values before/after an asset's lifetime.\n        - Values for asset_ids not contained in asset_info.\n        - Locs defined in `holes`.\n    \"\"\"\n    if colname == 'volume':\n        dtype = uint32\n        missing = 0\n    else:\n        dtype = float64\n        missing = float('nan')\n\n    data = full((len(dates), len(assets)), missing, dtype=dtype)\n    for j, asset in enumerate(assets):\n        # Use missing values when asset_id is not contained in asset_info.\n        if asset not in asset_info.index:\n            continue\n\n        start = asset_start(asset_info, asset)\n        end = asset_end(asset_info, asset)\n        for i, date in enumerate(dates):\n            # No value expected for dates outside the asset's start/end\n            # date.\n            if not (start <= date <= end):\n                continue\n\n            if holes is not None:\n                expected = expected_bar_value_with_holes(\n                    asset,\n                    date,\n                    colname,\n                    holes,\n                    missing,\n                )\n            else:\n                expected = expected_bar_value(asset, date, colname)\n\n            data[i, j] = expected\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_adjusted_array(self, domain, columns, dates, sids, mask):\n        out = {}\n        for col in columns:\n            try:\n                loader = self._loaders.get(col)\n                if loader is None:\n                    loader = self._loaders[col.unspecialize()]\n            except KeyError:\n                raise ValueError(\"Couldn't find loader for %s\" % col)\n            out.update(\n                loader.load_adjusted_array(domain, [col], dates, sids, mask)\n            )\n        return out", "response": "Load by delegating to sub - loaders."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef values(self, dtype, dates, sids):\n        shape = (len(dates), len(sids))\n        return {\n            datetime64ns_dtype: self._datetime_values,\n            float64_dtype: self._float_values,\n            int64_dtype: self._int_values,\n            bool_dtype: self._bool_values,\n            object_dtype: self._object_values,\n        }[dtype](shape)", "response": "Return a random array of the specified dtype with the given dates and sids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn uniformly - distributed floats between - 0. 0 and 100. 0.", "response": "def _float_values(self, shape):\n        \"\"\"\n        Return uniformly-distributed floats between -0.0 and 100.0.\n        \"\"\"\n        return self.state.uniform(low=0.0, high=100.0, size=shape)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _int_values(self, shape):\n        return (self.state.randint(low=0, high=100, size=shape)\n                .astype('int64'))", "response": "Return uniformly - distributed integers between 0 and 100."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn uniformly - distributed dates in 2014.", "response": "def _datetime_values(self, shape):\n        \"\"\"\n        Return uniformly-distributed dates in 2014.\n        \"\"\"\n        start = Timestamp('2014', tz='UTC').asm8\n        offsets = self.state.randint(\n            low=0,\n            high=364,\n            size=shape,\n        ).astype('timedelta64[D]')\n        return start + offsets"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute rowwise array quantiles on an input.", "response": "def quantiles(data, nbins_or_partition_bounds):\n    \"\"\"\n    Compute rowwise array quantiles on an input.\n    \"\"\"\n    return apply_along_axis(\n        qcut,\n        1,\n        data,\n        q=nbins_or_partition_bounds, labels=False,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling the close of the given minute in minute emission.", "response": "def handle_minute_close(self, dt, data_portal):\n        \"\"\"\n        Handles the close of the given minute in minute emission.\n\n        Parameters\n        ----------\n        dt : Timestamp\n            The minute that is ending\n\n        Returns\n        -------\n        A minute perf packet.\n        \"\"\"\n        self.sync_last_sale_prices(dt, data_portal)\n\n        packet = {\n            'period_start': self._first_session,\n            'period_end': self._last_session,\n            'capital_base': self._capital_base,\n            'minute_perf': {\n                'period_open': self._market_open,\n                'period_close': dt,\n            },\n            'cumulative_perf': {\n                'period_open': self._first_session,\n                'period_close': self._last_session,\n            },\n            'progress': self._progress(self),\n            'cumulative_risk_metrics': {},\n        }\n        ledger = self._ledger\n        ledger.end_of_bar(self._session_count)\n        self.end_of_bar(\n            packet,\n            ledger,\n            dt,\n            self._session_count,\n            data_portal,\n        )\n        return packet"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling the start of each session.", "response": "def handle_market_open(self, session_label, data_portal):\n        \"\"\"Handles the start of each session.\n\n        Parameters\n        ----------\n        session_label : Timestamp\n            The label of the session that is about to begin.\n        data_portal : DataPortal\n            The current data portal.\n        \"\"\"\n        ledger = self._ledger\n        ledger.start_of_session(session_label)\n\n        adjustment_reader = data_portal.adjustment_reader\n        if adjustment_reader is not None:\n            # this is None when running with a dataframe source\n            ledger.process_dividends(\n                session_label,\n                self._asset_finder,\n                adjustment_reader,\n            )\n\n        self._current_session = session_label\n\n        cal = self._trading_calendar\n        self._market_open, self._market_close = self._execution_open_and_close(\n            cal,\n            session_label,\n        )\n\n        self.start_of_session(ledger, session_label, data_portal)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_market_close(self, dt, data_portal):\n        completed_session = self._current_session\n\n        if self.emission_rate == 'daily':\n            # this method is called for both minutely and daily emissions, but\n            # this chunk of code here only applies for daily emissions. (since\n            # it's done every minute, elsewhere, for minutely emission).\n            self.sync_last_sale_prices(dt, data_portal)\n\n        session_ix = self._session_count\n        # increment the day counter before we move markers forward.\n        self._session_count += 1\n\n        packet = {\n            'period_start': self._first_session,\n            'period_end': self._last_session,\n            'capital_base': self._capital_base,\n            'daily_perf': {\n                'period_open': self._market_open,\n                'period_close': dt,\n            },\n            'cumulative_perf': {\n                'period_open': self._first_session,\n                'period_close': self._last_session,\n            },\n            'progress': self._progress(self),\n            'cumulative_risk_metrics': {},\n        }\n        ledger = self._ledger\n        ledger.end_of_session(session_ix)\n        self.end_of_session(\n            packet,\n            ledger,\n            completed_session,\n            session_ix,\n            data_portal,\n        )\n\n        return packet", "response": "Handles the close of the given day."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the simulation end of the simulation.", "response": "def handle_simulation_end(self, data_portal):\n        \"\"\"\n        When the simulation is complete, run the full period risk report\n        and send it out on the results socket.\n        \"\"\"\n        log.info(\n            'Simulated {} trading days\\n'\n            'first open: {}\\n'\n            'last close: {}',\n            self._session_count,\n            self._trading_calendar.session_open(self._first_session),\n            self._trading_calendar.session_close(self._last_session),\n        )\n\n        packet = {}\n        self.end_of_simulation(\n            packet,\n            self._ledger,\n            self._trading_calendar,\n            self._sessions,\n            data_portal,\n            self._benchmark_source,\n        )\n        return packet"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_args(args, root):\n\n    extension_args = {}\n\n    for arg in args:\n        parse_extension_arg(arg, extension_args)\n\n    for name in sorted(extension_args, key=len):\n        path = name.split('.')\n        update_namespace(root, path, extension_args[name])", "response": "Creates a set of custom command line arguments in key = value or key. namespace = value form into a chain of Namespace objects. Each element in the chain is an attribute of the Namespace object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_extension_arg(arg, arg_dict):\n\n    match = re.match(r'^(([^\\d\\W]\\w*)(\\.[^\\d\\W]\\w*)*)=(.*)$', arg)\n    if match is None:\n        raise ValueError(\n            \"invalid extension argument '%s', must be in key=value form\" % arg\n        )\n\n    name = match.group(1)\n    value = match.group(4)\n    arg_dict[name] = value", "response": "Converts argument strings in key = value or key. namespace = value form to dictionary entries\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new registry for an abstract data type.", "response": "def create_registry(interface):\n    \"\"\"\n    Create a new registry for an extensible interface.\n\n    Parameters\n    ----------\n    interface : type\n        The abstract data type for which to create a registry,\n        which will manage registration of factories for this type.\n\n    Returns\n    -------\n    interface : type\n        The data type specified/decorated, unaltered.\n    \"\"\"\n    if interface in custom_types:\n        raise ValueError('there is already a Registry instance '\n                         'for the specified type')\n    custom_types[interface] = Registry(interface)\n    return interface"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self, name):\n        try:\n            return self._factories[name]()\n        except KeyError:\n            raise ValueError(\n                \"no %s factory registered under name %r, options are: %r\" %\n                (self.interface.__name__, name, sorted(self._factories)),\n            )", "response": "Construct an object from a registered factory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_per_unit_commission(order,\n                                  transaction,\n                                  cost_per_unit,\n                                  initial_commission,\n                                  min_trade_cost):\n    \"\"\"\n    If there is a minimum commission:\n        If the order hasn't had a commission paid yet, pay the minimum\n        commission.\n\n        If the order has paid a commission, start paying additional\n        commission once the minimum commission has been reached.\n\n    If there is no minimum commission:\n        Pay commission based on number of units in the transaction.\n    \"\"\"\n    additional_commission = abs(transaction.amount * cost_per_unit)\n\n    if order.commission == 0:\n        # no commission paid yet, pay at least the minimum plus a one-time\n        # exchange fee.\n        return max(min_trade_cost, additional_commission + initial_commission)\n    else:\n        # we've already paid some commission, so figure out how much we\n        # would be paying if we only counted per unit.\n        per_unit_total = \\\n            abs(order.filled * cost_per_unit) + \\\n            additional_commission + \\\n            initial_commission\n\n        if per_unit_total < min_trade_cost:\n            # if we haven't hit the minimum threshold yet, don't pay\n            # additional commission\n            return 0\n        else:\n            # we've exceeded the threshold, so pay more commission.\n            return per_unit_total - order.commission", "response": "Calculates the commission of the order for a given amount of units."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the amount of commission based on dollar value of shares.", "response": "def calculate(self, order, transaction):\n        \"\"\"\n        Pay commission based on dollar value of shares.\n        \"\"\"\n        cost_per_share = transaction.price * self.cost_per_dollar\n        return abs(transaction.amount) * cost_per_share"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a dictionary representing the state of the risk report. Parameters ---------- start_session : pd.Timestamp Start of period (inclusive) to produce metrics on end_session : pd.Timestamp End of period (inclusive) to produce metrics on algorithm_returns : pd.Series(pd.Timestamp -> float) Series of algorithm returns as of the end of each session benchmark_returns : pd.Series(pd.Timestamp -> float) Series of benchmark returns as of the end of each session algorithm_leverages : pd.Series(pd.Timestamp -> float) Series of algorithm leverages as of the end of each session Returns ------- risk_metric : dict[str, any] Dict of metrics that with fields like: { 'algorithm_period_return': 0.0, 'benchmark_period_return': 0.0, 'treasury_period_return': 0, 'excess_return': 0.0, 'alpha': 0.0, 'beta': 0.0, 'sharpe': 0.0, 'sortino': 0.0, 'period_label': '1970-01', 'trading_days': 0, 'algo_volatility': 0.0, 'benchmark_volatility': 0.0, 'max_drawdown': 0.0, 'max_leverage': 0.0, }", "response": "def risk_metric_period(cls,\n                           start_session,\n                           end_session,\n                           algorithm_returns,\n                           benchmark_returns,\n                           algorithm_leverages):\n        \"\"\"\n        Creates a dictionary representing the state of the risk report.\n\n        Parameters\n        ----------\n        start_session : pd.Timestamp\n            Start of period (inclusive) to produce metrics on\n        end_session : pd.Timestamp\n            End of period (inclusive) to produce metrics on\n        algorithm_returns : pd.Series(pd.Timestamp -> float)\n            Series of algorithm returns as of the end of each session\n        benchmark_returns : pd.Series(pd.Timestamp -> float)\n            Series of benchmark returns as of the end of each session\n        algorithm_leverages : pd.Series(pd.Timestamp -> float)\n            Series of algorithm leverages as of the end of each session\n\n\n        Returns\n        -------\n        risk_metric : dict[str, any]\n            Dict of metrics that with fields like:\n                {\n                    'algorithm_period_return': 0.0,\n                    'benchmark_period_return': 0.0,\n                    'treasury_period_return': 0,\n                    'excess_return': 0.0,\n                    'alpha': 0.0,\n                    'beta': 0.0,\n                    'sharpe': 0.0,\n                    'sortino': 0.0,\n                    'period_label': '1970-01',\n                    'trading_days': 0,\n                    'algo_volatility': 0.0,\n                    'benchmark_volatility': 0.0,\n                    'max_drawdown': 0.0,\n                    'max_leverage': 0.0,\n                }\n        \"\"\"\n\n        algorithm_returns = algorithm_returns[\n            (algorithm_returns.index >= start_session) &\n            (algorithm_returns.index <= end_session)\n        ]\n\n        # Benchmark needs to be masked to the same dates as the algo returns\n        benchmark_returns = benchmark_returns[\n            (benchmark_returns.index >= start_session) &\n            (benchmark_returns.index <= algorithm_returns.index[-1])\n        ]\n\n        benchmark_period_returns = ep.cum_returns(benchmark_returns).iloc[-1]\n        algorithm_period_returns = ep.cum_returns(algorithm_returns).iloc[-1]\n\n        alpha, beta = ep.alpha_beta_aligned(\n            algorithm_returns.values,\n            benchmark_returns.values,\n        )\n\n        sharpe = ep.sharpe_ratio(algorithm_returns)\n\n        # The consumer currently expects a 0.0 value for sharpe in period,\n        # this differs from cumulative which was np.nan.\n        # When factoring out the sharpe_ratio, the different return types\n        # were collapsed into `np.nan`.\n        # TODO: Either fix consumer to accept `np.nan` or make the\n        # `sharpe_ratio` return type configurable.\n        # In the meantime, convert nan values to 0.0\n        if pd.isnull(sharpe):\n            sharpe = 0.0\n\n        sortino = ep.sortino_ratio(\n            algorithm_returns.values,\n            _downside_risk=ep.downside_risk(algorithm_returns.values),\n        )\n\n        rval = {\n            'algorithm_period_return': algorithm_period_returns,\n            'benchmark_period_return': benchmark_period_returns,\n            'treasury_period_return': 0,\n            'excess_return': algorithm_period_returns,\n            'alpha': alpha,\n            'beta': beta,\n            'sharpe': sharpe,\n            'sortino': sortino,\n            'period_label': end_session.strftime(\"%Y-%m\"),\n            'trading_days': len(benchmark_returns),\n            'algo_volatility': ep.annual_volatility(algorithm_returns),\n            'benchmark_volatility': ep.annual_volatility(benchmark_returns),\n            'max_drawdown': ep.max_drawdown(algorithm_returns.values),\n            'max_leverage': algorithm_leverages.max(),\n        }\n\n        # check if a field in rval is nan or inf, and replace it with None\n        # except period_label which is always a str\n        return {\n            k: (\n                None\n                if k != 'period_label' and not np.isfinite(v) else\n                v\n            )\n            for k, v in iteritems(rval)\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_active_contract_at_offset(self, root_symbol, dt, offset):\n        oc = self.asset_finder.get_ordered_contracts(root_symbol)\n        session = self.trading_calendar.minute_to_session_label(dt)\n        front = oc.contract_before_auto_close(session.value)\n        back = oc.contract_at_offset(front, 1, dt.value)\n        if back is None:\n            return front\n        primary = self._active_contract(oc, front, back, session)\n        return oc.contract_at_offset(primary, offset, session.value)", "response": "Returns the active contract at the given date and offset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the FutureCOOKIE that is active at the given dt.", "response": "def get_contract_center(self, root_symbol, dt, offset):\n        \"\"\"\n        Parameters\n        ----------\n        root_symbol : str\n            The root symbol for the contract chain.\n        dt : Timestamp\n            The datetime for which to retrieve the current contract.\n        offset : int\n            The offset from the primary contract.\n            0 is the primary, 1 is the secondary, etc.\n\n        Returns\n        -------\n        Future\n            The active future contract at the given dt.\n        \"\"\"\n        return self._get_active_contract_at_offset(root_symbol, dt, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the rolls for the given root symbol start and end dates.", "response": "def get_rolls(self, root_symbol, start, end, offset):\n        \"\"\"\n        Get the rolls, i.e. the session at which to hop from contract to\n        contract in the chain.\n\n        Parameters\n        ----------\n        root_symbol : str\n            The root symbol for which to calculate rolls.\n        start : Timestamp\n            Start of the date range.\n        end : Timestamp\n            End of the date range.\n        offset : int\n            Offset from the primary.\n\n        Returns\n        -------\n        rolls - list[tuple(sid, roll_date)]\n            A list of rolls, where first value is the first active `sid`,\n        and the `roll_date` on which to hop to the next contract.\n            The last pair in the chain has a value of `None` since the roll\n            is after the range.\n        \"\"\"\n        oc = self.asset_finder.get_ordered_contracts(root_symbol)\n        front = self._get_active_contract_at_offset(root_symbol, end, 0)\n        back = oc.contract_at_offset(front, 1, end.value)\n        if back is not None:\n            end_session = self.trading_calendar.minute_to_session_label(end)\n            first = self._active_contract(oc, front, back, end_session)\n        else:\n            first = front\n        first_contract = oc.sid_to_contract[first]\n        rolls = [((first_contract >> offset).contract.sid, None)]\n        tc = self.trading_calendar\n        sessions = tc.sessions_in_range(tc.minute_to_session_label(start),\n                                        tc.minute_to_session_label(end))\n        freq = sessions.freq\n        if first == front:\n            # This is a bit tricky to grasp. Once we have the active contract\n            # on the given end date, we want to start walking backwards towards\n            # the start date and checking for rolls. For this, we treat the\n            # previous month's contract as the 'first' contract, and the\n            # contract we just found to be active as the 'back'. As we walk\n            # towards the start date, if the 'back' is no longer active, we add\n            # that date as a roll.\n            curr = first_contract << 1\n        else:\n            curr = first_contract << 2\n        session = sessions[-1]\n\n        while session > start and curr is not None:\n            front = curr.contract.sid\n            back = rolls[0][0]\n            prev_c = curr.prev\n            while session > start:\n                prev = session - freq\n                if prev_c is not None:\n                    if prev < prev_c.contract.auto_close_date:\n                        break\n                if back != self._active_contract(oc, front, back, prev):\n                    # TODO: Instead of listing each contract with its roll date\n                    # as tuples, create a series which maps every day to the\n                    # active contract on that day.\n                    rolls.insert(0, ((curr >> offset).contract.sid, session))\n                    break\n                session = prev\n            curr = curr.prev\n            if curr is not None:\n                session = min(session, curr.contract.auto_close_date + freq)\n\n        return rolls"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _active_contract(self, oc, front, back, dt):\n        front_contract = oc.sid_to_contract[front].contract\n        back_contract = oc.sid_to_contract[back].contract\n\n        tc = self.trading_calendar\n        trading_day = tc.day\n        prev = dt - trading_day\n        get_value = self.session_reader.get_value\n\n        # If the front contract is past its auto close date it cannot be the\n        # active contract, so return the back contract. Similarly, if the back\n        # contract has not even started yet, just return the front contract.\n        # The reason for using 'prev' to see if the contracts are alive instead\n        # of using 'dt' is because we need to get each contract's volume on the\n        # previous day, so we need to make sure that each contract exists on\n        # 'prev' in order to call 'get_value' below.\n        if dt > min(front_contract.auto_close_date, front_contract.end_date):\n            return back\n        elif front_contract.start_date > prev:\n            return back\n        elif dt > min(back_contract.auto_close_date, back_contract.end_date):\n            return front\n        elif back_contract.start_date > prev:\n            return front\n\n        front_vol = get_value(front, prev, 'volume')\n        back_vol = get_value(back, prev, 'volume')\n        if back_vol > front_vol:\n            return back\n\n        gap_start = max(\n            back_contract.start_date,\n            front_contract.auto_close_date - (trading_day * self.GRACE_DAYS),\n        )\n        gap_end = prev - trading_day\n        if dt < gap_start:\n            return front\n\n        # If we are within `self.GRACE_DAYS` of the front contract's auto close\n        # date, and a volume flip happened during that period, return the back\n        # contract as the active one.\n        sessions = tc.sessions_in_range(\n            tc.minute_to_session_label(gap_start),\n            tc.minute_to_session_label(gap_end),\n        )\n        for session in sessions:\n            front_vol = get_value(front, session, 'volume')\n            back_vol = get_value(back, session, 'volume')\n            if back_vol > front_vol:\n                return back\n        return front", "response": "r Returns the active contract based on the previous trading day s volume."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_contract_center(self, root_symbol, dt, offset):\n        # When determining the center contract on a specific day using volume\n        # rolls, simply picking the contract with the highest volume could\n        # cause flip-flopping between active contracts each day if the front\n        # and back contracts are close in volume. Therefore, information about\n        # the surrounding rolls is required. The `get_rolls` logic prevents\n        # contracts from being considered active once they have rolled, so\n        # incorporating that logic here prevents flip-flopping.\n        day = self.trading_calendar.day\n        end_date = min(\n            dt + (ROLL_DAYS_FOR_CURRENT_CONTRACT * day),\n            self.session_reader.last_available_dt,\n        )\n        rolls = self.get_rolls(\n            root_symbol=root_symbol, start=dt, end=end_date, offset=offset,\n        )\n        sid, acd = rolls[0]\n        return self.asset_finder.retrieve_asset(sid)", "response": "Returns the asset center of the current contract at the given dt."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _normalize_array(data, missing_value):\n    if isinstance(data, LabelArray):\n        return data, {}\n\n    data_dtype = data.dtype\n    if data_dtype in BOOL_DTYPES:\n        return data.astype(uint8), {'dtype': dtype(bool_)}\n    elif data_dtype in FLOAT_DTYPES:\n        return data.astype(float64), {'dtype': dtype(float64)}\n    elif data_dtype in INT_DTYPES:\n        return data.astype(int64), {'dtype': dtype(int64)}\n    elif is_categorical(data_dtype):\n        if not isinstance(missing_value, LabelArray.SUPPORTED_SCALAR_TYPES):\n            raise TypeError(\n                \"Invalid missing_value for categorical array.\\n\"\n                \"Expected None, bytes or unicode. Got %r.\" % missing_value,\n            )\n        return LabelArray(data, missing_value), {}\n    elif data_dtype.kind == 'M':\n        try:\n            outarray = data.astype('datetime64[ns]').view('int64')\n            return outarray, {'dtype': datetime64ns_dtype}\n        except OverflowError:\n            raise ValueError(\n                \"AdjustedArray received a datetime array \"\n                \"not representable as datetime64[ns].\\n\"\n                \"Min Date: %s\\n\"\n                \"Max Date: %s\\n\"\n                % (data.min(), data.max())\n            )\n    else:\n        raise TypeError(\n            \"Don't know how to construct AdjustedArray \"\n            \"on data of type %s.\" % data_dtype\n        )", "response": "Coerce buffer data for an AdjustedArray into a standard scalar\n    representation returning the coerced array and a dict of argument to pass to np. view to use when providing a user - facing view of the underlying data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _merge_simple(adjustment_lists, front_idx, back_idx):\n    if len(adjustment_lists) == 1:\n        return list(adjustment_lists[0])\n    else:\n        return adjustment_lists[front_idx] + adjustment_lists[back_idx]", "response": "Merge a list of adjustments into a single node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ensure_ndarray(ndarray_or_adjusted_array):\n    if isinstance(ndarray_or_adjusted_array, ndarray):\n        return ndarray_or_adjusted_array\n    elif isinstance(ndarray_or_adjusted_array, AdjustedArray):\n        return ndarray_or_adjusted_array.data\n    else:\n        raise TypeError(\n            \"Can't convert %s to ndarray\" %\n            type(ndarray_or_adjusted_array).__name__\n        )", "response": "Ensures that the input is a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that a window of length window_length is well - defined on data.", "response": "def _check_window_params(data, window_length):\n    \"\"\"\n    Check that a window of length `window_length` is well-defined on `data`.\n\n    Parameters\n    ----------\n    data : np.ndarray[ndim=2]\n        The array of data to check.\n    window_length : int\n        Length of the desired window.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    WindowLengthNotPositive\n        If window_length < 1.\n    WindowLengthTooLong\n        If window_length is greater than the number of rows in `data`.\n    \"\"\"\n    if window_length < 1:\n        raise WindowLengthNotPositive(window_length=window_length)\n\n    if window_length > data.shape[0]:\n        raise WindowLengthTooLong(\n            nrows=data.shape[0],\n            window_length=window_length,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmerge adjustments with existing adjustments handling index collisions according to method.", "response": "def update_adjustments(self, adjustments, method):\n        \"\"\"\n        Merge ``adjustments`` with existing adjustments, handling index\n        collisions according to ``method``.\n\n        Parameters\n        ----------\n        adjustments : dict[int -> list[Adjustment]]\n            The mapping of row indices to lists of adjustments that should be\n            appended to existing adjustments.\n        method : {'append', 'prepend'}\n            How to handle index collisions. If 'append', new adjustments will\n            be applied after previously-existing adjustments. If 'prepend', new\n            adjustments will be applied before previously-existing adjustments.\n        \"\"\"\n        try:\n            merge_func = _merge_methods[method]\n        except KeyError:\n            raise ValueError(\n                \"Invalid merge method %s\\n\"\n                \"Valid methods are: %s\" % (method, ', '.join(_merge_methods))\n            )\n\n        self.adjustments = merge_with(\n            merge_func,\n            self.adjustments,\n            adjustments,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _iterator_type(self):\n        if isinstance(self._data, LabelArray):\n            return LabelWindow\n        return CONCRETE_WINDOW_TYPES[self._data.dtype]", "response": "Returns the type of the data in this array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an iterator over the data in the current window.", "response": "def traverse(self,\n                 window_length,\n                 offset=0,\n                 perspective_offset=0):\n        \"\"\"\n        Produce an iterator rolling windows rows over our data.\n        Each emitted window will have `window_length` rows.\n\n        Parameters\n        ----------\n        window_length : int\n            The number of rows in each emitted window.\n        offset : int, optional\n            Number of rows to skip before the first window.  Default is 0.\n        perspective_offset : int, optional\n            Number of rows past the end of the current window from which to\n            \"view\" the underlying data.\n        \"\"\"\n        data = self._data.copy()\n        _check_window_params(data, window_length)\n        return self._iterator_type(\n            data,\n            self._view_kwargs,\n            self.adjustments,\n            offset,\n            window_length,\n            perspective_offset,\n            rounding_places=None,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inspect(self):\n        return dedent(\n            \"\"\"\\\n            Adjusted Array ({dtype}):\n\n            Data:\n            {data!r}\n\n            Adjustments:\n            {adjustments}\n            \"\"\"\n        ).format(\n            dtype=self.dtype.name,\n            data=self.data,\n            adjustments=self.adjustments,\n        )", "response": "Return a string representation of the data stored in this array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_labels(self, func):\n        if not isinstance(self.data, LabelArray):\n            raise TypeError(\n                'update_labels only supported if data is of type LabelArray.'\n            )\n\n        # Map the baseline values.\n        self._data = self._data.map(func)\n\n        # Map each of the adjustments.\n        for _, row_adjustments in iteritems(self.adjustments):\n            for adjustment in row_adjustments:\n                adjustment.value = func(adjustment.value)", "response": "Map a function over baseline and adjustment values in place."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_violation(self, asset, amount, datetime, metadata=None):\n        constraint = self._constraint_msg(metadata)\n\n        if self.on_error == 'fail':\n            raise TradingControlViolation(\n                asset=asset,\n                amount=amount,\n                datetime=datetime,\n                constraint=constraint)\n        elif self.on_error == 'log':\n            log.error(\"Order for {amount} shares of {asset} at {dt} \"\n                      \"violates trading constraint {constraint}\",\n                      amount=amount, asset=asset, dt=datetime,\n                      constraint=constraint)", "response": "Handle a TradingControlViolation by raising or logging and a TradingControlViolation error with information about the failure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate(self,\n                 asset,\n                 amount,\n                 portfolio,\n                 algo_datetime,\n                 algo_current_data):\n        \"\"\"\n        Fail if we've already placed self.max_count orders today.\n        \"\"\"\n        algo_date = algo_datetime.date()\n\n        # Reset order count if it's a new day.\n        if self.current_date and self.current_date != algo_date:\n            self.orders_placed = 0\n        self.current_date = algo_date\n\n        if self.orders_placed >= self.max_count:\n            self.handle_violation(asset, amount, algo_datetime)\n        self.orders_placed += 1", "response": "Validate the order count."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self,\n                 asset,\n                 amount,\n                 portfolio,\n                 algo_datetime,\n                 algo_current_data):\n        \"\"\"\n        Fail if the asset is in the restricted_list.\n        \"\"\"\n        if self.restrictions.is_restricted(asset, algo_datetime):\n            self.handle_violation(asset, amount, algo_datetime)", "response": "Validate the asset and amount."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfails if the magnitude of the given order exceeds either self.max_shares or self.max_notional.", "response": "def validate(self,\n                 asset,\n                 amount,\n                 portfolio,\n                 algo_datetime,\n                 algo_current_data):\n        \"\"\"\n        Fail if the magnitude of the given order exceeds either self.max_shares\n        or self.max_notional.\n        \"\"\"\n\n        if self.asset is not None and self.asset != asset:\n            return\n\n        if self.max_shares is not None and abs(amount) > self.max_shares:\n            self.handle_violation(asset, amount, algo_datetime)\n\n        current_asset_price = algo_current_data.current(asset, \"price\")\n        order_value = amount * current_asset_price\n\n        too_much_value = (self.max_notional is not None and\n                          abs(order_value) > self.max_notional)\n\n        if too_much_value:\n            self.handle_violation(asset, amount, algo_datetime)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating the order of the asset.", "response": "def validate(self,\n                 asset,\n                 amount,\n                 portfolio,\n                 algo_datetime,\n                 algo_current_data):\n        \"\"\"\n        Fail if the given order would cause the magnitude of our position to be\n        greater in shares than self.max_shares or greater in dollar value than\n        self.max_notional.\n        \"\"\"\n\n        if self.asset is not None and self.asset != asset:\n            return\n\n        current_share_count = portfolio.positions[asset].amount\n        shares_post_order = current_share_count + amount\n\n        too_many_shares = (self.max_shares is not None and\n                           abs(shares_post_order) > self.max_shares)\n        if too_many_shares:\n            self.handle_violation(asset, amount, algo_datetime)\n\n        current_price = algo_current_data.current(asset, \"price\")\n        value_post_order = shares_post_order * current_price\n\n        too_much_value = (self.max_notional is not None and\n                          abs(value_post_order) > self.max_notional)\n\n        if too_much_value:\n            self.handle_violation(asset, amount, algo_datetime)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that we have a valid asset and amount of shares.", "response": "def validate(self,\n                 asset,\n                 amount,\n                 portfolio,\n                 algo_datetime,\n                 algo_current_data):\n        \"\"\"\n        Fail if we would hold negative shares of asset after completing this\n        order.\n        \"\"\"\n        if portfolio.positions[asset].amount + amount < 0:\n            self.handle_violation(asset, amount, algo_datetime)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating the order of the asset.", "response": "def validate(self,\n                 asset,\n                 amount,\n                 portfolio,\n                 algo_datetime,\n                 algo_current_data):\n        \"\"\"\n        Fail if the algo has passed this Asset's end_date, or before the\n        Asset's start date.\n        \"\"\"\n        # If the order is for 0 shares, then silently pass through.\n        if amount == 0:\n            return\n\n        normalized_algo_dt = pd.Timestamp(algo_datetime).normalize()\n\n        # Fail if the algo is before this Asset's start_date\n        if asset.start_date:\n            normalized_start = pd.Timestamp(asset.start_date).normalize()\n            if normalized_algo_dt < normalized_start:\n                metadata = {\n                    'asset_start_date': normalized_start\n                }\n                self.handle_violation(\n                    asset, amount, algo_datetime, metadata=metadata)\n        # Fail if the algo has passed this Asset's end_date\n        if asset.end_date:\n            normalized_end = pd.Timestamp(asset.end_date).normalize()\n            if normalized_algo_dt > normalized_end:\n                metadata = {\n                    'asset_end_date': normalized_end\n                }\n                self.handle_violation(\n                    asset, amount, algo_datetime, metadata=metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the leverage of the account.", "response": "def validate(self,\n                 _portfolio,\n                 _account,\n                 _algo_datetime,\n                 _algo_current_data):\n        \"\"\"\n        Fail if the leverage is greater than the allowed leverage.\n        \"\"\"\n        if _account.leverage > self.max_leverage:\n            self.fail()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if we are after the deadline and if we are less than the min leverage", "response": "def validate(self,\n                 _portfolio,\n                 account,\n                 algo_datetime,\n                 _algo_current_data):\n        \"\"\"\n        Make validation checks if we are after the deadline.\n        Fail if the leverage is less than the min leverage.\n        \"\"\"\n        if (algo_datetime > self.deadline and\n                account.leverage < self.min_leverage):\n            self.fail()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef alter_columns(op, name, *columns, **kwargs):\n    selection_string = kwargs.pop('selection_string', None)\n    if kwargs:\n        raise TypeError(\n            'alter_columns received extra arguments: %r' % sorted(kwargs),\n        )\n    if selection_string is None:\n        selection_string = ', '.join(column.name for column in columns)\n\n    tmp_name = '_alter_columns_' + name\n    op.rename_table(name, tmp_name)\n\n    for column in columns:\n        # Clear any indices that already exist on this table, otherwise we will\n        # fail to create the table because the indices will already be present.\n        # When we create the table below, the indices that we want to preserve\n        # will just get recreated.\n        for table in name, tmp_name:\n            try:\n                op.drop_index('ix_%s_%s' % (table, column.name))\n            except sa.exc.OperationalError:\n                pass\n\n    op.create_table(name, *columns)\n    op.execute(\n        'insert into %s select %s from %s' % (\n            name,\n            selection_string,\n            tmp_name,\n        ),\n    )\n    op.drop_table(tmp_name)", "response": "Alter columns of a specific resource in a specific table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef downgrade(engine, desired_version):\n\n    # Check the version of the db at the engine\n    with engine.begin() as conn:\n        metadata = sa.MetaData(conn)\n        metadata.reflect()\n        version_info_table = metadata.tables['version_info']\n        starting_version = sa.select((version_info_table.c.version,)).scalar()\n\n        # Check for accidental upgrade\n        if starting_version < desired_version:\n            raise AssetDBImpossibleDowngrade(db_version=starting_version,\n                                             desired_version=desired_version)\n\n        # Check if the desired version is already the db version\n        if starting_version == desired_version:\n            # No downgrade needed\n            return\n\n        # Create alembic context\n        ctx = MigrationContext.configure(conn)\n        op = Operations(ctx)\n\n        # Integer keys of downgrades to run\n        # E.g.: [5, 4, 3, 2] would downgrade v6 to v2\n        downgrade_keys = range(desired_version, starting_version)[::-1]\n\n        # Disable foreign keys until all downgrades are complete\n        _pragma_foreign_keys(conn, False)\n\n        # Execute the downgrades in order\n        for downgrade_key in downgrade_keys:\n            _downgrade_methods[downgrade_key](op, conn, version_info_table)\n\n        # Re-enable foreign keys\n        _pragma_foreign_keys(conn, True)", "response": "Downgrades the assets db at the given engine to the desired version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _downgrade_v1(op):\n    # Drop indices before batch\n    # This is to prevent index collision when creating the temp table\n    op.drop_index('ix_futures_contracts_root_symbol')\n    op.drop_index('ix_futures_contracts_symbol')\n\n    # Execute batch op to allow column modification in SQLite\n    with op.batch_alter_table('futures_contracts') as batch_op:\n\n        # Rename 'multiplier'\n        batch_op.alter_column(column_name='multiplier',\n                              new_column_name='contract_multiplier')\n\n        # Delete 'tick_size'\n        batch_op.drop_column('tick_size')\n\n    # Recreate indices after batch\n    op.create_index('ix_futures_contracts_root_symbol',\n                    table_name='futures_contracts',\n                    columns=['root_symbol'])\n    op.create_index('ix_futures_contracts_symbol',\n                    table_name='futures_contracts',\n                    columns=['symbol'],\n                    unique=True)", "response": "Downgrade assets db by removing the tick_size column and renaming the multiplier column."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _downgrade_v2(op):\n    # Drop indices before batch\n    # This is to prevent index collision when creating the temp table\n    op.drop_index('ix_equities_fuzzy_symbol')\n    op.drop_index('ix_equities_company_symbol')\n\n    # Execute batch op to allow column modification in SQLite\n    with op.batch_alter_table('equities') as batch_op:\n        batch_op.drop_column('auto_close_date')\n\n    # Recreate indices after batch\n    op.create_index('ix_equities_fuzzy_symbol',\n                    table_name='equities',\n                    columns=['fuzzy_symbol'])\n    op.create_index('ix_equities_company_symbol',\n                    table_name='equities',\n                    columns=['company_symbol'])", "response": "Downgrade assets db by removing the auto_close_date column."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndowngrading assets db by adding a not null constraint on the asset_name.", "response": "def _downgrade_v3(op):\n    \"\"\"\n    Downgrade assets db by adding a not null constraint on\n    ``equities.first_traded``\n    \"\"\"\n    op.create_table(\n        '_new_equities',\n        sa.Column(\n            'sid',\n            sa.Integer,\n            unique=True,\n            nullable=False,\n            primary_key=True,\n        ),\n        sa.Column('symbol', sa.Text),\n        sa.Column('company_symbol', sa.Text),\n        sa.Column('share_class_symbol', sa.Text),\n        sa.Column('fuzzy_symbol', sa.Text),\n        sa.Column('asset_name', sa.Text),\n        sa.Column('start_date', sa.Integer, default=0, nullable=False),\n        sa.Column('end_date', sa.Integer, nullable=False),\n        sa.Column('first_traded', sa.Integer, nullable=False),\n        sa.Column('auto_close_date', sa.Integer),\n        sa.Column('exchange', sa.Text),\n    )\n    op.execute(\n        \"\"\"\n        insert into _new_equities\n        select * from equities\n        where equities.first_traded is not null\n        \"\"\",\n    )\n    op.drop_table('equities')\n    op.rename_table('_new_equities', 'equities')\n    # we need to make sure the indices have the proper names after the rename\n    op.create_index(\n        'ix_equities_company_symbol',\n        'equities',\n        ['company_symbol'],\n    )\n    op.create_index(\n        'ix_equities_fuzzy_symbol',\n        'equities',\n        ['fuzzy_symbol'],\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndowngrade assets db by copying the exchange_full column to exchange and then dropping the exchange_full column.", "response": "def _downgrade_v4(op):\n    \"\"\"\n    Downgrades assets db by copying the `exchange_full` column to `exchange`,\n    then dropping the `exchange_full` column.\n    \"\"\"\n    op.drop_index('ix_equities_fuzzy_symbol')\n    op.drop_index('ix_equities_company_symbol')\n\n    op.execute(\"UPDATE equities SET exchange = exchange_full\")\n\n    with op.batch_alter_table('equities') as batch_op:\n        batch_op.drop_column('exchange_full')\n\n    op.create_index('ix_equities_fuzzy_symbol',\n                    table_name='equities',\n                    columns=['fuzzy_symbol'])\n    op.create_index('ix_equities_company_symbol',\n                    table_name='equities',\n                    columns=['company_symbol'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a family of functions that read from the same metrics set mapping.", "response": "def _make_metrics_set_core():\n    \"\"\"Create a family of metrics sets functions that read from the same\n    metrics set mapping.\n\n    Returns\n    -------\n    metrics_sets : mappingproxy\n        The mapping of metrics sets to load functions.\n    register : callable\n        The function which registers new metrics sets in the ``metrics_sets``\n        mapping.\n    unregister : callable\n        The function which deregisters metrics sets from the ``metrics_sets``\n        mapping.\n    load : callable\n        The function which loads the ingested metrics sets back into memory.\n    \"\"\"\n    _metrics_sets = {}\n    # Expose _metrics_sets through a proxy so that users cannot mutate this\n    # accidentally. Users may go through `register` to update this which will\n    # warn when trampling another metrics set.\n    metrics_sets = mappingproxy(_metrics_sets)\n\n    def register(name, function=None):\n        \"\"\"Register a new metrics set.\n\n        Parameters\n        ----------\n        name : str\n            The name of the metrics set\n        function : callable\n            The callable which produces the metrics set.\n\n        Notes\n        -----\n        This may be used as a decorator if only ``name`` is passed.\n\n        See Also\n        --------\n        zipline.finance.metrics.get_metrics_set\n        zipline.finance.metrics.unregister_metrics_set\n        \"\"\"\n        if function is None:\n            # allow as decorator with just name.\n            return partial(register, name)\n\n        if name in _metrics_sets:\n            raise ValueError('metrics set %r is already registered' % name)\n\n        _metrics_sets[name] = function\n\n        return function\n\n    def unregister(name):\n        \"\"\"Unregister an existing metrics set.\n\n        Parameters\n        ----------\n        name : str\n            The name of the metrics set\n\n        See Also\n        --------\n        zipline.finance.metrics.register_metrics_set\n        \"\"\"\n        try:\n            del _metrics_sets[name]\n        except KeyError:\n            raise ValueError(\n                'metrics set %r was not already registered' % name,\n            )\n\n    def load(name):\n        \"\"\"Return an instance of the metrics set registered with the given name.\n\n        Returns\n        -------\n        metrics : set[Metric]\n            A new instance of the metrics set.\n\n        Raises\n        ------\n        ValueError\n            Raised when no metrics set is registered to ``name``\n        \"\"\"\n        try:\n            function = _metrics_sets[name]\n        except KeyError:\n            raise ValueError(\n                'no metrics set registered as %r, options are: %r' % (\n                    name,\n                    sorted(_metrics_sets),\n                ),\n            )\n\n        return function()\n\n    return metrics_sets, register, unregister, load"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies that the columns of events are valid.", "response": "def validate_column_specs(events, columns):\n    \"\"\"\n    Verify that the columns of ``events`` can be used by a\n    EarningsEstimatesLoader to serve the BoundColumns described by\n    `columns`.\n    \"\"\"\n    required = required_estimates_fields(columns)\n    received = set(events.columns)\n    missing = required - received\n    if missing:\n        raise ValueError(\n            \"EarningsEstimatesLoader missing required columns {missing}.\\n\"\n            \"Got Columns: {received}\\n\"\n            \"Expected Columns: {required}\".format(\n                missing=sorted(missing),\n                received=sorted(received),\n                required=sorted(required),\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting the requested data for each date. Parameters ---------- zero_qtr_data : pd.DataFrame The 'time zero' data for each calendar date per sid. zeroth_quarter_idx : pd.Index An index of calendar dates, sid, and normalized quarters, for only the rows that have a next or previous earnings estimate. stacked_last_per_qtr : pd.DataFrame The latest estimate known with the dates, normalized quarter, and sid as the index. num_announcements : int The number of annoucements out the user requested relative to each date in the calendar dates. dates : pd.DatetimeIndex The calendar dates for which estimates data is requested. Returns -------- requested_qtr_data : pd.DataFrame The DataFrame with the latest values for the requested quarter for all columns; `dates` are the index and columns are a MultiIndex with sids at the top level and the dataset columns on the bottom.", "response": "def get_requested_quarter_data(self,\n                                   zero_qtr_data,\n                                   zeroth_quarter_idx,\n                                   stacked_last_per_qtr,\n                                   num_announcements,\n                                   dates):\n        \"\"\"\n        Selects the requested data for each date.\n\n        Parameters\n        ----------\n        zero_qtr_data : pd.DataFrame\n            The 'time zero' data for each calendar date per sid.\n        zeroth_quarter_idx : pd.Index\n            An index of calendar dates, sid, and normalized quarters, for only\n            the rows that have a next or previous earnings estimate.\n        stacked_last_per_qtr : pd.DataFrame\n            The latest estimate known with the dates, normalized quarter, and\n            sid as the index.\n        num_announcements : int\n            The number of annoucements out the user requested relative to\n            each date in the calendar dates.\n        dates : pd.DatetimeIndex\n            The calendar dates for which estimates data is requested.\n\n        Returns\n        --------\n        requested_qtr_data : pd.DataFrame\n            The DataFrame with the latest values for the requested quarter\n            for all columns; `dates` are the index and columns are a MultiIndex\n            with sids at the top level and the dataset columns on the bottom.\n        \"\"\"\n        zero_qtr_data_idx = zero_qtr_data.index\n        requested_qtr_idx = pd.MultiIndex.from_arrays(\n            [\n                zero_qtr_data_idx.get_level_values(0),\n                zero_qtr_data_idx.get_level_values(1),\n                self.get_shifted_qtrs(\n                    zeroth_quarter_idx.get_level_values(\n                        NORMALIZED_QUARTERS,\n                    ),\n                    num_announcements,\n                ),\n            ],\n            names=[\n                zero_qtr_data_idx.names[0],\n                zero_qtr_data_idx.names[1],\n                SHIFTED_NORMALIZED_QTRS,\n            ],\n        )\n        requested_qtr_data = stacked_last_per_qtr.loc[requested_qtr_idx]\n        requested_qtr_data = requested_qtr_data.reset_index(\n            SHIFTED_NORMALIZED_QTRS,\n        )\n        # Calculate the actual year/quarter being requested and add those in\n        # as columns.\n        (requested_qtr_data[FISCAL_YEAR_FIELD_NAME],\n         requested_qtr_data[FISCAL_QUARTER_FIELD_NAME]) = \\\n            split_normalized_quarters(\n                requested_qtr_data[SHIFTED_NORMALIZED_QTRS]\n            )\n        # Once we're left with just dates as the index, we can reindex by all\n        # dates so that we have a value for each calendar date.\n        return requested_qtr_data.unstack(SID_FIELD_NAME).reindex(dates)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the index in dates where the split - adjusted - asof - date falls in.", "response": "def get_split_adjusted_asof_idx(self, dates):\n        \"\"\"\n        Compute the index in `dates` where the split-adjusted-asof-date\n        falls. This is the date up to which, and including which, we will\n        need to unapply all adjustments for and then re-apply them as they\n        come in. After this date, adjustments are applied as normal.\n\n        Parameters\n        ----------\n        dates : pd.DatetimeIndex\n            The calendar dates over which the Pipeline is being computed.\n\n        Returns\n        -------\n        split_adjusted_asof_idx : int\n            The index in `dates` at which the data should be split.\n        \"\"\"\n        split_adjusted_asof_idx = dates.searchsorted(\n            self._split_adjusted_asof\n        )\n        # The split-asof date is after the date index.\n        if split_adjusted_asof_idx == len(dates):\n            split_adjusted_asof_idx = len(dates) - 1\n        elif self._split_adjusted_asof < dates[0].tz_localize(None):\n            split_adjusted_asof_idx = -1\n        return split_adjusted_asof_idx"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef collect_overwrites_for_sid(self,\n                                   group,\n                                   dates,\n                                   requested_qtr_data,\n                                   last_per_qtr,\n                                   sid_idx,\n                                   columns,\n                                   all_adjustments_for_sid,\n                                   sid):\n        \"\"\"\n        Given a sid, collect all overwrites that should be applied for this\n        sid at each quarter boundary.\n\n        Parameters\n        ----------\n        group : pd.DataFrame\n            The data for `sid`.\n        dates : pd.DatetimeIndex\n            The calendar dates for which estimates data is requested.\n        requested_qtr_data : pd.DataFrame\n            The DataFrame with the latest values for the requested quarter\n            for all columns.\n        last_per_qtr : pd.DataFrame\n            A DataFrame with a column MultiIndex of [self.estimates.columns,\n            normalized_quarters, sid] that allows easily getting the timeline\n            of estimates for a particular sid for a particular quarter.\n        sid_idx : int\n            The sid's index in the asset index.\n        columns : list of BoundColumn\n            The columns for which the overwrites should be computed.\n        all_adjustments_for_sid : dict[int -> AdjustedArray]\n            A dictionary of the integer index of each timestamp into the date\n            index, mapped to adjustments that should be applied at that\n            index for the given sid (`sid`). This dictionary is modified as\n            adjustments are collected.\n        sid : int\n            The sid for which overwrites should be computed.\n        \"\"\"\n        # If data was requested for only 1 date, there can never be any\n        # overwrites, so skip the extra work.\n        if len(dates) == 1:\n            return\n\n        next_qtr_start_indices = dates.searchsorted(\n            group[EVENT_DATE_FIELD_NAME].values,\n            side=self.searchsorted_side,\n        )\n\n        qtrs_with_estimates = group.index.get_level_values(\n            NORMALIZED_QUARTERS\n        ).values\n        for idx in next_qtr_start_indices:\n            if 0 < idx < len(dates):\n                # Find the quarter being requested in the quarter we're\n                # crossing into.\n                requested_quarter = requested_qtr_data[\n                    SHIFTED_NORMALIZED_QTRS, sid,\n                ].iloc[idx]\n                # Only add adjustments if the next quarter starts somewhere\n                # in our date index for this sid. Our 'next' quarter can\n                # never start at index 0; a starting index of 0 means that\n                # the next quarter's event date was NaT.\n                self.create_overwrites_for_quarter(\n                    all_adjustments_for_sid,\n                    idx,\n                    last_per_qtr,\n                    qtrs_with_estimates,\n                    requested_quarter,\n                    sid,\n                    sid_idx,\n                    columns\n                )", "response": "Given a sid collect all overwrites that should be applied for that sid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_into_adjustments_for_all_sids(self,\n                                            all_adjustments_for_sid,\n                                            col_to_all_adjustments):\n        \"\"\"\n        Merge adjustments for a particular sid into a dictionary containing\n        adjustments for all sids.\n\n        Parameters\n        ----------\n        all_adjustments_for_sid : dict[int -> AdjustedArray]\n            All adjustments for a particular sid.\n        col_to_all_adjustments : dict[int -> AdjustedArray]\n            All adjustments for all sids.\n        \"\"\"\n\n        for col_name in all_adjustments_for_sid:\n            if col_name not in col_to_all_adjustments:\n                col_to_all_adjustments[col_name] = {}\n            for ts in all_adjustments_for_sid[col_name]:\n                adjs = all_adjustments_for_sid[col_name][ts]\n                add_new_adjustments(col_to_all_adjustments,\n                                    adjs,\n                                    col_name,\n                                    ts)", "response": "Merge adjustments for a particular sid into a dictionary containing all adjustments for all sids."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_adjustments(self,\n                        zero_qtr_data,\n                        requested_qtr_data,\n                        last_per_qtr,\n                        dates,\n                        assets,\n                        columns,\n                        **kwargs):\n        \"\"\"\n        Creates an AdjustedArray from the given estimates data for the given\n        dates.\n\n        Parameters\n        ----------\n        zero_qtr_data : pd.DataFrame\n            The 'time zero' data for each calendar date per sid.\n        requested_qtr_data : pd.DataFrame\n            The requested quarter data for each calendar date per sid.\n        last_per_qtr : pd.DataFrame\n            A DataFrame with a column MultiIndex of [self.estimates.columns,\n            normalized_quarters, sid] that allows easily getting the timeline\n            of estimates for a particular sid for a particular quarter.\n        dates : pd.DatetimeIndex\n            The calendar dates for which estimates data is requested.\n        assets : pd.Int64Index\n            An index of all the assets from the raw data.\n        columns : list of BoundColumn\n            The columns for which adjustments need to be calculated.\n        kwargs :\n            Additional keyword arguments that should be forwarded to\n            `get_adjustments_for_sid` and to be used in computing adjustments\n            for each sid.\n\n        Returns\n        -------\n        col_to_all_adjustments : dict[int -> AdjustedArray]\n            A dictionary of all adjustments that should be applied.\n        \"\"\"\n\n        zero_qtr_data.sort_index(inplace=True)\n        # Here we want to get the LAST record from each group of records\n        # corresponding to a single quarter. This is to ensure that we select\n        # the most up-to-date event date in case the event date changes.\n        quarter_shifts = zero_qtr_data.groupby(\n            level=[SID_FIELD_NAME, NORMALIZED_QUARTERS]\n        ).nth(-1)\n\n        col_to_all_adjustments = {}\n        sid_to_idx = dict(zip(assets, range(len(assets))))\n        quarter_shifts.groupby(level=SID_FIELD_NAME).apply(\n            self.get_adjustments_for_sid,\n            dates,\n            requested_qtr_data,\n            last_per_qtr,\n            sid_to_idx,\n            columns,\n            col_to_all_adjustments,\n            **kwargs\n        )\n        return col_to_all_adjustments", "response": "This method creates an AdjustedArray from the given estimates data for the given calendar dates requested by the given quarter and the given assets and the given columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a list of overwrites for the given quarter.", "response": "def create_overwrites_for_quarter(self,\n                                      col_to_overwrites,\n                                      next_qtr_start_idx,\n                                      last_per_qtr,\n                                      quarters_with_estimates_for_sid,\n                                      requested_quarter,\n                                      sid,\n                                      sid_idx,\n                                      columns):\n        \"\"\"\n        Add entries to the dictionary of columns to adjustments for the given\n        sid and the given quarter.\n\n        Parameters\n        ----------\n        col_to_overwrites : dict [column_name -> list of ArrayAdjustment]\n            A dictionary mapping column names to all overwrites for those\n            columns.\n        next_qtr_start_idx : int\n            The index of the first day of the next quarter in the calendar\n            dates.\n        last_per_qtr : pd.DataFrame\n            A DataFrame with a column MultiIndex of [self.estimates.columns,\n            normalized_quarters, sid] that allows easily getting the timeline\n            of estimates for a particular sid for a particular quarter; this\n            is particularly useful for getting adjustments for 'next'\n            estimates.\n        quarters_with_estimates_for_sid : np.array\n            An array of all quarters for which there are estimates for the\n            given sid.\n        requested_quarter : float\n            The quarter for which the overwrite should be created.\n        sid : int\n            The sid for which to create overwrites.\n        sid_idx : int\n            The index of the sid in `assets`.\n        columns : list of BoundColumn\n            The columns for which to create overwrites.\n        \"\"\"\n        for col in columns:\n            column_name = self.name_map[col.name]\n            if column_name not in col_to_overwrites:\n                col_to_overwrites[column_name] = {}\n            # If there are estimates for the requested quarter,\n            # overwrite all values going up to the starting index of\n            # that quarter with estimates for that quarter.\n            if requested_quarter in quarters_with_estimates_for_sid:\n                adjs = self.create_overwrite_for_estimate(\n                    col,\n                    column_name,\n                    last_per_qtr,\n                    next_qtr_start_idx,\n                    requested_quarter,\n                    sid,\n                    sid_idx,\n                )\n                add_new_adjustments(col_to_overwrites,\n                                    adjs,\n                                    column_name,\n                                    next_qtr_start_idx)\n            # There are no estimates for the quarter. Overwrite all\n            # values going up to the starting index of that quarter\n            # with the missing value for this column.\n            else:\n                adjs = [self.overwrite_with_null(\n                        col,\n                        next_qtr_start_idx,\n                        sid_idx)]\n                add_new_adjustments(col_to_overwrites,\n                                    adjs,\n                                    column_name,\n                                    next_qtr_start_idx)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_last_data_per_qtr(self,\n                              assets_with_data,\n                              columns,\n                              dates,\n                              data_query_cutoff_times):\n        \"\"\"\n        Determine the last piece of information we know for each column on each\n        date in the index for each sid and quarter.\n\n        Parameters\n        ----------\n        assets_with_data : pd.Index\n            Index of all assets that appear in the raw data given to the\n            loader.\n        columns : iterable of BoundColumn\n            The columns that need to be loaded from the raw data.\n        data_query_cutoff_times : pd.DatetimeIndex\n            The calendar of dates for which data should be loaded.\n\n        Returns\n        -------\n        stacked_last_per_qtr : pd.DataFrame\n            A DataFrame indexed by [dates, sid, normalized_quarters] that has\n            the latest information for each row of the index, sorted by event\n            date.\n        last_per_qtr : pd.DataFrame\n            A DataFrame with columns that are a MultiIndex of [\n            self.estimates.columns, normalized_quarters, sid].\n        \"\"\"\n        # Get a DataFrame indexed by date with a MultiIndex of columns of\n        # [self.estimates.columns, normalized_quarters, sid], where each cell\n        # contains the latest data for that day.\n        last_per_qtr = last_in_date_group(\n            self.estimates,\n            data_query_cutoff_times,\n            assets_with_data,\n            reindex=True,\n            extra_groupers=[NORMALIZED_QUARTERS],\n        )\n        last_per_qtr.index = dates\n        # Forward fill values for each quarter/sid/dataset column.\n        ffill_across_cols(last_per_qtr, columns, self.name_map)\n        # Stack quarter and sid into the index.\n        stacked_last_per_qtr = last_per_qtr.stack(\n            [SID_FIELD_NAME, NORMALIZED_QUARTERS],\n        )\n        # Set date index name for ease of reference\n        stacked_last_per_qtr.index.set_names(\n            SIMULATION_DATES,\n            level=0,\n            inplace=True,\n        )\n        stacked_last_per_qtr = stacked_last_per_qtr.sort_values(\n            EVENT_DATE_FIELD_NAME,\n        )\n        stacked_last_per_qtr[EVENT_DATE_FIELD_NAME] = pd.to_datetime(\n            stacked_last_per_qtr[EVENT_DATE_FIELD_NAME]\n        )\n        return last_per_qtr, stacked_last_per_qtr", "response": "This method returns a DataFrame that contains the last piece of information for each quarter and sid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_zeroth_quarter_idx(self, stacked_last_per_qtr):\n        previous_releases_per_date = stacked_last_per_qtr.loc[\n            stacked_last_per_qtr[EVENT_DATE_FIELD_NAME] <=\n            stacked_last_per_qtr.index.get_level_values(SIMULATION_DATES)\n        ].groupby(\n            level=[SIMULATION_DATES, SID_FIELD_NAME],\n            as_index=False,\n            # Here we take advantage of the fact that `stacked_last_per_qtr` is\n            # sorted by event date.\n        ).nth(-1)\n        return previous_releases_per_date.index", "response": "Returns a MultiIndex of the most recent release for each quarter in the index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_adjustments_for_sid(self,\n                                group,\n                                dates,\n                                requested_qtr_data,\n                                last_per_qtr,\n                                sid_to_idx,\n                                columns,\n                                col_to_all_adjustments,\n                                split_adjusted_asof_idx=None,\n                                split_adjusted_cols_for_group=None):\n        \"\"\"\n        Collects both overwrites and adjustments for a particular sid.\n\n        Parameters\n        ----------\n        split_adjusted_asof_idx : int\n            The integer index of the date on which the data was split-adjusted.\n        split_adjusted_cols_for_group : list of str\n            The names of requested columns that should also be split-adjusted.\n        \"\"\"\n        all_adjustments_for_sid = {}\n        sid = int(group.name)\n        self.collect_overwrites_for_sid(group,\n                                        dates,\n                                        requested_qtr_data,\n                                        last_per_qtr,\n                                        sid_to_idx[sid],\n                                        columns,\n                                        all_adjustments_for_sid,\n                                        sid)\n        (pre_adjustments,\n         post_adjustments) = self.retrieve_split_adjustment_data_for_sid(\n            dates, sid, split_adjusted_asof_idx\n        )\n        sid_estimates = self.estimates[\n            self.estimates[SID_FIELD_NAME] == sid\n            ]\n        # We might not have any overwrites but still have\n        # adjustments, and we will need to manually add columns if\n        # that is the case.\n        for col_name in split_adjusted_cols_for_group:\n            if col_name not in all_adjustments_for_sid:\n                all_adjustments_for_sid[col_name] = {}\n\n        self.collect_split_adjustments(\n            all_adjustments_for_sid,\n            requested_qtr_data,\n            dates,\n            sid,\n            sid_to_idx[sid],\n            sid_estimates,\n            split_adjusted_asof_idx,\n            pre_adjustments,\n            post_adjustments,\n            split_adjusted_cols_for_group\n        )\n        self.merge_into_adjustments_for_all_sids(\n            all_adjustments_for_sid, col_to_all_adjustments\n        )", "response": "This method returns all adjustments for a particular sid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_adjustments(self,\n                        zero_qtr_data,\n                        requested_qtr_data,\n                        last_per_qtr,\n                        dates,\n                        assets,\n                        columns,\n                        **kwargs):\n        \"\"\"\n        Calculates both split adjustments and overwrites for all sids.\n        \"\"\"\n        split_adjusted_cols_for_group = [\n            self.name_map[col.name]\n            for col in columns\n            if self.name_map[col.name] in self._split_adjusted_column_names\n        ]\n        # Add all splits to the adjustment dict for this sid.\n        split_adjusted_asof_idx = self.get_split_adjusted_asof_idx(\n            dates\n        )\n        return super(SplitAdjustedEstimatesLoader, self).get_adjustments(\n            zero_qtr_data,\n            requested_qtr_data,\n            last_per_qtr,\n            dates,\n            assets,\n            columns,\n            split_adjusted_cols_for_group=split_adjusted_cols_for_group,\n            split_adjusted_asof_idx=split_adjusted_asof_idx\n        )", "response": "Returns a dictionary of split adjustments for all sids."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the date until which the adjustment at the given date index should be applied for the given quarter. Parameters ---------- adjustment_ts : pd.Timestamp The timestamp at which the adjustment occurs. dates : pd.DatetimeIndex The calendar dates over which the Pipeline is being computed. upper_bound : int The index of the upper bound in the calendar dates. This is the index until which the adjusment will be applied unless there is information for the requested quarter that comes in on or before that date. requested_quarter : float The quarter for which we are determining how the adjustment should be applied. sid_estimates : pd.DataFrame The DataFrame of estimates data for the sid for which we're applying the given adjustment. Returns ------- end_idx : int The last index to which the adjustment should be applied for the given quarter/sid.", "response": "def determine_end_idx_for_adjustment(self,\n                                         adjustment_ts,\n                                         dates,\n                                         upper_bound,\n                                         requested_quarter,\n                                         sid_estimates):\n        \"\"\"\n        Determines the date until which the adjustment at the given date\n        index should be applied for the given quarter.\n\n        Parameters\n        ----------\n        adjustment_ts : pd.Timestamp\n            The timestamp at which the adjustment occurs.\n        dates : pd.DatetimeIndex\n            The calendar dates over which the Pipeline is being computed.\n        upper_bound : int\n            The index of the upper bound in the calendar dates. This is the\n            index until which the adjusment will be applied unless there is\n            information for the requested quarter that comes in on or before\n            that date.\n        requested_quarter : float\n            The quarter for which we are determining how the adjustment\n            should be applied.\n        sid_estimates : pd.DataFrame\n            The DataFrame of estimates data for the sid for which we're\n            applying the given adjustment.\n\n        Returns\n        -------\n        end_idx : int\n            The last index to which the adjustment should be applied for the\n            given quarter/sid.\n        \"\"\"\n        end_idx = upper_bound\n        # Find the next newest kd that happens on or after\n        # the date of this adjustment\n        newest_kd_for_qtr = sid_estimates[\n            (sid_estimates[NORMALIZED_QUARTERS] == requested_quarter) &\n            (sid_estimates[TS_FIELD_NAME] >= adjustment_ts)\n        ][TS_FIELD_NAME].min()\n        if pd.notnull(newest_kd_for_qtr):\n            newest_kd_idx = dates.searchsorted(\n                newest_kd_for_qtr\n            )\n            # We have fresh information that comes in\n            # before the end of the overwrite and\n            # presumably is already split-adjusted to the\n            # current split. We should stop applying the\n            # adjustment the day before this new\n            # information comes in.\n            if newest_kd_idx <= upper_bound:\n                end_idx = newest_kd_idx - 1\n        return end_idx"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collect_pre_split_asof_date_adjustments(\n            self,\n            split_adjusted_asof_date_idx,\n            sid_idx,\n            pre_adjustments,\n            requested_split_adjusted_columns\n    ):\n        \"\"\"\n        Collect split adjustments that occur before the\n        split-adjusted-asof-date. All those adjustments must first be\n        UN-applied at the first date index and then re-applied on the\n        appropriate dates in order to match point in time share pricing data.\n\n        Parameters\n        ----------\n        split_adjusted_asof_date_idx : int\n            The index in the calendar dates as-of which all data was\n            split-adjusted.\n        sid_idx : int\n            The index of the sid for which adjustments should be collected in\n            the adjusted array.\n        pre_adjustments : tuple(list(float), list(int))\n            The adjustment values, indexes in `dates`, and timestamps for\n            adjustments that happened after the split-asof-date.\n        requested_split_adjusted_columns : list of str\n            The requested split adjusted columns.\n\n        Returns\n        -------\n        col_to_split_adjustments : dict[str -> dict[int -> list of Adjustment]]\n            The adjustments for this sid that occurred on or before the\n            split-asof-date.\n        \"\"\"\n        col_to_split_adjustments = {}\n        if len(pre_adjustments[0]):\n            adjustment_values, date_indexes = pre_adjustments\n            for column_name in requested_split_adjusted_columns:\n                col_to_split_adjustments[column_name] = {}\n                # We need to undo all adjustments that happen before the\n                # split_asof_date here by reversing the split ratio.\n                col_to_split_adjustments[column_name][0] = [Float64Multiply(\n                    0,\n                    split_adjusted_asof_date_idx,\n                    sid_idx,\n                    sid_idx,\n                    1 / future_adjustment\n                ) for future_adjustment in adjustment_values]\n\n                for adjustment, date_index in zip(adjustment_values,\n                                                  date_indexes):\n                    adj = Float64Multiply(\n                        0,\n                        split_adjusted_asof_date_idx,\n                        sid_idx,\n                        sid_idx,\n                        adjustment\n                    )\n                    add_new_adjustments(col_to_split_adjustments,\n                                        [adj],\n                                        column_name,\n                                        date_index)\n\n        return col_to_split_adjustments", "response": "Collect split adjustments that occur before the split - asof - date."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollect split adjustments that occur after the split - asof - date.", "response": "def collect_post_asof_split_adjustments(self,\n                                            post_adjustments,\n                                            requested_qtr_data,\n                                            sid,\n                                            sid_idx,\n                                            sid_estimates,\n                                            requested_split_adjusted_columns):\n        \"\"\"\n        Collect split adjustments that occur after the\n        split-adjusted-asof-date. Each adjustment needs to be applied to all\n        dates on which knowledge for the requested quarter was older than the\n        date of the adjustment.\n\n        Parameters\n        ----------\n        post_adjustments : tuple(list(float), list(int), pd.DatetimeIndex)\n            The adjustment values, indexes in `dates`, and timestamps for\n            adjustments that happened after the split-asof-date.\n        requested_qtr_data : pd.DataFrame\n            The requested quarter data for each calendar date per sid.\n        sid : int\n            The sid for which adjustments need to be collected.\n        sid_idx : int\n            The index of `sid` in the adjusted array.\n        sid_estimates : pd.DataFrame\n            The raw estimates data for this sid.\n        requested_split_adjusted_columns : list of str\n            The requested split adjusted columns.\n        Returns\n        -------\n        col_to_split_adjustments : dict[str -> dict[int -> list of Adjustment]]\n            The adjustments for this sid that occurred after the\n            split-asof-date.\n        \"\"\"\n        col_to_split_adjustments = {}\n        if post_adjustments:\n            # Get an integer index\n            requested_qtr_timeline = requested_qtr_data[\n                SHIFTED_NORMALIZED_QTRS\n            ][sid].reset_index()\n            requested_qtr_timeline = requested_qtr_timeline[\n                requested_qtr_timeline[sid].notnull()\n            ]\n\n            # Split the data into range by quarter and determine which quarter\n            # was being requested in each range.\n            # Split integer indexes up by quarter range\n            qtr_ranges_idxs = np.split(\n                requested_qtr_timeline.index,\n                np.where(np.diff(requested_qtr_timeline[sid]) != 0)[0] + 1\n            )\n            requested_quarters_per_range = [requested_qtr_timeline[sid][r[0]]\n                                            for r in qtr_ranges_idxs]\n            # Try to apply each adjustment to each quarter range.\n            for i, qtr_range in enumerate(qtr_ranges_idxs):\n                for adjustment, date_index, timestamp in zip(\n                        *post_adjustments\n                ):\n                    # In the default case, apply through the end of the quarter\n                    upper_bound = qtr_range[-1]\n                    # Find the smallest KD in estimates that is on or after the\n                    # date of the given adjustment. Apply the given adjustment\n                    # until that KD.\n                    end_idx = self.determine_end_idx_for_adjustment(\n                        timestamp,\n                        requested_qtr_data.index,\n                        upper_bound,\n                        requested_quarters_per_range[i],\n                        sid_estimates\n                    )\n                    # In the default case, apply adjustment on the first day of\n                    #  the quarter.\n                    start_idx = qtr_range[0]\n                    # If the adjustment happens during this quarter, apply the\n                    # adjustment on the day it happens.\n                    if date_index > start_idx:\n                        start_idx = date_index\n                    # We only want to apply the adjustment if we have any stale\n                    # data to apply it to.\n                    if qtr_range[0] <= end_idx:\n                        for column_name in requested_split_adjusted_columns:\n                            if column_name not in col_to_split_adjustments:\n                                col_to_split_adjustments[column_name] = {}\n                            adj = Float64Multiply(\n                                # Always apply from first day of qtr\n                                qtr_range[0],\n                                end_idx,\n                                sid_idx,\n                                sid_idx,\n                                adjustment\n                            )\n                            add_new_adjustments(\n                                col_to_split_adjustments,\n                                [adj],\n                                column_name,\n                                start_idx\n                            )\n\n        return col_to_split_adjustments"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef retrieve_split_adjustment_data_for_sid(self,\n                                               dates,\n                                               sid,\n                                               split_adjusted_asof_idx):\n        \"\"\"\n        dates : pd.DatetimeIndex\n            The calendar dates.\n        sid : int\n            The sid for which we want to retrieve adjustments.\n        split_adjusted_asof_idx : int\n            The index in `dates` as-of which the data is split adjusted.\n\n        Returns\n        -------\n        pre_adjustments : tuple(list(float), list(int), pd.DatetimeIndex)\n            The adjustment values and indexes in `dates` for\n            adjustments that happened before the split-asof-date.\n        post_adjustments : tuple(list(float), list(int), pd.DatetimeIndex)\n            The adjustment values, indexes in `dates`, and timestamps for\n            adjustments that happened after the split-asof-date.\n        \"\"\"\n        adjustments = self._split_adjustments.get_adjustments_for_sid(\n            'splits', sid\n        )\n        sorted(adjustments, key=lambda adj: adj[0])\n        # Get rid of any adjustments that happen outside of our date index.\n        adjustments = list(filter(lambda x: dates[0] <= x[0] <= dates[-1],\n                                  adjustments))\n        adjustment_values = np.array([adj[1] for adj in adjustments])\n        timestamps = pd.DatetimeIndex([adj[0] for adj in adjustments])\n        # We need the first date on which we would have known about each\n        # adjustment.\n        date_indexes = dates.searchsorted(timestamps)\n        pre_adjustment_idxs = np.where(\n            date_indexes <= split_adjusted_asof_idx\n        )[0]\n        last_adjustment_split_asof_idx = -1\n        if len(pre_adjustment_idxs):\n            last_adjustment_split_asof_idx = pre_adjustment_idxs.max()\n        pre_adjustments = (\n            adjustment_values[:last_adjustment_split_asof_idx + 1],\n            date_indexes[:last_adjustment_split_asof_idx + 1]\n        )\n        post_adjustments = (\n            adjustment_values[last_adjustment_split_asof_idx + 1:],\n            date_indexes[last_adjustment_split_asof_idx + 1:],\n            timestamps[last_adjustment_split_asof_idx + 1:]\n        )\n        return pre_adjustments, post_adjustments", "response": "Retrieves the split adjustments for a specific sid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmerging split adjustments with overwrites.", "response": "def merge_split_adjustments_with_overwrites(\n        self,\n        pre,\n        post,\n        overwrites,\n        requested_split_adjusted_columns\n    ):\n        \"\"\"\n        Merge split adjustments with the dict containing overwrites.\n\n        Parameters\n        ----------\n        pre : dict[str -> dict[int -> list]]\n            The adjustments that occur before the split-adjusted-asof-date.\n        post : dict[str -> dict[int -> list]]\n            The adjustments that occur after the split-adjusted-asof-date.\n        overwrites : dict[str -> dict[int -> list]]\n            The overwrites across all time. Adjustments will be merged into\n            this dictionary.\n        requested_split_adjusted_columns : list of str\n            List of names of split adjusted columns that are being requested.\n        \"\"\"\n        for column_name in requested_split_adjusted_columns:\n            # We can do a merge here because the timestamps in 'pre' and\n            # 'post' are guaranteed to not overlap.\n            if pre:\n                # Either empty or contains all columns.\n                for ts in pre[column_name]:\n                    add_new_adjustments(\n                        overwrites,\n                        pre[column_name][ts],\n                        column_name,\n                        ts\n                    )\n            if post:\n                # Either empty or contains all columns.\n                for ts in post[column_name]:\n                    add_new_adjustments(\n                        overwrites,\n                        post[column_name][ts],\n                        column_name,\n                        ts\n                    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollect split adjustments for previous quarters and apply them to the given dictionary of splits.", "response": "def collect_split_adjustments(self,\n                                  adjustments_for_sid,\n                                  requested_qtr_data,\n                                  dates,\n                                  sid,\n                                  sid_idx,\n                                  sid_estimates,\n                                  split_adjusted_asof_idx,\n                                  pre_adjustments,\n                                  post_adjustments,\n                                  requested_split_adjusted_columns):\n        \"\"\"\n        Collect split adjustments for previous quarters and apply them to the\n        given dictionary of splits for the given sid. Since overwrites just\n        replace all estimates before the new quarter with NaN, we don't need to\n        worry about re-applying split adjustments.\n\n        Parameters\n        ----------\n        adjustments_for_sid : dict[str -> dict[int -> list]]\n            The dictionary of adjustments to which splits need to be added.\n            Initially it contains only overwrites.\n        requested_qtr_data : pd.DataFrame\n            The requested quarter data for each calendar date per sid.\n        dates : pd.DatetimeIndex\n            The calendar dates for which estimates data is requested.\n        sid : int\n            The sid for which adjustments need to be collected.\n        sid_idx : int\n            The index of `sid` in the adjusted array.\n        sid_estimates : pd.DataFrame\n            The raw estimates data for the given sid.\n        split_adjusted_asof_idx : int\n            The index in `dates` as-of which the data is split adjusted.\n        pre_adjustments : tuple(list(float), list(int), pd.DatetimeIndex)\n            The adjustment values and indexes in `dates` for\n            adjustments that happened before the split-asof-date.\n        post_adjustments : tuple(list(float), list(int), pd.DatetimeIndex)\n            The adjustment values, indexes in `dates`, and timestamps for\n            adjustments that happened after the split-asof-date.\n        requested_split_adjusted_columns : list of str\n            List of requested split adjusted column names.\n        \"\"\"\n        (pre_adjustments_dict,\n         post_adjustments_dict) = self._collect_adjustments(\n            requested_qtr_data,\n            sid,\n            sid_idx,\n            sid_estimates,\n            split_adjusted_asof_idx,\n            pre_adjustments,\n            post_adjustments,\n            requested_split_adjusted_columns\n        )\n        self.merge_split_adjustments_with_overwrites(\n            pre_adjustments_dict,\n            post_adjustments_dict,\n            adjustments_for_sid,\n            requested_split_adjusted_columns\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef collect_split_adjustments(self,\n                                  adjustments_for_sid,\n                                  requested_qtr_data,\n                                  dates,\n                                  sid,\n                                  sid_idx,\n                                  sid_estimates,\n                                  split_adjusted_asof_idx,\n                                  pre_adjustments,\n                                  post_adjustments,\n                                  requested_split_adjusted_columns):\n        \"\"\"\n        Collect split adjustments for future quarters. Re-apply adjustments\n        that would be overwritten by overwrites. Merge split adjustments with\n        overwrites into the given dictionary of splits for the given sid.\n\n        Parameters\n        ----------\n        adjustments_for_sid : dict[str -> dict[int -> list]]\n            The dictionary of adjustments to which splits need to be added.\n            Initially it contains only overwrites.\n        requested_qtr_data : pd.DataFrame\n            The requested quarter data for each calendar date per sid.\n        dates : pd.DatetimeIndex\n            The calendar dates for which estimates data is requested.\n        sid : int\n            The sid for which adjustments need to be collected.\n        sid_idx : int\n            The index of `sid` in the adjusted array.\n        sid_estimates : pd.DataFrame\n            The raw estimates data for the given sid.\n        split_adjusted_asof_idx : int\n            The index in `dates` as-of which the data is split adjusted.\n        pre_adjustments : tuple(list(float), list(int), pd.DatetimeIndex)\n            The adjustment values and indexes in `dates` for\n            adjustments that happened before the split-asof-date.\n        post_adjustments : tuple(list(float), list(int), pd.DatetimeIndex)\n            The adjustment values, indexes in `dates`, and timestamps for\n            adjustments that happened after the split-asof-date.\n        requested_split_adjusted_columns : list of str\n            List of requested split adjusted column names.\n        \"\"\"\n        (pre_adjustments_dict,\n         post_adjustments_dict) = self._collect_adjustments(\n            requested_qtr_data,\n            sid,\n            sid_idx,\n            sid_estimates,\n            split_adjusted_asof_idx,\n            pre_adjustments,\n            post_adjustments,\n            requested_split_adjusted_columns,\n        )\n        for column_name in requested_split_adjusted_columns:\n            for overwrite_ts in adjustments_for_sid[column_name]:\n                # We need to cumulatively re-apply all adjustments up to the\n                # split-adjusted-asof-date. We might not have any\n                # pre-adjustments, so we should check for that.\n                if overwrite_ts <= split_adjusted_asof_idx \\\n                        and pre_adjustments_dict:\n                    for split_ts in pre_adjustments_dict[column_name]:\n                        # The split has to have occurred during the span of\n                        # the overwrite.\n                        if split_ts < overwrite_ts:\n                            # Create new adjustments here so that we can\n                            # re-apply all applicable adjustments to ONLY\n                            # the dates being overwritten.\n                            adjustments_for_sid[\n                                column_name\n                            ][overwrite_ts].extend([\n                                Float64Multiply(\n                                    0,\n                                    overwrite_ts - 1,\n                                    sid_idx,\n                                    sid_idx,\n                                    adjustment.value\n                                )\n                                for adjustment\n                                in pre_adjustments_dict[\n                                    column_name\n                                ][split_ts]\n                            ])\n                # After the split-adjusted-asof-date, we need to re-apply all\n                # adjustments that occur after that date and within the\n                # bounds of the overwrite. They need to be applied starting\n                # from the first date and until an end date. The end date is\n                # the date of the newest information we get about\n                # `requested_quarter` that is >= `split_ts`, or if there is no\n                # new knowledge before `overwrite_ts`, then it is the date\n                # before `overwrite_ts`.\n                else:\n                    # Overwrites happen at the first index of a new quarter,\n                    # so determine here which quarter that is.\n                    requested_quarter = requested_qtr_data[\n                        SHIFTED_NORMALIZED_QTRS, sid\n                    ].iloc[overwrite_ts]\n\n                    for adjustment_value, date_index, timestamp in zip(\n                            *post_adjustments\n                    ):\n                        if split_adjusted_asof_idx < date_index < overwrite_ts:\n                            # Assume the entire overwrite contains stale data\n                            upper_bound = overwrite_ts - 1\n                            end_idx = self.determine_end_idx_for_adjustment(\n                                timestamp,\n                                dates,\n                                upper_bound,\n                                requested_quarter,\n                                sid_estimates\n                            )\n                            adjustments_for_sid[\n                                column_name\n                            ][overwrite_ts].append(\n                                Float64Multiply(\n                                    0,\n                                    end_idx,\n                                    sid_idx,\n                                    sid_idx,\n                                    adjustment_value\n                                )\n                            )\n\n        self.merge_split_adjustments_with_overwrites(\n            pre_adjustments_dict,\n            post_adjustments_dict,\n            adjustments_for_sid,\n            requested_split_adjusted_columns\n        )", "response": "Collect split adjustments for future quarters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_halflife(cls, inputs, window_length, halflife, **kwargs):\n        if halflife <= 0:\n            raise ValueError(\n                \"`span` must be a positive number. %s was passed.\" % halflife\n            )\n        decay_rate = exp(log(.5) / halflife)\n        assert 0.0 < decay_rate <= 1.0\n\n        return cls(\n            inputs=inputs,\n            window_length=window_length,\n            decay_rate=decay_rate,\n            **kwargs\n        )", "response": "This constructor creates a new instance of the class from the given input and window length and halflife."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if two numbers are equal with some tolerance.", "response": "def tolerant_equals(a, b, atol=10e-7, rtol=10e-7, equal_nan=False):\n    \"\"\"Check if a and b are equal with some tolerance.\n\n    Parameters\n    ----------\n    a, b : float\n        The floats to check for equality.\n    atol : float, optional\n        The absolute tolerance.\n    rtol : float, optional\n        The relative tolerance.\n    equal_nan : bool, optional\n        Should NaN compare equal?\n\n    See Also\n    --------\n    numpy.isclose\n\n    Notes\n    -----\n    This function is just a scalar version of numpy.isclose for performance.\n    See the docstring of ``isclose`` for more information about ``atol`` and\n    ``rtol``.\n    \"\"\"\n    if equal_nan and isnan(a) and isnan(b):\n        return True\n    return math.fabs(a - b) <= (atol + rtol * math.fabs(b))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrounding a to the nearest integer if that integer is within epsilon.", "response": "def round_if_near_integer(a, epsilon=1e-4):\n    \"\"\"\n    Round a to the nearest integer if that integer is within an epsilon\n    of a.\n    \"\"\"\n    if abs(a - round(a)) <= epsilon:\n        return round(a)\n    else:\n        return a"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the expected return dtype for the given binary operator.", "response": "def binop_return_dtype(op, left, right):\n    \"\"\"\n    Compute the expected return dtype for the given binary operator.\n\n    Parameters\n    ----------\n    op : str\n        Operator symbol, (e.g. '+', '-', ...).\n    left : numpy.dtype\n        Dtype of left hand side.\n    right : numpy.dtype\n        Dtype of right hand side.\n\n    Returns\n    -------\n    outdtype : numpy.dtype\n        The dtype of the result of `left <op> right`.\n    \"\"\"\n    if is_comparison(op):\n        if left != right:\n            raise TypeError(\n                \"Don't know how to compute {left} {op} {right}.\\n\"\n                \"Comparisons are only supported between Factors of equal \"\n                \"dtypes.\".format(left=left, op=op, right=right)\n            )\n        return bool_dtype\n\n    elif left != float64_dtype or right != float64_dtype:\n        raise TypeError(\n            \"Don't know how to compute {left} {op} {right}.\\n\"\n            \"Arithmetic operators are only supported between Factors of \"\n            \"dtype 'float64'.\".format(\n                left=left.name,\n                op=op,\n                right=right.name,\n            )\n        )\n    return float64_dtype"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef binary_operator(op):\n    # When combining a Factor with a NumericalExpression, we use this\n    # attrgetter instance to defer to the commuted implementation of the\n    # NumericalExpression operator.\n    commuted_method_getter = attrgetter(method_name_for_op(op, commute=True))\n\n    @with_doc(\"Binary Operator: '%s'\" % op)\n    @with_name(method_name_for_op(op))\n    @coerce_numbers_to_my_dtype\n    def binary_operator(self, other):\n        # This can't be hoisted up a scope because the types returned by\n        # binop_return_type aren't defined when the top-level function is\n        # invoked in the class body of Factor.\n        return_type = binop_return_type(op)\n        if isinstance(self, NumExprFactor):\n            self_expr, other_expr, new_inputs = self.build_binary_op(\n                op, other,\n            )\n            return return_type(\n                \"({left}) {op} ({right})\".format(\n                    left=self_expr,\n                    op=op,\n                    right=other_expr,\n                ),\n                new_inputs,\n                dtype=binop_return_dtype(op, self.dtype, other.dtype),\n            )\n        elif isinstance(other, NumExprFactor):\n            # NumericalExpression overrides ops to correctly handle merging of\n            # inputs.  Look up and call the appropriate reflected operator with\n            # ourself as the input.\n            return commuted_method_getter(other)(self)\n        elif isinstance(other, Term):\n            if self is other:\n                return return_type(\n                    \"x_0 {op} x_0\".format(op=op),\n                    (self,),\n                    dtype=binop_return_dtype(op, self.dtype, other.dtype),\n                )\n            return return_type(\n                \"x_0 {op} x_1\".format(op=op),\n                (self, other),\n                dtype=binop_return_dtype(op, self.dtype, other.dtype),\n            )\n        elif isinstance(other, Number):\n            return return_type(\n                \"x_0 {op} ({constant})\".format(op=op, constant=other),\n                binds=(self,),\n                # .dtype access is safe here because coerce_numbers_to_my_dtype\n                # will convert any input numbers to numpy equivalents.\n                dtype=binop_return_dtype(op, self.dtype, other.dtype)\n            )\n        raise BadBinaryOperator(op, self, other)\n\n    return binary_operator", "response": "Returns a function that can be used to make a binary operator on a Factor subclass."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a factory function for making binary operator methods on a Factor.", "response": "def reflected_binary_operator(op):\n    \"\"\"\n    Factory function for making binary operator methods on a Factor.\n\n    Returns a function, \"reflected_binary_operator\" suitable for implementing\n    functions like __radd__.\n    \"\"\"\n    assert not is_comparison(op)\n\n    @with_name(method_name_for_op(op, commute=True))\n    @coerce_numbers_to_my_dtype\n    def reflected_binary_operator(self, other):\n\n        if isinstance(self, NumericalExpression):\n            self_expr, other_expr, new_inputs = self.build_binary_op(\n                op, other\n            )\n            return NumExprFactor(\n                \"({left}) {op} ({right})\".format(\n                    left=other_expr,\n                    right=self_expr,\n                    op=op,\n                ),\n                new_inputs,\n                dtype=binop_return_dtype(op, other.dtype, self.dtype)\n            )\n\n        # Only have to handle the numeric case because in all other valid cases\n        # the corresponding left-binding method will be called.\n        elif isinstance(other, Number):\n            return NumExprFactor(\n                \"{constant} {op} x_0\".format(op=op, constant=other),\n                binds=(self,),\n                dtype=binop_return_dtype(op, other.dtype, self.dtype),\n            )\n        raise BadBinaryOperator(op, other, self)\n    return reflected_binary_operator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a factory function for making unary operator methods for Factors.", "response": "def unary_operator(op):\n    \"\"\"\n    Factory function for making unary operator methods for Factors.\n    \"\"\"\n    # Only negate is currently supported.\n    valid_ops = {'-'}\n    if op not in valid_ops:\n        raise ValueError(\"Invalid unary operator %s.\" % op)\n\n    @with_doc(\"Unary Operator: '%s'\" % op)\n    @with_name(unary_op_name(op))\n    def unary_operator(self):\n        if self.dtype != float64_dtype:\n            raise TypeError(\n                \"Can't apply unary operator {op!r} to instance of \"\n                \"{typename!r} with dtype {dtypename!r}.\\n\"\n                \"{op!r} is only supported for Factors of dtype \"\n                \"'float64'.\".format(\n                    op=op,\n                    typename=type(self).__name__,\n                    dtypename=self.dtype.name,\n                )\n            )\n\n        # This can't be hoisted up a scope because the types returned by\n        # unary_op_return_type aren't defined when the top-level function is\n        # invoked.\n        if isinstance(self, NumericalExpression):\n            return NumExprFactor(\n                \"{op}({expr})\".format(op=op, expr=self._expr),\n                self.inputs,\n                dtype=float64_dtype,\n            )\n        else:\n            return NumExprFactor(\n                \"{op}x_0\".format(op=op),\n                (self,),\n                dtype=float64_dtype,\n            )\n    return unary_operator"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a factory function for producing function application methods for Factor subclasses.", "response": "def function_application(func):\n    \"\"\"\n    Factory function for producing function application methods for Factor\n    subclasses.\n    \"\"\"\n    if func not in NUMEXPR_MATH_FUNCS:\n        raise ValueError(\"Unsupported mathematical function '%s'\" % func)\n\n    @with_doc(func)\n    @with_name(func)\n    def mathfunc(self):\n        if isinstance(self, NumericalExpression):\n            return NumExprFactor(\n                \"{func}({expr})\".format(func=func, expr=self._expr),\n                self.inputs,\n                dtype=float64_dtype,\n            )\n        else:\n            return NumExprFactor(\n                \"{func}(x_0)\".format(func=func),\n                (self,),\n                dtype=float64_dtype,\n            )\n    return mathfunc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef winsorize(row, min_percentile, max_percentile):\n    a = row.copy()\n    nan_count = isnan(row).sum()\n    nonnan_count = a.size - nan_count\n\n    # NOTE: argsort() sorts nans to the end of the array.\n    idx = a.argsort()\n\n    # Set values at indices below the min percentile to the value of the entry\n    # at the cutoff.\n    if min_percentile > 0:\n        lower_cutoff = int(min_percentile * nonnan_count)\n        a[idx[:lower_cutoff]] = a[idx[lower_cutoff]]\n\n    # Set values at indices above the max percentile to the value of the entry\n    # at the cutoff.\n    if max_percentile < 1:\n        upper_cutoff = int(ceil(nonnan_count * max_percentile))\n        # if max_percentile is close to 1, then upper_cutoff might not\n        # remove any values.\n        if upper_cutoff < nonnan_count:\n            start_of_nans = (-nan_count) if nan_count else None\n            a[idx[upper_cutoff:start_of_nans]] = a[idx[upper_cutoff - 1]]\n\n    return a", "response": "This implementation is based on scipy. stats. winsorize\n    This implementation is based on scipy. stats. winsorize\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Factor that computes self and subtracts the mean from the result.", "response": "def demean(self, mask=NotSpecified, groupby=NotSpecified):\n        \"\"\"\n        Construct a Factor that computes ``self`` and subtracts the mean from\n        row of the result.\n\n        If ``mask`` is supplied, ignore values where ``mask`` returns False\n        when computing row means, and output NaN anywhere the mask is False.\n\n        If ``groupby`` is supplied, compute by partitioning each row based on\n        the values produced by ``groupby``, de-meaning the partitioned arrays,\n        and stitching the sub-results back together.\n\n        Parameters\n        ----------\n        mask : zipline.pipeline.Filter, optional\n            A Filter defining values to ignore when computing means.\n        groupby : zipline.pipeline.Classifier, optional\n            A classifier defining partitions over which to compute means.\n\n        Examples\n        --------\n        Let ``f`` be a Factor which would produce the following output::\n\n                         AAPL   MSFT    MCD     BK\n            2017-03-13    1.0    2.0    3.0    4.0\n            2017-03-14    1.5    2.5    3.5    1.0\n            2017-03-15    2.0    3.0    4.0    1.5\n            2017-03-16    2.5    3.5    1.0    2.0\n\n        Let ``c`` be a Classifier producing the following output::\n\n                         AAPL   MSFT    MCD     BK\n            2017-03-13      1      1      2      2\n            2017-03-14      1      1      2      2\n            2017-03-15      1      1      2      2\n            2017-03-16      1      1      2      2\n\n        Let ``m`` be a Filter producing the following output::\n\n                         AAPL   MSFT    MCD     BK\n            2017-03-13  False   True   True   True\n            2017-03-14   True  False   True   True\n            2017-03-15   True   True  False   True\n            2017-03-16   True   True   True  False\n\n        Then ``f.demean()`` will subtract the mean from each row produced by\n        ``f``.\n\n        ::\n\n                         AAPL   MSFT    MCD     BK\n            2017-03-13 -1.500 -0.500  0.500  1.500\n            2017-03-14 -0.625  0.375  1.375 -1.125\n            2017-03-15 -0.625  0.375  1.375 -1.125\n            2017-03-16  0.250  1.250 -1.250 -0.250\n\n        ``f.demean(mask=m)`` will subtract the mean from each row, but means\n        will be calculated ignoring values on the diagonal, and NaNs will\n        written to the diagonal in the output. Diagonal values are ignored\n        because they are the locations where the mask ``m`` produced False.\n\n        ::\n\n                         AAPL   MSFT    MCD     BK\n            2017-03-13    NaN -1.000  0.000  1.000\n            2017-03-14 -0.500    NaN  1.500 -1.000\n            2017-03-15 -0.166  0.833    NaN -0.666\n            2017-03-16  0.166  1.166 -1.333    NaN\n\n        ``f.demean(groupby=c)`` will subtract the group-mean of AAPL/MSFT and\n        MCD/BK from their respective entries.  The AAPL/MSFT are grouped\n        together because both assets always produce 1 in the output of the\n        classifier ``c``.  Similarly, MCD/BK are grouped together because they\n        always produce 2.\n\n        ::\n\n                         AAPL   MSFT    MCD     BK\n            2017-03-13 -0.500  0.500 -0.500  0.500\n            2017-03-14 -0.500  0.500  1.250 -1.250\n            2017-03-15 -0.500  0.500  1.250 -1.250\n            2017-03-16 -0.500  0.500 -0.500  0.500\n\n        ``f.demean(mask=m, groupby=c)`` will also subtract the group-mean of\n        AAPL/MSFT and MCD/BK, but means will be calculated ignoring values on\n        the diagonal , and NaNs will be written to the diagonal in the output.\n\n        ::\n\n                         AAPL   MSFT    MCD     BK\n            2017-03-13    NaN  0.000 -0.500  0.500\n            2017-03-14  0.000    NaN  1.250 -1.250\n            2017-03-15 -0.500  0.500    NaN  0.000\n            2017-03-16 -0.500  0.500  0.000    NaN\n\n        Notes\n        -----\n        Mean is sensitive to the magnitudes of outliers. When working with\n        factor that can potentially produce large outliers, it is often useful\n        to use the ``mask`` parameter to discard values at the extremes of the\n        distribution::\n\n            >>> base = MyFactor(...)  # doctest: +SKIP\n            >>> normalized = base.demean(\n            ...     mask=base.percentile_between(1, 99),\n            ... )  # doctest: +SKIP\n\n        ``demean()`` is only supported on Factors of dtype float64.\n\n        See Also\n        --------\n        :meth:`pandas.DataFrame.groupby`\n        \"\"\"\n        return GroupedRowTransform(\n            transform=demean,\n            transform_args=(),\n            factor=self,\n            groupby=groupby,\n            dtype=self.dtype,\n            missing_value=self.missing_value,\n            window_safe=self.window_safe,\n            mask=mask,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zscore(self, mask=NotSpecified, groupby=NotSpecified):\n        return GroupedRowTransform(\n            transform=zscore,\n            transform_args=(),\n            factor=self,\n            groupby=groupby,\n            dtype=self.dtype,\n            missing_value=self.missing_value,\n            mask=mask,\n            window_safe=True,\n        )", "response": "Constructs a Factor that Z - Scores each day s results."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new factor that computes the rank of each column in each row.", "response": "def rank(self,\n             method='ordinal',\n             ascending=True,\n             mask=NotSpecified,\n             groupby=NotSpecified):\n        \"\"\"\n        Construct a new Factor representing the sorted rank of each column\n        within each row.\n\n        Parameters\n        ----------\n        method : str, {'ordinal', 'min', 'max', 'dense', 'average'}\n            The method used to assign ranks to tied elements. See\n            `scipy.stats.rankdata` for a full description of the semantics for\n            each ranking method. Default is 'ordinal'.\n        ascending : bool, optional\n            Whether to return sorted rank in ascending or descending order.\n            Default is True.\n        mask : zipline.pipeline.Filter, optional\n            A Filter representing assets to consider when computing ranks.\n            If mask is supplied, ranks are computed ignoring any asset/date\n            pairs for which `mask` produces a value of False.\n        groupby : zipline.pipeline.Classifier, optional\n            A classifier defining partitions over which to perform ranking.\n\n        Returns\n        -------\n        ranks : zipline.pipeline.factors.Rank\n            A new factor that will compute the ranking of the data produced by\n            `self`.\n\n        Notes\n        -----\n        The default value for `method` is different from the default for\n        `scipy.stats.rankdata`.  See that function's documentation for a full\n        description of the valid inputs to `method`.\n\n        Missing or non-existent data on a given day will cause an asset to be\n        given a rank of NaN for that day.\n\n        See Also\n        --------\n        :func:`scipy.stats.rankdata`\n        :class:`zipline.pipeline.factors.factor.Rank`\n        \"\"\"\n\n        if groupby is NotSpecified:\n            return Rank(self, method=method, ascending=ascending, mask=mask)\n\n        return GroupedRowTransform(\n            transform=rankdata if ascending else rankdata_1d_descending,\n            transform_args=(method,),\n            factor=self,\n            groupby=groupby,\n            dtype=float64_dtype,\n            missing_value=nan,\n            mask=mask,\n            window_safe=True,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pearsonr(self, target, correlation_length, mask=NotSpecified):\n        from .statistical import RollingPearson\n        return RollingPearson(\n            base_factor=self,\n            target=target,\n            correlation_length=correlation_length,\n            mask=mask,\n        )", "response": "Constructs a Factor that computes the rolling pearson correlation between the target and the columns of self."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a Factor that computes the rolling spearman rank correlation between target and the columns of self.", "response": "def spearmanr(self, target, correlation_length, mask=NotSpecified):\n        \"\"\"\n        Construct a new Factor that computes rolling spearman rank correlation\n        coefficients between `target` and the columns of `self`.\n\n        This method can only be called on factors which are deemed safe for use\n        as inputs to other factors. This includes `Returns` and any factors\n        created from `Factor.rank` or `Factor.zscore`.\n\n        Parameters\n        ----------\n        target : zipline.pipeline.Term with a numeric dtype\n            The term used to compute correlations against each column of data\n            produced by `self`. This may be a Factor, a BoundColumn or a Slice.\n            If `target` is two-dimensional, correlations are computed\n            asset-wise.\n        correlation_length : int\n            Length of the lookback window over which to compute each\n            correlation coefficient.\n        mask : zipline.pipeline.Filter, optional\n            A Filter describing which assets should have their correlation with\n            the target slice computed each day.\n\n        Returns\n        -------\n        correlations : zipline.pipeline.factors.RollingSpearman\n            A new Factor that will compute correlations between `target` and\n            the columns of `self`.\n\n        Examples\n        --------\n        Suppose we want to create a factor that computes the correlation\n        between AAPL's 10-day returns and the 10-day returns of all other\n        assets, computing each correlation over 30 days. This can be achieved\n        by doing the following::\n\n            returns = Returns(window_length=10)\n            returns_slice = returns[sid(24)]\n            aapl_correlations = returns.spearmanr(\n                target=returns_slice, correlation_length=30,\n            )\n\n        This is equivalent to doing::\n\n            aapl_correlations = RollingSpearmanOfReturns(\n                target=sid(24), returns_length=10, correlation_length=30,\n            )\n\n        See Also\n        --------\n        :func:`scipy.stats.spearmanr`\n        :class:`zipline.pipeline.factors.RollingSpearmanOfReturns`\n        :meth:`Factor.pearsonr`\n        \"\"\"\n        from .statistical import RollingSpearman\n        return RollingSpearman(\n            base_factor=self,\n            target=target,\n            correlation_length=correlation_length,\n            mask=mask,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef linear_regression(self, target, regression_length, mask=NotSpecified):\n        from .statistical import RollingLinearRegression\n        return RollingLinearRegression(\n            dependent=self,\n            independent=target,\n            regression_length=regression_length,\n            mask=mask,\n        )", "response": "Construct a Factor that performs a linear regression predicting the columns of self from target."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef winsorize(self,\n                  min_percentile,\n                  max_percentile,\n                  mask=NotSpecified,\n                  groupby=NotSpecified):\n        \"\"\"\n        Construct a new factor that winsorizes the result of this factor.\n\n        Winsorizing changes values ranked less than the minimum percentile to\n        the value at the minimum percentile. Similarly, values ranking above\n        the maximum percentile are changed to the value at the maximum\n        percentile.\n\n        Winsorizing is useful for limiting the impact of extreme data points\n        without completely removing those points.\n\n        If ``mask`` is supplied, ignore values where ``mask`` returns False\n        when computing percentile cutoffs, and output NaN anywhere the mask is\n        False.\n\n        If ``groupby`` is supplied, winsorization is applied separately\n        separately to each group defined by ``groupby``.\n\n        Parameters\n        ----------\n        min_percentile: float, int\n            Entries with values at or below this percentile will be replaced\n            with the (len(input) * min_percentile)th lowest value. If low\n            values should not be clipped, use 0.\n        max_percentile: float, int\n            Entries with values at or above this percentile will be replaced\n            with the (len(input) * max_percentile)th lowest value. If high\n            values should not be clipped, use 1.\n        mask : zipline.pipeline.Filter, optional\n            A Filter defining values to ignore when winsorizing.\n        groupby : zipline.pipeline.Classifier, optional\n            A classifier defining partitions over which to winsorize.\n\n        Returns\n        -------\n        winsorized : zipline.pipeline.Factor\n            A Factor producing a winsorized version of self.\n\n        Examples\n        --------\n        .. code-block:: python\n\n            price = USEquityPricing.close.latest\n            columns={\n                'PRICE': price,\n                'WINSOR_1: price.winsorize(\n                    min_percentile=0.25, max_percentile=0.75\n                ),\n                'WINSOR_2': price.winsorize(\n                    min_percentile=0.50, max_percentile=1.0\n                ),\n                'WINSOR_3': price.winsorize(\n                    min_percentile=0.0, max_percentile=0.5\n                ),\n\n            }\n\n        Given a pipeline with columns, defined above, the result for a\n        given day could look like:\n\n        ::\n\n                    'PRICE' 'WINSOR_1' 'WINSOR_2' 'WINSOR_3'\n            Asset_1    1        2          4          3\n            Asset_2    2        2          4          3\n            Asset_3    3        3          4          3\n            Asset_4    4        4          4          4\n            Asset_5    5        5          5          4\n            Asset_6    6        5          5          4\n\n        See Also\n        --------\n        :func:`scipy.stats.mstats.winsorize`\n        :meth:`pandas.DataFrame.groupby`\n        \"\"\"\n        if not 0.0 <= min_percentile < max_percentile <= 1.0:\n            raise BadPercentileBounds(\n                min_percentile=min_percentile,\n                max_percentile=max_percentile,\n                upper_bound=1.0,\n            )\n        return GroupedRowTransform(\n            transform=winsorize,\n            transform_args=(min_percentile, max_percentile),\n            factor=self,\n            groupby=groupby,\n            dtype=self.dtype,\n            missing_value=self.missing_value,\n            mask=mask,\n            window_safe=self.window_safe,\n        )", "response": "Returns a new factor that winsorizes the result of this factor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing a Quantiles object which computes the quantiles of the output of this object.", "response": "def quantiles(self, bins, mask=NotSpecified):\n        \"\"\"\n        Construct a Classifier computing quantiles of the output of ``self``.\n\n        Every non-NaN data point the output is labelled with an integer value\n        from 0 to (bins - 1).  NaNs are labelled with -1.\n\n        If ``mask`` is supplied, ignore data points in locations for which\n        ``mask`` produces False, and emit a label of -1 at those locations.\n\n        Parameters\n        ----------\n        bins : int\n            Number of bins labels to compute.\n        mask : zipline.pipeline.Filter, optional\n            Mask of values to ignore when computing quantiles.\n\n        Returns\n        -------\n        quantiles : zipline.pipeline.classifiers.Quantiles\n            A Classifier producing integer labels ranging from 0 to (bins - 1).\n        \"\"\"\n        if mask is NotSpecified:\n            mask = self.mask\n        return Quantiles(inputs=(self,), bins=bins, mask=mask)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing a Filter matching the top N asset values of self each day.", "response": "def top(self, N, mask=NotSpecified, groupby=NotSpecified):\n        \"\"\"\n        Construct a Filter matching the top N asset values of self each day.\n\n        If ``groupby`` is supplied, returns a Filter matching the top N asset\n        values for each group.\n\n        Parameters\n        ----------\n        N : int\n            Number of assets passing the returned filter each day.\n        mask : zipline.pipeline.Filter, optional\n            A Filter representing assets to consider when computing ranks.\n            If mask is supplied, top values are computed ignoring any\n            asset/date pairs for which `mask` produces a value of False.\n        groupby : zipline.pipeline.Classifier, optional\n            A classifier defining partitions over which to perform ranking.\n\n        Returns\n        -------\n        filter : zipline.pipeline.filters.Filter\n        \"\"\"\n        if N == 1:\n            # Special case: if N == 1, we can avoid doing a full sort on every\n            # group, which is a big win.\n            return self._maximum(mask=mask, groupby=groupby)\n        return self.rank(ascending=False, mask=mask, groupby=groupby) <= N"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bottom(self, N, mask=NotSpecified, groupby=NotSpecified):\n        return self.rank(ascending=True, mask=mask, groupby=groupby) <= N", "response": "Construct a Filter matching the bottom N asset values of self each day."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new Filter representing entries that fall within the specified percentile range.", "response": "def percentile_between(self,\n                           min_percentile,\n                           max_percentile,\n                           mask=NotSpecified):\n        \"\"\"\n        Construct a new Filter representing entries from the output of this\n        Factor that fall within the percentile range defined by min_percentile\n        and max_percentile.\n\n        Parameters\n        ----------\n        min_percentile : float [0.0, 100.0]\n            Return True for assets falling above this percentile in the data.\n        max_percentile : float [0.0, 100.0]\n            Return True for assets falling below this percentile in the data.\n        mask : zipline.pipeline.Filter, optional\n            A Filter representing assets to consider when percentile\n            calculating thresholds.  If mask is supplied, percentile cutoffs\n            are computed each day using only assets for which ``mask`` returns\n            True.  Assets for which ``mask`` produces False will produce False\n            in the output of this Factor as well.\n\n        Returns\n        -------\n        out : zipline.pipeline.filters.PercentileFilter\n            A new filter that will compute the specified percentile-range mask.\n\n        See Also\n        --------\n        zipline.pipeline.filters.filter.PercentileFilter\n        \"\"\"\n        return PercentileFilter(\n            self,\n            min_percentile=min_percentile,\n            max_percentile=max_percentile,\n            mask=mask,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _validate(self):\n        if self._method not in _RANK_METHODS:\n            raise UnknownRankMethod(\n                method=self._method,\n                choices=set(_RANK_METHODS),\n            )\n        return super(Rank, self)._validate()", "response": "Verify that the stored rank method is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute(self, arrays, dates, assets, mask):\n        return masked_rankdata_2d(\n            arrays[0],\n            mask,\n            self.inputs[0].missing_value,\n            self._method,\n            self._ascending,\n        )", "response": "Compute a like - shaped array of per - row ranks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _time_to_micros(time):\n    seconds = time.hour * 60 * 60 + time.minute * 60 + time.second\n    return 1000000 * seconds + time.microsecond", "response": "Convert a time into microseconds since midnight."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a mask of all of the datetimes in dts that are between start and end.", "response": "def mask_between_time(dts, start, end, include_start=True, include_end=True):\n    \"\"\"Return a mask of all of the datetimes in ``dts`` that are between\n    ``start`` and ``end``.\n    Parameters\n    ----------\n    dts : pd.DatetimeIndex\n        The index to mask.\n    start : time\n        Mask away times less than the start.\n    end : time\n        Mask away times greater than the end.\n    include_start : bool, optional\n        Inclusive on ``start``.\n    include_end : bool, optional\n        Inclusive on ``end``.\n    Returns\n    -------\n    mask : np.ndarray[bool]\n        A bool array masking ``dts``.\n    See Also\n    --------\n    :meth:`pandas.DatetimeIndex.indexer_between_time`\n    \"\"\"\n    # This function is adapted from\n    # `pandas.Datetime.Index.indexer_between_time` which was originally\n    # written by Wes McKinney, Chang She, and Grant Roch.\n    time_micros = dts._get_time_micros()\n    start_micros = _time_to_micros(start)\n    end_micros = _time_to_micros(end)\n\n    left_op, right_op, join_op = _opmap[\n        bool(include_start),\n        bool(include_end),\n        start_micros <= end_micros,\n    ]\n\n    return join_op(\n        left_op(start_micros, time_micros),\n        right_op(time_micros, end_micros),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the index of dt in dts.", "response": "def find_in_sorted_index(dts, dt):\n    \"\"\"\n    Find the index of ``dt`` in ``dts``.\n\n    This function should be used instead of `dts.get_loc(dt)` if the index is\n    large enough that we don't want to initialize a hash table in ``dts``. In\n    particular, this should always be used on minutely trading calendars.\n\n    Parameters\n    ----------\n    dts : pd.DatetimeIndex\n        Index in which to look up ``dt``. **Must be sorted**.\n    dt : pd.Timestamp\n        ``dt`` to be looked up.\n\n    Returns\n    -------\n    ix : int\n        Integer index such that dts[ix] == dt.\n\n    Raises\n    ------\n    KeyError\n        If dt is not in ``dts``.\n    \"\"\"\n    ix = dts.searchsorted(dt)\n    if ix == len(dts) or dts[ix] != dt:\n        raise LookupError(\"{dt} is not in {dts}\".format(dt=dt, dts=dts))\n    return ix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding values in dts closest but not equal to dt.", "response": "def nearest_unequal_elements(dts, dt):\n    \"\"\"\n    Find values in ``dts`` closest but not equal to ``dt``.\n\n    Returns a pair of (last_before, first_after).\n\n    When ``dt`` is less than any element in ``dts``, ``last_before`` is None.\n    When ``dt`` is greater any element in ``dts``, ``first_after`` is None.\n\n    ``dts`` must be unique and sorted in increasing order.\n\n    Parameters\n    ----------\n    dts : pd.DatetimeIndex\n        Dates in which to search.\n    dt : pd.Timestamp\n        Date for which to find bounds.\n    \"\"\"\n    if not dts.is_unique:\n        raise ValueError(\"dts must be unique\")\n\n    if not dts.is_monotonic_increasing:\n        raise ValueError(\"dts must be sorted in increasing order\")\n\n    if not len(dts):\n        return None, None\n\n    sortpos = dts.searchsorted(dt, side='left')\n    try:\n        sortval = dts[sortpos]\n    except IndexError:\n        # dt is greater than any value in the array.\n        return dts[-1], None\n\n    if dt < sortval:\n        lower_ix = sortpos - 1\n        upper_ix = sortpos\n    elif dt == sortval:\n        lower_ix = sortpos - 1\n        upper_ix = sortpos + 1\n    else:\n        lower_ix = sortpos\n        upper_ix = sortpos + 1\n\n    lower_value = dts[lower_ix] if lower_ix >= 0 else None\n    upper_value = dts[upper_ix] if upper_ix < len(dts) else None\n\n    return lower_value, upper_value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconcatenate a list of pandas DataFrames with categorical columns.", "response": "def categorical_df_concat(df_list, inplace=False):\n    \"\"\"\n    Prepare list of pandas DataFrames to be used as input to pd.concat.\n    Ensure any columns of type 'category' have the same categories across each\n    dataframe.\n\n    Parameters\n    ----------\n    df_list : list\n        List of dataframes with same columns.\n    inplace : bool\n        True if input list can be modified. Default is False.\n\n    Returns\n    -------\n    concatenated : df\n        Dataframe of concatenated list.\n    \"\"\"\n\n    if not inplace:\n        df_list = deepcopy(df_list)\n\n    # Assert each dataframe has the same columns/dtypes\n    df = df_list[0]\n    if not all([(df.dtypes.equals(df_i.dtypes)) for df_i in df_list[1:]]):\n        raise ValueError(\"Input DataFrames must have the same columns/dtypes.\")\n\n    categorical_columns = df.columns[df.dtypes == 'category']\n\n    for col in categorical_columns:\n        new_categories = sorted(\n            set().union(\n                *(frame[col].cat.categories for frame in df_list)\n            )\n        )\n\n        with ignore_pandas_nan_categorical_warning():\n            for df in df_list:\n                df[col].cat.set_categories(new_categories, inplace=True)\n\n    return pd.concat(df_list)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks that a list of Index objects are all equal.", "response": "def check_indexes_all_same(indexes, message=\"Indexes are not equal.\"):\n    \"\"\"Check that a list of Index objects are all equal.\n\n    Parameters\n    ----------\n    indexes : iterable[pd.Index]\n        Iterable of indexes to check.\n\n    Raises\n    ------\n    ValueError\n        If the indexes are not all the same.\n    \"\"\"\n    iterator = iter(indexes)\n    first = next(iterator)\n    for other in iterator:\n        same = (first == other)\n        if not same.all():\n            bad_loc = np.flatnonzero(~same)[0]\n            raise ValueError(\n                \"{}\\nFirst difference is at index {}: \"\n                \"{} != {}\".format(\n                    message, bad_loc, first[bad_loc], other[bad_loc]\n                ),\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the set of resource columns required to serve the event indexers.", "response": "def required_event_fields(next_value_columns, previous_value_columns):\n    \"\"\"\n    Compute the set of resource columns required to serve\n    ``next_value_columns`` and ``previous_value_columns``.\n    \"\"\"\n    # These metadata columns are used to align event indexers.\n    return {\n        TS_FIELD_NAME,\n        SID_FIELD_NAME,\n        EVENT_DATE_FIELD_NAME,\n    }.union(\n        # We also expect any of the field names that our loadable columns\n        # are mapped to.\n        viewvalues(next_value_columns),\n        viewvalues(previous_value_columns),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that the columns of events are valid.", "response": "def validate_column_specs(events, next_value_columns, previous_value_columns):\n    \"\"\"\n    Verify that the columns of ``events`` can be used by an EventsLoader to\n    serve the BoundColumns described by ``next_value_columns`` and\n    ``previous_value_columns``.\n    \"\"\"\n    required = required_event_fields(next_value_columns,\n                                     previous_value_columns)\n    received = set(events.columns)\n    missing = required - received\n    if missing:\n        raise ValueError(\n            \"EventsLoader missing required columns {missing}.\\n\"\n            \"Got Columns: {received}\\n\"\n            \"Expected Columns: {required}\".format(\n                missing=sorted(missing),\n                received=sorted(received),\n                required=sorted(required),\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting requested_columns into next and previous event columns.", "response": "def split_next_and_previous_event_columns(self, requested_columns):\n        \"\"\"\n        Split requested columns into columns that should load the next known\n        value and columns that should load the previous known value.\n\n        Parameters\n        ----------\n        requested_columns : iterable[BoundColumn]\n\n        Returns\n        -------\n        next_cols, previous_cols : iterable[BoundColumn], iterable[BoundColumn]\n            ``requested_columns``, partitioned into sub-sequences based on\n            whether the column should produce values from the next event or the\n            previous event\n        \"\"\"\n        def next_or_previous(c):\n            if c in self.next_value_columns:\n                return 'next'\n            elif c in self.previous_value_columns:\n                return 'previous'\n\n            raise ValueError(\n                \"{c} not found in next_value_columns \"\n                \"or previous_value_columns\".format(c=c)\n            )\n        groups = groupby(next_or_previous, requested_columns)\n        return groups.get('next', ()), groups.get('previous', ())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare_arrays(left, right):\n    \"Eq check with a short-circuit for identical objects.\"\n    return (\n        left is right\n        or ((left.shape == right.shape) and (left == right).all())\n    )", "response": "Eq check with a short - circuit for identical objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef as_int_array(self):\n        return self.view(\n            type=ndarray,\n            dtype=unsigned_int_dtype_with_size_in_bytes(self.itemsize),\n        )", "response": "Convert self into a regular ndarray of ints."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncoerces self into a pandas categorical.", "response": "def as_categorical(self):\n        \"\"\"\n        Coerce self into a pandas categorical.\n\n        This is only defined on 1D arrays, since that's all pandas supports.\n        \"\"\"\n        if len(self.shape) > 1:\n            raise ValueError(\"Can't convert a 2D array to a categorical.\")\n\n        with ignore_pandas_nan_categorical_warning():\n            return pd.Categorical.from_codes(\n                self.as_int_array(),\n                # We need to make a copy because pandas >= 0.17 fails if this\n                # buffer isn't writeable.\n                self.categories.copy(),\n                ordered=False,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_categorical_frame(self, index, columns, name=None):\n        if len(self.shape) != 2:\n            raise ValueError(\n                \"Can't convert a non-2D LabelArray into a DataFrame.\"\n            )\n\n        expected_shape = (len(index), len(columns))\n        if expected_shape != self.shape:\n            raise ValueError(\n                \"Can't construct a DataFrame with provided indices:\\n\\n\"\n                \"LabelArray shape is {actual}, but index and columns imply \"\n                \"that shape should be {expected}.\".format(\n                    actual=self.shape,\n                    expected=expected_shape,\n                )\n            )\n\n        return pd.Series(\n            index=pd.MultiIndex.from_product([index, columns]),\n            data=self.ravel().as_categorical(),\n            name=name,\n        ).unstack()", "response": "Coerce self into a pandas DataFrame of Categoricals."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the scalar value into the array.", "response": "def set_scalar(self, indexer, value):\n        \"\"\"\n        Set scalar value into the array.\n\n        Parameters\n        ----------\n        indexer : any\n            The indexer to set the value at.\n        value : str\n            The value to assign at the given locations.\n\n        Raises\n        ------\n        ValueError\n            Raised when ``value`` is not a value element of this this label\n            array.\n        \"\"\"\n        try:\n            value_code = self.reverse_categories[value]\n        except KeyError:\n            raise ValueError(\"%r is not in LabelArray categories.\" % value)\n\n        self.as_int_array()[indexer] = value_code"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _equality_check(op):\n        def method(self, other):\n\n            if isinstance(other, LabelArray):\n                self_mv = self.missing_value\n                other_mv = other.missing_value\n                if self_mv != other_mv:\n                    raise MissingValueMismatch(self_mv, other_mv)\n\n                self_categories = self.categories\n                other_categories = other.categories\n                if not compare_arrays(self_categories, other_categories):\n                    raise CategoryMismatch(self_categories, other_categories)\n\n                return (\n                    op(self.as_int_array(), other.as_int_array())\n                    & self.not_missing()\n                    & other.not_missing()\n                )\n\n            elif isinstance(other, ndarray):\n                # Compare to ndarrays as though we were an array of strings.\n                # This is fairly expensive, and should generally be avoided.\n                return op(self.as_string_array(), other) & self.not_missing()\n\n            elif isinstance(other, self.SUPPORTED_SCALAR_TYPES):\n                i = self._reverse_categories.get(other, -1)\n                return op(self.as_int_array(), i) & self.not_missing()\n\n            return op(super(LabelArray, self), other)\n        return method", "response": "A method that returns a comparison of two LabelArrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef empty_like(self, shape):\n        return type(self).from_codes_and_metadata(\n            codes=np.full(\n                shape,\n                self.reverse_categories[self.missing_value],\n                dtype=unsigned_int_dtype_with_size_in_bytes(self.itemsize),\n            ),\n            categories=self.categories,\n            reverse_categories=self.reverse_categories,\n            missing_value=self.missing_value,\n        )", "response": "Make an empty LabelArray with the same categories as self filled with self. missing_value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmap a function from str - > bool element - wise over self.", "response": "def map_predicate(self, f):\n        \"\"\"\n        Map a function from str -> bool element-wise over ``self``.\n\n        ``f`` will be applied exactly once to each non-missing unique value in\n        ``self``. Missing values will always return False.\n        \"\"\"\n        # Functions passed to this are of type str -> bool.  Don't ever call\n        # them on None, which is the only non-str value we ever store in\n        # categories.\n        if self.missing_value is None:\n            def f_to_use(x):\n                return False if x is None else f(x)\n        else:\n            f_to_use = f\n\n        # Call f on each unique value in our categories.\n        results = np.vectorize(f_to_use, otypes=[bool_dtype])(self.categories)\n\n        # missing_value should produce False no matter what\n        results[self.reverse_categories[self.missing_value]] = False\n\n        # unpack the results form each unique value into their corresponding\n        # locations in our indices.\n        return results[self.as_int_array()]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map(self, f):\n        # f() should only return None if None is our missing value.\n        if self.missing_value is None:\n            allowed_outtypes = self.SUPPORTED_SCALAR_TYPES\n        else:\n            allowed_outtypes = self.SUPPORTED_NON_NONE_SCALAR_TYPES\n\n        def f_to_use(x,\n                     missing_value=self.missing_value,\n                     otypes=allowed_outtypes):\n\n            # Don't call f on the missing value; those locations don't exist\n            # semantically. We return _sortable_sentinel rather than None\n            # because the np.unique call below sorts the categories array,\n            # which raises an error on Python 3 because None and str aren't\n            # comparable.\n            if x == missing_value:\n                return _sortable_sentinel\n\n            ret = f(x)\n\n            if not isinstance(ret, otypes):\n                raise TypeError(\n                    \"LabelArray.map expected function {f} to return a string\"\n                    \" or None, but got {type} instead.\\n\"\n                    \"Value was {value}.\".format(\n                        f=f.__name__,\n                        type=type(ret).__name__,\n                        value=ret,\n                    )\n                )\n\n            if ret == missing_value:\n                return _sortable_sentinel\n\n            return ret\n\n        new_categories_with_duplicates = (\n            np.vectorize(f_to_use, otypes=[object])(self.categories)\n        )\n\n        # If f() maps multiple inputs to the same output, then we can end up\n        # with the same code duplicated multiple times. Compress the categories\n        # by running them through np.unique, and then use the reverse lookup\n        # table to compress codes as well.\n        new_categories, bloated_inverse_index = np.unique(\n            new_categories_with_duplicates,\n            return_inverse=True\n        )\n\n        if new_categories[0] is _sortable_sentinel:\n            # f_to_use return _sortable_sentinel for locations that should be\n            # missing values in our output. Since np.unique returns the uniques\n            # in sorted order, and since _sortable_sentinel sorts before any\n            # string, we only need to check the first array entry.\n            new_categories[0] = self.missing_value\n\n        # `reverse_index` will always be a 64 bit integer even if we can hold a\n        # smaller array.\n        reverse_index = bloated_inverse_index.astype(\n            smallest_uint_that_can_hold(len(new_categories))\n        )\n        new_codes = np.take(reverse_index, self.as_int_array())\n\n        return self.from_codes_and_metadata(\n            new_codes,\n            new_categories,\n            dict(zip(new_categories, range(len(new_categories)))),\n            missing_value=self.missing_value,\n        )", "response": "Map a function from str - > str element - wise over self."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking to make sure the stop and limit prices are reasonable and raise a BadOrderParameters exception if not.", "response": "def check_stoplimit_prices(price, label):\n    \"\"\"\n    Check to make sure the stop/limit prices are reasonable and raise\n    a BadOrderParameters exception if not.\n    \"\"\"\n    try:\n        if not isfinite(price):\n            raise BadOrderParameters(\n                msg=\"Attempted to place an order with a {} price \"\n                    \"of {}.\".format(label, price)\n            )\n    # This catches arbitrary objects\n    except TypeError:\n        raise BadOrderParameters(\n            msg=\"Attempted to place an order with a {} price \"\n                \"of {}.\".format(label, type(price))\n        )\n\n    if price < 0:\n        raise BadOrderParameters(\n            msg=\"Can't place a {} order with a negative price.\".format(label)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a zipline data bundle from a directory with csv files.", "response": "def csvdir_bundle(environ,\n                  asset_db_writer,\n                  minute_bar_writer,\n                  daily_bar_writer,\n                  adjustment_writer,\n                  calendar,\n                  start_session,\n                  end_session,\n                  cache,\n                  show_progress,\n                  output_dir,\n                  tframes=None,\n                  csvdir=None):\n    \"\"\"\n    Build a zipline data bundle from the directory with csv files.\n    \"\"\"\n    if not csvdir:\n        csvdir = environ.get('CSVDIR')\n        if not csvdir:\n            raise ValueError(\"CSVDIR environment variable is not set\")\n\n    if not os.path.isdir(csvdir):\n        raise ValueError(\"%s is not a directory\" % csvdir)\n\n    if not tframes:\n        tframes = set([\"daily\", \"minute\"]).intersection(os.listdir(csvdir))\n\n        if not tframes:\n            raise ValueError(\"'daily' and 'minute' directories \"\n                             \"not found in '%s'\" % csvdir)\n\n    divs_splits = {'divs': DataFrame(columns=['sid', 'amount',\n                                              'ex_date', 'record_date',\n                                              'declared_date', 'pay_date']),\n                   'splits': DataFrame(columns=['sid', 'ratio',\n                                                'effective_date'])}\n    for tframe in tframes:\n        ddir = os.path.join(csvdir, tframe)\n\n        symbols = sorted(item.split('.csv')[0]\n                         for item in os.listdir(ddir)\n                         if '.csv' in item)\n        if not symbols:\n            raise ValueError(\"no <symbol>.csv* files found in %s\" % ddir)\n\n        dtype = [('start_date', 'datetime64[ns]'),\n                 ('end_date', 'datetime64[ns]'),\n                 ('auto_close_date', 'datetime64[ns]'),\n                 ('symbol', 'object')]\n        metadata = DataFrame(empty(len(symbols), dtype=dtype))\n\n        if tframe == 'minute':\n            writer = minute_bar_writer\n        else:\n            writer = daily_bar_writer\n\n        writer.write(_pricing_iter(ddir, symbols, metadata,\n                     divs_splits, show_progress),\n                     show_progress=show_progress)\n\n        # Hardcode the exchange to \"CSVDIR\" for all assets and (elsewhere)\n        # register \"CSVDIR\" to resolve to the NYSE calendar, because these\n        # are all equities and thus can use the NYSE calendar.\n        metadata['exchange'] = \"CSVDIR\"\n\n        asset_db_writer.write(equities=metadata)\n\n        divs_splits['divs']['sid'] = divs_splits['divs']['sid'].astype(int)\n        divs_splits['splits']['sid'] = divs_splits['splits']['sid'].astype(int)\n        adjustment_writer.write(splits=divs_splits['splits'],\n                                dividends=divs_splits['divs'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restrict_to_dtype(dtype, message_template):\n    def processor(term_method, _, term_instance):\n        term_dtype = term_instance.dtype\n        if term_dtype != dtype:\n            raise TypeError(\n                message_template.format(\n                    method_name=term_method.__name__,\n                    expected_dtype=dtype.name,\n                    received_dtype=term_dtype,\n                )\n            )\n        return term_instance\n    return preprocess(self=processor)", "response": "Returns a new object that restricts terms to only be callable on terms of a specific dtype."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef daily_returns(self, start, end=None):\n        if end is None:\n            return self._daily_returns[start]\n\n        return self._daily_returns[start:end]", "response": "Returns the daily returns for the given period."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run(handle_data,\n         initialize,\n         before_trading_start,\n         analyze,\n         algofile,\n         algotext,\n         defines,\n         data_frequency,\n         capital_base,\n         bundle,\n         bundle_timestamp,\n         start,\n         end,\n         output,\n         trading_calendar,\n         print_algo,\n         metrics_set,\n         local_namespace,\n         environ,\n         blotter,\n         benchmark_returns):\n    \"\"\"Run a backtest for the given algorithm.\n\n    This is shared between the cli and :func:`zipline.run_algo`.\n    \"\"\"\n    if benchmark_returns is None:\n        benchmark_returns, _ = load_market_data(environ=environ)\n\n    if algotext is not None:\n        if local_namespace:\n            ip = get_ipython()  # noqa\n            namespace = ip.user_ns\n        else:\n            namespace = {}\n\n        for assign in defines:\n            try:\n                name, value = assign.split('=', 2)\n            except ValueError:\n                raise ValueError(\n                    'invalid define %r, should be of the form name=value' %\n                    assign,\n                )\n            try:\n                # evaluate in the same namespace so names may refer to\n                # eachother\n                namespace[name] = eval(value, namespace)\n            except Exception as e:\n                raise ValueError(\n                    'failed to execute definition for name %r: %s' % (name, e),\n                )\n    elif defines:\n        raise _RunAlgoError(\n            'cannot pass define without `algotext`',\n            \"cannot pass '-D' / '--define' without '-t' / '--algotext'\",\n        )\n    else:\n        namespace = {}\n        if algofile is not None:\n            algotext = algofile.read()\n\n    if print_algo:\n        if PYGMENTS:\n            highlight(\n                algotext,\n                PythonLexer(),\n                TerminalFormatter(),\n                outfile=sys.stdout,\n            )\n        else:\n            click.echo(algotext)\n\n    if trading_calendar is None:\n        trading_calendar = get_calendar('XNYS')\n\n    # date parameter validation\n    if trading_calendar.session_distance(start, end) < 1:\n        raise _RunAlgoError(\n            'There are no trading days between %s and %s' % (\n                start.date(),\n                end.date(),\n            ),\n        )\n\n    bundle_data = bundles.load(\n        bundle,\n        environ,\n        bundle_timestamp,\n    )\n\n    first_trading_day = \\\n        bundle_data.equity_minute_bar_reader.first_trading_day\n\n    data = DataPortal(\n        bundle_data.asset_finder,\n        trading_calendar=trading_calendar,\n        first_trading_day=first_trading_day,\n        equity_minute_reader=bundle_data.equity_minute_bar_reader,\n        equity_daily_reader=bundle_data.equity_daily_bar_reader,\n        adjustment_reader=bundle_data.adjustment_reader,\n    )\n\n    pipeline_loader = USEquityPricingLoader(\n        bundle_data.equity_daily_bar_reader,\n        bundle_data.adjustment_reader,\n    )\n\n    def choose_loader(column):\n        if column in USEquityPricing.columns:\n            return pipeline_loader\n        raise ValueError(\n            \"No PipelineLoader registered for column %s.\" % column\n        )\n\n    if isinstance(metrics_set, six.string_types):\n        try:\n            metrics_set = metrics.load(metrics_set)\n        except ValueError as e:\n            raise _RunAlgoError(str(e))\n\n    if isinstance(blotter, six.string_types):\n        try:\n            blotter = load(Blotter, blotter)\n        except ValueError as e:\n            raise _RunAlgoError(str(e))\n\n    perf = TradingAlgorithm(\n        namespace=namespace,\n        data_portal=data,\n        get_pipeline_loader=choose_loader,\n        trading_calendar=trading_calendar,\n        sim_params=SimulationParameters(\n            start_session=start,\n            end_session=end,\n            trading_calendar=trading_calendar,\n            capital_base=capital_base,\n            data_frequency=data_frequency,\n        ),\n        metrics_set=metrics_set,\n        blotter=blotter,\n        benchmark_returns=benchmark_returns,\n        **{\n            'initialize': initialize,\n            'handle_data': handle_data,\n            'before_trading_start': before_trading_start,\n            'analyze': analyze,\n        } if algotext is None else {\n            'algo_filename': getattr(algofile, 'name', '<algorithm>'),\n            'script': algotext,\n        }\n    ).run()\n\n    if output == '-':\n        click.echo(str(perf))\n    elif output != os.devnull:  # make the zipline magic not write any data\n        perf.to_pickle(output)\n\n    return perf", "response": "Run a single algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_extensions(default, extensions, strict, environ, reload=False):\n    if default:\n        default_extension_path = pth.default_extension(environ=environ)\n        pth.ensure_file(default_extension_path)\n        # put the default extension first so other extensions can depend on\n        # the order they are loaded\n        extensions = concatv([default_extension_path], extensions)\n\n    for ext in extensions:\n        if ext in _loaded_extensions and not reload:\n            continue\n        try:\n            # load all of the zipline extensionss\n            if ext.endswith('.py'):\n                with open(ext) as f:\n                    ns = {}\n                    six.exec_(compile(f.read(), ext, 'exec'), ns, ns)\n            else:\n                __import__(ext)\n        except Exception as e:\n            if strict:\n                # if `strict` we should raise the actual exception and fail\n                raise\n            # without `strict` we should just log the failure\n            warnings.warn(\n                'Failed to load extension: %r\\n%s' % (ext, e),\n                stacklevel=2\n            )\n        else:\n            _loaded_extensions.add(ext)", "response": "Load all of the given extensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a trading algorithm. Parameters ---------- start : datetime The start date of the backtest. end : datetime The end date of the backtest.. initialize : callable[context -> None] The initialize function to use for the algorithm. This is called once at the very begining of the backtest and should be used to set up any state needed by the algorithm. capital_base : float The starting capital for the backtest. handle_data : callable[(context, BarData) -> None], optional The handle_data function to use for the algorithm. This is called every minute when ``data_frequency == 'minute'`` or every day when ``data_frequency == 'daily'``. before_trading_start : callable[(context, BarData) -> None], optional The before_trading_start function for the algorithm. This is called once before each trading day (after initialize on the first day). analyze : callable[(context, pd.DataFrame) -> None], optional The analyze function to use for the algorithm. This function is called once at the end of the backtest and is passed the context and the performance data. data_frequency : {'daily', 'minute'}, optional The data frequency to run the algorithm at. bundle : str, optional The name of the data bundle to use to load the data to run the backtest with. This defaults to 'quantopian-quandl'. bundle_timestamp : datetime, optional The datetime to lookup the bundle data for. This defaults to the current time. trading_calendar : TradingCalendar, optional The trading calendar to use for your backtest. metrics_set : iterable[Metric] or str, optional The set of metrics to compute in the simulation. If a string is passed, resolve the set with :func:`zipline.finance.metrics.load`. default_extension : bool, optional Should the default zipline extension be loaded. This is found at ``$ZIPLINE_ROOT/extension.py`` extensions : iterable[str], optional The names of any other extensions to load. Each element may either be a dotted module path like ``a.b.c`` or a path to a python file ending in ``.py`` like ``a/b/c.py``. strict_extensions : bool, optional Should the run fail if any extensions fail to load. If this is false, a warning will be raised instead. environ : mapping[str -> str], optional The os environment to use. Many extensions use this to get parameters. This defaults to ``os.environ``. blotter : str or zipline.finance.blotter.Blotter, optional Blotter to use with this algorithm. If passed as a string, we look for a blotter construction function registered with ``zipline.extensions.register`` and call it with no parameters. Default is a :class:`zipline.finance.blotter.SimulationBlotter` that never cancels orders. Returns ------- perf : pd.DataFrame The daily performance of the algorithm. See Also -------- zipline.data.bundles.bundles : The available data bundles.", "response": "def run_algorithm(start,\n                  end,\n                  initialize,\n                  capital_base,\n                  handle_data=None,\n                  before_trading_start=None,\n                  analyze=None,\n                  data_frequency='daily',\n                  bundle='quantopian-quandl',\n                  bundle_timestamp=None,\n                  trading_calendar=None,\n                  metrics_set='default',\n                  benchmark_returns=None,\n                  default_extension=True,\n                  extensions=(),\n                  strict_extensions=True,\n                  environ=os.environ,\n                  blotter='default'):\n    \"\"\"\n    Run a trading algorithm.\n\n    Parameters\n    ----------\n    start : datetime\n        The start date of the backtest.\n    end : datetime\n        The end date of the backtest..\n    initialize : callable[context -> None]\n        The initialize function to use for the algorithm. This is called once\n        at the very begining of the backtest and should be used to set up\n        any state needed by the algorithm.\n    capital_base : float\n        The starting capital for the backtest.\n    handle_data : callable[(context, BarData) -> None], optional\n        The handle_data function to use for the algorithm. This is called\n        every minute when ``data_frequency == 'minute'`` or every day\n        when ``data_frequency == 'daily'``.\n    before_trading_start : callable[(context, BarData) -> None], optional\n        The before_trading_start function for the algorithm. This is called\n        once before each trading day (after initialize on the first day).\n    analyze : callable[(context, pd.DataFrame) -> None], optional\n        The analyze function to use for the algorithm. This function is called\n        once at the end of the backtest and is passed the context and the\n        performance data.\n    data_frequency : {'daily', 'minute'}, optional\n        The data frequency to run the algorithm at.\n    bundle : str, optional\n        The name of the data bundle to use to load the data to run the backtest\n        with. This defaults to 'quantopian-quandl'.\n    bundle_timestamp : datetime, optional\n        The datetime to lookup the bundle data for. This defaults to the\n        current time.\n    trading_calendar : TradingCalendar, optional\n        The trading calendar to use for your backtest.\n    metrics_set : iterable[Metric] or str, optional\n        The set of metrics to compute in the simulation. If a string is passed,\n        resolve the set with :func:`zipline.finance.metrics.load`.\n    default_extension : bool, optional\n        Should the default zipline extension be loaded. This is found at\n        ``$ZIPLINE_ROOT/extension.py``\n    extensions : iterable[str], optional\n        The names of any other extensions to load. Each element may either be\n        a dotted module path like ``a.b.c`` or a path to a python file ending\n        in ``.py`` like ``a/b/c.py``.\n    strict_extensions : bool, optional\n        Should the run fail if any extensions fail to load. If this is false,\n        a warning will be raised instead.\n    environ : mapping[str -> str], optional\n        The os environment to use. Many extensions use this to get parameters.\n        This defaults to ``os.environ``.\n    blotter : str or zipline.finance.blotter.Blotter, optional\n        Blotter to use with this algorithm. If passed as a string, we look for\n        a blotter construction function registered with\n        ``zipline.extensions.register`` and call it with no parameters.\n        Default is a :class:`zipline.finance.blotter.SimulationBlotter` that\n        never cancels orders.\n\n    Returns\n    -------\n    perf : pd.DataFrame\n        The daily performance of the algorithm.\n\n    See Also\n    --------\n    zipline.data.bundles.bundles : The available data bundles.\n    \"\"\"\n    load_extensions(default_extension, extensions, strict_extensions, environ)\n\n    return _run(\n        handle_data=handle_data,\n        initialize=initialize,\n        before_trading_start=before_trading_start,\n        analyze=analyze,\n        algofile=None,\n        algotext=None,\n        defines=(),\n        data_frequency=data_frequency,\n        capital_base=capital_base,\n        bundle=bundle,\n        bundle_timestamp=bundle_timestamp,\n        start=start,\n        end=end,\n        output=os.devnull,\n        trading_calendar=trading_calendar,\n        print_algo=False,\n        metrics_set=metrics_set,\n        local_namespace=False,\n        environ=environ,\n        blotter=blotter,\n        benchmark_returns=benchmark_returns,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_extra_source(self, source_df, sim_params):\n        if source_df is None:\n            return\n\n        # Normalize all the dates in the df\n        source_df.index = source_df.index.normalize()\n\n        # source_df's sid column can either consist of assets we know about\n        # (such as sid(24)) or of assets we don't know about (such as\n        # palladium).\n        #\n        # In both cases, we break up the dataframe into individual dfs\n        # that only contain a single asset's information.  ie, if source_df\n        # has data for PALLADIUM and GOLD, we split source_df into two\n        # dataframes, one for each. (same applies if source_df has data for\n        # AAPL and IBM).\n        #\n        # We then take each child df and reindex it to the simulation's date\n        # range by forward-filling missing values. this makes reads simpler.\n        #\n        # Finally, we store the data. For each column, we store a mapping in\n        # self.augmented_sources_map from the column to a dictionary of\n        # asset -> df.  In other words,\n        # self.augmented_sources_map['days_to_cover']['AAPL'] gives us the df\n        # holding that data.\n        source_date_index = self.trading_calendar.sessions_in_range(\n            sim_params.start_session,\n            sim_params.end_session\n        )\n\n        # Break the source_df up into one dataframe per sid.  This lets\n        # us (more easily) calculate accurate start/end dates for each sid,\n        # de-dup data, and expand the data to fit the backtest start/end date.\n        grouped_by_sid = source_df.groupby([\"sid\"])\n        group_names = grouped_by_sid.groups.keys()\n        group_dict = {}\n        for group_name in group_names:\n            group_dict[group_name] = grouped_by_sid.get_group(group_name)\n\n        # This will be the dataframe which we query to get fetcher assets at\n        # any given time. Get's overwritten every time there's a new fetcher\n        # call\n        extra_source_df = pd.DataFrame()\n\n        for identifier, df in iteritems(group_dict):\n            # Since we know this df only contains a single sid, we can safely\n            # de-dupe by the index (dt). If minute granularity, will take the\n            # last data point on any given day\n            df = df.groupby(level=0).last()\n\n            # Reindex the dataframe based on the backtest start/end date.\n            # This makes reads easier during the backtest.\n            df = self._reindex_extra_source(df, source_date_index)\n\n            for col_name in df.columns.difference(['sid']):\n                if col_name not in self._augmented_sources_map:\n                    self._augmented_sources_map[col_name] = {}\n\n                self._augmented_sources_map[col_name][identifier] = df\n\n            # Append to extra_source_df the reindexed dataframe for the single\n            # sid\n            extra_source_df = extra_source_df.append(df)\n\n        self._extra_source_df = extra_source_df", "response": "This function handles extra sources that are not in the same simulation as the original data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_last_traded_dt(self, asset, dt, data_frequency):\n        return self._get_pricing_reader(data_frequency).get_last_traded_dt(\n            asset, dt)", "response": "Returns the last traded dt for the given asset and dt."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _is_extra_source(asset, field, map):\n        # If we have an extra source with a column called \"price\", only look\n        # at it if it's on something like palladium and not AAPL (since our\n        # own price data always wins when dealing with assets).\n\n        return not (field in BASE_FIELDS and\n                    (isinstance(asset, (Asset, ContinuousFuture))))", "response": "Internal method that determines if this asset or field combination is an extra source."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_spot_value(self, assets, field, dt, data_frequency):\n        assets_is_scalar = False\n        if isinstance(assets, (AssetConvertible, PricingDataAssociable)):\n            assets_is_scalar = True\n        else:\n            # If 'assets' was not one of the expected types then it should be\n            # an iterable.\n            try:\n                iter(assets)\n            except TypeError:\n                raise TypeError(\n                    \"Unexpected 'assets' value of type {}.\"\n                    .format(type(assets))\n                )\n\n        session_label = self.trading_calendar.minute_to_session_label(dt)\n\n        if assets_is_scalar:\n            return self._get_single_asset_value(\n                session_label,\n                assets,\n                field,\n                dt,\n                data_frequency,\n            )\n        else:\n            get_single_asset_value = self._get_single_asset_value\n            return [\n                get_single_asset_value(\n                    session_label,\n                    asset,\n                    field,\n                    dt,\n                    data_frequency,\n                )\n                for asset in assets\n            ]", "response": "Private method that returns a scalar value representing the value of the asset s field at either the given dt or the last traded value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_scalar_asset_spot_value(self, asset, field, dt, data_frequency):\n        return self._get_single_asset_value(\n            self.trading_calendar.minute_to_session_label(dt),\n            asset,\n            field,\n            dt,\n            data_frequency,\n        )", "response": "Returns a scalar value representing the value of the asset s spot value at the given dt."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_adjustments(self, assets, field, dt, perspective_dt):\n        if isinstance(assets, Asset):\n            assets = [assets]\n\n        adjustment_ratios_per_asset = []\n\n        def split_adj_factor(x):\n            return x if field != 'volume' else 1.0 / x\n\n        for asset in assets:\n            adjustments_for_asset = []\n            split_adjustments = self._get_adjustment_list(\n                asset, self._splits_dict, \"SPLITS\"\n            )\n            for adj_dt, adj in split_adjustments:\n                if dt < adj_dt <= perspective_dt:\n                    adjustments_for_asset.append(split_adj_factor(adj))\n                elif adj_dt > perspective_dt:\n                    break\n\n            if field != 'volume':\n                merger_adjustments = self._get_adjustment_list(\n                    asset, self._mergers_dict, \"MERGERS\"\n                )\n                for adj_dt, adj in merger_adjustments:\n                    if dt < adj_dt <= perspective_dt:\n                        adjustments_for_asset.append(adj)\n                    elif adj_dt > perspective_dt:\n                        break\n\n                dividend_adjustments = self._get_adjustment_list(\n                    asset, self._dividends_dict, \"DIVIDENDS\",\n                )\n                for adj_dt, adj in dividend_adjustments:\n                    if dt < adj_dt <= perspective_dt:\n                        adjustments_for_asset.append(adj)\n                    elif adj_dt > perspective_dt:\n                        break\n\n            ratio = reduce(mul, adjustments_for_asset, 1.0)\n            adjustment_ratios_per_asset.append(ratio)\n\n        return adjustment_ratios_per_asset", "response": "Returns a list of adjustments between the dt and perspective_dt for the given field and list of assets."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the value of the asset s field at the given dt with adjustments applied.", "response": "def get_adjusted_value(self, asset, field, dt,\n                           perspective_dt,\n                           data_frequency,\n                           spot_value=None):\n        \"\"\"\n        Returns a scalar value representing the value\n        of the desired asset's field at the given dt with adjustments applied.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset whose data is desired.\n        field : {'open', 'high', 'low', 'close', 'volume', \\\n                 'price', 'last_traded'}\n            The desired field of the asset.\n        dt : pd.Timestamp\n            The timestamp for the desired value.\n        perspective_dt : pd.Timestamp\n            The timestamp from which the data is being viewed back from.\n        data_frequency : str\n            The frequency of the data to query; i.e. whether the data is\n            'daily' or 'minute' bars\n\n        Returns\n        -------\n        value : float, int, or pd.Timestamp\n            The value of the given ``field`` for ``asset`` at ``dt`` with any\n            adjustments known by ``perspective_dt`` applied. The return type is\n            based on the ``field`` requested. If the field is one of 'open',\n            'high', 'low', 'close', or 'price', the value will be a float. If\n            the ``field`` is 'volume' the value will be a int. If the ``field``\n            is 'last_traded' the value will be a Timestamp.\n        \"\"\"\n        if spot_value is None:\n            # if this a fetcher field, we want to use perspective_dt (not dt)\n            # because we want the new value as of midnight (fetcher only works\n            # on a daily basis, all timestamps are on midnight)\n            if self._is_extra_source(asset, field,\n                                     self._augmented_sources_map):\n                spot_value = self.get_spot_value(asset, field, perspective_dt,\n                                                 data_frequency)\n            else:\n                spot_value = self.get_spot_value(asset, field, dt,\n                                                 data_frequency)\n\n        if isinstance(asset, Equity):\n            ratio = self.get_adjustments(asset, field, dt, perspective_dt)[0]\n            spot_value *= ratio\n\n        return spot_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_history_minute_window(self, assets, end_dt, bar_count,\n                                   field_to_use):\n        \"\"\"\n        Internal method that returns a dataframe containing history bars\n        of minute frequency for the given sids.\n        \"\"\"\n        # get all the minutes for this window\n        try:\n            minutes_for_window = self.trading_calendar.minutes_window(\n                end_dt, -bar_count\n            )\n        except KeyError:\n            self._handle_minute_history_out_of_bounds(bar_count)\n\n        if minutes_for_window[0] < self._first_trading_minute:\n            self._handle_minute_history_out_of_bounds(bar_count)\n\n        asset_minute_data = self._get_minute_window_data(\n            assets,\n            field_to_use,\n            minutes_for_window,\n        )\n\n        return pd.DataFrame(\n            asset_minute_data,\n            index=minutes_for_window,\n            columns=assets\n        )", "response": "Internal method that returns a dataframe containing history bars for the given sids and the given field_to_use."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_history_window(self,\n                           assets,\n                           end_dt,\n                           bar_count,\n                           frequency,\n                           field,\n                           data_frequency,\n                           ffill=True):\n        \"\"\"\n        Public API method that returns a dataframe containing the requested\n        history window.  Data is fully adjusted.\n\n        Parameters\n        ----------\n        assets : list of zipline.data.Asset objects\n            The assets whose data is desired.\n\n        bar_count: int\n            The number of bars desired.\n\n        frequency: string\n            \"1d\" or \"1m\"\n\n        field: string\n            The desired field of the asset.\n\n        data_frequency: string\n            The frequency of the data to query; i.e. whether the data is\n            'daily' or 'minute' bars.\n\n        ffill: boolean\n            Forward-fill missing values. Only has effect if field\n            is 'price'.\n\n        Returns\n        -------\n        A dataframe containing the requested data.\n        \"\"\"\n        if field not in OHLCVP_FIELDS and field != 'sid':\n            raise ValueError(\"Invalid field: {0}\".format(field))\n\n        if bar_count < 1:\n            raise ValueError(\n                \"bar_count must be >= 1, but got {}\".format(bar_count)\n            )\n\n        if frequency == \"1d\":\n            if field == \"price\":\n                df = self._get_history_daily_window(assets, end_dt, bar_count,\n                                                    \"close\", data_frequency)\n            else:\n                df = self._get_history_daily_window(assets, end_dt, bar_count,\n                                                    field, data_frequency)\n        elif frequency == \"1m\":\n            if field == \"price\":\n                df = self._get_history_minute_window(assets, end_dt, bar_count,\n                                                     \"close\")\n            else:\n                df = self._get_history_minute_window(assets, end_dt, bar_count,\n                                                     field)\n        else:\n            raise ValueError(\"Invalid frequency: {0}\".format(frequency))\n\n        # forward-fill price\n        if field == \"price\":\n            if frequency == \"1m\":\n                ffill_data_frequency = 'minute'\n            elif frequency == \"1d\":\n                ffill_data_frequency = 'daily'\n            else:\n                raise Exception(\n                    \"Only 1d and 1m are supported for forward-filling.\")\n\n            assets_with_leading_nan = np.where(isnull(df.iloc[0]))[0]\n\n            history_start, history_end = df.index[[0, -1]]\n            if ffill_data_frequency == 'daily' and data_frequency == 'minute':\n                # When we're looking for a daily value, but we haven't seen any\n                # volume in today's minute bars yet, we need to use the\n                # previous day's ffilled daily price. Using today's daily price\n                # could yield a value from later today.\n                history_start -= self.trading_calendar.day\n\n            initial_values = []\n            for asset in df.columns[assets_with_leading_nan]:\n                last_traded = self.get_last_traded_dt(\n                    asset,\n                    history_start,\n                    ffill_data_frequency,\n                )\n                if isnull(last_traded):\n                    initial_values.append(nan)\n                else:\n                    initial_values.append(\n                        self.get_adjusted_value(\n                            asset,\n                            field,\n                            dt=last_traded,\n                            perspective_dt=history_end,\n                            data_frequency=ffill_data_frequency,\n                        )\n                    )\n\n            # Set leading values for assets that were missing data, then ffill.\n            df.ix[0, assets_with_leading_nan] = np.array(\n                initial_values,\n                dtype=np.float64\n            )\n            df.fillna(method='ffill', inplace=True)\n\n            # forward-filling will incorrectly produce values after the end of\n            # an asset's lifetime, so write NaNs back over the asset's\n            # end_date.\n            normed_index = df.index.normalize()\n            for asset in df.columns:\n                if history_end >= asset.end_date:\n                    # if the window extends past the asset's end date, set\n                    # all post-end-date values to NaN in that asset's series\n                    df.loc[normed_index > asset.end_date, asset] = nan\n        return df", "response": "Returns a dataframe containing the requested data for the specified assets and bar count and frequency."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_minute_window_data(self, assets, field, minutes_for_window):\n        return self._minute_history_loader.history(assets,\n                                                   minutes_for_window,\n                                                   field,\n                                                   False)", "response": "Internal method that returns a window of adjusted minute data for an asset and a specific field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_daily_window_data(self,\n                               assets,\n                               field,\n                               days_in_window,\n                               extra_slot=True):\n        \"\"\"\n        Internal method that gets a window of adjusted daily data for a sid\n        and specified date range.  Used to support the history API method for\n        daily bars.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset whose data is desired.\n\n        start_dt: pandas.Timestamp\n            The start of the desired window of data.\n\n        bar_count: int\n            The number of days of data to return.\n\n        field: string\n            The specific field to return.  \"open\", \"high\", \"close_price\", etc.\n\n        extra_slot: boolean\n            Whether to allocate an extra slot in the returned numpy array.\n            This extra slot will hold the data for the last partial day.  It's\n            much better to create it here than to create a copy of the array\n            later just to add a slot.\n\n        Returns\n        -------\n        A numpy array with requested values.  Any missing slots filled with\n        nan.\n\n        \"\"\"\n        bar_count = len(days_in_window)\n        # create an np.array of size bar_count\n        dtype = float64 if field != 'sid' else int64\n        if extra_slot:\n            return_array = np.zeros((bar_count + 1, len(assets)), dtype=dtype)\n        else:\n            return_array = np.zeros((bar_count, len(assets)), dtype=dtype)\n\n        if field != \"volume\":\n            # volumes default to 0, so we don't need to put NaNs in the array\n            return_array[:] = np.NAN\n\n        if bar_count != 0:\n            data = self._history_loader.history(assets,\n                                                days_in_window,\n                                                field,\n                                                extra_slot)\n            if extra_slot:\n                return_array[:len(return_array) - 1, :] = data\n            else:\n                return_array[:len(data)] = data\n        return return_array", "response": "Internal method that returns a daily window of data for a specific asset and a specific date range."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns any splits for the given sids and the given dt.", "response": "def get_splits(self, assets, dt):\n        \"\"\"\n        Returns any splits for the given sids and the given dt.\n\n        Parameters\n        ----------\n        assets : container\n            Assets for which we want splits.\n        dt : pd.Timestamp\n            The date for which we are checking for splits. Note: this is\n            expected to be midnight UTC.\n\n        Returns\n        -------\n        splits : list[(asset, float)]\n            List of splits, where each split is a (asset, ratio) tuple.\n        \"\"\"\n        if self._adjustment_reader is None or not assets:\n            return []\n\n        # convert dt to # of seconds since epoch, because that's what we use\n        # in the adjustments db\n        seconds = int(dt.value / 1e9)\n\n        splits = self._adjustment_reader.conn.execute(\n            \"SELECT sid, ratio FROM SPLITS WHERE effective_date = ?\",\n            (seconds,)).fetchall()\n\n        splits = [split for split in splits if split[0] in assets]\n        splits = [(self.asset_finder.retrieve_asset(split[0]), split[1])\n                  for split in splits]\n\n        return splits"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all stock dividends for a specific sid that occur in the given trading range.", "response": "def get_stock_dividends(self, sid, trading_days):\n        \"\"\"\n        Returns all the stock dividends for a specific sid that occur\n        in the given trading range.\n\n        Parameters\n        ----------\n        sid: int\n            The asset whose stock dividends should be returned.\n\n        trading_days: pd.DatetimeIndex\n            The trading range.\n\n        Returns\n        -------\n        list: A list of objects with all relevant attributes populated.\n        All timestamp fields are converted to pd.Timestamps.\n        \"\"\"\n\n        if self._adjustment_reader is None:\n            return []\n\n        if len(trading_days) == 0:\n            return []\n\n        start_dt = trading_days[0].value / 1e9\n        end_dt = trading_days[-1].value / 1e9\n\n        dividends = self._adjustment_reader.conn.execute(\n            \"SELECT * FROM stock_dividend_payouts WHERE sid = ? AND \"\n            \"ex_date > ? AND pay_date < ?\", (int(sid), start_dt, end_dt,)).\\\n            fetchall()\n\n        dividend_info = []\n        for dividend_tuple in dividends:\n            dividend_info.append({\n                \"declared_date\": dividend_tuple[1],\n                \"ex_date\": pd.Timestamp(dividend_tuple[2], unit=\"s\"),\n                \"pay_date\": pd.Timestamp(dividend_tuple[3], unit=\"s\"),\n                \"payment_sid\": dividend_tuple[4],\n                \"ratio\": dividend_tuple[5],\n                \"record_date\": pd.Timestamp(dividend_tuple[6], unit=\"s\"),\n                \"sid\": dividend_tuple[7]\n            })\n\n        return dividend_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_fetcher_assets(self, dt):\n        # return a list of assets for the current date, as defined by the\n        # fetcher source\n        if self._extra_source_df is None:\n            return []\n\n        day = normalize_date(dt)\n\n        if day in self._extra_source_df.index:\n            assets = self._extra_source_df.loc[day]['sid']\n        else:\n            return []\n\n        if isinstance(assets, pd.Series):\n            return [x for x in assets if isinstance(x, Asset)]\n        else:\n            return [assets] if isinstance(assets, Asset) else []", "response": "Returns a list of assets for the current date."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the future chain for the current contracts at the given dt according to the continuous future specification.", "response": "def get_current_future_chain(self, continuous_future, dt):\n        \"\"\"\n        Retrieves the future chain for the contract at the given `dt` according\n        the `continuous_future` specification.\n\n        Returns\n        -------\n\n        future_chain : list[Future]\n            A list of active futures, where the first index is the current\n            contract specified by the continuous future definition, the second\n            is the next upcoming contract and so on.\n        \"\"\"\n        rf = self._roll_finders[continuous_future.roll_style]\n        session = self.trading_calendar.minute_to_session_label(dt)\n        contract_center = rf.get_contract_center(\n            continuous_future.root_symbol, session,\n            continuous_future.offset)\n        oc = self.asset_finder.get_ordered_contracts(\n            continuous_future.root_symbol)\n        chain = oc.active_chain(contract_center, session.value)\n        return self.asset_finder.retrieve_all(chain)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_kind_check(python_types, numpy_kind):\n    def check(value):\n        if hasattr(value, 'dtype'):\n            return value.dtype.kind == numpy_kind\n        return isinstance(value, python_types)\n    return check", "response": "Make a function that checks whether a scalar or array is of a given kind\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncoerces a value with the specified numpy dtype.", "response": "def coerce_to_dtype(dtype, value):\n    \"\"\"\n    Make a value with the specified numpy dtype.\n\n    Only datetime64[ns] and datetime64[D] are supported for datetime dtypes.\n    \"\"\"\n    name = dtype.name\n    if name.startswith('datetime64'):\n        if name == 'datetime64[D]':\n            return make_datetime64D(value)\n        elif name == 'datetime64[ns]':\n            return make_datetime64ns(value)\n        else:\n            raise TypeError(\n                \"Don't know how to coerce values of dtype %s\" % dtype\n            )\n    return dtype.type(value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef repeat_first_axis(array, count):\n    return as_strided(array, (count,) + array.shape, (0,) + array.strides)", "response": "Restrides array along the first axis and returns the resulting array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isnat(obj):\n    if obj.dtype.kind not in ('m', 'M'):\n        raise ValueError(\"%s is not a numpy datetime or timedelta\")\n    return obj.view(int64_dtype) == iNaT", "response": "Check if a value is np. NaT."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_missing(data, missing_value):\n    if is_float(data) and isnan(missing_value):\n        return isnan(data)\n    elif is_datetime(data) and isnat(missing_value):\n        return isnat(data)\n    return (data == missing_value)", "response": "Generic is_missing function that handles NaN and NaT."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef changed_locations(a, include_first):\n    if a.ndim > 1:\n        raise ValueError(\"indices_of_changed_values only supports 1D arrays.\")\n    indices = flatnonzero(diff(a)) + 1\n\n    if not include_first:\n        return indices\n\n    return hstack([[0], indices])", "response": "Compute indices of values in a that differ from the previous value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the start and end dates for a single segment of the pipeline for a given date range.", "response": "def compute_date_range_chunks(sessions, start_date, end_date, chunksize):\n    \"\"\"Compute the start and end dates to run a pipeline for.\n\n    Parameters\n    ----------\n    sessions : DatetimeIndex\n        The available dates.\n    start_date : pd.Timestamp\n        The first date in the pipeline.\n    end_date : pd.Timestamp\n        The last date in the pipeline.\n    chunksize : int or None\n        The size of the chunks to run. Setting this to None returns one chunk.\n\n    Returns\n    -------\n    ranges : iterable[(np.datetime64, np.datetime64)]\n        A sequence of start and end dates to run the pipeline for.\n    \"\"\"\n    if start_date not in sessions:\n        raise KeyError(\"Start date %s is not found in calendar.\" %\n                       (start_date.strftime(\"%Y-%m-%d\"),))\n    if end_date not in sessions:\n        raise KeyError(\"End date %s is not found in calendar.\" %\n                       (end_date.strftime(\"%Y-%m-%d\"),))\n    if end_date < start_date:\n        raise ValueError(\"End date %s cannot precede start date %s.\" %\n                         (end_date.strftime(\"%Y-%m-%d\"),\n                          start_date.strftime(\"%Y-%m-%d\")))\n\n    if chunksize is None:\n        return [(start_date, end_date)]\n\n    start_ix, end_ix = sessions.slice_locs(start_date, end_date)\n    return (\n        (r[0], r[-1]) for r in partition_all(\n            chunksize, sessions[start_ix:end_ix]\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute a pipeline and return a DataFrame of results.", "response": "def run_pipeline(self, pipeline, start_date, end_date):\n        \"\"\"\n        Compute a pipeline.\n\n        Parameters\n        ----------\n        pipeline : zipline.pipeline.Pipeline\n            The pipeline to run.\n        start_date : pd.Timestamp\n            Start date of the computed matrix.\n        end_date : pd.Timestamp\n            End date of the computed matrix.\n\n        Returns\n        -------\n        result : pd.DataFrame\n            A frame of computed results.\n\n            The ``result`` columns correspond to the entries of\n            `pipeline.columns`, which should be a dictionary mapping strings to\n            instances of :class:`zipline.pipeline.term.Term`.\n\n            For each date between ``start_date`` and ``end_date``, ``result``\n            will contain a row for each asset that passed `pipeline.screen`.\n            A screen of ``None`` indicates that a row should be returned for\n            each asset that existed each day.\n\n        See Also\n        --------\n        :meth:`zipline.pipeline.engine.PipelineEngine.run_pipeline`\n        :meth:`zipline.pipeline.engine.PipelineEngine.run_chunked_pipeline`\n        \"\"\"\n        # See notes at the top of this module for a description of the\n        # algorithm implemented here.\n        if end_date < start_date:\n            raise ValueError(\n                \"start_date must be before or equal to end_date \\n\"\n                \"start_date=%s, end_date=%s\" % (start_date, end_date)\n\n            )\n\n        domain = self.resolve_domain(pipeline)\n\n        graph = pipeline.to_execution_plan(\n            domain, self._root_mask_term, start_date, end_date,\n        )\n        extra_rows = graph.extra_rows[self._root_mask_term]\n        root_mask = self._compute_root_mask(\n            domain, start_date, end_date, extra_rows,\n        )\n        dates, assets, root_mask_values = explode(root_mask)\n\n        initial_workspace = self._populate_initial_workspace(\n            {\n                self._root_mask_term: root_mask_values,\n                self._root_mask_dates_term: as_column(dates.values)\n            },\n            self._root_mask_term,\n            graph,\n            dates,\n            assets,\n        )\n\n        results = self.compute_chunk(graph, dates, assets, initial_workspace)\n\n        return self._to_narrow(\n            graph.outputs,\n            results,\n            results.pop(graph.screen_name),\n            dates[extra_rows:],\n            assets,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_root_mask(self, domain, start_date, end_date, extra_rows):\n        sessions = domain.all_sessions()\n\n        if start_date not in sessions:\n            raise ValueError(\n                \"Pipeline start date ({}) is not a trading session for \"\n                \"domain {}.\".format(start_date, domain)\n            )\n\n        elif end_date not in sessions:\n            raise ValueError(\n                \"Pipeline end date {} is not a trading session for \"\n                \"domain {}.\".format(end_date, domain)\n            )\n\n        start_idx, end_idx = sessions.slice_locs(start_date, end_date)\n        if start_idx < extra_rows:\n            raise NoFurtherDataError.from_lookback_window(\n                initial_message=\"Insufficient data to compute Pipeline:\",\n                first_date=sessions[0],\n                lookback_start=start_date,\n                lookback_length=extra_rows,\n            )\n\n        # NOTE: This logic should probably be delegated to the domain once we\n        #       start adding more complex domains.\n        #\n        # Build lifetimes matrix reaching back to `extra_rows` days before\n        # `start_date.`\n        finder = self._finder\n        lifetimes = finder.lifetimes(\n            sessions[start_idx - extra_rows:end_idx],\n            include_start_date=False,\n            country_codes=(domain.country_code,),\n        )\n\n        if not lifetimes.columns.unique:\n            columns = lifetimes.columns\n            duplicated = columns[columns.duplicated()].unique()\n            raise AssertionError(\"Duplicated sids: %d\" % duplicated)\n\n        # Filter out columns that didn't exist from the farthest look back\n        # window through the end of the requested dates.\n        existed = lifetimes.any()\n        ret = lifetimes.loc[:, existed]\n        num_assets = ret.shape[1]\n\n        if num_assets == 0:\n            raise ValueError(\n                \"Failed to find any assets with country_code {!r} that traded \"\n                \"between {} and {}.\\n\"\n                \"This probably means that your asset db is old or that it has \"\n                \"incorrect country/exchange metadata.\".format(\n                    domain.country_code, start_date, end_date,\n                )\n            )\n\n        return ret", "response": "Compute a lifetimes matrix from our AssetFinder and then drop columns that didn t exist at all during the query dates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_chunk(self, graph, dates, sids, initial_workspace):\n        self._validate_compute_chunk_params(\n            graph, dates, sids, initial_workspace,\n        )\n        get_loader = self._get_loader\n\n        # Copy the supplied initial workspace so we don't mutate it in place.\n        workspace = initial_workspace.copy()\n        refcounts = graph.initial_refcounts(workspace)\n        execution_order = graph.execution_order(refcounts)\n        domain = graph.domain\n\n        # Many loaders can fetch data more efficiently if we ask them to\n        # retrieve all their inputs at once. For example, a loader backed by a\n        # SQL database can fetch multiple columns from the database in a single\n        # query.\n        #\n        # To enable these loaders to fetch their data efficiently, we group\n        # together requests for LoadableTerms if they are provided by the same\n        # loader and they require the same number of extra rows.\n        #\n        # The extra rows condition is a simplification: we don't currently have\n        # a mechanism for asking a loader to fetch different windows of data\n        # for different terms, so we only batch requests together when they're\n        # going to produce data for the same set of dates. That may change in\n        # the future if we find a loader that can still benefit significantly\n        # from batching unequal-length requests.\n        def loader_group_key(term):\n            loader = get_loader(term)\n            extra_rows = graph.extra_rows[term]\n            return loader, extra_rows\n\n        # Only produce loader groups for the terms we expect to load.  This\n        # ensures that we can run pipelines for graphs where we don't have a\n        # loader registered for an atomic term if all the dependencies of that\n        # term were supplied in the initial workspace.\n        will_be_loaded = graph.loadable_terms - viewkeys(workspace)\n        loader_groups = groupby(\n            loader_group_key,\n            (t for t in execution_order if t in will_be_loaded),\n        )\n\n        for term in graph.execution_order(refcounts):\n            # `term` may have been supplied in `initial_workspace`, and in the\n            # future we may pre-compute loadable terms coming from the same\n            # dataset.  In either case, we will already have an entry for this\n            # term, which we shouldn't re-compute.\n            if term in workspace:\n                continue\n\n            # Asset labels are always the same, but date labels vary by how\n            # many extra rows are needed.\n            mask, mask_dates = graph.mask_and_dates_for_term(\n                term,\n                self._root_mask_term,\n                workspace,\n                dates,\n            )\n\n            if isinstance(term, LoadableTerm):\n                loader = get_loader(term)\n                to_load = sorted(\n                    loader_groups[loader_group_key(term)],\n                    key=lambda t: t.dataset\n                )\n                loaded = loader.load_adjusted_array(\n                    domain, to_load, mask_dates, sids, mask,\n                )\n                assert set(loaded) == set(to_load), (\n                    'loader did not return an AdjustedArray for each column\\n'\n                    'expected: %r\\n'\n                    'got:      %r' % (sorted(to_load), sorted(loaded))\n                )\n                workspace.update(loaded)\n            else:\n                workspace[term] = term._compute(\n                    self._inputs_for_term(term, workspace, graph, domain),\n                    mask_dates,\n                    sids,\n                    mask,\n                )\n                if term.ndim == 2:\n                    assert workspace[term].shape == mask.shape\n                else:\n                    assert workspace[term].shape == (mask.shape[0], 1)\n\n                # Decref dependencies of ``term``, and clear any terms whose\n                # refcounts hit 0.\n                for garbage_term in graph.decref_dependencies(term, refcounts):\n                    del workspace[garbage_term]\n\n        # At this point, all the output terms are in the workspace.\n        out = {}\n        graph_extra_rows = graph.extra_rows\n        for name, term in iteritems(graph.outputs):\n            # Truncate off extra rows from outputs.\n            out[name] = workspace[term][graph_extra_rows[term]:]\n        return out", "response": "Compute the pipeline terms for the requested start and end dates and store the results in a dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _to_narrow(self, terms, data, mask, dates, assets):\n        if not mask.any():\n            # Manually handle the empty DataFrame case. This is a workaround\n            # to pandas failing to tz_localize an empty dataframe with a\n            # MultiIndex. It also saves us the work of applying a known-empty\n            # mask to each array.\n            #\n            # Slicing `dates` here to preserve pandas metadata.\n            empty_dates = dates[:0]\n            empty_assets = array([], dtype=object)\n            return DataFrame(\n                data={\n                    name: array([], dtype=arr.dtype)\n                    for name, arr in iteritems(data)\n                },\n                index=MultiIndex.from_arrays([empty_dates, empty_assets]),\n            )\n\n        resolved_assets = array(self._finder.retrieve_all(assets))\n        dates_kept = repeat_last_axis(dates.values, len(assets))[mask]\n        assets_kept = repeat_first_axis(resolved_assets, len(dates))[mask]\n\n        final_columns = {}\n        for name in data:\n            # Each term that computed an output has its postprocess method\n            # called on the filtered result.\n            #\n            # As of Mon May 2 15:38:47 2016, we only use this to convert\n            # LabelArrays into categoricals.\n            final_columns[name] = terms[name].postprocess(data[name][mask])\n\n        return DataFrame(\n            data=final_columns,\n            index=MultiIndex.from_arrays([dates_kept, assets_kept]),\n        ).tz_localize('UTC', level=0)", "response": "Convert raw computed pipeline results into a DataFrame for public APIs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_compute_chunk_params(self,\n                                       graph,\n                                       dates,\n                                       sids,\n                                       initial_workspace):\n        \"\"\"\n        Verify that the values passed to compute_chunk are well-formed.\n        \"\"\"\n        root = self._root_mask_term\n        clsname = type(self).__name__\n\n        # Writing this out explicitly so this errors in testing if we change\n        # the name without updating this line.\n        compute_chunk_name = self.compute_chunk.__name__\n        if root not in initial_workspace:\n            raise AssertionError(\n                \"root_mask values not supplied to {cls}.{method}\".format(\n                    cls=clsname,\n                    method=compute_chunk_name,\n                )\n            )\n\n        shape = initial_workspace[root].shape\n        implied_shape = len(dates), len(sids)\n        if shape != implied_shape:\n            raise AssertionError(\n                \"root_mask shape is {shape}, but received dates/assets \"\n                \"imply that shape should be {implied}\".format(\n                    shape=shape,\n                    implied=implied_shape,\n                )\n            )\n\n        for term in initial_workspace:\n            if self._is_special_root_term(term):\n                continue\n\n            if term.domain is GENERIC:\n                # XXX: We really shouldn't allow **any** generic terms to be\n                # populated in the initial workspace. A generic term, by\n                # definition, can't correspond to concrete data until it's\n                # paired with a domain, and populate_initial_workspace isn't\n                # given the domain of execution, so it can't possibly know what\n                # data to use when populating a generic term.\n                #\n                # In our current implementation, however, we don't have a good\n                # way to represent specializations of ComputableTerms that take\n                # only generic inputs, so there's no good way for the initial\n                # workspace to provide data for such terms except by populating\n                # the generic ComputableTerm.\n                #\n                # The right fix for the above is to implement \"full\n                # specialization\", i.e., implementing ``specialize`` uniformly\n                # across all terms, not just LoadableTerms. Having full\n                # specialization will also remove the need for all of the\n                # remaining ``maybe_specialize`` calls floating around in this\n                # file.\n                #\n                # In the meantime, disallowing ComputableTerms in the initial\n                # workspace would break almost every test in\n                # `test_filter`/`test_factor`/`test_classifier`, and fixing\n                # them would require updating all those tests to compute with\n                # more specialized terms. Once we have full specialization, we\n                # can fix all the tests without a large volume of edits by\n                # simply specializing their workspaces, so for now I'm leaving\n                # this in place as a somewhat sharp edge.\n                if isinstance(term, LoadableTerm):\n                    raise ValueError(\n                        \"Loadable workspace terms must be specialized to a \"\n                        \"domain, but got generic term {}\".format(term)\n                    )\n\n            elif term.domain != graph.domain:\n                raise ValueError(\n                    \"Initial workspace term {} has domain {}. \"\n                    \"Does not match pipeline domain {}\".format(\n                        term, term.domain, graph.domain,\n                    )\n                )", "response": "Verify that the values passed to compute_chunk are well - formed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolve_domain(self, pipeline):\n        domain = pipeline.domain(default=self._default_domain)\n        if domain is GENERIC:\n            raise ValueError(\n                \"Unable to determine domain for Pipeline.\\n\"\n                \"Pass domain=<desired domain> to your Pipeline to set a \"\n                \"domain.\"\n            )\n        return domain", "response": "Resolve a concrete domain for the given Pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bulleted_list(items, indent=0, bullet_type='-'):\n    format_string = ' ' * indent + bullet_type + ' {}'\n    return \"\\n\".join(map(format_string.format, items))", "response": "Format a bulleted list of values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_rotating_equity_info(num_assets,\n                              first_start,\n                              frequency,\n                              periods_between_starts,\n                              asset_lifetime,\n                              exchange='TEST'):\n    \"\"\"\n    Create a DataFrame representing lifetimes of assets that are constantly\n    rotating in and out of existence.\n\n    Parameters\n    ----------\n    num_assets : int\n        How many assets to create.\n    first_start : pd.Timestamp\n        The start date for the first asset.\n    frequency : str or pd.tseries.offsets.Offset (e.g. trading_day)\n        Frequency used to interpret next two arguments.\n    periods_between_starts : int\n        Create a new asset every `frequency` * `periods_between_new`\n    asset_lifetime : int\n        Each asset exists for `frequency` * `asset_lifetime` days.\n    exchange : str, optional\n        The exchange name.\n\n    Returns\n    -------\n    info : pd.DataFrame\n        DataFrame representing newly-created assets.\n    \"\"\"\n    return pd.DataFrame(\n        {\n            'symbol': [chr(ord('A') + i) for i in range(num_assets)],\n            # Start a new asset every `periods_between_starts` days.\n            'start_date': pd.date_range(\n                first_start,\n                freq=(periods_between_starts * frequency),\n                periods=num_assets,\n            ),\n            # Each asset lasts for `asset_lifetime` days.\n            'end_date': pd.date_range(\n                first_start + (asset_lifetime * frequency),\n                freq=(periods_between_starts * frequency),\n                periods=num_assets,\n            ),\n            'exchange': exchange,\n        },\n        index=range(num_assets),\n    )", "response": "Creates a DataFrame representing lifetimes of assets that are constantly rotating in and out of existence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_simple_equity_info(sids,\n                            start_date,\n                            end_date,\n                            symbols=None,\n                            names=None,\n                            exchange='TEST'):\n    \"\"\"\n    Create a DataFrame representing assets that exist for the full duration\n    between `start_date` and `end_date`.\n\n    Parameters\n    ----------\n    sids : array-like of int\n    start_date : pd.Timestamp, optional\n    end_date : pd.Timestamp, optional\n    symbols : list, optional\n        Symbols to use for the assets.\n        If not provided, symbols are generated from the sequence 'A', 'B', ...\n    names : list, optional\n        Names to use for the assets.\n        If not provided, names are generated by adding \" INC.\" to each of the\n        symbols (which might also be auto-generated).\n    exchange : str, optional\n        The exchange name.\n\n    Returns\n    -------\n    info : pd.DataFrame\n        DataFrame representing newly-created assets.\n    \"\"\"\n    num_assets = len(sids)\n    if symbols is None:\n        symbols = list(ascii_uppercase[:num_assets])\n    else:\n        symbols = list(symbols)\n\n    if names is None:\n        names = [str(s) + \" INC.\" for s in symbols]\n\n    return pd.DataFrame(\n        {\n            'symbol': symbols,\n            'start_date': pd.to_datetime([start_date] * num_assets),\n            'end_date': pd.to_datetime([end_date] * num_assets),\n            'asset_name': list(names),\n            'exchange': exchange,\n        },\n        index=sids,\n        columns=(\n            'start_date',\n            'end_date',\n            'symbol',\n            'exchange',\n            'asset_name',\n        ),\n    )", "response": "Create a DataFrame representing assets that exist for the full duration\n    between start_date and end_date."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a DataFrame representing assets that exist for the full duration between start_date and end_date from multiple countries.", "response": "def make_simple_multi_country_equity_info(countries_to_sids,\n                                          countries_to_exchanges,\n                                          start_date,\n                                          end_date):\n    \"\"\"Create a DataFrame representing assets that exist for the full duration\n    between `start_date` and `end_date`, from multiple countries.\n    \"\"\"\n    sids = []\n    symbols = []\n    exchanges = []\n\n    for country, country_sids in countries_to_sids.items():\n        exchange = countries_to_exchanges[country]\n        for i, sid in enumerate(country_sids):\n            sids.append(sid)\n            symbols.append('-'.join([country, str(i)]))\n            exchanges.append(exchange)\n\n    return pd.DataFrame(\n        {\n            'symbol': symbols,\n            'start_date': start_date,\n            'end_date': end_date,\n            'asset_name': symbols,\n            'exchange': exchanges,\n        },\n        index=sids,\n        columns=(\n            'start_date',\n            'end_date',\n            'symbol',\n            'exchange',\n            'asset_name',\n        ),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a DataFrame representing assets that all begin at the same start_date but have cascading end dates.", "response": "def make_jagged_equity_info(num_assets,\n                            start_date,\n                            first_end,\n                            frequency,\n                            periods_between_ends,\n                            auto_close_delta):\n    \"\"\"\n    Create a DataFrame representing assets that all begin at the same start\n    date, but have cascading end dates.\n\n    Parameters\n    ----------\n    num_assets : int\n        How many assets to create.\n    start_date : pd.Timestamp\n        The start date for all the assets.\n    first_end : pd.Timestamp\n        The date at which the first equity will end.\n    frequency : str or pd.tseries.offsets.Offset (e.g. trading_day)\n        Frequency used to interpret the next argument.\n    periods_between_ends : int\n        Starting after the first end date, end each asset every\n        `frequency` * `periods_between_ends`.\n\n    Returns\n    -------\n    info : pd.DataFrame\n        DataFrame representing newly-created assets.\n    \"\"\"\n    frame = pd.DataFrame(\n        {\n            'symbol': [chr(ord('A') + i) for i in range(num_assets)],\n            'start_date': start_date,\n            'end_date': pd.date_range(\n                first_end,\n                freq=(periods_between_ends * frequency),\n                periods=num_assets,\n            ),\n            'exchange': 'TEST',\n        },\n        index=range(num_assets),\n    )\n\n    # Explicitly pass None to disable setting the auto_close_date column.\n    if auto_close_delta is not None:\n        frame['auto_close_date'] = frame['end_date'] + auto_close_delta\n\n    return frame"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_future_info(first_sid,\n                     root_symbols,\n                     years,\n                     notice_date_func,\n                     expiration_date_func,\n                     start_date_func,\n                     month_codes=None,\n                     multiplier=500):\n    \"\"\"\n    Create a DataFrame representing futures for `root_symbols` during `year`.\n\n    Generates a contract per triple of (symbol, year, month) supplied to\n    `root_symbols`, `years`, and `month_codes`.\n\n    Parameters\n    ----------\n    first_sid : int\n        The first sid to use for assigning sids to the created contracts.\n    root_symbols : list[str]\n        A list of root symbols for which to create futures.\n    years : list[int or str]\n        Years (e.g. 2014), for which to produce individual contracts.\n    notice_date_func : (Timestamp) -> Timestamp\n        Function to generate notice dates from first of the month associated\n        with asset month code.  Return NaT to simulate futures with no notice\n        date.\n    expiration_date_func : (Timestamp) -> Timestamp\n        Function to generate expiration dates from first of the month\n        associated with asset month code.\n    start_date_func : (Timestamp) -> Timestamp, optional\n        Function to generate start dates from first of the month associated\n        with each asset month code.  Defaults to a start_date one year prior\n        to the month_code date.\n    month_codes : dict[str -> [1..12]], optional\n        Dictionary of month codes for which to create contracts.  Entries\n        should be strings mapped to values from 1 (January) to 12 (December).\n        Default is zipline.futures.CMES_CODE_TO_MONTH\n    multiplier : int\n        The contract multiplier.\n\n    Returns\n    -------\n    futures_info : pd.DataFrame\n        DataFrame of futures data suitable for passing to an AssetDBWriter.\n    \"\"\"\n    if month_codes is None:\n        month_codes = CMES_CODE_TO_MONTH\n\n    year_strs = list(map(str, years))\n    years = [pd.Timestamp(s, tz='UTC') for s in year_strs]\n\n    # Pairs of string/date like ('K06', 2006-05-01)\n    contract_suffix_to_beginning_of_month = tuple(\n        (month_code + year_str[-2:], year + MonthBegin(month_num))\n        for ((year, year_str), (month_code, month_num))\n        in product(\n            zip(years, year_strs),\n            iteritems(month_codes),\n        )\n    )\n\n    contracts = []\n    parts = product(root_symbols, contract_suffix_to_beginning_of_month)\n    for sid, (root_sym, (suffix, month_begin)) in enumerate(parts, first_sid):\n        contracts.append({\n            'sid': sid,\n            'root_symbol': root_sym,\n            'symbol': root_sym + suffix,\n            'start_date': start_date_func(month_begin),\n            'notice_date': notice_date_func(month_begin),\n            'expiration_date': notice_date_func(month_begin),\n            'multiplier': multiplier,\n            'exchange': \"TEST\",\n        })\n    return pd.DataFrame.from_records(contracts, index='sid')", "response": "Create a DataFrame representing futures for the given root_symbols and years."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new future info for a commodity.", "response": "def make_commodity_future_info(first_sid,\n                               root_symbols,\n                               years,\n                               month_codes=None,\n                               multiplier=500):\n    \"\"\"\n    Make futures testing data that simulates the notice/expiration date\n    behavior of physical commodities like oil.\n\n    Parameters\n    ----------\n    first_sid : int\n        The first sid to use for assigning sids to the created contracts.\n    root_symbols : list[str]\n        A list of root symbols for which to create futures.\n    years : list[int or str]\n        Years (e.g. 2014), for which to produce individual contracts.\n    month_codes : dict[str -> [1..12]], optional\n        Dictionary of month codes for which to create contracts.  Entries\n        should be strings mapped to values from 1 (January) to 12 (December).\n        Default is zipline.futures.CMES_CODE_TO_MONTH\n    multiplier : int\n        The contract multiplier.\n\n    Expiration dates are on the 20th of the month prior to the month code.\n    Notice dates are are on the 20th two months prior to the month code.\n    Start dates are one year before the contract month.\n\n    See Also\n    --------\n    make_future_info\n    \"\"\"\n    nineteen_days = pd.Timedelta(days=19)\n    one_year = pd.Timedelta(days=365)\n    return make_future_info(\n        first_sid=first_sid,\n        root_symbols=root_symbols,\n        years=years,\n        notice_date_func=lambda dt: dt - MonthBegin(2) + nineteen_days,\n        expiration_date_func=lambda dt: dt - MonthBegin(1) + nineteen_days,\n        start_date_func=lambda dt: dt - one_year,\n        month_codes=month_codes,\n        multiplier=multiplier,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eq(self, other):\n        # We treat this as an error because missing_values have NaN semantics,\n        # which means this would return an array of all False, which is almost\n        # certainly not what the user wants.\n        if other == self.missing_value:\n            raise ValueError(\n                \"Comparison against self.missing_value ({value!r}) in\"\n                \" {typename}.eq().\\n\"\n                \"Missing values have NaN semantics, so the \"\n                \"requested comparison would always produce False.\\n\"\n                \"Use the isnull() method to check for missing values.\".format(\n                    value=other,\n                    typename=(type(self).__name__),\n                )\n            )\n\n        if isinstance(other, Number) != (self.dtype == int64_dtype):\n            raise InvalidClassifierComparison(self, other)\n\n        if isinstance(other, Number):\n            return NumExprFilter.create(\n                \"x_0 == {other}\".format(other=int(other)),\n                binds=(self,),\n            )\n        else:\n            return ArrayPredicate(\n                term=self,\n                op=operator.eq,\n                opargs=(other,),\n            )", "response": "Construct a Filter returning True for asset / date pairs where the output\n        of self matches other."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs a Filter matching values starting with prefix against self.", "response": "def startswith(self, prefix):\n        \"\"\"\n        Construct a Filter matching values starting with ``prefix``.\n\n        Parameters\n        ----------\n        prefix : str\n            String prefix against which to compare values produced by ``self``.\n\n        Returns\n        -------\n        matches : Filter\n            Filter returning True for all sid/date pairs for which ``self``\n            produces a string starting with ``prefix``.\n        \"\"\"\n        return ArrayPredicate(\n            term=self,\n            op=LabelArray.startswith,\n            opargs=(prefix,),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef endswith(self, suffix):\n        return ArrayPredicate(\n            term=self,\n            op=LabelArray.endswith,\n            opargs=(suffix,),\n        )", "response": "Construct a Filter matching values ending with suffix against self."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct a Filter matching values produced by self containing substring.", "response": "def has_substring(self, substring):\n        \"\"\"\n        Construct a Filter matching values containing ``substring``.\n\n        Parameters\n        ----------\n        substring : str\n            Sub-string against which to compare values produced by ``self``.\n\n        Returns\n        -------\n        matches : Filter\n            Filter returning True for all sid/date pairs for which ``self``\n            produces a string containing ``substring``.\n        \"\"\"\n        return ArrayPredicate(\n            term=self,\n            op=LabelArray.has_substring,\n            opargs=(substring,),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef matches(self, pattern):\n        return ArrayPredicate(\n            term=self,\n            op=LabelArray.matches,\n            opargs=(pattern,),\n        )", "response": "Construct a Filter that checks regex matches against pattern."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a Filter indicating whether values are in choices.", "response": "def element_of(self, choices):\n        \"\"\"\n        Construct a Filter indicating whether values are in ``choices``.\n\n        Parameters\n        ----------\n        choices : iterable[str or int]\n            An iterable of choices.\n\n        Returns\n        -------\n        matches : Filter\n            Filter returning True for all sid/date pairs for which ``self``\n            produces an entry in ``choices``.\n        \"\"\"\n        try:\n            choices = frozenset(choices)\n        except Exception as e:\n            raise TypeError(\n                \"Expected `choices` to be an iterable of hashable values,\"\n                \" but got {} instead.\\n\"\n                \"This caused the following error: {!r}.\".format(choices, e)\n            )\n\n        if self.missing_value in choices:\n            raise ValueError(\n                \"Found self.missing_value ({mv!r}) in choices supplied to\"\n                \" {typename}.{meth_name}().\\n\"\n                \"Missing values have NaN semantics, so the\"\n                \" requested comparison would always produce False.\\n\"\n                \"Use the isnull() method to check for missing values.\\n\"\n                \"Received choices were {choices}.\".format(\n                    mv=self.missing_value,\n                    typename=(type(self).__name__),\n                    choices=sorted(choices),\n                    meth_name=self.element_of.__name__,\n                )\n            )\n\n        def only_contains(type_, values):\n            return all(isinstance(v, type_) for v in values)\n\n        if self.dtype == int64_dtype:\n            if only_contains(int, choices):\n                return ArrayPredicate(\n                    term=self,\n                    op=vectorized_is_element,\n                    opargs=(choices,),\n                )\n            else:\n                raise TypeError(\n                    \"Found non-int in choices for {typename}.element_of.\\n\"\n                    \"Supplied choices were {choices}.\".format(\n                        typename=type(self).__name__,\n                        choices=choices,\n                    )\n                )\n        elif self.dtype == categorical_dtype:\n            if only_contains((bytes, unicode), choices):\n                return ArrayPredicate(\n                    term=self,\n                    op=LabelArray.element_of,\n                    opargs=(choices,),\n                )\n            else:\n                raise TypeError(\n                    \"Found non-string in choices for {typename}.element_of.\\n\"\n                    \"Supplied choices were {choices}.\".format(\n                        typename=type(self).__name__,\n                        choices=choices,\n                    )\n                )\n        assert False, \"Unknown dtype in Classifier.element_of %s.\" % self.dtype"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_workspace_value(self, result, assets):\n        if self.dtype == int64_dtype:\n            return super(Classifier, self).to_workspace_value(result, assets)\n\n        assert isinstance(result.values, pd.Categorical), (\n            'Expected a Categorical, got %r.' % type(result.values)\n        )\n        with_missing = pd.Series(\n            data=pd.Categorical(\n                result.values,\n                result.values.categories.union([self.missing_value]),\n            ),\n            index=result.index,\n        )\n        return LabelArray(\n            super(Classifier, self).to_workspace_value(\n                with_missing,\n                assets,\n            ),\n            self.missing_value,\n        )", "response": "Returns a workspace value for the result of a pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an array produced by this classifier into an array of integer labels and a missing value label.", "response": "def _to_integral(self, output_array):\n        \"\"\"\n        Convert an array produced by this classifier into an array of integer\n        labels and a missing value label.\n        \"\"\"\n        if self.dtype == int64_dtype:\n            group_labels = output_array\n            null_label = self.missing_value\n        elif self.dtype == categorical_dtype:\n            # Coerce LabelArray into an isomorphic array of ints.  This is\n            # necessary because np.where doesn't know about LabelArrays or the\n            # void dtype.\n            group_labels = output_array.as_int_array()\n            null_label = output_array.missing_value_code\n        else:\n            raise AssertionError(\n                \"Unexpected Classifier dtype: %s.\" % self.dtype\n            )\n        return group_labels, null_label"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides the default array allocation to produce a LabelArray when we have a string - like dtype.", "response": "def _allocate_output(self, windows, shape):\n        \"\"\"\n        Override the default array allocation to produce a LabelArray when we\n        have a string-like dtype.\n        \"\"\"\n        if self.dtype == int64_dtype:\n            return super(CustomClassifier, self)._allocate_output(\n                windows,\n                shape,\n            )\n\n        # This is a little bit of a hack.  We might not know what the\n        # categories for a LabelArray are until it's actually been loaded, so\n        # we need to look at the underlying data.\n        return windows[0].data.empty_like(shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that all axes of a pandas object are unique.", "response": "def verify_indices_all_unique(obj):\n    \"\"\"\n    Check that all axes of a pandas object are unique.\n\n    Parameters\n    ----------\n    obj : pd.Series / pd.DataFrame / pd.Panel\n        The object to validate.\n\n    Returns\n    -------\n    obj : pd.Series / pd.DataFrame / pd.Panel\n        The validated object, unchanged.\n\n    Raises\n    ------\n    ValueError\n        If any axis has duplicate entries.\n    \"\"\"\n    axis_names = [\n        ('index',),                            # Series\n        ('index', 'columns'),                  # DataFrame\n        ('items', 'major_axis', 'minor_axis')  # Panel\n    ][obj.ndim - 1]  # ndim = 1 should go to entry 0,\n\n    for axis_name, index in zip(axis_names, obj.axes):\n        if index.is_unique:\n            continue\n\n        raise ValueError(\n            \"Duplicate entries in {type}.{axis}: {dupes}.\".format(\n                type=type(obj).__name__,\n                axis=axis_name,\n                dupes=sorted(index[index.duplicated()]),\n            )\n        )\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmodifying a preprocessor to allow None.", "response": "def optionally(preprocessor):\n    \"\"\"Modify a preprocessor to explicitly allow `None`.\n\n    Parameters\n    ----------\n    preprocessor : callable[callable, str, any -> any]\n        A preprocessor to delegate to when `arg is not None`.\n\n    Returns\n    -------\n    optional_preprocessor : callable[callable, str, any -> any]\n        A preprocessor that delegates to `preprocessor` when `arg is not None`.\n\n    Examples\n    --------\n    >>> def preprocessor(func, argname, arg):\n    ...     if not isinstance(arg, int):\n    ...         raise TypeError('arg must be int')\n    ...     return arg\n    ...\n    >>> @preprocess(a=optionally(preprocessor))\n    ... def f(a):\n    ...     return a\n    ...\n    >>> f(1)  # call with int\n    1\n    >>> f('a')  # call with not int\n    Traceback (most recent call last):\n       ...\n    TypeError: arg must be int\n    >>> f(None) is None  # call with explicit None\n    True\n    \"\"\"\n    @wraps(preprocessor)\n    def wrapper(func, argname, arg):\n        return arg if arg is None else preprocessor(func, argname, arg)\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ensure_dtype(func, argname, arg):\n    try:\n        return dtype(arg)\n    except TypeError:\n        raise TypeError(\n            \"{func}() couldn't convert argument \"\n            \"{argname}={arg!r} to a numpy dtype.\".format(\n                func=_qualified_name(func),\n                argname=argname,\n                arg=arg,\n            ),\n        )", "response": "Ensures that the input is a numpy dtype."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef expect_dtypes(__funcname=_qualified_name, **named):\n    for name, type_ in iteritems(named):\n        if not isinstance(type_, (dtype, tuple)):\n            raise TypeError(\n                \"expect_dtypes() expected a numpy dtype or tuple of dtypes\"\n                \" for argument {name!r}, but got {dtype} instead.\".format(\n                    name=name, dtype=dtype,\n                )\n            )\n\n    if isinstance(__funcname, str):\n        def get_funcname(_):\n            return __funcname\n    else:\n        get_funcname = __funcname\n\n    @preprocess(dtypes=call(lambda x: x if isinstance(x, tuple) else (x,)))\n    def _expect_dtype(dtypes):\n        \"\"\"\n        Factory for dtype-checking functions that work with the @preprocess\n        decorator.\n        \"\"\"\n        def error_message(func, argname, value):\n            # If the bad value has a dtype, but it's wrong, show the dtype\n            # name.  Otherwise just show the value.\n            try:\n                value_to_show = value.dtype.name\n            except AttributeError:\n                value_to_show = value\n            return (\n                \"{funcname}() expected a value with dtype {dtype_str} \"\n                \"for argument {argname!r}, but got {value!r} instead.\"\n            ).format(\n                funcname=get_funcname(func),\n                dtype_str=' or '.join(repr(d.name) for d in dtypes),\n                argname=argname,\n                value=value_to_show,\n            )\n\n        def _actual_preprocessor(func, argname, argvalue):\n            if getattr(argvalue, 'dtype', object()) not in dtypes:\n                raise TypeError(error_message(func, argname, argvalue))\n            return argvalue\n\n        return _actual_preprocessor\n\n    return preprocess(**valmap(_expect_dtype, named))", "response": "Decorator that verifies inputs have expected numpy dtypes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_check(exc_type, template, pred, actual, funcname):\n    if isinstance(funcname, str):\n        def get_funcname(_):\n            return funcname\n    else:\n        get_funcname = funcname\n\n    def _check(func, argname, argvalue):\n        if pred(argvalue):\n            raise exc_type(\n                template % {\n                    'funcname': get_funcname(func),\n                    'argname': argname,\n                    'actual': actual(argvalue),\n                },\n            )\n        return argvalue\n    return _check", "response": "Creates a preprocessing function that checks a predicate on the input value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expect_element(__funcname=_qualified_name, **named):\n    def _expect_element(collection):\n        if isinstance(collection, (set, frozenset)):\n            # Special case the error message for set and frozen set to make it\n            # less verbose.\n            collection_for_error_message = tuple(sorted(collection))\n        else:\n            collection_for_error_message = collection\n\n        template = (\n            \"%(funcname)s() expected a value in {collection} \"\n            \"for argument '%(argname)s', but got %(actual)s instead.\"\n        ).format(collection=collection_for_error_message)\n        return make_check(\n            ValueError,\n            template,\n            complement(op.contains(collection)),\n            repr,\n            funcname=__funcname,\n        )\n    return preprocess(**valmap(_expect_element, named))", "response": "Preprocessing decorator that verifies inputs are elements of some\n            expected collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expect_bounded(__funcname=_qualified_name, **named):\n    def _make_bounded_check(bounds):\n        (lower, upper) = bounds\n        if lower is None:\n            def should_fail(value):\n                return value > upper\n            predicate_descr = \"less than or equal to \" + str(upper)\n        elif upper is None:\n            def should_fail(value):\n                return value < lower\n            predicate_descr = \"greater than or equal to \" + str(lower)\n        else:\n            def should_fail(value):\n                return not (lower <= value <= upper)\n            predicate_descr = \"inclusively between %s and %s\" % bounds\n\n        template = (\n            \"%(funcname)s() expected a value {predicate}\"\n            \" for argument '%(argname)s', but got %(actual)s instead.\"\n        ).format(predicate=predicate_descr)\n\n        return make_check(\n            exc_type=ValueError,\n            template=template,\n            pred=should_fail,\n            actual=repr,\n            funcname=__funcname,\n        )\n\n    return _expect_bounded(_make_bounded_check, __funcname=__funcname, **named)", "response": "Decorator that verifies that the input is in the INCLUSIVELY between the two input parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef coerce_types(**kwargs):\n    def _coerce(types):\n        return coerce(*types)\n\n    return preprocess(**valmap(_coerce, kwargs))", "response": "Returns a new\n    that can be used to coerce types."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate that a dictionary has an expected set of keys.", "response": "def validate_keys(dict_, expected, funcname):\n    \"\"\"Validate that a dictionary has an expected set of keys.\n    \"\"\"\n    expected = set(expected)\n    received = set(dict_)\n\n    missing = expected - received\n    if missing:\n        raise ValueError(\n            \"Missing keys in {}:\\n\"\n            \"Expected Keys: {}\\n\"\n            \"Received Keys: {}\".format(\n                funcname,\n                sorted(expected),\n                sorted(received),\n            )\n        )\n\n    unexpected = received - expected\n    if unexpected:\n        raise ValueError(\n            \"Unexpected keys in {}:\\n\"\n            \"Expected Keys: {}\\n\"\n            \"Received Keys: {}\".format(\n                funcname,\n                sorted(expected),\n                sorted(received),\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enum(option, *options):\n    options = (option,) + options\n    rangeob = range(len(options))\n\n    try:\n        inttype = _inttypes[int(np.log2(len(options) - 1)) // 8]\n    except IndexError:\n        raise OverflowError(\n            'Cannot store enums with more than sys.maxsize elements, got %d' %\n            len(options),\n        )\n\n    class _enum(Structure):\n        _fields_ = [(o, inttype) for o in options]\n\n        def __iter__(self):\n            return iter(rangeob)\n\n        def __contains__(self, value):\n            return 0 <= value < len(options)\n\n        def __repr__(self):\n            return '<enum: %s>' % (\n                ('%d fields' % len(options))\n                if len(options) > 10 else\n                repr(options)\n            )\n\n    return _enum(*rangeob)", "response": "Construct a new enum object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef oldest_frame(self, raw=False):\n        if raw:\n            return self.buffer.values[:, self._start_index, :]\n        return self.buffer.iloc[:, self._start_index, :]", "response": "Get the oldest frame in the panel."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extend_back(self, missing_dts):\n        delta = len(missing_dts)\n\n        if not delta:\n            raise ValueError(\n                'missing_dts must be a non-empty index',\n            )\n\n        self._window += delta\n\n        self._pos += delta\n\n        self.date_buf = self.date_buf.copy()\n        self.date_buf.resize(self.cap)\n        self.date_buf = np.roll(self.date_buf, delta)\n\n        old_vals = self.buffer.values\n        shape = old_vals.shape\n        nan_arr = np.empty((shape[0], delta, shape[2]))\n        nan_arr.fill(np.nan)\n\n        new_vals = np.column_stack(\n            (nan_arr,\n             old_vals,\n             np.empty((shape[0], delta * (self.cap_multiple - 1), shape[2]))),\n        )\n\n        self.buffer = pd.Panel(\n            data=new_vals,\n            items=self.items,\n            minor_axis=self.minor_axis,\n            major_axis=np.arange(self.cap),\n            dtype=self.dtype,\n        )\n\n        # Fill the delta with the dates we calculated.\n        where = slice(self._start_index, self._start_index + delta)\n        self.date_buf[where] = missing_dts", "response": "Extend the buffer with the given missing_dts."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_current(self, item=None, raw=False, start=None, end=None):\n        item_indexer = slice(None)\n        if item:\n            item_indexer = self.items.get_loc(item)\n\n        start_index = self._start_index\n        end_index = self._pos\n\n        # get inital date window\n        where = slice(start_index, end_index)\n        current_dates = self.date_buf[where]\n\n        def convert_datelike_to_long(dt):\n            if isinstance(dt, pd.Timestamp):\n                return dt.asm8\n            if isinstance(dt, datetime.datetime):\n                return np.datetime64(dt)\n            return dt\n\n        # constrict further by date\n        if start:\n            start = convert_datelike_to_long(start)\n            start_index += current_dates.searchsorted(start)\n\n        if end:\n            end = convert_datelike_to_long(end)\n            _end = current_dates.searchsorted(end, 'right')\n            end_index -= len(current_dates) - _end\n\n        where = slice(start_index, end_index)\n\n        values = self.buffer.values[item_indexer, where, :]\n        current_dates = self.date_buf[where]\n\n        if raw:\n            # return copy so we can change it without side effects here\n            return values.copy()\n\n        major_axis = pd.DatetimeIndex(deepcopy(current_dates), tz='utc')\n        if values.ndim == 3:\n            return pd.Panel(values, self.items, major_axis, self.minor_axis,\n                            dtype=self.dtype)\n\n        elif values.ndim == 2:\n            return pd.DataFrame(values, major_axis, self.minor_axis,\n                                dtype=self.dtype)", "response": "Get a Panel that is the current data in view. It is safe to persist these objects because they are not in view."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the values stored in our current in - view data to be values of the passed panel.", "response": "def set_current(self, panel):\n        \"\"\"\n        Set the values stored in our current in-view data to be values of the\n        passed panel.  The passed panel must have the same indices as the panel\n        that would be returned by self.get_current.\n        \"\"\"\n        where = slice(self._start_index, self._pos)\n        self.buffer.values[:, where, :] = panel.values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _roll_data(self):\n\n        self.buffer.values[:, :self._window, :] = \\\n            self.buffer.values[:, -self._window:, :]\n        self.date_buf[:self._window] = self.date_buf[-self._window:]\n        self._pos = self._window", "response": "Roll the data up to the position zero."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef oldest_frame(self, raw=False):\n        if raw:\n            return self.buffer.values[:, self._oldest_frame_idx(), :]\n        return self.buffer.iloc[:, self._oldest_frame_idx(), :]", "response": "Get the oldest frame in the panel."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a Panel that is the current data in view.", "response": "def get_current(self):\n        \"\"\"\n        Get a Panel that is the current data in view. It is not safe to persist\n        these objects because internal data might change\n        \"\"\"\n\n        where = slice(self._oldest_frame_idx(), self._pos)\n        major_axis = pd.DatetimeIndex(deepcopy(self.date_buf[where]), tz='utc')\n        return pd.Panel(self.buffer.values[:, where, :], self.items,\n                        major_axis, self.minor_axis, dtype=self.dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_triggers(self, price, dt):\n        stop_reached, limit_reached, sl_stop_reached = \\\n            self.check_order_triggers(price)\n        if (stop_reached, limit_reached) \\\n                != (self.stop_reached, self.limit_reached):\n            self.dt = dt\n        self.stop_reached = stop_reached\n        self.limit_reached = limit_reached\n        if sl_stop_reached:\n            # Change the STOP LIMIT order into a LIMIT order\n            self.stop = None", "response": "Update internal state based on price triggers and the order triggers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the order triggers the current price.", "response": "def check_order_triggers(self, current_price):\n        \"\"\"\n        Given an order and a trade event, return a tuple of\n        (stop_reached, limit_reached).\n        For market orders, will return (False, False).\n        For stop orders, limit_reached will always be False.\n        For limit orders, stop_reached will always be False.\n        For stop limit orders a Boolean is returned to flag\n        that the stop has been reached.\n\n        Orders that have been triggered already (price targets reached),\n        the order's current values are returned.\n        \"\"\"\n        if self.triggered:\n            return (self.stop_reached, self.limit_reached, False)\n\n        stop_reached = False\n        limit_reached = False\n        sl_stop_reached = False\n\n        order_type = 0\n\n        if self.amount > 0:\n            order_type |= BUY\n        else:\n            order_type |= SELL\n\n        if self.stop is not None:\n            order_type |= STOP\n\n        if self.limit is not None:\n            order_type |= LIMIT\n\n        if order_type == BUY | STOP | LIMIT:\n            if current_price >= self.stop:\n                sl_stop_reached = True\n                if current_price <= self.limit:\n                    limit_reached = True\n        elif order_type == SELL | STOP | LIMIT:\n            if current_price <= self.stop:\n                sl_stop_reached = True\n                if current_price >= self.limit:\n                    limit_reached = True\n        elif order_type == BUY | STOP:\n            if current_price >= self.stop:\n                stop_reached = True\n        elif order_type == SELL | STOP:\n            if current_price <= self.stop:\n                stop_reached = True\n        elif order_type == BUY | LIMIT:\n            if current_price <= self.limit:\n                limit_reached = True\n        elif order_type == SELL | LIMIT:\n            # This is a SELL LIMIT order\n            if current_price >= self.limit:\n                limit_reached = True\n\n        return (stop_reached, limit_reached, sl_stop_reached)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef triggered(self):\n        if self.stop is not None and not self.stop_reached:\n            return False\n\n        if self.limit is not None and not self.limit_reached:\n            return False\n\n        return True", "response": "Returns True if the assessment was triggered."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup(self,\n          np=np,\n          numpy_version=numpy_version,\n          StrictVersion=StrictVersion,\n          new_pandas=new_pandas):\n    \"\"\"Lives in zipline.__init__ for doctests.\"\"\"\n\n    if numpy_version >= StrictVersion('1.14'):\n        self.old_opts = np.get_printoptions()\n        np.set_printoptions(legacy='1.13')\n    else:\n        self.old_opts = None\n\n    if new_pandas:\n        self.old_err = np.geterr()\n        # old pandas has numpy compat that sets this\n        np.seterr(all='ignore')\n    else:\n        self.old_err = None", "response": "Lives in zipline. __init__ for doctests."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlives in zipline. __init__ for doctests.", "response": "def teardown(self, np=np):\n    \"\"\"Lives in zipline.__init__ for doctests.\"\"\"\n\n    if self.old_err is not None:\n        np.seterr(**self.old_err)\n\n    if self.old_opts is not None:\n        np.set_printoptions(**self.old_opts)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hash_args(*args, **kwargs):\n    arg_string = '_'.join([str(arg) for arg in args])\n    kwarg_string = '_'.join([str(key) + '=' + str(value)\n                             for key, value in iteritems(kwargs)])\n    combined = ':'.join([arg_string, kwarg_string])\n\n    hasher = md5()\n    hasher.update(b(combined))\n    return hasher.hexdigest()", "response": "Define a unique string for any set of representable args."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assert_datasource_protocol(event):\n\n    assert event.type in DATASOURCE_TYPE\n\n    # Done packets have no dt.\n    if not event.type == DATASOURCE_TYPE.DONE:\n        assert isinstance(event.dt, datetime)\n        assert event.dt.tzinfo == pytz.utc", "response": "Assert that an event meets the protocol for datasource outputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assert_trade_protocol(event):\n    assert_datasource_protocol(event)\n\n    assert event.type == DATASOURCE_TYPE.TRADE\n    assert isinstance(event.price, numbers.Real)\n    assert isinstance(event.volume, numbers.Integral)\n    assert isinstance(event.dt, datetime)", "response": "Assert that an event meets the protocol for datasource TRADE outputs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef date_sorted_sources(*sources):\n    sorted_stream = heapq.merge(*(_decorate_source(s) for s in sources))\n\n    # Strip out key decoration\n    for _, message in sorted_stream:\n        yield message", "response": "Takes an iterable of sources generating namestrings and\n    piping their output into date_sort."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a daily trade source for each sids list.", "response": "def create_daily_trade_source(sids,\n                              sim_params,\n                              asset_finder,\n                              trading_calendar):\n    \"\"\"\n    creates trade_count trades for each sid in sids list.\n    first trade will be on sim_params.start_session, and daily\n    thereafter for each sid. Thus, two sids should result in two trades per\n    day.\n    \"\"\"\n    return create_trade_source(\n        sids,\n        timedelta(days=1),\n        sim_params,\n        asset_finder,\n        trading_calendar=trading_calendar,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_data_table(file,\n                    index_col,\n                    show_progress=False):\n    \"\"\" Load data table from zip file provided by Quandl.\n    \"\"\"\n    with ZipFile(file) as zip_file:\n        file_names = zip_file.namelist()\n        assert len(file_names) == 1, \"Expected a single file from Quandl.\"\n        wiki_prices = file_names.pop()\n        with zip_file.open(wiki_prices) as table_file:\n            if show_progress:\n                log.info('Parsing raw data.')\n            data_table = pd.read_csv(\n                table_file,\n                parse_dates=['date'],\n                index_col=index_col,\n                usecols=[\n                    'ticker',\n                    'date',\n                    'open',\n                    'high',\n                    'low',\n                    'close',\n                    'volume',\n                    'ex-dividend',\n                    'split_ratio',\n                ],\n            )\n\n    data_table.rename(\n        columns={\n            'ticker': 'symbol',\n            'ex-dividend': 'ex_dividend',\n        },\n        inplace=True,\n        copy=False,\n    )\n    return data_table", "response": "Load data table from a zip file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch_data_table(api_key,\n                     show_progress,\n                     retries):\n    \"\"\" Fetch WIKI Prices data table from Quandl\n    \"\"\"\n    for _ in range(retries):\n        try:\n            if show_progress:\n                log.info('Downloading WIKI metadata.')\n\n            metadata = pd.read_csv(\n                format_metadata_url(api_key)\n            )\n            # Extract link from metadata and download zip file.\n            table_url = metadata.loc[0, 'file.link']\n            if show_progress:\n                raw_file = download_with_progress(\n                    table_url,\n                    chunk_size=ONE_MEGABYTE,\n                    label=\"Downloading WIKI Prices table from Quandl\"\n                )\n            else:\n                raw_file = download_without_progress(table_url)\n\n            return load_data_table(\n                file=raw_file,\n                index_col=None,\n                show_progress=show_progress,\n            )\n\n        except Exception:\n            log.exception(\"Exception raised reading Quandl data. Retrying.\")\n\n    else:\n        raise ValueError(\n            \"Failed to download Quandl data after %d attempts.\" % (retries)\n        )", "response": "Download WIKI Prices data table from Quandl."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef quandl_bundle(environ,\n                  asset_db_writer,\n                  minute_bar_writer,\n                  daily_bar_writer,\n                  adjustment_writer,\n                  calendar,\n                  start_session,\n                  end_session,\n                  cache,\n                  show_progress,\n                  output_dir):\n    \"\"\"\n    quandl_bundle builds a daily dataset using Quandl's WIKI Prices dataset.\n\n    For more information on Quandl's API and how to obtain an API key,\n    please visit https://docs.quandl.com/docs#section-authentication\n    \"\"\"\n    api_key = environ.get('QUANDL_API_KEY')\n    if api_key is None:\n        raise ValueError(\n            \"Please set your QUANDL_API_KEY environment variable and retry.\"\n        )\n\n    raw_data = fetch_data_table(\n        api_key,\n        show_progress,\n        environ.get('QUANDL_DOWNLOAD_ATTEMPTS', 5)\n    )\n    asset_metadata = gen_asset_metadata(\n        raw_data[['symbol', 'date']],\n        show_progress\n    )\n    asset_db_writer.write(asset_metadata)\n\n    symbol_map = asset_metadata.symbol\n    sessions = calendar.sessions_in_range(start_session, end_session)\n\n    raw_data.set_index(['date', 'symbol'], inplace=True)\n    daily_bar_writer.write(\n        parse_pricing_and_vol(\n            raw_data,\n            sessions,\n            symbol_map\n        ),\n        show_progress=show_progress\n    )\n\n    raw_data.reset_index(inplace=True)\n    raw_data['symbol'] = raw_data['symbol'].astype('category')\n    raw_data['sid'] = raw_data.symbol.cat.codes\n    adjustment_writer.write(\n        splits=parse_splits(\n            raw_data[[\n                'sid',\n                'date',\n                'split_ratio',\n            ]].loc[raw_data.split_ratio != 1],\n            show_progress=show_progress\n        ),\n        dividends=parse_dividends(\n            raw_data[[\n                'sid',\n                'date',\n                'ex_dividend',\n            ]].loc[raw_data.ex_dividend != 0],\n            show_progress=show_progress\n        )\n    )", "response": "Builds a daily dataset using Quandl s WIKI Prices dataset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_with_progress(url, chunk_size, **progress_kwargs):\n    resp = requests.get(url, stream=True)\n    resp.raise_for_status()\n\n    total_size = int(resp.headers['content-length'])\n    data = BytesIO()\n    with progressbar(length=total_size, **progress_kwargs) as pbar:\n        for chunk in resp.iter_content(chunk_size=chunk_size):\n            data.write(chunk)\n            pbar.update(len(chunk))\n\n    data.seek(0)\n    return data", "response": "Download streaming data from a URL and print progress information to the terminal."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads data from a URL returning a BytesIO containing the downloaded data.", "response": "def download_without_progress(url):\n    \"\"\"\n    Download data from a URL, returning a BytesIO containing the loaded data.\n\n    Parameters\n    ----------\n    url : str\n        A URL that can be understood by ``requests.get``.\n\n    Returns\n    -------\n    data : BytesIO\n        A BytesIO containing the downloaded data.\n    \"\"\"\n    resp = requests.get(url)\n    resp.raise_for_status()\n    return BytesIO(resp.content)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef minute_to_session(column, close_locs, data, out):\n    if column == 'open':\n        _minute_to_session_open(close_locs, data, out)\n    elif column == 'high':\n        _minute_to_session_high(close_locs, data, out)\n    elif column == 'low':\n        _minute_to_session_low(close_locs, data, out)\n    elif column == 'close':\n        _minute_to_session_close(close_locs, data, out)\n    elif column == 'volume':\n        _minute_to_session_volume(close_locs, data, out)\n    return out", "response": "Resample an array with minute data into an array with session data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the smallest low seen between the market open and the current dt.", "response": "def lows(self, assets, dt):\n        \"\"\"\n        The low field's aggregation returns the smallest low seen between\n        the market open and the current dt.\n        If there has been no data on or before the `dt` the low is `nan`.\n\n        Returns\n        -------\n        np.array with dtype=float64, in order of assets parameter.\n        \"\"\"\n        market_open, prev_dt, dt_value, entries = self._prelude(dt, 'low')\n\n        lows = []\n        session_label = self._trading_calendar.minute_to_session_label(dt)\n\n        for asset in assets:\n            if not asset.is_alive_for_session(session_label):\n                lows.append(np.NaN)\n                continue\n\n            if prev_dt is None:\n                val = self._minute_reader.get_value(asset, dt, 'low')\n                entries[asset] = (dt_value, val)\n                lows.append(val)\n                continue\n            else:\n                try:\n                    last_visited_dt, last_min = entries[asset]\n                    if last_visited_dt == dt_value:\n                        lows.append(last_min)\n                        continue\n                    elif last_visited_dt == prev_dt:\n                        curr_val = self._minute_reader.get_value(\n                            asset, dt, 'low')\n                        val = np.nanmin([last_min, curr_val])\n                        entries[asset] = (dt_value, val)\n                        lows.append(val)\n                        continue\n                    else:\n                        after_last = pd.Timestamp(\n                            last_visited_dt + self._one_min, tz='UTC')\n                        window = self._minute_reader.load_raw_arrays(\n                            ['low'],\n                            after_last,\n                            dt,\n                            [asset],\n                        )[0].T\n                        val = np.nanmin(np.append(window, last_min))\n                        entries[asset] = (dt_value, val)\n                        lows.append(val)\n                        continue\n                except KeyError:\n                    window = self._minute_reader.load_raw_arrays(\n                        ['low'],\n                        market_open,\n                        dt,\n                        [asset],\n                    )[0].T\n                    val = np.nanmin(window)\n                    entries[asset] = (dt_value, val)\n                    lows.append(val)\n                    continue\n        return np.array(lows)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef closes(self, assets, dt):\n        market_open, prev_dt, dt_value, entries = self._prelude(dt, 'close')\n\n        closes = []\n        session_label = self._trading_calendar.minute_to_session_label(dt)\n\n        def _get_filled_close(asset):\n            \"\"\"\n            Returns the most recent non-nan close for the asset in this\n            session. If there has been no data in this session on or before the\n            `dt`, returns `nan`\n            \"\"\"\n            window = self._minute_reader.load_raw_arrays(\n                ['close'],\n                market_open,\n                dt,\n                [asset],\n            )[0]\n            try:\n                return window[~np.isnan(window)][-1]\n            except IndexError:\n                return np.NaN\n\n        for asset in assets:\n            if not asset.is_alive_for_session(session_label):\n                closes.append(np.NaN)\n                continue\n\n            if prev_dt is None:\n                val = self._minute_reader.get_value(asset, dt, 'close')\n                entries[asset] = (dt_value, val)\n                closes.append(val)\n                continue\n            else:\n                try:\n                    last_visited_dt, last_close = entries[asset]\n                    if last_visited_dt == dt_value:\n                        closes.append(last_close)\n                        continue\n                    elif last_visited_dt == prev_dt:\n                        val = self._minute_reader.get_value(\n                            asset, dt, 'close')\n                        if pd.isnull(val):\n                            val = last_close\n                        entries[asset] = (dt_value, val)\n                        closes.append(val)\n                        continue\n                    else:\n                        val = self._minute_reader.get_value(\n                            asset, dt, 'close')\n                        if pd.isnull(val):\n                            val = _get_filled_close(asset)\n                        entries[asset] = (dt_value, val)\n                        closes.append(val)\n                        continue\n                except KeyError:\n                    val = self._minute_reader.get_value(\n                        asset, dt, 'close')\n                    if pd.isnull(val):\n                        val = _get_filled_close(asset)\n                    entries[asset] = (dt_value, val)\n                    closes.append(val)\n                    continue\n        return np.array(closes)", "response": "Returns the latest close at the given dt."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_domain(terms):\n    domains = {t.domain for t in terms}\n    num_domains = len(domains)\n\n    if num_domains == 0:\n        return GENERIC\n    elif num_domains == 1:\n        return domains.pop()\n    elif num_domains == 2 and GENERIC in domains:\n        domains.remove(GENERIC)\n        return domains.pop()\n    else:\n        # Remove GENERIC if it's present before raising. Showing it to the user\n        # is confusing because it doesn't contribute to the error.\n        domains.discard(GENERIC)\n        raise AmbiguousDomain(sorted(domains, key=repr))", "response": "Infer the domain from a collection of terms."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrolls a date to the calendar of the pipeline s domain.", "response": "def roll_forward(self, dt):\n        \"\"\"\n        Given a date, align it to the calendar of the pipeline's domain.\n\n        Parameters\n        ----------\n        dt : pd.Timestamp\n\n        Returns\n        -------\n        pd.Timestamp\n        \"\"\"\n        dt = pd.Timestamp(dt, tz='UTC')\n\n        trading_days = self.all_sessions()\n        try:\n            return trading_days[trading_days.searchsorted(dt)]\n        except IndexError:\n            raise ValueError(\n                \"Date {} was past the last session for domain {}. \"\n                \"The last session for this domain is {}.\".format(\n                    dt.date(),\n                    self,\n                    trading_days[-1].date()\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the date index and sid columns shared by a list of dataframes.", "response": "def days_and_sids_for_frames(frames):\n    \"\"\"\n    Returns the date index and sid columns shared by a list of dataframes,\n    ensuring they all match.\n\n    Parameters\n    ----------\n    frames : list[pd.DataFrame]\n        A list of dataframes indexed by day, with a column per sid.\n\n    Returns\n    -------\n    days : np.array[datetime64[ns]]\n        The days in these dataframes.\n    sids : np.array[int64]\n        The sids in these dataframes.\n\n    Raises\n    ------\n    ValueError\n        If the dataframes passed are not all indexed by the same days\n        and sids.\n    \"\"\"\n    if not frames:\n        days = np.array([], dtype='datetime64[ns]')\n        sids = np.array([], dtype='int64')\n        return days, sids\n\n    # Ensure the indices and columns all match.\n    check_indexes_all_same(\n        [frame.index for frame in frames],\n        message='Frames have mistmatched days.',\n    )\n    check_indexes_all_same(\n        [frame.columns for frame in frames],\n        message='Frames have mismatched sids.',\n    )\n\n    return frames[0].index.values, frames[0].columns.values"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the start and end dates of the asset tables for the given OHLCV fields.", "response": "def compute_asset_lifetimes(frames):\n    \"\"\"\n    Parameters\n    ----------\n    frames : dict[str, pd.DataFrame]\n        A dict mapping each OHLCV field to a dataframe with a row for\n        each date and a column for each sid, as passed to write().\n\n    Returns\n    -------\n    start_date_ixs : np.array[int64]\n        The index of the first date with non-nan values, for each sid.\n    end_date_ixs : np.array[int64]\n        The index of the last date with non-nan values, for each sid.\n    \"\"\"\n    # Build a 2D array (dates x sids), where an entry is True if all\n    # fields are nan for the given day and sid.\n    is_null_matrix = np.logical_and.reduce(\n        [frames[field].isnull().values for field in FIELDS],\n    )\n    if not is_null_matrix.size:\n        empty = np.array([], dtype='int64')\n        return empty, empty.copy()\n\n    # Offset of the first null from the start of the input.\n    start_date_ixs = is_null_matrix.argmin(axis=0)\n    # Offset of the last null from the **end** of the input.\n    end_offsets = is_null_matrix[::-1].argmin(axis=0)\n    # Offset of the last null from the start of the input\n    end_date_ixs = is_null_matrix.shape[0] - end_offsets - 1\n\n    return start_date_ixs, end_date_ixs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the OHLCV data for one country to the HDF5 file.", "response": "def write(self, country_code, frames, scaling_factors=None):\n        \"\"\"Write the OHLCV data for one country to the HDF5 file.\n\n        Parameters\n        ----------\n        country_code : str\n            The ISO 3166 alpha-2 country code for this country.\n        frames : dict[str, pd.DataFrame]\n            A dict mapping each OHLCV field to a dataframe with a row\n            for each date and a column for each sid. The dataframes need\n            to have the same index and columns.\n        scaling_factors : dict[str, float], optional\n            A dict mapping each OHLCV field to a scaling factor, which\n            is applied (as a multiplier) to the values of field to\n            efficiently store them as uint32, while maintaining desired\n            precision. These factors are written to the file as metadata,\n            which is consumed by the reader to adjust back to the original\n            float values. Default is None, in which case\n            DEFAULT_SCALING_FACTORS is used.\n        \"\"\"\n        if scaling_factors is None:\n            scaling_factors = DEFAULT_SCALING_FACTORS\n\n        with self.h5_file(mode='a') as h5_file:\n            # ensure that the file version has been written\n            h5_file.attrs['version'] = VERSION\n\n            country_group = h5_file.create_group(country_code)\n\n            data_group = country_group.create_group(DATA)\n            index_group = country_group.create_group(INDEX)\n            lifetimes_group = country_group.create_group(LIFETIMES)\n\n            # Note that this functions validates that all of the frames\n            # share the same days and sids.\n            days, sids = days_and_sids_for_frames(list(frames.values()))\n\n            # Write sid and date indices.\n            index_group.create_dataset(SID, data=sids)\n\n            # h5py does not support datetimes, so they need to be stored\n            # as integers.\n            index_group.create_dataset(DAY, data=days.astype(np.int64))\n\n            log.debug(\n                'Wrote {} group to file {}',\n                index_group.name,\n                self._filename,\n            )\n\n            # Write start and end dates for each sid.\n            start_date_ixs, end_date_ixs = compute_asset_lifetimes(frames)\n\n            lifetimes_group.create_dataset(START_DATE, data=start_date_ixs)\n            lifetimes_group.create_dataset(END_DATE, data=end_date_ixs)\n\n            if len(sids):\n                chunks = (len(sids), min(self._date_chunk_size, len(days)))\n            else:\n                # h5py crashes if we provide chunks for empty data.\n                chunks = None\n\n            for field in FIELDS:\n                frame = frames[field]\n\n                # Sort rows by increasing sid, and columns by increasing date.\n                frame.sort_index(inplace=True)\n                frame.sort_index(axis='columns', inplace=True)\n\n                data = coerce_to_uint32(\n                    frame.T.fillna(0).values,\n                    scaling_factors[field],\n                )\n\n                dataset = data_group.create_dataset(\n                    field,\n                    compression='lzf',\n                    shuffle=True,\n                    data=data,\n                    chunks=chunks,\n                )\n\n                dataset.attrs[SCALING_FACTOR] = scaling_factors[field]\n\n                log.debug(\n                    'Writing dataset {} to file {}',\n                    dataset.name, self._filename\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_from_sid_df_pairs(self,\n                                country_code,\n                                data,\n                                scaling_factors=None):\n        \"\"\"\n        Parameters\n        ----------\n        country_code : str\n            The ISO 3166 alpha-2 country code for this country.\n        data : iterable[tuple[int, pandas.DataFrame]]\n            The data chunks to write. Each chunk should be a tuple of\n            sid and the data for that asset.\n        scaling_factors : dict[str, float], optional\n            A dict mapping each OHLCV field to a scaling factor, which\n            is applied (as a multiplier) to the values of field to\n            efficiently store them as uint32, while maintaining desired\n            precision. These factors are written to the file as metadata,\n            which is consumed by the reader to adjust back to the original\n            float values. Default is None, in which case\n            DEFAULT_SCALING_FACTORS is used.\n        \"\"\"\n        data = list(data)\n        if not data:\n            empty_frame = pd.DataFrame(\n                data=None,\n                index=np.array([], dtype='datetime64[ns]'),\n                columns=np.array([], dtype='int64'),\n            )\n            return self.write(\n                country_code,\n                {f: empty_frame.copy() for f in FIELDS},\n                scaling_factors,\n            )\n\n        sids, frames = zip(*data)\n        ohlcv_frame = pd.concat(frames)\n\n        # Repeat each sid for each row in its corresponding frame.\n        sid_ix = np.repeat(sids, [len(f) for f in frames])\n\n        # Add id to the index, so the frame is indexed by (date, id).\n        ohlcv_frame.set_index(sid_ix, append=True, inplace=True)\n\n        frames = {\n            field: ohlcv_frame[field].unstack()\n            for field in FIELDS\n        }\n\n        return self.write(country_code, frames, scaling_factors)", "response": "Writes the data to the ISO 3166 - 2 file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_file(cls, h5_file, country_code):\n        if h5_file.attrs['version'] != VERSION:\n            raise ValueError(\n                'mismatched version: file is of version %s, expected %s' % (\n                    h5_file.attrs['version'],\n                    VERSION,\n                ),\n            )\n\n        return cls(h5_file[country_code])", "response": "Construct a new object from an HDF5 daily pricing file and a country code."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_path(cls, path, country_code):\n        return cls.from_file(h5py.File(path), country_code)", "response": "Construct a new Australian Australia object from a file path and a country code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the raw data from the h5 file into a list of np. ndarrays.", "response": "def load_raw_arrays(self,\n                        columns,\n                        start_date,\n                        end_date,\n                        assets):\n        \"\"\"\n        Parameters\n        ----------\n        columns : list of str\n           'open', 'high', 'low', 'close', or 'volume'\n        start_date: Timestamp\n           Beginning of the window range.\n        end_date: Timestamp\n           End of the window range.\n        assets : list of int\n           The asset identifiers in the window.\n\n        Returns\n        -------\n        list of np.ndarray\n            A list with an entry per field of ndarrays with shape\n            (minutes in range, sids) with a dtype of float64, containing the\n            values for the respective field over start and end dt range.\n        \"\"\"\n        self._validate_timestamp(start_date)\n        self._validate_timestamp(end_date)\n\n        start = start_date.asm8\n        end = end_date.asm8\n        date_slice = self._compute_date_range_slice(start, end)\n        n_dates = date_slice.stop - date_slice.start\n\n        # Create a buffer into which we'll read data from the h5 file.\n        # Allocate an extra row of space that will always contain null values.\n        # We'll use that space to provide \"data\" for entries in ``assets`` that\n        # are unknown to us.\n        full_buf = np.zeros((len(self.sids) + 1, n_dates), dtype=np.uint32)\n        # We'll only read values into this portion of the read buf.\n        mutable_buf = full_buf[:-1]\n\n        # Indexer that converts an array aligned to self.sids (which is what we\n        # pull from the h5 file) into an array aligned to ``assets``.\n        #\n        # Unknown assets will have an index of -1, which means they'll always\n        # pull from the last row of the read buffer. We allocated an extra\n        # empty row above so that these lookups will cause us to fill our\n        # output buffer with \"null\" values.\n        sid_selector = self._make_sid_selector(assets)\n\n        out = []\n        for column in columns:\n            # Zero the buffer to prepare to receive new data.\n            mutable_buf.fill(0)\n\n            dataset = self._country_group[DATA][column]\n\n            # Fill the mutable portion of our buffer with data from the file.\n            dataset.read_direct(\n                mutable_buf,\n                np.s_[:, date_slice],\n            )\n\n            # Select data from the **full buffer**. Unknown assets will pull\n            # from the last row, which is always empty.\n            out.append(self._postprocessors[column](full_buf[sid_selector].T))\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_sid_selector(self, assets):\n        assets = np.array(assets)\n        sid_selector = self.sids.searchsorted(assets)\n        unknown = np.in1d(assets, self.sids, invert=True)\n        sid_selector[unknown] = -1\n        return sid_selector", "response": "Build an indexer mapping self. sids to assets."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_assets(self, assets):\n        missing_sids = np.setdiff1d(assets, self.sids)\n\n        if len(missing_sids):\n            raise NoDataForSid(\n                'Assets not contained in daily pricing file: {}'.format(\n                    missing_sids\n                )\n            )", "response": "Validate that the provided asset identifiers are contained in the daily bars."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_value(self, sid, dt, field):\n        self._validate_assets([sid])\n        self._validate_timestamp(dt)\n\n        sid_ix = self.sids.searchsorted(sid)\n        dt_ix = self.dates.searchsorted(dt.asm8)\n\n        value = self._postprocessors[field](\n            self._country_group[DATA][field][sid_ix, dt_ix]\n        )\n\n        # When the value is nan, this dt may be outside the asset's lifetime.\n        # If that's the case, the proper NoDataOnDate exception is raised.\n        # Otherwise (when there's just a hole in the middle of the data), the\n        # nan is returned.\n        if np.isnan(value):\n            if dt.asm8 < self.asset_start_dates[sid_ix]:\n                raise NoDataBeforeDate()\n\n            if dt.asm8 > self.asset_end_dates[sid_ix]:\n                raise NoDataAfterDate()\n\n        return value", "response": "Retrieves the value at the given coordinates."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_last_traded_dt(self, asset, dt):\n        sid_ix = self.sids.searchsorted(asset.sid)\n        # Used to get a slice of all dates up to and including ``dt``.\n        dt_limit_ix = self.dates.searchsorted(dt.asm8, side='right')\n\n        # Get the indices of all dates with nonzero volume.\n        nonzero_volume_ixs = np.ravel(\n            np.nonzero(self._country_group[DATA][VOLUME][sid_ix, :dt_limit_ix])\n        )\n\n        if len(nonzero_volume_ixs) == 0:\n            return pd.NaT\n\n        return pd.Timestamp(self.dates[nonzero_volume_ixs][-1], tz='UTC')", "response": "Get the latest day on or before dt in which asset traded."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing a new object from an HDF5 daily pricing file.", "response": "def from_file(cls, h5_file):\n        \"\"\"\n        Construct from an h5py.File.\n\n        Parameters\n        ----------\n        h5_file : h5py.File\n            An HDF5 daily pricing file.\n        \"\"\"\n        return cls({\n            country: HDF5DailyBarReader.from_file(h5_file, country)\n            for country in h5_file.keys()\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the raw arrays of the specified columns from the specified start and end dates.", "response": "def load_raw_arrays(self,\n                        columns,\n                        start_date,\n                        end_date,\n                        assets):\n        \"\"\"\n        Parameters\n        ----------\n        columns : list of str\n           'open', 'high', 'low', 'close', or 'volume'\n        start_date: Timestamp\n           Beginning of the window range.\n        end_date: Timestamp\n           End of the window range.\n        assets : list of int\n           The asset identifiers in the window.\n\n        Returns\n        -------\n        list of np.ndarray\n            A list with an entry per field of ndarrays with shape\n            (minutes in range, sids) with a dtype of float64, containing the\n            values for the respective field over start and end dt range.\n        \"\"\"\n        country_code = self._country_code_for_assets(assets)\n\n        return self._readers[country_code].load_raw_arrays(\n            columns,\n            start_date,\n            end_date,\n            assets,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the datetime of the last set of session labels that are in the current set of assets.", "response": "def sessions(self):\n        \"\"\"\n        Returns\n        -------\n        sessions : DatetimeIndex\n           All session labels (unioning the range for all assets) which the\n           reader can provide.\n        \"\"\"\n        return pd.to_datetime(\n            reduce(\n                np.union1d,\n                (reader.dates for reader in self._readers.values()),\n            ),\n            utc=True,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_value(self, sid, dt, field):\n        try:\n            country_code = self._country_code_for_assets([sid])\n        except ValueError as exc:\n            raise_from(\n                NoDataForSid(\n                    'Asset not contained in daily pricing file: {}'.format(sid)\n                ),\n                exc\n            )\n        return self._readers[country_code].get_value(sid, dt, field)", "response": "Retrieves the value for the given data point at the given coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the latest day on or before dt in which the asset traded.", "response": "def get_last_traded_dt(self, asset, dt):\n        \"\"\"\n        Get the latest day on or before ``dt`` in which ``asset`` traded.\n\n        If there are no trades on or before ``dt``, returns ``pd.NaT``.\n\n        Parameters\n        ----------\n        asset : zipline.asset.Asset\n            The asset for which to get the last traded day.\n        dt : pd.Timestamp\n            The dt at which to start searching for the last traded day.\n\n        Returns\n        -------\n        last_traded : pd.Timestamp\n            The day of the last trade for the given asset, using the\n            input dt as a vantage point.\n        \"\"\"\n        country_code = self._country_code_for_assets([asset.sid])\n        return self._readers[country_code].get_last_traded_dt(asset, dt)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _normalize_index_columns_in_place(equities,\n                                      equity_supplementary_mappings,\n                                      futures,\n                                      exchanges,\n                                      root_symbols):\n    \"\"\"\n    Update dataframes in place to set indentifier columns as indices.\n\n    For each input frame, if the frame has a column with the same name as its\n    associated index column, set that column as the index.\n\n    Otherwise, assume the index already contains identifiers.\n\n    If frames are passed as None, they're ignored.\n    \"\"\"\n    for frame, column_name in ((equities, 'sid'),\n                               (equity_supplementary_mappings, 'sid'),\n                               (futures, 'sid'),\n                               (exchanges, 'exchange'),\n                               (root_symbols, 'root_symbol')):\n        if frame is not None and column_name in frame:\n            frame.set_index(column_name, inplace=True)", "response": "Update dataframes in place to set indentifier columns as indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_delimited_symbol(symbol):\n    # return blank strings for any bad fuzzy symbols, like NaN or None\n    if symbol in _delimited_symbol_default_triggers:\n        return '', ''\n\n    symbol = symbol.upper()\n\n    split_list = re.split(\n        pattern=_delimited_symbol_delimiters_regex,\n        string=symbol,\n        maxsplit=1,\n    )\n\n    # Break the list up in to its two components, the company symbol and the\n    # share class symbol\n    company_symbol = split_list[0]\n    if len(split_list) > 1:\n        share_class_symbol = split_list[1]\n    else:\n        share_class_symbol = ''\n\n    return company_symbol, share_class_symbol", "response": "Splits a symbol that may be delimited and returns the company symbol and share class symbol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating an output dataframe from the given subset of user - provided asset - type data and the given column names and default values.", "response": "def _generate_output_dataframe(data_subset, defaults):\n    \"\"\"\n    Generates an output dataframe from the given subset of user-provided\n    data, the given column names, and the given default values.\n\n    Parameters\n    ----------\n    data_subset : DataFrame\n        A DataFrame, usually from an AssetData object,\n        that contains the user's input metadata for the asset type being\n        processed\n    defaults : dict\n        A dict where the keys are the names of the columns of the desired\n        output DataFrame and the values are a function from dataframe and\n        column name to the default values to insert in the DataFrame if no user\n        data is provided\n\n    Returns\n    -------\n    DataFrame\n        A DataFrame containing all user-provided metadata, and default values\n        wherever user-provided metadata was missing\n    \"\"\"\n    # The columns provided.\n    cols = set(data_subset.columns)\n    desired_cols = set(defaults)\n\n    # Drop columns with unrecognised headers.\n    data_subset.drop(cols - desired_cols,\n                     axis=1,\n                     inplace=True)\n\n    # Get those columns which we need but\n    # for which no data has been supplied.\n    for col in desired_cols - cols:\n        # write the default value for any missing columns\n        data_subset[col] = defaults[col](data_subset, col)\n\n    return data_subset"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that there are no cases where multiple assets held the same atom.", "response": "def _check_symbol_mappings(df, exchanges, asset_exchange):\n    \"\"\"Check that there are no cases where multiple symbols resolve to the same\n    asset at the same time in the same country.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The equity symbol mappings table.\n    exchanges : pd.DataFrame\n        The exchanges table.\n    asset_exchange : pd.Series\n        A series that maps sids to the exchange the asset is in.\n\n    Raises\n    ------\n    ValueError\n        Raised when there are ambiguous symbol mappings.\n    \"\"\"\n    mappings = df.set_index('sid')[list(mapping_columns)].copy()\n    mappings['country_code'] = exchanges['country_code'][\n        asset_exchange.loc[df['sid']]\n    ].values\n    ambigious = {}\n\n    def check_intersections(persymbol):\n        intersections = list(intersecting_ranges(map(\n            from_tuple,\n            zip(persymbol.start_date, persymbol.end_date),\n        )))\n        if intersections:\n            data = persymbol[\n                ['start_date', 'end_date']\n            ].astype('datetime64[ns]')\n            # indent the dataframe string, also compute this early because\n            # ``persymbol`` is a view and ``astype`` doesn't copy the index\n            # correctly in pandas 0.22\n            msg_component = '\\n  '.join(str(data).splitlines())\n            ambigious[persymbol.name] = intersections, msg_component\n\n    mappings.groupby(['symbol', 'country_code']).apply(check_intersections)\n\n    if ambigious:\n        raise ValueError(\n            'Ambiguous ownership for %d symbol%s, multiple assets held the'\n            ' following symbols:\\n%s' % (\n                len(ambigious),\n                '' if len(ambigious) == 1 else 's',\n                '\\n'.join(\n                    '%s (%s):\\n  intersections: %s\\n  %s' % (\n                        symbol,\n                        country_code,\n                        tuple(map(_format_range, intersections)),\n                        cs,\n                    )\n                    for (symbol, country_code), (intersections, cs) in sorted(\n                        ambigious.items(),\n                        key=first,\n                    ),\n                ),\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _split_symbol_mappings(df, exchanges):\n    mappings = df[list(mapping_columns)]\n    with pd.option_context('mode.chained_assignment', None):\n        mappings['sid'] = mappings.index\n    mappings.reset_index(drop=True, inplace=True)\n\n    # take the most recent sid->exchange mapping based on end date\n    asset_exchange = df[\n        ['exchange', 'end_date']\n    ].sort_values('end_date').groupby(level=0)['exchange'].nth(-1)\n\n    _check_symbol_mappings(mappings, exchanges, asset_exchange)\n    return (\n        df.groupby(level=0).apply(_check_asset_group),\n        mappings,\n    )", "response": "Split out the symbol - sid mappings from the raw data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a timeseries into an Int64Index of nanoseconds since the epoch.", "response": "def _dt_to_epoch_ns(dt_series):\n    \"\"\"Convert a timeseries into an Int64Index of nanoseconds since the epoch.\n\n    Parameters\n    ----------\n    dt_series : pd.Series\n        The timeseries to convert.\n\n    Returns\n    -------\n    idx : pd.Int64Index\n        The index converted to nanoseconds since the epoch.\n    \"\"\"\n    index = pd.to_datetime(dt_series.values)\n    if index.tzinfo is None:\n        index = index.tz_localize('UTC')\n    else:\n        index = index.tz_convert('UTC')\n    return index.view(np.int64)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks for a version value in the version table.", "response": "def check_version_info(conn, version_table, expected_version):\n    \"\"\"\n    Checks for a version value in the version table.\n\n    Parameters\n    ----------\n    conn : sa.Connection\n        The connection to use to perform the check.\n    version_table : sa.Table\n        The version table of the asset database\n    expected_version : int\n        The expected version of the asset database\n\n    Raises\n    ------\n    AssetDBVersionError\n        If the version is in the table and not equal to ASSET_DB_VERSION.\n    \"\"\"\n\n    # Read the version out of the table\n    version_from_table = conn.execute(\n        sa.select((version_table.c.version,)),\n    ).scalar()\n\n    # A db without a version is considered v0\n    if version_from_table is None:\n        version_from_table = 0\n\n    # Raise an error if the versions do not match\n    if (version_from_table != expected_version):\n        raise AssetDBVersionError(db_version=version_from_table,\n                                  expected_version=expected_version)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_version_info(conn, version_table, version_value):\n    conn.execute(sa.insert(version_table, values={'version': version_value}))", "response": "Writes the version value in to the version table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_direct(self,\n                     equities=None,\n                     equity_symbol_mappings=None,\n                     equity_supplementary_mappings=None,\n                     futures=None,\n                     exchanges=None,\n                     root_symbols=None,\n                     chunk_size=DEFAULT_CHUNK_SIZE):\n        \"\"\"Write asset metadata to a sqlite database in the format that it is\n        stored in the assets db.\n\n        Parameters\n        ----------\n        equities : pd.DataFrame, optional\n            The equity metadata. The columns for this dataframe are:\n\n              symbol : str\n                  The ticker symbol for this equity.\n              asset_name : str\n                  The full name for this asset.\n              start_date : datetime\n                  The date when this asset was created.\n              end_date : datetime, optional\n                  The last date we have trade data for this asset.\n              first_traded : datetime, optional\n                  The first date we have trade data for this asset.\n              auto_close_date : datetime, optional\n                  The date on which to close any positions in this asset.\n              exchange : str\n                  The exchange where this asset is traded.\n\n            The index of this dataframe should contain the sids.\n        futures : pd.DataFrame, optional\n            The future contract metadata. The columns for this dataframe are:\n\n              symbol : str\n                  The ticker symbol for this futures contract.\n              root_symbol : str\n                  The root symbol, or the symbol with the expiration stripped\n                  out.\n              asset_name : str\n                  The full name for this asset.\n              start_date : datetime, optional\n                  The date when this asset was created.\n              end_date : datetime, optional\n                  The last date we have trade data for this asset.\n              first_traded : datetime, optional\n                  The first date we have trade data for this asset.\n              exchange : str\n                  The exchange where this asset is traded.\n              notice_date : datetime\n                  The date when the owner of the contract may be forced\n                  to take physical delivery of the contract's asset.\n              expiration_date : datetime\n                  The date when the contract expires.\n              auto_close_date : datetime\n                  The date when the broker will automatically close any\n                  positions in this contract.\n              tick_size : float\n                  The minimum price movement of the contract.\n              multiplier: float\n                  The amount of the underlying asset represented by this\n                  contract.\n        exchanges : pd.DataFrame, optional\n            The exchanges where assets can be traded. The columns of this\n            dataframe are:\n\n              exchange : str\n                  The full name of the exchange.\n              canonical_name : str\n                  The canonical name of the exchange.\n              country_code : str\n                  The ISO 3166 alpha-2 country code of the exchange.\n        root_symbols : pd.DataFrame, optional\n            The root symbols for the futures contracts. The columns for this\n            dataframe are:\n\n              root_symbol : str\n                  The root symbol name.\n              root_symbol_id : int\n                  The unique id for this root symbol.\n              sector : string, optional\n                  The sector of this root symbol.\n              description : string, optional\n                  A short description of this root symbol.\n              exchange : str\n                  The exchange where this root symbol is traded.\n        equity_supplementary_mappings : pd.DataFrame, optional\n            Additional mappings from values of abitrary type to assets.\n        chunk_size : int, optional\n            The amount of rows to write to the SQLite table at once.\n            This defaults to the default number of bind params in sqlite.\n            If you have compiled sqlite3 with more bind or less params you may\n            want to pass that value here.\n\n        \"\"\"\n        if equities is not None:\n            equities = _generate_output_dataframe(\n                equities,\n                _direct_equities_defaults,\n            )\n            if equity_symbol_mappings is None:\n                raise ValueError(\n                    'equities provided with no symbol mapping data',\n                )\n\n            equity_symbol_mappings = _generate_output_dataframe(\n                equity_symbol_mappings,\n                _equity_symbol_mappings_defaults,\n            )\n            _check_symbol_mappings(\n                equity_symbol_mappings,\n                exchanges,\n                equities['exchange'],\n            )\n\n        if equity_supplementary_mappings is not None:\n            equity_supplementary_mappings = _generate_output_dataframe(\n                equity_supplementary_mappings,\n                _equity_supplementary_mappings_defaults,\n            )\n\n        if futures is not None:\n            futures = _generate_output_dataframe(_futures_defaults, futures)\n\n        if exchanges is not None:\n            exchanges = _generate_output_dataframe(\n                exchanges.set_index('exchange'),\n                _exchanges_defaults,\n            )\n\n        if root_symbols is not None:\n            root_symbols = _generate_output_dataframe(\n                root_symbols,\n                _root_symbols_defaults,\n            )\n\n        # Set named identifier columns as indices, if provided.\n        _normalize_index_columns_in_place(\n            equities=equities,\n            equity_supplementary_mappings=equity_supplementary_mappings,\n            futures=futures,\n            exchanges=exchanges,\n            root_symbols=root_symbols,\n        )\n\n        self._real_write(\n            equities=equities,\n            equity_symbol_mappings=equity_symbol_mappings,\n            equity_supplementary_mappings=equity_supplementary_mappings,\n            futures=futures,\n            exchanges=exchanges,\n            root_symbols=root_symbols,\n            chunk_size=chunk_size,\n        )", "response": "This function writes the asset metadata to a sqlite database in the format that it is stored in the assets db."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite asset metadata to a sqlite database.", "response": "def write(self,\n              equities=None,\n              futures=None,\n              exchanges=None,\n              root_symbols=None,\n              equity_supplementary_mappings=None,\n              chunk_size=DEFAULT_CHUNK_SIZE):\n        \"\"\"Write asset metadata to a sqlite database.\n\n        Parameters\n        ----------\n        equities : pd.DataFrame, optional\n            The equity metadata. The columns for this dataframe are:\n\n              symbol : str\n                  The ticker symbol for this equity.\n              asset_name : str\n                  The full name for this asset.\n              start_date : datetime\n                  The date when this asset was created.\n              end_date : datetime, optional\n                  The last date we have trade data for this asset.\n              first_traded : datetime, optional\n                  The first date we have trade data for this asset.\n              auto_close_date : datetime, optional\n                  The date on which to close any positions in this asset.\n              exchange : str\n                  The exchange where this asset is traded.\n\n            The index of this dataframe should contain the sids.\n        futures : pd.DataFrame, optional\n            The future contract metadata. The columns for this dataframe are:\n\n              symbol : str\n                  The ticker symbol for this futures contract.\n              root_symbol : str\n                  The root symbol, or the symbol with the expiration stripped\n                  out.\n              asset_name : str\n                  The full name for this asset.\n              start_date : datetime, optional\n                  The date when this asset was created.\n              end_date : datetime, optional\n                  The last date we have trade data for this asset.\n              first_traded : datetime, optional\n                  The first date we have trade data for this asset.\n              exchange : str\n                  The exchange where this asset is traded.\n              notice_date : datetime\n                  The date when the owner of the contract may be forced\n                  to take physical delivery of the contract's asset.\n              expiration_date : datetime\n                  The date when the contract expires.\n              auto_close_date : datetime\n                  The date when the broker will automatically close any\n                  positions in this contract.\n              tick_size : float\n                  The minimum price movement of the contract.\n              multiplier: float\n                  The amount of the underlying asset represented by this\n                  contract.\n        exchanges : pd.DataFrame, optional\n            The exchanges where assets can be traded. The columns of this\n            dataframe are:\n\n              exchange : str\n                  The full name of the exchange.\n              canonical_name : str\n                  The canonical name of the exchange.\n              country_code : str\n                  The ISO 3166 alpha-2 country code of the exchange.\n        root_symbols : pd.DataFrame, optional\n            The root symbols for the futures contracts. The columns for this\n            dataframe are:\n\n              root_symbol : str\n                  The root symbol name.\n              root_symbol_id : int\n                  The unique id for this root symbol.\n              sector : string, optional\n                  The sector of this root symbol.\n              description : string, optional\n                  A short description of this root symbol.\n              exchange : str\n                  The exchange where this root symbol is traded.\n        equity_supplementary_mappings : pd.DataFrame, optional\n            Additional mappings from values of abitrary type to assets.\n        chunk_size : int, optional\n            The amount of rows to write to the SQLite table at once.\n            This defaults to the default number of bind params in sqlite.\n            If you have compiled sqlite3 with more bind or less params you may\n            want to pass that value here.\n\n        See Also\n        --------\n        zipline.assets.asset_finder\n        \"\"\"\n        if exchanges is None:\n            exchange_names = [\n                df['exchange']\n                for df in (equities, futures, root_symbols)\n                if df is not None\n            ]\n            if exchange_names:\n                exchanges = pd.DataFrame({\n                    'exchange': pd.concat(exchange_names).unique(),\n                })\n\n        data = self._load_data(\n            equities if equities is not None else pd.DataFrame(),\n            futures if futures is not None else pd.DataFrame(),\n            exchanges if exchanges is not None else pd.DataFrame(),\n            root_symbols if root_symbols is not None else pd.DataFrame(),\n            (\n                equity_supplementary_mappings\n                if equity_supplementary_mappings is not None\n                else pd.DataFrame()\n            ),\n        )\n        self._real_write(\n            equities=data.equities,\n            equity_symbol_mappings=data.equities_mappings,\n            equity_supplementary_mappings=data.equity_supplementary_mappings,\n            futures=data.futures,\n            root_symbols=data.root_symbols,\n            exchanges=data.exchanges,\n            chunk_size=chunk_size,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if any tables are present in the current assets database.", "response": "def _all_tables_present(self, txn):\n        \"\"\"\n        Checks if any tables are present in the current assets database.\n\n        Parameters\n        ----------\n        txn : Transaction\n            The open transaction to check in.\n\n        Returns\n        -------\n        has_tables : bool\n            True if any tables are present, otherwise False.\n        \"\"\"\n        conn = txn.connect()\n        for table_name in asset_db_table_names:\n            if txn.dialect.has_table(conn, table_name):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_db(self, txn=None):\n        with ExitStack() as stack:\n            if txn is None:\n                txn = stack.enter_context(self.engine.begin())\n\n            tables_already_exist = self._all_tables_present(txn)\n\n            # Create the SQL tables if they do not already exist.\n            metadata.create_all(txn, checkfirst=True)\n\n            if tables_already_exist:\n                check_version_info(txn, version_info, ASSET_DB_VERSION)\n            else:\n                write_version_info(txn, version_info, ASSET_DB_VERSION)", "response": "Connect to database and create tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_data(self,\n                   equities,\n                   futures,\n                   exchanges,\n                   root_symbols,\n                   equity_supplementary_mappings):\n        \"\"\"\n        Returns a standard set of pandas.DataFrames:\n        equities, futures, exchanges, root_symbols\n        \"\"\"\n        # Set named identifier columns as indices, if provided.\n        _normalize_index_columns_in_place(\n            equities=equities,\n            equity_supplementary_mappings=equity_supplementary_mappings,\n            futures=futures,\n            exchanges=exchanges,\n            root_symbols=root_symbols,\n        )\n\n        futures_output = self._normalize_futures(futures)\n\n        equity_supplementary_mappings_output = (\n            self._normalize_equity_supplementary_mappings(\n                equity_supplementary_mappings,\n            )\n        )\n\n        exchanges_output = _generate_output_dataframe(\n            data_subset=exchanges,\n            defaults=_exchanges_defaults,\n        )\n\n        equities_output, equities_mappings = self._normalize_equities(\n            equities,\n            exchanges_output,\n        )\n\n        root_symbols_output = _generate_output_dataframe(\n            data_subset=root_symbols,\n            defaults=_root_symbols_defaults,\n        )\n\n        return AssetData(\n            equities=equities_output,\n            equities_mappings=equities_mappings,\n            futures=futures_output,\n            exchanges=exchanges_output,\n            root_symbols=root_symbols_output,\n            equity_supplementary_mappings=equity_supplementary_mappings_output,\n        )", "response": "Load the data for the given asset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_raw_data(assets,\n                  data_query_cutoff_times,\n                  expr,\n                  odo_kwargs,\n                  checkpoints=None):\n    \"\"\"\n    Given an expression representing data to load, perform normalization and\n    forward-filling and return the data, materialized. Only accepts data with a\n    `sid` field.\n\n    Parameters\n    ----------\n    assets : pd.int64index\n        the assets to load data for.\n    data_query_cutoff_times : pd.DatetimeIndex\n        The datetime when data should no longer be considered available for\n        a session.\n    expr : expr\n        the expression representing the data to load.\n    odo_kwargs : dict\n        extra keyword arguments to pass to odo when executing the expression.\n    checkpoints : expr, optional\n        the expression representing the checkpointed data for `expr`.\n\n    Returns\n    -------\n    raw : pd.dataframe\n        The result of computing expr and materializing the result as a\n        dataframe.\n    \"\"\"\n    lower_dt, upper_dt = data_query_cutoff_times[[0, -1]]\n    raw = ffill_query_in_range(\n        expr,\n        lower_dt,\n        upper_dt,\n        checkpoints=checkpoints,\n        odo_kwargs=odo_kwargs,\n    )\n    sids = raw[SID_FIELD_NAME]\n    raw.drop(\n        sids[~sids.isin(assets)].index,\n        inplace=True\n    )\n    return raw", "response": "This function loads the data for a specific asset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a tuple into a range with error handling.", "response": "def from_tuple(tup):\n    \"\"\"Convert a tuple into a range with error handling.\n\n    Parameters\n    ----------\n    tup : tuple (len 2 or 3)\n        The tuple to turn into a range.\n\n    Returns\n    -------\n    range : range\n        The range from the tuple.\n\n    Raises\n    ------\n    ValueError\n        Raised when the tuple length is not 2 or 3.\n    \"\"\"\n    if len(tup) not in (2, 3):\n        raise ValueError(\n            'tuple must contain 2 or 3 elements, not: %d (%r' % (\n                len(tup),\n                tup,\n            ),\n        )\n    return range(*tup)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef maybe_from_tuple(tup_or_range):\n    if isinstance(tup_or_range, tuple):\n        return from_tuple(tup_or_range)\n    elif isinstance(tup_or_range, range):\n        return tup_or_range\n\n    raise ValueError(\n        'maybe_from_tuple expects a tuple or range, got %r: %r' % (\n            type(tup_or_range).__name__,\n            tup_or_range,\n        ),\n    )", "response": "Convert a tuple into a range but pass ranges through silently."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks that the steps of a and b are both 1.", "response": "def _check_steps(a, b):\n    \"\"\"Check that the steps of ``a`` and ``b`` are both 1.\n\n    Parameters\n    ----------\n    a : range\n        The first range to check.\n    b : range\n        The second range to check.\n\n    Raises\n    ------\n    ValueError\n        Raised when either step is not 1.\n    \"\"\"\n    if a.step != 1:\n        raise ValueError('a.step must be equal to 1, got: %s' % a.step)\n    if b.step != 1:\n        raise ValueError('b.step must be equal to 1, got: %s' % b.step)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if two ranges overlap.", "response": "def overlap(a, b):\n    \"\"\"Check if  two ranges overlap.\n\n    Parameters\n    ----------\n    a : range\n        The first range.\n    b : range\n        The second range.\n\n    Returns\n    -------\n    overlaps : bool\n        Do these ranges overlap.\n\n    Notes\n    -----\n    This function does not support ranges with step != 1.\n    \"\"\"\n    _check_steps(a, b)\n    return a.stop >= b.start and b.stop >= a.start"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmerging two ranges with step == 1.", "response": "def merge(a, b):\n    \"\"\"Merge two ranges with step == 1.\n\n    Parameters\n    ----------\n    a : range\n        The first range.\n    b : range\n        The second range.\n    \"\"\"\n    _check_steps(a, b)\n    return range(min(a.start, b.start), max(a.stop, b.stop))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef intersecting_ranges(ranges):\n    ranges = sorted(ranges, key=op.attrgetter('start'))\n    return sorted_diff(ranges, group_ranges(ranges))", "response": "Return any ranges that intersect."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a filepath to the data file.", "response": "def get_data_filepath(name, environ=None):\n    \"\"\"\n    Returns a handle to data file.\n\n    Creates containing directory, if needed.\n    \"\"\"\n    dr = data_root(environ)\n\n    if not os.path.exists(dr):\n        os.makedirs(dr)\n\n    return os.path.join(dr, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes series_or_df have data on or before first_date and on or after last_date?", "response": "def has_data_for_dates(series_or_df, first_date, last_date):\n    \"\"\"\n    Does `series_or_df` have data on or before first_date and on or after\n    last_date?\n    \"\"\"\n    dts = series_or_df.index\n    if not isinstance(dts, pd.DatetimeIndex):\n        raise TypeError(\"Expected a DatetimeIndex, but got %s.\" % type(dts))\n    first, last = dts[[0, -1]]\n    return (first <= first_date) and (last >= last_date)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads benchmark returns and treasury yield curves for the given calendar and benchmark symbol.", "response": "def load_market_data(trading_day=None, trading_days=None, bm_symbol='SPY',\n                     environ=None):\n    \"\"\"\n    Load benchmark returns and treasury yield curves for the given calendar and\n    benchmark symbol.\n\n    Benchmarks are downloaded as a Series from IEX Trading.  Treasury curves\n    are US Treasury Bond rates and are downloaded from 'www.federalreserve.gov'\n    by default.  For Canadian exchanges, a loader for Canadian bonds from the\n    Bank of Canada is also available.\n\n    Results downloaded from the internet are cached in\n    ~/.zipline/data. Subsequent loads will attempt to read from the cached\n    files before falling back to redownload.\n\n    Parameters\n    ----------\n    trading_day : pandas.CustomBusinessDay, optional\n        A trading_day used to determine the latest day for which we\n        expect to have data.  Defaults to an NYSE trading day.\n    trading_days : pd.DatetimeIndex, optional\n        A calendar of trading days.  Also used for determining what cached\n        dates we should expect to have cached. Defaults to the NYSE calendar.\n    bm_symbol : str, optional\n        Symbol for the benchmark index to load. Defaults to 'SPY', the ticker\n        for the S&P 500, provided by IEX Trading.\n\n    Returns\n    -------\n    (benchmark_returns, treasury_curves) : (pd.Series, pd.DataFrame)\n\n    Notes\n    -----\n\n    Both return values are DatetimeIndexed with values dated to midnight in UTC\n    of each stored date.  The columns of `treasury_curves` are:\n\n    '1month', '3month', '6month',\n    '1year','2year','3year','5year','7year','10year','20year','30year'\n    \"\"\"\n    if trading_day is None:\n        trading_day = get_calendar('XNYS').day\n    if trading_days is None:\n        trading_days = get_calendar('XNYS').all_sessions\n\n    first_date = trading_days[0]\n    now = pd.Timestamp.utcnow()\n\n    # we will fill missing benchmark data through latest trading date\n    last_date = trading_days[trading_days.get_loc(now, method='ffill')]\n\n    br = ensure_benchmark_data(\n        bm_symbol,\n        first_date,\n        last_date,\n        now,\n        # We need the trading_day to figure out the close prior to the first\n        # date so that we can compute returns for the first date.\n        trading_day,\n        environ,\n    )\n    tc = ensure_treasury_data(\n        bm_symbol,\n        first_date,\n        last_date,\n        now,\n        environ,\n    )\n\n    # combine dt indices and reindex using ffill then bfill\n    all_dt = br.index.union(tc.index)\n    br = br.reindex(all_dt, method='ffill').fillna(method='bfill')\n    tc = tc.reindex(all_dt, method='ffill').fillna(method='bfill')\n\n    benchmark_returns = br[br.index.slice_indexer(first_date, last_date)]\n    treasury_curves = tc[tc.index.slice_indexer(first_date, last_date)]\n    return benchmark_returns, treasury_curves"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures that we have benchmark data for symbol from first_date to last_date.", "response": "def ensure_benchmark_data(symbol, first_date, last_date, now, trading_day,\n                          environ=None):\n    \"\"\"\n    Ensure we have benchmark data for `symbol` from `first_date` to `last_date`\n\n    Parameters\n    ----------\n    symbol : str\n        The symbol for the benchmark to load.\n    first_date : pd.Timestamp\n        First required date for the cache.\n    last_date : pd.Timestamp\n        Last required date for the cache.\n    now : pd.Timestamp\n        The current time.  This is used to prevent repeated attempts to\n        re-download data that isn't available due to scheduling quirks or other\n        failures.\n    trading_day : pd.CustomBusinessDay\n        A trading day delta.  Used to find the day before first_date so we can\n        get the close of the day prior to first_date.\n\n    We attempt to download data unless we already have data stored at the data\n    cache for `symbol` whose first entry is before or on `first_date` and whose\n    last entry is on or after `last_date`.\n\n    If we perform a download and the cache criteria are not satisfied, we wait\n    at least one hour before attempting a redownload.  This is determined by\n    comparing the current time to the result of os.path.getmtime on the cache\n    path.\n    \"\"\"\n    filename = get_benchmark_filename(symbol)\n    data = _load_cached_data(filename, first_date, last_date, now, 'benchmark',\n                             environ)\n    if data is not None:\n        return data\n\n    # If no cached data was found or it was missing any dates then download the\n    # necessary data.\n    logger.info(\n        ('Downloading benchmark data for {symbol!r} '\n            'from {first_date} to {last_date}'),\n        symbol=symbol,\n        first_date=first_date - trading_day,\n        last_date=last_date\n    )\n\n    try:\n        data = get_benchmark_returns(symbol)\n        data.to_csv(get_data_filepath(filename, environ))\n    except (OSError, IOError, HTTPError):\n        logger.exception('Failed to cache the new benchmark returns')\n        raise\n    if not has_data_for_dates(data, first_date, last_date):\n        logger.warn(\n            (\"Still don't have expected benchmark data for {symbol!r} \"\n                \"from {first_date} to {last_date} after redownload!\"),\n            symbol=symbol,\n            first_date=first_date - trading_day,\n            last_date=last_date\n        )\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that we have treasury data for the given symbol.", "response": "def ensure_treasury_data(symbol, first_date, last_date, now, environ=None):\n    \"\"\"\n    Ensure we have treasury data from treasury module associated with\n    `symbol`.\n\n    Parameters\n    ----------\n    symbol : str\n        Benchmark symbol for which we're loading associated treasury curves.\n    first_date : pd.Timestamp\n        First date required to be in the cache.\n    last_date : pd.Timestamp\n        Last date required to be in the cache.\n    now : pd.Timestamp\n        The current time.  This is used to prevent repeated attempts to\n        re-download data that isn't available due to scheduling quirks or other\n        failures.\n\n    We attempt to download data unless we already have data stored in the cache\n    for `module_name` whose first entry is before or on `first_date` and whose\n    last entry is on or after `last_date`.\n\n    If we perform a download and the cache criteria are not satisfied, we wait\n    at least one hour before attempting a redownload.  This is determined by\n    comparing the current time to the result of os.path.getmtime on the cache\n    path.\n    \"\"\"\n    loader_module, filename, source = INDEX_MAPPING.get(\n        symbol, INDEX_MAPPING['SPY'],\n    )\n    first_date = max(first_date, loader_module.earliest_possible_date())\n\n    data = _load_cached_data(filename, first_date, last_date, now, 'treasury',\n                             environ)\n    if data is not None:\n        return data\n\n    # If no cached data was found or it was missing any dates then download the\n    # necessary data.\n    logger.info(\n        ('Downloading treasury data for {symbol!r} '\n            'from {first_date} to {last_date}'),\n        symbol=symbol,\n        first_date=first_date,\n        last_date=last_date\n    )\n\n    try:\n        data = loader_module.get_treasury_data(first_date, last_date)\n        data.to_csv(get_data_filepath(filename, environ))\n    except (OSError, IOError, HTTPError):\n        logger.exception('failed to cache treasury data')\n    if not has_data_for_dates(data, first_date, last_date):\n        logger.warn(\n            (\"Still don't have expected treasury data for {symbol!r} \"\n                \"from {first_date} to {last_date} after redownload!\"),\n            symbol=symbol,\n            first_date=first_date,\n            last_date=last_date\n        )\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nspecializes a term if it s loadable.", "response": "def maybe_specialize(term, domain):\n    \"\"\"Specialize a term if it's loadable.\n    \"\"\"\n    if isinstance(term, LoadableTerm):\n        return term.specialize(domain)\n    return term"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_to_graph(self, term, parents):\n        if self._frozen:\n            raise ValueError(\n                \"Can't mutate %s after construction.\" % type(self).__name__\n            )\n\n        # If we've seen this node already as a parent of the current traversal,\n        # it means we have an unsatisifiable dependency.  This should only be\n        # possible if the term's inputs are mutated after construction.\n        if term in parents:\n            raise CyclicDependency(term)\n\n        parents.add(term)\n\n        self.graph.add_node(term)\n\n        for dependency in term.dependencies:\n            self._add_to_graph(dependency, parents)\n            self.graph.add_edge(dependency, term)\n\n        parents.remove(term)", "response": "Add a term and all its children to the graph."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execution_order(self, refcounts):\n        return iter(nx.topological_sort(\n            self.graph.subgraph(\n                {term for term, refcount in refcounts.items() if refcount > 0},\n            ),\n        ))", "response": "Return an iterator over the terms in self which have refcounts > 0."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating initial refcounts for execution of this graph.", "response": "def initial_refcounts(self, initial_terms):\n        \"\"\"\n        Calculate initial refcounts for execution of this graph.\n\n        Parameters\n        ----------\n        initial_terms : iterable[Term]\n            An iterable of terms that were pre-computed before graph execution.\n\n        Each node starts with a refcount equal to its outdegree, and output\n        nodes get one extra reference to ensure that they're still in the graph\n        at the end of execution.\n        \"\"\"\n        refcounts = self.graph.out_degree()\n        for t in self.outputs.values():\n            refcounts[t] += 1\n\n        for t in initial_terms:\n            self._decref_dependencies_recursive(t, refcounts, set())\n\n        return refcounts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _decref_dependencies_recursive(self, term, refcounts, garbage):\n        # Edges are tuple of (from, to).\n        for parent, _ in self.graph.in_edges([term]):\n            refcounts[parent] -= 1\n            # No one else depends on this term. Remove it from the\n            # workspace to conserve memory.\n            if refcounts[parent] == 0:\n                garbage.add(parent)\n                self._decref_dependencies_recursive(parent, refcounts, garbage)", "response": "Decrement terms recursively.\n\n        Notes\n        -----\n        This should only be used to build the initial workspace, after that we\n        should use:\n        :meth:`~zipline.pipeline.graph.TermGraph.decref_dependencies`"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decref_dependencies(self, term, refcounts):\n        garbage = set()\n        # Edges are tuple of (from, to).\n        for parent, _ in self.graph.in_edges([term]):\n            refcounts[parent] -= 1\n            # No one else depends on this term. Remove it from the\n            # workspace to conserve memory.\n            if refcounts[parent] == 0:\n                garbage.add(parent)\n        return garbage", "response": "Decrement in - edges for term after computation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extra_rows(self):\n        return {\n            term: attrs['extra_rows']\n            for term, attrs in iteritems(self.graph.node)\n        }", "response": "A dict mapping term to number of extra rows to load and compute of term."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure that we re going to compute at least N extra rows of term.", "response": "def _ensure_extra_rows(self, term, N):\n        \"\"\"\n        Ensure that we're going to compute at least N extra rows of `term`.\n        \"\"\"\n        attrs = self.graph.node[term]\n        attrs['extra_rows'] = max(N, attrs.get('extra_rows', 0))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mask_and_dates_for_term(self,\n                                term,\n                                root_mask_term,\n                                workspace,\n                                all_dates):\n        \"\"\"\n        Load mask and mask row labels for term.\n\n        Parameters\n        ----------\n        term : Term\n            The term to load the mask and labels for.\n        root_mask_term : Term\n            The term that represents the root asset exists mask.\n        workspace : dict[Term, any]\n            The values that have been computed for each term.\n        all_dates : pd.DatetimeIndex\n            All of the dates that are being computed for in the pipeline.\n\n        Returns\n        -------\n        mask : np.ndarray\n            The correct mask for this term.\n        dates : np.ndarray\n            The slice of dates for this term.\n        \"\"\"\n        mask = term.mask\n        mask_offset = self.extra_rows[mask] - self.extra_rows[term]\n\n        # This offset is computed against root_mask_term because that is what\n        # determines the shape of the top-level dates array.\n        dates_offset = (\n            self.extra_rows[root_mask_term] - self.extra_rows[term]\n        )\n\n        return workspace[mask][mask_offset:], all_dates[dates_offset:]", "response": "Load mask and labels for a term and return the mask and dates for that term."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes sure that all loadable terms in the graph are in the given domain.", "response": "def _assert_all_loadable_terms_specialized_to(self, domain):\n        \"\"\"Make sure that we've specialized all loadable terms in the graph.\n        \"\"\"\n        for term in self.graph.node:\n            if isinstance(term, LoadableTerm):\n                assert term.domain is domain"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake an extension for an AdjustedArrayWindow specialization.", "response": "def window_specialization(typename):\n    \"\"\"Make an extension for an AdjustedArrayWindow specialization.\"\"\"\n    return Extension(\n        'zipline.lib._{name}window'.format(name=typename),\n        ['zipline/lib/_{name}window.pyx'.format(name=typename)],\n        depends=['zipline/lib/_windowtemplate.pxi'],\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a requirements. txt file expressed as a path relative to Zipline root.", "response": "def read_requirements(path,\n                      strict_bounds,\n                      conda_format=False,\n                      filter_names=None):\n    \"\"\"\n    Read a requirements.txt file, expressed as a path relative to Zipline root.\n\n    Returns requirements with the pinned versions as lower bounds\n    if `strict_bounds` is falsey.\n    \"\"\"\n    real_path = join(dirname(abspath(__file__)), path)\n    with open(real_path) as f:\n        reqs = _filter_requirements(f.readlines(), filter_names=filter_names,\n                                    filter_sys_version=not conda_format)\n\n        if not strict_bounds:\n            reqs = map(_with_bounds, reqs)\n\n        if conda_format:\n            reqs = map(_conda_format, reqs)\n\n        return list(reqs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nensures that the time is UTC.", "response": "def ensure_utc(time, tz='UTC'):\n    \"\"\"\n    Normalize a time. If the time is tz-naive, assume it is UTC.\n    \"\"\"\n    if not time.tzinfo:\n        time = time.replace(tzinfo=pytz.timezone(tz))\n    return time.replace(tzinfo=pytz.utc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds the offset argument for event rules.", "response": "def _build_offset(offset, kwargs, default):\n    \"\"\"\n    Builds the offset argument for event rules.\n    \"\"\"\n    if offset is None:\n        if not kwargs:\n            return default  # use the default.\n        else:\n            return _td_check(datetime.timedelta(**kwargs))\n    elif kwargs:\n        raise ValueError('Cannot pass kwargs and an offset')\n    elif isinstance(offset, datetime.timedelta):\n        return _td_check(offset)\n    else:\n        raise TypeError(\"Must pass 'hours' and/or 'minutes' as keywords\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_date(date, kwargs):\n    if date is None:\n        if not kwargs:\n            raise ValueError('Must pass a date or kwargs')\n        else:\n            return datetime.date(**kwargs)\n\n    elif kwargs:\n        raise ValueError('Cannot pass kwargs and a date')\n    else:\n        return date", "response": "Builds the date argument for event rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_time(time, kwargs):\n    tz = kwargs.pop('tz', 'UTC')\n    if time:\n        if kwargs:\n            raise ValueError('Cannot pass kwargs and a time')\n        else:\n            return ensure_utc(time, tz)\n    elif not kwargs:\n        raise ValueError('Must pass a time or kwargs')\n    else:\n        return datetime.time(**kwargs)", "response": "Builds the time argument for event rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lossless_float_to_int(funcname, func, argname, arg):\n    if not isinstance(arg, float):\n        return arg\n\n    arg_as_int = int(arg)\n    if arg == arg_as_int:\n        warnings.warn(\n            \"{f} expected an int for argument {name!r}, but got float {arg}.\"\n            \" Coercing to int.\".format(\n                f=funcname,\n                name=argname,\n                arg=arg,\n            ),\n        )\n        return arg_as_int\n\n    raise TypeError(arg)", "response": "A preprocessor that coerces non - integral floats to ints."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs an event rule from the factory api.", "response": "def make_eventrule(date_rule, time_rule, cal, half_days=True):\n    \"\"\"\n    Constructs an event rule from the factory api.\n    \"\"\"\n    _check_if_not_called(date_rule)\n    _check_if_not_called(time_rule)\n\n    if half_days:\n        inner_rule = date_rule & time_rule\n    else:\n        inner_rule = date_rule & time_rule & NotHalfDay()\n\n    opd = OncePerDay(rule=inner_rule)\n    # This is where a scheduled function's rule is associated with a calendar.\n    opd.cal = cal\n    return opd"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_event(self, event, prepend=False):\n        if prepend:\n            self._events.insert(0, event)\n        else:\n            self._events.append(event)", "response": "Adds an event to the manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_data(self, context, data, dt):\n        if self.rule.should_trigger(dt):\n            self.callback(context, data)", "response": "Called when the data is received from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef should_trigger(self, dt):\n        return self.composer(\n            self.first.should_trigger,\n            self.second.should_trigger,\n            dt\n        )", "response": "Returns True if the two elements of the set are should trigger."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_dates(self, dt):\n        period_start, period_close = self.cal.open_and_close_for_session(\n            self.cal.minute_to_session_label(dt),\n        )\n\n        # Align the market open and close times here with the execution times\n        # used by the simulation clock. This ensures that scheduled functions\n        # trigger at the correct times.\n        self._period_start = self.cal.execution_time_from_open(period_start)\n        self._period_close = self.cal.execution_time_from_close(period_close)\n\n        self._period_end = self._period_start + self.offset - self._one_minute", "response": "Calculate the start and end dates for the given date."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the dates for the given date.", "response": "def calculate_dates(self, dt):\n        \"\"\"\n        Given a dt, find that day's close and period start (close - offset).\n        \"\"\"\n        period_end = self.cal.open_and_close_for_session(\n            self.cal.minute_to_session_label(dt),\n        )[1]\n\n        # Align the market close time here with the execution time used by the\n        # simulation clock. This ensures that scheduled functions trigger at\n        # the correct times.\n        self._period_end = self.cal.execution_time_from_close(period_end)\n\n        self._period_start = self._period_end - self.offset\n        self._period_close = self._period_end"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef winsorise_uint32(df, invalid_data_behavior, column, *columns):\n    columns = list((column,) + columns)\n    mask = df[columns] > UINT32_MAX\n\n    if invalid_data_behavior != 'ignore':\n        mask |= df[columns].isnull()\n    else:\n        # we are not going to generate a warning or error for this so just use\n        # nan_to_num\n        df[columns] = np.nan_to_num(df[columns])\n\n    mv = mask.values\n    if mv.any():\n        if invalid_data_behavior == 'raise':\n            raise ValueError(\n                '%d values out of bounds for uint32: %r' % (\n                    mv.sum(), df[mask.any(axis=1)],\n                ),\n            )\n        if invalid_data_behavior == 'warn':\n            warnings.warn(\n                'Ignoring %d values because they are out of bounds for'\n                ' uint32: %r' % (\n                    mv.sum(), df[mask.any(axis=1)],\n                ),\n                stacklevel=3,  # one extra frame for `expect_element`\n            )\n\n    df[mask] = 0\n    return df", "response": "This function will drop any record where a value would not fit into a uint32."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self,\n              data,\n              assets=None,\n              show_progress=False,\n              invalid_data_behavior='warn'):\n        \"\"\"\n        Parameters\n        ----------\n        data : iterable[tuple[int, pandas.DataFrame or bcolz.ctable]]\n            The data chunks to write. Each chunk should be a tuple of sid\n            and the data for that asset.\n        assets : set[int], optional\n            The assets that should be in ``data``. If this is provided\n            we will check ``data`` against the assets and provide better\n            progress information.\n        show_progress : bool, optional\n            Whether or not to show a progress bar while writing.\n        invalid_data_behavior : {'warn', 'raise', 'ignore'}, optional\n            What to do when data is encountered that is outside the range of\n            a uint32.\n\n        Returns\n        -------\n        table : bcolz.ctable\n            The newly-written table.\n        \"\"\"\n        ctx = maybe_show_progress(\n            (\n                (sid, self.to_ctable(df, invalid_data_behavior))\n                for sid, df in data\n            ),\n            show_progress=show_progress,\n            item_show_func=self.progress_bar_item_show_func,\n            label=self.progress_bar_message,\n            length=len(assets) if assets is not None else None,\n        )\n        with ctx as it:\n            return self._write_internal(it, assets)", "response": "Writes the data to the internal ctable."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads CSVs from our asset map and write them to the local file system.", "response": "def write_csvs(self,\n                   asset_map,\n                   show_progress=False,\n                   invalid_data_behavior='warn'):\n        \"\"\"Read CSVs as DataFrames from our asset map.\n\n        Parameters\n        ----------\n        asset_map : dict[int -> str]\n            A mapping from asset id to file path with the CSV data for that\n            asset\n        show_progress : bool\n            Whether or not to show a progress bar while writing.\n        invalid_data_behavior : {'warn', 'raise', 'ignore'}\n            What to do when data is encountered that is outside the range of\n            a uint32.\n        \"\"\"\n        read = partial(\n            read_csv,\n            parse_dates=['day'],\n            index_col='day',\n            dtype=self._csv_dtypes,\n        )\n        return self.write(\n            ((asset, read(path)) for asset, path in iteritems(asset_map)),\n            assets=viewkeys(asset_map),\n            show_progress=show_progress,\n            invalid_data_behavior=invalid_data_behavior,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_slices(self, start_idx, end_idx, assets):\n        # The core implementation of the logic here is implemented in Cython\n        # for efficiency.\n        return _compute_row_slices(\n            self._first_rows,\n            self._last_rows,\n            self._calendar_offsets,\n            start_idx,\n            end_idx,\n            assets,\n        )", "response": "Compute the raw row indices to load for each asset on a query for the\n            given dates after applying a shift."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _spot_col(self, colname):\n        try:\n            col = self._spot_cols[colname]\n        except KeyError:\n            col = self._spot_cols[colname] = self._table[colname]\n        return col", "response": "Get the column from the daily_bar_table and read all of it into memory and cache the result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the index into the data tape for the given sid and day.", "response": "def sid_day_index(self, sid, day):\n        \"\"\"\n        Parameters\n        ----------\n        sid : int\n            The asset identifier.\n        day : datetime64-like\n            Midnight of the day for which data is requested.\n\n        Returns\n        -------\n        int\n            Index into the data tape for the given sid and day.\n            Raises a NoDataOnDate exception if the given day and sid is before\n            or after the date range of the equity.\n        \"\"\"\n        try:\n            day_loc = self.sessions.get_loc(day)\n        except Exception:\n            raise NoDataOnDate(\"day={0} is outside of calendar={1}\".format(\n                day, self.sessions))\n        offset = day_loc - self._calendar_offsets[sid]\n        if offset < 0:\n            raise NoDataBeforeDate(\n                \"No data on or before day={0} for sid={1}\".format(\n                    day, sid))\n        ix = self._first_rows[sid] + offset\n        if ix > self._last_rows[sid]:\n            raise NoDataAfterDate(\n                \"No data on or after day={0} for sid={1}\".format(\n                    day, sid))\n        return ix"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the value of the given asset for the given sid on the given dt and field.", "response": "def get_value(self, sid, dt, field):\n        \"\"\"\n        Parameters\n        ----------\n        sid : int\n            The asset identifier.\n        day : datetime64-like\n            Midnight of the day for which data is requested.\n        colname : string\n            The price field. e.g. ('open', 'high', 'low', 'close', 'volume')\n\n        Returns\n        -------\n        float\n            The spot price for colname of the given sid on the given day.\n            Raises a NoDataOnDate exception if the given day and sid is before\n            or after the date range of the equity.\n            Returns -1 if the day is within the date range, but the price is\n            0.\n        \"\"\"\n        ix = self.sid_day_index(sid, dt)\n        price = self._spot_col(field)[ix]\n        if field != 'volume':\n            if price == 0:\n                return nan\n            else:\n                return price * 0.001\n        else:\n            return price"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_engine(self, get_loader):\n        if get_loader is not None:\n            self.engine = SimplePipelineEngine(\n                get_loader,\n                self.asset_finder,\n                self.default_pipeline_domain(self.trading_calendar),\n            )\n        else:\n            self.engine = ExplodingPipelineEngine()", "response": "Constructs and stores an ExplodingPipelineEngine from the given get_loader."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize(self, *args, **kwargs):\n        with ZiplineAPI(self):\n            self._initialize(self, *args, **kwargs)", "response": "Call self. _initialize with self made available to Zipline API\n        functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new instance of the class that is used by the simulation.", "response": "def _create_clock(self):\n        \"\"\"\n        If the clock property is not set, then create one based on frequency.\n        \"\"\"\n        trading_o_and_c = self.trading_calendar.schedule.ix[\n            self.sim_params.sessions]\n        market_closes = trading_o_and_c['market_close']\n        minutely_emission = False\n\n        if self.sim_params.data_frequency == 'minute':\n            market_opens = trading_o_and_c['market_open']\n            minutely_emission = self.sim_params.emission_rate == \"minute\"\n\n            # The calendar's execution times are the minutes over which we\n            # actually want to run the clock. Typically the execution times\n            # simply adhere to the market open and close times. In the case of\n            # the futures calendar, for example, we only want to simulate over\n            # a subset of the full 24 hour calendar, so the execution times\n            # dictate a market open time of 6:31am US/Eastern and a close of\n            # 5:00pm US/Eastern.\n            execution_opens = \\\n                self.trading_calendar.execution_time_from_open(market_opens)\n            execution_closes = \\\n                self.trading_calendar.execution_time_from_close(market_closes)\n        else:\n            # in daily mode, we want to have one bar per session, timestamped\n            # as the last minute of the session.\n            execution_closes = \\\n                self.trading_calendar.execution_time_from_close(market_closes)\n            execution_opens = execution_closes\n\n        # FIXME generalize these values\n        before_trading_start_minutes = days_at_time(\n            self.sim_params.sessions,\n            time(8, 45),\n            \"US/Eastern\"\n        )\n\n        return MinuteSimulationClock(\n            self.sim_params.sessions,\n            execution_opens,\n            execution_closes,\n            before_trading_start_minutes,\n            minute_emission=minutely_emission,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_eager_pipelines(self):\n        for name, pipe in self._pipelines.items():\n            if pipe.eager:\n                self.pipeline_output(name)", "response": "Compute any pipelines attached with eager = True."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_capital_changes(self, dt, emission_rate, is_interday,\n                                  portfolio_value_adjustment=0.0):\n        \"\"\"\n        If there is a capital change for a given dt, this means the the change\n        occurs before `handle_data` on the given dt. In the case of the\n        change being a target value, the change will be computed on the\n        portfolio value according to prices at the given dt\n\n        `portfolio_value_adjustment`, if specified, will be removed from the\n        portfolio_value of the cumulative performance when calculating deltas\n        from target capital changes.\n        \"\"\"\n        try:\n            capital_change = self.capital_changes[dt]\n        except KeyError:\n            return\n\n        self._sync_last_sale_prices()\n        if capital_change['type'] == 'target':\n            target = capital_change['value']\n            capital_change_amount = (\n                target -\n                (\n                    self.portfolio.portfolio_value -\n                    portfolio_value_adjustment\n                )\n            )\n\n            log.info('Processing capital change to target %s at %s. Capital '\n                     'change delta is %s' % (target, dt,\n                                             capital_change_amount))\n        elif capital_change['type'] == 'delta':\n            target = None\n            capital_change_amount = capital_change['value']\n            log.info('Processing capital change of delta %s at %s'\n                     % (capital_change_amount, dt))\n        else:\n            log.error(\"Capital change %s does not indicate a valid type \"\n                      \"('target' or 'delta')\" % capital_change)\n            return\n\n        self.capital_change_deltas.update({dt: capital_change_amount})\n        self.metrics_tracker.capital_change(capital_change_amount)\n\n        yield {\n            'capital_change':\n                {'date': dt,\n                 'type': 'cash',\n                 'target': target,\n                 'delta': capital_change_amount}\n        }", "response": "Calculates the capital changes for a given dt."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_environment(self, field='platform'):\n        env = {\n            'arena': self.sim_params.arena,\n            'data_frequency': self.sim_params.data_frequency,\n            'start': self.sim_params.first_open,\n            'end': self.sim_params.last_close,\n            'capital_base': self.sim_params.capital_base,\n            'platform': self._platform\n        }\n        if field == '*':\n            return env\n        else:\n            try:\n                return env[field]\n            except KeyError:\n                raise ValueError(\n                    '%r is not a valid field for get_environment' % field,\n                )", "response": "Query the execution environment for the specified field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch_csv(self,\n                  url,\n                  pre_func=None,\n                  post_func=None,\n                  date_column='date',\n                  date_format=None,\n                  timezone=pytz.utc.zone,\n                  symbol=None,\n                  mask=True,\n                  symbol_column=None,\n                  special_params_checker=None,\n                  country_code=None,\n                  **kwargs):\n        \"\"\"Fetch a csv from a remote url and register the data so that it is\n        queryable from the ``data`` object.\n\n        Parameters\n        ----------\n        url : str\n            The url of the csv file to load.\n        pre_func : callable[pd.DataFrame -> pd.DataFrame], optional\n            A callback to allow preprocessing the raw data returned from\n            fetch_csv before dates are paresed or symbols are mapped.\n        post_func : callable[pd.DataFrame -> pd.DataFrame], optional\n            A callback to allow postprocessing of the data after dates and\n            symbols have been mapped.\n        date_column : str, optional\n            The name of the column in the preprocessed dataframe containing\n            datetime information to map the data.\n        date_format : str, optional\n            The format of the dates in the ``date_column``. If not provided\n            ``fetch_csv`` will attempt to infer the format. For information\n            about the format of this string, see :func:`pandas.read_csv`.\n        timezone : tzinfo or str, optional\n            The timezone for the datetime in the ``date_column``.\n        symbol : str, optional\n            If the data is about a new asset or index then this string will\n            be the name used to identify the values in ``data``. For example,\n            one may use ``fetch_csv`` to load data for VIX, then this field\n            could be the string ``'VIX'``.\n        mask : bool, optional\n            Drop any rows which cannot be symbol mapped.\n        symbol_column : str\n            If the data is attaching some new attribute to each asset then this\n            argument is the name of the column in the preprocessed dataframe\n            containing the symbols. This will be used along with the date\n            information to map the sids in the asset finder.\n        country_code : str, optional\n            Country code to use to disambiguate symbol lookups.\n        **kwargs\n            Forwarded to :func:`pandas.read_csv`.\n\n        Returns\n        -------\n        csv_data_source : zipline.sources.requests_csv.PandasRequestsCSV\n            A requests source that will pull data from the url specified.\n        \"\"\"\n        if country_code is None:\n            country_code = self.default_fetch_csv_country_code(\n                self.trading_calendar,\n            )\n\n        # Show all the logs every time fetcher is used.\n        csv_data_source = PandasRequestsCSV(\n            url,\n            pre_func,\n            post_func,\n            self.asset_finder,\n            self.trading_calendar.day,\n            self.sim_params.start_session,\n            self.sim_params.end_session,\n            date_column,\n            date_format,\n            timezone,\n            symbol,\n            mask,\n            symbol_column,\n            data_frequency=self.data_frequency,\n            country_code=country_code,\n            special_params_checker=special_params_checker,\n            **kwargs\n        )\n\n        # ingest this into dataportal\n        self.data_portal.handle_extra_source(csv_data_source.df,\n                                             self.sim_params)\n\n        return csv_data_source", "response": "Fetch a CSV file from a remote url and register the data in the data object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an event to the EventManager.", "response": "def add_event(self, rule, callback):\n        \"\"\"Adds an event to the algorithm's EventManager.\n\n        Parameters\n        ----------\n        rule : EventRule\n            The rule for when the callback should be triggered.\n        callback : callable[(context, data) -> None]\n            The function to execute when the rule is triggered.\n        \"\"\"\n        self.event_manager.add_event(\n            zipline.utils.events.Event(rule, callback),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nschedule a function to be called according to some timed rules.", "response": "def schedule_function(self,\n                          func,\n                          date_rule=None,\n                          time_rule=None,\n                          half_days=True,\n                          calendar=None):\n        \"\"\"Schedules a function to be called according to some timed rules.\n\n        Parameters\n        ----------\n        func : callable[(context, data) -> None]\n            The function to execute when the rule is triggered.\n        date_rule : EventRule, optional\n            The rule for the dates to execute this function.\n        time_rule : EventRule, optional\n            The rule for the times to execute this function.\n        half_days : bool, optional\n            Should this rule fire on half days?\n        calendar : Sentinel, optional\n            Calendar used to reconcile date and time rules.\n\n        See Also\n        --------\n        :class:`zipline.api.date_rules`\n        :class:`zipline.api.time_rules`\n        \"\"\"\n\n        # When the user calls schedule_function(func, <time_rule>), assume that\n        # the user meant to specify a time rule but no date rule, instead of\n        # a date rule and no time rule as the signature suggests\n        if isinstance(date_rule, (AfterOpen, BeforeClose)) and not time_rule:\n            warnings.warn('Got a time rule for the second positional argument '\n                          'date_rule. You should use keyword argument '\n                          'time_rule= when calling schedule_function without '\n                          'specifying a date_rule', stacklevel=3)\n\n        date_rule = date_rule or date_rules.every_day()\n        time_rule = ((time_rule or time_rules.every_minute())\n                     if self.sim_params.data_frequency == 'minute' else\n                     # If we are in daily mode the time_rule is ignored.\n                     time_rules.every_minute())\n\n        # Check the type of the algorithm's schedule before pulling calendar\n        # Note that the ExchangeTradingSchedule is currently the only\n        # TradingSchedule class, so this is unlikely to be hit\n        if calendar is None:\n            cal = self.trading_calendar\n        elif calendar is calendars.US_EQUITIES:\n            cal = get_calendar('XNYS')\n        elif calendar is calendars.US_FUTURES:\n            cal = get_calendar('us_futures')\n        else:\n            raise ScheduleFunctionInvalidCalendar(\n                given_calendar=calendar,\n                allowed_calendars=(\n                    '[calendars.US_EQUITIES, calendars.US_FUTURES]'\n                ),\n            )\n\n        self.add_event(\n            make_eventrule(date_rule, time_rule, cal, half_days),\n            func,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef continuous_future(self,\n                          root_symbol_str,\n                          offset=0,\n                          roll='volume',\n                          adjustment='mul'):\n        \"\"\"Create a specifier for a continuous contract.\n\n        Parameters\n        ----------\n        root_symbol_str : str\n            The root symbol for the future chain.\n\n        offset : int, optional\n            The distance from the primary contract. Default is 0.\n\n        roll_style : str, optional\n            How rolls are determined. Default is 'volume'.\n\n        adjustment : str, optional\n            Method for adjusting lookback prices between rolls. Options are\n            'mul', 'add', and None. Default is 'mul'.\n\n        Returns\n        -------\n        continuous_future : ContinuousFuture\n            The continuous future specifier.\n        \"\"\"\n        return self.asset_finder.create_continuous_future(\n            root_symbol_str,\n            offset,\n            roll,\n            adjustment,\n        )", "response": "Create a ContinuousFuture object for a continuous contract."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef symbol(self, symbol_str, country_code=None):\n        # If the user has not set the symbol lookup date,\n        # use the end_session as the date for symbol->sid resolution.\n        _lookup_date = self._symbol_lookup_date \\\n            if self._symbol_lookup_date is not None \\\n            else self.sim_params.end_session\n\n        return self.asset_finder.lookup_symbol(\n            symbol_str,\n            as_of_date=_lookup_date,\n            country_code=country_code,\n        )", "response": "Lookup an Equity by its ticker symbol."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef symbols(self, *args, **kwargs):\n        return [self.symbol(identifier, **kwargs) for identifier in args]", "response": "Returns a list of ticker symbols that are held on the current multuple equities."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calculate_order_value_amount(self, asset, value):\n        # Make sure the asset exists, and that there is a last price for it.\n        # FIXME: we should use BarData's can_trade logic here, but I haven't\n        # yet found a good way to do that.\n        normalized_date = normalize_date(self.datetime)\n\n        if normalized_date < asset.start_date:\n            raise CannotOrderDelistedAsset(\n                msg=\"Cannot order {0}, as it started trading on\"\n                    \" {1}.\".format(asset.symbol, asset.start_date)\n            )\n        elif normalized_date > asset.end_date:\n            raise CannotOrderDelistedAsset(\n                msg=\"Cannot order {0}, as it stopped trading on\"\n                    \" {1}.\".format(asset.symbol, asset.end_date)\n            )\n        else:\n            last_price = \\\n                self.trading_client.current_data.current(asset, \"price\")\n\n            if np.isnan(last_price):\n                raise CannotOrderDelistedAsset(\n                    msg=\"Cannot order {0} on {1} as there is no last \"\n                        \"price for the security.\".format(asset.symbol,\n                                                         self.datetime)\n                )\n\n        if tolerant_equals(last_price, 0):\n            zero_message = \"Price of 0 for {psid}; can't infer value\".format(\n                psid=asset\n            )\n            if self.logger:\n                self.logger.debug(zero_message)\n            # Don't place any order\n            return 0\n\n        value_multiplier = asset.price_multiplier\n\n        return value / (last_price * value_multiplier)", "response": "Calculates how many shares and contracts to order based on the type of asset being ordered."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplacing an order. Parameters ---------- asset : Asset The asset that this order is for. amount : int The amount of shares to order. If ``amount`` is positive, this is the number of shares to buy or cover. If ``amount`` is negative, this is the number of shares to sell or short. limit_price : float, optional The limit price for the order. stop_price : float, optional The stop price for the order. style : ExecutionStyle, optional The execution style for the order. Returns ------- order_id : str or None The unique identifier for this order, or None if no order was placed. Notes ----- The ``limit_price`` and ``stop_price`` arguments provide shorthands for passing common execution styles. Passing ``limit_price=N`` is equivalent to ``style=LimitOrder(N)``. Similarly, passing ``stop_price=M`` is equivalent to ``style=StopOrder(M)``, and passing ``limit_price=N`` and ``stop_price=M`` is equivalent to ``style=StopLimitOrder(N, M)``. It is an error to pass both a ``style`` and ``limit_price`` or ``stop_price``. See Also -------- :class:`zipline.finance.execution.ExecutionStyle` :func:`zipline.api.order_value` :func:`zipline.api.order_percent`", "response": "def order(self,\n              asset,\n              amount,\n              limit_price=None,\n              stop_price=None,\n              style=None):\n        \"\"\"Place an order.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        amount : int\n            The amount of shares to order. If ``amount`` is positive, this is\n            the number of shares to buy or cover. If ``amount`` is negative,\n            this is the number of shares to sell or short.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle, optional\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str or None\n            The unique identifier for this order, or None if no order was\n            placed.\n\n        Notes\n        -----\n        The ``limit_price`` and ``stop_price`` arguments provide shorthands for\n        passing common execution styles. Passing ``limit_price=N`` is\n        equivalent to ``style=LimitOrder(N)``. Similarly, passing\n        ``stop_price=M`` is equivalent to ``style=StopOrder(M)``, and passing\n        ``limit_price=N`` and ``stop_price=M`` is equivalent to\n        ``style=StopLimitOrder(N, M)``. It is an error to pass both a ``style``\n        and ``limit_price`` or ``stop_price``.\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order_value`\n        :func:`zipline.api.order_percent`\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        amount, style = self._calculate_order(asset, amount,\n                                              limit_price, stop_price, style)\n        return self.blotter.order(asset, amount, style)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_order_params(self,\n                              asset,\n                              amount,\n                              limit_price,\n                              stop_price,\n                              style):\n        \"\"\"\n        Helper method for validating parameters to the order API function.\n\n        Raises an UnsupportedOrderParameters if invalid arguments are found.\n        \"\"\"\n\n        if not self.initialized:\n            raise OrderDuringInitialize(\n                msg=\"order() can only be called from within handle_data()\"\n            )\n\n        if style:\n            if limit_price:\n                raise UnsupportedOrderParameters(\n                    msg=\"Passing both limit_price and style is not supported.\"\n                )\n\n            if stop_price:\n                raise UnsupportedOrderParameters(\n                    msg=\"Passing both stop_price and style is not supported.\"\n                )\n\n        for control in self.trading_controls:\n            control.validate(asset,\n                             amount,\n                             self.portfolio,\n                             self.get_datetime(),\n                             self.trading_client.current_data)", "response": "Helper method for validating parameters to the order API function.\n\n        Raises an UnsupportedOrderParameters if invalid arguments are found."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef order_value(self,\n                    asset,\n                    value,\n                    limit_price=None,\n                    stop_price=None,\n                    style=None):\n        \"\"\"Place an order by desired value rather than desired number of\n        shares.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        value : float\n            If the requested asset exists, the requested value is\n            divided by its price to imply the number of shares to transact.\n            If the Asset being ordered is a Future, the 'value' calculated\n            is actually the exposure, as Futures have no 'value'.\n\n            value > 0 :: Buy/Cover\n            value < 0 :: Sell/Short\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str\n            The unique identifier for this order.\n\n        Notes\n        -----\n        See :func:`zipline.api.order` for more information about\n        ``limit_price``, ``stop_price``, and ``style``\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order`\n        :func:`zipline.api.order_percent`\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        amount = self._calculate_order_value_amount(asset, value)\n        return self.order(asset, amount,\n                          limit_price=limit_price,\n                          stop_price=stop_price,\n                          style=style)", "response": "Place an order by desired value rather than desired number of shares."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_dt_changed(self, dt):\n        self.datetime = dt\n        self.blotter.set_date(dt)", "response": "Called by the simulation loop when the current datetime is changed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_datetime(self, tz=None):\n        dt = self.datetime\n        assert dt.tzinfo == pytz.utc, \"Algorithm should have a utc datetime\"\n        if tz is not None:\n            dt = dt.astimezone(tz)\n        return dt", "response": "Returns the current simulation datetime."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_slippage(self, us_equities=None, us_futures=None):\n        if self.initialized:\n            raise SetSlippagePostInit()\n\n        if us_equities is not None:\n            if Equity not in us_equities.allowed_asset_types:\n                raise IncompatibleSlippageModel(\n                    asset_type='equities',\n                    given_model=us_equities,\n                    supported_asset_types=us_equities.allowed_asset_types,\n                )\n            self.blotter.slippage_models[Equity] = us_equities\n\n        if us_futures is not None:\n            if Future not in us_futures.allowed_asset_types:\n                raise IncompatibleSlippageModel(\n                    asset_type='futures',\n                    given_model=us_futures,\n                    supported_asset_types=us_futures.allowed_asset_types,\n                )\n            self.blotter.slippage_models[Future] = us_futures", "response": "Set the slippage models for the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the commission models for the simulation.", "response": "def set_commission(self, us_equities=None, us_futures=None):\n        \"\"\"Sets the commission models for the simulation.\n\n        Parameters\n        ----------\n        us_equities : EquityCommissionModel\n            The commission model to use for trading US equities.\n        us_futures : FutureCommissionModel\n            The commission model to use for trading US futures.\n\n        See Also\n        --------\n        :class:`zipline.finance.commission.PerShare`\n        :class:`zipline.finance.commission.PerTrade`\n        :class:`zipline.finance.commission.PerDollar`\n        \"\"\"\n        if self.initialized:\n            raise SetCommissionPostInit()\n\n        if us_equities is not None:\n            if Equity not in us_equities.allowed_asset_types:\n                raise IncompatibleCommissionModel(\n                    asset_type='equities',\n                    given_model=us_equities,\n                    supported_asset_types=us_equities.allowed_asset_types,\n                )\n            self.blotter.commission_models[Equity] = us_equities\n\n        if us_futures is not None:\n            if Future not in us_futures.allowed_asset_types:\n                raise IncompatibleCommissionModel(\n                    asset_type='futures',\n                    given_model=us_futures,\n                    supported_asset_types=us_futures.allowed_asset_types,\n                )\n            self.blotter.commission_models[Future] = us_futures"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the order cancellation policy for the simulation.", "response": "def set_cancel_policy(self, cancel_policy):\n        \"\"\"Sets the order cancellation policy for the simulation.\n\n        Parameters\n        ----------\n        cancel_policy : CancelPolicy\n            The cancellation policy to use.\n\n        See Also\n        --------\n        :class:`zipline.api.EODCancel`\n        :class:`zipline.api.NeverCancel`\n        \"\"\"\n        if not isinstance(cancel_policy, CancelPolicy):\n            raise UnsupportedCancelPolicy()\n\n        if self.initialized:\n            raise SetCancelPolicyPostInit()\n\n        self.blotter.cancel_policy = cancel_policy"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the date for which symbols will be resolved to their assets at .", "response": "def set_symbol_lookup_date(self, dt):\n        \"\"\"Set the date for which symbols will be resolved to their assets\n        (symbols may map to different firms or underlying assets at\n        different times)\n\n        Parameters\n        ----------\n        dt : datetime\n            The new symbol lookup date.\n        \"\"\"\n        try:\n            self._symbol_lookup_date = pd.Timestamp(dt, tz='UTC')\n        except ValueError:\n            raise UnsupportedDatetimeFormat(input=dt,\n                                            method='set_symbol_lookup_date')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef order_percent(self,\n                      asset,\n                      percent,\n                      limit_price=None,\n                      stop_price=None,\n                      style=None):\n        \"\"\"Place an order in the specified asset corresponding to the given\n        percent of the current portfolio value.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        percent : float\n            The percentage of the portfolio value to allocate to ``asset``.\n            This is specified as a decimal, for example: 0.50 means 50%.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str\n            The unique identifier for this order.\n\n        Notes\n        -----\n        See :func:`zipline.api.order` for more information about\n        ``limit_price``, ``stop_price``, and ``style``\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order`\n        :func:`zipline.api.order_value`\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        amount = self._calculate_order_percent_amount(asset, percent)\n        return self.order(asset, amount,\n                          limit_price=limit_price,\n                          stop_price=stop_price,\n                          style=style)", "response": "Place an order in the specified asset corresponding to the given percent of the current portfolio value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef order_target(self,\n                     asset,\n                     target,\n                     limit_price=None,\n                     stop_price=None,\n                     style=None):\n        \"\"\"Place an order to adjust a position to a target number of shares. If\n        the position doesn't already exist, this is equivalent to placing a new\n        order. If the position does exist, this is equivalent to placing an\n        order for the difference between the target number of shares and the\n        current number of shares.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        target : int\n            The desired number of shares of ``asset``.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str\n            The unique identifier for this order.\n\n\n        Notes\n        -----\n        ``order_target`` does not take into account any open orders. For\n        example:\n\n        .. code-block:: python\n\n           order_target(sid(0), 10)\n           order_target(sid(0), 10)\n\n        This code will result in 20 shares of ``sid(0)`` because the first\n        call to ``order_target`` will not have been filled when the second\n        ``order_target`` call is made.\n\n        See :func:`zipline.api.order` for more information about\n        ``limit_price``, ``stop_price``, and ``style``\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order`\n        :func:`zipline.api.order_target_percent`\n        :func:`zipline.api.order_target_value`\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        amount = self._calculate_order_target_amount(asset, target)\n        return self.order(asset, amount,\n                          limit_price=limit_price,\n                          stop_price=stop_price,\n                          style=style)", "response": "Place an order to adjust a position to a target number of shares."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplace an order to adjust a position to a target value.", "response": "def order_target_value(self,\n                           asset,\n                           target,\n                           limit_price=None,\n                           stop_price=None,\n                           style=None):\n        \"\"\"Place an order to adjust a position to a target value. If\n        the position doesn't already exist, this is equivalent to placing a new\n        order. If the position does exist, this is equivalent to placing an\n        order for the difference between the target value and the\n        current value.\n        If the Asset being ordered is a Future, the 'target value' calculated\n        is actually the target exposure, as Futures have no 'value'.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        target : float\n            The desired total value of ``asset``.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str\n            The unique identifier for this order.\n\n        Notes\n        -----\n        ``order_target_value`` does not take into account any open orders. For\n        example:\n\n        .. code-block:: python\n\n           order_target_value(sid(0), 10)\n           order_target_value(sid(0), 10)\n\n        This code will result in 20 dollars of ``sid(0)`` because the first\n        call to ``order_target_value`` will not have been filled when the\n        second ``order_target_value`` call is made.\n\n        See :func:`zipline.api.order` for more information about\n        ``limit_price``, ``stop_price``, and ``style``\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order`\n        :func:`zipline.api.order_target`\n        :func:`zipline.api.order_target_percent`\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        target_amount = self._calculate_order_value_amount(asset, target)\n        amount = self._calculate_order_target_amount(asset, target_amount)\n        return self.order(asset, amount,\n                          limit_price=limit_price,\n                          stop_price=stop_price,\n                          style=style)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef order_target_percent(self, asset, target,\n                             limit_price=None, stop_price=None, style=None):\n        \"\"\"Place an order to adjust a position to a target percent of the\n        current portfolio value. If the position doesn't already exist, this is\n        equivalent to placing a new order. If the position does exist, this is\n        equivalent to placing an order for the difference between the target\n        percent and the current percent.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        target : float\n            The desired percentage of the portfolio value to allocate to\n            ``asset``. This is specified as a decimal, for example:\n            0.50 means 50%.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str\n            The unique identifier for this order.\n\n        Notes\n        -----\n        ``order_target_value`` does not take into account any open orders. For\n        example:\n\n        .. code-block:: python\n\n           order_target_percent(sid(0), 10)\n           order_target_percent(sid(0), 10)\n\n        This code will result in 20% of the portfolio being allocated to sid(0)\n        because the first call to ``order_target_percent`` will not have been\n        filled when the second ``order_target_percent`` call is made.\n\n        See :func:`zipline.api.order` for more information about\n        ``limit_price``, ``stop_price``, and ``style``\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order`\n        :func:`zipline.api.order_target`\n        :func:`zipline.api.order_target_value`\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        amount = self._calculate_order_target_percent_amount(asset, target)\n        return self.order(asset, amount,\n                          limit_price=limit_price,\n                          stop_price=stop_price,\n                          style=style)", "response": "Place an order to adjust a position to a target percent of the current portfolio value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef batch_market_order(self, share_counts):\n        style = MarketOrder()\n        order_args = [\n            (asset, amount, style)\n            for (asset, amount) in iteritems(share_counts)\n            if amount\n        ]\n        return self.blotter.batch_order(order_args)", "response": "Place a batch market order for multiple assets."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all of the open orders for the given asset.", "response": "def get_open_orders(self, asset=None):\n        \"\"\"Retrieve all of the current open orders.\n\n        Parameters\n        ----------\n        asset : Asset\n            If passed and not None, return only the open orders for the given\n            asset instead of all open orders.\n\n        Returns\n        -------\n        open_orders : dict[list[Order]] or list[Order]\n            If no asset is passed this will return a dict mapping Assets\n            to a list containing all the open orders for the asset.\n            If an asset is passed then this will return a list of the open\n            orders for this asset.\n        \"\"\"\n        if asset is None:\n            return {\n                key: [order.to_api_obj() for order in orders]\n                for key, orders in iteritems(self.blotter.open_orders)\n                if orders\n            }\n        if asset in self.blotter.open_orders:\n            orders = self.blotter.open_orders[asset]\n            return [order.to_api_obj() for order in orders]\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncancels an open order.", "response": "def cancel_order(self, order_param):\n        \"\"\"Cancel an open order.\n\n        Parameters\n        ----------\n        order_param : str or Order\n            The order_id or order object to cancel.\n        \"\"\"\n        order_id = order_param\n        if isinstance(order_param, zipline.protocol.Order):\n            order_id = order_param.id\n\n        self.blotter.cancel(order_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef history(self, bar_count, frequency, field, ffill=True):\n        warnings.warn(\n            \"The `history` method is deprecated.  Use `data.history` instead.\",\n            category=ZiplineDeprecationWarning,\n            stacklevel=4\n        )\n\n        return self.get_history_window(\n            bar_count,\n            frequency,\n            self._calculate_universe(),\n            field,\n            ffill\n        )", "response": "DEPRECATED: use ``data.history`` instead."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_account_control(self, control):\n        if self.initialized:\n            raise RegisterAccountControlPostInit()\n        self.account_controls.append(control)", "response": "Register a new AccountControl to be checked on each bar."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets a limit on the minimum leverage of the algorithm.", "response": "def set_min_leverage(self, min_leverage, grace_period):\n        \"\"\"Set a limit on the minimum leverage of the algorithm.\n\n        Parameters\n        ----------\n        min_leverage : float\n            The minimum leverage for the algorithm.\n        grace_period : pd.Timedelta\n            The offset from the start date used to enforce a minimum leverage.\n        \"\"\"\n        deadline = self.sim_params.start_session + grace_period\n        control = MinLeverage(min_leverage, deadline)\n        self.register_account_control(control)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a new TradingControl to be checked prior to order calls.", "response": "def register_trading_control(self, control):\n        \"\"\"\n        Register a new TradingControl to be checked prior to order calls.\n        \"\"\"\n        if self.initialized:\n            raise RegisterTradingControlPostInit()\n        self.trading_controls.append(control)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_max_position_size(self,\n                              asset=None,\n                              max_shares=None,\n                              max_notional=None,\n                              on_error='fail'):\n        \"\"\"Set a limit on the number of shares and/or dollar value held for the\n        given sid. Limits are treated as absolute values and are enforced at\n        the time that the algo attempts to place an order for sid. This means\n        that it's possible to end up with more than the max number of shares\n        due to splits/dividends, and more than the max notional due to price\n        improvement.\n\n        If an algorithm attempts to place an order that would result in\n        increasing the absolute value of shares/dollar value exceeding one of\n        these limits, raise a TradingControlException.\n\n        Parameters\n        ----------\n        asset : Asset, optional\n            If provided, this sets the guard only on positions in the given\n            asset.\n        max_shares : int, optional\n            The maximum number of shares to hold for an asset.\n        max_notional : float, optional\n            The maximum value to hold for an asset.\n        \"\"\"\n        control = MaxPositionSize(asset=asset,\n                                  max_shares=max_shares,\n                                  max_notional=max_notional,\n                                  on_error=on_error)\n        self.register_trading_control(control)", "response": "Set the maximum position size for the given asset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_max_order_size(self,\n                           asset=None,\n                           max_shares=None,\n                           max_notional=None,\n                           on_error='fail'):\n        \"\"\"Set a limit on the number of shares and/or dollar value of any single\n        order placed for sid.  Limits are treated as absolute values and are\n        enforced at the time that the algo attempts to place an order for sid.\n\n        If an algorithm attempts to place an order that would result in\n        exceeding one of these limits, raise a TradingControlException.\n\n        Parameters\n        ----------\n        asset : Asset, optional\n            If provided, this sets the guard only on positions in the given\n            asset.\n        max_shares : int, optional\n            The maximum number of shares that can be ordered at one time.\n        max_notional : float, optional\n            The maximum value that can be ordered at one time.\n        \"\"\"\n        control = MaxOrderSize(asset=asset,\n                               max_shares=max_shares,\n                               max_notional=max_notional,\n                               on_error=on_error)\n        self.register_trading_control(control)", "response": "Set the maximum number of shares and dollar values of any single\n            order placed for sid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_max_order_count(self, max_count, on_error='fail'):\n        control = MaxOrderCount(on_error, max_count)\n        self.register_trading_control(control)", "response": "Set a limit on the number of orders that can be placed on a single entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the asset restrictions on which assets can be ordered.", "response": "def set_asset_restrictions(self, restrictions, on_error='fail'):\n        \"\"\"Set a restriction on which assets can be ordered.\n\n        Parameters\n        ----------\n        restricted_list : Restrictions\n            An object providing information about restricted assets.\n\n        See Also\n        --------\n        zipline.finance.asset_restrictions.Restrictions\n        \"\"\"\n        control = RestrictedListOrder(on_error, restrictions)\n        self.register_trading_control(control)\n        self.restrictions |= restrictions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef attach_pipeline(self, pipeline, name, chunks=None, eager=True):\n        if chunks is None:\n            # Make the first chunk smaller to get more immediate results:\n            # (one week, then every half year)\n            chunks = chain([5], repeat(126))\n        elif isinstance(chunks, int):\n            chunks = repeat(chunks)\n\n        if name in self._pipelines:\n            raise DuplicatePipelineName(name=name)\n\n        self._pipelines[name] = AttachedPipeline(pipeline, iter(chunks), eager)\n\n        # Return the pipeline to allow expressions like\n        # p = attach_pipeline(Pipeline(), 'name')\n        return pipeline", "response": "Register a pipeline to be computed at the start of each day."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pipeline_output(self, name):\n        try:\n            pipe, chunks, _ = self._pipelines[name]\n        except KeyError:\n            raise NoSuchPipeline(\n                name=name,\n                valid=list(self._pipelines.keys()),\n            )\n        return self._pipeline_output(pipe, chunks, name)", "response": "Get the results of the pipeline that was attached with the name name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute pipeline and return a DataFrame containing data for at least start_date and end_date.", "response": "def run_pipeline(self, pipeline, start_session, chunksize):\n        \"\"\"\n        Compute `pipeline`, providing values for at least `start_date`.\n\n        Produces a DataFrame containing data for days between `start_date` and\n        `end_date`, where `end_date` is defined by:\n\n            `end_date = min(start_date + chunksize trading days,\n                            simulation_end)`\n\n        Returns\n        -------\n        (data, valid_until) : tuple (pd.DataFrame, pd.Timestamp)\n\n        See Also\n        --------\n        PipelineEngine.run_pipeline\n        \"\"\"\n        sessions = self.trading_calendar.all_sessions\n\n        # Load data starting from the previous trading day...\n        start_date_loc = sessions.get_loc(start_session)\n\n        # ...continuing until either the day before the simulation end, or\n        # until chunksize days of data have been loaded.\n        sim_end_session = self.sim_params.end_session\n\n        end_loc = min(\n            start_date_loc + chunksize,\n            sessions.get_loc(sim_end_session)\n        )\n\n        end_session = sessions[end_loc]\n\n        return \\\n            self.engine.run_pipeline(pipeline, start_session, end_session), \\\n            end_session"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef all_api_methods(cls):\n        return [\n            fn for fn in itervalues(vars(cls))\n            if getattr(fn, 'is_api_method', False)\n        ]", "response": "Return a list of all the TradingAlgorithm API methods in the class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bulleted_list(items, max_count=None, indent=2):\n    if max_count is not None and len(items) > max_count:\n        item_list = list(items)\n        items = item_list[:max_count - 1]\n        items.append('...')\n        items.append(item_list[-1])\n\n    line_template = (\" \" * indent) + \"- {}\"\n    return \"\\n\".join(map(line_template.format, items))", "response": "Format a bulleted list of values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks for the presence of an extra to the argument list. Raises expections if it is missing and expected.", "response": "def _expect_extra(expected, present, exc_unexpected, exc_missing, exc_args):\n    \"\"\"\n    Checks for the presence of an extra to the argument list. Raises expections\n    if this is unexpected or if it is missing and expected.\n    \"\"\"\n    if present:\n        if not expected:\n            raise exc_unexpected(*exc_args)\n    elif expected and expected is not Argument.ignore:\n        raise exc_missing(*exc_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks the given callable to make sure that it satisfies the given expectations.", "response": "def verify_callable_argspec(callable_,\n                            expected_args=Argument.ignore,\n                            expect_starargs=Argument.ignore,\n                            expect_kwargs=Argument.ignore):\n    \"\"\"\n    Checks the callable_ to make sure that it satisfies the given\n    expectations.\n    expected_args should be an iterable of Arguments in the order you expect to\n    receive them.\n    expect_starargs means that the function should or should not take a *args\n    param. expect_kwargs says the callable should or should not take  **kwargs\n    param.\n    If expected_args, expect_starargs, or expect_kwargs is Argument.ignore,\n    then the checks related to that argument will not occur.\n\n    Example usage:\n\n    callable_check(\n        f,\n        [Argument('a'), Argument('b', 1)],\n        expect_starargs=True,\n        expect_kwargs=Argument.ignore\n    )\n    \"\"\"\n    if not callable(callable_):\n        raise NotCallable(callable_)\n\n    expected_arg_list = list(\n        expected_args if expected_args is not Argument.ignore else []\n    )\n\n    args, starargs, kwargs = Argument.parse_argspec(callable_)\n\n    exc_args = callable_, args, starargs, kwargs\n\n    # Check the *args.\n    _expect_extra(\n        expect_starargs,\n        starargs,\n        UnexpectedStarargs,\n        NoStarargs,\n        exc_args,\n    )\n    # Check the **kwargs.\n    _expect_extra(\n        expect_kwargs,\n        kwargs,\n        UnexpectedKwargs,\n        NoKwargs,\n        exc_args,\n    )\n\n    if expected_args is Argument.ignore:\n        # Ignore the argument list checks.\n        return\n\n    if len(args) < len(expected_arg_list):\n        # One or more argument that we expected was not present.\n        raise NotEnoughArguments(\n            callable_,\n            args,\n            starargs,\n            kwargs,\n            [arg for arg in expected_arg_list if arg not in args],\n        )\n    elif len(args) > len(expected_arg_list):\n        raise TooManyArguments(\n            callable_, args, starargs, kwargs\n        )\n\n    # Empty argument that will not match with any actual arguments.\n    missing_arg = Argument(object(), object())\n\n    for expected, provided in zip_longest(expected_arg_list,\n                                          args,\n                                          fillvalue=missing_arg):\n        if not expected.matches(provided):\n            raise MismatchedArguments(\n                callable_, args, starargs, kwargs\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a callable and returns a tuple with the list of Argument objects varargs and keywords.", "response": "def parse_argspec(callable_):\n        \"\"\"\n        Takes a callable and returns a tuple with the list of Argument objects,\n        the name of *args, and the name of **kwargs.\n        If *args or **kwargs is not present, it will be None.\n        This returns a namedtuple called Argspec that has three fields named:\n        args, starargs, and kwargs.\n        \"\"\"\n        args, varargs, keywords, defaults = getargspec(callable_)\n        defaults = list(defaults or [])\n\n        if getattr(callable_, '__self__', None) is not None:\n            # This is a bound method, drop the self param.\n            args = args[1:]\n\n        first_default = len(args) - len(defaults)\n        return Argspec(\n            [Argument(arg, Argument.no_default\n                      if n < first_default else defaults[n - first_default])\n             for n, arg in enumerate(args)],\n            varargs,\n            keywords,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the asset is restricted for all dts.", "response": "def is_restricted(self, assets, dt):\n        \"\"\"\n        An asset is restricted for all dts if it is in the static list.\n        \"\"\"\n        if isinstance(assets, Asset):\n            return assets in self._restricted_set\n        return pd.Series(\n            index=pd.Index(assets),\n            data=vectorized_is_element(assets, self._restricted_set)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning whether or not an asset or iterable of assets is restricted on a dt.", "response": "def is_restricted(self, assets, dt):\n        \"\"\"\n        Returns whether or not an asset or iterable of assets is restricted\n        on a dt.\n        \"\"\"\n        if isinstance(assets, Asset):\n            return self._is_restricted_for_asset(assets, dt)\n\n        is_restricted = partial(self._is_restricted_for_asset, dt=dt)\n        return pd.Series(\n            index=pd.Index(assets),\n            data=vectorize(is_restricted, otypes=[bool])(assets)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling a list of splits by modifying any positions as needed.", "response": "def handle_splits(self, splits):\n        \"\"\"Processes a list of splits by modifying any positions as needed.\n\n        Parameters\n        ----------\n        splits: list\n            A list of splits.  Each split is a tuple of (asset, ratio).\n\n        Returns\n        -------\n        int: The leftover cash from fractional shares after modifying each\n            position.\n        \"\"\"\n        total_leftover_cash = 0\n\n        for asset, ratio in splits:\n            if asset in self.positions:\n                self._dirty_stats = True\n\n                # Make the position object handle the split. It returns the\n                # leftover cash from a fractional share, if there is any.\n                position = self.positions[asset]\n                leftover_cash = position.handle_split(asset, ratio)\n                total_leftover_cash += leftover_cash\n\n        return total_leftover_cash"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef earn_dividends(self, cash_dividends, stock_dividends):\n        for cash_dividend in cash_dividends:\n            self._dirty_stats = True  # only mark dirty if we pay a dividend\n\n            # Store the earned dividends so that they can be paid on the\n            # dividends' pay_dates.\n            div_owed = self.positions[cash_dividend.asset].earn_dividend(\n                cash_dividend,\n            )\n            try:\n                self._unpaid_dividends[cash_dividend.pay_date].append(div_owed)\n            except KeyError:\n                self._unpaid_dividends[cash_dividend.pay_date] = [div_owed]\n\n        for stock_dividend in stock_dividends:\n            self._dirty_stats = True  # only mark dirty if we pay a dividend\n\n            div_owed = self.positions[\n                stock_dividend.asset\n            ].earn_stock_dividend(stock_dividend)\n            try:\n                self._unpaid_stock_dividends[stock_dividend.pay_date].append(\n                    div_owed,\n                )\n            except KeyError:\n                self._unpaid_stock_dividends[stock_dividend.pay_date] = [\n                    div_owed,\n                ]", "response": "This method calculates and stores the cash and stock payments for each dividend s pay date."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a cash payment based on the dividends that should be paid outby the accumulated bookkeeping of earned unpaid and stock dividends.", "response": "def pay_dividends(self, next_trading_day):\n        \"\"\"\n        Returns a cash payment based on the dividends that should be paid out\n        according to the accumulated bookkeeping of earned, unpaid, and stock\n        dividends.\n        \"\"\"\n        net_cash_payment = 0.0\n\n        try:\n            payments = self._unpaid_dividends[next_trading_day]\n            # Mark these dividends as paid by dropping them from our unpaid\n            del self._unpaid_dividends[next_trading_day]\n        except KeyError:\n            payments = []\n\n        # representing the fact that we're required to reimburse the owner of\n        # the stock for any dividends paid while borrowing.\n        for payment in payments:\n            net_cash_payment += payment['amount']\n\n        # Add stock for any stock dividends paid.  Again, the values here may\n        # be negative in the case of short positions.\n        try:\n            stock_payments = self._unpaid_stock_dividends[next_trading_day]\n        except KeyError:\n            stock_payments = []\n\n        for stock_payment in stock_payments:\n            payment_asset = stock_payment['payment_asset']\n            share_count = stock_payment['share_count']\n            # note we create a Position for stock dividend if we don't\n            # already own the asset\n            if payment_asset in self.positions:\n                position = self.positions[payment_asset]\n            else:\n                position = self.positions[payment_asset] = Position(\n                    payment_asset,\n                )\n\n            position.amount += share_count\n\n        return net_cash_payment"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the current status of the positions.", "response": "def stats(self):\n        \"\"\"The current status of the positions.\n\n        Returns\n        -------\n        stats : PositionStats\n            The current stats position stats.\n\n        Notes\n        -----\n        This is cached, repeated access will not recompute the stats until\n        the stats may have changed.\n        \"\"\"\n        if self._dirty_stats:\n            calculate_position_tracker_stats(self.positions, self._stats)\n            self._dirty_stats = False\n\n        return self._stats"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_transaction(self, transaction):\n        asset = transaction.asset\n        if isinstance(asset, Future):\n            try:\n                old_price = self._payout_last_sale_prices[asset]\n            except KeyError:\n                self._payout_last_sale_prices[asset] = transaction.price\n            else:\n                position = self.position_tracker.positions[asset]\n                amount = position.amount\n                price = transaction.price\n\n                self._cash_flow(\n                    self._calculate_payout(\n                        asset.price_multiplier,\n                        amount,\n                        old_price,\n                        price,\n                    ),\n                )\n\n                if amount + transaction.amount == 0:\n                    del self._payout_last_sale_prices[asset]\n                else:\n                    self._payout_last_sale_prices[asset] = price\n        else:\n            self._cash_flow(-(transaction.price * transaction.amount))\n\n        self.position_tracker.execute_transaction(transaction)\n\n        # we only ever want the dict form from now on\n        transaction_dict = transaction.to_dict()\n        try:\n            self._processed_transactions[transaction.dt].append(\n                transaction_dict,\n            )\n        except KeyError:\n            self._processed_transactions[transaction.dt] = [transaction_dict]", "response": "Add a transaction to the ledger updating the current state as needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_splits(self, splits):\n        leftover_cash = self.position_tracker.handle_splits(splits)\n        if leftover_cash > 0:\n            self._cash_flow(leftover_cash)", "response": "Processes a list of splits by modifying any positions as needed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_order(self, order):\n        try:\n            dt_orders = self._orders_by_modified[order.dt]\n        except KeyError:\n            self._orders_by_modified[order.dt] = OrderedDict([\n                (order.id, order),\n            ])\n            self._orders_by_id[order.id] = order\n        else:\n            self._orders_by_id[order.id] = dt_orders[order.id] = order\n            # to preserve the order of the orders by modified date\n            move_to_end(dt_orders, order.id, last=True)\n\n        move_to_end(self._orders_by_id, order.id, last=True)", "response": "Keep track of an order that was placed.\n\n        Parameters\n        ----------\n        order : zp.Order\n            The order to record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing the commission. Parameters ---------- commission : zp.Event The commission being paid.", "response": "def process_commission(self, commission):\n        \"\"\"Process the commission.\n\n        Parameters\n        ----------\n        commission : zp.Event\n            The commission being paid.\n        \"\"\"\n        asset = commission['asset']\n        cost = commission['cost']\n\n        self.position_tracker.handle_commission(asset, cost)\n        self._cash_flow(-cost)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_dividends(self, next_session, asset_finder, adjustment_reader):\n        position_tracker = self.position_tracker\n\n        # Earn dividends whose ex_date is the next trading day. We need to\n        # check if we own any of these stocks so we know to pay them out when\n        # the pay date comes.\n        held_sids = set(position_tracker.positions)\n        if held_sids:\n            cash_dividends = adjustment_reader.get_dividends_with_ex_date(\n                held_sids,\n                next_session,\n                asset_finder\n            )\n            stock_dividends = (\n                adjustment_reader.get_stock_dividends_with_ex_date(\n                    held_sids,\n                    next_session,\n                    asset_finder\n                )\n            )\n\n            # Earning a dividend just marks that we need to get paid out on\n            # the dividend's pay-date. This does not affect our cash yet.\n            position_tracker.earn_dividends(\n                cash_dividends,\n                stock_dividends,\n            )\n\n        # Pay out the dividends whose pay-date is the next session. This does\n        # affect out cash.\n        self._cash_flow(\n            position_tracker.pay_dividends(\n                next_session,\n            ),\n        )", "response": "Process the dividends for the next session."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the dict - form of all of the transactions in a given bar or for the whole simulation.", "response": "def transactions(self, dt=None):\n        \"\"\"Retrieve the dict-form of all of the transactions in a given bar or\n        for the whole simulation.\n\n        Parameters\n        ----------\n        dt : pd.Timestamp or None, optional\n            The particular datetime to look up transactions for. If not passed,\n            or None is explicitly passed, all of the transactions will be\n            returned.\n\n        Returns\n        -------\n        transactions : list[dict]\n            The transaction information.\n        \"\"\"\n        if dt is None:\n            # flatten the by-day transactions\n            return [\n                txn\n                for by_day in itervalues(self._processed_transactions)\n                for txn in by_day\n            ]\n\n        return self._processed_transactions.get(dt, [])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the dict - form of all of the orders in a given bar or for the whole simulation.", "response": "def orders(self, dt=None):\n        \"\"\"Retrieve the dict-form of all of the orders in a given bar or for\n        the whole simulation.\n\n        Parameters\n        ----------\n        dt : pd.Timestamp or None, optional\n            The particular datetime to look up order for. If not passed, or\n            None is explicitly passed, all of the orders will be returned.\n\n        Returns\n        -------\n        orders : list[dict]\n            The order information.\n        \"\"\"\n        if dt is None:\n            # orders by id is already flattened\n            return [o.to_dict() for o in itervalues(self._orders_by_id)]\n\n        return [\n            o.to_dict()\n            for o in itervalues(self._orders_by_modified.get(dt, {}))\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the current portfolio state.", "response": "def update_portfolio(self):\n        \"\"\"Force a computation of the current portfolio state.\n        \"\"\"\n        if not self._dirty_portfolio:\n            return\n\n        portfolio = self._portfolio\n        pt = self.position_tracker\n\n        portfolio.positions = pt.get_positions()\n        position_stats = pt.stats\n\n        portfolio.positions_value = position_value = (\n            position_stats.net_value\n        )\n        portfolio.positions_exposure = position_stats.net_exposure\n        self._cash_flow(self._get_payout_total(pt.positions))\n\n        start_value = portfolio.portfolio_value\n\n        # update the new starting value\n        portfolio.portfolio_value = end_value = portfolio.cash + position_value\n\n        pnl = end_value - start_value\n        if start_value != 0:\n            returns = pnl / start_value\n        else:\n            returns = 0.0\n\n        portfolio.pnl += pnl\n        portfolio.returns = (\n            (1 + portfolio.returns) *\n            (1 + returns) -\n            1\n        )\n\n        # the portfolio has been fully synced\n        self._dirty_portfolio = False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef override_account_fields(self,\n                                settled_cash=not_overridden,\n                                accrued_interest=not_overridden,\n                                buying_power=not_overridden,\n                                equity_with_loan=not_overridden,\n                                total_positions_value=not_overridden,\n                                total_positions_exposure=not_overridden,\n                                regt_equity=not_overridden,\n                                regt_margin=not_overridden,\n                                initial_margin_requirement=not_overridden,\n                                maintenance_margin_requirement=not_overridden,\n                                available_funds=not_overridden,\n                                excess_liquidity=not_overridden,\n                                cushion=not_overridden,\n                                day_trades_remaining=not_overridden,\n                                leverage=not_overridden,\n                                net_leverage=not_overridden,\n                                net_liquidation=not_overridden):\n        \"\"\"Override fields on ``self.account``.\n        \"\"\"\n        # mark that the portfolio is dirty to override the fields again\n        self._dirty_account = True\n        self._account_overrides = kwargs = {\n            k: v for k, v in locals().items() if v is not not_overridden\n        }\n        del kwargs['self']", "response": "Override fields on self. account."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef datashape_type_to_numpy(type_):\n    if isinstance(type_, Option):\n        type_ = type_.ty\n    if isinstance(type_, DateTime):\n        return np.dtype('datetime64[ns]')\n    if isinstance(type_, String):\n        return np.dtype(object)\n    if type_ in integral:\n        return np.dtype('int64')\n    else:\n        return type_.to_numpy_dtype()", "response": "Returns the associated numpy type for a datashape type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new dataset from a blaze expression.", "response": "def new_dataset(expr, missing_values, domain):\n    \"\"\"\n    Creates or returns a dataset from a blaze expression.\n\n    Parameters\n    ----------\n    expr : Expr\n        The blaze expression representing the values.\n    missing_values : frozenset((name, value) pairs\n        Association pairs column name and missing_value for that column.\n\n        This needs to be a frozenset rather than a dict or tuple of tuples\n        because we want a collection that's unordered but still hashable.\n    domain : zipline.pipeline.domain.Domain\n        Domain of the dataset to be created.\n\n    Returns\n    -------\n    ds : type\n        A new dataset type.\n\n    Notes\n    -----\n    This function is memoized. repeated calls with the same inputs will return\n    the same type.\n    \"\"\"\n    missing_values = dict(missing_values)\n    class_dict = {'ndim': 2 if SID_FIELD_NAME in expr.fields else 1}\n    for name, type_ in expr.dshape.measure.fields:\n        # Don't generate a column for sid or timestamp, since they're\n        # implicitly the labels if the arrays that will be passed to pipeline\n        # Terms.\n        if name in (SID_FIELD_NAME, TS_FIELD_NAME):\n            continue\n        type_ = datashape_type_to_numpy(type_)\n        if can_represent_dtype(type_):\n            col = Column(\n                type_,\n                missing_values.get(name, NotSpecified),\n            )\n        else:\n            col = NonPipelineField(name, type_)\n        class_dict[name] = col\n\n    if 'domain' in class_dict:\n        raise ValueError(\"Got a column named 'domain' in new_dataset(). \"\n                         \"'domain' is reserved.\")\n    class_dict['domain'] = domain\n\n    name = expr._name\n    if name is None:\n        name = next(_new_names)\n\n    # unicode is a name error in py3 but the branch is only hit\n    # when we are in python 2.\n    if PY2 and isinstance(name, unicode):  # pragma: no cover # noqa\n        name = name.encode('utf-8')\n\n    return type(name, (DataSet,), class_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates that the expression and resources passed match up.", "response": "def _check_resources(name, expr, resources):\n    \"\"\"Validate that the expression and resources passed match up.\n\n    Parameters\n    ----------\n    name : str\n        The name of the argument we are checking.\n    expr : Expr\n        The potentially bound expr.\n    resources\n        The explicitly passed resources to compute expr.\n\n    Raises\n    ------\n    ValueError\n        If the resources do not match for an expression.\n    \"\"\"\n    if expr is None:\n        return\n    bound = expr._resources()\n    if not bound and resources is None:\n        raise ValueError('no resources provided to compute %s' % name)\n    if bound and resources:\n        raise ValueError(\n            'explicit and implicit resources provided to compute %s' % name,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_datetime_field(name, measure):\n    if not isinstance(measure[name], (Date, DateTime)):\n        raise TypeError(\n            \"'{name}' field must be a '{dt}', not: '{dshape}'\".format(\n                name=name,\n                dt=DateTime(),\n                dshape=measure[name],\n            ),\n        )", "response": "Checks that a field is a datetime inside some measure."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the correct metadata expression for the expression.", "response": "def _get_metadata(field, expr, metadata_expr, no_metadata_rule):\n    \"\"\"Find the correct metadata expression for the expression.\n\n    Parameters\n    ----------\n    field : {'deltas', 'checkpoints'}\n        The kind of metadata expr to lookup.\n    expr : Expr\n        The baseline expression.\n    metadata_expr : Expr, 'auto', or None\n        The metadata argument. If this is 'auto', then the metadata table will\n        be searched for by walking up the expression tree. If this cannot be\n        reflected, then an action will be taken based on the\n        ``no_metadata_rule``.\n    no_metadata_rule : {'warn', 'raise', 'ignore'}\n        How to handle the case where the metadata_expr='auto' but no expr\n        could be found.\n\n    Returns\n    -------\n    metadata : Expr or None\n        The deltas or metadata table to use.\n    \"\"\"\n    if isinstance(metadata_expr, bz.Expr) or metadata_expr is None:\n        return metadata_expr\n\n    try:\n        return expr._child['_'.join(((expr._name or ''), field))]\n    except (ValueError, AttributeError):\n        if no_metadata_rule == 'raise':\n            raise ValueError(\n                \"no %s table could be reflected for %s\" % (field, expr)\n            )\n        elif no_metadata_rule == 'warn':\n            warnings.warn(NoMetaDataWarning(expr, field), stacklevel=4)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ensure_timestamp_field(dataset_expr, deltas, checkpoints):\n    measure = dataset_expr.dshape.measure\n    if TS_FIELD_NAME not in measure.names:\n        dataset_expr = bz.transform(\n            dataset_expr,\n            **{TS_FIELD_NAME: dataset_expr[AD_FIELD_NAME]}\n        )\n        deltas = _ad_as_ts(deltas)\n        checkpoints = _ad_as_ts(checkpoints)\n    else:\n        _check_datetime_field(TS_FIELD_NAME, measure)\n\n    return dataset_expr, deltas, checkpoints", "response": "Verify that the baseline and deltas expressions have a timestamp field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_blaze(expr,\n               deltas='auto',\n               checkpoints='auto',\n               loader=None,\n               resources=None,\n               odo_kwargs=None,\n               missing_values=None,\n               domain=GENERIC,\n               no_deltas_rule='warn',\n               no_checkpoints_rule='warn'):\n    \"\"\"Create a Pipeline API object from a blaze expression.\n\n    Parameters\n    ----------\n    expr : Expr\n        The blaze expression to use.\n    deltas : Expr, 'auto' or None, optional\n        The expression to use for the point in time adjustments.\n        If the string 'auto' is passed, a deltas expr will be looked up\n        by stepping up the expression tree and looking for another field\n        with the name of ``expr._name`` + '_deltas'. If None is passed, no\n        deltas will be used.\n    checkpoints : Expr, 'auto' or None, optional\n        The expression to use for the forward fill checkpoints.\n        If the string 'auto' is passed, a checkpoints expr will be looked up\n        by stepping up the expression tree and looking for another field\n        with the name of ``expr._name`` + '_checkpoints'. If None is passed,\n        no checkpoints will be used.\n    loader : BlazeLoader, optional\n        The blaze loader to attach this pipeline dataset to. If None is passed,\n        the global blaze loader is used.\n    resources : dict or any, optional\n        The data to execute the blaze expressions against. This is used as the\n        scope for ``bz.compute``.\n    odo_kwargs : dict, optional\n        The keyword arguments to pass to odo when evaluating the expressions.\n    domain : zipline.pipeline.domain.Domain\n        Domain of the dataset to be created.\n    missing_values : dict[str -> any], optional\n        A dict mapping column names to missing values for those columns.\n        Missing values are required for integral columns.\n    no_deltas_rule : {'warn', 'raise', 'ignore'}, optional\n        What should happen if ``deltas='auto'`` but no deltas can be found.\n        'warn' says to raise a warning but continue.\n        'raise' says to raise an exception if no deltas can be found.\n        'ignore' says take no action and proceed with no deltas.\n    no_checkpoints_rule : {'warn', 'raise', 'ignore'}, optional\n        What should happen if ``checkpoints='auto'`` but no checkpoints can be\n        found. 'warn' says to raise a warning but continue.\n        'raise' says to raise an exception if no deltas can be found.\n        'ignore' says take no action and proceed with no deltas.\n\n    Returns\n    -------\n    pipeline_api_obj : DataSet or BoundColumn\n        Either a new dataset or bound column based on the shape of the expr\n        passed in. If a table shaped expression is passed, this will return\n        a ``DataSet`` that represents the whole table. If an array-like shape\n        is passed, a ``BoundColumn`` on the dataset that would be constructed\n        from passing the parent is returned.\n    \"\"\"\n    if 'auto' in {deltas, checkpoints}:\n        invalid_nodes = tuple(filter(is_invalid_deltas_node, expr._subterms()))\n        if invalid_nodes:\n            raise TypeError(\n                'expression with auto %s may only contain (%s) nodes,'\n                \" found: %s\" % (\n                    ' or '.join(\n                        ['deltas'] if deltas is not None else [] +\n                        ['checkpoints'] if checkpoints is not None else [],\n                    ),\n                    ', '.join(map(get__name__, valid_deltas_node_types)),\n                    ', '.join(\n                        set(map(compose(get__name__, type), invalid_nodes)),\n                    ),\n                ),\n            )\n    deltas = _get_metadata(\n        'deltas',\n        expr,\n        deltas,\n        no_deltas_rule,\n    )\n    checkpoints = _get_metadata(\n        'checkpoints',\n        expr,\n        checkpoints,\n        no_checkpoints_rule,\n    )\n\n    # Check if this is a single column out of a dataset.\n    if bz.ndim(expr) != 1:\n        raise TypeError(\n            'expression was not tabular or array-like,'\n            ' %s dimensions: %d' % (\n                'too many' if bz.ndim(expr) > 1 else 'not enough',\n                bz.ndim(expr),\n            ),\n        )\n\n    single_column = None\n    if isscalar(expr.dshape.measure):\n        # This is a single column. Record which column we are to return\n        # but create the entire dataset.\n        single_column = rename = expr._name\n        field_hit = False\n        if not isinstance(expr, traversable_nodes):\n            raise TypeError(\n                \"expression '%s' was array-like but not a simple field of\"\n                \" some larger table\" % str(expr),\n            )\n        while isinstance(expr, traversable_nodes):\n            if isinstance(expr, bz.expr.Field):\n                if not field_hit:\n                    field_hit = True\n                else:\n                    break\n            rename = expr._name\n            expr = expr._child\n        dataset_expr = expr.relabel({rename: single_column})\n    else:\n        dataset_expr = expr\n\n    measure = dataset_expr.dshape.measure\n    if not isrecord(measure) or AD_FIELD_NAME not in measure.names:\n        raise TypeError(\n            \"The dataset must be a collection of records with at least an\"\n            \" '{ad}' field. Fields provided: '{fields}'\\nhint: maybe you need\"\n            \" to use `relabel` to change your field names\".format(\n                ad=AD_FIELD_NAME,\n                fields=measure,\n            ),\n        )\n    _check_datetime_field(AD_FIELD_NAME, measure)\n    dataset_expr, deltas, checkpoints = _ensure_timestamp_field(\n        dataset_expr,\n        deltas,\n        checkpoints,\n    )\n\n    if deltas is not None and (sorted(deltas.dshape.measure.fields) !=\n                               sorted(measure.fields)):\n        raise TypeError(\n            'baseline measure != deltas measure:\\n%s != %s' % (\n                measure,\n                deltas.dshape.measure,\n            ),\n        )\n    if (checkpoints is not None and\n        (sorted(checkpoints.dshape.measure.fields) !=\n         sorted(measure.fields))):\n        raise TypeError(\n            'baseline measure != checkpoints measure:\\n%s != %s' % (\n                measure,\n                checkpoints.dshape.measure,\n            ),\n        )\n\n    # Ensure that we have a data resource to execute the query against.\n    _check_resources('expr', dataset_expr, resources)\n    _check_resources('deltas', deltas, resources)\n    _check_resources('checkpoints', checkpoints, resources)\n\n    # Create or retrieve the Pipeline API dataset.\n    if missing_values is None:\n        missing_values = {}\n    ds = new_dataset(dataset_expr, frozenset(missing_values.items()), domain)\n\n    # Register our new dataset with the loader.\n    (loader if loader is not None else global_loader).register_dataset(\n        ds,\n        bind_expression_to_resources(dataset_expr, resources),\n        bind_expression_to_resources(deltas, resources)\n        if deltas is not None else\n        None,\n        bind_expression_to_resources(checkpoints, resources)\n        if checkpoints is not None else\n        None,\n        odo_kwargs=odo_kwargs,\n    )\n    if single_column is not None:\n        # We were passed a single column, extract and return it.\n        return getattr(ds, single_column)\n    return ds", "response": "Create a Pipeline API object from a blaze expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bind_expression_to_resources(expr, resources):\n    # bind the resources into the expression\n    if resources is None:\n        resources = {}\n\n    # _subs stands for substitute.  It's not actually private, blaze just\n    # prefixes symbol-manipulation methods with underscores to prevent\n    # collisions with data column names.\n    return expr._subs({\n        k: bz.data(v, dshape=k.dshape) for k, v in iteritems(resources)\n    })", "response": "Bind a Blaze expression to resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_materialized_checkpoints(checkpoints, colnames, lower_dt, odo_kwargs):\n    if checkpoints is not None:\n        ts = checkpoints[TS_FIELD_NAME]\n        checkpoints_ts = odo(\n            ts[ts < lower_dt].max(),\n            pd.Timestamp,\n            **odo_kwargs\n        )\n        if pd.isnull(checkpoints_ts):\n            # We don't have a checkpoint for before our start date so just\n            # don't constrain the lower date.\n            materialized_checkpoints = pd.DataFrame(columns=colnames)\n            lower = None\n        else:\n            materialized_checkpoints = odo(\n                checkpoints[ts == checkpoints_ts][colnames],\n                pd.DataFrame,\n                **odo_kwargs\n            )\n            lower = checkpoints_ts\n    else:\n        materialized_checkpoints = pd.DataFrame(columns=colnames)\n        lower = None  # we don't have a good lower date constraint\n    return lower, materialized_checkpoints", "response": "Calculates a lower bound and a DataFrame of checkpoints for a given set of columns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ffill_query_in_range(expr,\n                         lower,\n                         upper,\n                         checkpoints=None,\n                         odo_kwargs=None,\n                         ts_field=TS_FIELD_NAME):\n    \"\"\"Query a blaze expression in a given time range properly forward filling\n    from values that fall before the lower date.\n\n    Parameters\n    ----------\n    expr : Expr\n        Bound blaze expression.\n    lower : datetime\n        The lower date to query for.\n    upper : datetime\n        The upper date to query for.\n    checkpoints : Expr, optional\n        Bound blaze expression for a checkpoints table from which to get a\n        computed lower bound.\n    odo_kwargs : dict, optional\n        The extra keyword arguments to pass to ``odo``.\n    ts_field : str, optional\n        The name of the timestamp field in the given blaze expression.\n\n    Returns\n    -------\n    raw : pd.DataFrame\n        A strict dataframe for the data in the given date range. This may\n        start before the requested start date if a value is needed to ffill.\n    \"\"\"\n    odo_kwargs = odo_kwargs or {}\n    computed_lower, materialized_checkpoints = get_materialized_checkpoints(\n        checkpoints,\n        expr.fields,\n        lower,\n        odo_kwargs,\n    )\n\n    pred = expr[ts_field] <= upper\n\n    if computed_lower is not None:\n        # only constrain the lower date if we computed a new lower date\n        pred &= expr[ts_field] >= computed_lower\n\n    raw = pd.concat(\n        (\n            materialized_checkpoints,\n            odo(\n                expr[pred],\n                pd.DataFrame,\n                **odo_kwargs\n            ),\n        ),\n        ignore_index=True,\n    )\n    raw.loc[:, ts_field] = raw.loc[:, ts_field].astype('datetime64[ns]')\n    return raw", "response": "Query a blaze expression in a given time range properly forward filling the raw data frame for the data in the given date range."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a dict of mappings where the values are lists of OwnershipPeriod objects returns a dict of the same structure with the new OwnershipPeriod objects adjusted so that the periods have no gaps.", "response": "def merge_ownership_periods(mappings):\n    \"\"\"\n    Given a dict of mappings where the values are lists of\n    OwnershipPeriod objects, returns a dict with the same structure with\n    new OwnershipPeriod objects adjusted so that the periods have no\n    gaps.\n\n    Orders the periods chronologically, and pushes forward the end date\n    of each period to match the start date of the following period. The\n    end date of the last period pushed forward to the max Timestamp.\n    \"\"\"\n    return valmap(\n        lambda v: tuple(\n            OwnershipPeriod(\n                a.start,\n                b.start,\n                a.sid,\n                a.value,\n            ) for a, b in sliding_window(\n                2,\n                concatv(\n                    sorted(v),\n                    # concat with a fake ownership object to make the last\n                    # end date be max timestamp\n                    [OwnershipPeriod(\n                        pd.Timestamp.max.tz_localize('utc'),\n                        None,\n                        None,\n                        None,\n                    )],\n                ),\n            )\n        ),\n        mappings,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a dict mapping to lists of OwnershipPeriods from a db table.", "response": "def build_ownership_map(table, key_from_row, value_from_row):\n    \"\"\"\n    Builds a dict mapping to lists of OwnershipPeriods, from a db table.\n    \"\"\"\n    return _build_ownership_map_from_rows(\n        sa.select(table.c).execute().fetchall(),\n        key_from_row,\n        value_from_row,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_grouped_ownership_map(table,\n                                key_from_row,\n                                value_from_row,\n                                group_key):\n    \"\"\"\n    Builds a dict mapping group keys to maps of keys to to lists of\n    OwnershipPeriods, from a db table.\n    \"\"\"\n    grouped_rows = groupby(\n        group_key,\n        sa.select(table.c).execute().fetchall(),\n    )\n    return {\n        key: _build_ownership_map_from_rows(\n            rows,\n            key_from_row,\n            value_from_row,\n        )\n        for key, rows in grouped_rows.items()\n    }", "response": "Builds a dict mapping group keys to lists of keys to lists of OwnershipPeriods from a db table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering out kwargs from a dictionary.", "response": "def _filter_kwargs(names, dict_):\n    \"\"\"Filter out kwargs from a dictionary.\n\n    Parameters\n    ----------\n    names : set[str]\n        The names to select from ``dict_``.\n    dict_ : dict[str, any]\n        The dictionary to select from.\n\n    Returns\n    -------\n    kwargs : dict[str, any]\n        ``dict_`` where the keys intersect with ``names`` and the values are\n        not None.\n    \"\"\"\n    return {k: v for k, v in dict_.items() if k in names and v is not None}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking in a dict of Asset init args and converts dates to pd. Timestamps", "response": "def _convert_asset_timestamp_fields(dict_):\n    \"\"\"\n    Takes in a dict of Asset init args and converts dates to pd.Timestamps\n    \"\"\"\n    for key in _asset_timestamp_fields & viewkeys(dict_):\n        value = pd.Timestamp(dict_[key], tz='UTC')\n        dict_[key] = None if isnull(value) else value\n    return dict_"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if the asset was active at the specified time corresponding to the specified date value.", "response": "def was_active(reference_date_value, asset):\n    \"\"\"\n    Whether or not `asset` was active at the time corresponding to\n    `reference_date_value`.\n\n    Parameters\n    ----------\n    reference_date_value : int\n        Date, represented as nanoseconds since EPOCH, for which we want to know\n        if `asset` was alive.  This is generally the result of accessing the\n        `value` attribute of a pandas Timestamp.\n    asset : Asset\n        The asset object to check.\n\n    Returns\n    -------\n    was_active : bool\n        Whether or not the `asset` existed at the specified time.\n    \"\"\"\n    return (\n        asset.start_date.value\n        <= reference_date_value\n        <= asset.end_date.value\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the asset types for a list of sids.", "response": "def lookup_asset_types(self, sids):\n        \"\"\"\n        Retrieve asset types for a list of sids.\n\n        Parameters\n        ----------\n        sids : list[int]\n\n        Returns\n        -------\n        types : dict[sid -> str or None]\n            Asset types for the provided sids.\n        \"\"\"\n        found = {}\n        missing = set()\n\n        for sid in sids:\n            try:\n                found[sid] = self._asset_type_cache[sid]\n            except KeyError:\n                missing.add(sid)\n\n        if not missing:\n            return found\n\n        router_cols = self.asset_router.c\n\n        for assets in group_into_chunks(missing):\n            query = sa.select((router_cols.sid, router_cols.asset_type)).where(\n                self.asset_router.c.sid.in_(map(int, assets))\n            )\n            for sid, type_ in query.execute().fetchall():\n                missing.remove(sid)\n                found[sid] = self._asset_type_cache[sid] = type_\n\n            for sid in missing:\n                found[sid] = self._asset_type_cache[sid] = None\n\n        return found"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving all assets in sids.", "response": "def retrieve_all(self, sids, default_none=False):\n        \"\"\"\n        Retrieve all assets in `sids`.\n\n        Parameters\n        ----------\n        sids : iterable of int\n            Assets to retrieve.\n        default_none : bool\n            If True, return None for failed lookups.\n            If False, raise `SidsNotFound`.\n\n        Returns\n        -------\n        assets : list[Asset or None]\n            A list of the same length as `sids` containing Assets (or Nones)\n            corresponding to the requested sids.\n\n        Raises\n        ------\n        SidsNotFound\n            When a requested sid is not found and default_none=False.\n        \"\"\"\n        sids = list(sids)\n        hits, missing, failures = {}, set(), []\n        for sid in sids:\n            try:\n                asset = self._asset_cache[sid]\n                if not default_none and asset is None:\n                    # Bail early if we've already cached that we don't know\n                    # about an asset.\n                    raise SidsNotFound(sids=[sid])\n                hits[sid] = asset\n            except KeyError:\n                missing.add(sid)\n\n        # All requests were cache hits.  Return requested sids in order.\n        if not missing:\n            return [hits[sid] for sid in sids]\n\n        update_hits = hits.update\n\n        # Look up cache misses by type.\n        type_to_assets = self.group_by_type(missing)\n\n        # Handle failures\n        failures = {failure: None for failure in type_to_assets.pop(None, ())}\n        update_hits(failures)\n        self._asset_cache.update(failures)\n\n        if failures and not default_none:\n            raise SidsNotFound(sids=list(failures))\n\n        # We don't update the asset cache here because it should already be\n        # updated by `self.retrieve_equities`.\n        update_hits(self.retrieve_equities(type_to_assets.pop('equity', ())))\n        update_hits(\n            self.retrieve_futures_contracts(type_to_assets.pop('future', ()))\n        )\n\n        # We shouldn't know about any other asset types.\n        if type_to_assets:\n            raise AssertionError(\n                \"Found asset types: %s\" % list(type_to_assets.keys())\n            )\n\n        return [hits[sid] for sid in sids]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a select statement that returns the most recent symbol for a set of sids.", "response": "def _select_most_recent_symbols_chunk(self, sid_group):\n        \"\"\"Retrieve the most recent symbol for a set of sids.\n\n        Parameters\n        ----------\n        sid_group : iterable[int]\n            The sids to lookup. The length of this sequence must be less than\n            or equal to SQLITE_MAX_VARIABLE_NUMBER because the sids will be\n            passed in as sql bind params.\n\n        Returns\n        -------\n        sel : Selectable\n            The sqlalchemy selectable that will query for the most recent\n            symbol for each sid.\n\n        Notes\n        -----\n        This is implemented as an inner select of the columns of interest\n        ordered by the end date of the (sid, symbol) mapping. We then group\n        that inner select on the sid with no aggregations to select the last\n        row per group which gives us the most recently active symbol for all\n        of the sids.\n        \"\"\"\n        cols = self.equity_symbol_mappings.c\n\n        # These are the columns we actually want.\n        data_cols = (cols.sid,) + tuple(cols[name] for name in symbol_columns)\n\n        # Also select the max of end_date so that all non-grouped fields take\n        # on the value associated with the max end_date. The SQLite docs say\n        # this:\n        #\n        # When the min() or max() aggregate functions are used in an aggregate\n        # query, all bare columns in the result set take values from the input\n        # row which also contains the minimum or maximum. Only the built-in\n        # min() and max() functions work this way.\n        #\n        # See https://www.sqlite.org/lang_select.html#resultset, for more info.\n        to_select = data_cols + (sa.func.max(cols.end_date),)\n\n        return sa.select(\n            to_select,\n        ).where(\n            cols.sid.in_(map(int, sid_group))\n        ).group_by(\n            cols.sid,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _lookup_symbol_strict(self,\n                              ownership_map,\n                              multi_country,\n                              symbol,\n                              as_of_date):\n        \"\"\"\n        Resolve a symbol to an asset object without fuzzy matching.\n\n        Parameters\n        ----------\n        ownership_map : dict[(str, str), list[OwnershipPeriod]]\n            The mapping from split symbols to ownership periods.\n        multi_country : bool\n            Does this mapping span multiple countries?\n        symbol : str\n            The symbol to look up.\n        as_of_date : datetime or None\n            If multiple assets have held this sid, which day should the\n            resolution be checked against? If this value is None and multiple\n            sids have held the ticker, then a MultipleSymbolsFound error will\n            be raised.\n\n        Returns\n        -------\n        asset : Asset\n            The asset that held the given symbol.\n\n        Raises\n        ------\n        SymbolNotFound\n            Raised when the symbol or symbol as_of_date pair do not map to\n            any assets.\n        MultipleSymbolsFound\n            Raised when multiple assets held the symbol. This happens if\n            multiple assets held the symbol at disjoint times and\n            ``as_of_date`` is None, or if multiple assets held the symbol at\n            the same time and``multi_country`` is True.\n\n        Notes\n        -----\n        The resolution algorithm is as follows:\n\n        - Split the symbol into the company and share class component.\n        - Do a dictionary lookup of the\n          ``(company_symbol, share_class_symbol)`` in the provided ownership\n          map.\n        - If there is no entry in the dictionary, we don't know about this\n          symbol so raise a ``SymbolNotFound`` error.\n        - If ``as_of_date`` is None:\n          - If more there is more than one owner, raise\n            ``MultipleSymbolsFound``\n          - Otherwise, because the list mapped to a symbol cannot be empty,\n            return the single asset.\n        - Iterate through all of the owners:\n          - If the ``as_of_date`` is between the start and end of the ownership\n            period:\n            - If multi_country is False, return the found asset.\n            - Otherwise, put the asset in a list.\n        - At the end of the loop, if there are no candidate assets, raise a\n          ``SymbolNotFound``.\n        - If there is exactly one candidate, return it.\n        - Othewise, raise ``MultipleSymbolsFound`` because the ticker is not\n          unique across countries.\n        \"\"\"\n        # split the symbol into the components, if there are no\n        # company/share class parts then share_class_symbol will be empty\n        company_symbol, share_class_symbol = split_delimited_symbol(symbol)\n        try:\n            owners = ownership_map[company_symbol, share_class_symbol]\n            assert owners, 'empty owners list for %r' % symbol\n        except KeyError:\n            # no equity has ever held this symbol\n            raise SymbolNotFound(symbol=symbol)\n\n        if not as_of_date:\n            # exactly one equity has ever held this symbol, we may resolve\n            # without the date\n            if len(owners) == 1:\n                return self.retrieve_asset(owners[0].sid)\n\n            options = {self.retrieve_asset(owner.sid) for owner in owners}\n\n            if multi_country:\n                country_codes = map(attrgetter('country_code'), options)\n\n                if len(set(country_codes)) > 1:\n                    raise SameSymbolUsedAcrossCountries(\n                        symbol=symbol,\n                        options=dict(zip(country_codes, options))\n                    )\n\n            # more than one equity has held this ticker, this\n            # is ambiguous without the date\n            raise MultipleSymbolsFound(symbol=symbol, options=options)\n\n        options = []\n        country_codes = []\n        for start, end, sid, _ in owners:\n            if start <= as_of_date < end:\n                # find the equity that owned it on the given asof date\n                asset = self.retrieve_asset(sid)\n\n                # if this asset owned the symbol on this asof date and we are\n                # only searching one country, return that asset\n                if not multi_country:\n                    return asset\n                else:\n                    options.append(asset)\n                    country_codes.append(asset.country_code)\n\n        if not options:\n            # no equity held the ticker on the given asof date\n            raise SymbolNotFound(symbol=symbol)\n\n        # if there is one valid option given the asof date, return that option\n        if len(options) == 1:\n            return options[0]\n\n        # if there's more than one option given the asof date, a country code\n        # must be passed to resolve the symbol to an asset\n        raise SameSymbolUsedAcrossCountries(\n            symbol=symbol,\n            options=dict(zip(country_codes, options))\n        )", "response": "Internal method that does a strict lookup of a given symbol in the asset object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lookup_symbol(self,\n                      symbol,\n                      as_of_date,\n                      fuzzy=False,\n                      country_code=None):\n        \"\"\"Lookup an equity by symbol.\n\n        Parameters\n        ----------\n        symbol : str\n            The ticker symbol to resolve.\n        as_of_date : datetime or None\n            Look up the last owner of this symbol as of this datetime.\n            If ``as_of_date`` is None, then this can only resolve the equity\n            if exactly one equity has ever owned the ticker.\n        fuzzy : bool, optional\n            Should fuzzy symbol matching be used? Fuzzy symbol matching\n            attempts to resolve differences in representations for\n            shareclasses. For example, some people may represent the ``A``\n            shareclass of ``BRK`` as ``BRK.A``, where others could write\n            ``BRK_A``.\n        country_code : str or None, optional\n            The country to limit searches to. If not provided, the search will\n            span all countries which increases the likelihood of an ambiguous\n            lookup.\n\n        Returns\n        -------\n        equity : Equity\n            The equity that held ``symbol`` on the given ``as_of_date``, or the\n            only equity to hold ``symbol`` if ``as_of_date`` is None.\n\n        Raises\n        ------\n        SymbolNotFound\n            Raised when no equity has ever held the given symbol.\n        MultipleSymbolsFound\n            Raised when no ``as_of_date`` is given and more than one equity\n            has held ``symbol``. This is also raised when ``fuzzy=True`` and\n            there are multiple candidates for the given ``symbol`` on the\n            ``as_of_date``. Also raised when no ``country_code`` is given and\n            the symbol is ambiguous across multiple countries.\n        \"\"\"\n        if symbol is None:\n            raise TypeError(\"Cannot lookup asset for symbol of None for \"\n                            \"as of date %s.\" % as_of_date)\n\n        if fuzzy:\n            f = self._lookup_symbol_fuzzy\n            mapping = self._choose_fuzzy_symbol_ownership_map(country_code)\n        else:\n            f = self._lookup_symbol_strict\n            mapping = self._choose_symbol_ownership_map(country_code)\n\n        if mapping is None:\n            raise SymbolNotFound(symbol=symbol)\n        return f(\n            mapping,\n            country_code is None,\n            symbol,\n            as_of_date,\n        )", "response": "Lookup an asset by symbol."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of all possible equities for the given ticker symbols.", "response": "def lookup_symbols(self,\n                       symbols,\n                       as_of_date,\n                       fuzzy=False,\n                       country_code=None):\n        \"\"\"\n        Lookup a list of equities by symbol.\n\n        Equivalent to::\n\n            [finder.lookup_symbol(s, as_of, fuzzy) for s in symbols]\n\n        but potentially faster because repeated lookups are memoized.\n\n        Parameters\n        ----------\n        symbols : sequence[str]\n            Sequence of ticker symbols to resolve.\n        as_of_date : pd.Timestamp\n            Forwarded to ``lookup_symbol``.\n        fuzzy : bool, optional\n            Forwarded to ``lookup_symbol``.\n        country_code : str or None, optional\n            The country to limit searches to. If not provided, the search will\n            span all countries which increases the likelihood of an ambiguous\n            lookup.\n\n        Returns\n        -------\n        equities : list[Equity]\n        \"\"\"\n        if not symbols:\n            return []\n\n        multi_country = country_code is None\n        if fuzzy:\n            f = self._lookup_symbol_fuzzy\n            mapping = self._choose_fuzzy_symbol_ownership_map(country_code)\n        else:\n            f = self._lookup_symbol_strict\n            mapping = self._choose_symbol_ownership_map(country_code)\n\n        if mapping is None:\n            raise SymbolNotFound(symbol=symbols[0])\n\n        memo = {}\n        out = []\n        append_output = out.append\n        for sym in symbols:\n            if sym in memo:\n                append_output(memo[sym])\n            else:\n                equity = memo[sym] = f(\n                    mapping,\n                    multi_country,\n                    sym,\n                    as_of_date,\n                )\n                append_output(equity)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_supplementary_field(self, sid, field_name, as_of_date):\n        try:\n            periods = self.equity_supplementary_map_by_sid[\n                field_name,\n                sid,\n            ]\n            assert periods, 'empty periods list for %r' % (field_name, sid)\n        except KeyError:\n            raise NoValueForSid(field=field_name, sid=sid)\n\n        if not as_of_date:\n            if len(periods) > 1:\n                # This equity has held more than one value, this is ambigious\n                # without the date\n                raise MultipleValuesFoundForSid(\n                    field=field_name,\n                    sid=sid,\n                    options={p.value for p in periods},\n                )\n            # this equity has only ever held this value, we may resolve\n            # without the date\n            return periods[0].value\n\n        for start, end, _, value in periods:\n            if start <= as_of_date < end:\n                return value\n\n        # Could not find a value for this sid on the as_of_date.\n        raise NoValueForSid(field=field_name, sid=sid)", "response": "Get the value of a supplementary field for an asset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert asset_convertible to an asset. On success, append to matches. On failure, append to missing.", "response": "def _lookup_generic_scalar(self,\n                               obj,\n                               as_of_date,\n                               country_code,\n                               matches,\n                               missing):\n        \"\"\"\n        Convert asset_convertible to an asset.\n\n        On success, append to matches.\n        On failure, append to missing.\n        \"\"\"\n        result = self._lookup_generic_scalar_helper(\n            obj, as_of_date, country_code,\n        )\n        if result is not None:\n            matches.append(result)\n        else:\n            missing.append(obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup_generic(self, obj, as_of_date, country_code):\n        matches = []\n        missing = []\n\n        # Interpret input as scalar.\n        if isinstance(obj, (AssetConvertible, ContinuousFuture)):\n            self._lookup_generic_scalar(\n                obj=obj,\n                as_of_date=as_of_date,\n                country_code=country_code,\n                matches=matches,\n                missing=missing,\n            )\n            try:\n                return matches[0], missing\n            except IndexError:\n                if hasattr(obj, '__int__'):\n                    raise SidsNotFound(sids=[obj])\n                else:\n                    raise SymbolNotFound(symbol=obj)\n\n        # Interpret input as iterable.\n        try:\n            iterator = iter(obj)\n        except TypeError:\n            raise NotAssetConvertible(\n                \"Input was not a AssetConvertible \"\n                \"or iterable of AssetConvertible.\"\n            )\n\n        for obj in iterator:\n            self._lookup_generic_scalar(\n                obj=obj,\n                as_of_date=as_of_date,\n                country_code=country_code,\n                matches=matches,\n                missing=missing,\n            )\n\n        return matches, missing", "response": "This method is used to convert an object into an Asset or sequence of Assets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute and cache a recarray of asset lifetimes.", "response": "def _compute_asset_lifetimes(self, country_codes):\n        \"\"\"\n        Compute and cache a recarray of asset lifetimes.\n        \"\"\"\n        equities_cols = self.equities.c\n        if country_codes:\n            buf = np.array(\n                tuple(\n                    sa.select((\n                        equities_cols.sid,\n                        equities_cols.start_date,\n                        equities_cols.end_date,\n                    )).where(\n                        (self.exchanges.c.exchange == equities_cols.exchange) &\n                        (self.exchanges.c.country_code.in_(country_codes))\n                    ).execute(),\n                ),\n                dtype='f8',  # use doubles so we get NaNs\n            )\n        else:\n            buf = np.array([], dtype='f8')\n\n        lifetimes = np.recarray(\n            buf=buf,\n            shape=(len(buf),),\n            dtype=[\n                ('sid', 'f8'),\n                ('start', 'f8'),\n                ('end', 'f8')\n            ],\n        )\n        start = lifetimes.start\n        end = lifetimes.end\n        start[np.isnan(start)] = 0  # convert missing starts to 0\n        end[np.isnan(end)] = np.iinfo(int).max  # convert missing end to INTMAX\n        # Cast the results back down to int.\n        return lifetimes.astype([\n            ('sid', 'i8'),\n            ('start', 'i8'),\n            ('end', 'i8'),\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lifetimes(self, dates, include_start_date, country_codes):\n        if isinstance(country_codes, string_types):\n            raise TypeError(\n                \"Got string {!r} instead of an iterable of strings in \"\n                \"AssetFinder.lifetimes.\".format(country_codes),\n            )\n\n        # normalize to a cache-key so that we can memoize results.\n        country_codes = frozenset(country_codes)\n\n        lifetimes = self._asset_lifetimes.get(country_codes)\n        if lifetimes is None:\n            self._asset_lifetimes[country_codes] = lifetimes = (\n                self._compute_asset_lifetimes(country_codes)\n            )\n\n        raw_dates = as_column(dates.asi8)\n        if include_start_date:\n            mask = lifetimes.start <= raw_dates\n        else:\n            mask = lifetimes.start < raw_dates\n        mask &= (raw_dates <= lifetimes.end)\n\n        return pd.DataFrame(mask, index=dates, columns=lifetimes.sid)", "response": "Compute a DataFrame representing asset lifetimes for the specified dates and country codes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all of the sids for a given country.", "response": "def equities_sids_for_country_code(self, country_code):\n        \"\"\"Return all of the sids for a given country.\n\n        Parameters\n        ----------\n        country_code : str\n            An ISO 3166 alpha-2 country code.\n\n        Returns\n        -------\n        tuple[int]\n            The sids whose exchanges are in this country.\n        \"\"\"\n        sids = self._compute_asset_lifetimes([country_code]).sid\n        return tuple(sids.tolist())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_raw_arrays(self, columns, start_date, end_date, assets):\n        rolls_by_asset = {}\n        for asset in assets:\n            rf = self._roll_finders[asset.roll_style]\n            rolls_by_asset[asset] = rf.get_rolls(\n                asset.root_symbol,\n                start_date,\n                end_date,\n                asset.offset\n            )\n\n        num_sessions = len(\n            self.trading_calendar.sessions_in_range(start_date, end_date)\n        )\n        shape = num_sessions, len(assets)\n\n        results = []\n\n        tc = self._bar_reader.trading_calendar\n        sessions = tc.sessions_in_range(start_date, end_date)\n\n        # Get partitions\n        partitions_by_asset = {}\n        for asset in assets:\n            partitions = []\n            partitions_by_asset[asset] = partitions\n\n            rolls = rolls_by_asset[asset]\n            start = start_date\n\n            for roll in rolls:\n                sid, roll_date = roll\n                start_loc = sessions.get_loc(start)\n\n                if roll_date is not None:\n                    end = roll_date - sessions.freq\n                    end_loc = sessions.get_loc(end)\n                else:\n                    end = end_date\n                    end_loc = len(sessions) - 1\n\n                partitions.append((sid, start, end, start_loc, end_loc))\n\n                if roll_date is not None:\n                    start = sessions[end_loc + 1]\n\n        for column in columns:\n            if column != 'volume' and column != 'sid':\n                out = np.full(shape, np.nan)\n            else:\n                out = np.zeros(shape, dtype=np.int64)\n\n            for i, asset in enumerate(assets):\n                partitions = partitions_by_asset[asset]\n\n                for sid, start, end, start_loc, end_loc in partitions:\n                    if column != 'sid':\n                        result = self._bar_reader.load_raw_arrays(\n                            [column], start, end, [sid])[0][:, 0]\n                    else:\n                        result = int(sid)\n                    out[start_loc:end_loc + 1, i] = result\n\n            results.append(out)\n\n        return results", "response": "Loads the raw data for the given columns and assets into a list of np. ndarrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the value at the given coordinates.", "response": "def get_value(self, continuous_future, dt, field):\n        \"\"\"\n        Retrieve the value at the given coordinates.\n\n        Parameters\n        ----------\n        sid : int\n            The asset identifier.\n        dt : pd.Timestamp\n            The timestamp for the desired data point.\n        field : string\n            The OHLVC name for the desired data point.\n\n        Returns\n        -------\n        value : float|int\n            The value at the given coordinates, ``float`` for OHLC, ``int``\n            for 'volume'.\n\n        Raises\n        ------\n        NoDataOnDate\n            If the given dt is not a valid market minute (in minute mode) or\n            session (in daily mode) according to this reader's tradingcalendar.\n        \"\"\"\n        rf = self._roll_finders[continuous_future.roll_style]\n        sid = (rf.get_contract_center(continuous_future.root_symbol,\n                                      dt,\n                                      continuous_future.offset))\n        return self._bar_reader.get_value(sid, dt, field)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the latest minute on or before dt in which asset traded.", "response": "def get_last_traded_dt(self, asset, dt):\n        \"\"\"\n        Get the latest minute on or before ``dt`` in which ``asset`` traded.\n\n        If there are no trades on or before ``dt``, returns ``pd.NaT``.\n\n        Parameters\n        ----------\n        asset : zipline.asset.Asset\n            The asset for which to get the last traded minute.\n        dt : pd.Timestamp\n            The minute at which to start searching for the last traded minute.\n\n        Returns\n        -------\n        last_traded : pd.Timestamp\n            The dt of the last trade for the given asset, using the input\n            dt as a vantage point.\n        \"\"\"\n        rf = self._roll_finders[asset.roll_style]\n        sid = (rf.get_contract_center(asset.root_symbol,\n                                      dt,\n                                      asset.offset))\n        if sid is None:\n            return pd.NaT\n        contract = rf.asset_finder.retrieve_asset(sid)\n        return self._bar_reader.get_last_traded_dt(contract, dt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_raw_arrays(self, columns, start_date, end_date, assets):\n        rolls_by_asset = {}\n\n        tc = self.trading_calendar\n        start_session = tc.minute_to_session_label(start_date)\n        end_session = tc.minute_to_session_label(end_date)\n\n        for asset in assets:\n            rf = self._roll_finders[asset.roll_style]\n            rolls_by_asset[asset] = rf.get_rolls(\n                asset.root_symbol,\n                start_session,\n                end_session, asset.offset)\n\n        sessions = tc.sessions_in_range(start_date, end_date)\n\n        minutes = tc.minutes_in_range(start_date, end_date)\n        num_minutes = len(minutes)\n        shape = num_minutes, len(assets)\n\n        results = []\n\n        # Get partitions\n        partitions_by_asset = {}\n        for asset in assets:\n            partitions = []\n            partitions_by_asset[asset] = partitions\n            rolls = rolls_by_asset[asset]\n            start = start_date\n            for roll in rolls:\n                sid, roll_date = roll\n                start_loc = minutes.searchsorted(start)\n                if roll_date is not None:\n                    _, end = tc.open_and_close_for_session(\n                        roll_date - sessions.freq)\n                    end_loc = minutes.searchsorted(end)\n                else:\n                    end = end_date\n                    end_loc = len(minutes) - 1\n                partitions.append((sid, start, end, start_loc, end_loc))\n                if roll[-1] is not None:\n                    start, _ = tc.open_and_close_for_session(\n                        tc.minute_to_session_label(minutes[end_loc + 1]))\n\n        for column in columns:\n            if column != 'volume':\n                out = np.full(shape, np.nan)\n            else:\n                out = np.zeros(shape, dtype=np.uint32)\n            for i, asset in enumerate(assets):\n                partitions = partitions_by_asset[asset]\n                for sid, start, end, start_loc, end_loc in partitions:\n                    if column != 'sid':\n                        result = self._bar_reader.load_raw_arrays(\n                            [column], start, end, [sid])[0][:, 0]\n                    else:\n                        result = int(sid)\n                    out[start_loc:end_loc + 1, i] = result\n            results.append(out)\n        return results", "response": "Loads the raw data for the given columns and assets into a list of np. ndarrays."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef current_portfolio_weights(self):\n        position_values = pd.Series({\n            asset: (\n                    position.last_sale_price *\n                    position.amount *\n                    asset.price_multiplier\n            )\n            for asset, position in self.positions.items()\n        })\n        return position_values / self.portfolio_value", "response": "Compute the weight of each asset in the portfolio by calculating its held\n        value divided by the total value of all positions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __hosting_wechat_img(self, content_info, hosting_callback):\n        assert callable(hosting_callback)\n\n        content_img_list = content_info.pop(\"content_img_list\")\n        content_html = content_info.pop(\"content_html\")\n        for idx, img_url in enumerate(content_img_list):\n            hosting_img_url = hosting_callback(img_url)\n            if not hosting_img_url:\n                # todo \u5b9a\u4e49\u6807\u51c6\u5f02\u5e38\n                raise Exception()\n            content_img_list[idx] = hosting_img_url\n            content_html = content_html.replace(img_url, hosting_img_url)\n\n        return dict(content_img_list=content_img_list, content_html=content_html)", "response": "This method is used to create a new wechat image."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_gzh_info(self, wecgat_id_or_name, unlock_callback=None, identify_image_callback=None, decode_url=True):\n        info = self.search_gzh(wecgat_id_or_name, 1, unlock_callback, identify_image_callback, decode_url)\n        try:\n            return next(info)\n        except StopIteration:\n            return None", "response": "Returns a dict of information about a wechat."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search_gzh(self, keyword, page=1, unlock_callback=None, identify_image_callback=None, decode_url=True):\n        url = WechatSogouRequest.gen_search_gzh_url(keyword, page)\n        session = requests.session()\n        resp = self.__get_by_unlock(url,\n                                    unlock_platform=self.__unlock_sogou,\n                                    unlock_callback=unlock_callback,\n                                    identify_image_callback=identify_image_callback,\n                                    session=session)\n        gzh_list = WechatSogouStructuring.get_gzh_by_search(resp.text)\n        for i in gzh_list:\n            if decode_url:\n                i['profile_url'] = self.__format_url(i['profile_url'], url, resp.text, unlock_callback=unlock_callback, identify_image_callback=identify_image_callback, session=session)\n            yield i", "response": "Search for a given keyword in the Wechatsogou database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching article by keyword.", "response": "def search_article(self, keyword, page=1, timesn=WechatSogouConst.search_article_time.anytime,\n                       article_type=WechatSogouConst.search_article_type.all, ft=None, et=None,\n                       unlock_callback=None,\n                       identify_image_callback=None,\n                       decode_url=True):\n        \"\"\"\u641c\u7d22 \u6587\u7ae0\n\n        \u5bf9\u4e8e\u51fa\u73b0\u9a8c\u8bc1\u7801\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u7531\u4f7f\u7528\u8005\u81ea\u5df1\u63d0\u4f9b\uff1a\n            1\u3001\u51fd\u6570 unlock_callback \uff0c\u8fd9\u4e2a\u51fd\u6570 handle \u51fa\u73b0\u9a8c\u8bc1\u7801\u5230\u89e3\u51b3\u7684\u6574\u4e2a\u6d41\u7a0b\n            2\u3001\u4e5f\u53ef\u4ee5 \u53ea\u63d0\u4f9b\u51fd\u6570 identify_image_callback\uff0c\u8fd9\u4e2a\u51fd\u6570\u8f93\u5165\u9a8c\u8bc1\u7801\u4e8c\u8fdb\u5236\u6570\u636e\uff0c\u8f93\u51fa\u9a8c\u8bc1\u7801\u6587\u5b57\uff0c\u5269\u4e0b\u7684\u7531 wechatsogou \u5305\u6765\u89e3\u51b3\n        \u6ce8\u610f\uff1a\n            \u51fd\u6570 unlock_callback \u548c identify_image_callback \u53ea\u9700\u8981\u63d0\u4f9b\u4e00\u4e2a\uff0c\u5982\u679c\u90fd\u63d0\u4f9b\u4e86\uff0c\u90a3\u4e48 identify_image_callback \u4e0d\u8d77\u4f5c\u7528\n\n        Parameters\n        ----------\n        keyword : str or unicode\n            \u641c\u7d22\u6587\u5b57\n        page : int, optional\n            \u9875\u6570 the default is 1\n        timesn : WechatSogouConst.search_article_time\n            \u65f6\u95f4 anytime \u6ca1\u6709\u9650\u5236 / day \u4e00\u5929 / week \u4e00\u5468 / month \u4e00\u6708 / year \u4e00\u5e74 / specific \u81ea\u5b9a\n            the default is anytime\n        article_type : WechatSogouConst.search_article_type\n            \u542b\u6709\u5185\u5bb9\u7684\u7c7b\u578b image \u6709\u56fe / video \u6709\u89c6\u9891 / rich \u6709\u56fe\u548c\u89c6\u9891 / all \u5565\u90fd\u6709\n        ft, et : datetime.date or None\n            \u5f53 tsn \u662f specific \u65f6\uff0cft \u4ee3\u8868\u5f00\u59cb\u65f6\u95f4\uff0c\u5982\uff1a 2017-07-01\n            \u5f53 tsn \u662f specific \u65f6\uff0cet \u4ee3\u8868\u7ed3\u675f\u65f6\u95f4\uff0c\u5982\uff1a 2017-07-15\n        unlock_callback : callable\n            \u5904\u7406\u51fa\u73b0\u9a8c\u8bc1\u7801\u9875\u9762\u7684\u51fd\u6570\uff0c\u53c2\u89c1 unlock_callback_example\n        identify_image_callback : callable\n            \u5904\u7406\u9a8c\u8bc1\u7801\u51fd\u6570\uff0c\u8f93\u5165\u9a8c\u8bc1\u7801\u4e8c\u8fdb\u5236\u6570\u636e\uff0c\u8f93\u51fa\u6587\u5b57\uff0c\u53c2\u89c1 identify_image_callback_example\n        decode_url : bool\n            \u662f\u5426\u89e3\u6790 url\n\n        Returns\n        -------\n        list[dict]\n            {\n                'article': {\n                    'title': '',  # \u6587\u7ae0\u6807\u9898\n                    'url': '',  # \u6587\u7ae0\u94fe\u63a5\n                    'imgs': '',  # \u6587\u7ae0\u56fe\u7247list\n                    'abstract': '',  # \u6587\u7ae0\u6458\u8981\n                    'time': ''  # \u6587\u7ae0\u63a8\u9001\u65f6\u95f4\n                },\n                'gzh': {\n                    'profile_url': '',  # \u516c\u4f17\u53f7\u6700\u8fd110\u6761\u7fa4\u53d1\u9875\u94fe\u63a5\n                    'headimage': '',  # \u5934\u50cf\n                    'wechat_name': '',  # \u540d\u79f0\n                    'isv': '',  # \u662f\u5426\u52a0v\n                }\n            }\n\n        Raises\n        ------\n        WechatSogouRequestsException\n            requests error\n        \"\"\"\n        url = WechatSogouRequest.gen_search_article_url(keyword, page, timesn, article_type, ft, et)\n        session = requests.session()\n        resp = self.__get_by_unlock(url, WechatSogouRequest.gen_search_article_url(keyword),\n                                    unlock_platform=self.__unlock_sogou,\n                                    unlock_callback=unlock_callback,\n                                    identify_image_callback=identify_image_callback,\n                                    session=session)\n\n        article_list = WechatSogouStructuring.get_article_by_search(resp.text)\n        for i in article_list:\n            if decode_url:\n                i['article']['url'] = self.__format_url(i['article']['url'], url, resp.text, unlock_callback=unlock_callback, identify_image_callback=identify_image_callback, session=session)\n                i['gzh']['profile_url'] = self.__format_url(i['gzh']['profile_url'], url, resp.text, unlock_callback=unlock_callback, identify_image_callback=identify_image_callback, session=session)\n            yield i"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a list of all wechat s articles by a given hot index.", "response": "def get_gzh_article_by_hot(self, hot_index, page=1, unlock_callback=None, identify_image_callback=None):\n        \"\"\"\u83b7\u53d6 \u9996\u9875\u70ed\u95e8\u6587\u7ae0\n\n        Parameters\n        ----------\n        hot_index : WechatSogouConst.hot_index\n            \u9996\u9875\u70ed\u95e8\u6587\u7ae0\u7684\u5206\u7c7b\uff08\u5e38\u91cf\uff09\uff1aWechatSogouConst.hot_index.xxx\n        page : int\n            \u9875\u6570\n\n        Returns\n        -------\n        list[dict]\n            {\n                'gzh': {\n                    'headimage': str,  # \u516c\u4f17\u53f7\u5934\u50cf\n                    'wechat_name': str,  # \u516c\u4f17\u53f7\u540d\u79f0\n                },\n                'article': {\n                    'url': str,  # \u6587\u7ae0\u4e34\u65f6\u94fe\u63a5\n                    'title': str,  # \u6587\u7ae0\u6807\u9898\n                    'abstract': str,  # \u6587\u7ae0\u6458\u8981\n                    'time': int,  # \u63a8\u9001\u65f6\u95f4\uff0c10\u4f4d\u65f6\u95f4\u6233\n                    'open_id': str,  # open id\n                    'main_img': str  # \u5c01\u9762\u56fe\u7247\n                }\n            }\n        \"\"\"\n\n        assert hasattr(WechatSogouConst.hot_index, hot_index)\n        assert isinstance(page, int) and page > 0\n\n        url = WechatSogouRequest.gen_hot_url(hot_index, page)\n        resp = self.__get_by_unlock(url,\n                                    unlock_platform=self.__unlock_sogou,\n                                    unlock_callback=unlock_callback,\n                                    identify_image_callback=identify_image_callback)\n\n        resp.encoding = 'utf-8'\n        return WechatSogouStructuring.get_gzh_article_by_hot(resp.text)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_article_content(self, url, del_qqmusic=True, del_mpvoice=True, unlock_callback=None,\n                            identify_image_callback=None, hosting_callback=None, raw=False):\n        \"\"\"\u83b7\u53d6\u6587\u7ae0\u539f\u6587\uff0c\u907f\u514d\u4e34\u65f6\u94fe\u63a5\u5931\u6548\n\n        Parameters\n        ----------\n        url : str or unicode\n            \u539f\u6587\u94fe\u63a5\uff0c\u4e34\u65f6\u94fe\u63a5\n        raw : bool\n            True: \u8fd4\u56de\u539f\u59cbhtml\n            False: \u8fd4\u56de\u5904\u7406\u540e\u7684html\n        del_qqmusic: bool\n            True:\u5fae\u4fe1\u539f\u6587\u4e2d\u6709\u63d2\u5165\u7684qq\u97f3\u4e50\uff0c\u5219\u5220\u9664\n            False:\u5fae\u4fe1\u6e90\u6587\u4e2d\u6709\u63d2\u5165\u7684qq\u97f3\u4e50\uff0c\u5219\u4fdd\u7559\n        del_mpvoice: bool\n            True:\u5fae\u4fe1\u539f\u6587\u4e2d\u6709\u63d2\u5165\u7684\u8bed\u97f3\u6d88\u606f\uff0c\u5219\u5220\u9664\n            False:\u5fae\u4fe1\u6e90\u6587\u4e2d\u6709\u63d2\u5165\u7684\u8bed\u97f3\u6d88\u606f\uff0c\u5219\u4fdd\u7559\n        unlock_callback : callable\n            \u5904\u7406 \u6587\u7ae0\u660e\u7ec6 \u7684\u65f6\u5019\u51fa\u73b0\u9a8c\u8bc1\u7801\u7684\u51fd\u6570\uff0c\u53c2\u89c1 unlock_callback_example\n        identify_image_callback : callable\n            \u5904\u7406 \u6587\u7ae0\u660e\u7ec6 \u7684\u65f6\u5019\u5904\u7406\u9a8c\u8bc1\u7801\u51fd\u6570\uff0c\u8f93\u5165\u9a8c\u8bc1\u7801\u4e8c\u8fdb\u5236\u6570\u636e\uff0c\u8f93\u51fa\u6587\u5b57\uff0c\u53c2\u89c1 identify_image_callback_example\n        hosting_callback: callable\n            \u5c06\u5fae\u4fe1\u91c7\u96c6\u7684\u6587\u7ae0\u6258\u7ba1\u52307\u725b\u6216\u8005\u963f\u91cc\u4e91\u56de\u8c03\u51fd\u6570\uff0c\u8f93\u5165\u5fae\u4fe1\u56fe\u7247\u6e90\u5730\u5740\uff0c\u8fd4\u56de\u6258\u7ba1\u540e\u5730\u5740\n\n        Returns\n        -------\n        content_html\n            \u539f\u6587\u5185\u5bb9\n        content_img_list\n            \u6587\u7ae0\u4e2d\u56fe\u7247\u5217\u8868\n\n        Raises\n        ------\n        WechatSogouRequestsException\n        \"\"\"\n\n        resp = self.__get_by_unlock(url,\n                                    unlock_platform=self.__unlock_wechat,\n                                    unlock_callback=unlock_callback,\n                                    identify_image_callback=identify_image_callback)\n\n        resp.encoding = 'utf-8'\n        if '\u94fe\u63a5\u5df2\u8fc7\u671f' in resp.text:\n            raise WechatSogouException('get_article_content \u94fe\u63a5 [{}] \u5df2\u8fc7\u671f'.format(url))\n        if raw:\n            return resp.text\n        content_info = WechatSogouStructuring.get_article_detail(resp.text, del_qqmusic=del_qqmusic,\n                                                                 del_voice=del_mpvoice)\n        if hosting_callback:\n            content_info = self.__hosting_wechat_img(content_info, hosting_callback)\n        return content_info", "response": "Get article content from url."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a list of unique items from the Sogou Sugg.", "response": "def get_sugg(self, keyword):\n        \"\"\"\u83b7\u53d6\u5fae\u4fe1\u641c\u72d7\u641c\u7d22\u5173\u952e\u8bcd\u8054\u60f3\n\n        Parameters\n        ----------\n        keyword : str or unicode\n            \u5173\u952e\u8bcd\n\n        Returns\n        -------\n        list[str]\n            \u8054\u60f3\u5173\u952e\u8bcd\u5217\u8868\n\n        Raises\n        ------\n        WechatSogouRequestsException\n        \"\"\"\n        url = 'http://w.sugg.sogou.com/sugg/ajaj_json.jsp?key={}&type=wxpub&pr=web'.format(\n            quote(keyword.encode('utf-8')))\n        r = requests.get(url)\n        if not r.ok:\n            raise WechatSogouRequestsException('get_sugg', r)\n\n        sugg = re.findall(u'\\[\"' + keyword + '\",(.*?),\\[\"', r.text)[0]\n        return json.loads(sugg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unlock_sogou_callback_example(url, req, resp, img, identify_image_callback):\n    # no use resp\n    url_quote = url.split('weixin.sogou.com/')[-1]\n    unlock_url = 'http://weixin.sogou.com/antispider/thank.php'\n    data = {\n        'c': identify_image_callback(img),\n        'r': '%2F' + url_quote,\n        'v': 5\n    }\n    headers = {\n        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n        'Referer': 'http://weixin.sogou.com/antispider/?from=%2f' + url_quote\n    }\n    r_unlock = req.post(unlock_url, data, headers=headers)\n    r_unlock.encoding = 'utf-8'\n    if not r_unlock.ok:\n        raise WechatSogouVcodeOcrException(\n            'unlock[{}] failed: {}'.format(unlock_url, r_unlock.text, r_unlock.status_code))\n\n    return r_unlock.json()", "response": "Unlock the Sogou API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unlock_weixin_callback_example(url, req, resp, img, identify_image_callback):\n    # no use resp\n\n    unlock_url = 'https://mp.weixin.qq.com/mp/verifycode'\n    data = {\n        'cert': time.time() * 1000,\n        'input': identify_image_callback(img)\n    }\n    headers = {\n        'Host': 'mp.weixin.qq.com',\n        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n        'Referer': url\n    }\n    r_unlock = req.post(unlock_url, data, headers=headers)\n    if not r_unlock.ok:\n        raise WechatSogouVcodeOcrException(\n            'unlock[{}] failed: {}[{}]'.format(unlock_url, r_unlock.text, r_unlock.status_code))\n\n    return r_unlock.json()", "response": "unlock_weixin_callback_example - unlocks the url and returns the url"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates search article url.", "response": "def gen_search_article_url(keyword, page=1, timesn=WechatSogouConst.search_article_time.anytime,\n                               article_type=WechatSogouConst.search_article_type.all, ft=None, et=None):\n        \"\"\"\u62fc\u63a5\u641c\u7d22 \u6587\u7ae0 URL\n\n        Parameters\n        ----------\n        keyword : str or unicode\n            \u641c\u7d22\u6587\u5b57\n        page : int, optional\n            \u9875\u6570 the default is 1\n        timesn : WechatSogouConst.search_article_time\n            \u65f6\u95f4 anytime \u6ca1\u6709\u9650\u5236 / day \u4e00\u5929 / week \u4e00\u5468 / month \u4e00\u6708 / year \u4e00\u5e74 / specific \u81ea\u5b9a\n            \u9ed8\u8ba4\u662f anytime\n        article_type : WechatSogouConst.search_article_type\n            \u542b\u6709\u5185\u5bb9\u7684\u7c7b\u578b image \u6709\u56fe / video \u6709\u89c6\u9891 / rich \u6709\u56fe\u548c\u89c6\u9891 / all \u5565\u90fd\u6709\n            \u9ed8\u8ba4\u662f all\n        ft, et : datetime.date\n            \u5f53 tsn \u662f specific \u65f6\uff0cft \u4ee3\u8868\u5f00\u59cb\u65f6\u95f4\uff0c\u5982\uff1a 2017-07-01\n            \u5f53 tsn \u662f specific \u65f6\uff0cet \u4ee3\u8868\u7ed3\u675f\u65f6\u95f4\uff0c\u5982\uff1a 2017-07-15\n\n        Returns\n        -------\n        str\n            search_article_url\n        \"\"\"\n        assert isinstance(page, int) and page > 0\n        assert timesn in [WechatSogouConst.search_article_time.anytime,\n                          WechatSogouConst.search_article_time.day,\n                          WechatSogouConst.search_article_time.week,\n                          WechatSogouConst.search_article_time.month,\n                          WechatSogouConst.search_article_time.year,\n                          WechatSogouConst.search_article_time.specific]\n\n        if timesn == WechatSogouConst.search_article_time.specific:\n            assert isinstance(ft, datetime.date)\n            assert isinstance(et, datetime.date)\n            assert ft <= et\n        else:\n            ft = ''\n            et = ''\n\n        interation_image = 458754\n        interation_video = 458756\n        if article_type == WechatSogouConst.search_article_type.rich:\n            interation = '{},{}'.format(interation_image, interation_video)\n        elif article_type == WechatSogouConst.search_article_type.image:\n            interation = interation_image\n        elif article_type == WechatSogouConst.search_article_type.video:\n            interation = interation_video\n        else:\n            interation = ''\n\n        qs_dict = OrderedDict()\n        qs_dict['type'] = _search_type_article\n        qs_dict['page'] = page\n        qs_dict['ie'] = 'utf8'\n        qs_dict['query'] = keyword\n        qs_dict['interation'] = interation\n        if timesn != 0:\n            qs_dict['tsn'] = timesn\n            qs_dict['ft'] = str(ft)\n            qs_dict['et'] = str(et)\n\n        # TODO \u8d26\u53f7\u5185\u641c\u7d22\n        # '\u8d26\u53f7\u5185 http://weixin.sogou.com/weixin?type=2&ie=utf8&query=%E9%AB%98%E8%80%83&tsn=3&ft=&et=&interation=458754\n        # &wxid=oIWsFt1tmWoG6vO6BcsS7St61bRE&usip=nanhangqinggong'\n        # qs['wxid'] = wxid\n        # qs['usip'] = usip\n\n        return 'http://weixin.sogou.com/weixin?{}'.format(urlencode(qs_dict))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gen_search_gzh_url(keyword, page=1):\n        assert isinstance(page, int) and page > 0\n\n        qs_dict = OrderedDict()\n        qs_dict['type'] = _search_type_gzh\n        qs_dict['page'] = page\n        qs_dict['ie'] = 'utf8'\n        qs_dict['query'] = keyword\n\n        return 'http://weixin.sogou.com/weixin?{}'.format(urlencode(qs_dict))", "response": "Generate URL for search_gzh."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gen_hot_url(hot_index, page=1):\n\n        assert hasattr(WechatSogouConst.hot_index, hot_index)\n        assert isinstance(page, int) and page > 0\n\n        index_urls = {\n            WechatSogouConst.hot_index.hot: 0,  # \u70ed\u95e8\n            WechatSogouConst.hot_index.gaoxiao: 1,  # \u641e\u7b11\n            WechatSogouConst.hot_index.health: 2,  # \u517b\u751f\n            WechatSogouConst.hot_index.sifanghua: 3,  # \u79c1\u623f\u8bdd\n            WechatSogouConst.hot_index.gossip: 4,  # \u516b\u5366\n            WechatSogouConst.hot_index.technology: 5,  # \u79d1\u6280\n            WechatSogouConst.hot_index.finance: 6,  # \u8d22\u7ecf\n            WechatSogouConst.hot_index.car: 7,  # \u6c7d\u8f66\n            WechatSogouConst.hot_index.life: 8,  # \u751f\u6d3b\n            WechatSogouConst.hot_index.fashion: 9,  # \u65f6\u5c1a\n            WechatSogouConst.hot_index.mummy: 10,  # \u8fa3\u5988 / \u80b2\u513f\n            WechatSogouConst.hot_index.travel: 11,  # \u65c5\u884c\n            WechatSogouConst.hot_index.job: 12,  # \u804c\u573a\n            WechatSogouConst.hot_index.food: 13,  # \u7f8e\u98df\n            WechatSogouConst.hot_index.history: 14,  # \u5386\u53f2\n            WechatSogouConst.hot_index.study: 15,  # \u5b66\u9738 / \u6559\u80b2\n            WechatSogouConst.hot_index.constellation: 16,  # \u661f\u5ea7\n            WechatSogouConst.hot_index.sport: 17,  # \u4f53\u80b2\n            WechatSogouConst.hot_index.military: 18,  # \u519b\u4e8b\n            WechatSogouConst.hot_index.game: 19,  # \u6e38\u620f\n            WechatSogouConst.hot_index.pet: 20,  # \u840c\u5ba0\n\n        }\n        return 'http://weixin.sogou.com/wapindex/wap/0612/wap_{}/{}.html'.format(index_urls[hot_index], page - 1)", "response": "Generate URL for a specific hot index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_first_of_element(element, sub, contype=None):\n    content = element.xpath(sub)\n    return list_or_empty(content, contype)", "response": "Get first element in tree"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_encoding_from_reponse(r):\n    encoding = requests.utils.get_encodings_from_content(r.text)\n    return encoding[0] if encoding else requests.utils.get_encoding_from_headers(r.headers)", "response": "Get the encoding from the response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _replace_str_html(s):\n    html_str_list = [\n        ('&#39;', '\\''),\n        ('&quot;', '\"'),\n        ('&amp;', '&'),\n        ('&yen;', '\u00a5'),\n        ('amp;', ''),\n        ('&lt;', '<'),\n        ('&gt;', '>'),\n        ('&nbsp;', ' '),\n        ('\\\\', '')\n    ]\n    for i in html_str_list:\n        s = s.replace(i[0], i[1])\n    return s", "response": "Replace html with html"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_gzh_by_search(text):\n        post_view_perms = WechatSogouStructuring.__get_post_view_perm(text)\n\n        page = etree.HTML(text)\n        lis = page.xpath('//ul[@class=\"news-list2\"]/li')\n        relist = []\n        for li in lis:\n            url = get_first_of_element(li, 'div/div[1]/a/@href')\n            headimage = format_image_url(get_first_of_element(li, 'div/div[1]/a/img/@src'))\n            wechat_name = get_elem_text(get_first_of_element(li, 'div/div[2]/p[1]'))\n            info = get_elem_text(get_first_of_element(li, 'div/div[2]/p[2]'))\n            qrcode = get_first_of_element(li, 'div/div[3]/span/img[1]/@src')\n            introduction = get_elem_text(get_first_of_element(li, 'dl[1]/dd'))\n            authentication = get_first_of_element(li, 'dl[2]/dd/text()')\n\n            relist.append({\n                'open_id': headimage.split('/')[-1],\n                'profile_url': url,\n                'headimage': headimage,\n                'wechat_name': wechat_name.replace('red_beg', '').replace('red_end', ''),\n                'wechat_id': info.replace('\u5fae\u4fe1\u53f7\uff1a', ''),\n                'qrcode': qrcode,\n                'introduction': introduction.replace('red_beg', '').replace('red_end', ''),\n                'authentication': authentication,\n                'post_perm': -1,\n                'view_perm': -1,\n            })\n\n        if post_view_perms:\n            for i in relist:\n                if i['open_id'] in post_view_perms:\n                    post_view_perm = post_view_perms[i['open_id']].split(',')\n                    if len(post_view_perm) == 2:\n                        i['post_perm'] = int(post_view_perm[0])\n                        i['view_perm'] = int(post_view_perm[1])\n        return relist", "response": "Returns a list of dicts with the keys open_id profile_url wechat_name and view_perm."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of dicts containing the article and the article s content.", "response": "def get_article_by_search(text):\n        \"\"\"\u4ece\u641c\u7d22\u6587\u7ae0\u83b7\u5f97\u7684\u6587\u672c \u63d0\u53d6\u7ae0\u5217\u8868\u4fe1\u606f\n\n        Parameters\n        ----------\n        text : str or unicode\n            \u641c\u7d22\u6587\u7ae0\u83b7\u5f97\u7684\u6587\u672c\n\n        Returns\n        -------\n        list[dict]\n            {\n                'article': {\n                    'title': '',  # \u6587\u7ae0\u6807\u9898\n                    'url': '',  # \u6587\u7ae0\u94fe\u63a5\n                    'imgs': '',  # \u6587\u7ae0\u56fe\u7247list\n                    'abstract': '',  # \u6587\u7ae0\u6458\u8981\n                    'time': ''  # \u6587\u7ae0\u63a8\u9001\u65f6\u95f4\n                },\n                'gzh': {\n                    'profile_url': '',  # \u516c\u4f17\u53f7\u6700\u8fd110\u6761\u7fa4\u53d1\u9875\u94fe\u63a5\n                    'headimage': '',  # \u5934\u50cf\n                    'wechat_name': '',  # \u540d\u79f0\n                    'isv': '',  # \u662f\u5426\u52a0v\n                }\n            }\n        \"\"\"\n        page = etree.HTML(text)\n        lis = page.xpath('//ul[@class=\"news-list\"]/li')\n\n        articles = []\n        for li in lis:\n            url = get_first_of_element(li, 'div[1]/a/@href')\n            if url:\n                title = get_first_of_element(li, 'div[2]/h3/a')\n                imgs = li.xpath('div[1]/a/img/@src')\n                abstract = get_first_of_element(li, 'div[2]/p')\n                time = get_first_of_element(li, 'div[2]/div/span/script/text()')\n                gzh_info = li.xpath('div[2]/div/a')[0]\n            else:\n                url = get_first_of_element(li, 'div/h3/a/@href')\n                title = get_first_of_element(li, 'div/h3/a')\n                imgs = []\n                spans = li.xpath('div/div[1]/a')\n                for span in spans:\n                    img = span.xpath('span/img/@src')\n                    if img:\n                        imgs.append(img)\n                abstract = get_first_of_element(li, 'div/p')\n                time = get_first_of_element(li, 'div/div[2]/span/script/text()')\n                gzh_info = li.xpath('div/div[2]/a')[0]\n\n            if title is not None:\n                title = get_elem_text(title).replace(\"red_beg\", \"\").replace(\"red_end\", \"\")\n            if abstract is not None:\n                abstract = get_elem_text(abstract).replace(\"red_beg\", \"\").replace(\"red_end\", \"\")\n\n            time = re.findall('timeConvert\\(\\'(.*?)\\'\\)', time)\n            time = list_or_empty(time, int)\n            profile_url = get_first_of_element(gzh_info, '@href')\n            headimage = get_first_of_element(gzh_info, '@data-headimage')\n            wechat_name = get_first_of_element(gzh_info, 'text()')\n            gzh_isv = get_first_of_element(gzh_info, '@data-isv', int)\n\n            articles.append({\n                'article': {\n                    'title': title,\n                    'url': url,\n                    'imgs': format_image_url(imgs),\n                    'abstract': abstract,\n                    'time': time\n                },\n                'gzh': {\n                    'profile_url': profile_url,\n                    'headimage': headimage,\n                    'wechat_name': wechat_name,\n                    'isv': gzh_isv,\n                }\n            })\n        return articles"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn dict of all info from Zh Info by history.", "response": "def get_gzh_info_by_history(text):\n        \"\"\"\u4ece \u5386\u53f2\u6d88\u606f\u9875\u7684\u6587\u672c \u63d0\u53d6\u516c\u4f17\u53f7\u4fe1\u606f\n\n        Parameters\n        ----------\n        text : str or unicode\n            \u5386\u53f2\u6d88\u606f\u9875\u7684\u6587\u672c\n\n        Returns\n        -------\n        dict\n            {\n                'wechat_name': '',  # \u540d\u79f0\n                'wechat_id': '',  # \u5fae\u4fe1id\n                'introduction': '',  # \u63cf\u8ff0\n                'authentication': '',  # \u8ba4\u8bc1\n                'headimage': ''  # \u5934\u50cf\n            }\n        \"\"\"\n\n        page = etree.HTML(text)\n        profile_area = get_first_of_element(page, '//div[@class=\"profile_info_area\"]')\n\n        profile_img = get_first_of_element(profile_area, 'div[1]/span/img/@src')\n        profile_name = get_first_of_element(profile_area, 'div[1]/div/strong/text()')\n        profile_wechat_id = get_first_of_element(profile_area, 'div[1]/div/p/text()')\n        profile_desc = get_first_of_element(profile_area, 'ul/li[1]/div/text()')\n        profile_principal = get_first_of_element(profile_area, 'ul/li[2]/div/text()')\n\n        return {\n            'wechat_name': profile_name.strip(),\n            'wechat_id': profile_wechat_id.replace('\u5fae\u4fe1\u53f7: ', '').strip('\\n'),\n            'introduction': profile_desc,\n            'authentication': profile_principal,\n            'headimage': profile_img\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_article_by_history_json(text, article_json=None):\n        if article_json is None:\n            article_json = find_article_json_re.findall(text)\n            if not article_json:\n                return []\n            article_json = article_json[0] + '}}]}'\n            article_json = json.loads(article_json)\n\n        items = list()\n\n        for listdic in article_json['list']:\n            if str(listdic['comm_msg_info'].get('type', '')) != '49':\n                continue\n\n            comm_msg_info = listdic['comm_msg_info']\n            app_msg_ext_info = listdic['app_msg_ext_info']\n            send_id = comm_msg_info.get('id', '')\n            msg_datetime = comm_msg_info.get('datetime', '')\n            msg_type = str(comm_msg_info.get('type', ''))\n\n            items.append({\n                'send_id': send_id,\n                'datetime': msg_datetime,\n                'type': msg_type,\n                'main': 1, 'title': app_msg_ext_info.get('title', ''),\n                'abstract': app_msg_ext_info.get('digest', ''),\n                'fileid': app_msg_ext_info.get('fileid', ''),\n                'content_url': WechatSogouStructuring.__handle_content_url(app_msg_ext_info.get('content_url')),\n                'source_url': app_msg_ext_info.get('source_url', ''),\n                'cover': app_msg_ext_info.get('cover', ''),\n                'author': app_msg_ext_info.get('author', ''),\n                'copyright_stat': app_msg_ext_info.get('copyright_stat', '')\n            })\n\n            if app_msg_ext_info.get('is_multi', 0) == 1:\n                for multi_dict in app_msg_ext_info['multi_app_msg_item_list']:\n                    items.append({\n                        'send_id': send_id,\n                        'datetime': msg_datetime,\n                        'type': msg_type,\n                        'main': 0, 'title': multi_dict.get('title', ''),\n                        'abstract': multi_dict.get('digest', ''),\n                        'fileid': multi_dict.get('fileid', ''),\n                        'content_url': WechatSogouStructuring.__handle_content_url(multi_dict.get('content_url')),\n                        'source_url': multi_dict.get('source_url', ''),\n                        'cover': multi_dict.get('cover', ''),\n                        'author': multi_dict.get('author', ''),\n                        'copyright_stat': multi_dict.get('copyright_stat', '')\n                    })\n\n        return list(filter(lambda x: x['content_url'], items))", "response": "Returns a list of dicts containing the article by the given text and article_json."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of dicts with the keys article url title abstract time open_id main_img and open_id.", "response": "def get_gzh_article_by_hot(text):\n        \"\"\"\u4ece \u9996\u9875\u70ed\u95e8\u641c\u7d22 \u63d0\u53d6\u516c\u4f17\u53f7\u4fe1\u606f \u548c \u6587\u7ae0\u5217\u8868\u4fe1\u606f\n\n        Parameters\n        ----------\n        text : str or unicode\n            \u9996\u9875\u70ed\u95e8\u641c\u7d22 \u9875 \u4e2d \u67d0\u4e00\u9875 \u7684\u6587\u672c\n\n        Returns\n        -------\n        list[dict]\n            {\n                'gzh': {\n                    'headimage': str,  # \u516c\u4f17\u53f7\u5934\u50cf\n                    'wechat_name': str,  # \u516c\u4f17\u53f7\u540d\u79f0\n                },\n                'article': {\n                    'url': str,  # \u6587\u7ae0\u4e34\u65f6\u94fe\u63a5\n                    'title': str,  # \u6587\u7ae0\u6807\u9898\n                    'abstract': str,  # \u6587\u7ae0\u6458\u8981\n                    'time': int,  # \u63a8\u9001\u65f6\u95f4\uff0c10\u4f4d\u65f6\u95f4\u6233\n                    'open_id': str,  # open id\n                    'main_img': str  # \u5c01\u9762\u56fe\u7247\n                }\n            }\n        \"\"\"\n        page = etree.HTML(text)\n        lis = page.xpath('/html/body/li')\n        gzh_article_list = []\n        for li in lis:\n            url = get_first_of_element(li, 'div[1]/h4/a/@href')\n            title = get_first_of_element(li, 'div[1]/h4/a/div/text()')\n            abstract = get_first_of_element(li, 'div[1]/p[1]/text()')\n            xpath_time = get_first_of_element(li, 'div[1]/p[2]')\n            open_id = get_first_of_element(xpath_time, 'span/@data-openid')\n            headimage = get_first_of_element(xpath_time, 'span/@data-headimage')\n            gzh_name = get_first_of_element(xpath_time, 'span/text()')\n            send_time = xpath_time.xpath('a/span/@data-lastmodified')\n            main_img = get_first_of_element(li, 'div[2]/a/img/@src')\n\n            try:\n                send_time = int(send_time[0])\n            except ValueError:\n                send_time = send_time[0]\n\n            gzh_article_list.append({\n                'gzh': {\n                    'headimage': headimage,\n                    'wechat_name': gzh_name,\n                },\n                'article': {\n                    'url': url,\n                    'title': title,\n                    'abstract': abstract,\n                    'time': send_time,\n                    'open_id': open_id,\n                    'main_img': main_img\n                }\n            })\n\n        return gzh_article_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget article detail from text.", "response": "def get_article_detail(text, del_qqmusic=True, del_voice=True):\n        \"\"\"\u6839\u636e\u5fae\u4fe1\u6587\u7ae0\u7684\u4e34\u65f6\u94fe\u63a5\u83b7\u53d6\u660e\u7ec6\n\n        1. \u83b7\u53d6\u6587\u672c\u4e2d\u6240\u6709\u7684\u56fe\u7247\u94fe\u63a5\u5217\u8868\n        2. \u83b7\u53d6\u5fae\u4fe1\u6587\u7ae0\u7684html\u5185\u5bb9\u9875\u9762(\u53bb\u9664\u6807\u9898\u7b49\u4fe1\u606f)\n\n        Parameters\n        ----------\n        text : str or unicode\n            \u4e00\u7bc7\u5fae\u4fe1\u6587\u7ae0\u7684\u6587\u672c\n        del_qqmusic: bool\n            \u5220\u9664\u6587\u7ae0\u4e2d\u7684qq\u97f3\u4e50\n        del_voice: bool\n            \u5220\u9664\u6587\u7ae0\u4e2d\u7684\u8bed\u97f3\u5185\u5bb9\n\n        Returns\n        -------\n        dict\n        {\n            'content_html': str # \u5fae\u4fe1\u6587\u672c\u5185\u5bb9\n            'content_img_list': list[img_url1, img_url2, ...] # \u5fae\u4fe1\u6587\u672c\u4e2d\u56fe\u7247\u5217\u8868\n\n        }\n        \"\"\"\n        # 1. \u83b7\u53d6\u5fae\u4fe1\u6587\u672ccontent\n        html_obj = BeautifulSoup(text, \"lxml\")\n        content_text = html_obj.find('div', {'class': 'rich_media_content', 'id': 'js_content'})\n\n        # 2. \u5220\u9664\u90e8\u5206\u6807\u7b7e\n        if del_qqmusic:\n            qqmusic = content_text.find_all('qqmusic') or []\n            for music in qqmusic:\n                music.parent.decompose()\n\n        if del_voice:\n            # voice\u662f\u4e00\u4e2ap\u6807\u7b7e\u4e0b\u7684mpvoice\u6807\u7b7e\u4ee5\u53caclass\u4e3a'js_audio_frame db'\u7684span\u6784\u6210\uff0c\u6240\u4ee5\u5c06\u7236\u6807\u7b7e\u5220\u9664\n            voices = content_text.find_all('mpvoice') or []\n            for voice in voices:\n                voice.parent.decompose()\n\n        # 3. \u83b7\u53d6\u6240\u6709\u7684\u56fe\u7247 [img\u6807\u7b7e\uff0c\u548cstyle\u4e2d\u7684background-image]\n        all_img_set = set()\n        all_img_element = content_text.find_all('img') or []\n        for ele in all_img_element:\n            # \u5220\u9664\u90e8\u5206\u5c5e\u6027\n            img_url = format_image_url(ele.attrs['data-src'])\n            del ele.attrs['data-src']\n\n            ele.attrs['src'] = img_url\n\n            if not img_url.startswith('http'):\n                raise WechatSogouException('img_url [{}] \u4e0d\u5408\u6cd5'.format(img_url))\n            all_img_set.add(img_url)\n\n        backgroud_image = content_text.find_all(style=re.compile(\"background-image\")) or []\n        for ele in backgroud_image:\n            # \u5220\u9664\u90e8\u5206\u5c5e\u6027\n            if ele.attrs.get('data-src'):\n                del ele.attrs['data-src']\n\n            if ele.attrs.get('data-wxurl'):\n                del ele.attrs['data-wxurl']\n            img_url = re.findall(backgroud_image_p, str(ele))\n            if not img_url:\n                continue\n            all_img_set.add(img_url[0])\n\n        # 4. \u5904\u7406iframe\n        all_img_element = content_text.find_all('iframe') or []\n        for ele in all_img_element:\n            # \u5220\u9664\u90e8\u5206\u5c5e\u6027\n            img_url = ele.attrs['data-src']\n            del ele.attrs['data-src']\n            ele.attrs['src'] = img_url\n\n        # 5. \u8fd4\u56de\u6570\u636e\n        all_img_list = list(all_img_set)\n        content_html = content_text.prettify()\n        # \u53bb\u9664div[id=js_content]\n        content_html = re.findall(js_content, content_html)[0][0]\n        return {\n            'content_html': content_html,\n            'content_img_list': all_img_list\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread and decodes an image from a file object as a Numpy array.", "response": "def _decode_image(fobj, session, filename):\n  \"\"\"Reads and decodes an image from a file object as a Numpy array.\n\n  The SUN dataset contains images in several formats (despite the fact that\n  all of them have .jpg extension). Some of them are:\n    - BMP (RGB)\n    - PNG (grayscale, RGBA, RGB interlaced)\n    - JPEG (RGB)\n    - GIF (1-frame RGB)\n  Since TFDS assumes that all images have the same number of channels, we\n  convert all of them to RGB.\n\n  Args:\n    fobj: File object to read from.\n    session: TF session used to decode the images.\n    filename: Filename of the original image in the archive.\n\n  Returns:\n    Numpy array with shape (height, width, channels).\n  \"\"\"\n\n  buf = fobj.read()\n  image = tfds.core.lazy_imports.cv2.imdecode(\n      np.fromstring(buf, dtype=np.uint8), flags=3)  # Note: Converts to RGB.\n  if image is None:\n    logging.warning(\n        \"Image %s could not be decoded by OpenCV, falling back to TF\", filename)\n    try:\n      image = tf.image.decode_image(buf, channels=3)\n      image = session.run(image)\n    except tf.errors.InvalidArgumentError:\n      logging.fatal(\"Image %s could not be decoded by Tensorflow\", filename)\n\n  # The GIF images contain a single frame.\n  if len(image.shape) == 4:  # rank=4 -> rank=3\n    image = image.reshape(image.shape[1:])\n\n  return image"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess image files from the dataset.", "response": "def _process_image_file(fobj, session, filename):\n  \"\"\"Process image files from the dataset.\"\"\"\n  # We need to read the image files and convert them to JPEG, since some files\n  # actually contain GIF, PNG or BMP data (despite having a .jpg extension) and\n  # some encoding options that will make TF crash in general.\n  image = _decode_image(fobj, session, filename=filename)\n  return _encode_jpeg(image)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_parallel_sentences(f1, f2):\n  def _parse_text(path):\n    \"\"\"Returns the sentences from a single text file, which may be gzipped.\"\"\"\n    split_path = path.split(\".\")\n\n    if split_path[-1] == \"gz\":\n      lang = split_path[-2]\n      with tf.io.gfile.GFile(path) as f, gzip.GzipFile(fileobj=f) as g:\n        return g.read().split(\"\\n\"), lang\n\n    if split_path[-1] == \"txt\":\n      # CWMT\n      lang = split_path[-2].split(\"_\")[-1]\n      lang = \"zh\" if lang in (\"ch\", \"cn\") else lang\n    else:\n      lang = split_path[-1]\n    with tf.io.gfile.GFile(path) as f:\n      return f.read().split(\"\\n\"), lang\n\n  def _parse_sgm(path):\n    \"\"\"Returns sentences from a single SGML file.\"\"\"\n    lang = path.split(\".\")[-2]\n    sentences = []\n    # Note: We can't use the XML parser since some of the files are badly\n    # formatted.\n    seg_re = re.compile(r\"<seg id=\\\"\\d+\\\">(.*)</seg>\")\n    with tf.io.gfile.GFile(path) as f:\n      for line in f:\n        seg_match = re.match(seg_re, line)\n        if seg_match:\n          assert len(seg_match.groups()) == 1\n          sentences.append(seg_match.groups()[0])\n    return sentences, lang\n\n  parse_file = _parse_sgm if f1.endswith(\".sgm\") else _parse_text\n\n  # Some datasets (e.g., CWMT) contain multiple parallel files specified with\n  # a wildcard. We sort both sets to align them and parse them one by one.\n  f1_files = tf.io.gfile.glob(f1)\n  f2_files = tf.io.gfile.glob(f2)\n\n  assert f1_files and f2_files, \"No matching files found: %s, %s.\" % (f1, f2)\n  assert len(f1_files) == len(f2_files), (\n      \"Number of files do not match: %d vs %d for %s vs %s.\" % (\n          len(f1_files), len(f2_files), f1, f2))\n\n  for f1_i, f2_i in zip(sorted(f1_files), sorted(f2_files)):\n    l1_sentences, l1 = parse_file(f1_i)\n    l2_sentences, l2 = parse_file(f2_i)\n\n    assert len(l1_sentences) == len(l2_sentences), (\n        \"Sizes do not match: %d vs %d for %s vs %s.\" % (\n            len(l1_sentences), len(l2_sentences), f1_i, f2_i))\n\n    for s1, s2 in zip(l1_sentences, l2_sentences):\n      yield {\n          l1: s1,\n          l2: s2\n      }", "response": "Parses the parallel SGML or text files and returns a list of sentences."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates examples from TMX file.", "response": "def _parse_tmx(path):\n  \"\"\"Generates examples from TMX file.\"\"\"\n  def _get_tuv_lang(tuv):\n    for k, v in tuv.items():\n      if k.endswith(\"}lang\"):\n        return v\n    raise AssertionError(\"Language not found in `tuv` attributes.\")\n\n  def _get_tuv_seg(tuv):\n    segs = tuv.findall(\"seg\")\n    assert len(segs) == 1, \"Invalid number of segments: %d\" % len(segs)\n    return segs[0].text\n\n  with tf.io.gfile.GFile(path) as f:\n    for _, elem in ElementTree.iterparse(f):\n      if elem.tag == \"tu\":\n        yield {\n            _get_tuv_lang(tuv):\n                _get_tuv_seg(tuv) for tuv in elem.iterfind(\"tuv\")\n        }\n        elem.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_tsv(path, language_pair=None):\n  if language_pair is None:\n    lang_match = re.match(r\".*\\.([a-z][a-z])-([a-z][a-z])\\.tsv\", path)\n    assert lang_match is not None, \"Invalid TSV filename: %s\" % path\n    l1, l2 = lang_match.groups()\n  else:\n    l1, l2 = language_pair\n  with tf.io.gfile.GFile(path) as f:\n    for j, line in enumerate(f):\n      cols = line.split(\"\\t\")\n      if len(cols) != 2:\n        logging.warning(\n            \"Skipping line %d in TSV (%s) with %d != 2 columns.\",\n            j, path, len(cols))\n        continue\n      s1, s2 = cols\n      yield {\n          l1: s1.strip(),\n          l2: s2.strip()\n      }", "response": "Generates examples from TSV file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_wikiheadlines(path):\n  lang_match = re.match(r\".*\\.([a-z][a-z])-([a-z][a-z])$\", path)\n  assert lang_match is not None, \"Invalid Wikiheadlines filename: %s\" % path\n  l1, l2 = lang_match.groups()\n  with tf.io.gfile.GFile(path) as f:\n    for line in f:\n      s1, s2 = line.split(\"|||\")\n      yield {\n          l1: s1.strip(),\n          l2: s2.strip()\n      }", "response": "Generates examples from Wikiheadlines dataset file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating examples from CzEng v1. 6 with optional filtering for v1. 7.", "response": "def _parse_czeng(*paths, **kwargs):\n  \"\"\"Generates examples from CzEng v1.6, with optional filtering for v1.7.\"\"\"\n  filter_path = kwargs.get(\"filter_path\", None)\n  if filter_path:\n    re_block = re.compile(r\"^[^-]+-b(\\d+)-\\d\\d[tde]\")\n    with tf.io.gfile.GFile(filter_path) as f:\n      bad_blocks = {\n          blk for blk in re.search(\n              r\"qw{([\\s\\d]*)}\", f.read()).groups()[0].split()\n      }\n    logging.info(\n        \"Loaded %d bad blocks to filter from CzEng v1.6 to make v1.7.\",\n        len(bad_blocks))\n\n  for path in paths:\n    for gz_path in tf.io.gfile.glob(path):\n      with tf.io.gfile.GFile(gz_path, \"rb\") as g, gzip.GzipFile(fileobj=g) as f:\n        for line in f:\n          line = line.decode(\"utf-8\")  # required for py3\n          if not line.strip():\n            continue\n          id_, unused_score, cs, en = line.split(\"\\t\")\n          if filter_path:\n            block_match = re.match(re_block, id_)\n            if block_match and block_match.groups()[0] in bad_blocks:\n              continue\n          yield {\n              \"cs\": cs.strip(),\n              \"en\": en.strip(),\n          }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninjecting languages into ( potentially ) template strings.", "response": "def _inject_language(self, src, strings):\n    \"\"\"Injects languages into (potentially) template strings.\"\"\"\n    if src not in self.sources:\n      raise ValueError(\"Invalid source for '{0}': {1}\".format(self.name, src))\n    def _format_string(s):\n      if \"{0}\" in s and \"{1}\" and \"{src}\" in s:\n        return s.format(*sorted([src, self.target]), src=src)\n      elif \"{0}\" in s and \"{1}\" in s:\n        return s.format(*sorted([src, self.target]))\n      elif \"{src}\" in s:\n        return s.format(src=src)\n      else:\n        return s\n    return [_format_string(s) for s in strings]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of subsets that make up each split of the dataset for the language pair.", "response": "def subsets(self):\n    \"\"\"Subsets that make up each split of the dataset for the language pair.\"\"\"\n    source, target = self.builder_config.language_pair\n    filtered_subsets = {}\n    for split, ss_names in self._subsets.items():\n      filtered_subsets[split] = []\n      for ss_name in ss_names:\n        ds = DATASET_MAP[ss_name]\n        if ds.target != target or source not in ds.sources:\n          logging.info(\n              \"Skipping sub-dataset that does not include language pair: %s\",\n              ss_name)\n        else:\n          filtered_subsets[split].append(ss_name)\n    logging.info(\"Using sub-datasets: %s\", filtered_subsets)\n    return filtered_subsets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_examples(self, split_subsets, extraction_map):\n    source, _ = self.builder_config.language_pair\n\n    def _get_local_paths(ds, extract_dirs):\n      rel_paths = ds.get_path(source)\n      if len(extract_dirs) == 1:\n        extract_dirs = extract_dirs * len(rel_paths)\n      return [os.path.join(ex_dir, rel_path) if rel_path else ex_dir\n              for ex_dir, rel_path in zip(extract_dirs, rel_paths)]\n\n    for ss_name in split_subsets:\n      logging.info(\"Generating examples from: %s\", ss_name)\n      ds = DATASET_MAP[ss_name]\n      extract_dirs = extraction_map[ss_name]\n      files = _get_local_paths(ds, extract_dirs)\n      if ss_name.startswith(\"czeng\"):\n        if ss_name.endswith(\"16pre\"):\n          sub_generator = functools.partial(\n              _parse_tsv, language_pair=(\"en\", \"cs\"))\n        elif ss_name.endswith(\"17\"):\n          filter_path = _get_local_paths(\n              _CZENG17_FILTER, extraction_map[_CZENG17_FILTER.name])[0]\n          sub_generator = functools.partial(\n              _parse_czeng, filter_path=filter_path)\n        else:\n          sub_generator = _parse_czeng\n      elif len(files) == 2:\n        if ss_name.endswith(\"_frde\"):\n          sub_generator = _parse_frde_bitext\n        else:\n          sub_generator = _parse_parallel_sentences\n      elif len(files) == 1:\n        fname = files[0]\n         # Note: Due to formatting used by `download_manager`, the file\n         # extension may not be at the end of the file path.\n        if \".tsv\" in fname:\n          sub_generator = _parse_tsv\n        elif ss_name.startswith(\"newscommentary_v14\"):\n          sub_generator = functools.partial(\n              _parse_tsv, language_pair=self.builder_config.language_pair)\n        elif \"tmx\" in fname:\n          sub_generator = _parse_tmx\n        elif ss_name.startswith(\"wikiheadlines\"):\n          sub_generator = _parse_wikiheadlines\n        else:\n          raise ValueError(\"Unsupported file format: %s\" % fname)\n      else:\n        raise ValueError(\"Invalid number of files: %d\" % len(files))\n\n      for ex in sub_generator(*files):\n        if not all(ex.values()):\n          continue\n        # TODO(adarob): Add subset feature.\n        # ex[\"subset\"] = subset\n        yield ex", "response": "Generates the examples in the raw text form."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef builder(name, **builder_init_kwargs):\n  name, builder_kwargs = _dataset_name_and_kwargs_from_name_str(name)\n  builder_kwargs.update(builder_init_kwargs)\n  if name in _ABSTRACT_DATASET_REGISTRY:\n    raise DatasetNotFoundError(name, is_abstract=True)\n  if name in _IN_DEVELOPMENT_REGISTRY:\n    raise DatasetNotFoundError(name, in_development=True)\n  if name not in _DATASET_REGISTRY:\n    raise DatasetNotFoundError(name)\n  try:\n    return _DATASET_REGISTRY[name](**builder_kwargs)\n  except BaseException:\n    logging.error(\"Failed to construct dataset %s\", name)\n    raise", "response": "Fetches a tfds. core. DatasetBuilder by name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(name,\n         split=None,\n         data_dir=None,\n         batch_size=1,\n         download=True,\n         as_supervised=False,\n         with_info=False,\n         builder_kwargs=None,\n         download_and_prepare_kwargs=None,\n         as_dataset_kwargs=None,\n         try_gcs=False):\n  \"\"\"Loads the named dataset into a `tf.data.Dataset`.\n\n  If `split=None` (the default), returns all splits for the dataset. Otherwise,\n  returns the specified split.\n\n  `load` is a convenience method that fetches the `tfds.core.DatasetBuilder` by\n  string name, optionally calls `DatasetBuilder.download_and_prepare`\n  (if `download=True`), and then calls `DatasetBuilder.as_dataset`.\n  This is roughly equivalent to:\n\n  ```\n  builder = tfds.builder(name, data_dir=data_dir, **builder_kwargs)\n  if download:\n    builder.download_and_prepare(**download_and_prepare_kwargs)\n  ds = builder.as_dataset(\n      split=split, as_supervised=as_supervised, **as_dataset_kwargs)\n  if with_info:\n    return ds, builder.info\n  return ds\n  ```\n\n  If you'd like NumPy arrays instead of `tf.data.Dataset`s or `tf.Tensor`s,\n  you can pass the return value to `tfds.as_numpy`.\n\n  Callers must pass arguments as keyword arguments.\n\n  **Warning**: calling this function might potentially trigger the download\n  of hundreds of GiB to disk. Refer to the `download` argument.\n\n  Args:\n    name: `str`, the registered name of the `DatasetBuilder` (the snake case\n      version of the class name). This can be either `\"dataset_name\"` or\n      `\"dataset_name/config_name\"` for datasets with `BuilderConfig`s.\n      As a convenience, this string may contain comma-separated keyword\n      arguments for the builder. For example `\"foo_bar/a=True,b=3\"` would use\n      the `FooBar` dataset passing the keyword arguments `a=True` and `b=3`\n      (for builders with configs, it would be `\"foo_bar/zoo/a=True,b=3\"` to\n      use the `\"zoo\"` config and pass to the builder keyword arguments `a=True`\n      and `b=3`).\n    split: `tfds.Split` or `str`, which split of the data to load. If None,\n      will return a `dict` with all splits (typically `tfds.Split.TRAIN` and\n      `tfds.Split.TEST`).\n    data_dir: `str` (optional), directory to read/write data.\n      Defaults to \"~/tensorflow_datasets\".\n    batch_size: `int`, set to > 1 to get batches of examples. Note that\n      variable length features will be 0-padded. If\n      `batch_size=-1`, will return the full dataset as `tf.Tensor`s.\n    download: `bool` (optional), whether to call\n      `tfds.core.DatasetBuilder.download_and_prepare`\n      before calling `tf.DatasetBuilder.as_dataset`. If `False`, data is\n      expected to be in `data_dir`. If `True` and the data is already in\n      `data_dir`, `download_and_prepare` is a no-op.\n    as_supervised: `bool`, if `True`, the returned `tf.data.Dataset`\n      will have a 2-tuple structure `(input, label)` according to\n      `builder.info.supervised_keys`. If `False`, the default,\n      the returned `tf.data.Dataset` will have a dictionary with all the\n      features.\n    with_info: `bool`, if True, tfds.load will return the tuple\n      (tf.data.Dataset, tfds.core.DatasetInfo) containing the info associated\n      with the builder.\n    builder_kwargs: `dict` (optional), keyword arguments to be passed to the\n      `tfds.core.DatasetBuilder` constructor. `data_dir` will be passed\n      through by default.\n    download_and_prepare_kwargs: `dict` (optional) keyword arguments passed to\n      `tfds.core.DatasetBuilder.download_and_prepare` if `download=True`. Allow\n      to control where to download and extract the cached data. If not set,\n      cache_dir and manual_dir will automatically be deduced from data_dir.\n    as_dataset_kwargs: `dict` (optional), keyword arguments passed to\n      `tfds.core.DatasetBuilder.as_dataset`. `split` will be passed through by\n      default. Example: `{'shuffle_files': True}`.\n      Note that shuffle_files is False by default unless\n      `split == tfds.Split.TRAIN`.\n    try_gcs: `bool`, if True, tfds.load will see if the dataset exists on\n      the public GCS bucket before building it locally.\n\n  Returns:\n    ds: `tf.data.Dataset`, the dataset requested, or if `split` is None, a\n      `dict<key: tfds.Split, value: tfds.data.Dataset>`. If `batch_size=-1`,\n      these will be full datasets as `tf.Tensor`s.\n    ds_info: `tfds.core.DatasetInfo`, if `with_info` is True, then `tfds.load`\n      will return a tuple `(ds, ds_info)` containing dataset information\n      (version, features, splits, num_examples,...). Note that the `ds_info`\n      object documents the entire dataset, regardless of the `split` requested.\n      Split-specific information is available in `ds_info.splits`.\n  \"\"\"\n  name, name_builder_kwargs = _dataset_name_and_kwargs_from_name_str(name)\n  name_builder_kwargs.update(builder_kwargs or {})\n  builder_kwargs = name_builder_kwargs\n\n  # Set data_dir\n  if try_gcs and gcs_utils.is_dataset_on_gcs(name):\n    data_dir = constants.GCS_DATA_DIR\n  elif data_dir is None:\n    data_dir = constants.DATA_DIR\n\n  dbuilder = builder(name, data_dir=data_dir, **builder_kwargs)\n  if download:\n    download_and_prepare_kwargs = download_and_prepare_kwargs or {}\n    dbuilder.download_and_prepare(**download_and_prepare_kwargs)\n\n  if as_dataset_kwargs is None:\n    as_dataset_kwargs = {}\n  as_dataset_kwargs = dict(as_dataset_kwargs)\n  as_dataset_kwargs[\"split\"] = split\n  as_dataset_kwargs[\"as_supervised\"] = as_supervised\n  as_dataset_kwargs[\"batch_size\"] = batch_size\n\n  ds = dbuilder.as_dataset(**as_dataset_kwargs)\n  if with_info:\n    return ds, dbuilder.info\n  return ds", "response": "Loads a named dataset into a tf. data. Dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract dataset name and kwargs from name str.", "response": "def _dataset_name_and_kwargs_from_name_str(name_str):\n  \"\"\"Extract kwargs from name str.\"\"\"\n  res = _NAME_REG.match(name_str)\n  if not res:\n    raise ValueError(_NAME_STR_ERR.format(name_str))\n  name = res.group(\"dataset_name\")\n  kwargs = _kwargs_str_to_kwargs(res.group(\"kwargs\"))\n  try:\n    for attr in [\"config\", \"version\"]:\n      val = res.group(attr)\n      if val is None:\n        continue\n      if attr in kwargs:\n        raise ValueError(\"Dataset %s: cannot pass %s twice.\" % (name, attr))\n      kwargs[attr] = val\n    return name, kwargs\n  except:\n    logging.error(_NAME_STR_ERR.format(name_str))   # pylint: disable=logging-format-interpolation\n    raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry cast to int float bool str in that order.", "response": "def _cast_to_pod(val):\n  \"\"\"Try cast to int, float, bool, str, in that order.\"\"\"\n  bools = {\"True\": True, \"False\": False}\n  if val in bools:\n    return bools[val]\n  try:\n    return int(val)\n  except ValueError:\n    try:\n      return float(val)\n    except ValueError:\n      return tf.compat.as_text(val)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry importing a module and return the module object.", "response": "def _try_import(module_name):\n  \"\"\"Try importing a module, with an informative error message on failure.\"\"\"\n  try:\n    mod = importlib.import_module(module_name)\n    return mod\n  except ImportError:\n    err_msg = (\"Tried importing %s but failed. See setup.py extras_require. \"\n               \"The dataset you are trying to use may have additional \"\n               \"dependencies.\")\n    utils.reraise(err_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn list from list tuple or ndarray.", "response": "def np_to_list(elem):\n  \"\"\"Returns list from list, tuple or ndarray.\"\"\"\n  if isinstance(elem, list):\n    return elem\n  elif isinstance(elem, tuple):\n    return list(elem)\n  elif isinstance(elem, np.ndarray):\n    return list(elem)\n  else:\n    raise ValueError(\n        'Input elements of a sequence should be either a numpy array, a '\n        'python list or tuple. Got {}'.format(type(elem)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _transpose_dict_list(dict_list):\n  # 1. Unstack numpy arrays into list\n  dict_list = utils.map_nested(np_to_list, dict_list, dict_only=True)\n\n  # 2. Extract the sequence length (and ensure the length is constant for all\n  # elements)\n  length = {'value': None}  # dict because `nonlocal` is Python3 only\n  def update_length(elem):\n    if length['value'] is None:\n      length['value'] = len(elem)\n    elif length['value'] != len(elem):\n      raise ValueError(\n          'The length of all elements of one sequence should be the same. '\n          'Got {} != {}'.format(length['value'], len(elem)))\n    return elem\n  utils.map_nested(update_length, dict_list, dict_only=True)\n\n  # 3. Extract each individual elements\n  return [\n      utils.map_nested(lambda elem: elem[i], dict_list, dict_only=True)   # pylint: disable=cell-var-from-loop\n      for i in range(length['value'])\n  ]", "response": "Transpose a nested dict list into a list[nested dict ]."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a tensor_info object for each element of the sequence.", "response": "def get_tensor_info(self):\n    \"\"\"See base class for details.\"\"\"\n    # Add the additional length dimension to every shape\n\n    def add_length_dim(tensor_info):\n      return feature_lib.TensorInfo(\n          shape=(self._length,) + tensor_info.shape,\n          dtype=tensor_info.dtype,\n      )\n\n    tensor_info = super(SequenceDict, self).get_tensor_info()\n    return utils.map_nested(add_length_dim, tensor_info)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of serialized features for the current SequenceDict.", "response": "def get_serialized_info(self):\n    \"\"\"See base class for details.\"\"\"\n    # Add the additional length dimension to every serialized features\n\n    def add_length_dim(serialized_info):\n      \"\"\"Add the length dimension to the serialized_info.\n\n      Args:\n        serialized_info: One of tf.io.FixedLenFeature, tf.io.VarLenFeature,...\n\n      Returns:\n        new_serialized_info: serialized_info with extended first dimension\n      \"\"\"\n      if isinstance(serialized_info, tf.io.FixedLenFeature):\n        if self._length is not None:\n          return tf.io.FixedLenFeature(\n              shape=(self._length,) + serialized_info.shape,\n              dtype=serialized_info.dtype,\n          )\n        else:\n          return tf.io.FixedLenSequenceFeature(\n              shape=serialized_info.shape,\n              dtype=serialized_info.dtype,\n              allow_missing=True,\n          )\n      elif isinstance(serialized_info, tf.io.VarLenFeature):\n        return serialized_info\n      else:\n        raise ValueError(\n            'FixedLenSequenceFeature not supported inside SequenceDict'\n        )\n      return serialized_info\n\n    tensor_info = super(SequenceDict, self).get_serialized_info()\n    return utils.map_nested(add_length_dim, tensor_info)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _split_generators(self, dl_manager):\n    # Download the full MNIST Database\n    filenames = {\n        \"train_data\": _MNIST_TRAIN_DATA_FILENAME,\n        \"train_labels\": _MNIST_TRAIN_LABELS_FILENAME,\n        \"test_data\": _MNIST_TEST_DATA_FILENAME,\n        \"test_labels\": _MNIST_TEST_LABELS_FILENAME,\n    }\n    mnist_files = dl_manager.download_and_extract(\n        {k: urllib.parse.urljoin(self.URL, v) for k, v in filenames.items()})\n\n    # MNIST provides TRAIN and TEST splits, not a VALIDATION split, so we only\n    # write the TRAIN and TEST splits to disk.\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=10,\n            gen_kwargs=dict(\n                num_examples=_TRAIN_EXAMPLES,\n                data_path=mnist_files[\"train_data\"],\n                label_path=mnist_files[\"train_labels\"],\n            )),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=1,\n            gen_kwargs=dict(\n                num_examples=_TEST_EXAMPLES,\n                data_path=mnist_files[\"test_data\"],\n                label_path=mnist_files[\"test_labels\"],\n            )),\n    ]", "response": "Returns a list of SplitGenerators that will generate the train and test splits for the given base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_examples(self, num_examples, data_path, label_path):\n    images = _extract_mnist_images(data_path, num_examples)\n    labels = _extract_mnist_labels(label_path, num_examples)\n    data = list(zip(images, labels))\n\n    # Data is shuffled automatically to distribute classes uniformly.\n    for image, label in data:\n      yield {\n          \"image\": image,\n          \"label\": label,\n      }", "response": "Generate MNIST examples as dicts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _split_generators(self, dl_manager):\n    # Download images and annotations that come in separate archives.\n    # Note, that the extension of archives is .tar.gz even though the actual\n    # archives format is uncompressed tar.\n    dl_paths = dl_manager.download_and_extract({\n        \"images\": tfds.download.Resource(\n            url=os.path.join(_BASE_URL, \"102flowers.tgz\"),\n            extract_method=tfds.download.ExtractMethod.TAR),\n        \"labels\": os.path.join(_BASE_URL, \"imagelabels.mat\"),\n        \"setid\": os.path.join(_BASE_URL, \"setid.mat\"),\n    })\n\n    gen_kwargs = dict(\n        images_dir_path=os.path.join(dl_paths[\"images\"], \"jpg\"),\n        labels_path=dl_paths[\"labels\"],\n        setid_path=dl_paths[\"setid\"],\n    )\n\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=1,\n            gen_kwargs=dict(split_name=\"trnid\", **gen_kwargs)),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=1,\n            gen_kwargs=dict(split_name=\"tstid\", **gen_kwargs)),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.VALIDATION,\n            num_shards=1,\n            gen_kwargs=dict(split_name=\"valid\", **gen_kwargs)),\n    ]", "response": "Returns a list of SplitGenerators that will generate the single item for the current taxonomy."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate statistics for the specified split.", "response": "def get_dataset_feature_statistics(builder, split):\n  \"\"\"Calculate statistics for the specified split.\"\"\"\n  statistics = statistics_pb2.DatasetFeatureStatistics()\n\n  # Make this to the best of our abilities.\n  schema = schema_pb2.Schema()\n\n  dataset = builder.as_dataset(split=split)\n\n  # Just computing the number of examples for now.\n  statistics.num_examples = 0\n\n  # Feature dictionaries.\n  feature_to_num_examples = collections.defaultdict(int)\n  feature_to_min = {}\n  feature_to_max = {}\n\n  np_dataset = dataset_utils.as_numpy(dataset)\n  for example in utils.tqdm(np_dataset, unit=\" examples\", leave=False):\n    statistics.num_examples += 1\n\n    assert isinstance(example, dict)\n\n    feature_names = sorted(example.keys())\n    for feature_name in feature_names:\n\n      # Update the number of examples this feature appears in.\n      feature_to_num_examples[feature_name] += 1\n\n      feature_np = example[feature_name]\n\n      # For compatibility in graph and eager mode, we can get PODs here and\n      # everything may not be neatly wrapped up in numpy's ndarray.\n\n      feature_dtype = type(feature_np)\n\n      if isinstance(feature_np, np.ndarray):\n        # If we have an empty array, then don't proceed further with computing\n        # statistics on it.\n        if feature_np.size == 0:\n          continue\n\n        feature_dtype = feature_np.dtype.type\n\n      feature_min, feature_max = None, None\n      is_numeric = (np.issubdtype(feature_dtype, np.number) or\n                    feature_dtype == np.bool_)\n      if is_numeric:\n        feature_min = np.min(feature_np)\n        feature_max = np.max(feature_np)\n\n      # TODO(afrozm): What if shapes don't match? Populate ValueCount? Add\n      # logic for that.\n\n      # Set or update the min, max.\n      if is_numeric:\n        if ((feature_name not in feature_to_min) or\n            (feature_to_min[feature_name] > feature_min)):\n          feature_to_min[feature_name] = feature_min\n\n        if ((feature_name not in feature_to_max) or\n            (feature_to_max[feature_name] < feature_max)):\n          feature_to_max[feature_name] = feature_max\n\n  # Start here, we've processed all examples.\n\n  output_shapes_dict = dataset.output_shapes\n  output_types_dict = dataset.output_types\n\n  for feature_name in sorted(feature_to_num_examples.keys()):\n    # Try to fill in the schema.\n    feature = schema.feature.add()\n    feature.name = feature_name\n\n    # TODO(afrozm): Make this work with nested structures, currently the Schema\n    # proto has no support for it.\n    maybe_feature_shape = output_shapes_dict[feature_name]\n    if not isinstance(maybe_feature_shape, tf.TensorShape):\n      logging.error(\n          \"Statistics generation doesn't work for nested structures yet\")\n      continue\n\n    for dim in maybe_feature_shape.as_list():\n      # We denote `None`s as -1 in the shape proto.\n      feature.shape.dim.add().size = dim if dim else -1\n    feature_type = output_types_dict[feature_name]\n    feature.type = _FEATURE_TYPE_MAP.get(feature_type, schema_pb2.BYTES)\n\n    common_statistics = statistics_pb2.CommonStatistics()\n    common_statistics.num_non_missing = feature_to_num_examples[feature_name]\n    common_statistics.num_missing = (\n        statistics.num_examples - common_statistics.num_non_missing)\n\n    feature_name_statistics = statistics.features.add()\n    feature_name_statistics.name = feature_name\n\n    # TODO(afrozm): This can be skipped, since type information was added to\n    # the Schema.\n    feature_name_statistics.type = _SCHEMA_TYPE_MAP.get(\n        feature.type, statistics_pb2.FeatureNameStatistics.BYTES)\n\n    if feature.type == schema_pb2.INT or feature.type == schema_pb2.FLOAT:\n      numeric_statistics = statistics_pb2.NumericStatistics()\n      numeric_statistics.min = feature_to_min[feature_name]\n      numeric_statistics.max = feature_to_max[feature_name]\n      numeric_statistics.common_stats.CopyFrom(common_statistics)\n      feature_name_statistics.num_stats.CopyFrom(numeric_statistics)\n    else:\n      # Let's shove it into BytesStatistics for now.\n      bytes_statistics = statistics_pb2.BytesStatistics()\n      bytes_statistics.common_stats.CopyFrom(common_statistics)\n      feature_name_statistics.bytes_stats.CopyFrom(bytes_statistics)\n\n  return statistics, schema"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads JSON - formatted proto into DatasetInfo proto.", "response": "def read_from_json(json_filename):\n  \"\"\"Read JSON-formatted proto into DatasetInfo proto.\"\"\"\n  with tf.io.gfile.GFile(json_filename) as f:\n    dataset_info_json_str = f.read()\n  # Parse it back into a proto.\n  parsed_proto = json_format.Parse(dataset_info_json_str,\n                                   dataset_info_pb2.DatasetInfo())\n  return parsed_proto"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef full_name(self):\n    names = [self._builder.name]\n    if self._builder.builder_config:\n      names.append(self._builder.builder_config.name)\n    names.append(str(self.version))\n    return posixpath.join(*names)", "response": "Full canonical name of the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_splits_if_different(self, split_dict):\n    assert isinstance(split_dict, splits_lib.SplitDict)\n\n    # If splits are already defined and identical, then we do not update\n    if self._splits and splits_lib.check_splits_equals(\n        self._splits, split_dict):\n      return\n\n    self._set_splits(split_dict)", "response": "Updates the splits of the current node if they are different from the current ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_splits(self, split_dict):\n    # Update the dictionary representation.\n    # Use from/to proto for a clean copy\n    self._splits = split_dict.copy()\n\n    # Update the proto\n    del self.as_proto.splits[:]  # Clear previous\n    for split_info in split_dict.to_proto():\n      self.as_proto.splits.add().CopyFrom(split_info)", "response": "Split setter ( private method."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates from the DatasetBuilder.", "response": "def _compute_dynamic_properties(self, builder):\n    \"\"\"Update from the DatasetBuilder.\"\"\"\n    # Fill other things by going over the dataset.\n    splits = self.splits\n    for split_info in utils.tqdm(\n        splits.values(), desc=\"Computing statistics...\", unit=\" split\"):\n      try:\n        split_name = split_info.name\n        # Fill DatasetFeatureStatistics.\n        dataset_feature_statistics, schema = get_dataset_feature_statistics(\n            builder, split_name)\n\n        # Add the statistics to this split.\n        split_info.statistics.CopyFrom(dataset_feature_statistics)\n\n        # Set the schema at the top-level since this is independent of the\n        # split.\n        self.as_proto.schema.CopyFrom(schema)\n\n      except tf.errors.InvalidArgumentError:\n        # This means there is no such split, even though it was specified in the\n        # info, the least we can do is to log this.\n        logging.error((\"%s's info() property specifies split %s, but it \"\n                       \"doesn't seem to have been generated. Please ensure \"\n                       \"that the data was downloaded for this split and re-run \"\n                       \"download_and_prepare.\"), self.name, split_name)\n        raise\n\n    # Set splits to trigger proto update in setter\n    self._set_splits(splits)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_to_directory(self, dataset_info_dir):\n    # Save the metadata from the features (vocabulary, labels,...)\n    if self.features:\n      self.features.save_metadata(dataset_info_dir)\n\n    if self.redistribution_info.license:\n      with tf.io.gfile.GFile(self._license_filename(dataset_info_dir),\n                             \"w\") as f:\n        f.write(self.redistribution_info.license)\n\n    with tf.io.gfile.GFile(self._dataset_info_filename(dataset_info_dir),\n                           \"w\") as f:\n      f.write(self.as_json)", "response": "Write DatasetInfo as JSON to dataset_info_dir."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating DatasetInfo from the JSON file in dataset_info_dir.", "response": "def read_from_directory(self, dataset_info_dir):\n    \"\"\"Update DatasetInfo from the JSON file in `dataset_info_dir`.\n\n    This function updates all the dynamically generated fields (num_examples,\n    hash, time of creation,...) of the DatasetInfo.\n\n    This will overwrite all previous metadata.\n\n    Args:\n      dataset_info_dir: `str` The directory containing the metadata file. This\n        should be the root directory of a specific dataset version.\n    \"\"\"\n    if not dataset_info_dir:\n      raise ValueError(\n          \"Calling read_from_directory with undefined dataset_info_dir.\")\n\n    json_filename = self._dataset_info_filename(dataset_info_dir)\n\n    # Load the metadata from disk\n    parsed_proto = read_from_json(json_filename)\n\n    # Update splits\n    self._set_splits(splits_lib.SplitDict.from_proto(parsed_proto.splits))\n\n    # Restore the feature metadata (vocabulary, labels names,...)\n    if self.features:\n      self.features.load_metadata(dataset_info_dir)\n\n    # Update fields which are not defined in the code. This means that\n    # the code will overwrite fields which are present in\n    # dataset_info.json.\n    for field_name, field in self.as_proto.DESCRIPTOR.fields_by_name.items():\n      field_value = getattr(self._info_proto, field_name)\n      field_value_restored = getattr(parsed_proto, field_name)\n\n      try:\n        is_defined = self._info_proto.HasField(field_name)\n      except ValueError:\n        is_defined = bool(field_value)\n\n      try:\n        is_defined_in_restored = parsed_proto.HasField(field_name)\n      except ValueError:\n        is_defined_in_restored = bool(field_value_restored)\n\n      # If field is defined in code, we ignore the value\n      if is_defined:\n        if field_value != field_value_restored:\n          logging.info(\n              \"Field info.%s from disk and from code do not match. Keeping \"\n              \"the one from code.\", field_name)\n        continue\n      # If the field is also not defined in JSON file, we do nothing\n      if not is_defined_in_restored:\n        continue\n      # Otherwise, we restore the dataset_info.json value\n      if field.type == field.TYPE_MESSAGE:\n        field_value.MergeFrom(field_value_restored)\n      else:\n        setattr(self._info_proto, field_name, field_value_restored)\n\n    if self._builder._version != self.version:  # pylint: disable=protected-access\n      raise AssertionError(\n          \"The constructed DatasetInfo instance and the restored proto version \"\n          \"do not match. Builder version: {}. Proto version: {}\".format(\n              self._builder._version, self.version))  # pylint: disable=protected-access\n\n    # Mark as fully initialized.\n    self._fully_initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing DatasetInfo from GCS bucket info files.", "response": "def initialize_from_bucket(self):\n    \"\"\"Initialize DatasetInfo from GCS bucket info files.\"\"\"\n    # In order to support Colab, we use the HTTP GCS API to access the metadata\n    # files. They are copied locally and then loaded.\n    tmp_dir = tempfile.mkdtemp(\"tfds\")\n    data_files = gcs_utils.gcs_dataset_info_files(self.full_name)\n    if not data_files:\n      return\n    logging.info(\"Loading info from GCS for %s\", self.full_name)\n    for fname in data_files:\n      out_fname = os.path.join(tmp_dir, os.path.basename(fname))\n      gcs_utils.download_gcs_file(fname, out_fname)\n    self.read_from_directory(tmp_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of SplitGenerators for the current ISO.", "response": "def _split_generators(self, dl_manager):\n    \"\"\"Returns SplitGenerators.\"\"\"\n    url = _DL_URLS[self.builder_config.name]\n    data_dirs = dl_manager.download_and_extract(url)\n\n    path_to_dataset = os.path.join(data_dirs, tf.io.gfile.listdir(data_dirs)[0])\n\n    train_a_path = os.path.join(path_to_dataset, \"trainA\")\n    train_b_path = os.path.join(path_to_dataset, \"trainB\")\n    test_a_path = os.path.join(path_to_dataset, \"testA\")\n    test_b_path = os.path.join(path_to_dataset, \"testB\")\n\n    return [\n        tfds.core.SplitGenerator(\n            name=\"trainA\",\n            num_shards=10,\n            gen_kwargs={\n                \"path\": train_a_path,\n                \"label\": \"A\",\n            }),\n        tfds.core.SplitGenerator(\n            name=\"trainB\",\n            num_shards=10,\n            gen_kwargs={\n                \"path\": train_b_path,\n                \"label\": \"B\",\n            }),\n        tfds.core.SplitGenerator(\n            name=\"testA\",\n            num_shards=1,\n            gen_kwargs={\n                \"path\": test_a_path,\n                \"label\": \"A\",\n            }),\n        tfds.core.SplitGenerator(\n            name=\"testB\",\n            num_shards=1,\n            gen_kwargs={\n                \"path\": test_b_path,\n                \"label\": \"B\",\n            }),\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmapping the function into each element and resolve the promise.", "response": "def _map_promise(map_fn, all_inputs):\n  \"\"\"Map the function into each element and resolve the promise.\"\"\"\n  all_promises = utils.map_nested(map_fn, all_inputs)  # Apply the function\n  res = utils.map_nested(_wait_on_promise, all_promises)\n  return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_download_result(self, resource, tmp_dir_path, sha256, dl_size):\n    fnames = tf.io.gfile.listdir(tmp_dir_path)\n    if len(fnames) > 1:\n      raise AssertionError('More than one file in %s.' % tmp_dir_path)\n    original_fname = fnames[0]\n    tmp_path = os.path.join(tmp_dir_path, original_fname)\n    self._recorded_sizes_checksums[resource.url] = (dl_size, sha256)\n    if self._register_checksums:\n      self._record_sizes_checksums()\n    elif (dl_size, sha256) != self._sizes_checksums.get(resource.url, None):\n      raise NonMatchingChecksumError(resource.url, tmp_path)\n    download_path = self._get_final_dl_path(resource.url, sha256)\n    resource_lib.write_info_file(resource, download_path, self._dataset_name,\n                                 original_fname)\n    # Unconditionally overwrite because either file doesn't exist or\n    # FORCE_DOWNLOAD=true\n    tf.io.gfile.rename(tmp_path, download_path, overwrite=True)\n    tf.io.gfile.rmtree(tmp_dir_path)\n    return download_path", "response": "Store dled file to definitive place write INFO file return path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _download(self, resource):\n    if isinstance(resource, six.string_types):\n      resource = resource_lib.Resource(url=resource)\n    url = resource.url\n    if url in self._sizes_checksums:\n      expected_sha256 = self._sizes_checksums[url][1]\n      download_path = self._get_final_dl_path(url, expected_sha256)\n      if not self._force_download and resource.exists_locally(download_path):\n        logging.info('URL %s already downloaded: reusing %s.',\n                     url, download_path)\n        self._recorded_sizes_checksums[url] = self._sizes_checksums[url]\n        return promise.Promise.resolve(download_path)\n    # There is a slight difference between downloader and extractor here:\n    # the extractor manages its own temp directory, while the DownloadManager\n    # manages the temp directory of downloader.\n    download_dir_path = os.path.join(\n        self._download_dir,\n        '%s.tmp.%s' % (resource_lib.get_dl_dirname(url), uuid.uuid4().hex))\n    tf.io.gfile.makedirs(download_dir_path)\n    logging.info('Downloading %s into %s...', url, download_dir_path)\n\n    def callback(val):\n      checksum, dl_size = val\n      return self._handle_download_result(\n          resource, download_dir_path, checksum, dl_size)\n    return self._downloader.download(url, download_dir_path).then(callback)", "response": "Download resource returns Promise - > path to downloaded file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract(self, resource):\n    if isinstance(resource, six.string_types):\n      resource = resource_lib.Resource(path=resource)\n    path = resource.path\n    extract_method = resource.extract_method\n    if extract_method == resource_lib.ExtractMethod.NO_EXTRACT:\n      logging.info('Skipping extraction for %s (method=NO_EXTRACT).', path)\n      return promise.Promise.resolve(path)\n    method_name = resource_lib.ExtractMethod(extract_method).name\n    extract_path = os.path.join(self._extract_dir,\n                                '%s.%s' % (method_name, os.path.basename(path)))\n    if not self._force_extraction and tf.io.gfile.exists(extract_path):\n      logging.info('Reusing extraction of %s at %s.', path, extract_path)\n      return promise.Promise.resolve(extract_path)\n    return self._extractor.extract(path, extract_method, extract_path)", "response": "Extracts a single archive returns Promise - > path to extraction result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading - extract Resource or url returns Promise - > path.", "response": "def _download_extract(self, resource):\n    \"\"\"Download-extract `Resource` or url, returns Promise->path.\"\"\"\n    if isinstance(resource, six.string_types):\n      resource = resource_lib.Resource(url=resource)\n    def callback(path):\n      resource.path = path\n      return self._extract(resource)\n    return self._download(resource).then(callback)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading data for a given Kaggle competition.", "response": "def download_kaggle_data(self, competition_name):\n    \"\"\"Download data for a given Kaggle competition.\"\"\"\n    with self._downloader.tqdm():\n      kaggle_downloader = self._downloader.kaggle_downloader(competition_name)\n      urls = kaggle_downloader.competition_urls\n      files = kaggle_downloader.competition_files\n      return _map_promise(self._download,\n                          dict((f, u) for (f, u) in zip(files, urls)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download(self, url_or_urls):\n    # Add progress bar to follow the download state\n    with self._downloader.tqdm():\n      return _map_promise(self._download, url_or_urls)", "response": "Download given url or list of urls and extract the files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_archive(self, resource):\n    if isinstance(resource, six.string_types):\n      resource = resource_lib.Resource(path=resource)\n    return extractor.iter_archive(resource.path, resource.extract_method)", "response": "Returns iterator over files within archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts given path or list of path or dict of path of file to extract.", "response": "def extract(self, path_or_paths):\n    \"\"\"Extract given path(s).\n\n    Args:\n      path_or_paths: path or `list`/`dict` of path of file to extract. Each\n        path can be a `str` or `tfds.download.Resource`.\n\n    If not explicitly specified in `Resource`, the extraction method is deduced\n    from downloaded file name.\n\n    Returns:\n      extracted_path(s): `str`, The extracted paths matching the given input\n        path_or_paths.\n    \"\"\"\n    # Add progress bar to follow the download state\n    with self._extractor.tqdm():\n      return _map_promise(self._extract, path_or_paths)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download_and_extract(self, url_or_urls):\n    # Add progress bar to follow the download state\n    with self._downloader.tqdm():\n      with self._extractor.tqdm():\n        return _map_promise(self._download_extract, url_or_urls)", "response": "Download and extract given url_or_urls."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef manual_dir(self):\n    if not tf.io.gfile.exists(self._manual_dir):\n      raise AssertionError(\n          'Manual directory {} does not exist. Create it and download/extract '\n          'dataset artifacts in there.'.format(self._manual_dir))\n    return self._manual_dir", "response": "Returns the directory containing the manually extracted data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_builder_configs():\n  config_list = []\n  for corruption in _CORRUPTIONS:\n    for severity in range(1, 6):\n      config_list.append(\n          Cifar10CorruptedConfig(\n              name=corruption + '_' + str(severity),\n              version='0.0.1',\n              description='Corruption method: ' + corruption +\n              ', severity level: ' + str(severity),\n              corruption_type=corruption,\n              severity=severity,\n          ))\n  return config_list", "response": "Construct a list of 75 BuilderConfigs corresponding to\n."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _split_generators(self, dl_manager):\n    path = dl_manager.download_and_extract(_DOWNLOAD_URL)\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=1,\n            gen_kwargs={'data_dir': os.path.join(path, _DIRNAME)})\n    ]", "response": "Returns the test split of Cifar10."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_examples(self, data_dir):\n    corruption = self.builder_config.corruption\n    severity = self.builder_config.severity\n\n    images_file = os.path.join(data_dir, _CORRUPTIONS_TO_FILENAMES[corruption])\n    labels_file = os.path.join(data_dir, _LABELS_FILENAME)\n\n    with tf.io.gfile.GFile(labels_file, mode='rb') as f:\n      labels = np.load(f)\n\n    num_images = labels.shape[0] // 5\n    # Labels are stacked 5 times so we can just read the first iteration\n    labels = labels[:num_images]\n\n    with tf.io.gfile.GFile(images_file, mode='rb') as f:\n      images = np.load(f)\n\n    # Slice images corresponding to correct severity level\n    images = images[(severity - 1) * num_images:severity * num_images]\n\n    for image, label in zip(images, labels):\n      yield {\n          'image': image,\n          'label': label,\n      }", "response": "Generate corrupted Cifar10 test data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef document_single_builder(builder):\n  mod_name = builder.__class__.__module__\n  cls_name = builder.__class__.__name__\n  mod_file = sys.modules[mod_name].__file__\n  if mod_file.endswith(\"pyc\"):\n    mod_file = mod_file[:-1]\n\n  description_prefix = \"\"\n\n\n  if builder.builder_configs:\n    # Dataset with configs; document each one\n    config_docs = []\n    for config in builder.BUILDER_CONFIGS:\n      builder = tfds.builder(builder.name, config=config)\n      info = builder.info\n      # TODO(rsepassi): document the actual config object\n      config_doc = SINGLE_CONFIG_ENTRY.format(\n          builder_name=builder.name,\n          config_name=config.name,\n          description=config.description,\n          version=config.version,\n          feature_information=make_feature_information(info),\n          size=tfds.units.size_str(info.size_in_bytes),\n      )\n      config_docs.append(config_doc)\n    out_str = DATASET_WITH_CONFIGS_ENTRY.format(\n        snakecase_name=builder.name,\n        module_and_class=\"%s.%s\" % (tfds_mod_name(mod_name), cls_name),\n        cls_url=cls_url(mod_name),\n        config_names=\"\\n\".join([\n            CONFIG_BULLET.format(name=config.name,\n                                 description=config.description,\n                                 version=config.version,\n                                 size=tfds.units.size_str(tfds.builder(\n                                     builder.name, config=config)\n                                                          .info.size_in_bytes))\n            for config in builder.BUILDER_CONFIGS]),\n        config_cls=\"%s.%s\" % (tfds_mod_name(mod_name),\n                              type(builder.builder_config).__name__),\n        configs=\"\\n\".join(config_docs),\n        urls=format_urls(info.urls),\n        url=url_from_info(info),\n        supervised_keys=str(info.supervised_keys),\n        citation=make_citation(info.citation),\n        statistics_information=make_statistics_information(info),\n        description=builder.info.description,\n        description_prefix=description_prefix,\n    )\n  else:\n    info = builder.info\n    out_str = DATASET_ENTRY.format(\n        snakecase_name=builder.name,\n        module_and_class=\"%s.%s\" % (tfds_mod_name(mod_name), cls_name),\n        cls_url=cls_url(mod_name),\n        description=info.description,\n        description_prefix=description_prefix,\n        version=info.version,\n        feature_information=make_feature_information(info),\n        statistics_information=make_statistics_information(info),\n        urls=format_urls(info.urls),\n        url=url_from_info(info),\n        supervised_keys=str(info.supervised_keys),\n        citation=make_citation(info.citation),\n        size=tfds.units.size_str(info.size_in_bytes),\n    )\n\n  out_str = schema_org(builder) + \"\\n\" + out_str\n  return out_str", "response": "Doc string for a single builder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all builders organized by module in nested dicts.", "response": "def make_module_to_builder_dict(datasets=None):\n  \"\"\"Get all builders organized by module in nested dicts.\"\"\"\n  # pylint: disable=g-long-lambda\n  # dict to hold tfds->image->mnist->[builders]\n  module_to_builder = collections.defaultdict(\n      lambda: collections.defaultdict(\n          lambda: collections.defaultdict(list)))\n  # pylint: enable=g-long-lambda\n\n  if datasets:\n    builders = [tfds.builder(name) for name in datasets]\n  else:\n    builders = [\n        tfds.builder(name)\n        for name in tfds.list_builders()\n        if name not in BUILDER_BLACKLIST\n    ] + [tfds.builder(\"image_label_folder\", dataset_name=\"image_label_folder\")]\n\n  for builder in builders:\n    mod_name = builder.__class__.__module__\n    modules = mod_name.split(\".\")\n    if \"testing\" in modules:\n      continue\n\n    current_mod_ctr = module_to_builder\n    for mod in modules:\n      current_mod_ctr = current_mod_ctr[mod]\n    current_mod_ctr.append(builder)\n\n  module_to_builder = module_to_builder[\"tensorflow_datasets\"]\n  return module_to_builder"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_statistics_information(info):\n  if not info.splits.total_num_examples:\n    # That means that we have yet to calculate the statistics for this.\n    return \"None computed\"\n\n  stats = [(info.splits.total_num_examples, \"ALL\")]\n  for split_name, split_info in info.splits.items():\n    stats.append((split_info.num_examples, split_name.upper()))\n  # Sort reverse on number of examples.\n  stats.sort(reverse=True)\n  stats = \"\\n\".join([\n      \"{0:10} | {1:>10,}\".format(name, num_exs) for (num_exs, name) in stats\n  ])\n  return STATISTICS_TABLE.format(split_statistics=stats)", "response": "Make statistics information table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dataset_docs_str(datasets=None):\n  module_to_builder = make_module_to_builder_dict(datasets)\n\n  sections = sorted(list(module_to_builder.keys()))\n  section_tocs = []\n  section_docs = []\n  for section in sections:\n    builders = tf.nest.flatten(module_to_builder[section])\n    builders = sorted(builders, key=lambda b: b.name)\n    builder_docs = [document_single_builder(builder) for builder in builders]\n    section_doc = SECTION_DATASETS.format(\n        section_name=section, datasets=\"\\n\".join(builder_docs))\n    section_toc = create_section_toc(section, builders)\n\n\n    section_docs.append(section_doc)\n    section_tocs.append(section_toc)\n\n  full_doc = DOC.format(toc=\"\\n\".join(section_tocs),\n                        datasets=\"\\n\".join(section_docs))\n  return full_doc", "response": "Create dataset documentation string for given datasets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef schema_org(builder):\n  # pylint: disable=line-too-long\n  \"\"\"Builds schema.org microdata for DatasetSearch from DatasetBuilder.\n\n  Markup spec: https://developers.google.com/search/docs/data-types/dataset#dataset\n  Testing tool: https://search.google.com/structured-data/testing-tool\n  For Google Dataset Search: https://toolbox.google.com/datasetsearch\n\n  Microdata format was chosen over JSON-LD due to the fact that Markdown\n  rendering engines remove all <script> tags.\n\n  Args:\n    builder: `tfds.core.DatasetBuilder`\n\n  Returns:\n    HTML string with microdata\n  \"\"\"\n  # pylint: enable=line-too-long\n\n  properties = [\n      (lambda x: x.name, SCHEMA_ORG_NAME),\n      (lambda x: x.description, SCHEMA_ORG_DESC),\n      (lambda x: x.name, SCHEMA_ORG_URL),\n      (lambda x: (x.urls and x.urls[0]) or \"\", SCHEMA_ORG_SAMEAS)\n  ]\n\n  info = builder.info\n  out_str = SCHEMA_ORG_PRE\n  for extractor, template in properties:\n    val = extractor(info)\n    if val:\n      # We are using cgi module instead of html due to Python 2 compatibility\n      out_str += template.format(val=cgi.escape(val, quote=True).strip())\n  out_str += SCHEMA_ORG_POST\n\n  return out_str", "response": "Builds schema. org microdata for DatasetSearch from DatasetBuilder."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disk(radius, alias_blur=0.1, dtype=np.float32):\n  if radius <= 8:\n    length = np.arange(-8, 8 + 1)\n    ksize = (3, 3)\n  else:\n    length = np.arange(-radius, radius + 1)\n    ksize = (5, 5)\n  x_axis, y_axis = np.meshgrid(length, length)\n  aliased_disk = np.array((x_axis**2 + y_axis**2) <= radius**2, dtype=dtype)\n  aliased_disk /= np.sum(aliased_disk)\n  # supersample disk to antialias\n  return tfds.core.lazy_imports.cv2.GaussianBlur(\n      aliased_disk, ksize=ksize, sigmaX=alias_blur)", "response": "Generate a Gaussian blurring kernel with disk shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clipped_zoom(img, zoom_factor):\n  h = img.shape[0]\n  ch = int(np.ceil(h / float(zoom_factor)))\n  top_h = (h - ch) // 2\n\n  w = img.shape[1]\n  cw = int(np.ceil(w / float(zoom_factor)))\n  top_w = (w - cw) // 2\n\n  img = tfds.core.lazy_imports.scipy.ndimage.zoom(\n      img[top_h:top_h + ch, top_w:top_w + cw], (zoom_factor, zoom_factor, 1),\n      order=1)\n\n  # trim off any extra pixels\n  trim_top_h = (img.shape[0] - h) // 2\n  trim_top_w = (img.shape[1] - w) // 2\n\n  return img[trim_top_h:trim_top_h + h, trim_top_w:trim_top_w + w]", "response": "Zoom the image with clipping."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a heightmap using diamond - square algorithm.", "response": "def plasma_fractal(mapsize=512, wibbledecay=3):\n  \"\"\"Generate a heightmap using diamond-square algorithm.\n\n  Modification of the algorithm in\n  https://github.com/FLHerne/mapgen/blob/master/diamondsquare.py\n\n  Args:\n    mapsize: side length of the heightmap, must be a power of two.\n    wibbledecay: integer, decay factor.\n\n  Returns:\n    numpy 2d array, side length 'mapsize', of floats in [0,255].\n  \"\"\"\n  if mapsize & (mapsize - 1) != 0:\n    raise ValueError('mapsize must be a power of two.')\n  maparray = np.empty((mapsize, mapsize), dtype=np.float_)\n  maparray[0, 0] = 0\n  stepsize = mapsize\n  wibble = 100\n\n  def wibbledmean(array):\n    return array / 4 + wibble * np.random.uniform(-wibble, wibble, array.shape)\n\n  def fillsquares():\n    \"\"\"For each square, calculate middle value as mean of points + wibble.\"\"\"\n    cornerref = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n    squareaccum = cornerref + np.roll(cornerref, shift=-1, axis=0)\n    squareaccum += np.roll(squareaccum, shift=-1, axis=1)\n    maparray[stepsize // 2:mapsize:stepsize, stepsize //\n             2:mapsize:stepsize] = wibbledmean(squareaccum)\n\n  def filldiamonds():\n    \"\"\"For each diamond, calculate middle value as meanof points + wibble.\"\"\"\n    mapsize = maparray.shape[0]\n    drgrid = maparray[stepsize // 2:mapsize:stepsize, stepsize //\n                      2:mapsize:stepsize]\n    ulgrid = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n    ldrsum = drgrid + np.roll(drgrid, 1, axis=0)\n    lulsum = ulgrid + np.roll(ulgrid, -1, axis=1)\n    ltsum = ldrsum + lulsum\n    maparray[0:mapsize:stepsize, stepsize //\n             2:mapsize:stepsize] = wibbledmean(ltsum)\n    tdrsum = drgrid + np.roll(drgrid, 1, axis=1)\n    tulsum = ulgrid + np.roll(ulgrid, -1, axis=0)\n    ttsum = tdrsum + tulsum\n    maparray[stepsize //\n             2:mapsize:stepsize, 0:mapsize:stepsize] = wibbledmean(ttsum)\n\n  while stepsize >= 2:\n    fillsquares()\n    filldiamonds()\n    stepsize //= 2\n    wibble /= wibbledecay\n\n  maparray -= maparray.min()\n  return maparray / maparray.max()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gaussian_noise(x, severity=1):\n  c = [.08, .12, 0.18, 0.26, 0.38][severity - 1]\n  x = np.array(x) / 255.\n  x_clip = np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n  return around_and_astype(x_clip)", "response": "Gaussian noise corruption to images.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshoots noise corruption to images.", "response": "def shot_noise(x, severity=1):\n  \"\"\"Shot noise corruption to images.\n\n  Args:\n    x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255].\n    severity: integer, severity of corruption.\n\n  Returns:\n    numpy array, image with uint8 pixels in [0,255]. Added shot noise.\n  \"\"\"\n  c = [60, 25, 12, 5, 3][severity - 1]\n  x = np.array(x) / 255.\n  x_clip = np.clip(np.random.poisson(x * c) / float(c), 0, 1) * 255\n  return around_and_astype(x_clip)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef impulse_noise(x, severity=1):\n  c = [.03, .06, .09, 0.17, 0.27][severity - 1]\n  x = tfds.core.lazy_imports.skimage.util.random_noise(\n      np.array(x) / 255., mode='s&p', amount=c)\n  x_clip = np.clip(x, 0, 1) * 255\n  return around_and_astype(x_clip)", "response": "Impulse noise corruption to images.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef defocus_blur(x, severity=1):\n  c = [(3, 0.1), (4, 0.5), (6, 0.5), (8, 0.5), (10, 0.5)][severity - 1]\n  x = np.array(x) / 255.\n  kernel = disk(radius=c[0], alias_blur=c[1])\n  channels = []\n  for d in range(3):\n    channels.append(tfds.core.lazy_imports.cv2.filter2D(x[:, :, d], -1, kernel))\n  channels = np.array(channels).transpose((1, 2, 0))  # 3x224x224 -> 224x224x3\n  x_clip = np.clip(channels, 0, 1) * 255\n  return around_and_astype(x_clip)", "response": "Defocus blurring to images."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfrosts glass blurring to images.", "response": "def frosted_glass_blur(x, severity=1):\n  \"\"\"Frosted glass blurring to images.\n\n  Apply frosted glass blurring to images by shuffling pixels locally.\n\n  Args:\n    x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255].\n    severity: integer, severity of corruption.\n\n  Returns:\n    numpy array, image with uint8 pixels in [0,255]. Applied frosted glass blur.\n  \"\"\"\n  # sigma, max_delta, iterations\n  c = [(0.7, 1, 2), (0.9, 2, 1), (1, 2, 3), (1.1, 3, 2), (1.5, 4,\n                                                          2)][severity - 1]\n  x = np.uint8(\n      tfds.core.lazy_imports.skimage.filters.gaussian(\n          np.array(x) / 255., sigma=c[0], multichannel=True) * 255)\n\n  # locally shuffle pixels\n  for _ in range(c[2]):\n    for h in range(x.shape[0] - c[1], c[1], -1):\n      for w in range(x.shape[1] - c[1], c[1], -1):\n        dx, dy = np.random.randint(-c[1], c[1], size=(2,))\n        h_prime, w_prime = h + dy, w + dx\n        # swap\n        x[h, w], x[h_prime, w_prime] = x[h_prime, w_prime], x[h, w]\n  x_clip = np.clip(\n      tfds.core.lazy_imports.skimage.filters.gaussian(\n          x / 255., sigma=c[0], multichannel=True), 0, 1)\n  x_clip *= 255\n  return around_and_astype(x_clip)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nzooming blurring to images.", "response": "def zoom_blur(x, severity=1):\n  \"\"\"Zoom blurring to images.\n\n  Applying zoom blurring to images by zooming the central part of the images.\n\n  Args:\n    x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255].\n    severity: integer, severity of corruption.\n\n  Returns:\n    numpy array, image with uint8 pixels in [0,255]. Applied zoom blur.\n  \"\"\"\n  c = [\n      np.arange(1, 1.11, 0.01),\n      np.arange(1, 1.16, 0.01),\n      np.arange(1, 1.21, 0.02),\n      np.arange(1, 1.26, 0.02),\n      np.arange(1, 1.31, 0.03)\n  ][severity - 1]\n  x = (np.array(x) / 255.).astype(np.float32)\n  out = np.zeros_like(x)\n  for zoom_factor in c:\n    out += clipped_zoom(x, zoom_factor)\n  x = (x + out) / (len(c) + 1)\n  x_clip = np.clip(x, 0, 1) * 255\n  return around_and_astype(x_clip)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfogging corruption to images.", "response": "def fog(x, severity=1):\n  \"\"\"Fog corruption to images.\n\n  Adding fog to images. Fog is generated by diamond-square algorithm.\n\n  Args:\n    x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255].\n    severity: integer, severity of corruption.\n\n  Returns:\n    numpy array, image with uint8 pixels in [0,255]. Added fog.\n  \"\"\"\n  c = [(1.5, 2), (2., 2), (2.5, 1.7), (2.5, 1.5), (3., 1.4)][severity - 1]\n  x = np.array(x) / 255.\n  max_val = x.max()\n  mapsize = 512\n  shape = x.shape\n  max_length = max(shape[0], shape[1])\n  if max_length > mapsize:\n    mapsize = 2**int(np.ceil(np.log2(float(max_length))))\n  tmp = plasma_fractal(mapsize=mapsize, wibbledecay=c[1])\n  tmp = tmp[:x.shape[0], :x.shape[1]]\n  tmp = tmp[..., np.newaxis]\n  x += c[0] * tmp\n  x_clip = np.clip(x * max_val / (max_val + c[0]), 0, 1) * 255\n  return around_and_astype(x_clip)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef brightness(x, severity=1):\n  c = [.1, .2, .3, .4, .5][severity - 1]\n\n  x = np.array(x) / 255.\n  x = tfds.core.lazy_imports.skimage.color.rgb2hsv(x)\n  x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n  x = tfds.core.lazy_imports.skimage.color.hsv2rgb(x)\n  x_clip = np.clip(x, 0, 1) * 255\n  return around_and_astype(x_clip)", "response": "Change brightness of images."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contrast(x, severity=1):\n  c = [0.4, .3, .2, .1, .05][severity - 1]\n\n  x = np.array(x) / 255.\n  means = np.mean(x, axis=(0, 1), keepdims=True)\n  x_clip = np.clip((x - means) * c + means, 0, 1) * 255\n  return around_and_astype(x_clip)", "response": "Change contrast of images."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconducts elastic transform to images.", "response": "def elastic(x, severity=1):\n  \"\"\"Conduct elastic transform to images.\n\n  Elastic transform is performed on small patches of the images.\n\n  Args:\n    x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255].\n    severity: integer, severity of corruption.\n\n  Returns:\n    numpy array, image with uint8 pixels in [0,255]. Applied elastic transform.\n  \"\"\"\n  c = [(244 * 2, 244 * 0.7, 244 * 0.1), (244 * 2, 244 * 0.08, 244 * 0.2),\n       (244 * 0.05, 244 * 0.01, 244 * 0.02), (244 * 0.07, 244 * 0.01,\n                                              244 * 0.02),\n       (244 * 0.12, 244 * 0.01, 244 * 0.02)][severity - 1]\n\n  image = np.array(x, dtype=np.float32) / 255.\n  shape = image.shape\n  shape_size = shape[:2]\n\n  # random affine\n  center_square = np.float32(shape_size) // 2\n  square_size = min(shape_size) // 3\n  pts1 = np.float32([\n      center_square + square_size,\n      [center_square[0] + square_size, center_square[1] - square_size],\n      center_square - square_size\n  ])\n  pts2 = pts1 + np.random.uniform(\n      -c[2], c[2], size=pts1.shape).astype(np.float32)\n  affine_trans = tfds.core.lazy_imports.cv2.getAffineTransform(pts1, pts2)\n  image = tfds.core.lazy_imports.cv2.warpAffine(\n      image,\n      affine_trans,\n      shape_size[::-1],\n      borderMode=tfds.core.lazy_imports.cv2.BORDER_REFLECT_101)\n\n  dx = (tfds.core.lazy_imports.skimage.filters.gaussian(\n      np.random.uniform(-1, 1, size=shape[:2]),\n      c[1],\n      mode='reflect',\n      truncate=3) * c[0]).astype(np.float32)\n  dy = (tfds.core.lazy_imports.skimage.filters.gaussian(\n      np.random.uniform(-1, 1, size=shape[:2]),\n      c[1],\n      mode='reflect',\n      truncate=3) * c[0]).astype(np.float32)\n  dx, dy = dx[..., np.newaxis], dy[..., np.newaxis]\n\n  x, y, z = np.meshgrid(\n      np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n  indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx,\n                                                    (-1, 1)), np.reshape(\n                                                        z, (-1, 1))\n  x_clip = np.clip(\n      tfds.core.lazy_imports.scipy.ndimage.interpolation.map_coordinates(\n          image, indices, order=1, mode='reflect').reshape(shape), 0, 1) * 255\n  return around_and_astype(x_clip)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pixelate(x, severity=1):\n  c = [0.6, 0.5, 0.4, 0.3, 0.25][severity - 1]\n  shape = x.shape\n  x = tfds.core.lazy_imports.PIL_Image.fromarray(x.astype(np.uint8))\n  x = x.resize((int(shape[1] * c), int(shape[0] * c)))\n  x = x.resize((shape[1], shape[0]))\n  return np.asarray(x)", "response": "Pixelate images.\n Conduct pixelating corruptions to images by first shrinking the images and then resizing to original size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef jpeg_compression(x, severity=1):\n  c = [25, 18, 15, 10, 7][severity - 1]\n  x = tfds.core.lazy_imports.PIL_Image.fromarray(x.astype(np.uint8))\n  output = io.BytesIO()\n  x.save(output, 'JPEG', quality=c)\n  output.seek(0)\n  x = tfds.core.lazy_imports.PIL_Image.open(output)\n  return np.asarray(x)", "response": "Conduct jpeg compression to images.\n Arguments x numpy array uncorrupted image"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating over items of dictionaries grouped by their keys.", "response": "def zip_dict(*dicts):\n  \"\"\"Iterate over items of dictionaries grouped by their keys.\"\"\"\n  for key in set(itertools.chain(*dicts)):  # set merge all keys\n    # Will raise KeyError if the dict don't have the same keys\n    yield key, tuple(d[key] for d in dicts)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying a function recursively to each element of a nested data struct.", "response": "def map_nested(function, data_struct, dict_only=False, map_tuple=False):\n  \"\"\"Apply a function recursively to each element of a nested data struct.\"\"\"\n\n  # Could add support for more exotic data_struct, like OrderedDict\n  if isinstance(data_struct, dict):\n    return {\n        k: map_nested(function, v, dict_only, map_tuple)\n        for k, v in data_struct.items()\n    }\n  elif not dict_only:\n    types = [list]\n    if map_tuple:\n      types.append(tuple)\n    if isinstance(data_struct, tuple(types)):\n      mapped = [map_nested(function, v, dict_only, map_tuple)\n                for v in data_struct]\n      if isinstance(data_struct, list):\n        return mapped\n      else:\n        return tuple(mapped)\n  # Singleton\n  return function(data_struct)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nzips together and return a data struct with the same shape.", "response": "def zip_nested(arg0, *args, **kwargs):\n  \"\"\"Zip data struct together and return a data struct with the same shape.\"\"\"\n  # Python 2 do not support kwargs only arguments\n  dict_only = kwargs.pop(\"dict_only\", False)\n  assert not kwargs\n\n  # Could add support for more exotic data_struct, like OrderedDict\n  if isinstance(arg0, dict):\n    return {\n        k: zip_nested(*a, dict_only=dict_only) for k, a in zip_dict(arg0, *args)\n    }\n  elif not dict_only:\n    if isinstance(arg0, list):\n      return [zip_nested(*a, dict_only=dict_only) for a in zip(arg0, *args)]\n  # Singleton\n  return (arg0,) + args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_proto_cls(proto_cls):\n\n  def decorator(cls):\n    \"\"\"Decorator applied to the class.\"\"\"\n\n    class ProtoCls(object):\n      \"\"\"Base class simulating the protobuf.\"\"\"\n\n      def __init__(self, *args, **kwargs):\n        super(ProtoCls, self).__setattr__(\n            \"_ProtoCls__proto\",\n            proto_cls(*args, **kwargs),\n        )\n\n      def __getattr__(self, attr_name):\n        return getattr(self.__proto, attr_name)\n\n      def __setattr__(self, attr_name, new_value):\n        try:\n          return setattr(self.__proto, attr_name, new_value)\n        except AttributeError:\n          return super(ProtoCls, self).__setattr__(attr_name, new_value)\n\n      def __eq__(self, other):\n        return self.__proto, other.get_proto()\n\n      def get_proto(self):\n        return self.__proto\n\n      def __repr__(self):\n        return \"<{cls_name}\\n{proto_repr}\\n>\".format(\n            cls_name=cls.__name__, proto_repr=repr(self.__proto))\n\n    decorator_cls = type(cls.__name__, (cls, ProtoCls), {\n        \"__doc__\": cls.__doc__,\n    })\n    return decorator_cls\n  return decorator", "response": "A class decorator that creates a new object that is a copy of the given protobuf class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tfds_dir():\n  return os.path.dirname(os.path.dirname(os.path.dirname(__file__)))", "response": "Path to tensorflow_datasets directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef atomic_write(path, mode):\n  tmp_path = \"%s%s_%s\" % (path, constants.INCOMPLETE_SUFFIX, uuid.uuid4().hex)\n  with tf.io.gfile.GFile(tmp_path, mode) as file_:\n    yield file_\n  tf.io.gfile.rename(tmp_path, path, overwrite=True)", "response": "Writes to path atomically by writing to temp file and renaming it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_checksum_digest(path, checksum_cls=hashlib.sha256):\n  checksum = checksum_cls()\n  size = 0\n  with tf.io.gfile.GFile(path, \"rb\") as f:\n    while True:\n      block = f.read(io.DEFAULT_BUFFER_SIZE)\n      size += len(block)\n      if not block:\n        break\n      checksum.update(block)\n  return checksum.hexdigest(), size", "response": "Given a hash constructor returns a checksum digest and size of file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reraise(additional_msg):\n  exc_type, exc_value, exc_traceback = sys.exc_info()\n  msg = str(exc_value) + \"\\n\" + additional_msg\n  six.reraise(exc_type, exc_type(msg), exc_traceback)", "response": "Reraise an exception with an additional message."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget attr that handles dots in attr name.", "response": "def rgetattr(obj, attr, *args):\n  \"\"\"Get attr that handles dots in attr name.\"\"\"\n  def _getattr(obj, attr):\n    return getattr(obj, attr, *args)\n  return functools.reduce(_getattr, [obj] + attr.split(\".\"))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of SplitGenerators.", "response": "def _split_generators(self, dl_manager):\n    \"\"\"Returns SplitGenerators.\"\"\"\n    image_tar_file = os.path.join(dl_manager.manual_dir,\n                                  self.builder_config.file_name)\n    if not tf.io.gfile.exists(image_tar_file):\n      # The current celebahq generation code depends on a concrete version of\n      # pillow library and cannot be easily ported into tfds.\n      msg = \"You must download the dataset files manually and place them in: \"\n      msg += dl_manager.manual_dir\n      msg += \" as .tar files. See testing/test_data/fake_examples/celeb_a_hq \"\n      raise AssertionError(msg)\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=50,\n            gen_kwargs={\"archive\": dl_manager.iter_archive(image_tar_file)},\n        )\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_examples(self, filepath):\n    rows_per_pair_id = collections.defaultdict(list)\n\n    with tf.io.gfile.GFile(filepath) as f:\n      reader = csv.DictReader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n      for row in reader:\n        rows_per_pair_id[row['pairID']].append(row)\n\n    for rows in six.itervalues(rows_per_pair_id):\n      premise = {row['language']: row['sentence1'] for row in rows}\n      hypothesis = {row['language']: row['sentence2'] for row in rows}\n      yield {\n          'premise': premise,\n          'hypothesis': hypothesis,\n          'label': rows[0]['gold_label'],\n      }", "response": "This function returns the examples in the raw text form."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a single example from the VOC2007 data.", "response": "def _generate_example(self, data_path, image_id):\n    \"\"\"Yields examples.\"\"\"\n    image_filepath = os.path.join(\n        data_path, \"VOCdevkit/VOC2007/JPEGImages\", \"{}.jpg\".format(image_id))\n    annon_filepath = os.path.join(\n        data_path, \"VOCdevkit/VOC2007/Annotations\", \"{}.xml\".format(image_id))\n\n    def _get_example_objects():\n      \"\"\"Function to get all the objects from the annotation XML file.\"\"\"\n      with tf.io.gfile.GFile(annon_filepath, \"r\") as f:\n        root = xml.etree.ElementTree.parse(f).getroot()\n\n        size = root.find(\"size\")\n        width = float(size.find(\"width\").text)\n        height = float(size.find(\"height\").text)\n\n        for obj in root.findall(\"object\"):\n          # Get object's label name.\n          label = obj.find(\"name\").text.lower()\n          # Get objects' pose name.\n          pose = obj.find(\"pose\").text.lower()\n          is_truncated = (obj.find(\"truncated\").text == \"1\")\n          is_difficult = (obj.find(\"difficult\").text == \"1\")\n          bndbox = obj.find(\"bndbox\")\n          xmax = float(bndbox.find(\"xmax\").text)\n          xmin = float(bndbox.find(\"xmin\").text)\n          ymax = float(bndbox.find(\"ymax\").text)\n          ymin = float(bndbox.find(\"ymin\").text)\n          yield {\n              \"label\": label,\n              \"pose\": pose,\n              \"bbox\": tfds.features.BBox(\n                  ymin / height, xmin / width, ymax / height, xmax / width),\n              \"is_truncated\": is_truncated,\n              \"is_difficult\": is_difficult,\n          }\n\n    objects = list(_get_example_objects())\n    # Use set() to remove duplicates\n    labels = sorted(set(obj[\"label\"] for obj in objects))\n    labels_no_difficult = sorted(set(\n        obj[\"label\"] for obj in objects if obj[\"is_difficult\"] == 0\n    ))\n    return {\n        \"image\": image_filepath,\n        \"image/filename\": image_id + \".jpg\",\n        \"objects\": objects,\n        \"labels\": labels,\n        \"labels_no_difficult\": labels_no_difficult,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_encoding_format(self, encoding_format):\n    supported = ENCODE_FN.keys()\n    if encoding_format not in supported:\n      raise ValueError('`encoding_format` must be one of %s.' % supported)\n    self._encoding_format = encoding_format", "response": "Update the encoding format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _encode_image(self, np_image):\n    if np_image.dtype != np.uint8:\n      raise ValueError('Image should be uint8. Detected: %s.' % np_image.dtype)\n    utils.assert_shape_match(np_image.shape, self._shape)\n    return self._runner.run(ENCODE_FN[self._encoding_format], np_image)", "response": "Returns np_image encoded as jpeg or png."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_example(self, image_or_path_or_fobj):\n    if isinstance(image_or_path_or_fobj, np.ndarray):\n      encoded_image = self._encode_image(image_or_path_or_fobj)\n    elif isinstance(image_or_path_or_fobj, six.string_types):\n      with tf.io.gfile.GFile(image_or_path_or_fobj, 'rb') as image_f:\n        encoded_image = image_f.read()\n    else:\n      encoded_image = image_or_path_or_fobj.read()\n    return encoded_image", "response": "Convert the given image into a dict convertible to tf example."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode_example(self, example):\n    img = tf.image.decode_image(\n        example, channels=self._shape[-1], dtype=tf.uint8)\n    img.set_shape(self._shape)\n    return img", "response": "Reconstruct the image from the tf example."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the metadata of the current dataset to a file.", "response": "def save_metadata(self, data_dir, feature_name=None):\n    \"\"\"See base class for details.\"\"\"\n    filepath = _get_metadata_filepath(data_dir, feature_name)\n    with tf.io.gfile.GFile(filepath, 'w') as f:\n      json.dump({\n          'shape': [-1 if d is None else d for d in self._shape],\n          'encoding_format': self._encoding_format,\n      }, f, sort_keys=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_metadata(self, data_dir, feature_name=None):\n    # Restore names if defined\n    filepath = _get_metadata_filepath(data_dir, feature_name)\n    if tf.io.gfile.exists(filepath):\n      with tf.io.gfile.GFile(filepath, 'r') as f:\n        info_data = json.load(f)\n      self.set_encoding_format(info_data['encoding_format'])\n      self.set_shape([None if d == -1 else d for d in info_data['shape']])", "response": "Load metadata from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_moving_sequence(image, pad_lefts, total_padding):\n\n  with tf.name_scope(\"moving_sequence\"):\n    def get_padded_image(args):\n      pad_left, = args\n      pad_right = total_padding - pad_left\n      padding = tf.stack([pad_left, pad_right], axis=-1)\n      z = tf.zeros((1, 2), dtype=pad_left.dtype)\n      padding = tf.concat([padding, z], axis=0)\n      return tf.pad(image, padding)\n\n    padded_images = tf.map_fn(\n        get_padded_image, [pad_lefts], dtype=tf.uint8, infer_shape=False,\n        back_prop=False)\n\n  return padded_images", "response": "Create a moving image sequence from the given image a left padding values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a linear trajectory from x0.", "response": "def _get_linear_trajectory(x0, velocity, t):\n  \"\"\"Construct a linear trajectory from x0.\n\n  Args:\n    x0: N-D float tensor.\n    velocity: N-D float tensor\n    t: [sequence_length]-length float tensor\n\n  Returns:\n    x: [sequence_length, ndims] float tensor.\n  \"\"\"\n  x0 = tf.convert_to_tensor(x0)\n  velocity = tf.convert_to_tensor(velocity)\n  t = tf.convert_to_tensor(t)\n  if x0.shape.ndims != 1:\n    raise ValueError(\"x0 must be a rank 1 tensor\")\n  if velocity.shape.ndims != 1:\n    raise ValueError(\"velocity must be a rank 1 tensor\")\n  if t.shape.ndims != 1:\n    raise ValueError(\"t must be a rank 1 tensor\")\n  x0 = tf.expand_dims(x0, axis=0)\n  velocity = tf.expand_dims(velocity, axis=0)\n  dx = velocity * tf.expand_dims(t, axis=-1)\n  linear_trajectories = x0 + dx\n  assert linear_trajectories.shape.ndims == 2, \\\n    \"linear_trajectories should be a rank 2 tensor\"\n  return linear_trajectories"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef image_as_moving_sequence(\n    image, sequence_length=20, output_size=(64, 64), velocity=0.1,\n    start_position=None):\n  \"\"\"Turn simple static images into sequences of the originals bouncing around.\n\n  Adapted from Srivastava et al.\n  http://www.cs.toronto.edu/~nitish/unsupervised_video/\n\n  Example usage:\n  ```python\n  import tensorflow as tf\n  import tensorflow_datasets as tfds\n  from tensorflow_datasets.video import moving_sequence\n  tf.compat.v1.enable_eager_execution()\n\n  def animate(sequence):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.animation as animation\n    sequence = np.squeeze(sequence, axis=-1)\n\n    fig = plt.figure()\n    plt.axis(\"off\")\n    ims = [[plt.imshow(im, cmap=\"gray\", animated=True)] for im in sequence]\n    # don't remove `anim =` as linter may suggets\n    # weird behaviour, plot will freeze on last frame\n    anim = animation.ArtistAnimation(\n        fig, ims, interval=50, blit=True, repeat_delay=100)\n\n    plt.show()\n    plt.close()\n\n\n  tf.enable_eager_execution()\n  mnist_ds = tfds.load(\"mnist\", split=tfds.Split.TRAIN, as_supervised=True)\n  mnist_ds = mnist_ds.repeat().shuffle(1024)\n\n  def map_fn(image, label):\n    sequence = moving_sequence.image_as_moving_sequence(\n        image, sequence_length=20)\n    return sequence.image_sequence\n\n  moving_mnist_ds = mnist_ds.map(map_fn).batch(2).map(\n      lambda x: dict(image_sequence=tf.reduce_max(x, axis=0)))\n\n  # # for comparison with test data provided by original authors\n  # moving_mnist_ds = tfds.load(\"moving_mnist\", split=tfds.Split.TEST)\n\n  for seq in moving_mnist_ds:\n    animate(seq[\"image_sequence\"].numpy())\n  ```\n\n  Args:\n    image: [in_h, in_w, n_channels] tensor defining the sub-image to be bouncing\n        around.\n    sequence_length: int, length of sequence.\n    output_size: (out_h, out_w) size returned images.\n    velocity: scalar speed or 2D velocity of image. If scalar, the 2D\n        velocity is randomly generated with this magnitude. This is the\n        normalized distance moved each time step by the sub-image, where\n        normalization occurs over the feasible distance the sub-image can move\n        e.g if the input image is [10 x 10] and the output image is [60 x 60],\n        a speed of 0.1 means the sub-image moves (60 - 10) * 0.1 = 5 pixels per\n        time step.\n    start_position: 2D float32 normalized initial position of each\n        image in [0, 1]. Randomized uniformly if not given.\n\n  Returns:\n    `MovingSequence` namedtuple containing:\n        `image_sequence`:\n          [sequence_length, out_h, out_w, n_channels] image at each time step.\n          padded values are all zero. Same dtype as input image.\n        `trajectory`: [sequence_length, 2] float32 in [0, 1]\n          2D normalized coordinates of the image at every time step.\n        `start_position`: 2D float32 initial position in [0, 1].\n          2D normalized initial position of image. Same as input if provided,\n          otherwise the randomly value generated.\n        `velocity`: 2D float32 normalized velocity. Same as input velocity\n          if provided as a 2D tensor, otherwise the random velocity generated.\n  \"\"\"\n  ndims = 2\n  image = tf.convert_to_tensor(image)\n  if image.shape.ndims != 3:\n    raise ValueError(\"image must be rank 3, got %s\" % str(image))\n  output_size = tf.TensorShape(output_size)\n  if len(output_size) != ndims:\n    raise ValueError(\"output_size must have exactly %d elements, got %s\"\n                     % (ndims, output_size))\n  image_shape = tf.shape(image)\n  if start_position is None:\n    start_position = tf.random.uniform((ndims,), dtype=tf.float32)\n  elif start_position.shape != (ndims,):\n    raise ValueError(\"start_positions must (%d,)\" % ndims)\n  velocity = tf.convert_to_tensor(velocity, dtype=tf.float32)\n  if velocity.shape.ndims == 0:\n    velocity = _get_random_unit_vector(ndims, tf.float32) * velocity\n  elif velocity.shape.ndims != 1:\n    raise ValueError(\"velocity must be rank 0 or rank 1, got %s\" % velocity)\n  t = tf.range(sequence_length, dtype=tf.float32)\n  trajectory = _get_linear_trajectory(start_position, velocity, t)\n  trajectory = _bounce_to_bbox(trajectory)\n\n  total_padding = output_size - image_shape[:2]\n\n  if not tf.executing_eagerly():\n    cond = tf.compat.v1.assert_greater(total_padding, -1)\n    with tf.control_dependencies([cond]):\n      total_padding = tf.identity(total_padding)\n\n  sequence_pad_lefts = tf.cast(\n      tf.math.round(trajectory * tf.cast(total_padding, tf.float32)), tf.int32)\n\n  sequence = _create_moving_sequence(image, sequence_pad_lefts, total_padding)\n  sequence.set_shape(\n      [sequence_length] + output_size.as_list() + [image.shape[-1]])\n  return MovingSequence(\n      image_sequence=sequence,\n      trajectory=trajectory,\n      start_position=start_position,\n      velocity=velocity)", "response": "Turn simple static images into sequences of the originals bouncing around."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _str_to_version(version_str, allow_wildcard=False):\n  reg = _VERSION_WILDCARD_REG if allow_wildcard else _VERSION_RESOLVED_REG\n  res = reg.match(version_str)\n  if not res:\n    msg = \"Invalid version '{}'. Format should be x.y.z\".format(version_str)\n    if allow_wildcard:\n      msg += \" with {x,y,z} being digits or wildcard.\"\n    else:\n      msg += \" with {x,y,z} being digits.\"\n    raise ValueError(msg)\n  return tuple(\n      v if v == \"*\" else int(v)\n      for v in [res.group(\"major\"), res.group(\"minor\"), res.group(\"patch\")])", "response": "Return the tuple major minor patch from the str."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if other_version matches the current version.", "response": "def match(self, other_version):\n    \"\"\"Returns True if other_version matches.\n\n    Args:\n      other_version: string, of the form \"x[.y[.x]]\" where {x,y,z} can be a\n        number or a wildcard.\n    \"\"\"\n    major, minor, patch = _str_to_version(other_version, allow_wildcard=True)\n    return (major in [self.major, \"*\"] and minor in [self.minor, \"*\"]\n            and patch in [self.patch, \"*\"])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning labels for validation.", "response": "def _get_validation_labels(val_path):\n    \"\"\"Returns labels for validation.\n\n    Args:\n      val_path: path to TAR file containing validation images. It is used to\n      retrieve the name of pictures and associate them to labels.\n\n    Returns:\n      dict, mapping from image name (str) to label (str).\n    \"\"\"\n    labels_path = tfds.core.get_tfds_path(_VALIDATION_LABELS_FNAME)\n    with tf.io.gfile.GFile(labels_path) as labels_f:\n      labels = labels_f.read().strip().split('\\n')\n    with tf.io.gfile.GFile(val_path, 'rb') as tar_f_obj:\n      tar = tarfile.open(mode='r:', fileobj=tar_f_obj)\n      images = sorted(tar.getnames())\n    return dict(zip(images, labels))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a temporary filename based on filename.", "response": "def get_incomplete_path(filename):\n  \"\"\"Returns a temporary filename based on filename.\"\"\"\n  random_suffix = \"\".join(\n      random.choice(string.ascii_uppercase + string.digits) for _ in range(6))\n  return filename + \".incomplete\" + random_suffix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate temporary files for filenames and rename on exit.", "response": "def _incomplete_files(filenames):\n  \"\"\"Create temporary files for filenames and rename on exit.\"\"\"\n  tmp_files = [get_incomplete_path(f) for f in filenames]\n  try:\n    yield tmp_files\n    for tmp, output in zip(tmp_files, filenames):\n      tf.io.gfile.rename(tmp, output)\n  finally:\n    for tmp in tmp_files:\n      if tf.io.gfile.exists(tmp):\n        tf.io.gfile.remove(tmp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates temporary dir for dirname and rename on exit.", "response": "def incomplete_dir(dirname):\n  \"\"\"Create temporary dir for dirname and rename on exit.\"\"\"\n  tmp_dir = get_incomplete_path(dirname)\n  tf.io.gfile.makedirs(tmp_dir)\n  try:\n    yield tmp_dir\n    tf.io.gfile.rename(tmp_dir, dirname)\n  finally:\n    if tf.io.gfile.exists(tmp_dir):\n      tf.io.gfile.rmtree(tmp_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _shuffle_tfrecord(path, random_gen):\n  # Read all records\n  record_iter = tf.compat.v1.io.tf_record_iterator(path)\n  all_records = [\n      r for r in utils.tqdm(\n          record_iter, desc=\"Reading...\", unit=\" examples\", leave=False)\n  ]\n  # Shuffling in memory\n  random_gen.shuffle(all_records)\n  # Write all record back\n  with tf.io.TFRecordWriter(path) as writer:\n    for record in utils.tqdm(\n        all_records, desc=\"Writing...\", unit=\" examples\", leave=False):\n      writer.write(record)", "response": "Shuffle a single record file in memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite generated str records to output_files in round - robin order.", "response": "def _write_tfrecords_from_generator(generator, output_files, shuffle=True):\n  \"\"\"Writes generated str records to output_files in round-robin order.\"\"\"\n  if do_files_exist(output_files):\n    raise ValueError(\n        \"Pre-processed files already exists: {}.\".format(output_files))\n\n  with _incomplete_files(output_files) as tmp_files:\n    # Write all shards\n    writers = [tf.io.TFRecordWriter(fname) for fname in tmp_files]\n    with _close_on_exit(writers) as writers:\n      logging.info(\"Writing TFRecords\")\n      _round_robin_write(writers, generator)\n    # Shuffle each shard\n    if shuffle:\n      # WARNING: Using np instead of Python random because Python random\n      # produce different values between Python 2 and 3 and between\n      # architectures\n      random_gen = np.random.RandomState(42)\n      for path in utils.tqdm(\n          tmp_files, desc=\"Shuffling...\", unit=\" shard\", leave=False):\n        _shuffle_tfrecord(path, random_gen=random_gen)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _round_robin_write(writers, generator):\n  for i, example in enumerate(utils.tqdm(\n      generator, unit=\" examples\", leave=False)):\n    writers[i % len(writers)].write(example)", "response": "Write records from generator round - robin across writers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a single item to a tf. train. Feature.", "response": "def _item_to_tf_feature(item, key_name):\n  \"\"\"Single item to a tf.train.Feature.\"\"\"\n  v = item\n  if isinstance(v, (list, tuple)) and not v:\n    raise ValueError(\n        \"Feature {} received an empty list value, so is unable to infer the \"\n        \"feature type to record. To support empty value, the corresponding \"\n        \"FeatureConnector should return a numpy array with the correct dtype \"\n        \"instead of a Python list.\".format(key_name)\n    )\n\n  # Handle strings/bytes first\n  if isinstance(v, (six.binary_type, six.string_types)):\n    v = [tf.compat.as_bytes(v)]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n  elif (isinstance(v, (tuple, list)) and\n        all(isinstance(x, (six.binary_type, six.string_types)) for x in v)):\n    v = [tf.compat.as_bytes(x) for x in v]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n  elif (isinstance(v, np.ndarray) and\n        (v.dtype.kind in (\"U\", \"S\") or v.dtype == object)):  # binary or unicode\n    v = [tf.compat.as_bytes(x) for x in v.flatten()]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n\n  # Use NumPy for numeric types\n  v = np.array(v).flatten()  # Convert v into a 1-d array\n\n  if np.issubdtype(v.dtype, np.integer):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n  elif np.issubdtype(v.dtype, np.floating):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=v))\n  else:\n    raise ValueError(\n        \"Value received: {}.\\n\"\n        \"tf.train.Feature does not support type {} for feature key {}. \"\n        \"This may indicate that one of the FeatureConnectors received an \"\n        \"unsupported value as input.\".format(repr(v), repr(type(v)), key_name)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _dict_to_tf_features(example_dict):\n  features = {k: _item_to_tf_feature(v, k) for k, v\n              in six.iteritems(example_dict)}\n  return tf.train.Features(feature=features)", "response": "Builds tf. train. Features from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps around Tqdm which can be updated in threads. Arguments are passed to utils. async_tqdm.", "response": "def _async_tqdm(*args, **kwargs):\n  \"\"\"Wrapper around Tqdm which can be updated in threads.\n\n  Usage:\n\n  ```\n  with utils.async_tqdm(...) as pbar:\n    # pbar can then be modified inside a thread\n    # pbar.update_total(3)\n    # pbar.update()\n  ```\n\n  Args:\n    *args: args of tqdm\n    **kwargs: kwargs of tqdm\n\n  Yields:\n    pbar: Async pbar which can be shared between threads.\n  \"\"\"\n  with tqdm_lib.tqdm(*args, **kwargs) as pbar:\n    pbar = _TqdmPbarAsync(pbar)\n    yield pbar\n    pbar.clear()  # pop pbar from the active list of pbar\n    print()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_total(self, n=1):\n    with self._lock:\n      self._pbar.total += n\n      self.refresh()", "response": "Increment total pbar value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_pcollection(self, pipeline, folder, split):\n    beam = tfds.core.lazy_imports.apache_beam\n\n    split_type = self.builder_config.split_type\n    filename = os.path.join(folder, \"{}.tar.gz\".format(split_type))\n\n\n    def _extract_data(inputs):\n      \"\"\"Extracts files from the tar archives.\"\"\"\n      filename, split = inputs\n      with tf.io.gfile.GFile(filename, \"rb\") as f:\n        with tarfile.open(fileobj=f, mode=\"r\") as tar:\n          for tarinfo in tar:\n            split_name = tarinfo.name.split(\"_\")\n            if len(split_name) > 2 and split_name[2] == split:\n              buf = six.BytesIO()\n              shutil.copyfileobj(tar.extractfile(tarinfo), buf)\n              yield [tarinfo.name, buf.getvalue()]\n\n    def _process_example(inputs):\n      filename, data_string = inputs\n      buf = six.BytesIO(data_string)\n      buf.seek(0)\n      data = np.load(buf)\n      # Extract the images and convert to uint8. The reshape is required, see\n      # https://github.com/deepmind/abstract-reasoning-matrices.\n      all_images = np.uint8(data[\"image\"].reshape(16, 160, 160, 1))\n      return {\n          \"relation_structure_encoded\": data[\"relation_structure_encoded\"],\n          \"target\": data[\"target\"],\n          \"meta_target\": data[\"meta_target\"],\n          \"context\": all_images[:8],\n          \"answers\": all_images[8:],\n          \"filename\": filename,\n      }\n\n    # Beam might fuse together the _extract_data and _process_example which\n    # defeats the purpose of parallel processing. As a result, we reshard by\n    # doing a GroupByKey on random keys, and then flattening again.\n    def _add_random_keys(inputs):\n      key = str(random.randrange(10**10))\n      return key, inputs\n\n    def _remove_keys(inputs):\n      _, rows = inputs\n      for row in rows:\n        yield row\n\n    return (pipeline\n            | beam.Create([(filename, split)])\n            | beam.FlatMap(_extract_data)\n            | beam.Map(_add_random_keys)\n            | beam.GroupByKey()\n            | beam.FlatMap(_remove_keys)\n            | beam.Map(_process_example))", "response": "Builds a collection of examples."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies data from src file obj to dest file in dest_path.", "response": "def _copy(src_file, dest_path):\n  \"\"\"Copy data read from src file obj to new file in dest_path.\"\"\"\n  tf.io.gfile.makedirs(os.path.dirname(dest_path))\n  with tf.io.gfile.GFile(dest_path, 'wb') as dest_file:\n    while True:\n      data = src_file.read(io.DEFAULT_BUFFER_SIZE)\n      if not data:\n        break\n      dest_file.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\niterate over the archive and yield the filepath and extracted_fobj tuples.", "response": "def iter_tar(arch_f, gz=False, stream=False):\n  \"\"\"Iter over tar archive, yielding (path, object-like) tuples.\n\n  Args:\n    arch_f: File object of the archive to iterate.\n    gz: If True, open a gzip'ed archive.\n    stream: If True, open the archive in stream mode which allows for faster\n      processing and less temporary disk consumption, but random access to the\n      file is not allowed.\n\n  Yields:\n    (filepath, extracted_fobj) for each file in the archive.\n  \"\"\"\n  read_type = 'r' + ('|' if stream else ':')\n  if gz:\n    read_type += 'gz'\n\n  with _open_or_pass(arch_f) as fobj:\n    tar = tarfile.open(mode=read_type, fileobj=fobj)\n    for member in tar:\n      extract_file = tar.extractfile(member)\n      if extract_file:  # File with data (not directory):\n        path = _normpath(member.path)\n        if not path:\n          continue\n        yield [path, extract_file]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tqdm(self):\n    with utils.async_tqdm(\n        total=0, desc='Extraction completed...', unit=' file') as pbar_path:\n      self._pbar_path = pbar_path\n      yield", "response": "Add a progression bar for the current extraction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract the given file to the given path.", "response": "def extract(self, path, extract_method, to_path):\n    \"\"\"Returns `promise.Promise` => to_path.\"\"\"\n    self._pbar_path.update_total(1)\n    if extract_method not in _EXTRACT_METHODS:\n      raise ValueError('Unknown extraction method \"%s\".' % extract_method)\n    future = self._executor.submit(self._sync_extract,\n                                   path, extract_method, to_path)\n    return promise.Promise.resolve(future)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning to_path once resource has been extracted there.", "response": "def _sync_extract(self, from_path, method, to_path):\n    \"\"\"Returns `to_path` once resource has been extracted there.\"\"\"\n    to_path_tmp = '%s%s_%s' % (to_path, constants.INCOMPLETE_SUFFIX,\n                               uuid.uuid4().hex)\n    try:\n      for path, handle in iter_archive(from_path, method):\n        _copy(handle, path and os.path.join(to_path_tmp, path) or to_path_tmp)\n    except BaseException as err:\n      msg = 'Error while extracting %s to %s : %s' % (from_path, to_path, err)\n      raise ExtractError(msg)\n    # `tf.io.gfile.Rename(overwrite=True)` doesn't work for non empty\n    # directories, so delete destination first, if it already exists.\n    if tf.io.gfile.exists(to_path):\n      tf.io.gfile.rmtree(to_path)\n    tf.io.gfile.rename(to_path_tmp, to_path)\n    self._pbar_path.update(1)\n    return to_path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a TensorInfo object into a feature proto object.", "response": "def to_serialized_field(tensor_info):\n  \"\"\"Convert a `TensorInfo` object into a feature proto object.\"\"\"\n  # Select the type\n  dtype = tensor_info.dtype\n\n  # TODO(b/119937875): TF Examples proto only support int64, float32 and string\n  # This create limitation like float64 downsampled to float32, bool converted\n  # to int64 which is space ineficient, no support for complexes or quantized\n  if tensor_info.dtype.is_integer or tensor_info.dtype.is_bool:\n    dtype = tf.int64\n  elif tensor_info.dtype.is_floating:\n    dtype = tf.float32\n  # It seems quite space inefficient to convert bool to int64\n  # We may want to add support for complex, quantize dtype in the future\n\n  # TFRecord only support 3 types\n  if dtype not in (tf.int64, tf.float32, tf.string):\n    raise NotImplementedError(\n        'Serialization not implemented for {}'.format(dtype))\n\n  # Select the feature proto type in function of the unknown shape\n  if (tensor_info.shape is not None and  # Shape is a sequence (None, ...)\n      tensor_info.shape.count(None) == 1 and\n      tensor_info.shape[0] is None):\n    return tf.io.FixedLenSequenceFeature(\n        shape=tensor_info.shape[1:],\n        dtype=dtype,\n        allow_missing=True,\n    )\n  # At least one dimension is undefined\n  elif tensor_info.shape is None or None in tensor_info.shape:\n    return tf.io.VarLenFeature(dtype=dtype)\n  else:\n    return tf.io.FixedLenFeature(\n        shape=tensor_info.shape,\n        dtype=dtype,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_feature(value):\n  if isinstance(value, FeatureConnector):\n    return value\n  elif utils.is_dtype(value):  # tf.int32, tf.string,...\n    return Tensor(shape=(), dtype=tf.as_dtype(value))\n  elif isinstance(value, dict):\n    return FeaturesDict(value)\n  else:\n    raise ValueError('Feature not supported: {}'.format(value))", "response": "Convert the given value to Feature if necessary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode_single_feature_from_dict(\n    feature_k,\n    feature,\n    tfexample_dict):\n  \"\"\"Decode the given feature from the tfexample_dict.\n\n  Args:\n    feature_k (str): Feature key in the tfexample_dict\n    feature (FeatureConnector): Connector object to use to decode the field\n    tfexample_dict (dict): Dict containing the data to decode.\n\n  Returns:\n    decoded_feature: The output of the feature.decode_example\n  \"\"\"\n  # Singleton case\n  if not feature.serialized_keys:\n    data_to_decode = tfexample_dict[feature_k]\n  # Feature contains sub features\n  else:\n    # Extract the sub-features from the global feature dict\n    data_to_decode = {\n        k: tfexample_dict[posixpath.join(feature_k, k)]\n        for k in feature.serialized_keys\n    }\n  return feature.decode_example(data_to_decode)", "response": "Decode a single feature from the tfexample_dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures the two list of keys matches.", "response": "def _assert_keys_match(keys1, keys2):\n  \"\"\"Ensure the two list of keys matches.\"\"\"\n  if set(keys1) != set(keys2):\n    raise ValueError('{} {}'.format(list(keys1), list(keys2)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsee base class for details.", "response": "def get_tensor_info(self):\n    \"\"\"See base class for details.\"\"\"\n    return {\n        feature_key: feature.get_tensor_info()\n        for feature_key, feature in self._feature_dict.items()\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_serialized_info(self):\n    # Flatten tf-example features dict\n    # Use NonMutableDict to ensure there is no collision between features keys\n    features_dict = utils.NonMutableDict()\n    for feature_key, feature in self._feature_dict.items():\n      serialized_info = feature.get_serialized_info()\n\n      # Features can be either containers (dict of other features) or plain\n      # features (ex: single tensor). Plain features have a None\n      # feature.features_keys\n      if not feature.serialized_keys:\n        features_dict[feature_key] = serialized_info\n      else:\n        # Sanity check which should always be True, as feature.serialized_keys\n        # is computed using feature.get_serialized_info()\n        _assert_keys_match(serialized_info.keys(), feature.serialized_keys)\n        features_dict.update({\n            posixpath.join(feature_key, k): v\n            for k, v in serialized_info.items()\n        })\n\n    return features_dict", "response": "Returns a dictionary of serialized info for all features in the current dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding the example dict into a tf - example dict.", "response": "def encode_example(self, example_dict):\n    \"\"\"See base class for details.\"\"\"\n    # Flatten dict matching the tf-example features\n    # Use NonMutableDict to ensure there is no collision between features keys\n    tfexample_dict = utils.NonMutableDict()\n\n    # Iterate over example fields\n    for feature_key, (feature, example_value) in utils.zip_dict(\n        self._feature_dict, example_dict):\n      # Encode the field with the associated encoder\n      encoded_feature = feature.encode_example(example_value)\n\n      # Singleton case\n      if not feature.serialized_keys:\n        tfexample_dict[feature_key] = encoded_feature\n      # Feature contains sub features\n      else:\n        _assert_keys_match(encoded_feature.keys(), feature.serialized_keys)\n        tfexample_dict.update({\n            posixpath.join(feature_key, k): encoded_feature[k]\n            for k in feature.serialized_keys\n        })\n    return tfexample_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode a single example from a Tensor dict.", "response": "def decode_example(self, tfexample_dict):\n    \"\"\"See base class for details.\"\"\"\n    tensor_dict = {}\n    # Iterate over the Tensor dict keys\n    for feature_key, feature in six.iteritems(self._feature_dict):\n      decoded_feature = decode_single_feature_from_dict(\n          feature_k=feature_key,\n          feature=feature,\n          tfexample_dict=tfexample_dict,\n      )\n      tensor_dict[feature_key] = decoded_feature\n    return tensor_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves all the features in the tree to a directory.", "response": "def save_metadata(self, data_dir, feature_name=None):\n    \"\"\"See base class for details.\"\"\"\n    # Recursively save all child features\n    for feature_key, feature in six.iteritems(self._feature_dict):\n      if feature_name:\n        feature_key = '-'.join((feature_name, feature_key))\n      feature.save_metadata(data_dir, feature_name=feature_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencodes the example data into a sequence of examples.", "response": "def encode_example(self, example_data):\n    \"\"\"See base class for details.\"\"\"\n    np_dtype = np.dtype(self._dtype.as_numpy_dtype)\n    # Convert to numpy if possible\n    if not isinstance(example_data, np.ndarray):\n      example_data = np.array(example_data, dtype=np_dtype)\n    # Ensure the shape and dtype match\n    if example_data.dtype != np_dtype:\n      raise ValueError('Dtype {} do not match {}'.format(\n          example_data.dtype, np_dtype))\n    utils.assert_shape_match(example_data.shape, self._shape)\n    # For booleans, convert to integer (tf.train.Example does not support bool)\n    if example_data.dtype == np.bool_:\n      example_data = example_data.astype(int)\n    return example_data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_example(self, tfexample_data):\n    # TODO(epot): Support dynamic shape\n    if self.shape.count(None) < 2:\n      # Restore the shape if possible. TF Example flattened it.\n      shape = [-1 if i is None else i for i in self.shape]\n      tfexample_data = tf.reshape(tfexample_data, shape)\n    if tfexample_data.dtype != self.dtype:\n      tfexample_data = tf.dtypes.cast(tfexample_data, self.dtype)\n    return tfexample_data", "response": "Decode a TF Example into a tuple of shape and dtype."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _process_celeba_config_file(self, file_path):\n    with tf.io.gfile.GFile(file_path) as f:\n      data_raw = f.read()\n    lines = data_raw.split(\"\\n\")\n\n    keys = lines[1].strip().split()\n    values = {}\n    # Go over each line (skip the last one, as it is empty).\n    for line in lines[2:-1]:\n      row_values = line.strip().split()\n      # Each row start with the 'file_name' and then space-separated values.\n      values[row_values[0]] = [int(v) for v in row_values[1:]]\n    return keys, values", "response": "Unpack the celeba config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_examples(self, file_paths):\n    for label, path in sorted(file_paths.items(), key=lambda x: x[0]):\n      with tf.io.gfile.GFile(path, \"rb\") as f:\n        class_images = np.load(f)\n        for np_image in class_images:\n          yield {\n              \"image\": np_image.reshape(_QUICKDRAW_IMAGE_SHAPE),\n              \"label\": label,\n          }", "response": "Generate QuickDraw bitmap examples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattempting to import tensorflow and ensure its version is sufficient.", "response": "def ensure_tf_install():  # pylint: disable=g-statement-before-imports\n  \"\"\"Attempt to import tensorflow, and ensure its version is sufficient.\n\n  Raises:\n    ImportError: if either tensorflow is not importable or its version is\n    inadequate.\n  \"\"\"\n  try:\n    import tensorflow as tf\n  except ImportError:\n    # Print more informative error message, then reraise.\n    print(\"\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not \"\n          \"installed by default when you install TensorFlow Datasets. This is \"\n          \"so that users can decide whether to install the GPU-enabled \"\n          \"TensorFlow package. To use TensorFlow Datasets, please install the \"\n          \"most recent version of TensorFlow, by following instructions at \"\n          \"https://tensorflow.org/install.\\n\\n\")\n    raise\n\n  tf_version = distutils.version.LooseVersion(tf.__version__)\n  v_1_12 = distutils.version.LooseVersion(\"1.12.0\")\n  if tf_version < v_1_12:\n    raise ImportError(\n        \"This version of TensorFlow Datasets requires TensorFlow \"\n        \"version >= {required}; Detected an installation of version {present}. \"\n        \"Please upgrade TensorFlow to proceed.\".format(\n            required=\"1.12.0\",\n            present=tf.__version__))\n  _patch_tf(tf)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npatching TF to maintain compatibility across versions.", "response": "def _patch_tf(tf):\n  \"\"\"Patch TF to maintain compatibility across versions.\"\"\"\n  global TF_PATCH\n  if TF_PATCH:\n    return\n\n  v_1_12 = distutils.version.LooseVersion(\"1.12.0\")\n  v_1_13 = distutils.version.LooseVersion(\"1.13.0\")\n  v_2 = distutils.version.LooseVersion(\"2.0.0\")\n  tf_version = distutils.version.LooseVersion(tf.__version__)\n  if v_1_12 <= tf_version < v_1_13:\n    # TODO(b/123930850): remove when 1.13 is stable.\n    TF_PATCH = \"tf1_12\"\n    _patch_for_tf1_12(tf)\n  elif v_1_13 <= tf_version < v_2:\n    TF_PATCH = \"tf1_13\"\n    _patch_for_tf1_13(tf)\n  else:\n    TF_PATCH = \"tf2\"\n    _patch_for_tf2(tf)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _patch_for_tf1_12(tf):\n  tf.io.gfile = tf.gfile\n  tf.io.gfile.copy = tf.gfile.Copy\n  tf.io.gfile.exists = tf.gfile.Exists\n  tf.io.gfile.glob = tf.gfile.Glob\n  tf.io.gfile.isdir = tf.gfile.IsDirectory\n  tf.io.gfile.listdir = tf.gfile.ListDirectory\n  tf.io.gfile.makedirs = tf.gfile.MakeDirs\n  tf.io.gfile.mkdir = tf.gfile.MkDir\n  tf.io.gfile.remove = tf.gfile.Remove\n  tf.io.gfile.rename = tf.gfile.Rename\n  tf.io.gfile.rmtree = tf.gfile.DeleteRecursively\n  tf.io.gfile.stat = tf.gfile.Stat\n  tf.io.gfile.walk = tf.gfile.Walk\n  tf.io.gfile.GFile = tf.gfile.GFile\n  tf.data.experimental = tf.contrib.data\n  tf.compat.v1 = types.ModuleType(\"tf.compat.v1\")\n  tf.compat.v1.assert_greater = tf.assert_greater\n  tf.compat.v1.placeholder = tf.placeholder\n  tf.compat.v1.ConfigProto = tf.ConfigProto\n  tf.compat.v1.Session = tf.Session\n  tf.compat.v1.enable_eager_execution = tf.enable_eager_execution\n  tf.compat.v1.io = tf.io\n  tf.compat.v1.data = tf.data\n  tf.compat.v1.data.Dataset = tf.data.Dataset\n  tf.compat.v1.data.make_one_shot_iterator = (\n      lambda ds: ds.make_one_shot_iterator())\n  tf.compat.v1.train = tf.train\n  tf.compat.v1.global_variables_initializer = tf.global_variables_initializer\n  tf.compat.v1.test = tf.test\n  tf.compat.v1.test.get_temp_dir = tf.test.get_temp_dir\n  tf.nest = tf.contrib.framework.nest", "response": "Monkey patch tf 1. 12 so tfds can use it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _patch_for_tf1_13(tf):\n  if not hasattr(tf.io.gfile, \"GFile\"):\n    tf.io.gfile.GFile = tf.gfile.GFile\n  if not hasattr(tf, \"nest\"):\n    tf.nest = tf.contrib.framework.nest\n  if not hasattr(tf.compat, \"v2\"):\n    tf.compat.v2 = types.ModuleType(\"tf.compat.v2\")\n    tf.compat.v2.data = types.ModuleType(\"tf.compat.v2.data\")\n    from tensorflow.python.data.ops import dataset_ops\n    tf.compat.v2.data.Dataset = dataset_ops.DatasetV2\n  if not hasattr(tf.compat.v2.data.Dataset, \"output_shapes\"):\n    from tensorflow.python.data.ops import dataset_ops\n    if hasattr(dataset_ops, \"get_legacy_output_shapes\"):\n      tf.compat.v2.data.Dataset.output_shapes = property(\n          dataset_ops.get_legacy_output_shapes)\n      tf.compat.v2.data.Dataset.output_types = property(\n          dataset_ops.get_legacy_output_types)", "response": "Monkey patch tf 1. 13 so tfds can use it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_dataset(ds):\n  import tensorflow as tf\n  from tensorflow_datasets.core.utils import py_utils\n  dataset_types = [tf.data.Dataset]\n  v1_ds = py_utils.rgetattr(tf, \"compat.v1.data.Dataset\", None)\n  v2_ds = py_utils.rgetattr(tf, \"compat.v2.data.Dataset\", None)\n  if v1_ds is not None:\n    dataset_types.append(v1_ds)\n  if v2_ds is not None:\n    dataset_types.append(v2_ds)\n  return isinstance(ds, tuple(dataset_types))", "response": "Whether ds is a Dataset. Compatible across TF versions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_examples(self, filepath):\n    for idx, line in enumerate(tf.io.gfile.GFile(filepath, \"rb\")):\n      if idx == 0:\n        continue  # skip header\n      line = tf.compat.as_text(line.strip())\n      split_line = line.split(\"\\t\")\n      # Examples not marked with a three out of five consensus are marked with\n      # \"-\" and should not be used in standard evaluations.\n      if split_line[0] == \"-\":\n        continue\n      # Works for both splits even though dev has some extra human labels.\n      yield {\n          \"premise\": split_line[5],\n          \"hypothesis\": split_line[6],\n          \"label\": split_line[0]\n      }", "response": "Generate mnli examples.\n\n    Args:\n      filepath: a string\n\n    Yields:\n      dictionaries containing \"premise\", \"hypothesis\" and \"label\" strings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning SplitGenerators from the folder names.", "response": "def _split_generators(self, dl_manager):\n    \"\"\"Returns SplitGenerators from the folder names.\"\"\"\n    # At data creation time, parse the folder to deduce number of splits,\n    # labels, image size,\n\n    # The splits correspond to the high level folders\n    split_names = list_folders(dl_manager.manual_dir)\n\n    # Extract all label names and associated images\n    split_label_images = {}  # dict[split_name][label_name] = list(img_paths)\n    for split_name in split_names:\n      split_dir = os.path.join(dl_manager.manual_dir, split_name)\n      split_label_images[split_name] = {\n          label_name: list_imgs(os.path.join(split_dir, label_name))\n          for label_name in list_folders(split_dir)\n      }\n\n    # Merge all label names from all splits to get the final list of labels\n    # Sorted list for determinism\n    labels = [split.keys() for split in split_label_images.values()]\n    labels = list(sorted(set(itertools.chain(*labels))))\n\n    # Could improve the automated encoding format detection\n    # Extract the list of all image paths\n    image_paths = [\n        image_paths\n        for label_images in split_label_images.values()\n        for image_paths in label_images.values()\n    ]\n    if any(f.lower().endswith(\".png\") for f in itertools.chain(*image_paths)):\n      encoding_format = \"png\"\n    else:\n      encoding_format = \"jpeg\"\n\n    # Update the info.features. Those info will be automatically resored when\n    # the dataset is re-created\n    self.info.features[\"image\"].set_encoding_format(encoding_format)\n    self.info.features[\"label\"].names = labels\n\n    def num_examples(label_images):\n      return sum(len(imgs) for imgs in label_images.values())\n\n    # Define the splits\n    return [\n        tfds.core.SplitGenerator(\n            name=split_name,\n            # The number of shards is a dynamic function of the total\n            # number of images (between 0-10)\n            num_shards=min(10, max(num_examples(label_images) // 1000, 1)),\n            gen_kwargs=dict(label_images=label_images,),\n        ) for split_name, label_images in split_label_images.items()\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate examples for each image in the dict.", "response": "def _generate_examples(self, label_images):\n    \"\"\"Generate example for each image in the dict.\"\"\"\n\n    for label, image_paths in label_images.items():\n      for image_path in image_paths:\n        yield {\n            \"image\": image_path,\n            \"label\": label,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new dataset from a template.", "response": "def create_dataset_file(root_dir, data):\n  \"\"\"Create a new dataset from a template.\"\"\"\n  file_path = os.path.join(root_dir, '{dataset_type}', '{dataset_name}.py')\n  context = (\n      _HEADER + _DATASET_DEFAULT_IMPORTS + _CITATION\n      + _DESCRIPTION + _DATASET_DEFAULTS\n  )\n\n  with gfile.GFile(file_path.format(**data), 'w') as f:\n    f.write(context.format(**data))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_the_init(root_dir, data):\n  init_file = os.path.join(root_dir, '{dataset_type}', '__init__.py')\n  context = (\n      'from tensorflow_datasets.{dataset_type}.{dataset_name} import '\n      '{dataset_cls}  # {TODO} Sort alphabetically\\n'\n  )\n  with gfile.GFile(init_file.format(**data), 'a') as f:\n    f.write(context.format(**data))", "response": "Append the new dataset file to the __init__. py file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating examples as dicts.", "response": "def _generate_examples(self, filepath):\n    \"\"\"Generate examples as dicts.\n\n    Args:\n      filepath: `str` path of the file to process.\n\n    Yields:\n      Generator yielding the next samples\n    \"\"\"\n    with tf.io.gfile.GFile(filepath, \"rb\") as f:\n      data = tfds.core.lazy_imports.scipy.io.loadmat(f)\n\n    # Maybe should shuffle ?\n\n    assert np.max(data[\"y\"]) <= 10  # Sanity check\n    assert np.min(data[\"y\"]) > 0\n\n    for image, label in zip(np.rollaxis(data[\"X\"], -1), data[\"y\"]):\n      yield {\n          \"image\": image,\n          \"label\": label % 10,  # digit 0 is saved as 0 (instead of 10)\n      }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of SplitGenerators.", "response": "def _split_generators(self, dl_manager):\n    \"\"\"Returns SplitGenerators.\"\"\"\n    path = dl_manager.manual_dir\n    train_path = os.path.join(path, _TRAIN_DIR)\n    val_path = os.path.join(path, _VALIDATION_DIR)\n\n    if not tf.io.gfile.exists(train_path) or not tf.io.gfile.exists(val_path):\n      msg = (\"You must download the dataset folder from CheXpert\"\n             \"website manually and place it into %s.\" % path)\n      raise AssertionError(msg)\n\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=100,\n            gen_kwargs={\n                \"imgs_path\": path,  # Relative img path is provided in csv\n                \"csv_path\": os.path.join(path, _TRAIN_LABELS_FNAME)\n            },\n        ),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.VALIDATION,\n            num_shards=10,\n            gen_kwargs={\n                \"imgs_path\": path,\n                \"csv_path\": os.path.join(path, _VALIDATION_LABELS_FNAME)\n            },\n        ),\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing a list of Imagenet2012CorruptedConfig objects corresponding to .", "response": "def _make_builder_configs():\n  \"\"\"Construct a list of BuilderConfigs.\n\n  Construct a list of 60 Imagenet2012CorruptedConfig objects, corresponding to\n  the 12 corruption types, with each type having 5 severities.\n\n  Returns:\n    A list of 60 Imagenet2012CorruptedConfig objects.\n  \"\"\"\n  config_list = []\n  for each_corruption in TYPE_LIST:\n    for each_severity in range(1, 6):\n      name_str = each_corruption + '_' + str(each_severity)\n      version_str = '0.0.1'\n      description_str = 'corruption type = ' + each_corruption + ', severity = '\n      description_str += str(each_severity)\n      config_list.append(\n          Imagenet2012CorruptedConfig(\n              name=name_str,\n              version=version_str,\n              description=description_str,\n              corruption_type=each_corruption,\n              severity=each_severity,\n          ))\n  return config_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the validation split of ImageNet2012.", "response": "def _split_generators(self, dl_manager):\n    \"\"\"Return the validation split of ImageNet2012.\n\n    Args:\n      dl_manager: download manager object.\n\n    Returns:\n      validation split.\n    \"\"\"\n    splits = super(Imagenet2012Corrupted, self)._split_generators(dl_manager)\n    validation = splits[1]\n    return [validation]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates corrupted imagenet validation data.", "response": "def _generate_examples_validation(self, archive, labels):\n    \"\"\"Generate corrupted imagenet validation data.\n\n    Apply corruptions to the raw images according to self.corruption_type.\n\n    Args:\n      archive: an iterator for the raw dataset.\n      labels: a dictionary that maps the file names to imagenet labels.\n\n    Yields:\n      dictionary with the file name, an image file objective, and label of each\n      imagenet validation data.\n    \"\"\"\n    # Get the current random seeds.\n    numpy_st0 = np.random.get_state()\n    # Set new random seeds.\n    np.random.seed(135)\n    logging.warning('Overwriting cv2 RNG seed.')\n    tfds.core.lazy_imports.cv2.setRNGSeed(357)\n\n    for example in super(Imagenet2012Corrupted,\n                         self)._generate_examples_validation(archive, labels):\n      with tf.Graph().as_default():\n        tf_img = tf.image.decode_jpeg(example['image'].read(), channels=3)\n        image_np = tfds.as_numpy(tf_img)\n      example['image'] = self._get_corrupted_example(image_np)\n      yield example\n    # Reset the seeds back to their original values.\n    np.random.set_state(numpy_st0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_corrupted_example(self, x):\n    corruption_type = self.builder_config.corruption_type\n    severity = self.builder_config.severity\n\n    return {\n        'gaussian_noise': corruptions.gaussian_noise,\n        'shot_noise': corruptions.shot_noise,\n        'impulse_noise': corruptions.impulse_noise,\n        'defocus_blur': corruptions.defocus_blur,\n        'frosted_glass_blur': corruptions.frosted_glass_blur,\n        'zoom_blur': corruptions.zoom_blur,\n        'fog': corruptions.fog,\n        'brightness': corruptions.brightness,\n        'contrast': corruptions.contrast,\n        'elastic': corruptions.elastic,\n        'pixelate': corruptions.pixelate,\n        'jpeg_compression': corruptions.jpeg_compression,\n    }[corruption_type](x, severity)", "response": "Return corrupted images.\n\n    Args:\n      x: numpy array, uncorrupted image.\n\n    Returns:\n      numpy array, corrupted images."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nensuring that the shape1 and shape2 match the pattern given by shape2.", "response": "def assert_shape_match(shape1, shape2):\n  \"\"\"Ensure the shape1 match the pattern given by shape2.\n\n  Ex:\n    assert_shape_match((64, 64, 3), (None, None, 3))\n\n  Args:\n    shape1 (tuple): Static shape\n    shape2 (tuple): Dynamic shape (can contain None)\n  \"\"\"\n  shape1 = tf.TensorShape(shape1)\n  shape2 = tf.TensorShape(shape2)\n  if shape1.ndims is None or shape2.ndims is None:\n    raise ValueError('Shapes must have known rank. Got %s and %s.' %\n                     (shape1.ndims, shape2.ndims))\n  shape1.assert_same_rank(shape2)\n  shape1.assert_is_compatible_with(shape2)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef raw_nogpu_session(graph=None):\n  config = tf.compat.v1.ConfigProto(device_count={'GPU': 0})\n  return tf.compat.v1.Session(config=config, graph=graph)", "response": "Returns a tf. Session that is not hiding GPUs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef maybe_with_graph(graph=None, create_if_none=True):\n  if tf.executing_eagerly():\n    yield None\n  else:\n    if graph is None and create_if_none:\n      graph = tf.Graph()\n\n    if graph is None:\n      yield None\n    else:\n      with graph.as_default():\n        yield graph", "response": "Eager - compatible Graph. as_default() yielding the graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting the given TensorFlow function.", "response": "def run(self, fct, input_):\n    \"\"\"Execute the given TensorFlow function.\"\"\"\n    # TF 2.0\n    if tf.executing_eagerly():\n      return fct(input_).numpy()\n    # TF 1.0\n    else:\n      # Should compile the function if this is the first time encountered\n      if not isinstance(input_, np.ndarray):\n        input_ = np.array(input_)\n      run_args = RunArgs(fct=fct, input=input_)\n      signature = self._build_signature(run_args)\n      if signature not in self._graph_run_cache:\n        graph_run = self._build_graph_run(run_args)\n        self._graph_run_cache[signature] = graph_run\n      else:\n        graph_run = self._graph_run_cache[signature]\n\n      # Then execute the cached graph\n      return graph_run.session.run(\n          graph_run.output,\n          feed_dict={graph_run.placeholder: input_},\n      )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_graph_run(self, run_args):\n    # Could try to use tfe.py_func(fct) but this would require knowing\n    # information about the signature of the function.\n\n    # Create a new graph:\n    with tf.Graph().as_default() as g:\n      # Create placeholder\n      input_ = run_args.input\n      placeholder = tf.compat.v1.placeholder(\n          dtype=input_.dtype, shape=input_.shape)\n      output = run_args.fct(placeholder)\n    return GraphRun(\n        session=raw_nogpu_session(g),\n        graph=g,\n        placeholder=placeholder,\n        output=output,\n    )", "response": "Create a new graph for the given args."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a unique signature for each fct and inputs.", "response": "def _build_signature(self, run_args):\n    \"\"\"Create a unique signature for each fct/inputs.\"\"\"\n    return (id(run_args.fct), run_args.input.dtype, run_args.input.shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the given image into a dict convertible to tf example.", "response": "def encode_example(self, video_or_path_or_fobj):\n    \"\"\"Converts the given image into a dict convertible to tf example.\"\"\"\n    if isinstance(video_or_path_or_fobj, six.string_types):\n      if not os.path.isfile(video_or_path_or_fobj):\n        _, video_temp_path = tempfile.mkstemp()\n        try:\n          tf.gfile.Copy(video_or_path_or_fobj, video_temp_path, overwrite=True)\n          encoded_video = self._ffmpeg_decode(video_temp_path)\n        finally:\n          os.unlink(video_temp_path)\n      else:\n        encoded_video = self._ffmpeg_decode(video_or_path_or_fobj)\n    elif hasattr(video_or_path_or_fobj, 'read'):\n      encoded_video = self._ffmpeg_decode(video_or_path_or_fobj)\n    else:\n      encoded_video = video_or_path_or_fobj\n    return super(Video, self).encode_example(encoded_video)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating rock paper or scissors images and labels given the directory path.", "response": "def _generate_examples(self, archive):\n    \"\"\"Generate rock, paper or scissors images and labels given the directory path.\n\n    Args:\n      archive: object that iterates over the zip.\n\n    Yields:\n      The image path and its corresponding label.\n    \"\"\"\n\n    for fname, fobj in archive:\n      res = _NAME_RE.match(fname)\n      if not res:  # if anything other than .png; skip\n        continue\n      label = res.group(2).lower()\n      yield {\n          \"image\": fobj,\n          \"label\": label,\n      }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_examples(self, file_path):\n\n    with tf.io.gfile.GFile(file_path) as f:\n      raw_data = csv.DictReader(f)\n      for row in raw_data:\n        survive_val = row.pop(\"survived\")\n        yield {\n            \"survived\": convert_to_label(survive_val, _SURVIVED_DICT),\n            \"features\": {\n                name: FEATURE_DICT[name][1](value)\n                for name, value in row.items()\n            }\n        }", "response": "Generate the features and target from the CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstrip ID 0 and decrement ids by 1.", "response": "def pad_decr(ids):\n  \"\"\"Strip ID 0 and decrement ids by 1.\"\"\"\n  if len(ids) < 1:\n    return list(ids)\n  if not any(ids):\n    return []  # all padding.\n  idx = -1\n  while not ids[idx]:\n    idx -= 1\n  if idx == -1:\n    ids = ids\n  else:\n    ids = ids[:idx + 1]\n  return [i - 1 for i in ids]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing reserved tokens and a regex for splitting them out of strings.", "response": "def _prepare_reserved_tokens(reserved_tokens):\n  \"\"\"Prepare reserved tokens and a regex for splitting them out of strings.\"\"\"\n  reserved_tokens = [tf.compat.as_text(tok) for tok in reserved_tokens or []]\n  dups = _find_duplicates(reserved_tokens)\n  if dups:\n    raise ValueError(\"Duplicates found in tokens: %s\" % dups)\n  reserved_tokens_re = _make_reserved_tokens_re(reserved_tokens)\n  return reserved_tokens, reserved_tokens_re"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing compiled regex to parse out reserved tokens.", "response": "def _make_reserved_tokens_re(reserved_tokens):\n  \"\"\"Constructs compiled regex to parse out reserved tokens.\"\"\"\n  if not reserved_tokens:\n    return None\n  escaped_tokens = [_re_escape(rt) for rt in reserved_tokens]\n  pattern = \"(%s)\" % \"|\".join(escaped_tokens)\n  reserved_tokens_re = _re_compile(pattern)\n  return reserved_tokens_re"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_lines_to_file(cls_name, filename, lines, metadata_dict):\n  metadata_dict = metadata_dict or {}\n  header_line = \"%s%s\" % (_HEADER_PREFIX, cls_name)\n  metadata_line = \"%s%s\" % (_METADATA_PREFIX,\n                            json.dumps(metadata_dict, sort_keys=True))\n  with tf.io.gfile.GFile(filename, \"wb\") as f:\n    for line in [header_line, metadata_line]:\n      f.write(tf.compat.as_bytes(line))\n      f.write(tf.compat.as_bytes(\"\\n\"))\n    if lines:\n      f.write(tf.compat.as_bytes(\"\\n\".join(lines)))\n      f.write(tf.compat.as_bytes(\"\\n\"))", "response": "Writes lines to file prepended by header and metadata."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads lines from file parsing out header and metadata.", "response": "def read_lines_from_file(cls_name, filename):\n  \"\"\"Read lines from file, parsing out header and metadata.\"\"\"\n  with tf.io.gfile.GFile(filename, \"rb\") as f:\n    lines = [tf.compat.as_text(line)[:-1] for line in f]\n  header_line = \"%s%s\" % (_HEADER_PREFIX, cls_name)\n  if lines[0] != header_line:\n    raise ValueError(\"File {fname} does not seem to have been created from \"\n                     \"{name}.save_to_file.\".format(\n                         fname=filename, name=cls_name))\n  metadata_dict = json.loads(lines[1][len(_METADATA_PREFIX):])\n  return lines[2:], metadata_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tokenize(self, s):\n    s = tf.compat.as_text(s)\n\n    if self.reserved_tokens:\n      # First split out the reserved tokens\n      substrs = self._reserved_tokens_re.split(s)\n    else:\n      substrs = [s]\n\n    toks = []\n    for substr in substrs:\n      if substr in self.reserved_tokens:\n        toks.append(substr)\n      else:\n        toks.extend(self._alphanum_re.split(substr))\n\n    # Filter out empty strings\n    toks = [t for t in toks if t]\n    return toks", "response": "Splits a string into tokens."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef slice_to_percent_mask(slice_value):\n  if slice_value is None:\n    slice_value = slice(None)\n  # Select only the elements of the slice\n  selected = set(list(range(100))[slice_value])\n  # Create the binary mask\n  return [i in selected for i in range(100)]", "response": "Convert a python slice [ 15 : 50 ] into a list [ bool ] mask of 100 elements."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the mapping shard_id = > num_examples assuming round - robin.", "response": "def get_shard_id2num_examples(num_shards, total_num_examples):\n  \"\"\"Return the mapping shard_id=>num_examples, assuming round-robin.\"\"\"\n  # TODO(b/130353071): This has the strong assumption that the shards have\n  # been written in a round-robin fashion. This assumption does not hold, for\n  # instance, with Beam generation. The mapping shard_id=>num_examples\n  # should be computed during generation.\n\n  # Minimum number of example per shards\n  num_example_in_shard = total_num_examples // num_shards\n  shard_id2num_examples = [num_example_in_shard for _ in range(num_shards)]\n  # If there are remaining examples, we add them to the first shards\n  for shard_id in range(total_num_examples % num_shards):\n    shard_id2num_examples[shard_id] += 1\n  return shard_id2num_examples"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the list of offsets associated with each shard.", "response": "def compute_mask_offsets(shard_id2num_examples):\n  \"\"\"Return the list of offsets associated with each shards.\n\n  Args:\n    shard_id2num_examples: `list[int]`, mapping shard_id=>num_examples\n\n  Returns:\n    mask_offsets: `list[int]`, offset to skip for each of the shard\n  \"\"\"\n  total_num_examples = sum(shard_id2num_examples)\n\n  mask_offsets = []\n  total_num_examples = 0\n  for num_examples_in_shard in shard_id2num_examples:\n    # The offset (nb of examples to skip in the next shard) correspond to the\n    # number of examples remaining in the current shard\n    mask_offsets.append(total_num_examples % 100)\n    total_num_examples += num_examples_in_shard\n\n  return mask_offsets"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that the two split dicts have the same names and num_shards.", "response": "def check_splits_equals(splits1, splits2):\n  \"\"\"Check that the two split dicts have the same names and num_shards.\"\"\"\n  if set(splits1) ^ set(splits2):  # Name intersection should be null\n    return False\n  for _, (split1, split2) in utils.zip_dict(splits1, splits2):\n    if split1.num_shards != split2.num_shards:\n      return False\n  return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the split info.", "response": "def add(self, split_info):\n    \"\"\"Add the split info.\"\"\"\n    if split_info.name in self:\n      raise ValueError(\"Split {} already present\".format(split_info.name))\n    # TODO(epot): Make sure this works with Named splits correctly.\n    super(SplitDict, self).__setitem__(split_info.name, split_info)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_proto(cls, repeated_split_infos):\n    split_dict = cls()\n    for split_info_proto in repeated_split_infos:\n      split_info = SplitInfo()\n      split_info.CopyFrom(split_info_proto)\n      split_dict.add(split_info)\n    return split_dict", "response": "Returns a new SplitDict initialized from the repeated_split_infos."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_proto(self):\n    # Return the proto.SplitInfo, sorted by name\n    return sorted((s.get_proto() for s in self.values()), key=lambda s: s.name)", "response": "Returns a list of SplitInfo protos that we have."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_examples(self, data_file):\n    target_language = self.builder_config.target_language\n\n    with tf.io.gfile.GFile(data_file) as f:\n      for i, line in enumerate(f):\n        line_parts = line.strip().split(\"\\t\")\n        if len(line_parts) != 2:\n          raise ValueError((\"Wrong data format in line {}. The line '{}' does \"\n                            \"not have exactly one delimiter.\").format(i, line))\n        source, target = line_parts[0].strip(), line_parts[1].strip()\n\n        yield {\"en\": source, target_language: target}", "response": "This function returns the examples in the raw text form."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_synchronize_decorator():\n  lock = threading.Lock()\n\n  def lock_decorator(fn):\n\n    @functools.wraps(fn)\n    def lock_decorated(*args, **kwargs):\n      with lock:\n        return fn(*args, **kwargs)\n\n    return lock_decorated\n\n  return lock_decorator", "response": "Returns a decorator which prevents concurrent calls to functions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_file_name(url):\n  return os.path.basename(urllib.parse.urlparse(url).path) or 'unknown_name'", "response": "Returns the file name of the given url."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_builder_configs():\n  text_encoder_configs = [\n      None,\n      tfds.features.text.TextEncoderConfig(\n          name=\"bytes\", encoder=tfds.features.text.ByteTextEncoder()),\n      tfds.features.text.TextEncoderConfig(\n          name=\"subwords8k\",\n          encoder_cls=tfds.features.text.SubwordTextEncoder,\n          vocab_size=2**13),\n      tfds.features.text.TextEncoderConfig(\n          name=\"subwords32k\",\n          encoder_cls=tfds.features.text.SubwordTextEncoder,\n          vocab_size=2**15),\n  ]\n  version = \"0.1.0\"\n  configs = []\n  for text_encoder_config in text_encoder_configs:\n    for data in _DATA_OPTIONS:\n      config = LibrispeechConfig(\n          version=version, text_encoder_config=text_encoder_config, data=data)\n      configs.append(config)\n  return configs", "response": "Makes a list of Librispeech BuilderConfigs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwalk a Librispeech directory and yield LibrispeechExample objects.", "response": "def _walk_librispeech_dir(directory):\n  \"\"\"Walk a Librispeech directory and yield examples.\"\"\"\n  directory = os.path.join(directory, \"LibriSpeech\")\n  for path, _, files in tf.io.gfile.walk(directory):\n    if not files:\n      continue\n\n    transcript_file = [f for f in files if f.endswith(\".txt\")]\n    if not transcript_file:\n      continue\n    assert len(transcript_file) == 1\n    transcript_file, = transcript_file\n    transcripts = {}\n    with tf.io.gfile.GFile(os.path.join(path, transcript_file)) as f:\n      for line in f:\n        line = line.strip()\n        key, transcript = line.split(\" \", 1)\n        transcripts[key] = transcript\n    audio_files = [f for f in files if not f.endswith(\".txt\")]\n    for audio_file in audio_files:\n      assert audio_file.endswith(\".flac\")\n      key = audio_file[:-len(\".flac\")]\n      transcript = transcripts[key]\n      speaker_id, chapter_id = [int(el) for el in key.split(\"-\")[:2]]\n      yield LibrispeechExample(\n          speaker_id=speaker_id,\n          chapter_id=chapter_id,\n          audio_file=os.path.join(path, audio_file),\n          transcript=transcript)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns download urls for this config.", "response": "def download_urls(self):\n    \"\"\"Returns download urls for this config.\"\"\"\n    urls = {\n        tfds.Split.TRAIN: [\"train_clean100\"],\n        tfds.Split.VALIDATION: [\"dev_clean\"],\n        tfds.Split.TEST: [\"test_clean\"],\n    }\n    if self.data in [\"all\", \"clean360\"]:\n      urls[tfds.Split.TRAIN].append(\"train_clean360\")\n    if self.data == \"all\":\n      urls[tfds.Split.TRAIN].extend([\"train_clean360\", \"train_other500\"])\n      urls[tfds.Split.VALIDATION].append(\"dev_other\")\n      urls[tfds.Split.TEST].append(\"test_other\")\n\n    urls = {\n        split: [_DL_URLS[name] for name in names\n               ] for split, names in urls.items()\n    }\n    return urls"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef int2str(self, int_value):\n    if self._int2str:\n      # Maybe should support batched np array/eager tensors, to allow things\n      # like\n      # out_ids = model(inputs)\n      # labels = cifar10.info.features['label'].int2str(out_ids)\n      return self._int2str[int_value]\n\n    # No names provided, return str(int)\n    if not 0 <= int_value < self._num_classes:\n      raise ValueError(\"Invalid integer class label %d\" % int_value)\n    return tf.compat.as_text(str(int_value))", "response": "Convert an integer to a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave the metadata of the current node to the file.", "response": "def save_metadata(self, data_dir, feature_name=None):\n    \"\"\"See base class for details.\"\"\"\n    # Save names if defined\n    if self._str2int is not None:\n      names_filepath = _get_names_filepath(data_dir, feature_name)\n      _write_names_to_file(names_filepath, self.names)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload metadata from file.", "response": "def load_metadata(self, data_dir, feature_name=None):\n    \"\"\"See base class for details.\"\"\"\n    # Restore names if defined\n    names_filepath = _get_names_filepath(data_dir, feature_name)\n    if tf.io.gfile.exists(names_filepath):\n      self.names = _load_names_from_file(names_filepath)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild token counts from generator.", "response": "def _token_counts_from_generator(generator, max_chars, reserved_tokens):\n  \"\"\"Builds token counts from generator.\"\"\"\n  reserved_tokens = list(reserved_tokens) + [_UNDERSCORE_REPLACEMENT]\n  tokenizer = text_encoder.Tokenizer(\n      alphanum_only=False, reserved_tokens=reserved_tokens)\n  num_chars = 0\n  token_counts = collections.defaultdict(int)\n  for s in generator:\n    s = tf.compat.as_text(s)\n    if max_chars and (num_chars + len(s)) >= max_chars:\n      s = s[:(max_chars - num_chars)]\n    tokens = tokenizer.tokenize(s)\n    tokens = _prepare_tokens_for_encode(tokens)\n    for t in tokens:\n      token_counts[t] += 1\n    if max_chars:\n      num_chars += len(s)\n      if num_chars > max_chars:\n        break\n  return token_counts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate arguments for SubwordTextEncoder. build_from_corpus.", "response": "def _validate_build_arguments(max_subword_length, reserved_tokens,\n                              target_vocab_size):\n  \"\"\"Validate arguments for SubwordTextEncoder.build_from_corpus.\"\"\"\n  if max_subword_length <= 0:\n    raise ValueError(\n        \"max_subword_length must be > 0. Note that memory and compute for \"\n        \"building the vocabulary scale quadratically in the length of the \"\n        \"longest token.\")\n  for t in reserved_tokens:\n    if t.endswith(\"_\") or not text_encoder.is_mixed_alphanum(t):\n      raise ValueError(\n          \"Reserved tokens must not end with _ and they must contain a mix \"\n          \"of alphanumeric and non-alphanumeric characters. For example, \"\n          \"'<EOS>'.\")\n  # Minimum vocab size = bytes + pad + 1\n  minimum_vocab_size = text_encoder.NUM_BYTES + 1 + 1\n  if target_vocab_size < minimum_vocab_size:\n    raise ValueError(\"target_vocab_size must be >= %d. Got %d\" %\n                     (minimum_vocab_size, target_vocab_size))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prepare_tokens_for_encode(tokens):\n  prepared_tokens = []\n\n  def _prepare_token(t, next_t):\n    skip_next = False\n    t = _escape(t)\n    # If next token is a single space, add _ suffix to token and skip the\n    # empty space.\n    if next_t == \" \":\n      t += \"_\"\n      skip_next = True\n    return t, skip_next\n\n  next_tokens = tokens[1:] + [None]\n  skip_single_token = False\n  for token, next_token in zip(tokens, next_tokens):\n    if skip_single_token:\n      skip_single_token = False\n      continue\n\n    # If the user-supplied string contains the underscore replacement string,\n    # break it into 2 tokens and encode those separately.\n    if token == _UNDERSCORE_REPLACEMENT:\n      t1, t2 = _UNDERSCORE_REPLACEMENT[:2], _UNDERSCORE_REPLACEMENT[2:]\n      t1, _ = _prepare_token(t1, None)\n      t2, _ = _prepare_token(t2, next_token)\n      prepared_tokens.append(t1)\n      prepared_tokens.append(t2)\n      continue\n\n    token, skip_single_token = _prepare_token(token, next_token)\n    prepared_tokens.append(token)\n  return prepared_tokens", "response": "Prepare tokens for encoding."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode text into a list of integers.", "response": "def encode(self, s):\n    \"\"\"Encodes text into a list of integers.\"\"\"\n    s = tf.compat.as_text(s)\n    tokens = self._tokenizer.tokenize(s)\n    tokens = _prepare_tokens_for_encode(tokens)\n    ids = []\n    for token in tokens:\n      ids.extend(self._token_to_ids(token))\n    return text_encoder.pad_incr(ids)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode a list of integers into text.", "response": "def decode(self, ids):\n    \"\"\"Decodes a list of integers into text.\"\"\"\n    ids = text_encoder.pad_decr(ids)\n    subword_ids = ids\n    del ids\n\n    subwords = []\n\n    # Some ids correspond to bytes. Because unicode characters are composed of\n    # possibly multiple bytes, we attempt to decode contiguous lists of bytes\n    # all together. Invalid byte sequences are replaced with the unicode\n    # replacement (i.e. unknown) character U+FFFD.\n    prev_bytes = []\n\n    def consume_prev_bytes():\n      if prev_bytes:\n        bytestr = b\"\".join(prev_bytes)\n        bytes_text = bytestr.decode(\"utf-8\", \"replace\")\n        subwords.append(bytes_text)\n      return []\n\n    for subword_id in subword_ids:\n      subword = self._id_to_subword(subword_id)\n      if isinstance(subword, six.binary_type):\n        # Byte-encoded\n        prev_bytes.append(subword)\n      else:\n        # If there were bytes previously, convert to unicode.\n        prev_bytes = consume_prev_bytes()\n        trimmed, add_space = _trim_underscore_and_tell(subword)\n        subwords.append(trimmed)\n        if add_space:\n          subwords.append(\" \")\n    # If there were trailing bytes, convert to unicode.\n    prev_bytes = consume_prev_bytes()\n\n    return tf.compat.as_text(\"\".join(subwords))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _token_to_ids(self, token):\n    # Check cache\n    cache_location = hash(token) % self._cache_size\n    cache_key, cache_value = self._token_to_ids_cache[cache_location]\n    if cache_key == token:\n      return cache_value\n\n    subwords = self._token_to_subwords(token)\n    ids = []\n    for subword in subwords:\n      if subword == _UNDERSCORE_REPLACEMENT:\n        ids.append(len(self._subwords) + ord(\"_\"))\n        continue\n      subword_id = self._subword_to_id.get(subword)\n      if subword_id is None:\n        # Byte-encode\n        ids.extend(self._byte_encode(subword))\n      else:\n        ids.append(subword_id)\n\n    # Update cache\n    self._token_to_ids_cache[cache_location] = (token, ids)\n\n    return ids", "response": "Convert a single token to a list of integer ids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nencode a single token byte - wise into integer ids.", "response": "def _byte_encode(self, token):\n    \"\"\"Encode a single token byte-wise into integer ids.\"\"\"\n    # Vocab ids for all bytes follow ids for the subwords\n    offset = len(self._subwords)\n    if token == \"_\":\n      return [len(self._subwords) + ord(\" \")]\n    return [i + offset for i in list(bytearray(tf.compat.as_bytes(token)))]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a subword integer ID to a subword string.", "response": "def _id_to_subword(self, subword_id):\n    \"\"\"Converts a subword integer ID to a subword string.\"\"\"\n    if subword_id < 0 or subword_id >= (self.vocab_size - 1):\n      raise ValueError(\"Received id %d which is invalid. Ids must be within \"\n                       \"[0, %d).\" % (subword_id + 1, self.vocab_size))\n\n    if 0 <= subword_id < len(self._subwords):\n      # Subword\n      return self._subwords[subword_id]\n    else:\n      # Byte\n      offset = len(self._subwords)\n      subword_id -= offset\n      bytestr = bytes(bytearray([subword_id]))\n      return bytestr"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _init_from_list(self, subwords):\n    subwords = [tf.compat.as_text(s) for s in subwords if s]\n    self._subwords = subwords\n    # Note that internally everything is 0-indexed. Padding is dealt with at the\n    # end of encode and the beginning of decode.\n    self._subword_to_id = {s: i for i, s in enumerate(subwords)}\n\n    # We remember the maximum length of any subword to avoid having to\n    # check arbitrarily long strings.\n    self._max_subword_len = max(\n        len(_UNDERSCORE_REPLACEMENT), max([len(s) for s in subwords] or [1]))\n\n    # Initialize the cache\n    self._cache_size = 2**20\n    self._token_to_ids_cache = [(None, None)] * self._cache_size\n\n    # Setup tokenizer\n    # Reserved tokens are all tokens that are mixed alphanum and non-alphanum.\n    reserved_tokens = set([_UNDERSCORE_REPLACEMENT])\n    for t in self._subwords:\n      if text_encoder.is_mixed_alphanum(t):\n        reserved_tokens.add(t)\n    self._tokenizer = text_encoder.Tokenizer(\n        alphanum_only=False, reserved_tokens=reserved_tokens)", "response": "Initializes the encoder from a list of subwords."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_to_file(self, filename_prefix):\n    # Wrap in single quotes to make it easier to see the full subword when\n    # it has spaces and make it easier to search with ctrl+f.\n    filename = self._filename(filename_prefix)\n    lines = [\"'%s'\" % s for s in self._subwords]\n    self._write_lines_to_file(filename, lines)", "response": "Save the vocabulary to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts list of subwords from file.", "response": "def load_from_file(cls, filename_prefix):\n    \"\"\"Extracts list of subwords from file.\"\"\"\n    filename = cls._filename(filename_prefix)\n    lines, _ = cls._read_lines_from_file(filename)\n    # Strip wrapping single quotes\n    vocab_list = [line[1:-1] for line in lines]\n    return cls(vocab_list=vocab_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a SubwordTextEncoder from a generator yielding str from which subwords will be constructed.", "response": "def build_from_corpus(cls,\n                        corpus_generator,\n                        target_vocab_size,\n                        max_subword_length=20,\n                        max_corpus_chars=None,\n                        reserved_tokens=None):\n    \"\"\"Builds a `SubwordTextEncoder` based on the `corpus_generator`.\n\n    Args:\n      corpus_generator: generator yielding `str`, from which subwords will be\n        constructed.\n      target_vocab_size: `int`, approximate size of the vocabulary to create.\n      max_subword_length: `int`, maximum length of a subword. Note that memory\n        and compute scale quadratically in the length of the longest token.\n      max_corpus_chars: `int`, the maximum number of characters to consume from\n        `corpus_generator` for the purposes of building the subword vocabulary.\n      reserved_tokens: `list<str>`, list of tokens that will always be treated\n        as whole tokens and not split up. Note that these must contain a mix of\n        alphanumeric and non-alphanumeric characters (e.g. \"<EOS>\") and not end\n        in an underscore.\n\n    Returns:\n      `SubwordTextEncoder`.\n    \"\"\"\n    reserved_tokens = reserved_tokens or []\n    _validate_build_arguments(\n        max_subword_length=max_subword_length,\n        reserved_tokens=reserved_tokens,\n        target_vocab_size=target_vocab_size)\n    token_counts = _token_counts_from_generator(\n        generator=corpus_generator,\n        max_chars=max_corpus_chars,\n        reserved_tokens=reserved_tokens)\n\n    # Binary search on the minimum token count to build a vocabulary with\n    # approximately the right size\n    def _binary_search(min_token_count, max_token_count):\n      \"\"\"Binary search min_token_count to build SubwordTextEncoder vocab.\"\"\"\n      candidate_min = (min_token_count + max_token_count) // 2\n      logging.info(\"SubwordTextEncoder build: trying min_token_count %d\",\n                   candidate_min)\n      encoder = cls._build_from_token_counts(\n          token_counts=token_counts,\n          min_token_count=candidate_min,\n          reserved_tokens=reserved_tokens,\n          num_iterations=4,\n          max_subword_length=max_subword_length)\n      vocab_size = encoder.vocab_size\n\n      # Being within 1% of the target vocab size is ok\n      target_achieved = (\n          abs(vocab_size - target_vocab_size) * 100 < target_vocab_size)\n      if (target_achieved or min_token_count >= max_token_count or\n          candidate_min <= 1):\n        # Search complete\n        return encoder\n\n      # Recurse\n      if vocab_size > target_vocab_size:\n        next_encoder = _binary_search(candidate_min + 1, max_token_count)\n      else:\n        next_encoder = _binary_search(min_token_count, candidate_min - 1)\n\n      # Return the one that's closest to the target_vocab_size\n      if (abs(vocab_size - target_vocab_size) <\n          abs(next_encoder.vocab_size - target_vocab_size)):\n        return encoder\n      else:\n        return next_encoder\n\n    # Get min and max token counts.\n    min_token_count = max(min(token_counts.values()), 1)\n    max_token_count = max(token_counts.values())\n\n    # Another option could be to do a binary search over *ranks* of the tokens.\n    return _binary_search(min_token_count, max_token_count)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_examples(self, file_path):\n\n    fieldnames = [\n        'class_label', 'lepton_pT', 'lepton_eta', 'lepton_phi',\n        'missing_energy_magnitude', 'missing_energy_phi', 'jet_1_pt',\n        'jet_1_eta', 'jet_1_phi', 'jet_1_b-tag', 'jet_2_pt', 'jet_2_eta',\n        'jet_2_phi', 'jet_2_b-tag', 'jet_3_pt', 'jet_3_eta', 'jet_3_phi',\n        'jet_3_b-tag', 'jet_4_pt', 'jet_4_eta', 'jet_4_phi', 'jet_4_b-tag',\n        'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb'\n    ]\n\n    with tf.io.gfile.GFile(file_path) as csvfile:\n      reader = csv.DictReader(csvfile, fieldnames=fieldnames)\n      for row in reader:\n        yield row", "response": "Generate the examples for the current log entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating Cats vs Dogs images and labels given a directory path.", "response": "def _generate_examples(self, archive):\n    \"\"\"Generate Cats vs Dogs images and labels given a directory path.\"\"\"\n    num_skipped = 0\n    for fname, fobj in archive:\n      res = _NAME_RE.match(fname)\n      if not res:  # README file, ...\n        continue\n      label = res.group(1).lower()\n      if tf.compat.as_bytes(\"JFIF\") not in fobj.peek(10):\n        num_skipped += 1\n        continue\n      yield {\n          \"image\": fobj,\n          \"image/filename\": fname,\n          \"label\": label,\n      }\n\n    if num_skipped != _NUM_CORRUPT_IMAGES:\n      raise ValueError(\"Expected %d corrupt images, but found %d\" % (\n          _NUM_CORRUPT_IMAGES, num_skipped))\n    logging.warning(\"%d images were corrupted and were skipped\", num_skipped)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a data chunk as specified by the paths.", "response": "def _load_chunk(dat_path, cat_path, info_path):\n  \"\"\"Loads a data chunk as specified by the paths.\n\n  Args:\n    dat_path: Path to dat file of the chunk.\n    cat_path: Path to cat file of the chunk.\n    info_path: Path to info file of the chunk.\n\n  Returns:\n    Tuple with the dat, cat, info_arrays.\n  \"\"\"\n  dat_array = read_binary_matrix(dat_path)\n  # Even if the image is gray scale, we need to add an extra channel dimension\n  # to be compatible with tfds.features.Image.\n  dat_array = np.expand_dims(dat_array, -1)\n\n  cat_array = read_binary_matrix(cat_path)\n\n  info_array = read_binary_matrix(info_path)\n  info_array = np.copy(info_array)  # Make read-only buffer array writable.\n  # Azimuth values are 0, 2, 4, .., 34. We divide by 2 to get proper labels.\n  info_array[:, 2] = info_array[:, 2] / 2\n\n  return dat_array, cat_array, info_array"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_binary_matrix(filename):\n  with tf.io.gfile.GFile(filename, \"rb\") as f:\n    s = f.read()\n\n    # Data is stored in little-endian byte order.\n    int32_dtype = np.dtype(\"int32\").newbyteorder(\"<\")\n\n    # The first 4 bytes contain a magic code that specifies the data type.\n    magic = int(np.frombuffer(s, dtype=int32_dtype, count=1))\n    if magic == 507333717:\n      data_dtype = np.dtype(\"uint8\")  # uint8 does not have a byte order.\n    elif magic == 507333716:\n      data_dtype = np.dtype(\"int32\").newbyteorder(\"<\")\n    else:\n      raise ValueError(\"Invalid magic value for data type!\")\n\n    # The second 4 bytes contain an int32 with the number of dimensions of the\n    # stored array.\n    ndim = int(np.frombuffer(s, dtype=int32_dtype, count=1, offset=4))\n\n    # The next ndim x 4 bytes contain the shape of the array in int32.\n    dims = np.frombuffer(s, dtype=int32_dtype, count=ndim, offset=8)\n\n    # If the array has less than three dimensions, three int32 are still used to\n    # save the shape info (remaining int32 are simply set to 1). The shape info\n    # hence uses max(3, ndim) bytes.\n    bytes_used_for_shape_info = max(3, ndim) * 4\n\n    # The remaining bytes are the array.\n    data = np.frombuffer(\n        s, dtype=data_dtype, offset=8 + bytes_used_for_shape_info)\n  return data.reshape(tuple(dims))", "response": "Reads and returns a binary formatted matrix stored in filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating examples for the Smallnorb dataset.", "response": "def _generate_examples(self, dat_path, cat_path, info_path):\n    \"\"\"Generate examples for the Smallnorb dataset.\n\n    Args:\n      dat_path: Path to dat file of the chunk.\n      cat_path: Path to cat file of the chunk.\n      info_path: Path to info file of the chunk.\n\n    Yields:\n      Dictionaries with images and the different labels.\n    \"\"\"\n    dat_arr, cat_arr, info_arr = _load_chunk(dat_path, cat_path, info_path)\n\n    for image, category, info_vec in moves.zip(dat_arr, cat_arr, info_arr):\n      yield {\n          \"image\": image[0],\n          \"image2\": image[1],\n          \"label_category\": category,\n          \"instance\": info_vec[0],\n          \"label_elevation\": info_vec[1],\n          \"label_azimuth\": info_vec[2],\n          \"label_lighting\": info_vec[3],\n      }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_dataset(instruction_dicts,\n                  dataset_from_file_fn,\n                  shuffle_files=False,\n                  parallel_reads=64):\n  \"\"\"Constructs a `tf.data.Dataset` from TFRecord files.\n\n  Args:\n    instruction_dicts: `list` of {'filepath':, 'mask':, 'offset_mask':}\n      containing the information about which files and which examples to use.\n      The boolean mask will be repeated and zipped with the examples from\n      filepath.\n    dataset_from_file_fn: function returning a `tf.data.Dataset` given a\n      filename.\n    shuffle_files: `bool`, Whether to shuffle the input filenames.\n    parallel_reads: `int`, how many files to read in parallel.\n\n  Returns:\n    `tf.data.Dataset`\n  \"\"\"\n\n  # First case: All examples are taken (No value skipped)\n  if _no_examples_skipped(instruction_dicts):\n    # Only use the filenames as instruction\n    instruction_ds = tf.data.Dataset.from_tensor_slices([\n        d[\"filepath\"] for d in instruction_dicts\n    ])\n    build_ds_from_instruction = dataset_from_file_fn\n  # Second case: Use the instructions to read the examples\n  else:\n    instruction_ds = _build_instruction_ds(instruction_dicts)\n    build_ds_from_instruction = functools.partial(\n        _build_ds_from_instruction,\n        ds_from_file_fn=dataset_from_file_fn,\n    )\n\n  # If shuffle is True, we shuffle the instructions/shards\n  if shuffle_files:\n    instruction_ds = instruction_ds.shuffle(len(instruction_dicts))\n\n  # Use interleave to parallel read files and decode records\n  ds = instruction_ds.interleave(\n      build_ds_from_instruction,\n      cycle_length=parallel_reads,\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  return ds", "response": "Constructs a tf. data. Dataset from TFRecord files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild a tf. data. Dataset containing individual instruction for each shard.", "response": "def _build_instruction_ds(instructions):\n  \"\"\"Create a dataset containing individual instruction for each shard.\n\n  Each instruction is a dict:\n  ```\n  {\n      \"filepath\": tf.Tensor(shape=(), dtype=tf.string),\n      \"mask_offset\": tf.Tensor(shape=(), dtype=tf.int64),\n      \"mask\": tf.Tensor(shape=(100,), dtype=tf.bool),\n  }\n  ```\n\n  Args:\n    instructions: `list[dict]`, the list of instruction dict\n\n  Returns:\n    instruction_ds: The dataset containing the instruction. The dataset size is\n      the number of shard.\n  \"\"\"\n  # Transpose the list[dict] into dict[list]\n  tensor_inputs = {\n      # offset_mask need to be converted to int64 explicitly\n      k: np.array(vals, dtype=np.int64) if k == \"mask_offset\" else list(vals)\n      for k, vals in utils.zip_dict(*instructions)\n  }\n  return tf.data.Dataset.from_tensor_slices(tensor_inputs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_mask_ds(mask, mask_offset):\n  mask_ds = tf.data.Dataset.from_tensor_slices(mask)\n  mask_ds = mask_ds.repeat()\n  mask_ds = mask_ds.skip(mask_offset)\n  return mask_ds", "response": "Build the mask dataset to indicate which elements to skip."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_ds_from_instruction(instruction, ds_from_file_fn):\n  # Create the example and mask ds for this particular shard\n  examples_ds = ds_from_file_fn(instruction[\"filepath\"])\n  mask_ds = _build_mask_ds(\n      mask_offset=instruction[\"mask_offset\"],\n      mask=instruction[\"mask\"],\n  )\n\n  # Zip the mask and real examples\n  ds = tf.data.Dataset.zip((examples_ds, mask_ds))\n  # Filter according to the mask (only keep True)\n  ds = ds.filter(lambda example, mask: mask)\n  # Only keep the examples\n  ds = ds.map(lambda example, mask: example)\n  return ds", "response": "Map an instruction to a real datasets for one particular shard."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a tf. data. Dataset to an iterable of NumPy arrays and NumPy arrays respectively.", "response": "def as_numpy(dataset, graph=None):\n  \"\"\"Converts a `tf.data.Dataset` to an iterable of NumPy arrays.\n\n  `as_numpy` converts a possibly nested structure of `tf.data.Dataset`s\n  and `tf.Tensor`s to iterables of NumPy arrays and NumPy arrays, respectively.\n\n  Args:\n    dataset: a possibly nested structure of `tf.data.Dataset`s and/or\n      `tf.Tensor`s.\n    graph: `tf.Graph`, optional, explicitly set the graph to use.\n\n  Returns:\n    A structure matching `dataset` where `tf.data.Dataset`s are converted to\n    generators of NumPy arrays and `tf.Tensor`s are converted to NumPy arrays.\n  \"\"\"\n  nested_ds = dataset\n  del dataset\n\n  # Flatten\n  flat_ds = tf.nest.flatten(nested_ds)\n  flat_np = []\n\n  # Type check for Tensors and Datasets\n  for ds_el in flat_ds:\n    types = [type(el) for el in flat_ds]\n    types = tf.nest.pack_sequence_as(nested_ds, types)\n    if not (isinstance(ds_el, tf.Tensor) or tf_compat.is_dataset(ds_el)):\n      raise ValueError(\"Arguments to as_numpy must be tf.Tensors or \"\n                       \"tf.data.Datasets. Got: %s\" % types)\n\n  if tf.executing_eagerly():\n    # Eager mode\n    for ds_el in flat_ds:\n      if isinstance(ds_el, tf.Tensor):\n        np_el = ds_el.numpy()\n      elif tf_compat.is_dataset(ds_el):\n        np_el = _eager_dataset_iterator(ds_el)\n      else:\n        assert False\n      flat_np.append(np_el)\n  else:\n    # Graph mode\n\n    # First create iterators for datasets\n    with utils.maybe_with_graph(graph, create_if_none=False):\n      ds_iters = [\n          tf.compat.v1.data.make_one_shot_iterator(ds_el).get_next()\n          for ds_el in flat_ds if tf_compat.is_dataset(ds_el)\n      ]\n    ds_iters = [_graph_dataset_iterator(ds_iter, graph) for ds_iter in ds_iters]\n\n    # Then create numpy arrays for tensors\n    with utils.nogpu_session(graph) as sess:  # Shared session for tf.Tensor\n      # Calling sess.run once so that randomness is shared.\n      np_arrays = sess.run([tensor for tensor in flat_ds\n                            if not tf_compat.is_dataset(tensor)])\n\n    # Merge the dataset iterators and np arrays\n    iter_ds = iter(ds_iters)\n    iter_array = iter(np_arrays)\n    flat_np = [\n        next(iter_ds) if tf_compat.is_dataset(ds_el) else next(iter_array)\n        for ds_el in flat_ds\n    ]\n\n  # Nest\n  return tf.nest.pack_sequence_as(nested_ds, flat_np)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_data(filepath):\n  with h5py.File(filepath, \"r\") as h5dataset:\n    image_array = np.array(h5dataset[\"images\"])\n    # The 'label' data set in the hdf5 file actually contains the float values\n    # and not the class labels.\n    values_array = np.array(h5dataset[\"labels\"])\n  return image_array, values_array", "response": "Loads the images and latent values into Numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_examples(self, filepath):\n    # Simultaneously iterating through the different data sets in the hdf5\n    # file will be slow with a single file. Instead, we first load everything\n    # into memory before yielding the samples.\n    image_array, values_array = _load_data(filepath)\n\n    # We need to calculate the class labels from the float values in the file.\n    labels_array = np.zeros_like(values_array, dtype=np.int64)\n    for i in range(values_array.shape[1]):\n      labels_array[:, i] = _discretize(values_array[:, i])  # pylint: disable=unsupported-assignment-operation\n\n    for image, labels, values in moves.zip(image_array, labels_array,\n                                           values_array):\n      yield {\n          \"image\": image,\n          \"label_floor_hue\": labels[0],\n          \"label_wall_hue\": labels[1],\n          \"label_object_hue\": labels[2],\n          \"label_scale\": labels[3],\n          \"label_shape\": labels[4],\n          \"label_orientation\": labels[5],\n          \"value_floor_hue\": values[0],\n          \"value_wall_hue\": values[1],\n          \"value_object_hue\": values[2],\n          \"value_scale\": values[3],\n          \"value_shape\": values[4],\n          \"value_orientation\": values[5],\n      }", "response": "Generate examples for the Shapes3d dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstrip formatting and unwanted sections from raw page content.", "response": "def _parse_and_clean_wikicode(raw_content):\n  \"\"\"Strips formatting and unwanted sections from raw page content.\"\"\"\n  wikicode = tfds.core.lazy_imports.mwparserfromhell.parse(raw_content)\n\n  # Filters for references, tables, and file/image links.\n  re_rm_wikilink = re.compile(\n      \"^(?:File|Image|Media):\", flags=re.IGNORECASE | re.UNICODE)\n  def rm_wikilink(obj):\n    return bool(re_rm_wikilink.match(six.text_type(obj.title)))\n  def rm_tag(obj):\n    return six.text_type(obj.tag) in {\"ref\", \"table\"}\n  def rm_template(obj):\n    return obj.name.lower() in {\n        \"reflist\", \"notelist\", \"notelist-ua\", \"notelist-lr\", \"notelist-ur\",\n        \"notelist-lg\"}\n\n  def try_remove_obj(obj, section):\n    try:\n      section.remove(obj)\n    except ValueError:\n      # For unknown reasons, objects are sometimes not found.\n      pass\n\n  section_text = []\n  # Filter individual sections to clean.\n  for section in wikicode.get_sections(\n      flat=True, include_lead=True, include_headings=True):\n    for obj in section.ifilter_wikilinks(matches=rm_wikilink, recursive=True):\n      try_remove_obj(obj, section)\n    for obj in section.ifilter_templates(matches=rm_template, recursive=True):\n      try_remove_obj(obj, section)\n    for obj in section.ifilter_tags(matches=rm_tag, recursive=True):\n      try_remove_obj(obj, section)\n\n    section_text.append(section.strip_code().strip())\n  return \"\\n\\n\".join(section_text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a PCollection from a list of filepaths.", "response": "def _build_pcollection(self, pipeline, filepaths, language):\n    \"\"\"Build PCollection of examples in the raw (text) form.\"\"\"\n\n    beam = tfds.core.lazy_imports.apache_beam\n\n    def _extract_content(filepath):\n      \"\"\"Extracts article content from a single WikiMedia XML file.\"\"\"\n      logging.info(\"generating examples from = %s\", filepath)\n      with tf.io.gfile.GFile(filepath) as f:\n        for _, elem in etree.iterparse(f, events=(\"end\",)):\n          if not elem.tag.endswith(\"page\"):\n            continue\n          namespace = elem.tag[:-4]\n          title = elem.find(\"./{0}title\".format(namespace)).text\n          ns = elem.find(\"./{0}ns\".format(namespace)).text\n\n          # Filter pages that are not in the \"main\" namespace.\n          if ns != \"0\":\n            continue\n\n          raw_content = elem.find(\n              \"./{0}revision/{0}text\".format(namespace)).text\n          elem.clear()\n\n          # Filter redirects.\n          if raw_content is None or raw_content.lower().startswith(\"#redirect\"):\n            beam.metrics.Metrics.counter(language, \"filtered-redirects\").inc()\n            continue\n\n          beam.metrics.Metrics.counter(language, \"extracted-examples\").inc()\n          yield (title, raw_content)\n\n    def _clean_content(inputs):\n      \"\"\"Cleans raw wikicode to extract text.\"\"\"\n      title, raw_content = inputs\n      try:\n        text = _parse_and_clean_wikicode(raw_content)\n      except (\n          tfds.core.lazy_imports.mwparserfromhell.parser.ParserError) as e:\n        beam.metrics.Metrics.counter(language, \"parser-error\").inc()\n        logging.error(\"mwparserfromhell ParseError: %s\", e)\n        return\n\n      beam.metrics.Metrics.counter(language, \"cleaned-examples\").inc()\n\n      yield {\n          \"title\": title,\n          \"text\": text\n      }\n\n    return (\n        pipeline\n        | beam.Create(filepaths)\n        | beam.FlatMap(_extract_content)\n        | beam.FlatMap(_clean_content)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_and_prepare(builder):\n  print(\"download_and_prepare for dataset {}...\".format(builder.info.full_name))\n\n  dl_config = download_config()\n\n  if isinstance(builder, tfds.core.BeamBasedBuilder):\n    beam = tfds.core.lazy_imports.apache_beam\n    # TODO(b/129149715): Restore compute stats. Currently skipped because not\n    # beam supported.\n    dl_config.compute_stats = tfds.download.ComputeStatsMode.SKIP\n    dl_config.beam_options = beam.options.pipeline_options.PipelineOptions()\n\n  builder.download_and_prepare(\n      download_dir=FLAGS.download_dir,\n      download_config=dl_config,\n  )\n  termcolor.cprint(str(builder.info.as_proto), attrs=[\"bold\"])\n\n  if FLAGS.debug:\n    dataset = builder.as_dataset(split=tfds.Split.TRAIN)\n    pdb.set_trace()\n    del dataset", "response": "Download and prepare a dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nencoding an example for a BBox feature.", "response": "def encode_example(self, bbox):\n    \"\"\"See base class for details.\"\"\"\n    # Validate the coordinates\n    for coordinate in bbox:\n      if not isinstance(coordinate, float):\n        raise ValueError(\n            'BBox coordinates should be float. Got {}.'.format(bbox))\n      if not 0.0 <= coordinate <= 1.0:\n        raise ValueError(\n            'BBox coordinates should be between 0 and 1. Got {}.'.format(bbox))\n      if bbox.xmax < bbox.xmin or bbox.ymax < bbox.ymin:\n        raise ValueError(\n            'BBox coordinates should have min <= max. Got {}.'.format(bbox))\n\n    return super(BBoxFeature, self).encode_example(\n        [bbox.ymin, bbox.xmin, bbox.ymax, bbox.xmax]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_data(path, labels_number=1):\n  with tf.io.gfile.GFile(path, \"rb\") as f:\n    data = f.read()\n  offset = 0\n  max_offset = len(data) - 1\n  while offset < max_offset:\n    labels = np.frombuffer(data, dtype=np.uint8, count=labels_number,\n                           offset=offset).reshape((labels_number,))\n    # 1 byte per label, 1024 * 3 = 3072 bytes for the image.\n    offset += labels_number\n    img = (np.frombuffer(data, dtype=np.uint8, count=3072, offset=offset)\n           .reshape((3, _CIFAR_IMAGE_SIZE, _CIFAR_IMAGE_SIZE))\n           .transpose((1, 2, 0))\n          )\n    offset += 3072\n    yield labels, img", "response": "Load the image data from a CIFAR file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _split_generators(self, dl_manager):\n    cifar_path = dl_manager.download_and_extract(self._cifar_info.url)\n    cifar_info = self._cifar_info\n\n    cifar_path = os.path.join(cifar_path, cifar_info.prefix)\n\n    # Load the label names\n    for label_key, label_file in zip(cifar_info.label_keys,\n                                     cifar_info.label_files):\n      labels_path = os.path.join(cifar_path, label_file)\n      with tf.io.gfile.GFile(labels_path) as label_f:\n        label_names = [name for name in label_f.read().split(\"\\n\") if name]\n      self.info.features[label_key].names = label_names\n\n    # Define the splits\n    def gen_filenames(filenames):\n      for f in filenames:\n        yield os.path.join(cifar_path, f)\n\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=10,\n            gen_kwargs={\"filepaths\": gen_filenames(cifar_info.train_files)}),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=1,\n            gen_kwargs={\"filepaths\": gen_filenames(cifar_info.test_files)}),\n    ]", "response": "Returns a list of SplitGenerators."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_examples(self, filepaths):\n    label_keys = self._cifar_info.label_keys\n    for path in filepaths:\n      for labels, np_image in _load_data(path, len(label_keys)):\n        row = dict(zip(label_keys, labels))\n        row[\"image\"] = np_image\n        yield row", "response": "Generate CIFAR examples as dicts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disallow_positional_args(wrapped=None, allowed=None):\n  # See\n  # https://wrapt.readthedocs.io/en/latest/decorators.html#decorators-with-optional-arguments\n  # for decorator pattern.\n  if wrapped is None:\n    return functools.partial(disallow_positional_args, allowed=allowed)\n\n  @wrapt.decorator\n  def disallow_positional_args_dec(fn, instance, args, kwargs):\n    ismethod = instance is not None\n    _check_no_positional(fn, args, ismethod, allowed=allowed)\n    _check_required(fn, kwargs)\n    return fn(*args, **kwargs)\n\n  return disallow_positional_args_dec(wrapped)", "response": "Requires function to be called using keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _required_args(fn):\n  spec = getargspec(fn)\n  if not spec.defaults:\n    return []\n\n  arg_names = spec.args[-len(spec.defaults):]\n  return [name for name, val in zip(arg_names, spec.defaults)\n          if val is REQUIRED_ARG]", "response": "Returns arguments of fn with default = REQUIRED_ARG."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download_gcs_file(path, out_fname=None, prefix_filter=None):\n  url = posixpath.join(GCS_BUCKET, path)\n  if prefix_filter:\n    url += \"?prefix=%s\" % prefix_filter\n  stream = bool(out_fname)\n  resp = requests.get(url, stream=stream)\n  if not resp.ok:\n    raise ValueError(\"GCS bucket inaccessible\")\n  if out_fname:\n    with tf.io.gfile.GFile(out_fname, \"wb\") as f:\n      for chunk in resp.iter_content(1024):\n        f.write(chunk)\n  else:\n    return resp.content", "response": "Download a file from GCS optionally to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gcs_files(prefix_filter=None):\n  top_level_xml_str = download_gcs_file(\"\", prefix_filter=prefix_filter)\n  xml_root = ElementTree.fromstring(top_level_xml_str)\n  filenames = [el[0].text for el in xml_root if el.tag.endswith(\"Contents\")]\n  return filenames", "response": "List all files in GCS bucket."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gcs_dataset_info_files(dataset_dir):\n  prefix = posixpath.join(GCS_DATASET_INFO_DIR, dataset_dir, \"\")\n  # Filter for this dataset\n  filenames = [el for el in gcs_files(prefix_filter=prefix)\n               if el.startswith(prefix) and len(el) > len(prefix)]\n  return filenames", "response": "Return paths to GCS files in the given dataset directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the dataset is available on GCS bucket gs://tfds - data.", "response": "def is_dataset_on_gcs(dataset_name):\n  \"\"\"If the dataset is available on the GCS bucket gs://tfds-data/datasets.\"\"\"\n  dir_name = posixpath.join(GCS_DATASETS_DIR, dataset_name)\n  return len(gcs_files(prefix_filter=dir_name)) > 2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_kaggle_command(command_args, competition_name):\n  try:\n    output = sp.check_output(command_args)\n    return tf.compat.as_text(output)\n  except sp.CalledProcessError as err:\n    output = err.output\n    _log_command_output(output, error=True)\n    if output.startswith(b\"404\"):\n      logging.error(_NOT_FOUND_ERR_MSG, competition_name)\n      raise\n    logging.error(_ERR_MSG, competition_name)\n    raise", "response": "Run kaggle command with subprocess."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist of competition files.", "response": "def competition_files(self):\n    \"\"\"List of competition files.\"\"\"\n    command = [\n        \"kaggle\",\n        \"datasets\" if \"/\" in self._competition_name else \"competitions\",\n        \"files\",\n        \"-v\",\n        self._competition_name,\n    ]\n    output = _run_kaggle_command(command, self._competition_name)\n    return sorted([\n        line.split(\",\")[0] for line in output.split(\"\\n\")[1:] if line\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of kaggle. net urls.", "response": "def competition_urls(self):\n    \"\"\"Returns 'kaggle://' urls.\"\"\"\n    return [\n        KaggleFile(self._competition_name, fname).to_url()\n        for fname in self.competition_files  # pylint: disable=not-an-iterable\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_file(self, fname, output_dir):\n    if fname not in self.competition_files:  # pylint: disable=unsupported-membership-test\n      raise ValueError(\"%s is not one of the competition's \"\n                       \"files: %s\" % (fname, self.competition_files))\n    command = [\n        \"kaggle\",\n        \"competitions\",\n        \"download\",\n        \"--file\",\n        fname,\n        \"--path\",\n        output_dir,\n        \"-c\",\n        self._competition_name,\n    ]\n    _run_kaggle_command(command, self._competition_name)\n    return os.path.join(output_dir, fname)", "response": "Downloads a competition file to output_dir."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate flower images and labels given the image directory path.", "response": "def _generate_examples(self, images_dir_path):\n    \"\"\"Generate flower images and labels given the image directory path.\n\n    Args:\n      images_dir_path: path to the directory where the images are stored.\n\n    Yields:\n      The image path and its corresponding label.\n    \"\"\"\n    parent_dir = tf.io.gfile.listdir(images_dir_path)[0]\n    walk_dir = os.path.join(images_dir_path, parent_dir)\n    dirs = tf.io.gfile.listdir(walk_dir)\n\n    for d in dirs:\n      if tf.io.gfile.isdir(os.path.join(walk_dir, d)):\n        for full_path, _, fname in tf.io.gfile.walk(os.path.join(walk_dir, d)):\n          for image_file in fname:\n            if image_file.endswith(\".jpg\"):\n              image_path = os.path.join(full_path, image_file)\n              yield {\n                  \"image\": image_path,\n                  \"label\": d.lower(),\n              }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _checksum_paths():\n  dataset2path = {}\n  for dir_path in _CHECKSUM_DIRS:\n    for fname in _list_dir(dir_path):\n      if not fname.endswith(_CHECKSUM_SUFFIX):\n        continue\n      fpath = os.path.join(dir_path, fname)\n      dataset_name = fname[:-len(_CHECKSUM_SUFFIX)]\n      dataset2path[dataset_name] = fpath\n  return dataset2path", "response": "Returns dict containing dataset_name = > path to checksums file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the path to where checksums are stored for a given dataset.", "response": "def _get_path(dataset_name):\n  \"\"\"Returns path to where checksums are stored for a given dataset.\"\"\"\n  path = _checksum_paths().get(dataset_name, None)\n  if path:\n    return path\n  msg = ('No checksums file could be find for dataset %s. Please create one in '\n         'one of: %s') % (dataset_name, ', '.join(_CHECKSUM_DIRS))\n  raise AssertionError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_sizes_checksums(checksums_path):\n  checksums = {}\n  for line in _read_file(checksums_path).split('\\n'):\n    if not line:\n      continue\n    # URL might have spaces inside, but size and checksum will not.\n    url, size, checksum = line.rsplit(' ', 2)\n    checksums[url] = (int(size), checksum)\n  return checksums", "response": "Returns a dictionary of URL - > size and checksum for each file in the checksums_path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_sizes_checksums():\n  sizes_checksums = {}\n  for path in _checksum_paths().values():\n    data = _get_sizes_checksums(path)\n    for url, size_checksum in data.items():\n      if (url in sizes_checksums and\n          sizes_checksums[url] != size_checksum):\n        raise AssertionError(\n            'URL %s is registered with 2+ distinct size/checksum tuples.' % url)\n    sizes_checksums.update(data)\n  return sizes_checksums", "response": "Returns dict associating URL to size and sha256."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring given checksums and sizes for a specific dataset.", "response": "def store_checksums(dataset_name, sizes_checksums):\n  \"\"\"Store given checksums and sizes for specific dataset.\n\n  Content of file is never disgarded, only updated. This is to ensure that if\n  process is killed right after first download finishes, checksums registered\n  during previous runs aren't lost.\n\n  It is the responsibility of the caller not to call function multiple times in\n  parallel for a given dataset.\n\n  Only original file content is updated. This means the entire set of new sizes\n  and checksums must be given at every call.\n\n  Args:\n    dataset_name: string.\n    sizes_checksums: dict, {url: (size_in_bytes, checksum)}.\n  \"\"\"\n  path = _get_path(dataset_name)\n  original_data = _get_sizes_checksums(path)\n  new_data = original_data.copy()\n  new_data.update(sizes_checksums)\n  if original_data == new_data:\n    return\n  with tf.io.gfile.GFile(path, 'w') as f:\n    for url, (size, checksum) in sorted(new_data.items()):\n      f.write('%s %s %s\\n' % (url, size, checksum))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nguess extraction method given file name or path.", "response": "def _guess_extract_method(fname):\n  \"\"\"Guess extraction method, given file name (or path).\"\"\"\n  for method, extensions in _EXTRACTION_METHOD_TO_EXTS:\n    for ext in extensions:\n      if fname.endswith(ext):\n        return method\n  return ExtractMethod.NO_EXTRACT"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsanitizes and shorten url.", "response": "def _sanitize_url(url, max_length):\n  \"\"\"Sanitize and shorten url to fit in max_length.\n\n  Function is stable: same input MUST ALWAYS give same result, accros changes\n  in code as well. Different URLs might give same result.\n  As much as possible, the extension should be kept.\n\n  Heuristics are applied to only keep useful info from url.\n\n  1- Drop generic [sub]domains.\n    'www.cs.toronto.edu/...' -> 'cs.toronto.edu/...'\n    'storage.googleapis.com/foo/...' -> 'foo/...'\n    'drive.google.com/bar/...' -> 'bar/...'\n    'github.com/baz/...' -> 'baz/...'\n\n  2- Remove leading '0's from url components:\n    'foo/train-00004-of-00010.tfrecords' -> 'foo/train-4-of-10.tfrecords'\n\n  3- Truncate each component of url until total size fits or each component is\n     left with 4 chars (or total size is <= limit):\n     'MoveUnitToBorder_64x64_png/train-4-of-10.tfrecords'\n     (here truncate components to 4 chars per component max)\n      -> 'Move_64x6_png/trai-4-of-10.tfrecords'\n\n  4- Truncate result, keeping prefix: 'abc_def_ghi_jkl' -> 'abc_def'\n\n  Args:\n    url: string, url to sanitize and shorten.\n    max_length: int, max length of result.\n\n  Returns:\n    (string, string): sanitized and shorted url, file extension.\n  \"\"\"\n  url = urllib.parse.urlparse(url)\n  netloc = url.netloc\n  for prefix in _NETLOC_COMMON_PREFIXES:\n    if netloc.startswith(prefix):\n      netloc = netloc[len(prefix):]\n  for suffix in _NETLOC_COMMON_SUFFIXES:\n    if netloc.endswith(suffix):\n      netloc = netloc[:-len(suffix)]\n  url = '%s%s%s%s' % (netloc, url.path, url.params, url.query)\n  # Get the extension:\n  for ext in _KNOWN_EXTENSIONS:\n    if url.endswith(ext):\n      extension = ext\n      url = url[:-len(extension)]\n      break\n  else:\n    url, extension = os.path.splitext(url)\n  max_length -= len(extension)\n  # Replace non authorized chars (including '/') by '_':\n  url = re.sub(r'[^a-zA-Z0-9\\.\\-_]+', '_', url)\n  # Remove parts with no info:\n  for common_part in _URL_COMMON_PARTS:\n    url = url.replace(common_part, '_')\n  url = url.strip('_')\n  # Remove leading zeros in groups of numbers:\n  url = re.sub('(?<![0-9])0+(?=[0-9])', '', url)\n  # Decrease max size of URL components:\n  c_size = max(len(c) for c in re.split(r'[\\.\\-_]', url))\n  while c_size > 4 and len(url) > max_length:\n    c_size -= 1\n    url = re.sub(r'[^\\.\\-_]{4,}', lambda match: match.group(0)[:c_size], url)\n  return url[:max_length], extension"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dl_fname(url, checksum):\n  checksum = base64.urlsafe_b64encode(_decode_hex(checksum))\n  checksum = tf.compat.as_text(checksum)[:-1]\n  name, extension = _sanitize_url(url, max_length=46)\n  return '%s%s%s' % (name, checksum, extension)", "response": "Returns the name of the file for the given url and checksum."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_dl_dirname(url):\n  checksum = hashlib.sha256(tf.compat.as_bytes(url)).hexdigest()\n  return get_dl_fname(url, checksum)", "response": "Returns name of temp dir for given url."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_info(info_path):\n  if not tf.io.gfile.exists(info_path):\n    return None\n  with tf.io.gfile.GFile(info_path) as info_f:\n    return json.load(info_f)", "response": "Returns info dict or None."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the INFO file next to local file.", "response": "def write_info_file(resource, path, dataset_name, original_fname):\n  \"\"\"Write the INFO file next to local file.\n\n  Although the method is synchronized, there is still a risk two processes\n  running at the same time overlap here. Risk accepted, since potentially lost\n  data (`dataset_name`) is only for human consumption.\n\n  Args:\n    resource: resource for which to write the INFO file.\n    path: path of downloaded file.\n    dataset_name: data used to dl the file.\n    original_fname: name of file as downloaded.\n  \"\"\"\n  info_path = _get_info_path(path)\n  info = _read_info(info_path) or {}\n  urls = set(info.get('urls', []) + [resource.url])\n  dataset_names = info.get('dataset_names', [])\n  if dataset_name:\n    dataset_names.append(dataset_name)\n  if 'original_fname' in info and info['original_fname'] != original_fname:\n    raise AssertionError(\n        '`original_fname` \"%s\" stored in %s does NOT match \"%s\".' % (\n            info['original_fname'], info_path, original_fname))\n  info = dict(urls=list(urls), dataset_names=list(set(dataset_names)),\n              original_fname=original_fname)\n  with py_utils.atomic_write(info_path, 'w') as info_f:\n    json.dump(info, info_f, sort_keys=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning ExtractMethod to use on resource at path. Cannot be None.", "response": "def get_extract_method(path):\n  \"\"\"Returns `ExtractMethod` to use on resource at path. Cannot be None.\"\"\"\n  info_path = _get_info_path(path)\n  info = _read_info(info_path)\n  fname = info.get('original_fname', path) if info else path\n  return _guess_extract_method(fname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exists_locally(cls, path):\n    # If INFO file doesn't exist, consider resource does NOT exist, as it would\n    # prevent guessing the `extract_method`.\n    return (tf.io.gfile.exists(path) and\n            tf.io.gfile.exists(_get_info_path(path)))", "response": "Returns whether the resource exists locally at resource. path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _split_generators(self, dl_manager):\n    root_url = \"http://images.cocodataset.org/\"\n    urls = {\n        # Train/validation set\n        \"train_images\": \"zips/train2014.zip\",\n        \"val_images\": \"zips/val2014.zip\",\n        \"trainval_annotations\": \"annotations/annotations_trainval2014.zip\",\n        # Testing set (no annotations) (2014)\n        \"test_images\": \"zips/test2014.zip\",\n        \"test_annotations\": \"annotations/image_info_test2014.zip\",\n        # Testing set (no annotations) (2015)\n        \"test2015_images\": \"zips/test2015.zip\",\n        \"test2015_annotations\": \"annotations/image_info_test2015.zip\",\n    }\n    extracted_paths = dl_manager.download_and_extract({\n        key: root_url + url for key, url in urls.items()\n    })\n\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=10,\n            gen_kwargs=dict(\n                image_dir=extracted_paths[\"train_images\"],\n                annotation_dir=extracted_paths[\"trainval_annotations\"],\n                split_type=\"train2014\",\n            )),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.VALIDATION,\n            num_shards=10,\n            gen_kwargs=dict(\n                image_dir=extracted_paths[\"val_images\"],\n                annotation_dir=extracted_paths[\"trainval_annotations\"],\n                split_type=\"val2014\",\n            )),\n        # Warning: Testing split only contains the images without any annotation\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=10,\n            gen_kwargs=dict(\n                image_dir=extracted_paths[\"test_images\"],\n                annotation_dir=extracted_paths[\"test_annotations\"],\n                split_type=\"test2014\",\n                has_annotation=False,\n            )),\n        tfds.core.SplitGenerator(\n            name=\"test2015\",\n            num_shards=10,\n            gen_kwargs=dict(\n                image_dir=extracted_paths[\"test2015_images\"],\n                annotation_dir=extracted_paths[\"test2015_annotations\"],\n                split_type=\"test2015\",\n                has_annotation=False,\n            )),\n    ]", "response": "Returns a list of SplitGenerators for the current cocodataset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_examples(\n      self, image_dir, annotation_dir, split_type, has_annotation=True):\n    \"\"\"Generate examples as dicts.\n\n    Args:\n      image_dir: `str`, directory containing the images\n      annotation_dir: `str`, directory containing\n      split_type: `str`, <split_name><year> (ex: train2014)\n      has_annotation: `bool`, when False (for the testing set), the annotations\n        are not recorded\n\n    Yields:\n      Generator yielding the next samples\n    \"\"\"\n    if has_annotation:\n      instance_filename = \"instances_{}.json\"\n    else:\n      instance_filename = \"image_info_{}.json\"\n\n    # Load the label names and images\n    instance_path = os.path.join(\n        annotation_dir,\n        \"annotations\",\n        instance_filename.format(split_type),\n    )\n    coco_annotation = CocoAnnotation(instance_path)\n    # Each category is a dict:\n    # {\n    #    'id': 51,  # From 1-91, some entry missing\n    #    'name': 'bowl',\n    #    'supercategory': 'kitchen',\n    # }\n    categories = coco_annotation.categories\n    # Each image is a dict:\n    # {\n    #     'id': 262145,\n    #     'file_name': 'COCO_train2014_000000262145.jpg'\n    #     'flickr_url': 'http://farm8.staticflickr.com/7187/xyz.jpg',\n    #     'coco_url': 'http://images.cocodataset.org/train2014/xyz.jpg',\n    #     'license': 2,\n    #     'date_captured': '2013-11-20 02:07:55',\n    #     'height': 427,\n    #     'width': 640,\n    # }\n    images = coco_annotation.images\n\n    # TODO(b/121375022): ClassLabel names should also contains 'id' and\n    # and 'supercategory' (in addition to 'name')\n    # Warning: As Coco only use 80 out of the 91 labels, the c['id'] and\n    # dataset names ids won't match.\n    self.info.features[\"objects\"][\"label\"].names = [\n        c[\"name\"] for c in categories\n    ]\n    # TODO(b/121375022): Conversion should be done by ClassLabel\n    categories_id2name = {c[\"id\"]: c[\"name\"] for c in categories}\n\n    # Iterate over all images\n    annotation_skipped = 0\n    for image_info in sorted(images, key=lambda x: x[\"id\"]):\n      if has_annotation:\n        # Each instance annotation is a dict:\n        # {\n        #     'iscrowd': 0,\n        #     'bbox': [116.95, 305.86, 285.3, 266.03],\n        #     'image_id': 480023,\n        #     'segmentation': [[312.29, 562.89, 402.25, ...]],\n        #     'category_id': 58,\n        #     'area': 54652.9556,\n        #     'id': 86,\n        # }\n        instances = coco_annotation.get_annotations(img_id=image_info[\"id\"])\n      else:\n        instances = []  # No annotations\n\n      if not instances:\n        annotation_skipped += 1\n\n      def build_bbox(x, y, width, height):\n        # pylint: disable=cell-var-from-loop\n        # build_bbox is only used within the loop so it is ok to use image_info\n        return tfds.features.BBox(\n            ymin=y / image_info[\"height\"],\n            xmin=x / image_info[\"width\"],\n            ymax=(y + height) / image_info[\"height\"],\n            xmax=(x + width) / image_info[\"width\"],\n        )\n        # pylint: enable=cell-var-from-loop\n\n      yield {\n          \"image\": os.path.join(image_dir, split_type, image_info[\"file_name\"]),\n          \"image/filename\": image_info[\"file_name\"],\n          \"objects\": [{\n              \"bbox\": build_bbox(*instance_info[\"bbox\"]),\n              \"label\": categories_id2name[instance_info[\"category_id\"]],\n              \"is_crowd\": bool(instance_info[\"iscrowd\"]),\n          } for instance_info in instances],\n      }\n    logging.info(\n        \"%d/%d images do not contains any annotations\",\n        annotation_skipped,\n        len(images),\n    )", "response": "Generate examples as dicts."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert list [ int ] => decoded string.", "response": "def ints2str(self, int_values):\n    \"\"\"Conversion list[int] => decoded string.\"\"\"\n    if not self._encoder:\n      raise ValueError(\n          \"Text.ints2str is not available because encoder hasn't been defined.\")\n    return self._encoder.decode(int_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maybe_build_from_corpus(self, corpus_generator, **kwargs):\n    if self._encoder_cls is not text_lib.SubwordTextEncoder:\n      return\n    if self.encoder:\n      return\n\n    vocab_size = self._encoder_config.vocab_size\n    self.encoder = text_lib.SubwordTextEncoder.build_from_corpus(\n        corpus_generator=corpus_generator,\n        target_vocab_size=vocab_size,\n        **kwargs)", "response": "Call SubwordTextEncoder. build_from_corpus is encoder_cls is such."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwalking an Omniglot directory and yield examples.", "response": "def _walk_omniglot_dir(directory):\n  \"\"\"Walk an Omniglot directory and yield examples.\"\"\"\n  directory = os.path.join(directory, tf.io.gfile.listdir(directory)[0])\n  alphabets = sorted(tf.io.gfile.listdir(directory))\n  for alphabet in alphabets:\n    alphabet_dir = os.path.join(directory, alphabet)\n    characters = sorted(tf.io.gfile.listdir(alphabet_dir))\n    for character in characters:\n      character_id = int(character[len(\"character\"):]) - 1\n      character_dir = os.path.join(alphabet_dir, character)\n      images = tf.io.gfile.listdir(character_dir)\n      for image in images:\n        label, _ = image.split(\"_\")\n        label = int(label) - 1\n        image_path = os.path.join(character_dir, image)\n        yield alphabet, character_id, label, image_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets alphabet and label names union across all dirs.", "response": "def _get_names(dirs):\n  \"\"\"Get alphabet and label names, union across all dirs.\"\"\"\n  alphabets = set()\n  label_names = {}\n  for d in dirs:\n    for example in _walk_omniglot_dir(d):\n      alphabet, alphabet_char_id, label, _ = example\n      alphabets.add(alphabet)\n      label_name = \"%s_%d\" % (alphabet, alphabet_char_id)\n      if label in label_names:\n        assert label_names[label] == label_name\n      else:\n        label_names[label] = label_name\n  label_names = [label_names[k] for k in sorted(label_names)]\n  return alphabets, label_names"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef size_str(size_in_bytes):\n  if not size_in_bytes:\n    return \"?? GiB\"\n\n  size_in_bytes = float(size_in_bytes)\n  for (name, size_bytes) in _NAME_LIST:\n    value = size_in_bytes / size_bytes\n    if value >= 1.0:\n      return \"{:.2f} {}\".format(value, name)\n  return \"{} {}\".format(int(size_in_bytes), \"bytes\")", "response": "Returns a human - readable size string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a progression bar for the current download.", "response": "def tqdm(self):\n    \"\"\"Add a progression bar for the current download.\"\"\"\n    async_tqdm = utils.async_tqdm\n    with async_tqdm(total=0, desc='Dl Completed...', unit=' url') as pbar_url:\n      with async_tqdm(total=0, desc='Dl Size...', unit=' MiB') as pbar_dl_size:\n        self._pbar_url = pbar_url\n        self._pbar_dl_size = pbar_dl_size\n        yield"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading url to given path.", "response": "def download(self, url, destination_path):\n    \"\"\"Download url to given path.\n\n    Returns Promise -> sha256 of downloaded file.\n\n    Args:\n      url: address of resource to download.\n      destination_path: `str`, path to directory where to download the resource.\n\n    Returns:\n      Promise obj -> (`str`, int): (downloaded object checksum, size in bytes).\n    \"\"\"\n    self._pbar_url.update_total(1)\n    future = self._executor.submit(self._sync_download, url, destination_path)\n    return promise.Promise.resolve(future)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _sync_kaggle_download(self, kaggle_url, destination_path):\n    kaggle_file = kaggle.KaggleFile.from_url(kaggle_url)\n    downloader = self.kaggle_downloader(kaggle_file.competition)\n    filepath = downloader.download_file(kaggle_file.filename, destination_path)\n\n    dl_size = tf.io.gfile.stat(filepath).length\n    checksum = self._checksumer()\n    with tf.io.gfile.GFile(filepath, 'rb') as f:\n      while True:\n        block = f.read(io.DEFAULT_BUFFER_SIZE)\n        if not block:\n          break\n        checksum.update(block)\n    return checksum.hexdigest(), dl_size", "response": "Download with Kaggle API."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_drive_url(self, url, session):\n    response = session.get(url, stream=True)\n    if response.status_code != 200:\n      raise DownloadError(\n          'Failed to get url %s. HTTP code: %d.' % (url, response.status_code))\n    for k, v in response.cookies.items():\n      if k.startswith('download_warning'):\n        return url + '&confirm=' + v  # v is the confirm token\n    # No token found, let's try with original URL:\n    return url", "response": "Returns url possibly with confirmation token."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sync_download(self, url, destination_path):\n    proxies = {\n        'http': os.environ.get('TFDS_HTTP_PROXY', None),\n        'https': os.environ.get('TFDS_HTTPS_PROXY', None),\n        'ftp': os.environ.get('TFDS_FTP_PROXY', None)\n    }\n    if kaggle.KaggleFile.is_kaggle_url(url):\n      if proxies['http']:\n        os.environ['KAGGLE_PROXY'] = proxies['http']\n      return self._sync_kaggle_download(url, destination_path)\n\n    try:\n      # If url is on a filesystem that gfile understands, use copy. Otherwise,\n      # use requests.\n      if not url.startswith('http'):\n        return self._sync_file_copy(url, destination_path)\n    except tf.errors.UnimplementedError:\n      pass\n\n    session = requests.Session()\n    session.proxies = proxies\n    if _DRIVE_URL.match(url):\n      url = self._get_drive_url(url, session)\n    use_urllib = url.startswith('ftp')\n    if use_urllib:\n      if proxies['ftp']:\n        proxy = urllib.request.ProxyHandler({'ftp': proxies['ftp']})\n        opener = urllib.request.build_opener(proxy)\n        urllib.request.install_opener(opener)   # pylint: disable=too-many-function-args\n      request = urllib.request.Request(url)\n      response = urllib.request.urlopen(request)\n    else:\n      response = session.get(url, stream=True)\n      if response.status_code != 200:\n        raise DownloadError('Failed to get url %s. HTTP code: %d.' %\n                            (url, response.status_code))\n    fname = _get_filename(response)\n    path = os.path.join(destination_path, fname)\n    size = 0\n\n    size_mb = 0\n    unit_mb = units.MiB\n    self._pbar_dl_size.update_total(\n        int(response.headers.get('Content-length', 0)) // unit_mb)\n    with tf.io.gfile.GFile(path, 'wb') as file_:\n      checksum = self._checksumer()\n      if use_urllib:\n        iterator = iter(lambda: response.read(io.DEFAULT_BUFFER_SIZE), b'')\n      else:\n        iterator = response.iter_content(chunk_size=io.DEFAULT_BUFFER_SIZE)\n\n      for block in iterator:\n        size += len(block)\n\n        # Update the progress bar\n        size_mb += len(block)\n        if size_mb > unit_mb:\n          self._pbar_dl_size.update(size_mb // unit_mb)\n          size_mb %= unit_mb\n\n        checksum.update(block)\n        file_.write(block)\n    self._pbar_url.update(1)\n    return checksum.hexdigest(), size", "response": "Synchronous version of download method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _resize_image_if_necessary(image_fobj, target_pixels=None):\n  if target_pixels is None:\n    return image_fobj\n\n  cv2 = tfds.core.lazy_imports.cv2\n  # Decode image using OpenCV2.\n  image = cv2.imdecode(\n      np.fromstring(image_fobj.read(), dtype=np.uint8), flags=3)\n  # Get image height and width.\n  height, width, _ = image.shape\n  actual_pixels = height * width\n  if actual_pixels > target_pixels:\n    factor = np.sqrt(target_pixels / actual_pixels)\n    image = cv2.resize(image, dsize=None, fx=factor, fy=factor)\n  # Encode the image with quality=72 and store it in a BytesIO object.\n  _, buff = cv2.imencode(\".jpg\", image, [int(cv2.IMWRITE_JPEG_QUALITY), 72])\n  return io.BytesIO(buff.tostring())", "response": "Resizes an image to have the given number of target pixels."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_examples(self, images_dir_path, csv_path=None, csv_usage=None):\n    if csv_path:\n      with tf.io.gfile.GFile(csv_path) as csv_f:\n        reader = csv.DictReader(csv_f)\n        data = [(row[\"image\"], int(row[\"level\"]))\n                for row in reader\n                if csv_usage is None or row[\"Usage\"] == csv_usage]\n    else:\n      data = [(fname[:-5], -1)\n              for fname in tf.io.gfile.listdir(images_dir_path)\n              if fname.endswith(\".jpeg\")]\n    for name, label in data:\n      yield {\n          \"name\": name,\n          \"image\": _resize_image_if_necessary(\n              tf.io.gfile.GFile(\"%s/%s.jpeg\" % (images_dir_path, name),\n                                mode=\"rb\"),\n              target_pixels=self.builder_config.target_pixels),\n          \"label\": label,\n      }", "response": "Yields Example instances from given csv file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _slice_split_info_to_instruction_dicts(self, list_sliced_split_info):\n    instruction_dicts = []\n    for sliced_split_info in list_sliced_split_info:\n      mask = splits_lib.slice_to_percent_mask(sliced_split_info.slice_value)\n\n      # Compute filenames from the given split\n      filepaths = list(sorted(self._build_split_filenames(\n          split_info_list=[sliced_split_info.split_info],\n      )))\n\n      # Compute the offsets\n      if sliced_split_info.split_info.num_examples:\n        shard_id2num_examples = splits_lib.get_shard_id2num_examples(\n            sliced_split_info.split_info.num_shards,\n            sliced_split_info.split_info.num_examples,\n        )\n        mask_offsets = splits_lib.compute_mask_offsets(shard_id2num_examples)\n      else:\n        logging.warning(\n            \"Statistics not present in the dataset. TFDS is not able to load \"\n            \"the total number of examples, so using the subsplit API may not \"\n            \"provide precise subsplits.\"\n        )\n        mask_offsets = [0] * len(filepaths)\n\n      for filepath, mask_offset in zip(filepaths, mask_offsets):\n        instruction_dicts.append({\n            \"filepath\": filepath,\n            \"mask\": mask,\n            \"mask_offset\": mask_offset,\n        })\n    return instruction_dicts", "response": "Return the list of files and reading mask of the files to read."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs the split filenames associated with the split info.", "response": "def _build_split_filenames(self, split_info_list):\n    \"\"\"Construct the split filenames associated with the split info.\n\n    The filenames correspond to the pre-processed datasets files present in\n    the root directory of the dataset.\n\n    Args:\n      split_info_list: (list[SplitInfo]) List of split from which generate the\n        filenames\n\n    Returns:\n      filenames: (list[str]) The list of filenames path corresponding to the\n        split info object\n    \"\"\"\n\n    filenames = []\n    for split_info in split_info_list:\n      filenames.extend(naming.filepaths_for_dataset_split(\n          dataset_name=self.name,\n          split=split_info.name,\n          num_shards=split_info.num_shards,\n          data_dir=self._data_dir,\n          filetype_suffix=self._file_format_adapter.filetype_suffix,\n      ))\n    return filenames"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_examples(self, data_path):\n    with tf.io.gfile.GFile(data_path, \"rb\") as fp:\n      images = np.load(fp)\n    images = np.transpose(images, (1, 0, 2, 3))\n    images = np.expand_dims(images, axis=-1)\n    for sequence in images:\n      yield dict(image_sequence=sequence)", "response": "Generate MovingMnist sequences.\n\n    Args:\n      data_path (str): Path to the data file\n\n    Yields:\n      20 x 64 x 64 x 1 uint8 numpy arrays"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a single video from the input tfrecords.", "response": "def _parse_single_video(self, example_proto):\n    \"\"\"Parses single video from the input tfrecords.\n\n    Args:\n      example_proto: tfExample proto with a single video.\n\n    Returns:\n      dict with all frames, positions and actions.\n    \"\"\"\n    context_features = {\n        \"game_duration_loops\": tf.io.FixedLenFeature([1], tf.int64),\n        \"game_duration_seconds\": tf.io.FixedLenFeature([1], tf.float32),\n        \"n_steps\": tf.io.FixedLenFeature([1], tf.int64),\n        \"screen_size\": tf.io.FixedLenFeature([2], tf.int64),\n    }\n\n    sequence_features = {\n        \"rgb_screen\": tf.io.FixedLenSequenceFeature([], tf.string),\n    }\n\n    _, seq_feat = tf.io.parse_single_sequence_example(\n        example_proto,\n        context_features=context_features,\n        sequence_features=sequence_features)\n\n    video_frames = tf.map_fn(\n        tf.image.decode_png, seq_feat[\"rgb_screen\"], dtype=tf.uint8)\n    return video_frames"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate_examples(self, filepath):\n    # Simultaneously iterating through the different data sets in the hdf5\n    # file is >100x slower and the data set is small (26.7MB). Hence, we first\n    # load everything into memory before yielding the samples.\n    image_array, class_array, values_array = _load_data(filepath)\n    for image, classes, values in moves.zip(image_array, class_array,\n                                            values_array):\n      yield dict(\n          image=np.expand_dims(image, -1),\n          label_shape=classes[1],\n          label_scale=classes[2],\n          label_orientation=classes[3],\n          label_x_position=classes[4],\n          label_y_position=classes[5],\n          value_shape=values[1],\n          value_scale=values[2],\n          value_orientation=values[3],\n          value_x_position=values[4],\n          value_y_position=values[5])", "response": "Generates examples for the dSprites data set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn splits for the current node.", "response": "def _split_generators(self, dl_manager):\n    \"\"\"Returns splits.\"\"\"\n    # Download images and annotations that come in separate archives.\n    # Note, that the extension of archives is .tar.gz even though the actual\n    # archives format is uncompressed tar.\n    dl_paths = dl_manager.download_and_extract({\n        \"images\": tfds.download.Resource(\n            url=os.path.join(_BASE_URL, \"images.tar.gz\"),\n            extract_method=tfds.download.ExtractMethod.TAR),\n        \"annotations\": tfds.download.Resource(\n            url=os.path.join(_BASE_URL, \"annotations.tar.gz\"),\n            extract_method=tfds.download.ExtractMethod.TAR)\n    })\n\n    images_path_dir = os.path.join(dl_paths[\"images\"], \"images\")\n    annotations_path_dir = os.path.join(dl_paths[\"annotations\"], \"annotations\")\n\n    # Setup train and test splits\n    train_split = tfds.core.SplitGenerator(\n        name=\"train\",\n        num_shards=_NUM_SHARDS,\n        gen_kwargs={\n            \"images_dir_path\": images_path_dir,\n            \"images_list_file\": os.path.join(annotations_path_dir,\n                                             \"trainval.txt\"),\n            },\n        )\n    test_split = tfds.core.SplitGenerator(\n        name=\"test\",\n        num_shards=_NUM_SHARDS,\n        gen_kwargs={\n            \"images_dir_path\": images_path_dir,\n            \"images_list_file\": os.path.join(annotations_path_dir,\n                                             \"test.txt\")\n            },\n        )\n\n    return [train_split, test_split]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload objects from given csv files.", "response": "def _load_objects(csv_paths, csv_positions, prefix):\n  \"\"\"Returns objects listed within given CSV files.\"\"\"\n  logging.info('Loading CSVs %s from positions %s with prefix %s',\n               csv_paths, csv_positions, prefix)\n  objects = collections.defaultdict(list)\n  for i, labels_path in enumerate(csv_paths):\n    with tf.io.gfile.GFile(labels_path) as csv_f:\n      if csv_positions[i] > 0:\n        csv_f.seek(csv_positions[i])\n      else:\n        csv_f.readline()  # Drop headers\n      reader = csv.reader(csv_f)\n      for image_id, source, label, confidence in reader:\n        if prefix and image_id[0] != prefix:\n          break\n        csv_positions[i] = csv_f.tell()\n        image_id = int(image_id, 16)\n        current_obj = _Object(label, int(float(confidence) * 10), source)\n        objects[image_id].append(current_obj)\n  return dict(objects)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a list of bounded boxes from a given csv file.", "response": "def _load_bboxes(csv_path, csv_positions, prefix):\n  \"\"\"Returns bounded boxes listed within given CSV file.\"\"\"\n  logging.info('Loading CSVs %s from positions %s with prefix %s',\n               csv_path, csv_positions, prefix)\n  boxes = collections.defaultdict(list)\n  with tf.io.gfile.GFile(csv_path) as csv_f:\n    if csv_positions[0] > 0:\n      csv_f.seek(csv_positions[0])\n    else:\n      csv_f.readline()  # Drop headers\n    reader = csv.reader(csv_f)\n    for (image_id, source, label, confidence, xmin, xmax, ymin, ymax,\n         is_occluded, is_truncated, is_group_of, is_depiction, is_inside,\n        ) in reader:\n      if prefix and image_id[0] != prefix:\n        break\n      csv_positions[0] = csv_f.tell()\n      image_id = int(image_id, 16)\n      del confidence  # always 1 in bounding boxes.\n      current_row = _Bbox(\n          label, source, tfds.features.BBox(\n              float(ymin), float(xmin), float(ymax), float(xmax)),\n          int(is_occluded), int(is_truncated),\n          int(is_group_of), int(is_depiction), int(is_inside))\n      boxes[image_id].append(current_row)\n  return dict(boxes)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _split_generators(self, dl_manager):\n    paths = dl_manager.download_and_extract(_URLS)\n    # Load labels from CSVs:\n    def load(names):\n      csv_positions = [0] * len(names)\n      return functools.partial(_load_objects, [paths[name] for name in names],\n                               csv_positions)\n    train_objects = load(['train_human_labels', 'train_machine_labels'])\n    test_objects = load(['test_human_labels', 'test_machine_labels'])\n    validation_objects = load(['validation_human_labels',\n                               'validation_machine_labels'])\n    def load_boxes(name):\n      csv_positions = [0]\n      return functools.partial(_load_bboxes, paths[name], csv_positions)\n    train_bbox = load_boxes('train-annotations-bbox')\n    test_bbox = load_boxes('test-annotations-bbox')\n    validation_bbox = load_boxes('validation-annotations-bbox')\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=512,\n            gen_kwargs=dict(archive_paths=paths['train_images'],\n                            objects_getter=train_objects,\n                            bboxes_getter=train_bbox,\n                            prefixes='0123456789abcdef'),\n        ),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=36,\n            gen_kwargs=dict(archive_paths=[paths['test_images']],\n                            objects_getter=test_objects,\n                            bboxes_getter=test_bbox),\n        ),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.VALIDATION,\n            num_shards=12,\n            gen_kwargs=dict(archive_paths=[paths['validation_images']],\n                            objects_getter=validation_objects,\n                            bboxes_getter=validation_bbox),\n        ),\n    ]", "response": "Returns a list of SplitGenerators."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_url_hashes(path):\n  urls = _read_text_file(path)\n  def url_hash(u):\n    h = hashlib.sha1()\n    try:\n      u = u.encode('utf-8')\n    except UnicodeDecodeError:\n      logging.error('Cannot hash url: %s', u)\n    h.update(u)\n    return h.hexdigest()\n  return {url_hash(u): True for u in urls}", "response": "Get hashes of urls in file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding files corresponding to urls.", "response": "def _find_files(dl_paths, publisher, url_dict):\n  \"\"\"Find files corresponding to urls.\"\"\"\n  if publisher == 'cnn':\n    top_dir = os.path.join(dl_paths['cnn_stories'], 'cnn', 'stories')\n  elif publisher == 'dm':\n    top_dir = os.path.join(dl_paths['dm_stories'], 'dailymail', 'stories')\n  else:\n    logging.fatal('Unsupported publisher: %s', publisher)\n  files = tf.io.gfile.listdir(top_dir)\n\n  ret_files = []\n  for p in files:\n    basename = os.path.basename(p)\n    if basename[0:basename.find('.story')] in url_dict:\n      ret_files.append(os.path.join(top_dir, p))\n  return ret_files"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget filenames for a particular split.", "response": "def _subset_filenames(dl_paths, split):\n  \"\"\"Get filenames for a particular split.\"\"\"\n  assert isinstance(dl_paths, dict), dl_paths\n  # Get filenames for a split.\n  if split == tfds.Split.TRAIN:\n    urls = _get_url_hashes(dl_paths['train_urls'])\n  elif split == tfds.Split.VALIDATION:\n    urls = _get_url_hashes(dl_paths['val_urls'])\n  elif split == tfds.Split.TEST:\n    urls = _get_url_hashes(dl_paths['test_urls'])\n  else:\n    logging.fatal('Unsupported split: %s', split)\n  cnn = _find_files(dl_paths, 'cnn', urls)\n  dm = _find_files(dl_paths, 'dm', urls)\n  return cnn + dm"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget abstract and article from a story file path.", "response": "def _get_art_abs(story_file):\n  \"\"\"Get abstract (highlights) and article from a story file path.\"\"\"\n  # Based on https://github.com/abisee/cnn-dailymail/blob/master/\n  #     make_datafiles.py\n\n  lines = _read_text_file(story_file)\n\n  # Lowercase everything\n  lines = [line.lower() for line in lines]\n\n  # Put periods on the ends of lines that are missing them\n  # (this is a problem in the dataset because many image captions don't end in\n  # periods; consequently they end up in the body of the article as run-on\n  # sentences)\n  def fix_missing_period(line):\n    \"\"\"Adds a period to a line that is missing a period.\"\"\"\n    if '@highlight' in line: return line\n    if not line: return line\n    if line[-1] in END_TOKENS: return line\n    return line + ' .'\n  lines = [fix_missing_period(line) for line in lines]\n\n  # Separate out article and abstract sentences\n  article_lines = []\n  highlights = []\n  next_is_highlight = False\n  for line in lines:\n    if not line:\n      continue  # empty line\n    elif line.startswith('@highlight'):\n      next_is_highlight = True\n    elif next_is_highlight:\n      highlights.append(line)\n    else:\n      article_lines.append(line)\n\n  # Make article into a single string\n  article = ' '.join(article_lines)\n\n  # Make abstract into a single string, putting <s> and </s> tags around\n  # the sentences.\n  abstract = ' '.join(['%s %s %s' % (SENTENCE_START, sent,\n                                     SENTENCE_END) for sent in highlights])\n\n  return article, abstract"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting links from robots. txt and sitemap. xml.", "response": "def zap(input_url, archive, domain, host, internal, robots, proxies):\n    \"\"\"Extract links from robots.txt and sitemap.xml.\"\"\"\n    if archive:\n        print('%s Fetching URLs from archive.org' % run)\n        if False:\n            archived_urls = time_machine(domain, 'domain')\n        else:\n            archived_urls = time_machine(host, 'host')\n        print('%s Retrieved %i URLs from archive.org' % (\n            good, len(archived_urls) - 1))\n        for url in archived_urls:\n            verb('Internal page', url)\n            internal.add(url)\n    # Makes request to robots.txt\n    response = requests.get(input_url + '/robots.txt',\n                            proxies=random.choice(proxies)).text\n    # Making sure robots.txt isn't some fancy 404 page\n    if '<body' not in response:\n        # If you know it, you know it\n        matches = re.findall(r'Allow: (.*)|Disallow: (.*)', response)\n        if matches:\n            # Iterating over the matches, match is a tuple here\n            for match in matches:\n                # One item in match will always be empty so will combine both\n                # items\n                match = ''.join(match)\n                # If the URL doesn't use a wildcard\n                if '*' not in match:\n                    url = input_url + match\n                    # Add the URL to internal list for crawling\n                    internal.add(url)\n                    # Add the URL to robots list\n                    robots.add(url)\n            print('%s URLs retrieved from robots.txt: %s' % (good, len(robots)))\n    # Makes request to sitemap.xml\n    response = requests.get(input_url + '/sitemap.xml',\n                            proxies=random.choice(proxies)).text\n    # Making sure robots.txt isn't some fancy 404 page\n    if '<body' not in response:\n        matches = xml_parser(response)\n        if matches: # if there are any matches\n            print('%s URLs retrieved from sitemap.xml: %s' % (\n                good, len(matches)))\n            for match in matches:\n                verb('Internal page', match)\n                # Cleaning up the URL and adding it to the internal list for\n                # crawling\n                internal.add(match)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling the requests and return the response body.", "response": "def requester(\n        url,\n        main_url=None,\n        delay=0,\n        cook=None,\n        headers=None,\n        timeout=10,\n        host=None,\n        proxies=[None],\n        user_agents=[None],\n        failed=None,\n        processed=None\n    ):\n    \"\"\"Handle the requests and return the response body.\"\"\"\n    cook = cook or set()\n    headers = headers or set()\n    user_agents = user_agents or ['Photon']\n    failed = failed or set()\n    processed = processed or set()\n    # Mark the URL as crawled\n    processed.add(url)\n    # Pause/sleep the program for specified time\n    time.sleep(delay)\n\n    def make_request(url):\n        \"\"\"Default request\"\"\"\n        final_headers = headers or {\n            'Host': host,\n            # Selecting a random user-agent\n            'User-Agent': random.choice(user_agents),\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip',\n            'DNT': '1',\n            'Connection': 'close',\n        }\n        try:\n            response = SESSION.get(\n                url,\n                cookies=cook,\n                headers=final_headers,\n                verify=False,\n                timeout=timeout,\n                stream=True,\n                proxies=random.choice(proxies)\n            )\n        except TooManyRedirects:\n            return 'dummy'\n\n        if 'text/html' in response.headers['content-type'] or \\\n           'text/plain' in response.headers['content-type']:\n            if response.status_code != '404':\n                return response.text\n            else:\n                response.close()\n                failed.add(url)\n                return 'dummy'\n        else:\n            response.close()\n            return 'dummy'\n\n    return make_request(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts intel from the response body.", "response": "def intel_extractor(url, response):\r\n    \"\"\"Extract intel from the response body.\"\"\"\r\n    for rintel in rintels:\r\n        res = re.sub(r'<(script).*?</\\1>(?s)', '', response)\r\n        res = re.sub(r'<[^<]+?>', '', res)\r\n        matches = rintel[0].findall(res)\r\n        if matches:\r\n            for match in matches:\r\n                verb('Intel', match)\r\n                bad_intel.add((match, rintel[1], url))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef js_extractor(response):\r\n    # Extract .js files\r\n    matches = rscript.findall(response)\r\n    for match in matches:\r\n        match = match[2].replace('\\'', '').replace('\"', '')\r\n        verb('JS file', match)\r\n        bad_scripts.add(match)", "response": "Extract js files from the response body"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting details from the response body.", "response": "def extractor(url):\r\n    \"\"\"Extract details from the response body.\"\"\"\r\n    response = requester(url, main_url, delay, cook, headers, timeout, host, proxies, user_agents, failed, processed)\r\n    if clone:\r\n        mirror(url, response)\r\n    matches = rhref.findall(response)\r\n    for link in matches:\r\n        # Remove everything after a \"#\" to deal with in-page anchors\r\n        link = link[1].replace('\\'', '').replace('\"', '').split('#')[0]\r\n        # Checks if the URLs should be crawled\r\n        if is_link(link, processed, files):\r\n            if link[:4] == 'http':\r\n                if link.startswith(main_url):\r\n                    verb('Internal page', link)\r\n                    internal.add(link)\r\n                else:\r\n                    verb('External page', link)\r\n                    external.add(link)\r\n            elif link[:2] == '//':\r\n                if link.split('/')[2].startswith(host):\r\n                    verb('Internal page', link)\r\n                    internal.add(schema + '://' + link)\r\n                else:\r\n                    verb('External page', link)\r\n                    external.add(link)\r\n            elif link[:1] == '/':\r\n                verb('Internal page', link)\r\n                internal.add(remove_file(url) + link)\r\n            else:\r\n                verb('Internal page', link)\r\n                usable_url = remove_file(url)\r\n                if usable_url.endswith('/'):\r\n                    internal.add(usable_url + link)\r\n                elif link.startswith('/'):\r\n                    internal.add(usable_url + link)\r\n                else:\r\n                    internal.add(usable_url + '/' + link)\r\n\r\n    if not only_urls:\r\n        intel_extractor(url, response)\r\n        js_extractor(response)\r\n    if args.regex and not supress_regex:\r\n        regxy(args.regex, response, supress_regex, custom)\r\n    if api:\r\n        matches = rentropy.findall(response)\r\n        for match in matches:\r\n            if entropy(match) >= 4:\r\n                verb('Key', match)\r\n                keys.add(url + ': ' + match)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef jscanner(url):\r\n    response = requester(url, main_url, delay, cook, headers, timeout, host, proxies, user_agents, failed, processed)\r\n    # Extract URLs/endpoints\r\n    matches = rendpoint.findall(response)\r\n    # Iterate over the matches, match is a tuple\r\n    for match in matches:\r\n        # Combining the items because one of them is always empty\r\n        match = match[0] + match[1]\r\n        # Making sure it's not some JavaScript code\r\n        if not re.search(r'[}{><\"\\']', match) and not match == '/':\r\n            verb('JS endpoint', match)\r\n            endpoints.add(match)", "response": "Extract endpoints from JavaScript code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the current version of Photon.", "response": "def updater():\n    \"\"\"Update the current installation.\n\n    git clones the latest version and merges it with the current directory.\n    \"\"\"\n    print('%s Checking for updates' % run)\n    # Changes must be separated by ;\n    changes = '''major bug fixes;removed ninja mode;dropped python < 3.2 support;fixed unicode output;proxy support;more intels'''\n    latest_commit = requester('https://raw.githubusercontent.com/s0md3v/Photon/master/core/updater.py', host='raw.githubusercontent.com')\n    # Just a hack to see if a new version is available\n    if changes not in latest_commit:\n        changelog = re.search(r\"changes = '''(.*?)'''\", latest_commit)\n        # Splitting the changes to form a list\n        changelog = changelog.group(1).split(';')\n        print('%s A new version of Photon is available.' % good)\n        print('%s Changes:' % info)\n        for change in changelog: # print changes\n            print('%s>%s %s' % (green, end, change))\n\n        current_path = os.getcwd().split('/') # if you know it, you know it\n        folder = current_path[-1] # current directory name\n        path = '/'.join(current_path) # current directory path\n        choice = input('%s Would you like to update? [Y/n] ' % que).lower()\n\n        if choice != 'n':\n            print('%s Updating Photon' % run)\n            os.system('git clone --quiet https://github.com/s0md3v/Photon %s'\n                      % (folder))\n            os.system('cp -r %s/%s/* %s && rm -r %s/%s/ 2>/dev/null'\n                      % (path, folder, path, path, folder))\n            print('%s Update successful!' % good)\n    else:\n        print('%s Photon is up to date!' % good)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_subdomains(domain):\n    result = set()\n    response = get('https://findsubdomains.com/subdomains-of/' + domain).text\n    matches = findall(r'(?s)<div class=\"domains js-domain-name\">(.*?)</div>', response)\n    for match in matches:\n        result.add(match.replace(' ', '').replace('\\n', ''))\n    return list(result)", "response": "Find subdomains according to the TLD."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing the URLs and uses a threadpool to execute a function.", "response": "def flash(function, links, thread_count):\n    \"\"\"Process the URLs and uses a threadpool to execute a function.\"\"\"\n    # Convert links (set) to list\n    links = list(links)\n    threadpool = concurrent.futures.ThreadPoolExecutor(\n            max_workers=thread_count)\n    futures = (threadpool.submit(function, link) for link in links)\n    for i, _ in enumerate(concurrent.futures.as_completed(futures)):\n        if i + 1 == len(links) or (i + 1) % thread_count == 0:\n            print('%s Progress: %i/%i' % (info, i + 1, len(links)),\n                    end='\\r')\n    print('')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef regxy(pattern, response, supress_regex, custom):\n    try:\n        matches = re.findall(r'%s' % pattern, response)\n        for match in matches:\n            verb('Custom regex', match)\n            custom.add(match)\n    except:\n        supress_regex = True", "response": "Extract a string based on regex pattern supplied by user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_link(url, processed, files):\n    if url not in processed:\n        is_file = url.endswith(BAD_TYPES)\n        if is_file:\n            files.add(url)\n            return False\n        return True\n    return False", "response": "Determines whether or not a link should be crawled"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_regex(urls, regex):\n\n    if not regex:\n        return urls\n\n    # To avoid iterating over the characters of a string\n    if not isinstance(urls, (list, set, tuple)):\n        urls = [urls]\n\n    try:\n        non_matching_urls = [url for url in urls if not re.search(regex, url)]\n    except TypeError:\n        return []\n\n    return non_matching_urls", "response": "Parse a list of urls for non - matches to a regex."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timer(diff, processed):\n    # Changes seconds into minutes and seconds\n    minutes, seconds = divmod(diff, 60)\n    try:\n        # Finds average time taken by requests\n        time_per_request = diff / float(len(processed))\n    except ZeroDivisionError:\n        time_per_request = 0\n    return minutes, seconds, time_per_request", "response": "Return the passed time."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the entropy of a string.", "response": "def entropy(string):\n    \"\"\"Calculate the entropy of a string.\"\"\"\n    entropy = 0\n    for number in range(256):\n        result = float(string.encode('utf-8').count(\n            chr(number))) / len(string.encode('utf-8'))\n        if result != 0:\n            entropy = entropy - result * math.log(result, 2)\n    return entropy"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef top_level(url, fix_protocol=True):\n    ext = tld.get_tld(url, fix_protocol=fix_protocol)\n    toplevel = '.'.join(urlparse(url).netloc.split('.')[-2:]).split(\n        ext)[0] + ext\n    return toplevel", "response": "Extract the top level domain from an URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmatching IP or DOMAIN or PORT format", "response": "def proxy_type(v):\n    \"\"\" Match IP:PORT or DOMAIN:PORT in a losse manner \"\"\"\n    proxies = []\n    if re.match(r\"((http|socks5):\\/\\/.)?(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}):(\\d{1,5})\", v):\n        proxies.append({\"http\": v,\n                        \"https\": v})\n        return proxies\n    elif re.match(r\"((http|socks5):\\/\\/.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}:(\\d{1,5})\", v):\n        proxies.append({\"http\": v,\n                        \"https\": v})\n        return proxies\n    elif is_proxy_list(v, proxies):\n        return proxies\n    else:\n        raise argparse.ArgumentTypeError(\n            \"Proxy should follow IP:PORT or DOMAIN:PORT format\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prompt(default=None):\n    editor = 'nano'\n    with tempfile.NamedTemporaryFile(mode='r+') as tmpfile:\n        if default:\n            tmpfile.write(default)\n            tmpfile.flush()\n\n        child_pid = os.fork()\n        is_child = child_pid == 0\n\n        if is_child:\n            os.execvp(editor, [editor, tmpfile.name])\n        else:\n            os.waitpid(child_pid, 0)\n            tmpfile.seek(0)\n            return tmpfile.read().strip()", "response": "Presents the user a prompt."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_market(self):\n        # \u542f\u52a8 trade_engine \u7ebf\u7a0b\n        self.market.start()\n\n        # \u6ce8\u518c backtest_broker \uff0c\u5e76\u4e14\u542f\u52a8\u548c\u5b83\u5173\u8054\u7ebf\u7a0bQAThread \u5b58\u653e\u5728 kernels \u8bcd\u5178\u4e2d\uff0c { 'broker_name': QAThread }\n        #self.market.register(self.broker_name, self.broker)\n        self.market.connect(self.broker_name)", "response": "start the market thread and register backtest broker"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef message(self):\n        'the standard message which can be transfer'\n        return {\n            'source':\n            'account',\n            'frequence':\n            self.frequence,\n            'account_cookie':\n            self.account_cookie,\n            'portfolio_cookie':\n            self.portfolio_cookie,\n            'user_cookie':\n            self.user_cookie,\n            'broker':\n            self.broker,\n            'market_type':\n            self.market_type,\n            'strategy_name':\n            self.strategy_name,\n            'current_time':\n            str(self._currenttime),\n            'allow_sellopen':\n            self.allow_sellopen,\n            'allow_margin':\n            self.allow_margin,\n            'allow_t0':\n            self.allow_t0,\n            'margin_level':\n            self.margin_level,\n            'init_assets':\n            self.init_assets,\n            'init_cash':\n            self.init_cash,\n            'init_hold':\n            self.init_hold.to_dict(),\n            'commission_coeff':\n            self.commission_coeff,\n            'tax_coeff':\n            self.tax_coeff,\n            'cash':\n            self.cash,\n            'history':\n            self.history,\n            'trade_index':\n            self.time_index_max,\n            'running_time':\n            str(datetime.datetime.now())\n            if self.running_time is None else str(self.running_time),\n            'quantaxis_version':\n            self.quantaxis_version,\n            'running_environment':\n            self.running_environment,\n            'start_date':\n            self.start_date,\n            'end_date':\n            self.end_date,\n            'frozen':\n            self.frozen,\n            'finished_id':\n            self.finishedOrderid\n        }", "response": "the standard message which can be transfer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the hold with the account cookie", "response": "def init_hold_with_account(self):\n        \"\"\"\u5e26account_cookie\u7684\u521d\u59cb\u5316\u6301\u4ed3\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        return self.init_hold.reset_index().assign(\n            account_cookie=self.account_cookie\n        ).set_index(['code',\n                     'account_cookie'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the start date of the current user.", "response": "def start_date(self):\n        \"\"\"\u8d26\u6237\u7684\u8d77\u59cb\u4ea4\u6613\u65e5\u671f(\u53ea\u5728\u56de\u6d4b\u4e2d\u4f7f\u7528)\n\n        Raises:\n            RuntimeWarning -- [description]\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n        if self.start_==None:\n            if len(self.time_index_max) > 0:\n                return str(min(self.time_index_max))[0:10]\n            else:\n                print(\n                    RuntimeWarning(\n                        'QAACCOUNT: THIS ACCOUNT DOESNOT HAVE ANY TRADE'\n                    )\n                )\n        else:\n            return self.start_"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the end date of the current date", "response": "def end_date(self):\n        \"\"\"\u8d26\u6237\u7684\u4ea4\u6613\u7ed3\u675f\u65e5\u671f(\u53ea\u5728\u56de\u6d4b\u4e2d\u4f7f\u7528)\n\n        Raises:\n            RuntimeWarning -- [description]\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n        if self.start_==None:\n            if len(self.time_index_max) > 0:\n                return str(max(self.time_index_max))[0:10]\n            else:\n                print(\n                    RuntimeWarning(\n                        'QAACCOUNT: THIS ACCOUNT DOESNOT HAVE ANY TRADE'\n                    )\n                )\n        else:\n            return self.end_"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a DataFrame with the history of the current user.", "response": "def history_table(self):\n        '\u4ea4\u6613\u5386\u53f2\u7684table'\n        if len(self.history) > 0:\n            lens = len(self.history[0])\n        else:\n            lens = len(self._history_headers)\n\n        return pd.DataFrame(\n            data=self.history,\n            columns=self._history_headers[:lens]\n        ).sort_index()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a pandas. DataFrame containing the cash information.", "response": "def cash_table(self):\n        '\u73b0\u91d1\u7684table'\n        _cash = pd.DataFrame(\n            data=[self.cash[1::],\n                  self.time_index_max],\n            index=['cash',\n                   'datetime']\n        ).T\n        _cash = _cash.assign(\n            date=_cash.datetime.apply(lambda x: pd.to_datetime(str(x)[0:10]))\n        ).assign(account_cookie=self.account_cookie)                          # .sort_values('datetime')\n        return _cash.set_index(['datetime', 'account_cookie'], drop=False)\n        \"\"\"\n        \u5b9e\u9a8c\u6027\u8d28\n        @2018-06-09\n\n        # \u5bf9\u4e8e\u8d26\u6237\u6301\u4ed3\u7684\u5206\u89e3\n\n        1. \u771f\u5b9e\u6301\u4ed3hold:\n\n        \u6b63\u5e38\u6a21\u5f0f/TZero\u6a21\u5f0f:\n            hold = \u5386\u53f2\u6301\u4ed3(init_hold)+ \u521d\u59cb\u5316\u8d26\u6237\u540e\u53d1\u751f\u7684\u6240\u6709\u4ea4\u6613\u5bfc\u81f4\u7684\u6301\u4ed3(hold_available)\n\n        \u52a8\u6001\u6301\u4ed3(\u521d\u59cb\u5316\u8d26\u6237\u540e\u7684\u6301\u4ed3)hold_available:\n            self.history \u8ba1\u7b97\u800c\u5f97\n\n        2. \u8d26\u6237\u7684\u53ef\u5356\u989d\u5ea6(sell_available)\n\n        \u6b63\u5e38\u6a21\u5f0f:\n            sell_available\n                \u7ed3\u7b97\u524d: init_hold+ \u4e70\u5356\u4ea4\u6613(\u5356-)\n                \u7ed3\u7b97\u540e: init_hold+ \u4e70\u5356\u4ea4\u6613(\u4e70+ \u5356-)\n        TZero\u6a21\u5f0f:\n            sell_available\n                \u7ed3\u7b97\u524d: init_hold - \u4e70\u5356\u4ea4\u6613\u5360\u7528\u7684\u989d\u5ea6(abs(\u4e70+ \u5356-))\n                \u7ed3\u7b97\u8fc7\u7a0b \u662f\u4e3a\u4e86\u8865\u5e73(\u7b49\u4e8e\u8ba9hold={})\n                \u7ed3\u7b97\u540e: init_hold\n        \"\"\""}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hold_available(self):\n        return self.history_table.groupby('code').amount.sum().replace(\n            0,\n            np.nan\n        ).dropna().sort_index()", "response": "Return a list of hold available entries."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef trade(self):\n\n        return self.history_table.pivot_table(\n            index=['datetime',\n                   'account_cookie'],\n            columns='code',\n            values='amount',\n            aggfunc=np.sum\n        ).fillna(0).sort_index()", "response": "Returns a DataFrame with the amount of the katis."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hold_table(self, datetime=None):\n        \"\u5230\u67d0\u4e00\u4e2a\u65f6\u523b\u7684\u6301\u4ed3 \u5982\u679c\u7ed9\u7684\u662f\u65e5\u671f,\u5219\u8fd4\u56de\u5f53\u65e5\u5f00\u76d8\u524d\u7684\u6301\u4ed3\"\n        if datetime is None:\n            hold_available = self.history_table.set_index(\n                'datetime'\n            ).sort_index().groupby('code').amount.sum().sort_index()\n        else:\n            hold_available = self.history_table.set_index(\n                'datetime'\n            ).sort_index().loc[:datetime].groupby('code'\n                                                 ).amount.sum().sort_index()\n\n        return pd.concat([self.init_hold,\n                          hold_available]).groupby('code').sum().sort_index(\n                          ).apply(lambda x: x if x > 0 else None).dropna()", "response": "\u5230\u67d0\u4e00\u4e2a\u65f6\u523b\u7684\u6301\u4ed3 \u5982\u679c\u7ed9\u7684\u6301\u4ed3 \u5982\u679c\u7ed9\u7684\u662f\u65e5\u671f \u5219\u8fd4\u56de\u5f53\ufffd"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef current_hold_price(self):\n        \n        def weights(x):\n            n=len(x)\n            res=1\n            while res>0 or res<0:\n                res=sum(x[:n]['amount'])\n                n=n-1\n            \n            x=x[n+1:]     \n            \n            if sum(x['amount']) != 0:\n                return np.average(\n                    x['price'],\n                    weights=x['amount'],\n                    returned=True\n                )\n            else:\n                return np.nan\n        return self.history_table.set_index(\n            'datetime',\n            drop=False\n        ).sort_index().groupby('code').apply(weights).dropna()", "response": "Returns the current hold price in the history table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hold_price(self, datetime=None):\n\n        def weights(x):\n            if sum(x['amount']) != 0:\n                return np.average(\n                    x['price'],\n                    weights=x['amount'],\n                    returned=True\n                )\n            else:\n                return np.nan\n\n        if datetime is None:\n            return self.history_table.set_index(\n                'datetime',\n                drop=False\n            ).sort_index().groupby('code').apply(weights).dropna()\n        else:\n            return self.history_table.set_index(\n                'datetime',\n                drop=False\n            ).sort_index().loc[:datetime].groupby('code').apply(weights\n                                                               ).dropna()", "response": "Hold price of a specific date."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hold_time(self, datetime=None):\n\n        def weights(x):\n            if sum(x['amount']) != 0:\n                return pd.Timestamp(self.datetime\n                                   ) - pd.to_datetime(x.datetime.max())\n            else:\n                return np.nan\n\n        if datetime is None:\n            return self.history_table.set_index(\n                'datetime',\n                drop=False\n            ).sort_index().groupby('code').apply(weights).dropna()\n        else:\n            return self.history_table.set_index(\n                'datetime',\n                drop=False\n            ).sort_index().loc[:datetime].groupby('code').apply(weights\n                                                               ).dropna()", "response": "Return a table of hold time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting assets to default values", "response": "def reset_assets(self, init_cash=None):\n        'reset_history/cash/'\n        self.sell_available = copy.deepcopy(self.init_hold)\n        self.history = []\n        self.init_cash = init_cash\n        self.cash = [self.init_cash]\n        self.cash_available = self.cash[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef receive_simpledeal(\n            self,\n            code,\n            trade_price,\n            trade_amount,\n            trade_towards,\n            trade_time,\n            message=None,\n            order_id=None,\n            trade_id=None,\n            realorder_id=None\n    ):\n        \"\"\"\u5feb\u901f\u64ae\u5408\u6210\u4ea4\u63a5\u53e3\n\n\n        \u6b64\u63a5\u53e3\u662f\u4e00\u4e2a\u76f4\u63a5\u53ef\u4ee5\u6210\u4ea4\u7684\u63a5\u53e3, \u6240\u4ee5\u52a1\u5fc5\u786e\u4fdd\u7ed9\u51fa\u7684\u4fe1\u606f\u662f\u53ef\u4ee5\u6210\u4ea4\u7684\n\n        \u6b64\u63a5\u53e3\u6d89\u53ca\u7684\u662f\n        1. \u80a1\u7968/\u671f\u8d27\u7684\u6210\u4ea4\n        2. \u5386\u53f2\u8bb0\u5f55\u7684\u589e\u52a0\n        3. \u73b0\u91d1/\u6301\u4ed3/\u51bb\u7ed3\u8d44\u91d1\u7684\u5904\u7406\n\n        Arguments:\n            code {[type]} -- [description]\n            trade_price {[type]} -- [description]\n            trade_amount {[type]} -- [description]\n            trade_towards {[type]} -- [description]\n            trade_time {[type]} -- [description]\n\n        Keyword Arguments:\n            message {[type]} -- [description] (default: {None})\n\n\n        2018/11/7 @yutiansut\n        \u4fee\u590d\u4e00\u4e2abug: \u5728\u76f4\u63a5\u4f7f\u7528\u8be5\u5feb\u901f\u64ae\u5408\u63a5\u53e3\u7684\u65f6\u5019, \u671f\u8d27\u5356\u51fa\u4f1a\u6263\u51cf\u4fdd\u8bc1\u91d1, \u4e70\u56de\u6765\u7684\u65f6\u5019\u5e94\u8be5\u53cd\u7b97\u5229\u6da6\n\n        \u5982 3800\u5356\u7a7a 3700\u4e70\u56de\u5e73\u4ed3  \u5e94\u4e3a100\u5229\u6da6\n        @2018-12-31 \u4fdd\u8bc1\u91d1\u8d26\u6237ok\n\n\n        @2019/1/3 \u4e00\u4e9b\u91cd\u8981\u7684\u610f\u601d\n        frozen = self.market_preset.get_frozen(code) # \u4fdd\u8bc1\u91d1\u7387\n        unit = self.market_preset.get_unit(code)  # \u5408\u7ea6\u4e58\u6570\n        raw_trade_money = trade_price*trade_amount*market_towards  # \u603b\u5e02\u503c\n        value = raw_trade_money * unit  # \u5408\u7ea6\u603b\u4ef7\u503c\n        trade_money = value * frozen    # \u4ea4\u6613\u4fdd\u8bc1\u91d1\n        \"\"\"\n\n        self.datetime = trade_time\n        if realorder_id in self.finishedOrderid:\n            pass\n        else:\n            self.finishedOrderid.append(realorder_id)\n            \n        market_towards = 1 if trade_towards > 0 else -1\n        # value \u5408\u7ea6\u4ef7\u503c unit \u5408\u7ea6\u4e58\u6570\n        if self.allow_margin:\n            frozen = self.market_preset.get_frozen(code)                  # \u4fdd\u8bc1\u91d1\u7387\n            unit = self.market_preset.get_unit(code)                      # \u5408\u7ea6\u4e58\u6570\n            raw_trade_money = trade_price * trade_amount * market_towards # \u603b\u5e02\u503c\n            value = raw_trade_money * unit                                # \u5408\u7ea6\u603b\u4ef7\u503c\n            trade_money = value * frozen                                  # \u4ea4\u6613\u4fdd\u8bc1\u91d1\n        else:\n            trade_money = trade_price * trade_amount * market_towards\n            raw_trade_money = trade_money\n            value = trade_money\n            unit = 1\n            frozen = 1\n                                                                          # \u8ba1\u7b97\u8d39\u7528\n                                                                          # trade_price\n\n        if self.market_type == MARKET_TYPE.FUTURE_CN:\n            # \u671f\u8d27\u4e0d\u6536\u7a0e\n            # \u53cc\u8fb9\u624b\u7eed\u8d39 \u4e5f\u6ca1\u6709\u6700\u5c0f\u624b\u7eed\u8d39\u9650\u5236\n\n            commission_fee_preset = self.market_preset.get_code(code)\n            if trade_towards in [ORDER_DIRECTION.BUY_OPEN,\n                                 ORDER_DIRECTION.BUY_CLOSE,\n                                 ORDER_DIRECTION.SELL_CLOSE,\n                                 ORDER_DIRECTION.SELL_OPEN]:\n                commission_fee = commission_fee_preset['commission_coeff_pervol'] * trade_amount + \\\n                    commission_fee_preset['commission_coeff_peramount'] * \\\n                    abs(value)\n            elif trade_towards in [ORDER_DIRECTION.BUY_CLOSETODAY,\n                                   ORDER_DIRECTION.SELL_CLOSETODAY]:\n                commission_fee = commission_fee_preset['commission_coeff_today_pervol'] * trade_amount + \\\n                    commission_fee_preset['commission_coeff_today_peramount'] * \\\n                    abs(value)\n\n            tax_fee = 0 # \u4e70\u5165\u4e0d\u6536\u5370\u82b1\u7a0e\n        elif self.market_type == MARKET_TYPE.STOCK_CN:\n\n            commission_fee = self.commission_coeff * \\\n                abs(trade_money)\n\n            commission_fee = 5 if commission_fee < 5 else commission_fee\n            if int(trade_towards) > 0:\n                tax_fee = 0 # \u4e70\u5165\u4e0d\u6536\u5370\u82b1\u7a0e\n            else:\n                tax_fee = self.tax_coeff * abs(trade_money)\n\n        # \u7ed3\u7b97\u4ea4\u6613\n        if self.cash[-1] > trade_money + commission_fee + tax_fee:\n            self.time_index_max.append(trade_time)\n            # TODO: \u76ee\u524d\u8fd8\u4e0d\u652f\u6301\u671f\u8d27\u7684\u9501\u4ed3\n            if self.allow_sellopen:\n                if trade_towards in [ORDER_DIRECTION.BUY_OPEN,\n                                     ORDER_DIRECTION.SELL_OPEN]:\n                    # \u5f00\u4ed3\u5355\u5360\u7528\u73b0\u91d1 \u8ba1\u7b97avg\n                    # \u521d\u59cb\u5316\n                    if code in self.frozen.keys():\n                        if trade_towards in self.frozen[code].keys():\n                            pass\n                        else:\n                            self.frozen[code][str(trade_towards)] = {\n                                'money': 0,\n                                'amount': 0,\n                                'avg_price': 0\n                            }\n                    else:\n                        self.frozen[code] = {\n                            str(ORDER_DIRECTION.BUY_OPEN): {\n                                'money': 0,\n                                'amount': 0,\n                                'avg_price': 0\n                            },\n                            str(ORDER_DIRECTION.SELL_OPEN): {\n                                'money': 0,\n                                'amount': 0,\n                                'avg_price': 0\n                            }\n                        }\n                    \"\"\"[summary]\n                    # frozen\u7684\u8ba1\u7b97\n                    # money \u51bb\u7ed3\u7684\u8d44\u91d1\n                    # amount  \u51bb\u7ed3\u7684\u6570\u91cf\n\n                    2018-12-31                    \n\n                    \"\"\"\n\n                    self.frozen[code][str(trade_towards)]['money'] = (\n                        (\n                            self.frozen[code][str(trade_towards)]['money'] *\n                            self.frozen[code][str(trade_towards)]['amount']\n                        ) + abs(trade_money)\n                    ) / (\n                        self.frozen[code][str(trade_towards)]['amount'] +\n                        trade_amount\n                    )\n                    self.frozen[code][str(trade_towards)]['avg_price'] = (\n                        (\n                            self.frozen[code][str(trade_towards)]['avg_price'] *\n                            self.frozen[code][str(trade_towards)]['amount']\n                        ) + abs(raw_trade_money)\n                    ) / (\n                        self.frozen[code][str(trade_towards)]['amount'] +\n                        trade_amount\n                    )\n                    self.frozen[code][str(trade_towards)]['amount'] += trade_amount\n\n                    self.cash.append(\n                        self.cash[-1] - abs(trade_money) - commission_fee -\n                        tax_fee\n                    )\n                elif trade_towards in [ORDER_DIRECTION.BUY_CLOSE, ORDER_DIRECTION.BUY_CLOSETODAY,\n                                       ORDER_DIRECTION.SELL_CLOSE, ORDER_DIRECTION.SELL_CLOSETODAY]:\n                    # \u5e73\u4ed3\u5355\u91ca\u653e\u73b0\u91d1\n                    # if trade_towards == ORDER_DIRECTION.BUY_CLOSE:\n                    # \u5356\u7a7a\u5f00\u4ed3 \u5e73\u4ed3\u4e70\u5165\n                    # self.cash\n                    if trade_towards in [ORDER_DIRECTION.BUY_CLOSE, ORDER_DIRECTION.BUY_CLOSETODAY]:  # \u4e70\u5165\u5e73\u4ed3  \u4e4b\u524d\u662f\u7a7a\u5f00\n                                                                    # self.frozen[code][ORDER_DIRECTION.SELL_OPEN]['money'] -= trade_money\n                        self.frozen[code][str(ORDER_DIRECTION.SELL_OPEN)\n                                         ]['amount'] -= trade_amount\n\n                        frozen_part = self.frozen[code][\n                            str(ORDER_DIRECTION.SELL_OPEN)]['money'] * trade_amount\n                        # \u8d26\u6237\u7684\u73b0\u91d1+ \u51bb\u7ed3\u7684\u7684\u91ca\u653e + \u4e70\u5356\u4ef7\u5dee* \u6760\u6746\n                        self.cash.append(\n                            self.cash[-1] + frozen_part +\n                            (frozen_part - trade_money) / frozen -\n                            commission_fee - tax_fee\n                        )\n                        if self.frozen[code][str(ORDER_DIRECTION.SELL_OPEN)\n                                            ]['amount'] == 0:\n                            self.frozen[code][str(ORDER_DIRECTION.SELL_OPEN)\n                                             ]['money'] = 0\n                            self.frozen[code][str(ORDER_DIRECTION.SELL_OPEN)\n                                             ]['avg_price'] = 0\n\n                    elif trade_towards in [ORDER_DIRECTION.SELL_CLOSE, ORDER_DIRECTION.SELL_CLOSETODAY]: # \u5356\u51fa\u5e73\u4ed3  \u4e4b\u524d\u662f\u591a\u5f00\n                                                                      # self.frozen[code][ORDER_DIRECTION.BUY_OPEN]['money'] -= trade_money\n                        self.frozen[code][str(ORDER_DIRECTION.BUY_OPEN)\n                                         ]['amount'] -= trade_amount\n\n                        frozen_part = self.frozen[code][str(ORDER_DIRECTION.BUY_OPEN)\n                                                       ]['money'] * trade_amount\n                        self.cash.append(\n                            self.cash[-1] + frozen_part +\n                            (abs(trade_money) - frozen_part) / frozen -\n                            commission_fee - tax_fee\n                        )\n                        if self.frozen[code][str(ORDER_DIRECTION.BUY_OPEN)\n                                            ]['amount'] == 0:\n                            self.frozen[code][str(ORDER_DIRECTION.BUY_OPEN)\n                                             ]['money'] = 0\n                            self.frozen[code][str(ORDER_DIRECTION.BUY_OPEN)\n                                             ]['avg_price'] = 0\n            else: # \u4e0d\u5141\u8bb8\u5356\u7a7a\u5f00\u4ed3\u7684==> \u80a1\u7968\n\n                self.cash.append(\n                    self.cash[-1] - trade_money - tax_fee - commission_fee\n                )\n            if self.allow_t0 or trade_towards == ORDER_DIRECTION.SELL:\n                self.sell_available[code] = self.sell_available.get(\n                    code,\n                    0\n                ) + trade_amount * market_towards\n                self.buy_available = self.sell_available\n\n            self.cash_available = self.cash[-1]\n            frozen_money = abs(trade_money) if trade_towards in [\n                ORDER_DIRECTION.BUY_OPEN,\n                ORDER_DIRECTION.SELL_OPEN\n            ] else 0\n            self.history.append(\n                [\n                    str(trade_time),\n                    code,\n                    trade_price,\n                    market_towards * trade_amount,\n                    self.cash[-1],\n                    order_id,\n                    realorder_id,\n                    trade_id,\n                    self.account_cookie,\n                    commission_fee,\n                    tax_fee,\n                    message,\n                    frozen_money,\n                    trade_towards\n                ]\n            )\n\n        else:\n            print('ALERT MONEY NOT ENOUGH!!!')\n            print(self.cash[-1])\n            self.cash_available = self.cash[-1]", "response": "This method is used to receive a simple deal from the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef receive_deal(\n            self,\n            code: str,\n            trade_id: str,\n            order_id: str,\n            realorder_id: str,\n            trade_price: float,\n            trade_amount: int,\n            trade_towards: int,\n            trade_time: str,\n            message=None\n    ):\n        \"\"\"\u66f4\u65b0deal\n\n        Arguments:\n            code {str} -- [description]\n            trade_id {str} -- [description]\n            order_id {str} -- [description]\n            realorder_id {str} -- [description]\n            trade_price {float} -- [description]\n            trade_amount {int} -- [description]\n            trade_towards {int} -- [description]\n            trade_time {str} -- [description]\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!receive deal')\n\n        trade_time = str(trade_time)\n        code = str(code)\n        trade_price = float(trade_price)\n        trade_towards = int(trade_towards)\n        realorder_id = str(realorder_id)\n        trade_id = str(trade_id)\n        trade_amount = int(trade_amount)\n        order_id = str(order_id)\n\n        market_towards = 1 if trade_towards > 0 else -1\n        \"\"\"2019/01/03 \u76f4\u63a5\u4f7f\u7528\u5feb\u901f\u64ae\u5408\u63a5\u53e3\u4e86\n        2333 \u8fd9\u4e24\u4e2a\u63a5\u53e3\u73b0\u5728\u4e5f\u6ca1\u5565\u533a\u522b\u4e86....\n        \u592a\u7edd\u671b\u4e86\n        \"\"\"\n\n        self.receive_simpledeal(\n            code,\n            trade_price,\n            trade_amount,\n            trade_towards,\n            trade_time,\n            message=message,\n            order_id=order_id,\n            trade_id=trade_id,\n            realorder_id=realorder_id\n        )", "response": "Function to receive a deal from the server"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_order(\n            self,\n            code=None,\n            amount=None,\n            time=None,\n            towards=None,\n            price=None,\n            money=None,\n            order_model=None,\n            amount_model=None,\n            *args,\n            **kwargs\n    ):\n        \"\"\"\n        ATTENTION CHANGELOG 1.0.28\n        \u4fee\u6539\u4e86Account\u7684send_order\u65b9\u6cd5, \u533a\u5206\u6309\u6570\u91cf\u4e0b\u5355\u548c\u6309\u91d1\u989d\u4e0b\u5355\u4e24\u79cd\u65b9\u5f0f\n\n        - AMOUNT_MODEL.BY_PRICE ==> AMOUNT_MODEL.BY_MONEY # \u6309\u91d1\u989d\u4e0b\u5355\n        - AMOUNT_MODEL.BY_AMOUNT # \u6309\u6570\u91cf\u4e0b\u5355\n\n        \u5728\u6309\u91d1\u989d\u4e0b\u5355\u7684\u65f6\u5019,\u5e94\u7ed9\u4e88 money\u53c2\u6570\n        \u5728\u6309\u6570\u91cf\u4e0b\u5355\u7684\u65f6\u5019,\u5e94\u7ed9\u4e88 amount\u53c2\u6570\n\n        python code:\n        Account=QA.QA_Account()\n\n        Order_bymoney=Account.send_order(code='000001',\n                                        price=11,\n                                        money=0.3*Account.cash_available,\n                                        time='2018-05-09',\n                                        towards=QA.ORDER_DIRECTION.BUY,\n                                        order_model=QA.ORDER_MODEL.MARKET,\n                                        amount_model=QA.AMOUNT_MODEL.BY_MONEY\n                                        )\n\n        Order_byamount=Account.send_order(code='000001',\n                                        price=11,\n                                        amount=100,\n                                        time='2018-05-09',\n                                        towards=QA.ORDER_DIRECTION.BUY,\n                                        order_model=QA.ORDER_MODEL.MARKET,\n                                        amount_model=QA.AMOUNT_MODEL.BY_AMOUNT\n                                        )\n\n        :param code: \u8bc1\u5238\u4ee3\u7801\n        :param amount: \u4e70\u5356 \u6570\u91cf\u591a\u6570\u80a1\n        :param time:  Timestamp \u5bf9\u8c61 \u4e0b\u5355\u65f6\u95f4\n        :param towards: int , towards>0 \u4e70\u5165 towards<0 \u5356\u51fa\n        :param price: \u4e70\u5165\uff0c\u5356\u51fa \u6807\u7684\u8bc1\u5238\u7684\u4ef7\u683c\n        :param money: \u4e70\u5356 \u4ef7\u683c\n        :param order_model: \u7c7b\u578b QA.ORDER_MODE\n        :param amount_model:\u7c7b\u578b QA.AMOUNT_MODEL\n        :return:  QA_Order | False\n\n        @2018/12/23\n        send_order \u662fQA\u7684\u6807\u51c6\u8fd4\u56de, \u5982\u9700\u5bf9\u63a5\u5176\u4ed6\u63a5\u53e3, \u53ea\u9700\u8981\u5bf9\u4e8eQA_Order\u505a\u9002\u914d\u5373\u53ef\n\n\n        @2018/12/27\n        \u5728\u5224\u65ad\u8d26\u6237\u4e3a\u671f\u8d27\u8d26\u6237(\u53ca \u5141\u8bb8\u53cc\u5411\u4ea4\u6613)\n\n        @2018/12/30 \u4fdd\u8bc1\u91d1\u8d26\u6237\u7684\u4fee\u6539\n        1. \u4fdd\u8bc1\u91d1\u8d26\u6237\u51bb\u7ed3\u7684\u91d1\u989d\n        2. \u4fdd\u8bc1\u91d1\u8d26\u6237\u7684\u7ed3\u7b97\n        3. \u4fdd\u8bc1\u91d1\u8d26\u6237\u7684\u5224\u65ad\n\n        \"\"\"\n        wrong_reason = None\n        assert code is not None and time is not None and towards is not None and order_model is not None and amount_model is not None\n\n        # \ud83d\udee0todo \u79fb\u5230Utils\u7c7b\u4e2d\uff0c  \u65f6\u95f4\u8f6c\u6362\n        # date \u5b57\u7b26\u4e32 2011-10-11 \u957f\u5ea610\n        date = str(time)[0:10] if len(str(time)) == 19 else str(time)\n        # time \u5b57\u7b26\u4e32 20011-10-11 09:02:00  \u957f\u5ea6 19\n        time = str(time) if len(str(time)) == 19 else '{} 09:31:00'.format(\n            str(time)[0:10]\n        )\n\n        # \ud83d\udee0todo \u79fb\u5230Utils\u7c7b\u4e2d\uff0c  amount_to_money \u6210\u4ea4\u91cf\u8f6c\u91d1\u989d\n        # BY_MONEY :: amount --\u94b1 \u598210000\u5143  \u56e0\u6b64 by_money\u91cc\u9762 \u9700\u8981\u6307\u5b9a\u4ef7\u683c,\u6765\u8ba1\u7b97\u5b9e\u9645\u7684\u80a1\u7968\u6570\n        # by_amount :: amount --\u80a1\u6570 \u598210000\u80a1\n\n        if self.allow_margin:\n            amount = amount if amount_model is AMOUNT_MODEL.BY_AMOUNT else int(\n                money / (\n                    self.market_preset.get_unit(code) *\n                    self.market_preset.get_frozen(code) * price *\n                    (1 + self.commission_coeff)\n                ) / 100\n            ) * 100\n        else:\n\n            amount = amount if amount_model is AMOUNT_MODEL.BY_AMOUNT else int(\n                money / (price * (1 + self.commission_coeff)) / 100\n            ) * 100\n\n        # \ud83d\udee0todo \u79fb\u5230Utils\u7c7b\u4e2d\uff0c  money_to_amount \u91d1\u989d\u8f6c\u6210\u4ea4\u91cf\n        if self.allow_margin:\n            money = amount * price * self.market_preset.get_unit(code)*self.market_preset.get_frozen(code) * \\\n                (1+self.commission_coeff) if amount_model is AMOUNT_MODEL.BY_AMOUNT else money\n        else:\n            money = amount * price * \\\n                (1+self.commission_coeff) if amount_model is AMOUNT_MODEL.BY_AMOUNT else money\n\n        # flag \u5224\u65ad\u4e70\u5356 \u6570\u91cf\u548c\u4ef7\u683c\u4ee5\u53ca\u4e70\u5356\u65b9\u5411\u662f\u5426\u6b63\u786e\n        flag = False\n\n        assert (int(towards) != 0)\n        if int(towards) in [1, 2, 3]:\n            # \u662f\u4e70\u5165\u7684\u60c5\u51b5(\u5305\u62ec\u4e70\u5165.\u4e70\u5f00.\u4e70\u5e73)\n            if self.cash_available >= money:\n                if self.market_type == MARKET_TYPE.STOCK_CN: # \u5982\u679c\u662f\u80a1\u7968 \u4e70\u5165\u7684\u65f6\u5019\u6709100\u80a1\u7684\u6700\u5c0f\u9650\u5236\n                    amount = int(amount / 100) * 100\n                    self.cash_available -= money\n                    flag = True\n\n                if self.running_environment == RUNNING_ENVIRONMENT.TZERO:\n\n                    if abs(self.buy_available.get(code, 0)) >= amount:\n                        flag = True\n                        self.cash_available -= money\n                        self.buy_available[code] -= amount\n                    else:\n                        flag = False\n                        wrong_reason = 'T0\u4ea4\u6613\u4e70\u5165\u8d85\u51fa\u9650\u989d'\n\n                if self.market_type == MARKET_TYPE.FUTURE_CN:\n                    # \u5982\u679c\u6709\u8d1f\u6301\u4ed3-- \u5141\u8bb8\u5356\u7a7a\u7684\u65f6\u5019\n                    if towards == 3: # \u591a\u5e73\n                        _hold = self.sell_available.get(code, 0)\n                                     # \u5047\u8bbe\u6709\u8d1f\u6301\u4ed3:\n                                     # amount\u4e3a\u4e0b\u5355\u6570\u91cf \u5982  \u8d26\u6237\u539f\u5148-3\u624b \u73b0\u5728\u5e731\u624b\n\n                        #left_amount = amount+_hold if _hold < 0 else amount\n                        _money = abs(\n                            float(amount * price * (1 + self.commission_coeff))\n                        )\n\n                        print(_hold)\n                        if self.cash_available >= _money:\n                            if _hold < 0:\n                                self.cash_available -= _money\n\n                                flag = True\n                            else:\n                                wrong_reason = '\u7a7a\u5355\u4ed3\u4f4d\u4e0d\u8db3'\n                        else:\n                            wrong_reason = '\u5e73\u591a\u5269\u4f59\u8d44\u91d1\u4e0d\u591f'\n                    if towards == 2:\n                        self.cash_available -= money\n                        flag = True\n            else:\n                wrong_reason = 'QAACCOUNT: \u53ef\u7528\u8d44\u91d1\u4e0d\u8db3 cash_available {}  code {} time {} amount {} towards {}'.format(\n                    self.cash_available,\n                    code,\n                    time,\n                    amount,\n                    towards\n                )\n        elif int(towards) in [-1, -2, -3]:\n            # \u662f\u5356\u51fa\u7684\u60c5\u51b5(\u5305\u62ec\u5356\u51fa\uff0c\u5356\u51fa\u5f00\u4ed3allow_sellopen\u5982\u679c\u5141\u8bb8. \u5356\u51fa\u5e73\u4ed3)\n            # print(self.sell_available[code])\n            _hold = self.sell_available.get(code, 0) # _hold \u662f\u4f60\u7684\u6301\u4ed3\n\n            # \u5982\u679c\u4f60\u7684hold> amount>0\n            # \u6301\u4ed3\u6570\u91cf>\u5356\u51fa\u6570\u91cf\n            if _hold >= amount:\n                self.sell_available[code] -= amount\n                # towards = ORDER_DIRECTION.SELL\n                flag = True\n            # \u5982\u679c\u6301\u4ed3\u6570\u91cf<\u5356\u51fa\u6570\u91cf\n            else:\n\n                # \u5982\u679c\u662f\u5141\u8bb8\u5356\u7a7a\u5f00\u4ed3 \u5b9e\u9645\u8ba1\u7b97\u65f6  \u5148\u51cf\u53bb\u6301\u4ed3(\u6b63\u6301\u4ed3) \u518d\u8ba1\u7b97 \u8d1f\u6301\u4ed3 \u5c31\u6309\u539f\u5148\u7684\u5360\u7528\u91d1\u989d\u8ba1\u7b97\n                if self.allow_sellopen and towards == -2:\n\n                    if self.cash_available >= money: # \u5356\u7a7a\u7684\u5e02\u503c\u5c0f\u4e8e\u73b0\u91d1\uff08\u6709\u62c5\u4fdd\u7684\u5356\u7a7a\uff09\uff0c \u4e0d\u5141\u8bb8\u88f8\u5356\u7a7a\n                                                     # self.cash_available -= money\n                        flag = True\n                    else:\n                        print('sellavailable', _hold)\n                        print('amount', amount)\n                        print('aqureMoney', money)\n                        print('cash', self.cash_available)\n                        wrong_reason = \"\u5356\u7a7a\u8d44\u91d1\u4e0d\u8db3/\u4e0d\u5141\u8bb8\u88f8\u5356\u7a7a\"\n                else:\n                    wrong_reason = \"\u5356\u51fa\u4ed3\u4f4d\u4e0d\u8db3\"\n\n        if flag and (amount > 0):\n            _order = QA_Order(\n                user_cookie=self.user_cookie,\n                strategy=self.strategy_name,\n                frequence=self.frequence,\n                account_cookie=self.account_cookie,\n                code=code,\n                market_type=self.market_type,\n                date=date,\n                datetime=time,\n                sending_time=time,\n                callback=self.receive_deal,\n                amount=amount,\n                price=price,\n                order_model=order_model,\n                towards=towards,\n                money=money,\n                broker=self.broker,\n                amount_model=amount_model,\n                commission_coeff=self.commission_coeff,\n                tax_coeff=self.tax_coeff,\n                *args,\n                **kwargs\n            )                                                           # init\n                                                                        # \u5386\u53f2\u59d4\u6258order\u72b6\u6001\u5b58\u50a8\uff0c \u4fdd\u5b58\u5230 QA_Order \u5bf9\u8c61\u4e2d\u7684\u961f\u5217\u4e2d\n            self.datetime = time\n            self.orders.insert_order(_order)\n            return _order\n        else:\n            print(\n                'ERROR : CODE {} TIME {}  AMOUNT {} TOWARDS {}'.format(\n                    code,\n                    time,\n                    amount,\n                    towards\n                )\n            )\n            print(wrong_reason)\n            return False", "response": "send_order - Sends an order to the specified order_code amount_model price_model amount_model time time_model amount_model"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close_positions_order(self):\n\n        order_list = []\n        time = '{} 15:00:00'.format(self.date)\n        if self.running_environment == RUNNING_ENVIRONMENT.TZERO:\n            for code, amount in self.hold_available.iteritems():\n                order = False\n                if amount < 0:\n                    # \u5148\u5356\u51fa\u7684\u5355\u5b50 \u4e70\u5e73\n                    order = self.send_order(\n                        code=code,\n                        price=0,\n                        amount=abs(amount),\n                        time=time,\n                        towards=ORDER_DIRECTION.BUY,\n                        order_model=ORDER_MODEL.CLOSE,\n                        amount_model=AMOUNT_MODEL.BY_AMOUNT,\n                    )\n                elif amount > 0:\n                    # \u5148\u4e70\u5165\u7684\u5355\u5b50, \u5356\u5e73\n                    order = self.send_order(\n                        code=code,\n                        price=0,\n                        amount=abs(amount),\n                        time=time,\n                        towards=ORDER_DIRECTION.SELL,\n                        order_model=ORDER_MODEL.CLOSE,\n                        amount_model=AMOUNT_MODEL.BY_AMOUNT\n                    )\n                if order:\n                    order_list.append(order)\n            return order_list\n        else:\n            raise RuntimeError(\n                'QAACCOUNT with {} environments cannot use this methods'.format(\n                    self.running_environment\n                )\n            )", "response": "\u5e73\u4ed3\u5355\n\n        Raises:\n            RuntimeError -- if ACCOUNT.RUNNING_ENVIRONMENT is NOT TZERO\n\n        Returns:\n            list -- list with order"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef settle(self, settle_data = None):\n        #print('FROM QUANTAXIS QA_ACCOUNT: account settle')\n        if self.running_environment == RUNNING_ENVIRONMENT.TZERO and self.hold_available.sum(\n        ) != 0:\n            raise RuntimeError(\n                'QAACCOUNT: \u8be5T0\u8d26\u6237\u672a\u5f53\u65e5\u4ed3\u4f4d,\u8bf7\u5e73\u4ed3 {}'.format(\n                    self.hold_available.to_dict()\n                )\n            )\n        if self.market_type == MARKET_TYPE.FUTURE_CN:\n            # \u589e\u52a0\u9010\u65e5\u76ef\u5e02\u5236\u5ea6\n\n            self.static_balance['frozen'].append(\n                sum(\n                    [\n                        rx['money'] * rx['amount']\n                        for var in self.frozen.values()\n                        for rx in var.values()\n                    ]\n                )\n            )\n\n            self.static_balance['cash'].append(self.cash[-1])\n            self.static_balance['hold'].append(self.hold.to_dict())\n            self.static_balance['date'].append(self.date)\n            \"\"\"\u9759\u6001\u6743\u76ca\u7684\u7ed3\u7b97\n\n            \u53ea\u5173\u5fc3\u5f00\u4ed3\u4ef7/ \u4e0d\u505a\u76ef\u5e02\u5236\u5ea6\n\n            \u52a8\u6001\u6743\u76ca\u7684\u7ed3\u7b97\u9700\u8981\u5173\u5fc3\n\n            \"\"\"\n\n            self.static_balance['static_assets'].append(\n                self.static_balance['cash'][-1] +\n                self.static_balance['frozen'][-1]\n            )\n\n        self.sell_available = self.hold\n        self.buy_available = self.hold\n        self.cash_available = self.cash[-1]\n        self.datetime = '{} 09:30:00'.format(\n            QA_util_get_next_day(self.date)\n        ) if self.date is not None else None", "response": "settle \u8bbe\u7f6e\u80a1\u7968 \u80a1\u7968 \u80a1\ufffd"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_bar(self, event):\n        '''\n        \u7b56\u7565\u4e8b\u4ef6\n        :param event:\n        :return:\n        '''\n        'while updating the market data'\n\n        print(\n            \"on_bar account {} \".format(self.account_cookie),\n            event.market_data.data\n        )\n        print(event.send_order)\n        try:\n            for code in event.market_data.code:\n\n                if self.sell_available.get(code, 0) > 0:\n                    print('\u53ef\u4ee5\u5356\u51fa {}'.format(self._currenttime))\n                    event.send_order(\n                        account_cookie=self.account_cookie,\n                        amount=self.sell_available[code],\n                        amount_model=AMOUNT_MODEL.BY_AMOUNT,\n                        time=self.current_time,\n                        code=code,\n                        price=0,\n                        order_model=ORDER_MODEL.MARKET,\n                        towards=ORDER_DIRECTION.SELL,\n                        market_type=self.market_type,\n                        frequence=self.frequence,\n                        broker_name=self.broker\n                    )\n                else:\n                    print('{} \u65e0\u4ed3\u4f4d, \u4e70\u5165{}'.format(self._currenttime, code))\n                    event.send_order(\n                        account_cookie=self.account_cookie,\n                        amount=100,\n                        amount_model=AMOUNT_MODEL.BY_AMOUNT,\n                        time=self.current_time,\n                        code=code,\n                        price=0,\n                        order_model=ORDER_MODEL.MARKET,\n                        towards=ORDER_DIRECTION.BUY,\n                        market_type=self.market_type,\n                        frequence=self.frequence,\n                        broker_name=self.broker\n                    )\n        except Exception as e:\n            print(e)", "response": "while updating the market data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_message(self, message):\n        self.account_cookie = message.get('account_cookie', None)\n        self.portfolio_cookie = message.get('portfolio_cookie', None)\n        self.user_cookie = message.get('user_cookie', None)\n        self.broker = message.get('broker', None)\n        self.market_type = message.get('market_type', None)\n        self.strategy_name = message.get('strategy_name', None)\n        self._currenttime = message.get('current_time', None)\n        self.allow_sellopen = message.get('allow_sellopen', False)\n        self.allow_margin = message.get('allow_margin', False)\n        self.allow_t0 = message.get('allow_t0', False)\n        self.margin_level = message.get('margin_level', False)\n        self.frequence = message.get('frequence', FREQUENCE.FIFTEEN_MIN) #\u9ed8\u8ba415min\n        self.init_cash = message.get(\n            'init_cash',\n            message.get('init_assets',\n                        1000000)\n        )                                       # \u517c\u5bb9\u4fee\u6539\n        self.init_hold = pd.Series(message.get('init_hold', {}), name='amount')\n        self.init_hold.index.name = 'code'\n        self.commission_coeff = message.get('commission_coeff', 0.00015)\n        self.tax_coeff = message.get('tax_coeff', 0.0015)\n        self.history = message['history']\n        self.cash = message['cash']\n        self.time_index_max = message['trade_index']\n        self.running_time = message.get('running_time', None)\n        self.quantaxis_version = message.get('quantaxis_version', None)\n        self.running_environment = message.get(\n            'running_environment',\n            RUNNING_ENVIRONMENT.BACKETEST\n        )\n        self.frozen = message.get('frozen', {})\n        self.finishedOrderid = message.get('finished_id', [])\n        self.settle()\n        return self", "response": "resume the account from a standard message"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts OTP dict to a dict.", "response": "def from_otgdict(self, message):\n        \"\"\"[summary]\n        balance = static_balance + float_profit\n\n\n            \"currency\": \"\",  # \"CNY\" (\u5e01\u79cd)\n            \"pre_balance\": float(\"nan\"),  # 9912934.78 (\u6628\u65e5\u8d26\u6237\u6743\u76ca)\n            \"static_balance\": float(\"nan\"),  # (\u9759\u6001\u6743\u76ca)\n            \"balance\": float(\"nan\"),  # 9963216.55 (\u8d26\u6237\u6743\u76ca)\n            \"available\": float(\"nan\"),  # 9480176.15 (\u53ef\u7528\u8d44\u91d1)\n            \"float_profit\": float(\"nan\"),  # 8910.0 (\u6d6e\u52a8\u76c8\u4e8f)\n            \"position_profit\": float(\"nan\"),  # 1120.0(\u6301\u4ed3\u76c8\u4e8f)\n            \"close_profit\": float(\"nan\"),  # -11120.0 (\u672c\u4ea4\u6613\u65e5\u5185\u5e73\u4ed3\u76c8\u4e8f)\n            \"frozen_margin\": float(\"nan\"),  # 0.0(\u51bb\u7ed3\u4fdd\u8bc1\u91d1)\n            \"margin\": float(\"nan\"),  # 11232.23 (\u4fdd\u8bc1\u91d1\u5360\u7528)\n            \"frozen_commission\": float(\"nan\"),  # 0.0 (\u51bb\u7ed3\u624b\u7eed\u8d39)\n            \"commission\": float(\"nan\"),  # 123.0 (\u672c\u4ea4\u6613\u65e5\u5185\u4ea4\u7eb3\u7684\u624b\u7eed\u8d39)\n            \"frozen_premium\": float(\"nan\"),  # 0.0 (\u51bb\u7ed3\u6743\u5229\u91d1)\n            \"premium\": float(\"nan\"),  # 0.0 (\u672c\u4ea4\u6613\u65e5\u5185\u4ea4\u7eb3\u7684\u6743\u5229\u91d1)\n            \"deposit\": float(\"nan\"),  # 1234.0 (\u672c\u4ea4\u6613\u65e5\u5185\u7684\u5165\u91d1\u91d1\u989d)\n            \"withdraw\": float(\"nan\"),  # 890.0 (\u672c\u4ea4\u6613\u65e5\u5185\u7684\u51fa\u91d1\u91d1\u989d)\n            \"risk_ratio\": float(\"nan\"),  # 0.048482375 (\u98ce\u9669\u5ea6)\n        \"\"\"\n\n        self.allow_margin = True\n        self.allow_sellopen = True\n        self.allow_t0 = True\n\n        self.account_cookie = message['accounts']['user_id']\n        # \u53ef\u7528\u8d44\u91d1\n        self.cash_available = message['accounts']['available']\n        self.balance = message['accounts']['balance']\n\n        # \u90fd\u662f\u5728\u7ed3\u7b97\u7684\u65f6\u5019\u8ba1\u7b97\u7684\n        # \u6628\u65e5\u6743\u76ca/\u9759\u6001\u6743\u76ca ==> \u8fd9\u4e24\u4e2a\u662f\u4e00\u6837\u7684\n        self.static_balance = message['accounts']['static_balance']\n        self.pre_balance = message['accounts']['pre_balance']\n\n        # \u5e73\u4ed3\u76c8\u4e8f\n        self.close_profit = message['accounts']['close_profit']\n        # \u6301\u4ed3\u76c8\u4e8f\n        self.position_profit = message['accounts']['position_profit']\n\n        # \u52a8\u6001\u6743\u76ca\n        self.float_profit = message['accounts']['float_profit']\n\n        # \u5360\u7528\u4fdd\u8bc1\u91d1\n        self.margin = message['accounts']['margin']\n\n        self.commission = message['accounts']['commission']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a pandas. DataFrame of the current user s message and account cookie.", "response": "def table(self):\n        \"\"\"\n        \u6253\u5370\u51faaccount\u7684\u5185\u5bb9\n        \"\"\"\n        return pd.DataFrame([\n            self.message,\n        ]).set_index(\n            'account_cookie',\n            drop=False\n        ).T"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, event):\n        '''\n        \u8fd9\u4e2a\u65b9\u6cd5\u662f\u88ab QA_ThreadEngine \u5904\u7406\u961f\u5217\u65f6\u5019\u8c03\u7528\u7684\uff0c QA_Task \u4e2d do \u65b9\u6cd5\u8c03\u7528 run \uff08\u5728\u5176\u5b83\u7ebf\u7a0b\u4e2d\uff09\n       'QA_WORKER method \u91cd\u8f7d'\n        :param event: \u4e8b\u4ef6\u7c7b\u578b QA_Event\n        :return:\n        '''\n        'QA_WORKER method'\n        if event.event_type is ACCOUNT_EVENT.SETTLE:\n            print('account_settle')\n            self.settle()\n\n        # elif event.event_type is ACCOUNT_EVENT.UPDATE:\n        #     self.receive_deal(event.message)\n        elif event.event_type is ACCOUNT_EVENT.MAKE_ORDER:\n            \"\"\"generate order\n            if callback callback the order\n            if not return back the order\n            \"\"\"\n            data = self.send_order(\n                code=event.code,\n                amount=event.amount,\n                time=event.time,\n                amount_model=event.amount_model,\n                towards=event.towards,\n                price=event.price,\n                order_model=event.order_model\n            )\n            if event.callback:\n                event.callback(data)\n            else:\n                return data\n        elif event.event_type is ENGINE_EVENT.UPCOMING_DATA:\n            \"\"\"update the market_data\n            1. update the inside market_data struct\n            2. tell the on_bar methods\n\n            # \u8fd9\u6837\u6709\u70b9\u6162\n\n\n            \"\"\"\n\n            self._currenttime = event.market_data.datetime[0]\n            if self._market_data is None:\n                self._market_data = event.market_data\n            else:\n                self._market_data = self._market_data + event.market_data\n            self.on_bar(event)\n\n            if event.callback:\n                event.callback(event)", "response": "This method is called by the worker thread when the event is received."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sync_account(self, sync_message):\n\n        self.init_hold = sync_message['hold_available']\n        self.init_cash = sync_message['cash_available']\n\n        self.sell_available = copy.deepcopy(self.init_hold)\n        self.history = []\n        self.cash = [self.init_cash]\n        self.cash_available = self.cash[-1]", "response": "This method is used to sync the account with the current user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the cash amount of the user.", "response": "def change_cash(self, money):\n        \"\"\"\n        \u5916\u90e8\u64cd\u4f5c|\u9ad8\u5371|\n        \"\"\"\n        res = self.cash[-1] + money\n        if res >= 0:\n            # \u9ad8\u5371\u64cd\u4f5c\n            self.cash[-1] = res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a DataFrame of the history of the current user.", "response": "def get_history(self, start, end):\n        \"\"\"\u8fd4\u56de\u5386\u53f2\u6210\u4ea4\n\n        Arguments:\n            start {str} -- [description]\n            end {str]} -- [description]\n        \"\"\"\n        return self.history_table.set_index(\n            'datetime',\n            drop=False\n        ).loc[slice(pd.Timestamp(start),\n                    pd.Timestamp(end))]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_SU_save_order(orderlist, client=DATABASE):\n    if isinstance(orderlist, pd.DataFrame):\n\n        collection = client.order\n        collection.create_index(\n            [('account_cookie',\n              ASCENDING),\n             ('realorder_id',\n              ASCENDING)],\n            unique=True\n        )\n        try:\n\n            orderlist = QA_util_to_json_from_pandas(orderlist.reset_index())\n\n            for item in orderlist:\n                if item:\n                    #item['date']= QA_util_get_order_day()\n                    collection.update_one(\n                        {\n                            'account_cookie': item.get('account_cookie'),\n                            'realorder_id': item.get('realorder_id')\n                        },\n                        {'$set': item},\n                        upsert=True\n                    )\n        except Exception as e:\n            print(e)\n            pass", "response": "Save the order list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_SU_save_deal(dealist, client=DATABASE):\n\n    if isinstance(dealist, pd.DataFrame):\n\n        collection = client.deal\n\n        collection.create_index(\n            [('account_cookie',\n              ASCENDING),\n             ('trade_id',\n              ASCENDING)],\n            unique=True\n        )\n        try:\n            dealist = QA_util_to_json_from_pandas(dealist.reset_index())\n            collection.insert_many(dealist, ordered=False)\n        except Exception as e:\n\n            pass", "response": "Save a deal list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncounts - > COUNT", "response": "def COUNT(COND, N):\n    \"\"\"\n    2018/05/23 \u4fee\u6539\n\n    \u53c2\u8003https://github.com/QUANTAXIS/QUANTAXIS/issues/429\n\n    \u73b0\u5728\u8fd4\u56de\u7684\u662fseries\n    \"\"\"\n    return pd.Series(np.where(COND, 1, 0), index=COND.index).rolling(N).sum()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef LAST(COND, N1, N2):\n    N2 = 1 if N2 == 0 else N2\n    assert N2 > 0\n    assert N1 > N2\n    return COND.iloc[-N1:-N2].all()", "response": "Returns True if COND contains the last N1 and N2 elements in COND."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef AVEDEV(Series, N):\n    return Series.rolling(N).apply(lambda x: (np.abs(x - x.mean())).mean(), raw=True)", "response": "AveDEV \u5e73\u5747\u7edd\u5bf9\u504f\u5dee mean absolute deviation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef MACD(Series, FAST, SLOW, MID):\n    EMAFAST = EMA(Series, FAST)\n    EMASLOW = EMA(Series, SLOW)\n    DIFF = EMAFAST - EMASLOW\n    DEA = EMA(DIFF, MID)\n    MACD = (DIFF - DEA) * 2\n    DICT = {'DIFF': DIFF, 'DEA': DEA, 'MACD': MACD}\n    VAR = pd.DataFrame(DICT)\n    return VAR", "response": "MACD \u6307\u6807 \u4ec5\u9002\u7528\u4e8eSeries\n \u5bf9\u4e8e DATAFRAME\u7684\u5e94\u7528\u8bf7\u4f7f\u7528QA_indicator_macd\n \u5bf9\u4e8eQA_indicator_macd\n \u5bf9\u4e8eQA"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef BARLAST(cond, yes=True):\n    if isinstance(cond.index, pd.MultiIndex):\n        return len(cond)-cond.index.levels[0].tolist().index(cond[cond != yes].index[-1][0])-1\n    elif isinstance(cond.index, pd.DatetimeIndex):\n        return len(cond)-cond.index.tolist().index(cond[cond != yes].index[-1])-1", "response": "Returns the last item in the hierarchy of a given condition."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all the neccessary data from the stock list", "response": "def get_today_all(output='pd'):\n    \"\"\"today all\n\n    Returns:\n        [type] -- [description]\n    \"\"\"\n\n    data = []\n    today = str(datetime.date.today())\n    codes = QA_fetch_get_stock_list('stock').code.tolist()\n    bestip = select_best_ip()['stock']\n    for code in codes:\n        try:\n            l = QA_fetch_get_stock_day(\n                code, today, today, '00', ip=bestip)\n        except:\n            bestip = select_best_ip()['stock']\n            l = QA_fetch_get_stock_day(\n                code, today, today, '00', ip=bestip)\n        if l is not None:\n            data.append(l)\n\n    res = pd.concat(data)\n    if output in ['pd']:\n        return res\n    elif output in ['QAD']:\n        return QA_DataStruct_Stock_day(res.set_index(['date', 'code'], drop=False))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef QA_SU_save_stock_day(client=DATABASE, ui_log=None, ui_progress=None):\n    '''\n     save stock_day\n    \u4fdd\u5b58\u65e5\u7ebf\u6570\u636e\n    :param client:\n    :param ui_log:  \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress_int_value: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    '''\n    stock_list = QA_fetch_get_stock_list().code.unique().tolist()\n    coll_stock_day = client.stock_day\n    coll_stock_day.create_index(\n        [(\"code\",\n          pymongo.ASCENDING),\n         (\"date_stamp\",\n          pymongo.ASCENDING)]\n    )\n    err = []\n\n    # saveing result\n\n    def __gen_param(stock_list, coll_stock_day, ip_list=[]):\n        results = []\n        count = len(ip_list)\n        total = len(stock_list)\n        for item in range(len(stock_list)):\n            try:\n                code = stock_list[item]\n                QA_util_log_info(\n                    '##JOB01 Now Saving STOCK_DAY==== {}'.format(str(code)),\n                    ui_log\n                )\n\n                # \u9996\u9009\u67e5\u627e\u6570\u636e\u5e93 \u662f\u5426 \u6709 \u8fd9\u4e2a\u4ee3\u7801\u7684\u6570\u636e\n                search_cond = {'code': str(code)[0:6]}\n                ref = coll_stock_day.find(search_cond)\n                end_date = str(now_time())[0:10]\n                ref_count = coll_stock_day.count_documents(search_cond)\n\n                # \u5f53\u524d\u6570\u636e\u5e93\u5df2\u7ecf\u5305\u542b\u4e86\u8fd9\u4e2a\u4ee3\u7801\u7684\u6570\u636e\uff0c \u7ee7\u7eed\u589e\u91cf\u66f4\u65b0\n                # \u52a0\u5165\u8fd9\u4e2a\u5224\u65ad\u7684\u539f\u56e0\u662f\u56e0\u4e3a\u5982\u679c\u80a1\u7968\u662f\u521a\u4e0a\u5e02\u7684 \u6570\u636e\u5e93\u4f1a\u6ca1\u6709\u6570\u636e \u6240\u4ee5\u4f1a\u6709\u8d1f\u7d22\u5f15\u95ee\u9898\u51fa\u73b0\n                if ref_count > 0:\n                    # \u63a5\u7740\u4e0a\u6b21\u83b7\u53d6\u7684\u65e5\u671f\u7ee7\u7eed\u66f4\u65b0\n                    start_date = ref[ref_count - 1]['date']\n                    # print(\"ref[ref.count() - 1]['date'] {} {}\".format(ref.count(), coll_stock_day.count_documents({'code': str(code)[0:6]})))\n                else:\n                    # \u5f53\u524d\u6570\u636e\u5e93\u4e2d\u6ca1\u6709\u8fd9\u4e2a\u4ee3\u7801\u7684\u80a1\u7968\u6570\u636e\uff0c \u4ece1990-01-01 \u5f00\u59cb\u4e0b\u8f7d\u6240\u6709\u7684\u6570\u636e\n                    start_date = '1990-01-01'\n                QA_util_log_info(\n                    'UPDATE_STOCK_DAY \\n Trying updating {} from {} to {}'\n                        .format(code,\n                                start_date,\n                                end_date),\n                    ui_log\n                )\n                if start_date != end_date:\n                    # \u66f4\u65b0\u8fc7\u7684\uff0c\u4e0d\u66f4\u65b0\n                    results.extend([(code, start_date, end_date, '00', 'day', ip_list[item % count]['ip'],\n                                     ip_list[item % count]['port'], item, total, ui_log, ui_progress)])\n            except Exception as error0:\n                print('Exception:{}'.format(error0))\n                err.append(code)\n        return results\n\n    ips = get_ip_list_by_multi_process_ping(stock_ip_list, _type='stock')[:cpu_count() * 2 + 1]\n    param = __gen_param(stock_list, coll_stock_day, ips)\n    ps = QA_SU_save_stock_day_parallelism(processes=cpu_count() if len(ips) >= cpu_count() else len(ips),\n                                          client=client, ui_log=ui_log)\n    ps.add(do_saving_work, param)\n    ps.run()\n\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS save stock day ^_^', ui_log)\n    else:\n        QA_util_log_info('ERROR CODE \\n ', ui_log)\n        QA_util_log_info(err, ui_log)", "response": "save stock_day\n    \u4fdd\u5b58\u65e5\u7ebf\u6570\u636e\n    :param client:\n    :param ui_log:  \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress_int_value: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QA_user_sign_in(username, password):\n    #user = QA_User(name= name, password=password)\n    cursor = DATABASE.user.find_one(\n        {'username': username, 'password': password})\n    if cursor is None:\n        QA_util_log_info('SOMETHING WRONG')\n        return False\n    else:\n        return True", "response": "Check if user is signed in"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsign up a user", "response": "def QA_user_sign_up(name, password, client):\n    \"\"\"\u53ea\u505acheck! \u5177\u4f53\u903b\u8f91\u9700\u8981\u5728\u81ea\u5df1\u7684\u51fd\u6570\u4e2d\u5b9e\u73b0\n\n    \u53c2\u89c1:QAWEBSERVER\u4e2d\u7684\u5b9e\u73b0\n    \n    Arguments:\n        name {[type]} -- [description]\n        password {[type]} -- [description]\n        client {[type]} -- [description]\n    \n    Returns:\n        [type] -- [description]\n    \"\"\"\n\n    coll = client.user\n    if (coll.find({'username': name}).count() > 0):\n        print(name)\n        QA_util_log_info('user name is already exist')\n        return False\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwarp order to the next open or previous open.", "response": "def warp(self, order):\n        \"\"\"\u5bf9order/market\u7684\u5c01\u88c5\n\n        [description]\n\n        Arguments:\n            order {[type]} -- [description]\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        # \u56e0\u4e3a\u6210\u4ea4\u6a21\u5f0f\u5bf9\u65f6\u95f4\u7684\u5c01\u88c5\n\n        if order.order_model == ORDER_MODEL.MARKET:\n\n            if order.frequence is FREQUENCE.DAY:\n                # exact_time = str(datetime.datetime.strptime(\n                #     str(order.datetime), '%Y-%m-%d %H-%M-%S') + datetime.timedelta(day=1))\n\n                order.date = order.datetime[0:10]\n                order.datetime = '{} 09:30:00'.format(order.date)\n            elif order.frequence in [FREQUENCE.ONE_MIN,\n                                     FREQUENCE.FIVE_MIN,\n                                     FREQUENCE.FIFTEEN_MIN,\n                                     FREQUENCE.THIRTY_MIN,\n                                     FREQUENCE.SIXTY_MIN]:\n                exact_time = str(\n                    datetime.datetime\n                    .strptime(str(order.datetime),\n                              '%Y-%m-%d %H:%M:%S') +\n                    datetime.timedelta(minutes=1)\n                )\n                order.date = exact_time[0:10]\n                order.datetime = exact_time\n            self.market_data = self.get_market(order)\n            if self.market_data is None:\n                return order\n            order.price = (\n                float(self.market_data[\"high\"]) +\n                float(self.market_data[\"low\"])\n            ) * 0.5\n        elif order.order_model == ORDER_MODEL.NEXT_OPEN:\n            try:\n                exact_time = str(\n                    datetime.datetime\n                    .strptime(str(order.datetime),\n                              '%Y-%m-%d %H-%M-%S') + datetime.timedelta(day=1)\n                )\n                order.date = exact_time[0:10]\n                order.datetime = '{} 09:30:00'.format(order.date)\n            except:\n                order.datetime = '{} 15:00:00'.format(order.date)\n            self.market_data = self.get_market(order)\n            if self.market_data is None:\n                return order\n            order.price = float(self.market_data[\"close\"])\n        elif order.order_model == ORDER_MODEL.CLOSE:\n\n            try:\n                order.datetime = self.market_data.datetime\n            except:\n                if len(str(order.datetime)) == 19:\n                    pass\n                else:\n                    order.datetime = '{} 15:00:00'.format(order.date)\n            self.market_data = self.get_market(order)\n            if self.market_data is None:\n                return order\n            order.price = float(self.market_data[\"close\"])\n\n        elif order.order_model == ORDER_MODEL.STRICT:\n            '\u52a0\u5165\u4e25\u683c\u6a21\u5f0f'\n            if order.frequence is FREQUENCE.DAY:\n                exact_time = str(\n                    datetime.datetime\n                    .strptime(order.datetime,\n                              '%Y-%m-%d %H-%M-%S') + datetime.timedelta(day=1)\n                )\n\n                order.date = exact_time[0:10]\n                order.datetime = '{} 09:30:00'.format(order.date)\n            elif order.frequence in [FREQUENCE.ONE_MIN,\n                                     FREQUENCE.FIVE_MIN,\n                                     FREQUENCE.FIFTEEN_MIN,\n                                     FREQUENCE.THIRTY_MIN,\n                                     FREQUENCE.SIXTY_MIN]:\n                exact_time = str(\n                    datetime.datetime\n                    .strptime(order.datetime,\n                              '%Y-%m-%d %H-%M-%S') +\n                    datetime.timedelta(minute=1)\n                )\n                order.date = exact_time[0:10]\n                order.datetime = exact_time\n            self.market_data = self.get_market(order)\n            if self.market_data is None:\n                return order\n            if order.towards == 1:\n                order.price = float(self.market_data[\"high\"])\n            else:\n                order.price = float(self.market_data[\"low\"])\n\n        return order"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_filename():\n    return [(l[0],l[1]) for l in [line.strip().split(\",\") for line in requests.get(FINANCIAL_URL).text.strip().split('\\n')]]", "response": "get_filename - Get the filename of the last file in the sequence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget shanghai margin data", "response": "def QA_fetch_get_sh_margin(date):\n    \"\"\"return shanghai margin data\n\n    Arguments:\n        date {str YYYY-MM-DD} -- date format\n\n    Returns:\n        pandas.DataFrame -- res for margin data\n    \"\"\"\n    if date in trade_date_sse:\n        data= pd.read_excel(_sh_url.format(QA_util_date_str2int\n                                            (date)), 1).assign(date=date).assign(sse='sh')             \n        data.columns=['code','name','leveraged_balance','leveraged_buyout','leveraged_payoff','margin_left','margin_sell','margin_repay','date','sse']\n        return data\n    else:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_fetch_get_sz_margin(date):\n\n    if date in trade_date_sse:\n        return pd.read_excel(_sz_url.format(date)).assign(date=date).assign(sse='sz')", "response": "get the shenzhen margin data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upcoming_data(self, broker, data):\n        '''\n        \u66f4\u65b0\u5e02\u573a\u6570\u636e\n        broker \u4e3a\u540d\u5b57\uff0c\n        data \u662f\u5e02\u573a\u6570\u636e\n        \u88ab QABacktest \u4e2drun \u65b9\u6cd5\u8c03\u7528 upcoming_data\n        '''\n        # main thread'\n        # if self.running_time is not None and self.running_time!= data.datetime[0]:\n        #     for item in self.broker.keys():\n        #         self._settle(item)\n        self.running_time = data.datetime[0]\n        for account in self.session.values():\n            account.run(QA_Event(\n                event_type=ENGINE_EVENT.UPCOMING_DATA,\n                # args \u9644\u52a0\u7684\u53c2\u6570\n                market_data=data,\n                broker_name=broker,\n                send_order=self.insert_order,  # \ud83d\udee0todo insert_order = insert_order\n                query_data=self.query_data_no_wait,\n                query_order=self.query_order,\n                query_assets=self.query_assets,\n                query_trade=self.query_trade\n            ))", "response": "upcoming_data - \u53c2\u6570 broker \u4e3a\u540d\u5b57\uff0c data \u662f\u5e02\u573a\u6570\u636e broker \u4e3a\u540d\u5b57\uff0c data \u662f\u5e02\u573a\u6570\u636e broker \u4e3a\u540d\u5b57\uff0c data \u662f\u5e02\ufffd"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_order_threading(self):\n\n        self.if_start_orderthreading = True\n\n        self.order_handler.if_start_orderquery = True\n        self.trade_engine.create_kernel('ORDER', daemon=True)\n        self.trade_engine.start_kernel('ORDER')\n        self.sync_order_and_deal()", "response": "start threading the order"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef login(self, broker_name, account_cookie, account=None):\n        res = False\n        if account is None:\n            if account_cookie not in self.session.keys():\n                self.session[account_cookie] = QA_Account(\n                    account_cookie=account_cookie,\n                    broker=broker_name\n                )\n                if self.sync_account(broker_name, account_cookie):\n                    res = True\n\n                if self.if_start_orderthreading and res:\n                    #\n                    self.order_handler.subscribe(\n                        self.session[account_cookie],\n                        self.broker[broker_name]\n                    )\n\n        else:\n            if account_cookie not in self.session.keys():\n                account.broker = broker_name\n                self.session[account_cookie] = account\n                if self.sync_account(broker_name, account_cookie):\n                    res = True\n                if self.if_start_orderthreading and res:\n                    #\n                    self.order_handler.subscribe(\n                        account,\n                        self.broker[broker_name]\n                    )\n\n        if res:\n            return res\n        else:\n            try:\n                self.session.pop(account_cookie)\n            except:\n                pass\n            return False", "response": "login \u767b\u5f55\u5230\u4ea4\u6613\u524d\u7f6e\n\n        2018-07-02 \u5728\u5b9e\u76d8\u4e2d,\u767b\u5f55\u5230\u4ea4\u6613\u524d\u7f6e\u540e,\u9700\u8981\u540c\u6b65\u8d44\u4ea7\u72b6\u6001\n\n        Arguments:\n            broker_name {[type]} -- [description]\n            account_cookie {[type]} -- [description]\n\n        Keyword Arguments:\n            account {[type]} -- [description] (default: {None})\n\n        Returns:\n            [type] -- [description]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sync_account(self, broker_name, account_cookie):\n        try:\n            if isinstance(self.broker[broker_name], QA_BacktestBroker):\n                pass\n            else:\n                self.session[account_cookie].sync_account(\n                    self.broker[broker_name].query_positions(account_cookie)\n                )\n            return True\n        except Exception as e:\n            print(e)\n            return False", "response": "Syncs the account with the broker"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsettle order to broker", "response": "def settle_order(self):\n        \"\"\"\u4ea4\u6613\u524d\u7f6e\u7ed3\u7b97\n\n        1. \u56de\u6d4b: \u4ea4\u6613\u961f\u5217\u6e05\u7a7a,\u5f85\u4ea4\u6613\u961f\u5217\u6807\u8bb0SETTLE\n        2. \u8d26\u6237\u6bcf\u65e5\u7ed3\u7b97\n        3. broker\u7ed3\u7b97\u66f4\u65b0\n        \"\"\"\n\n        if self.if_start_orderthreading:\n\n            self.order_handler.run(\n                QA_Event(\n                    event_type=BROKER_EVENT.SETTLE,\n                    event_queue=self.trade_engine.kernels_dict['ORDER'].queue\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts pandas DataFrame to json", "response": "def QA_util_to_json_from_pandas(data):\n    \"\"\"\u9700\u8981\u5bf9\u4e8edatetime \u548cdate \u8fdb\u884c\u8f6c\u6362, \u4ee5\u514d\u76f4\u63a5\u88ab\u53d8\u6210\u4e86\u65f6\u95f4\u6233\"\"\"\n    if 'datetime' in data.columns:\n        data.datetime = data.datetime.apply(str)\n    if 'date' in data.columns:\n        data.date = data.date.apply(str)\n    return json.loads(data.to_json(orient='records'))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a string to a string", "response": "def QA_util_code_tostr(code):\n    \"\"\"\n    \u5c06\u6240\u6709\u6caa\u6df1\u80a1\u7968\u4ece\u6570\u5b57\u8f6c\u5316\u52306\u4f4d\u7684\u4ee3\u7801\n\n    \u56e0\u4e3a\u6709\u65f6\u5019\u5728csv\u7b49\u8f6c\u6362\u7684\u65f6\u5019,\u8bf8\u5982 000001\u7684\u80a1\u7968\u4f1a\u53d8\u6210office\u5f3a\u5236\u8f6c\u5316\u6210\u6570\u5b571\n\n    \"\"\"\n    if isinstance(code, int):\n        return \"{:>06d}\".format(code)\n    if isinstance(code, str):\n        # \u805a\u5bbd\u80a1\u7968\u4ee3\u7801\u683c\u5f0f '600000.XSHG'\n        # \u6398\u91d1\u80a1\u7968\u4ee3\u7801\u683c\u5f0f 'SHSE.600000'\n        # Wind\u80a1\u7968\u4ee3\u7801\u683c\u5f0f '600000.SH'\n        # \u5929\u8f6f\u80a1\u7968\u4ee3\u7801\u683c\u5f0f 'SH600000'\n        if len(code) == 6:\n            return code\n        if len(code) == 8:\n            # \u5929\u8f6f\u6570\u636e\n            return code[-6:]\n        if len(code) == 9:\n            return code[:6]\n        if len(code) == 11:\n            if code[0] in [\"S\"]:\n                return code.split(\".\")[1]\n            return code.split(\".\")[0]\n        raise ValueError(\"\u9519\u8bef\u7684\u80a1\u7968\u4ee3\u7801\u683c\u5f0f\")\n    if isinstance(code, list):\n        return QA_util_code_to_str(code[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsubscribe a specific strategy to a specific date.", "response": "def subscribe_strategy(\n            self,\n            strategy_id: str,\n            last: int,\n            today=datetime.date.today(),\n            cost_coins=10\n    ):\n        \"\"\"\u8ba2\u9605\u4e00\u4e2a\u7b56\u7565\n\n        \u4f1a\u6263\u51cf\u4f60\u7684\u79ef\u5206\n\n        Arguments:\n            strategy_id {str} -- [description]\n            last {int} -- [description]\n\n        Keyword Arguments:\n            today {[type]} -- [description] (default: {datetime.date.today()})\n            cost_coins {int} -- [description] (default: {10})\n        \"\"\"\n\n        if self.coins > cost_coins:\n            order_id = str(uuid.uuid1())\n            self._subscribed_strategy[strategy_id] = {\n                'lasttime':\n                last,\n                'start':\n                str(today),\n                'strategy_id':\n                strategy_id,\n                'end':\n                QA_util_get_next_day(\n                    QA_util_get_real_date(str(today),\n                                          towards=1),\n                    last\n                ),\n                'status':\n                'running',\n                'uuid':\n                order_id\n            }\n            self.coins -= cost_coins\n            self.coins_history.append(\n                [\n                    cost_coins,\n                    strategy_id,\n                    str(today),\n                    last,\n                    order_id,\n                    'subscribe'\n                ]\n            )\n            return True, order_id\n        else:\n            # return QAERROR.\n            return False, 'Not Enough Coins'"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unsubscribe_stratgy(self, strategy_id):\n\n        today = datetime.date.today()\n        order_id = str(uuid.uuid1())\n        if strategy_id in self._subscribed_strategy.keys():\n            self._subscribed_strategy[strategy_id]['status'] = 'canceled'\n\n        self.coins_history.append(\n            [0,\n             strategy_id,\n             str(today),\n             0,\n             order_id,\n             'unsubscribe']\n        )", "response": "Unsubscribe from a particular stratgy."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subscribing_strategy(self):\n\n        res = self.subscribed_strategy.assign(\n            remains=self.subscribed_strategy.end.apply(\n                lambda x: pd.Timestamp(x) - pd.Timestamp(datetime.date.today())\n            )\n        )\n        #res['left'] = res['end_time']\n        # res['remains']\n        res.assign(\n            status=res['remains'].apply(\n                lambda x: 'running'\n                if x > datetime.timedelta(days=0) else 'timeout'\n            )\n        )\n        return res.query('status==\"running\"')", "response": "Return a QuerySet of all the subscriptions that are in the scheduler."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new_portfolio(self, portfolio_cookie=None):\n        '''\n        \u6839\u636e self.user_cookie \u521b\u5efa\u4e00\u4e2a portfolio\n        :return:\n        \u5982\u679c\u5b58\u5728 \u8fd4\u56de \u65b0\u5efa\u7684 QA_Portfolio\n        \u5982\u679c\u5df2\u7ecf\u5b58\u5728 \u8fd4\u56de \u8fd9\u4e2aportfolio\n        '''\n        _portfolio = QA_Portfolio(\n            user_cookie=self.user_cookie,\n            portfolio_cookie=portfolio_cookie\n        )\n        if _portfolio.portfolio_cookie not in self.portfolio_list.keys():\n            self.portfolio_list[_portfolio.portfolio_cookie] = _portfolio\n            return _portfolio\n        else:\n            print(\n                \" prortfolio with user_cookie \",\n                self.user_cookie,\n                \" already exist!!\"\n            )\n            return self.portfolio_list[portfolio_cookie]", "response": "returns a new prortfolio object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_account(self, portfolio_cookie: str, account_cookie: str):\n\n        try:\n            return self.portfolio_list[portfolio_cookie][account_cookie]\n        except:\n            return None", "response": "Returns the account object for the given account_cookie"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_simpleaccount(self):\n        if len(self.portfolio_list.keys()) < 1:\n            po = self.new_portfolio()\n        else:\n            po = list(self.portfolio_list.values())[0]\n        ac = po.new_account()\n        return ac, po", "response": "make a simple account with a easier way\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering an account in the current portfolio.", "response": "def register_account(self, account, portfolio_cookie=None):\n        '''\n        \u6ce8\u518c\u4e00\u4e2aaccount\u5230portfolio\u7ec4\u5408\u4e2d\n        account \u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u7b56\u7565\u7c7b\uff0c\u5b9e\u73b0\u5176 on_bar \u65b9\u6cd5\n        :param account: \u88ab\u6ce8\u518c\u7684account\n        :return:\n        '''\n        # \u67e5\u627e portfolio\n        if len(self.portfolio_list.keys()) < 1:\n            po = self.new_portfolio()\n        elif portfolio_cookie is not None:\n            po = self.portfolio_list[portfolio_cookie]\n        else:\n            po = list(self.portfolio_list.values())[0]\n        # \u628aaccount \u6dfb\u52a0\u5230 portfolio\u4e2d\u53bb\n        po.add_account(account)\n        return (po, account)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self):\n        if self.wechat_id is not None:\n            self.client.update(\n                {'wechat_id': self.wechat_id},\n                {'$set': self.message},\n                upsert=True\n            )\n        else:\n            self.client.update(\n                {\n                    'username': self.username,\n                    'password': self.password\n                },\n                {'$set': self.message},\n                upsert=True\n            )\n\n        # user ==> portfolio \u7684\u5b58\u50a8\n        # account\u7684\u5b58\u50a8\u5728  portfolio.save ==> account.save \u4e2d\n        for portfolio in list(self.portfolio_list.values()):\n            portfolio.save()", "response": "Save user and portfolio"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reload(self, message):\n\n        self.phone = message.get('phone')\n        self.level = message.get('level')\n        self.utype = message.get('utype')\n        self.coins = message.get('coins')\n        self.wechat_id = message.get('wechat_id')\n        self.coins_history = message.get('coins_history')\n        self.money = message.get('money')\n        self._subscribed_strategy = message.get('subuscribed_strategy')\n        self._subscribed_code = message.get('subscribed_code')\n        self.username = message.get('username')\n        self.password = message.get('password')\n        self.user_cookie = message.get('user_cookie')\n        #\n        portfolio_list = [item['portfolio_cookie'] for item in DATABASE.portfolio.find(\n            {'user_cookie': self.user_cookie}, {'portfolio_cookie': 1, '_id': 0})]\n\n        # portfolio_list = message.get('portfolio_list')\n        if len(portfolio_list) > 0:\n            self.portfolio_list = dict(\n                zip(\n                    portfolio_list,\n                    [\n                        QA_Portfolio(\n                            user_cookie=self.user_cookie,\n                            portfolio_cookie=item\n                        ) for item in portfolio_list\n                    ]\n                )\n            )\n        else:\n            self.portfolio_list = {}", "response": "Reloads the internal state of the object from the given message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_util_format_date2str(cursor_date):\n    if isinstance(cursor_date, datetime.datetime):\n        cursor_date = str(cursor_date)[:10]\n    elif isinstance(cursor_date, str):\n        try:\n            cursor_date = str(pd.Timestamp(cursor_date))[:10]\n        except:\n            raise ValueError('\u8bf7\u8f93\u5165\u6b63\u786e\u7684\u65e5\u671f\u683c\u5f0f, \u5efa\u8bae \"%Y-%m-%d\"')\n    elif isinstance(cursor_date, int):\n        cursor_date = str(pd.Timestamp(\"{:<014d}\".format(cursor_date)))[:10]\n    else:\n        raise ValueError('\u8bf7\u8f93\u5165\u6b63\u786e\u7684\u65e5\u671f\u683c\u5f0f\uff0c\u5efa\u8bae \"%Y-%m-%d\"')\n    return cursor_date", "response": "QA - util - format date to str"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets next trade date", "response": "def QA_util_get_next_trade_date(cursor_date, n=1):\n    \"\"\"\n    \u5f97\u5230\u4e0b n \u4e2a\u4ea4\u6613\u65e5 (\u4e0d\u5305\u542b\u5f53\u524d\u4ea4\u6613\u65e5)\n    :param date:\n    :param n:\n    \"\"\"\n\n    cursor_date = QA_util_format_date2str(cursor_date)\n    if cursor_date in trade_date_sse:\n        # \u5982\u679c\u6307\u5b9a\u65e5\u671f\u4e3a\u4ea4\u6613\u65e5\n        return QA_util_date_gap(cursor_date, n, \"gt\")\n    real_pre_trade_date = QA_util_get_real_date(cursor_date)\n    return QA_util_date_gap(real_pre_trade_date, n, \"gt\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_util_get_pre_trade_date(cursor_date, n=1):\n\n    cursor_date = QA_util_format_date2str(cursor_date)\n    if cursor_date in trade_date_sse:\n        return QA_util_date_gap(cursor_date, n, \"lt\")\n    real_aft_trade_date = QA_util_get_real_date(cursor_date)\n    return QA_util_date_gap(real_aft_trade_date, n, \"lt\")", "response": "get pre trade date"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef QA_util_if_tradetime(\n        _time=datetime.datetime.now(),\n        market=MARKET_TYPE.STOCK_CN,\n        code=None\n):\n    '\u65f6\u95f4\u662f\u5426\u4ea4\u6613'\n    _time = datetime.datetime.strptime(str(_time)[0:19], '%Y-%m-%d %H:%M:%S')\n    if market is MARKET_TYPE.STOCK_CN:\n        if QA_util_if_trade(str(_time.date())[0:10]):\n            if _time.hour in [10, 13, 14]:\n                return True\n            elif _time.hour in [\n                    9\n            ] and _time.minute >= 15:  # \u4fee\u6539\u62109:15 \u52a0\u5165 9:15-9:30\u7684\u76d8\u524d\u7ade\u4ef7\u65f6\u95f4\n                return True\n            elif _time.hour in [11] and _time.minute <= 30:\n                return True\n            else:\n                return False\n        else:\n            return False\n    elif market is MARKET_TYPE.FUTURE_CN:                              \n        date_today=str(_time.date())    \n        date_yesterday=str((_time-datetime.timedelta(days=1)).date())                         \n        \n        is_today_open=QA_util_if_trade(date_today)\n        is_yesterday_open=QA_util_if_trade(date_yesterday)\n        \n        #\u8003\u8651\u5468\u516d\u65e5\u7684\u671f\u8d27\u591c\u76d8\u60c5\u51b5\n        if is_today_open==False: #\u53ef\u80fd\u662f\u5468\u516d\u6216\u8005\u5468\u65e5\n            if is_yesterday_open==False or (_time.hour > 2 or _time.hour == 2 and _time.minute > 30):\n                return False\n\n        shortName = \"\"                      # i , p\n        for i in range(len(code)):\n            ch = code[i]\n            if ch.isdigit():                # ch >= 48 and ch <= 57:\n                break\n            shortName += code[i].upper()\n\n        period = [\n            [9, 0, 10, 15],\n            [10, 30, 11, 30],\n            [13, 30, 15, 0]\n        ]\n        \n        if (shortName in [\"IH\", 'IF', 'IC']):\n            period = [\n                [9, 30, 11, 30],\n                [13, 0, 15, 0]\n            ]\n        elif (shortName in [\"T\", \"TF\"]):\n            period = [\n                [9, 15, 11, 30],\n                [13, 0, 15, 15]\n            ]\n        \n        if 0<=_time.weekday<=4:\n            for i in range(len(period)):\n                p = period[i]\n                if ((_time.hour > p[0] or (_time.hour == p[0] and _time.minute >= p[1])) and (_time.hour < p[2] or (_time.hour == p[2] and _time.minute < p[3]))):\n                    return True\n\n        #\u6700\u65b0\u591c\u76d8\u65f6\u95f4\u8868_2019.03.29\n        nperiod = [\n            [\n                ['AU', 'AG', 'SC'],\n                [21, 0, 2, 30]  \n            ],\n            [\n                ['CU', 'AL', 'ZN', 'PB', 'SN', 'NI'],\n                [21, 0, 1, 0]   \n            ],\n            [\n                ['RU', 'RB', 'HC', 'BU','FU','SP'],\n                [21, 0, 23, 0]\n            ],\n            [\n                ['A', 'B', 'Y', 'M', 'JM', 'J', 'P', 'I', 'L', 'V', 'PP', 'EG', 'C', 'CS'],\n                [21, 0, 23, 0]\n            ],\n            [\n                ['SR', 'CF', 'RM', 'MA', 'TA', 'ZC', 'FG', 'IO', 'CY'],\n                [21, 0, 23, 30]\n            ],\n        ]\n\n        for i in range(len(nperiod)):\n            for j in range(len(nperiod[i][0])):\n                if nperiod[i][0][j] == shortName:\n                    p = nperiod[i][1]\n                    condA = _time.hour > p[0] or (_time.hour == p[0] and _time.minute >= p[1])\n                    condB = _time.hour < p[2] or (_time.hour == p[2] and _time.minute < p[3])\n                    # in one day\n                    if p[2] >= p[0]:\n                        if ((_time.weekday >= 0 and _time.weekday <= 4) and condA and condB):\n                            return True\n                    else:\n                        if (((_time.weekday >= 0 and _time.weekday <= 4) and condA) or ((_time.weekday >= 1 and _time.weekday <= 5) and condB)):\n                            return True\n                    return False\n        return False", "response": "check if time is in the time range"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_util_get_real_date(date, trade_list=trade_date_sse, towards=-1):\n    date = str(date)[0:10]\n    if towards == 1:\n        while date not in trade_list:\n            date = str(\n                datetime.datetime.strptime(str(date)[0:10],\n                                           '%Y-%m-%d') +\n                datetime.timedelta(days=1)\n            )[0:10]\n        else:\n            return str(date)[0:10]\n    elif towards == -1:\n        while date not in trade_list:\n            date = str(\n                datetime.datetime.strptime(str(date)[0:10],\n                                           '%Y-%m-%d') -\n                datetime.timedelta(days=1)\n            )[0:10]\n        else:\n            return str(date)[0:10]", "response": "get real date from date"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_util_get_trade_gap(start, end):\n    '\u8fd4\u56destart_day\u5230end_day\u4e2d\u95f4\u6709\u591a\u5c11\u4e2a\u4ea4\u6613\u5929 \u7b97\u9996\u5c3e'\n    start, end = QA_util_get_real_datelist(start, end)\n    if start is not None:\n        return trade_date_sse.index(end) + 1 - trade_date_sse.index(start)\n    else:\n        return 0", "response": "get the gap between start and end"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_util_date_gap(date, gap, methods):\n    '''\n    :param date: \u5b57\u7b26\u4e32\u8d77\u59cb\u65e5 \u7c7b\u578b str eg: 2018-11-11\n    :param gap: \u6574\u6570 \u95f4\u9694\u591a\u6570\u4e2a\u4ea4\u6613\u65e5\n    :param methods:  gt\u5927\u4e8e \uff0cgte \u5927\u4e8e\u7b49\u4e8e\uff0c \u5c0f\u4e8elt \uff0c\u5c0f\u4e8e\u7b49\u4e8elte \uff0c \u7b49\u4e8e===\n    :return: \u5b57\u7b26\u4e32 eg\uff1a2000-01-01\n    '''\n    try:\n        if methods in ['>', 'gt']:\n            return trade_date_sse[trade_date_sse.index(date) + gap]\n        elif methods in ['>=', 'gte']:\n            return trade_date_sse[trade_date_sse.index(date) + gap - 1]\n        elif methods in ['<', 'lt']:\n            return trade_date_sse[trade_date_sse.index(date) - gap]\n        elif methods in ['<=', 'lte']:\n            return trade_date_sse[trade_date_sse.index(date) - gap + 1]\n        elif methods in ['==', '=', 'eq']:\n            return date\n\n    except:\n        return 'wrong date'", "response": "QA_util_date_gap - \u53d6\u5f97 \u5b57\u7b26\u4e32\u8d77\u59cb\u65e5 \u7c7b\u578b str eg : 2018 - 11 - 11"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_util_get_trade_datetime(dt=datetime.datetime.now()):\n\n    #dt= datetime.datetime.now()\n\n    if QA_util_if_trade(str(dt.date())) and dt.time() < datetime.time(15, 0, 0):\n        return str(dt.date())\n    else:\n        return QA_util_get_real_date(str(dt.date()), trade_date_sse, 1)", "response": "get the date of the last trade"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_util_get_order_datetime(dt):\n\n    #dt= datetime.datetime.now()\n    dt = datetime.datetime.strptime(str(dt)[0:19], '%Y-%m-%d %H:%M:%S')\n\n    if QA_util_if_trade(str(dt.date())) and dt.time() < datetime.time(15, 0, 0):\n        return str(dt)\n    else:\n        # print('before')\n        # print(QA_util_date_gap(str(dt.date()),1,'lt'))\n        return '{} {}'.format(\n            QA_util_date_gap(str(dt.date()),\n                             1,\n                             'lt'),\n            dt.time()\n        )", "response": "\u59d4\u6258\u7684\u771f\u5b9e\u65e5\u671f\n\n    Returns:\n        [type] -- [description]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction to convert a datetime from a future to a traded datetime", "response": "def QA_util_future_to_tradedatetime(real_datetime):\n    \"\"\"\u8f93\u5165\u662f\u771f\u5b9e\u4ea4\u6613\u65f6\u95f4,\u8fd4\u56de\u6309\u671f\u8d27\u4ea4\u6613\u6240\u89c4\u5b9a\u7684\u65f6\u95f4* \u9002\u7528\u4e8etb/\u6587\u534e/\u535a\u5f08\u7684\u8f6c\u6362\n\n    Arguments:\n        real_datetime {[type]} -- [description]\n\n    Returns:\n        [type] -- [description]\n    \"\"\"\n    if len(str(real_datetime)) >= 19:\n        dt = datetime.datetime.strptime(\n            str(real_datetime)[0:19],\n            '%Y-%m-%d %H:%M:%S'\n        )\n        return dt if dt.time(\n        ) < datetime.time(21,\n                          0) else QA_util_get_next_datetime(dt,\n                                                            1)\n    elif len(str(real_datetime)) == 16:\n        dt = datetime.datetime.strptime(\n            str(real_datetime)[0:16],\n            '%Y-%m-%d %H:%M'\n        )\n        return dt if dt.time(\n        ) < datetime.time(21,\n                          0) else QA_util_get_next_datetime(dt,\n                                                            1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef QA_util_future_to_realdatetime(trade_datetime):\n    if len(str(trade_datetime)) == 19:\n        dt = datetime.datetime.strptime(\n            str(trade_datetime)[0:19],\n            '%Y-%m-%d %H:%M:%S'\n        )\n        return dt if dt.time(\n        ) < datetime.time(21,\n                          0) else QA_util_get_last_datetime(dt,\n                                                            1)\n    elif len(str(trade_datetime)) == 16:\n        dt = datetime.datetime.strptime(\n            str(trade_datetime)[0:16],\n            '%Y-%m-%d %H:%M'\n        )\n        return dt if dt.time(\n        ) < datetime.time(21,\n                          0) else QA_util_get_last_datetime(dt,\n                                                            1)", "response": "Function to convert a future datetime to real datetime"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_util_make_hour_index(day, type_='1h'):\n\n    if QA_util_if_trade(day) is True:\n        return pd.date_range(\n            str(day) + ' 09:30:00',\n            str(day) + ' 11:30:00',\n            freq=type_,\n            closed='right'\n        ).append(\n            pd.date_range(\n                str(day) + ' 13:00:00',\n                str(day) + ' 15:00:00',\n                freq=type_,\n                closed='right'\n            )\n        )\n    else:\n        return pd.DataFrame(['No trade'])", "response": "This function creates a DataFrame that represents the index of the current day in the order of the year."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave a list of items to a CSV file", "response": "def QA_util_save_csv(data, name, column=None, location=None):\n    # \u91cd\u5199\u4e86\u4e00\u4e0b\u4fdd\u5b58\u7684\u6a21\u5f0f\n    # \u589e\u52a0\u4e86\u5bf9\u4e8e\u53ef\u8fed\u4ee3\u5bf9\u8c61\u7684\u5224\u65ad 2017/8/10\n    \"\"\"\n    QA_util_save_csv(data,name,column,location)\n\n    \u5c06list\u4fdd\u5b58\u6210csv\n    \u7b2c\u4e00\u4e2a\u53c2\u6570\u662flist\n    \u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u8981\u4fdd\u5b58\u7684\u540d\u5b57\n    \u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u884c\u7684\u540d\u79f0(\u53ef\u9009)\n    \u7b2c\u56db\u4e2a\u662f\u4fdd\u5b58\u4f4d\u7f6e(\u53ef\u9009)\n\n    @yutiansut\n    \"\"\"\n    assert isinstance(data, list)\n    if location is None:\n        path = './' + str(name) + '.csv'\n    else:\n        path = location + str(name) + '.csv'\n    with open(path, 'w', newline='') as f:\n        csvwriter = csv.writer(f)\n        if column is None:\n            pass\n        else:\n            csvwriter.writerow(column)\n\n        for item in data:\n\n            if isinstance(item, list):\n                csvwriter.writerow(item)\n            else:\n                csvwriter.writerow([item])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_positions(self, accounts):\n        try:\n            data = self.call(\"positions\", {'client': accounts})\n            if data is not None:\n                cash_part = data.get('subAccounts', {}).get('\u4eba\u6c11\u5e01', False)\n                if cash_part:\n                    cash_available = cash_part.get('\u53ef\u7528\u91d1\u989d', cash_part.get('\u53ef\u7528'))\n\n                position_part = data.get('dataTable', False)\n                if position_part:\n                    res = data.get('dataTable', False)\n                    if res:\n                        hold_headers = res['columns']\n                        hold_headers = [\n                            cn_en_compare[item] for item in hold_headers\n                        ]\n                        hold_available = pd.DataFrame(\n                            res['rows'],\n                            columns=hold_headers\n                        )\n                if len(hold_available) == 1 and hold_available.amount[0] in [\n                        None,\n                        '',\n                        0\n                ]:\n                    hold_available = pd.DataFrame(\n                        data=None,\n                        columns=hold_headers\n                    )\n                return {\n                    'cash_available':\n                    cash_available,\n                    'hold_available':\n                    hold_available.assign(\n                        amount=hold_available.amount.apply(float)\n                    ).loc[:,\n                          ['code',\n                           'amount']].set_index('code').amount\n                }\n            else:\n                print(data)\n                return False, 'None ACCOUNT'\n        except:\n            return False", "response": "Query the cash and hold available for a list of accounts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef query_clients(self):\n\n        try:\n            data = self.call(\"clients\", {'client': 'None'})\n            if len(data) > 0:\n                return pd.DataFrame(data).drop(\n                    ['commandLine',\n                     'processId'],\n                    axis=1\n                )\n            else:\n                return pd.DataFrame(\n                    None,\n                    columns=[\n                        'id',\n                        'name',\n                        'windowsTitle',\n                        'accountInfo',\n                        'status'\n                    ]\n                )\n        except Exception as e:\n            return False, e", "response": "Query clients in a specific language"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries all orders for a list of accounts", "response": "def query_orders(self, accounts, status='filled'):\n        \"\"\"\u67e5\u8be2\u8ba2\u5355\n\n        Arguments:\n            accounts {[type]} -- [description]\n\n        Keyword Arguments:\n            status {str} -- 'open' \u5f85\u6210\u4ea4 'filled' \u6210\u4ea4 (default: {'filled'})\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n        try:\n            data = self.call(\"orders\", {'client': accounts, 'status': status})\n\n            if data is not None:\n                orders = data.get('dataTable', False)\n\n                order_headers = orders['columns']\n                if ('\u6210\u4ea4\u72b6\u6001' in order_headers\n                        or '\u72b6\u6001\u8bf4\u660e' in order_headers) and ('\u5907\u6ce8' in order_headers):\n                    order_headers[order_headers.index('\u5907\u6ce8')] = '\u5e9f\u5f03'\n\n                order_headers = [cn_en_compare[item] for item in order_headers]\n                order_all = pd.DataFrame(\n                    orders['rows'],\n                    columns=order_headers\n                ).assign(account_cookie=accounts)\n\n                order_all.towards = order_all.towards.apply(\n                    lambda x: trade_towards_cn_en[x]\n                )\n                if 'order_time' in order_headers:\n                    # \u8fd9\u662forder_status\n                    order_all['status'] = order_all.status.apply(\n                        lambda x: order_status_cn_en[x]\n                    )\n                    if 'order_date' not in order_headers:\n                        order_all.order_time = order_all.order_time.apply(\n                            lambda x: QA_util_get_order_datetime(\n                                dt='{} {}'.format(datetime.date.today(),\n                                                  x)\n                            )\n                        )\n                    else:\n                        order_all = order_all.assign(\n                            order_time=order_all.order_date\n                            .apply(QA_util_date_int2str) + ' ' +\n                            order_all.order_time\n                        )\n\n                if 'trade_time' in order_headers:\n\n                    order_all.trade_time = order_all.trade_time.apply(\n                        lambda x: '{} {}'.format(datetime.date.today(),\n                                                 x)\n                    )\n\n                if status is 'filled':\n                    return order_all.loc[:,\n                                         self.dealstatus_headers].set_index(\n                                             ['account_cookie',\n                                              'realorder_id']\n                    ).sort_index()\n                else:\n                    return order_all.loc[:,\n                                         self.orderstatus_headers].set_index(\n                                             ['account_cookie',\n                                              'realorder_id']\n                    ).sort_index()\n            else:\n                print('response is None')\n                return False\n        except Exception as e:\n            print(e)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_order(\n            self,\n            accounts,\n            code='000001',\n            price=9,\n            amount=100,\n            order_direction=ORDER_DIRECTION.BUY,\n            order_model=ORDER_MODEL.LIMIT\n    ):\n        \"\"\"[summary]\n\n        Arguments:\n            accounts {[type]} -- [description]\n            code {[type]} -- [description]\n            price {[type]} -- [description]\n            amount {[type]} -- [description]\n\n        Keyword Arguments:\n            order_direction {[type]} -- [description] (default: {ORDER_DIRECTION.BUY})\n            order_model {[type]} -- [description] (default: {ORDER_MODEL.LIMIT})\n\n\n\n        priceType \u53ef\u9009\u62e9\uff1a \u4e0a\u6d77\u4ea4\u6613\u6240\uff1a\n\n            0 - \u9650\u4ef7\u59d4\u6258\n            4 - \u4e94\u6863\u5373\u65f6\u6210\u4ea4\u5269\u4f59\u64a4\u9500\n            6 - \u4e94\u6863\u5373\u65f6\u6210\u4ea4\u5269\u4f59\u8f6c\u9650\n\n        \u6df1\u5733\u4ea4\u6613\u6240\uff1a\n\n            0 - \u9650\u4ef7\u59d4\u6258\n            1 - \u5bf9\u624b\u65b9\u6700\u4f18\u4ef7\u683c\u59d4\u6258\n            2 - \u672c\u65b9\u6700\u4f18\u4ef7\u683c\u59d4\u6258\n            3 - \u5373\u65f6\u6210\u4ea4\u5269\u4f59\u64a4\u9500\u59d4\u6258\n            4 - \u4e94\u6863\u5373\u65f6\u6210\u4ea4\u5269\u4f59\u64a4\u9500\n            5 - \u5168\u989d\u6210\u4ea4\u6216\u64a4\u9500\u59d4\u6258\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n        try:\n            #print(code, price, amount)\n            return self.call_post(\n                'orders',\n                {\n                    'client': accounts,\n                    \"action\": 'BUY' if order_direction == 1 else 'SELL',\n                    \"symbol\": code,\n                    \"type\": order_model,\n                    \"priceType\": 0 if order_model == ORDER_MODEL.LIMIT else 4,\n                    \"price\": price,\n                    \"amount\": amount\n                }\n            )\n        except json.decoder.JSONDecodeError:\n            print(RuntimeError('TRADE ERROR'))\n            return None", "response": "Send an order to the specified list of accounts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a specific indicator from the data table.", "response": "def get_indicator(self, time, code, indicator_name=None):\n        \"\"\"\n        \u83b7\u53d6\u67d0\u4e00\u65f6\u95f4\u7684\u67d0\u4e00\u53ea\u80a1\u7968\u7684\u6307\u6807\n        \"\"\"\n        try:\n            return self.data.loc[(pd.Timestamp(time), code), indicator_name]\n        except:\n            raise ValueError('CANNOT FOUND THIS DATE&CODE')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_timerange(self, start, end, code=None):\n        try:\n            return self.data.loc[(slice(pd.Timestamp(start), pd.Timestamp(end)), slice(code)), :]\n        except:\n            return ValueError('CANNOT FOUND THIS TIME RANGE')", "response": "Get a list of timerange."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves stock terminated data to database", "response": "def QA_SU_save_stock_terminated(client=DATABASE):\n    '''\n    \u83b7\u53d6\u5df2\u7ecf\u88ab\u7ec8\u6b62\u4e0a\u5e02\u7684\u80a1\u7968\u5217\u8868\uff0c\u6570\u636e\u4ece\u4e0a\u4ea4\u6240\u83b7\u53d6\uff0c\u76ee\u524d\u53ea\u6709\u5728\u4e0a\u6d77\u8bc1\u5238\u4ea4\u6613\u6240\u4ea4\u6613\u88ab\u7ec8\u6b62\u7684\u80a1\u7968\u3002\n    collection\uff1a\n        code\uff1a\u80a1\u7968\u4ee3\u7801 name\uff1a\u80a1\u7968\u540d\u79f0 oDate:\u4e0a\u5e02\u65e5\u671f tDate:\u7ec8\u6b62\u4e0a\u5e02\u65e5\u671f\n    :param client:\n    :return: None\n    '''\n\n    # \ud83d\udee0todo \u5df2\u7ecf\u5931\u6548\u4ecewind \u8d44\u8baf\u91cc\u83b7\u53d6\n    # \u8fd9\u4e2a\u51fd\u6570\u5df2\u7ecf\u5931\u6548\n    print(\"\uff01\uff01\uff01 tushare \u8fd9\u4e2a\u51fd\u6570\u5df2\u7ecf\u5931\u6548\uff01\uff01\uff01\")\n    df = QATs.get_terminated()\n    #df = QATs.get_suspended()\n    print(\n        \" Get stock terminated from tushare,stock count is %d  (\u7ec8\u6b62\u4e0a\u5e02\u80a1\u7968\u5217\u8868)\" %\n        len(df)\n    )\n    coll = client.stock_terminated\n    client.drop_collection(coll)\n    json_data = json.loads(df.reset_index().to_json(orient='records'))\n    coll.insert(json_data)\n    print(\" \u4fdd\u5b58\u7ec8\u6b62\u4e0a\u5e02\u80a1\u7968\u5217\u8868 \u5230 stock_terminated collection\uff0c OK\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_SU_save_stock_day(client=DATABASE, ui_log=None, ui_progress=None):\n    '''\n     save stock_day\n    \u4fdd\u5b58\u65e5\u7ebf\u6570\u636e\n    :param client:\n    :param ui_log:  \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress_int_value: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    '''\n    stock_list = QA_fetch_get_stock_list()\n    # TODO: \u91cd\u547d\u540dstock_day_ts\n    coll_stock_day = client.stock_day_ts\n    coll_stock_day.create_index(\n        [(\"code\",\n          pymongo.ASCENDING),\n         (\"date_stamp\",\n          pymongo.ASCENDING)]\n    )\n\n    err = []\n    num_stocks = len(stock_list)\n    for index, ts_code in enumerate(stock_list):\n        QA_util_log_info('The {} of Total {}'.format(index, num_stocks))\n\n        strProgressToLog = 'DOWNLOAD PROGRESS {} {}'.format(\n            str(float(index / num_stocks * 100))[0:4] + '%',\n            ui_log\n        )\n        intProgressToLog = int(float(index / num_stocks * 100))\n        QA_util_log_info(\n            strProgressToLog,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intProgressToLog\n        )\n        _saving_work(ts_code,\n                     coll_stock_day,\n                     ui_log=ui_log,\n                     err=err)\n        # \u65e5\u7ebf\u884c\u60c5\u6bcf\u5206\u949f\u5185\u6700\u591a\u8c03\u53d6200\u6b21\uff0c\u8d85\u8fc75000\u79ef\u5206\u65e0\u9650\u5236\n        time.sleep(0.005)\n\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS save stock day ^_^', ui_log)\n    else:\n        QA_util_log_info('ERROR CODE \\n ', ui_log)\n        QA_util_log_info(err, ui_log)", "response": "save stock_day\n    \u4fdd\u5b58\u65e5\u7ebf\u6570\u636e\n    :param client:\n    :param ui_log:  \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress_int_value: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef QA_util_dict_remove_key(dicts, key):\n\n    if isinstance(key, list):\n        for item in key:\n            try:\n                dicts.pop(item)\n            except:\n                pass\n    else:\n        try:\n            dicts.pop(key)\n        except:\n            pass\n    return dicts", "response": "Remove a key from a list of dicts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to set the async IOMotorClient", "response": "def QA_util_sql_async_mongo_setting(uri='mongodb://localhost:27017/quantaxis'):\n    \"\"\"\u5f02\u6b65mongo\u793a\u4f8b\n\n    Keyword Arguments:\n        uri {str} -- [description] (default: {'mongodb://localhost:27017/quantaxis'})\n\n    Returns:\n        [type] -- [description]\n    \"\"\"\n    # loop = asyncio.new_event_loop()\n    # asyncio.set_event_loop(loop)\n    try:\n        loop = asyncio.get_event_loop()\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n    # async def client():\n    return AsyncIOMotorClient(uri, io_loop=loop)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drop_account(self, account_cookie):\n\n        if account_cookie in self.account_list:\n            res = self.account_list.remove(account_cookie)\n            self.cash.append(\n                self.cash[-1] + self.get_account_by_cookie(res).init_cash)\n            return True\n        else:\n            raise RuntimeError(\n                'account {} is not in the portfolio'.format(account_cookie)\n            )", "response": "Drop an account from the portfolio"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new_account(\n            self,\n            account_cookie=None,\n            init_cash=1000000,\n            market_type=MARKET_TYPE.STOCK_CN,\n            *args,\n            **kwargs\n    ):\n        \"\"\"\u521b\u5efa\u4e00\u4e2a\u65b0\u7684Account\n\n        Keyword Arguments:\n            account_cookie {[type]} -- [description] (default: {None})\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        if account_cookie is None:\n            \"\"\"\u521b\u5efa\u65b0\u7684account\n\n            Returns:\n                [type] -- [description]\n            \"\"\"\n            # \u5982\u679c\u7ec4\u5408\u7684cash_available>\u521b\u5efa\u65b0\u7684account\u6240\u9700cash\n            if self.cash_available >= init_cash:\n\n                temp = QA_Account(\n                    user_cookie=self.user_cookie,\n                    portfolio_cookie=self.portfolio_cookie,\n                    init_cash=init_cash,\n                    market_type=market_type,\n                    *args,\n                    **kwargs\n                )\n                if temp.account_cookie not in self.account_list:\n                    #self.accounts[temp.account_cookie] = temp\n                    self.account_list.append(temp.account_cookie)\n                    temp.save()\n                    self.cash.append(self.cash_available - init_cash)\n                    return temp\n\n                else:\n                    return self.new_account()\n        else:\n            if self.cash_available >= init_cash:\n                if account_cookie not in self.account_list:\n\n                    acc = QA_Account(\n                        portfolio_cookie=self.portfolio_cookie,\n                        user_cookie=self.user_cookie,\n                        init_cash=init_cash,\n                        market_type=market_type,\n                        account_cookie=account_cookie,\n                        *args,\n                        **kwargs\n                    )\n                    acc.save()\n                    self.account_list.append(acc.account_cookie)\n                    self.cash.append(self.cash_available - init_cash)\n                    return acc\n                else:\n                    return self.get_account_by_cookie(account_cookie)", "response": "Function to create a new account in the database"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_account_by_cookie(self, cookie):\n        '''\n        'give the account_cookie and return the account/strategy back'\n        :param cookie:\n        :return: QA_Account with cookie if in dict\n                 None not in list\n        '''\n        try:\n            return QA_Account(\n                account_cookie=cookie,\n                user_cookie=self.user_cookie,\n                portfolio_cookie=self.portfolio_cookie,\n                auto_reload=True\n            )\n        except:\n            QA_util_log_info('Can not find this account')\n            return None", "response": "give the account_cookie and return the account / strategy back"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the account by its cookie", "response": "def get_account(self, account):\n        '''\n        check the account whether in the protfolio dict or not\n        :param account:  QA_Account\n        :return: QA_Account if in dict\n                 None not in list\n        '''\n        try:\n            return self.get_account_by_cookie(account.account_cookie)\n        except:\n            QA_util_log_info(\n                'Can not find this account with cookies %s' %\n                account.account_cookie\n            )\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dict of the current state of the object.", "response": "def message(self):\n        \"\"\"portfolio \u7684cookie\n        \"\"\"\n        return {\n            'user_cookie': self.user_cookie,\n            'portfolio_cookie': self.portfolio_cookie,\n            'account_list': list(self.account_list),\n            'init_cash': self.init_cash,\n            'cash': self.cash,\n            'history': self.history\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_order(\n            self,\n            account_cookie: str,\n            code=None,\n            amount=None,\n            time=None,\n            towards=None,\n            price=None,\n            money=None,\n            order_model=None,\n            amount_model=None,\n            *args,\n            **kwargs\n    ):\n        \"\"\"\u57fa\u4e8eportfolio\u5bf9\u5b50\u8d26\u6237\u4e0b\u5355\n\n        Arguments:\n            account_cookie {str} -- [description]\n\n        Keyword Arguments:\n            code {[type]} -- [description] (default: {None})\n            amount {[type]} -- [description] (default: {None})\n            time {[type]} -- [description] (default: {None})\n            towards {[type]} -- [description] (default: {None})\n            price {[type]} -- [description] (default: {None})\n            money {[type]} -- [description] (default: {None})\n            order_model {[type]} -- [description] (default: {None})\n            amount_model {[type]} -- [description] (default: {None})\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        return self.get_account_by_cookie(account_cookie).send_order(\n            code=code,\n            amount=amount,\n            time=time,\n            towards=towards,\n            price=price,\n            money=money,\n            order_model=order_model,\n            amount_model=amount_model\n        )", "response": "send_order - Sends an order to a single node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self):\n        self.client.update(\n            {\n                'portfolio_cookie': self.portfolio_cookie,\n                'user_cookie': self.user_cookie\n            },\n            {'$set': self.message},\n            upsert=True\n        )", "response": "Save the current object to the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a DataFrame with the market value", "response": "def market_value(self):\n        \"\"\"\u6bcf\u65e5\u6bcf\u4e2a\u80a1\u7968\u6301\u4ed3\u5e02\u503c\u8868\n\n        Returns:\n            pd.DataFrame -- \u5e02\u503c\u8868\n        \"\"\"\n        if self.account.daily_hold is not None:\n            if self.if_fq:\n\n                return (\n                    self.market_data.to_qfq().pivot('close').fillna(\n                        method='ffill'\n                    ) * self.account.daily_hold.apply(abs)\n                ).fillna(method='ffill')\n            else:\n                return (\n                    self.market_data.pivot('close').fillna(method='ffill') *\n                    self.account.daily_hold.apply(abs)\n                ).fillna(method='ffill')\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef max_dropback(self):\n        return round(\n            float(\n                max(\n                    [\n                        (self.assets.iloc[idx] - self.assets.iloc[idx::].min())\n                        / self.assets.iloc[idx]\n                        for idx in range(len(self.assets))\n                    ]\n                )\n            ),\n            2\n        )", "response": "Return the max dropback of the current asset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing a dictionary of the neccesary properties of the neccesary object.", "response": "def profit_construct(self):\n        \"\"\"\u5229\u6da6\u6784\u6210\n\n        Returns:\n            dict -- \u5229\u6da6\u6784\u6210\u8868\n        \"\"\"\n\n        return {\n            'total_buyandsell':\n            round(\n                self.profit_money - self.total_commission - self.total_tax,\n                2\n            ),\n            'total_tax':\n            self.total_tax,\n            'total_commission':\n            self.total_commission,\n            'total_profit':\n            self.profit_money\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef profit_money(self):\n\n        return float(round(self.assets.iloc[-1] - self.assets.iloc[0], 2))", "response": "\u76c8\u5229\u989d\n\n        Returns:\n            [type] -- [description]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef annualize_return(self):\n\n        return round(\n            float(self.calc_annualize_return(self.assets,\n                                             self.time_gap)),\n            2\n        )", "response": "\u5e74\u5316\u6536\u76ca\n\n        Returns:\n            [type] -- [description]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef benchmark_assets(self):\n        return (\n            self.benchmark_data.close /\n            float(self.benchmark_data.close.iloc[0])\n            * float(self.assets[0])\n        )", "response": "get the total amount of assets"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef benchmark_annualize_return(self):\n\n        return round(\n            float(\n                self.calc_annualize_return(\n                    self.benchmark_assets,\n                    self.time_gap\n                )\n            ),\n            2\n        )", "response": "\u57fa\u51c6\u7ec4\u5408\u7684\u5e74\u5316\u6536\u76ca\n\n        Returns:\n            [type] -- [description]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget beta of the current state", "response": "def beta(self):\n        \"\"\"\n        beta\u6bd4\u7387 \u7ec4\u5408\u7684\u7cfb\u7edf\u6027\u98ce\u9669\n        \"\"\"\n        try:\n            res = round(\n                float(\n                    self.calc_beta(\n                        self.profit_pct.dropna(),\n                        self.benchmark_profitpct.dropna()\n                    )\n                ),\n                2\n            )\n        except:\n            print('\u8d1d\u5854\u8ba1\u7b97\u9519\u8bef\u3002\u3002')\n            res = 0\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef alpha(self):\n        return round(\n            float(\n                self.calc_alpha(\n                    self.annualize_return,\n                    self.benchmark_annualize_return,\n                    self.beta,\n                    0.05\n                )\n            ),\n            2\n        )", "response": "get alpha of the class"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting sharpe of a resource", "response": "def sharpe(self):\n        \"\"\"\n        \u590f\u666e\u6bd4\u7387\n\n        \"\"\"\n        return round(\n            float(\n                self.calc_sharpe(self.annualize_return,\n                                 self.volatility,\n                                 0.05)\n            ),\n            2\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the assets curve for the current resource.", "response": "def plot_assets_curve(self, length=14, height=12):\n        \"\"\"\n        \u8d44\u91d1\u66f2\u7ebf\u53e0\u52a0\u56fe\n        @Roy T.Burns 2018/05/29 \u4fee\u6539\u767e\u5206\u6bd4\u663e\u793a\u9519\u8bef\n        \"\"\"\n        plt.style.use('ggplot')\n        plt.figure(figsize=(length, height))\n        plt.subplot(211)\n        plt.title('BASIC INFO', fontsize=12)\n        plt.axis([0, length, 0, 0.6])\n        plt.axis('off')\n        i = 0\n        for item in ['account_cookie', 'portfolio_cookie', 'user_cookie']:\n            plt.text(\n                i,\n                0.5,\n                '{} : {}'.format(item,\n                                 self.message[item]),\n                fontsize=10,\n                rotation=0,\n                wrap=True\n            )\n            i += (length / 2.8)\n        i = 0\n        for item in ['benchmark_code', 'time_gap', 'max_dropback']:\n            plt.text(\n                i,\n                0.4,\n                '{} : {}'.format(item,\n                                 self.message[item]),\n                fontsize=10,\n                ha='left',\n                rotation=0,\n                wrap=True\n            )\n            i += (length / 2.8)\n        i = 0\n        for item in ['annualize_return', 'bm_annualizereturn', 'profit']:\n            plt.text(\n                i,\n                0.3,\n                '{} : {} %'.format(item,\n                                   self.message.get(item,\n                                                    0) * 100),\n                fontsize=10,\n                ha='left',\n                rotation=0,\n                wrap=True\n            )\n            i += length / 2.8\n        i = 0\n        for item in ['init_cash', 'last_assets', 'volatility']:\n            plt.text(\n                i,\n                0.2,\n                '{} : {} '.format(item,\n                                  self.message[item]),\n                fontsize=10,\n                ha='left',\n                rotation=0,\n                wrap=True\n            )\n            i += length / 2.8\n        i = 0\n        for item in ['alpha', 'beta', 'sharpe']:\n            plt.text(\n                i,\n                0.1,\n                '{} : {}'.format(item,\n                                 self.message[item]),\n                ha='left',\n                fontsize=10,\n                rotation=0,\n                wrap=True\n            )\n            i += length / 2.8\n        plt.subplot(212)\n        self.assets.plot()\n        self.benchmark_assets.xs(self.benchmark_code, level=1).plot()\n\n        asset_p = mpatches.Patch(\n            color='red',\n            label='{}'.format(self.account.account_cookie)\n        )\n        asset_b = mpatches.Patch(\n            label='benchmark {}'.format(self.benchmark_code)\n        )\n        plt.legend(handles=[asset_p, asset_b], loc=0)\n        plt.title('ASSET AND BENCKMARK')\n\n        return plt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the signal table.", "response": "def plot_signal(self, start=None, end=None):\n        \"\"\"\n        \u4f7f\u7528\u70ed\u529b\u56fe\u753b\u51fa\u4e70\u5356\u4fe1\u53f7\n        \"\"\"\n        start = self.account.start_date if start is None else start\n        end = self.account.end_date if end is None else end\n        _, ax = plt.subplots(figsize=(20, 18))\n        sns.heatmap(\n            self.account.trade.reset_index().drop(\n                'account_cookie',\n                axis=1\n            ).set_index('datetime').loc[start:end],\n            cmap=\"YlGnBu\",\n            linewidths=0.05,\n            ax=ax\n        )\n        ax.set_title(\n            'SIGNAL TABLE --ACCOUNT: {}'.format(self.account.account_cookie)\n        )\n        ax.set_xlabel('Code')\n        ax.set_ylabel('DATETIME')\n        return plt"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pnl_lifo(self):\n        X = dict(\n            zip(\n                self.target.code,\n                [LifoQueue() for i in range(len(self.target.code))]\n            )\n        )\n        pair_table = []\n        for _, data in self.target.history_table_min.iterrows():\n            while True:\n                if X[data.code].qsize() == 0:\n                    X[data.code].put((data.datetime, data.amount, data.price))\n                    break\n                else:\n                    l = X[data.code].get()\n                    if (l[1] * data.amount) < 0:\n                        # \u539f\u6709\u591a\u4ed3/ \u5e73\u4ed3 \u6216\u8005\u539f\u6709\u7a7a\u4ed3/\u5e73\u4ed3\n\n                        if abs(l[1]) > abs(data.amount):\n                            temp = (l[0], l[1] + data.amount, l[2])\n                            X[data.code].put_nowait(temp)\n                            if data.amount < 0:\n                                pair_table.append(\n                                    [\n                                        data.code,\n                                        data.datetime,\n                                        l[0],\n                                        abs(data.amount),\n                                        data.price,\n                                        l[2]\n                                    ]\n                                )\n                                break\n                            else:\n                                pair_table.append(\n                                    [\n                                        data.code,\n                                        l[0],\n                                        data.datetime,\n                                        abs(data.amount),\n                                        l[2],\n                                        data.price\n\n                                    ]\n                                )\n                                break\n\n                        elif abs(l[1]) < abs(data.amount):\n                            data.amount = data.amount + l[1]\n\n                            if data.amount < 0:\n                                pair_table.append(\n                                    [\n                                        data.code,\n                                        data.datetime,\n                                        l[0],\n                                        l[1],\n                                        data.price,\n                                        l[2]\n                                    ]\n                                )\n                            else:\n                                pair_table.append(\n                                    [\n                                        data.code,\n                                        l[0],\n                                        data.datetime,\n                                        l[1],\n                                        l[2],\n                                        data.price\n                                    ]\n                                )\n                        else:\n                            if data.amount < 0:\n                                pair_table.append(\n                                    [\n                                        data.code,\n                                        data.datetime,\n                                        l[0],\n                                        abs(data.amount),\n                                        data.price,\n                                        l[2]\n                                    ]\n                                )\n                                break\n                            else:\n                                pair_table.append(\n                                    [\n                                        data.code,\n                                        l[0],\n                                        data.datetime,\n                                        abs(data.amount),\n                                        l[2],\n                                        data.price\n                                    ]\n                                )\n                                break\n\n                    else:\n                        X[data.code].put_nowait(l)\n                        X[data.code].put_nowait(\n                            (data.datetime,\n                             data.amount,\n                             data.price)\n                        )\n                        break\n\n        pair_title = [\n            'code',\n            'sell_date',\n            'buy_date',\n            'amount',\n            'sell_price',\n            'buy_price'\n        ]\n        pnl = pd.DataFrame(pair_table, columns=pair_title).set_index('code')\n        pnl = pnl.assign(\n            unit=pnl.code.apply(lambda x: self.market_preset.get_unit(x)),\n            pnl_ratio=(pnl.sell_price / pnl.buy_price) - 1,\n            sell_date=pd.to_datetime(pnl.sell_date),\n            buy_date=pd.to_datetime(pnl.buy_date)\n        )\n        pnl = pnl.assign(\n            pnl_money=(pnl.sell_price - pnl.buy_price) * pnl.amount * pnl.unit,\n            hold_gap=abs(pnl.sell_date - pnl.buy_date),\n            if_buyopen=(pnl.sell_date -\n                        pnl.buy_date) > datetime.timedelta(days=0)\n        )\n        pnl = pnl.assign(\n            openprice=pnl.if_buyopen.apply(\n                lambda pnl: 1 if pnl else 0) * pnl.buy_price + pnl.if_buyopen.apply(lambda pnl: 0 if pnl else 1) * pnl.sell_price,\n            opendate=pnl.if_buyopen.apply(\n                lambda pnl: 1 if pnl else 0) * pnl.buy_date.map(str) + pnl.if_buyopen.apply(lambda pnl: 0 if pnl else 1) * pnl.sell_date.map(str),\n            closeprice=pnl.if_buyopen.apply(\n                lambda pnl: 0 if pnl else 1) * pnl.buy_price + pnl.if_buyopen.apply(lambda pnl: 1 if pnl else 0) * pnl.sell_price,\n            closedate=pnl.if_buyopen.apply(\n                lambda pnl: 0 if pnl else 1) * pnl.buy_date.map(str) + pnl.if_buyopen.apply(lambda pnl: 1 if pnl else 0) * pnl.sell_date.map(str))\n        return pnl.set_index('code')", "response": "This method creates a new life queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the pnl ratio of the current article.", "response": "def plot_pnlratio(self):\n        \"\"\"\n        \u753b\u51fapnl\u6bd4\u7387\u6563\u70b9\u56fe\n        \"\"\"\n\n        plt.scatter(x=self.pnl.sell_date.apply(str), y=self.pnl.pnl_ratio)\n        plt.gcf().autofmt_xdate()\n        return plt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_pnlmoney(self):\n        plt.scatter(x=self.pnl.sell_date.apply(str), y=self.pnl.pnl_money)\n        plt.gcf().autofmt_xdate()\n        return plt", "response": "Plot the pnl money"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the local time of the next schedule time this job will run.", "response": "def next_time(self, asc=False):\n        \"\"\"Get the local time of the next schedule time this job will run.\n        :param bool asc: Format the result with ``time.asctime()``\n        :returns: The epoch time or string representation of the epoch time that\n            the job should be run next\n        \"\"\"\n        _time = time.localtime(time.time() + self.next())\n\n        if asc:\n            return time.asctime(_time)\n\n        return time.mktime(_time)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget future transaction realtime", "response": "def QA_fetch_get_future_transaction_realtime(package, code):\n    \"\"\"\n    \u671f\u8d27\u5b9e\u65f6tick\n    \"\"\"\n    Engine = use(package)\n    if package in ['tdx', 'pytdx']:\n        return Engine.QA_fetch_get_future_transaction_realtime(code)\n    else:\n        return 'Unsupport packages'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the MACD of the given DataFrame", "response": "def QA_indicator_MACD(DataFrame, short=12, long=26, mid=9):\n    \"\"\"\n    MACD CALC\n    \"\"\"\n    CLOSE = DataFrame['close']\n\n    DIF = EMA(CLOSE, short)-EMA(CLOSE, long)\n    DEA = EMA(DIF, mid)\n    MACD = (DIF-DEA)*2\n\n    return pd.DataFrame({'DIF': DIF, 'DEA': DEA, 'MACD': MACD})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the indicator MTM", "response": "def QA_indicator_MTM(DataFrame, N=12, M=6):\n    '\u52a8\u91cf\u7ebf'\n    C = DataFrame.close\n    mtm = C - REF(C, N)\n    MTMMA = MA(mtm, M)\n    DICT = {'MTM': mtm, 'MTMMA': MTMMA}\n\n    return pd.DataFrame(DICT)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QA_indicator_BIAS(DataFrame, N1, N2, N3):\n    '\u4e56\u79bb\u7387'\n    CLOSE = DataFrame['close']\n    BIAS1 = (CLOSE - MA(CLOSE, N1)) / MA(CLOSE, N1) * 100\n    BIAS2 = (CLOSE - MA(CLOSE, N2)) / MA(CLOSE, N2) * 100\n    BIAS3 = (CLOSE - MA(CLOSE, N3)) / MA(CLOSE, N3) * 100\n    DICT = {'BIAS1': BIAS1, 'BIAS2': BIAS2, 'BIAS3': BIAS3}\n\n    return pd.DataFrame(DICT)", "response": "QA_indicator_BIAS returns a DataFrame with the BIAS1 BIAS2 and BIAS3 values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_indicator_WR(DataFrame, N, N1):\n    '\u5a01\u5ec9\u6307\u6807'\n    HIGH = DataFrame['high']\n    LOW = DataFrame['low']\n    CLOSE = DataFrame['close']\n    WR1 = 100 * (HHV(HIGH, N) - CLOSE) / (HHV(HIGH, N) - LLV(LOW, N))\n    WR2 = 100 * (HHV(HIGH, N1) - CLOSE) / (HHV(HIGH, N1) - LLV(LOW, N1))\n    DICT = {'WR1': WR1, 'WR2': WR2}\n\n    return pd.DataFrame(DICT)", "response": "QA_indicator_WR is a function that calculates the WR1 and WR2 of the indicator."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_indicator_ASI(DataFrame, M1=26, M2=10):\n    CLOSE = DataFrame['close']\n    HIGH = DataFrame['high']\n    LOW = DataFrame['low']\n    OPEN = DataFrame['open']\n    LC = REF(CLOSE, 1)\n    AA = ABS(HIGH - LC)\n    BB = ABS(LOW-LC)\n    CC = ABS(HIGH - REF(LOW, 1))\n    DD = ABS(LC - REF(OPEN, 1))\n\n    R = IFAND(AA > BB, AA > CC, AA+BB/2+DD/4,\n           IFAND(BB > CC, BB > AA, BB+AA/2+DD/4, CC+DD/4))\n    X = (CLOSE - LC + (CLOSE - OPEN) / 2 + LC - REF(OPEN, 1))\n    SI = 16*X/R*MAX(AA, BB)\n    ASI = SUM(SI, M1)\n    ASIT = MA(ASI, M2)\n    return pd.DataFrame({\n        'ASI': ASI, 'ASIT': ASIT\n    })", "response": "QA_indicator_ASI is a pandas DataFrame with the values of the ASI and ASIT fields."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef QA_indicator_MFI(DataFrame, N=14):\n    C = DataFrame['close']\n    H = DataFrame['high']\n    L = DataFrame['low']\n    VOL = DataFrame['volume']\n    TYP = (C + H + L) / 3\n    V1 = SUM(IF(TYP > REF(TYP, 1), TYP * VOL, 0), N) / \\\n        SUM(IF(TYP < REF(TYP, 1), TYP * VOL, 0), N)\n    mfi = 100 - (100 / (1 + V1))\n    DICT = {'MFI': mfi}\n\n    return pd.DataFrame(DICT)", "response": "QA_indicator_MFI = > MFI"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_indicator_SKDJ(DataFrame, N=9, M=3):\n    CLOSE = DataFrame['close']\n    LOWV = LLV(DataFrame['low'], N)\n    HIGHV = HHV(DataFrame['high'], N)\n    RSV = EMA((CLOSE - LOWV) / (HIGHV - LOWV) * 100, M)\n    K = EMA(RSV, M)\n    D = MA(K, M)\n    DICT = {'RSV': RSV, 'SKDJ_K': K, 'SKDJ_D': D}\n\n    return pd.DataFrame(DICT)", "response": "1.\u6307\u6807>80 \u65f6\uff0c\u56de\u6863\u673a\u7387\u5927\uff1b\u6307\u6807<20 \u65f6\uff0c\u53cd\u5f39\u673a\u7387\u5927\uff1b\n    2.K\u572820\u5de6\u53f3\u5411\u4e0a\u4ea4\u53c9D\u65f6\uff0c\u89c6\u4e3a\u4e70\u8fdb\u4fe1\u53f7\u53c2\u8003\uff1b \n    3.K\u572880\u5de6\u53f3\u5411\u4e0b\u4ea4\u53c9D\u65f6\uff0c\u89c6\u4e3a\u5356\u51fa\u4fe1\u53f7\u53c2\u8003\uff1b\n    4.SKDJ\u6ce2\u52a8\u4e8e50\u5de6\u53f3\u7684\u4efb\u4f55\u8baf\u53f7\uff0c\u5176\u4f5c\u7528\u4e0d\u5927\u3002"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets QA indicator shadow", "response": "def QA_indicator_shadow(DataFrame):\n    \"\"\"\n    \u4e0a\u4e0b\u5f71\u7ebf\u6307\u6807\n    \"\"\"\n    return {\n        'LOW': lower_shadow(DataFrame), 'UP': upper_shadow(DataFrame),\n        'BODY': body(DataFrame), 'BODY_ABS': body_abs(DataFrame), 'PRICE_PCG': price_pcg(DataFrame)\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the highest exponent of the given seriesLenght", "response": "def bestExponent(self, seriesLenght):\n        '''\n        :type seriesLenght: int\n        :rtype: int\n        '''\n        i = 0\n        cont = True\n        while(cont):\n            if(int(seriesLenght/int(math.pow(2, i))) <= 1):\n                cont = False\n            else:\n                i += 1\n        return int(i-1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mean(self, series, start, limit):\n        '''\n        :type start: int\n        :type limit: int\n        :rtype: float\n        '''\n        return float(np.mean(series[start:limit]))", "response": "get the mean value of a set of keys"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deviation(self, series, start, limit, mean):\n        '''\n        :type start: int\n        :type limit: int\n        :type mean: int\n        :rtype: list()\n        '''\n        d = []\n        for x in range(start, limit):\n            d.append(float(series[x] - mean))\n        return d", "response": "returns deviation of a set of keys in a time series"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the standart deviation of a set of keys", "response": "def standartDeviation(self, series, start, limit):\n        '''\n        :type start: int\n        :type limit: int\n        :rtype: float\n        '''\n        return float(np.std(series[start:limit]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculateHurst(self, series, exponent=None):\n        '''\n        :type series: List\n        :type exponent: int\n        :rtype: float\n        '''\n        rescaledRange = list()\n        sizeRange = list()\n        rescaledRangeMean = list()\n\n        if(exponent is None):\n            exponent = self.bestExponent(len(series))\n\n        for i in range(0, exponent):\n            partsNumber = int(math.pow(2, i))\n            size = int(len(series)/partsNumber)\n\n            sizeRange.append(size)\n            rescaledRange.append(0)\n            rescaledRangeMean.append(0)\n\n            for x in range(0, partsNumber):\n                start = int(size*(x))\n                limit = int(size*(x+1))\n\n                deviationAcumulative = self.sumDeviation(self.deviation(\n                    series, start, limit, self.mean(series, start, limit)))\n                deviationsDifference = float(\n                    max(deviationAcumulative) - min(deviationAcumulative))\n                standartDeviation = self.standartDeviation(\n                    series, start, limit)\n\n                if(deviationsDifference != 0 and standartDeviation != 0):\n                    rescaledRange[i] += (deviationsDifference /\n                                         standartDeviation)\n\n        y = 0\n        for x in rescaledRange:\n            rescaledRangeMean[y] = x/int(math.pow(2, y))\n            y = y+1\n\n        # log calculation\n        rescaledRangeLog = list()\n        sizeRangeLog = list()\n        for i in range(0, exponent):\n            rescaledRangeLog.append(math.log(rescaledRangeMean[i], 10))\n            sizeRangeLog.append(math.log(sizeRange[i], 10))\n\n        slope, intercept = np.polyfit(sizeRangeLog, rescaledRangeLog, 1)\n\n        ablineValues = [slope * i + intercept for i in sizeRangeLog]\n\n        plt.plot(sizeRangeLog, rescaledRangeLog, '--')\n        plt.plot(sizeRangeLog, ablineValues, 'b')\n        plt.title(slope)\n        # graphic dimension settings\n        limitUp = 0\n        if(max(sizeRangeLog) > max(rescaledRangeLog)):\n            limitUp = max(sizeRangeLog)\n        else:\n            limitUp = max(rescaledRangeLog)\n        limitDown = 0\n        if(min(sizeRangeLog) > min(rescaledRangeLog)):\n            limitDown = min(rescaledRangeLog)\n        else:\n            limitDown = min(sizeRangeLog)\n        plt.gca().set_xlim(limitDown, limitUp)\n        plt.gca().set_ylim(limitDown, limitUp)\n        print(\"Hurst exponent: \" + str(slope))\n        plt.show()\n\n        return slope", "response": "Calculates the burst of the given series."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a message to the server", "response": "def QA_util_send_mail(msg, title, from_user, from_password, to_addr, smtp):\n    \"\"\"\u90ae\u4ef6\u53d1\u9001\n    \n    Arguments:\n        msg {[type]} -- [description]\n        title {[type]} -- [description]\n        from_user {[type]} -- [description]\n        from_password {[type]} -- [description]\n        to_addr {[type]} -- [description]\n        smtp {[type]} -- [description]\n    \"\"\"\n\n    msg = MIMEText(msg, 'plain', 'utf-8')\n    msg['Subject'] = Header(title, 'utf-8').encode()\n\n    server = smtplib.SMTP(smtp, 25)  # SMTP\u534f\u8bae\u9ed8\u8ba4\u7aef\u53e3\u662f25\n    server.set_debuglevel(1)\n    server.login(from_user, from_password)\n    server.sendmail(from_user, [to_addr], msg.as_string())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_fetch_get_stock_analysis(code):\n    market = 'sh' if _select_market_code(code) == 1 else 'sz'\n    null = 'none'\n    data = eval(requests.get(BusinessAnalysis_url.format(\n        market, code), headers=headers_em).text)\n    zyfw = pd.DataFrame(data.get('zyfw', None))\n    jyps = pd.DataFrame(data.get('jyps', None))\n    zygcfx = data.get('zygcfx', [])\n    temp = []\n    for item in zygcfx:\n        try:\n            data_ = pd.concat([pd.DataFrame(item['hy']).assign(date=item['rq']).assign(classify='hy'),\n                               pd.DataFrame(item['cp']).assign(\n                                   date=item['rq']).assign(classify='cp'),\n                               pd.DataFrame(item['qy']).assign(date=item['rq']).assign(classify='qy')])\n\n            temp.append(data_)\n        except:\n            pass\n    try:\n        res_zyfcfx = pd.concat(temp).set_index(\n            ['date', 'classify'], drop=False)\n    except:\n        res_zyfcfx = None\n\n    return zyfw, jyps, res_zyfcfx", "response": "'zyfw', \u4e3b\u8425\u8303\u56f4 'jyps'#\u7ecf\u8425\u8bc4\u8ff0 'zygcfx' \u4e3b\u8425\u6784\u6210\u5206\u6790\n\n    date \u4e3b\u8425\u6784\u6210\t\u4e3b\u8425\u6536\u5165(\u5143)\t\u6536\u5165\u6bd4\u4f8bcbbl\t\u4e3b\u8425\u6210\u672c(\u5143)\t\u6210\u672c\u6bd4\u4f8b\t\u4e3b\u8425\u5229\u6da6(\u5143)\t\u5229\u6da6\u6bd4\u4f8b\t\u6bdb\u5229\u7387(%)\n    \u884c\u4e1a /\u4ea7\u54c1/ \u533a\u57df hq cp qy"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends order to a specific order", "response": "def send_order(self,  code, price, amount, towards, order_model, market=None):\n        \"\"\"\u4e0b\u5355\n\n        Arguments:\n            code {[type]} -- [description]\n            price {[type]} -- [description]\n            amount {[type]} -- [description]\n            towards {[type]} -- [description]\n            order_model {[type]} -- [description]\n            market:\u5e02\u573a\uff0cSZ \u6df1\u4ea4\u6240\uff0cSH \u4e0a\u4ea4\u6240\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        towards = 0 if towards == ORDER_DIRECTION.BUY else 1\n        if order_model == ORDER_MODEL.MARKET:\n            order_model = 4\n        elif order_model == ORDER_MODEL.LIMIT:\n            order_model = 0\n\n        if market is None:\n            market = QAFetch.base.get_stock_market(code)\n        if not isinstance(market, str):\n            raise Exception('%s\u4e0d\u6b63\u786e\uff0c\u8bf7\u68c0\u67e5code\u548cmarket\u53c2\u6570' % market)\n        market = market.lower()\n        if market not in ['sh', 'sz']:\n            raise Exception('%s\u4e0d\u652f\u6301\uff0c\u8bf7\u68c0\u67e5code\u548cmarket\u53c2\u6570' % market)\n\n        return self.data_to_df(self.call(\"send_order\", {\n            'client_id': self.client_id,\n            'category': towards,\n            'price_type': order_model,\n            'gddm': self.gddm_sh if market == 'sh' else self.gddm_sz,\n            'zqdm': code,\n            'price': price,\n            'quantity': amount\n        }))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_util_getBetweenMonth(from_date, to_date):\n    date_list = {}\n    begin_date = datetime.datetime.strptime(from_date, \"%Y-%m-%d\")\n    end_date = datetime.datetime.strptime(to_date, \"%Y-%m-%d\")\n    while begin_date <= end_date:\n        date_str = begin_date.strftime(\"%Y-%m\")\n        date_list[date_str] = ['%d-%d-01' % (begin_date.year, begin_date.month),\n                               '%d-%d-%d' % (begin_date.year, begin_date.month,\n                                             calendar.monthrange(begin_date.year, begin_date.month)[1])]\n        begin_date = QA_util_get_1st_of_next_month(begin_date)\n    return(date_list)", "response": "get a list of all the pages in a month between from_date and to_date"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QA_util_add_months(dt, months):\n    dt = datetime.datetime.strptime(\n        dt, \"%Y-%m-%d\") + relativedelta(months=months)\n    return(dt)", "response": "add months to the end of the datetimes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_util_get_1st_of_next_month(dt):\n    year = dt.year\n    month = dt.month\n    if month == 12:\n        month = 1\n        year += 1\n    else:\n        month += 1\n    res = datetime.datetime(year, month, 1)\n    return res", "response": "get 1st of next month"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_util_getBetweenQuarter(begin_date, end_date):\n    quarter_list = {}\n    month_list = QA_util_getBetweenMonth(begin_date, end_date)\n    for value in month_list:\n        tempvalue = value.split(\"-\")\n        year = tempvalue[0]\n        if tempvalue[1] in ['01', '02', '03']:\n            quarter_list[year + \"Q1\"] = ['%s-01-01' % year, '%s-03-31' % year]\n        elif tempvalue[1] in ['04', '05', '06']:\n            quarter_list[year + \"Q2\"] = ['%s-04-01' % year, '%s-06-30' % year]\n        elif tempvalue[1] in ['07', '08', '09']:\n            quarter_list[year + \"Q3\"] = ['%s-07-31' % year, '%s-09-30' % year]\n        elif tempvalue[1] in ['10', '11', '12']:\n            quarter_list[year + \"Q4\"] = ['%s-10-01' % year, '%s-12-31' % year]\n    return(quarter_list)", "response": "get a list of all the national quarter names between two dates"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves account record in database", "response": "def save_account(message, collection=DATABASE.account):\n    \"\"\"save account\n\n    Arguments:\n        message {[type]} -- [description]\n\n    Keyword Arguments:\n        collection {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n    try:\n        collection.create_index(\n            [(\"account_cookie\", ASCENDING), (\"user_cookie\", ASCENDING), (\"portfolio_cookie\", ASCENDING)], unique=True)\n    except:\n        pass\n    collection.update(\n        {'account_cookie': message['account_cookie'], 'portfolio_cookie':\n            message['portfolio_cookie'], 'user_cookie': message['user_cookie']},\n        {'$set': message},\n        upsert=True\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef QA_SU_save_financial_files():\n    download_financialzip()\n    coll = DATABASE.financial\n    coll.create_index(\n        [(\"code\", ASCENDING), (\"report_date\", ASCENDING)], unique=True)\n    for item in os.listdir(download_path):\n        if item[0:4] != 'gpcw':\n            print(\n                \"file \", item, \" is not start with gpcw , seems not a financial file , ignore!\")\n            continue\n\n        date = int(item.split('.')[0][-8:])\n        print('QUANTAXIS NOW SAVING {}'.format(date))\n        if coll.find({'report_date': date}).count() < 3600:\n\n            print(coll.find({'report_date': date}).count())\n            data = QA_util_to_json_from_pandas(parse_filelist([item]).reset_index(\n            ).drop_duplicates(subset=['code', 'report_date']).sort_index())\n            # data[\"crawl_date\"] = str(datetime.date.today())\n            try:\n                coll.insert_many(data, ordered=False)\n\n            except Exception as e:\n                if isinstance(e, MemoryError):\n                    coll.insert_many(data, ordered=True)\n                elif isinstance(e, pymongo.bulk.BulkWriteError):\n                    pass\n        else:\n            print('ALL READY IN DATABASE')\n\n    print('SUCCESSFULLY SAVE/UPDATE FINANCIAL DATA')", "response": "Save all financial files in the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_util_log_info(\n        logs,\n        ui_log=None,\n        ui_progress=None,\n        ui_progress_int_value=None,\n):\n    \"\"\"\n    QUANTAXIS Log Module\n    @yutiansut\n\n    QA_util_log_x is under [QAStandard#0.0.2@602-x] Protocol\n    \"\"\"\n    logging.warning(logs)\n\n    # \u7ed9GUI\u4f7f\u7528\uff0c\u66f4\u65b0\u5f53\u524d\u4efb\u52a1\u5230\u65e5\u5fd7\u548c\u8fdb\u5ea6\n    if ui_log is not None:\n        if isinstance(logs, str):\n            ui_log.emit(logs)\n        if isinstance(logs, list):\n            for iStr in logs:\n                ui_log.emit(iStr)\n\n    if ui_progress is not None and ui_progress_int_value is not None:\n        ui_progress.emit(ui_progress_int_value)", "response": "QA_util_log_x is under [ 0. 2@602 - x ] Protocol\n "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef QA_save_tdx_to_mongo(file_dir, client=DATABASE):\n\n    reader = TdxMinBarReader()\n    __coll = client.stock_min_five\n    for a, v, files in os.walk(file_dir):\n\n        for file in files:\n\n            if (str(file)[0:2] == 'sh' and int(str(file)[2]) == 6) or \\\n                (str(file)[0:2] == 'sz' and int(str(file)[2]) == 0) or \\\n                    (str(file)[0:2] == 'sz' and int(str(file)[2]) == 3):\n\n                QA_util_log_info('Now_saving ' + str(file)\n                                 [2:8] + '\\'s 5 min tick')\n                fname = file_dir + os.sep + file\n                df = reader.get_df(fname)\n                df['code'] = str(file)[2:8]\n                df['market'] = str(file)[0:2]\n                df['datetime'] = [str(x) for x in list(df.index)]\n                df['date'] = [str(x)[0:10] for x in list(df.index)]\n                df['time_stamp'] = df['datetime'].apply(\n                    lambda x: QA_util_time_stamp(x))\n                df['date_stamp'] = df['date'].apply(\n                    lambda x: QA_util_date_stamp(x))\n                data_json = json.loads(df.to_json(orient='records'))\n                __coll.insert_many(data_json)", "response": "Save TdxMinBar file to Mongo"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exclude_from_stock_ip_list(exclude_ip_list):\n    for exc in exclude_ip_list:\n        if exc in stock_ip_list:\n            stock_ip_list.remove(exc)\n\n    # \u6269\u5c55\u5e02\u573a\n    for exc in exclude_ip_list:\n        if exc in future_ip_list:\n            future_ip_list.remove(exc)", "response": "Remove items from the list of items in exclude_ip_list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the config value of a specific node in the MONGODB section.", "response": "def get_config(\n            self,\n            section='MONGODB',\n            option='uri',\n            default_value=DEFAULT_DB_URI\n    ):\n        \"\"\"[summary]\n\n        Keyword Arguments:\n            section {str} -- [description] (default: {'MONGODB'})\n            option {str} -- [description] (default: {'uri'})\n            default_value {[type]} -- [description] (default: {DEFAULT_DB_URI})\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n\n        res = self.client.quantaxis.usersetting.find_one({'section': section})\n        if res:\n            return res.get(option, default_value)\n        else:\n            self.set_config(section, option, default_value)\n            return default_value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_config(\n            self,\n            section='MONGODB',\n            option='uri',\n            default_value=DEFAULT_DB_URI\n    ):\n        \"\"\"[summary]\n\n        Keyword Arguments:\n            section {str} -- [description] (default: {'MONGODB'})\n            option {str} -- [description] (default: {'uri'})\n            default_value {[type]} -- [description] (default: {DEFAULT_DB_URI})\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n        t = {'section': section, option: default_value}\n        self.client.quantaxis.usersetting.update(\n            {'section': section}, {'$set':t}, upsert=True)", "response": "Set the config of the user in the cluster"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting or set the value of a section in the config file.", "response": "def get_or_set_section(\n            self,\n            config,\n            section,\n            option,\n            DEFAULT_VALUE,\n            method='get'\n    ):\n        \"\"\"[summary]\n\n        Arguments:\n            config {[type]} -- [description]\n            section {[type]} -- [description]\n            option {[type]} -- [description]\n            DEFAULT_VALUE {[type]} -- [description]\n\n        Keyword Arguments:\n            method {str} -- [description] (default: {'get'})\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        try:\n            if isinstance(DEFAULT_VALUE, str):\n                val = DEFAULT_VALUE\n            else:\n                val = json.dumps(DEFAULT_VALUE)\n            if method == 'get':\n                return self.get_config(section, option)\n            else:\n                self.set_config(section, option, val)\n                return val\n\n        except:\n            self.set_config(section, option, val)\n            return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts int date to string", "response": "def QA_util_date_int2str(int_date):\n    \"\"\"\n    \u7c7b\u578bdatetime.datatime\n    :param date: int 8\u4f4d\u6574\u6570\n    :return: \u7c7b\u578bstr\n    \"\"\"\n    date = str(int_date)\n    if len(date) == 8:\n        return str(date[0:4] + '-' + date[4:6] + '-' + date[6:8])\n    elif len(date) == 10:\n        return date"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts time to datetime", "response": "def QA_util_to_datetime(time):\n    \"\"\"\n    \u5b57\u7b26\u4e32 '2018-01-01'  \u8f6c\u53d8\u6210 datatime \u7c7b\u578b\n    :param time: \u5b57\u7b26\u4e32str -- \u683c\u5f0f\u5fc5\u987b\u662f 2018-01-01 \uff0c\u957f\u5ea610\n    :return: \u7c7b\u578bdatetime.datatime\n    \"\"\"\n    if len(str(time)) == 10:\n        _time = '{} 00:00:00'.format(time)\n    elif len(str(time)) == 19:\n        _time = str(time)\n    else:\n        QA_util_log_info('WRONG DATETIME FORMAT {}'.format(time))\n    return datetime.datetime.strptime(_time, '%Y-%m-%d %H:%M:%S')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QA_util_datetime_to_strdate(dt):\n    strdate = \"%04d-%02d-%02d\" % (dt.year, dt.month, dt.day)\n    return strdate", "response": "convert datetime to string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_util_datetime_to_strdatetime(dt):\n    strdatetime = \"%04d-%02d-%02d %02d:%02d:%02d\" % (\n        dt.year,\n        dt.month,\n        dt.day,\n        dt.hour,\n        dt.minute,\n        dt.second\n    )\n    return strdatetime", "response": "convert datetime to string"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_util_date_stamp(date):\n    datestr = str(date)[0:10]\n    date = time.mktime(time.strptime(datestr, '%Y-%m-%d'))\n    return date", "response": "QA - util date stamp"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts timestamp to datetime", "response": "def QA_util_stamp2datetime(timestamp):\n    \"\"\"\n    datestamp\u8f6cdatetime\n    pandas\u8f6c\u51fa\u6765\u7684timestamp\u662f13\u4f4d\u6574\u6570 \u8981/1000\n    It\u2019s common for this to be restricted to years from 1970 through 2038.\n    \u4ece1970\u5e74\u5f00\u59cb\u7684\u7eb3\u79d2\u5230\u5f53\u524d\u7684\u8ba1\u6570 \u8f6c\u53d8\u6210 float \u7c7b\u578b\u65f6\u95f4 \u7c7b\u4f3c time.time() \u8fd4\u56de\u7684\u7c7b\u578b\n    :param timestamp: long\u7c7b\u578b\n    :return: \u7c7b\u578bfloat\n    \"\"\"\n    try:\n        return datetime.datetime.fromtimestamp(timestamp)\n    except Exception as e:\n        # it won't work ??\n        try:\n            return datetime.datetime.fromtimestamp(timestamp / 1000)\n        except:\n            try:\n                return datetime.datetime.fromtimestamp(timestamp / 1000000)\n            except:\n                return datetime.datetime.fromtimestamp(timestamp / 1000000000)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_util_realtime(strtime, client):\n    time_stamp = QA_util_date_stamp(strtime)\n    coll = client.quantaxis.trade_date\n    temp_str = coll.find_one({'date_stamp': {\"$gte\": time_stamp}})\n    time_real = temp_str['date']\n    time_id = temp_str['num']\n    return {'time_real': time_real, 'id': time_id}", "response": "get the realtime of a single item"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_util_id2date(idx, client):\n    coll = client.quantaxis.trade_date\n    temp_str = coll.find_one({'num': idx})\n    return temp_str['date']", "response": "get date from id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a date is a trade", "response": "def QA_util_is_trade(date, code, client):\n    \"\"\"\n    \u5224\u65ad\u662f\u5426\u662f\u4ea4\u6613\u65e5\n    \u4ece\u6570\u636e\u5e93\u4e2d\u67e5\u8be2\n    :param date: str\u7c7b\u578b -- 1999-12-11 \u8fd9\u79cd\u683c\u5f0f    10\u4f4d\u5b57\u7b26\u4e32\n    :param code: str\u7c7b\u578b -- \u80a1\u7968\u4ee3\u7801 \u4f8b\u5982 603658 \uff0c 6\u4f4d\u5b57\u7b26\u4e32\n    :param client: pymongo.MongoClient\u7c7b\u578b    -- mongodb \u6570\u636e\u5e93 \u4ece QA_util_sql_mongo_setting \u4e2d QA_util_sql_mongo_setting \u83b7\u53d6\n    :return:  Boolean -- \u662f\u5426\u662f\u4ea4\u6613\u65f6\u95f4\n    \"\"\"\n    coll = client.quantaxis.stock_day\n    date = str(date)[0:10]\n    is_trade = coll.find_one({'code': code, 'date': date})\n    try:\n        len(is_trade)\n        return True\n    except:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_util_select_hours(time=None, gt=None, lt=None, gte=None, lte=None):\n    'quantaxis\u7684\u65f6\u95f4\u9009\u62e9\u51fd\u6570,\u7ea6\u5b9a\u65f6\u95f4\u7684\u8303\u56f4,\u6bd4\u5982\u65e9\u4e0a9\u70b9\u523011\u70b9'\n    if time is None:\n        __realtime = datetime.datetime.now()\n    else:\n        __realtime = time\n\n    fun_list = []\n    if gt != None:\n        fun_list.append('>')\n    if lt != None:\n        fun_list.append('<')\n    if gte != None:\n        fun_list.append('>=')\n    if lte != None:\n        fun_list.append('<=')\n\n    assert len(fun_list) > 0\n    true_list = []\n    try:\n        for item in fun_list:\n            if item == '>':\n                if __realtime.strftime('%H') > gt:\n                    true_list.append(0)\n                else:\n                    true_list.append(1)\n            elif item == '<':\n                if __realtime.strftime('%H') < lt:\n                    true_list.append(0)\n                else:\n                    true_list.append(1)\n            elif item == '>=':\n                if __realtime.strftime('%H') >= gte:\n                    true_list.append(0)\n                else:\n                    true_list.append(1)\n            elif item == '<=':\n                if __realtime.strftime('%H') <= lte:\n                    true_list.append(0)\n                else:\n                    true_list.append(1)\n\n    except:\n        return Exception\n    if sum(true_list) > 0:\n        return False\n    else:\n        return True", "response": "select hours from quantaxis"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to calculate time", "response": "def QA_util_calc_time(func, *args, **kwargs):\n    \"\"\"\n    '\u8017\u65f6\u957f\u5ea6\u7684\u88c5\u9970\u5668'\n    :param func:\n    :param args:\n    :param kwargs:\n    :return:\n    \"\"\"\n    _time = datetime.datetime.now()\n    func(*args, **kwargs)\n    print(datetime.datetime.now() - _time)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef high_limit(self):\n        '\u6da8\u505c\u4ef7'\n        return self.groupby(level=1).close.apply(lambda x: round((x.shift(1) + 0.0002)*1.1, 2)).sort_index()", "response": "High limit the result set to the last 2 entries."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_medium_order(self, lower=200000, higher=1000000):\n\n        return self.data.query('amount>={}'.format(lower)).query('amount<={}'.format(higher))", "response": "Returns a list of the medium order of the current user s national order."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the shadow of a resource.", "response": "def shadow_calc(data):\n    \"\"\"\u8ba1\u7b97\u4e0a\u4e0b\u5f71\u7ebf\n\n    Arguments:\n        data {DataStruct.slice} -- \u8f93\u5165\u7684\u662f\u4e00\u4e2a\u884c\u60c5\u5207\u7247\n\n    Returns:\n        up_shadow {float} -- \u4e0a\u5f71\u7ebf\n        down_shdow {float} -- \u4e0b\u5f71\u7ebf\n        entity {float} -- \u5b9e\u4f53\u90e8\u5206\n        date {str} -- \u65f6\u95f4\n        code {str} -- \u4ee3\u7801\n    \"\"\"\n\n    up_shadow = abs(data.high - (max(data.open, data.close)))\n    down_shadow = abs(data.low - (min(data.open, data.close)))\n    entity = abs(data.open - data.close)\n    towards = True if data.open < data.close else False\n    print('=' * 15)\n    print('up_shadow : {}'.format(up_shadow))\n    print('down_shadow : {}'.format(down_shadow))\n    print('entity: {}'.format(entity))\n    print('towards : {}'.format(towards))\n    return up_shadow, down_shadow, entity, data.date, data.code"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying data from the cache", "response": "def query_data(self, code, start, end, frequence, market_type=None):\n        \"\"\"\n        \u6807\u51c6\u683c\u5f0f\u662fnumpy\n        \"\"\"\n        try:\n            return self.fetcher[(market_type, frequence)](\n                code, start, end, frequence=frequence)\n        except:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef QA_SU_save_stock_min(client=DATABASE, ui_log=None, ui_progress=None):\n    # \u5bfc\u5165\u6398\u91d1\u6a21\u5757\u4e14\u8fdb\u884c\u767b\u5f55\n    try:\n        from gm.api import set_token\n        from gm.api import history\n        # \u8bf7\u81ea\u884c\u5c06\u6398\u91d1\u91cf\u5316\u7684 TOKEN \u66ff\u6362\u6389 GMTOKEN\n        set_token(\"9c5601171e97994686b47b5cbfe7b2fc8bb25b09\")\n    except:\n        raise ModuleNotFoundError\n\n    # \u80a1\u7968\u4ee3\u7801\u683c\u5f0f\u5316\n    code_list = list(\n        map(\n            lambda x: \"SHSE.\" + x if x[0] == \"6\" else \"SZSE.\" + x,\n            QA_fetch_get_stock_list().code.unique().tolist(),\n        ))\n    coll = client.stock_min\n    coll.create_index([\n        (\"code\", pymongo.ASCENDING),\n        (\"time_stamp\", pymongo.ASCENDING),\n        (\"date_stamp\", pymongo.ASCENDING),\n    ])\n    err = []\n\n    def __transform_gm_to_qa(df, type_):\n        \"\"\"\n        \u5c06\u6398\u91d1\u6570\u636e\u8f6c\u6362\u4e3a qa \u683c\u5f0f\n        \"\"\"\n\n        if df is None or len(df) == 0:\n            raise ValueError(\"\u6ca1\u6709\u6398\u91d1\u6570\u636e\")\n\n        df = df.rename(columns={\n            \"eob\": \"datetime\",\n            \"volume\": \"vol\",\n            \"symbol\": \"code\"\n        }).drop([\"bob\", \"frequency\", \"position\", \"pre_close\"], axis=1)\n        df[\"code\"] = df[\"code\"].map(str).str.slice(5, )\n        df[\"datetime\"] = pd.to_datetime(df[\"datetime\"].map(str).str.slice(\n            0, 19))\n        df[\"date\"] = df.datetime.map(str).str.slice(0, 10)\n        df = df.set_index(\"datetime\", drop=False)\n        df[\"date_stamp\"] = df[\"date\"].apply(lambda x: QA_util_date_stamp(x))\n        df[\"time_stamp\"] = (\n            df[\"datetime\"].map(str).apply(lambda x: QA_util_time_stamp(x)))\n        df[\"type\"] = type_\n\n        return df[[\n            \"open\",\n            \"close\",\n            \"high\",\n            \"low\",\n            \"vol\",\n            \"amount\",\n            \"datetime\",\n            \"code\",\n            \"date\",\n            \"date_stamp\",\n            \"time_stamp\",\n            \"type\",\n        ]]\n\n    def __saving_work(code, coll):\n        QA_util_log_info(\n            \"##JOB03 Now Saving STOCK_MIN ==== {}\".format(code), ui_log=ui_log)\n        try:\n            for type_ in [\"1min\", \"5min\", \"15min\", \"30min\", \"60min\"]:\n                col_filter = {\"code\": str(code)[5:], \"type\": type_}\n                ref_ = coll.find(col_filter)\n                end_time = str(now_time())[0:19]\n                if coll.count_documents(col_filter) > 0:\n                    start_time = ref_[coll.count_documents(\n                        col_filter) - 1][\"datetime\"]\n                    print(start_time)\n                    QA_util_log_info(\n                        \"##JOB03.{} Now Saving {} from {} to {} == {}\".format(\n                            [\"1min\",\n                             \"5min\",\n                             \"15min\",\n                             \"30min\",\n                             \"60min\"\n                             ].index(type_),\n                            str(code)[5:],\n                            start_time,\n                            end_time,\n                            type_,\n                        ),\n                        ui_log=ui_log,\n                    )\n                    if start_time != end_time:\n                        df = history(\n                            symbol=code,\n                            start_time=start_time,\n                            end_time=end_time,\n                            frequency=MIN_SEC[type_],\n                            df=True\n                        )\n                        __data = __transform_gm_to_qa(df, type_)\n                        if len(__data) > 1:\n                            # print(QA_util_to_json_from_pandas(__data)[1::])\n                            # print(__data)\n                            coll.insert_many(\n                                QA_util_to_json_from_pandas(__data)[1::])\n                else:\n                    start_time = \"2015-01-01 09:30:00\"\n                    QA_util_log_info(\n                        \"##JOB03.{} Now Saving {} from {} to {} == {}\".format(\n                            [\"1min\",\n                             \"5min\",\n                             \"15min\",\n                             \"30min\",\n                             \"60min\"\n                             ].index(type_),\n                            str(code)[5:],\n                            start_time,\n                            end_time,\n                            type_,\n                        ),\n                        ui_log=ui_log,\n                    )\n                    if start_time != end_time:\n                        df = history(\n                            symbol=code,\n                            start_time=start_time,\n                            end_time=end_time,\n                            frequency=MIN_SEC[type_],\n                            df=True\n                        )\n                        __data = __transform_gm_to_qa(df, type_)\n                        if len(__data) > 1:\n                            # print(__data)\n                            coll.insert_many(\n                                QA_util_to_json_from_pandas(__data)[1::])\n                            # print(QA_util_to_json_from_pandas(__data)[1::])\n        except Exception as e:\n            QA_util_log_info(e, ui_log=ui_log)\n            err.append(code)\n            QA_util_log_info(err, ui_log=ui_log)\n\n    executor = ThreadPoolExecutor(max_workers=2)\n    res = {\n        executor.submit(__saving_work, code_list[i_], coll)\n        for i_ in range(len(code_list))\n    }\n    count = 0\n    for i_ in concurrent.futures.as_completed(res):\n        QA_util_log_info(\n            'The {} of Total {}'.format(count,\n                                        len(code_list)),\n            ui_log=ui_log\n        )\n\n        strProgress = \"DOWNLOAD PROGRESS {} \".format(\n            str(float(count / len(code_list) * 100))[0:4] + \"%\")\n        intProgress = int(count / len(code_list) * 10000.0)\n        QA_util_log_info(\n            strProgress,\n            ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intProgress\n        )\n        count = count + 1\n    if len(err) < 1:\n        QA_util_log_info(\"SUCCESS\", ui_log=ui_log)\n    else:\n        QA_util_log_info(\" ERROR CODE \\n \", ui_log=ui_log)\n        QA_util_log_info(err, ui_log=ui_log)", "response": "\u6398\u91d1\u5b9e\u73b0\u65b9\u5f0f\n    save current day's stock_min data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalized(self):\n        '\u5f52\u4e00\u5316'\n        res = self.groupby('code').apply(lambda x: x / x.iloc[0])\n        return res", "response": "Return a new DStream with all the codes divided by the current one."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_dict(self, time, code):\n        '''\n        'give the time,code tuple and turn the dict'\n        :param time:\n        :param code:\n        :return:  \u5b57\u5178dict \u7c7b\u578b\n        '''\n        try:\n            return self.dicts[(QA_util_to_datetime(time), str(code))]\n        except Exception as e:\n            raise e", "response": "give the time code tuple and turn the dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the market data for the specified code.", "response": "def kline_echarts(self, code=None):\n\n        def kline_formater(param):\n            return param.name + ':' + vars(param)\n\n        \"\"\"plot the market_data\"\"\"\n        if code is None:\n            path_name = '.' + os.sep + 'QA_' + self.type + \\\n                '_codepackage_' + self.if_fq + '.html'\n            kline = Kline(\n                'CodePackage_' + self.if_fq + '_' + self.type,\n                width=1360,\n                height=700,\n                page_title='QUANTAXIS'\n            )\n\n            bar = Bar()\n            data_splits = self.splits()\n\n            for ds in data_splits:\n                data = []\n                axis = []\n                if ds.type[-3:] == 'day':\n                    datetime = np.array(ds.date.map(str))\n                else:\n                    datetime = np.array(ds.datetime.map(str))\n                ohlc = np.array(\n                    ds.data.loc[:,\n                                ['open',\n                                 'close',\n                                 'low',\n                                 'high']]\n                )\n\n                kline.add(\n                    ds.code[0],\n                    datetime,\n                    ohlc,\n                    mark_point=[\"max\",\n                                \"min\"],\n                    is_datazoom_show=True,\n                    datazoom_orient='horizontal'\n                )\n            return kline\n\n        else:\n            data = []\n            axis = []\n            ds = self.select_code(code)\n            data = []\n            #axis = []\n            if self.type[-3:] == 'day':\n                datetime = np.array(ds.date.map(str))\n            else:\n                datetime = np.array(ds.datetime.map(str))\n\n            ohlc = np.array(ds.data.loc[:, ['open', 'close', 'low', 'high']])\n            vol = np.array(ds.volume)\n            kline = Kline(\n                '{}__{}__{}'.format(code,\n                                    self.if_fq,\n                                    self.type),\n                width=1360,\n                height=700,\n                page_title='QUANTAXIS'\n            )\n            bar = Bar()\n            kline.add(self.code, datetime, ohlc,\n                      mark_point=[\"max\", \"min\"],\n                      # is_label_show=True,\n                      is_datazoom_show=True,\n                      is_xaxis_show=False,\n                      # is_toolbox_show=True,\n                      tooltip_formatter='{b}:{c}',  # kline_formater,\n                      # is_more_utils=True,\n                      datazoom_orient='horizontal')\n\n            bar.add(\n                self.code,\n                datetime,\n                vol,\n                is_datazoom_show=True,\n                datazoom_xaxis_index=[0,\n                                      1]\n            )\n\n            grid = Grid(width=1360, height=700, page_title='QUANTAXIS')\n            grid.add(bar, grid_top=\"80%\")\n            grid.add(kline, grid_bottom=\"30%\")\n            return grid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef query(self, context):\n        try:\n            return self.data.query(context)\n\n        except pd.core.computation.ops.UndefinedVariableError:\n            print('QA CANNOT QUERY THIS {}'.format(context))\n            pass", "response": "Query the data for a specific context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef groupby(\n            self,\n            by=None,\n            axis=0,\n            level=None,\n            as_index=True,\n            sort=False,\n            group_keys=False,\n            squeeze=False,\n            **kwargs\n    ):\n        \"\"\"\u4effdataframe\u7684groupby\u5199\u6cd5,\u4f46\u63a7\u5236\u4e86by\u7684code\u548cdatetime\n\n        Keyword Arguments:\n            by {[type]} -- [description] (default: {None})\n            axis {int} -- [description] (default: {0})\n            level {[type]} -- [description] (default: {None})\n            as_index {bool} -- [description] (default: {True})\n            sort {bool} -- [description] (default: {True})\n            group_keys {bool} -- [description] (default: {True})\n            squeeze {bool} -- [description] (default: {False})\n            observed {bool} -- [description] (default: {False})\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        if by == self.index.names[1]:\n            by = None\n            level = 1\n        elif by == self.index.names[0]:\n            by = None\n            level = 0\n        return self.data.groupby(\n            by=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze\n        )", "response": "\u4effdataframe\u7684groupby\u5199\u6cd5,\u4f46\u63a7\u5236\u4e86by\u7684code\u548cdatetime\n\n        Keyword Arguments:\n            by {[type]} -- [description] (default: {None})\n            axis {int} -- [description] (default: {0})\n            level {[type]} -- [description] (default: {None})\n            as_index {bool} -- [description] (default: {True})\n            sort {bool} -- [description] (default: {True})\n            group_keys {bool} -- [description] (default: {True})\n            squeeze {bool} -- [description] (default: {False})\n            observed {bool} -- [description] (default: {False})\n\n        Returns:\n            [type] -- [description]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new(self, data=None, dtype=None, if_fq=None):\n        data = self.data if data is None else data\n\n        dtype = self.type if dtype is None else dtype\n        if_fq = self.if_fq if if_fq is None else if_fq\n\n        temp = copy(self)\n        temp.__init__(data, dtype, if_fq)\n        return temp", "response": "create a new object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reindex(self, ind):\n\n        if isinstance(ind, pd.MultiIndex):\n            try:\n                return self.new(self.data.reindex(ind))\n            except:\n                raise RuntimeError('QADATASTRUCT ERROR: CANNOT REINDEX')\n        else:\n            raise RuntimeError(\n                'QADATASTRUCT ERROR: ONLY ACCEPT MULTI-INDEX FORMAT'\n            )", "response": "Reindex the object with the index ind."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_json(self):\n        \n        data = self.data\n        if self.type[-3:] != 'min':\n            data = self.data.assign(datetime= self.datetime)\n        return QA_util_to_json_from_pandas(data.reset_index())", "response": "Convert the data structure to json"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the data to HDF5.", "response": "def to_hdf(self, place, name):\n        'IO --> hdf5'\n        self.data.to_hdf(place, name)\n        return place, name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_same(self, DataStruct):\n        if self.type == DataStruct.type and self.if_fq == DataStruct.if_fq:\n            return True\n        else:\n            return False", "response": "Check if the two data structures are the same."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef splits(self):\n        return list(map(lambda x: self.select_code(x), self.code))", "response": "returns a list of all the code in the data struct"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a DataFrame with the data from the table.", "response": "def get_data(self, columns, type='ndarray', with_index=False):\n        \"\"\"\u83b7\u53d6\u4e0d\u540c\u683c\u5f0f\u7684\u6570\u636e\n\n        Arguments:\n            columns {[type]} -- [description]\n\n        Keyword Arguments:\n            type {str} -- [description] (default: {'ndarray'})\n            with_index {bool} -- [description] (default: {False})\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        res = self.select_columns(columns)\n        if type == 'ndarray':\n            if with_index:\n                return res.reset_index().values\n            else:\n                return res.values\n        elif type == 'list':\n            if with_index:\n                return res.reset_index().values.tolist()\n            else:\n                return res.values.tolist()\n        elif type == 'dataframe':\n            if with_index:\n                return res.reset_index()\n            else:\n                return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pivot(self, column_):\n        if isinstance(column_, str):\n            try:\n                return self.data.reset_index().pivot(\n                    index='datetime',\n                    columns='code',\n                    values=column_\n                )\n            except:\n                return self.data.reset_index().pivot(\n                    index='date',\n                    columns='code',\n                    values=column_\n                )\n        elif isinstance(column_, list):\n            try:\n                return self.data.reset_index().pivot_table(\n                    index='datetime',\n                    columns='code',\n                    values=column_\n                )\n            except:\n                return self.data.reset_index().pivot_table(\n                    index='date',\n                    columns='code',\n                    values=column_\n                )", "response": "Pivot a column of data into a new table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nselect a specific entry in the cache.", "response": "def selects(self, code, start, end=None):\n        \"\"\"\n        \u9009\u62e9code,start,end\n\n        \u5982\u679cend\u4e0d\u586b\u5199,\u9ed8\u8ba4\u83b7\u53d6\u5230\u7ed3\u5c3e\n\n        @2018/06/03 pandas \u7684\u7d22\u5f15\u95ee\u9898\u5bfc\u81f4\n        https://github.com/pandas-dev/pandas/issues/21299\n\n        \u56e0\u6b64\u5148\u7528set_index\u53bb\u91cd\u505a\u4e00\u6b21index\n        \u5f71\u54cd\u7684\u6709selects,select_time,select_month,get_bar\n\n        @2018/06/04\n        \u5f53\u9009\u62e9\u7684\u65f6\u95f4\u8d8a\u754c/\u80a1\u7968\u4e0d\u5b58\u5728,raise ValueError\n\n        @2018/06/04 pandas\u7d22\u5f15\u95ee\u9898\u5df2\u7ecf\u89e3\u51b3\n        \u5168\u90e8\u6062\u590d\n        \"\"\"\n\n        def _selects(code, start, end):\n            if end is not None:\n                return self.data.loc[(slice(pd.Timestamp(start), pd.Timestamp(end)), code), :]\n            else:\n                return self.data.loc[(slice(pd.Timestamp(start), None), code), :]\n\n        try:\n            return self.new(_selects(code, start, end), self.type, self.if_fq)\n        except:\n            raise ValueError(\n                'QA CANNOT GET THIS CODE {}/START {}/END{} '.format(\n                    code,\n                    start,\n                    end\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select_time(self, start, end=None):\n\n        def _select_time(start, end):\n            if end is not None:\n                return self.data.loc[(slice(pd.Timestamp(start), pd.Timestamp(end)), slice(None)), :]\n            else:\n                return self.data.loc[(slice(pd.Timestamp(start), None), slice(None)), :]\n\n        try:\n            return self.new(_select_time(start, end), self.type, self.if_fq)\n        except:\n            raise ValueError(\n                'QA CANNOT GET THIS START {}/END{} '.format(start,\n                                                            end)\n            )", "response": "select_time - Selects the time in the sequence of time."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nselects a specific day from the table.", "response": "def select_day(self, day):\n        \"\"\"\u9009\u53d6\u65e5\u671f(\u4e00\u822c\u7528\u4e8e\u5206\u949f\u7ebf)\n\n        Arguments:\n            day {[type]} -- [description]\n\n        Raises:\n            ValueError -- [description]\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        def _select_day(day):\n            return self.data.loc[day, slice(None)]\n\n        try:\n            return self.new(_select_day(day), self.type, self.if_fq)\n        except:\n            raise ValueError('QA CANNOT GET THIS Day {} '.format(day))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select_month(self, month):\n\n        def _select_month(month):\n            return self.data.loc[month, slice(None)]\n\n        try:\n            return self.new(_select_month(month), self.type, self.if_fq)\n        except:\n            raise ValueError('QA CANNOT GET THIS Month {} '.format(month))", "response": "select_month - Select the first instance of a specific month."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_code(self, code):\n\n        def _select_code(code):\n            return self.data.loc[(slice(None), code), :]\n\n        try:\n            return self.new(_select_code(code), self.type, self.if_fq)\n        except:\n            raise ValueError('QA CANNOT FIND THIS CODE {}'.format(code))", "response": "Select a specific ISO - 8601 code."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bar(self, code, time):\n        try:\n            return self.data.loc[(pd.Timestamp(time), code)]\n        except:\n            raise ValueError(\n                'DATASTRUCT CURRENTLY CANNOT FIND THIS BAR WITH {} {}'.format(\n                    code,\n                    time\n                )\n            )", "response": "Return a single bar with the given code and time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_best_ip_by_real_data_fetch(_type='stock'):\n    from QUANTAXIS.QAUtil.QADate import QA_util_today_str\n    import time\n    \n    #\u627e\u5230\u524d\u4e24\u5929\u7684\u6709\u6548\u4ea4\u6613\u65e5\u671f\n    pre_trade_date=QA_util_get_real_date(QA_util_today_str())\n    pre_trade_date=QA_util_get_real_date(pre_trade_date)\n    \n    # \u67d0\u4e2a\u51fd\u6570\u83b7\u53d6\u7684\u8017\u65f6\u6d4b\u8bd5\n    def get_stock_data_by_ip(ips):\n        start=time.time()\n        try:\n            QA_fetch_get_stock_transaction('000001',pre_trade_date,pre_trade_date,2,ips['ip'],ips['port'])\n            end=time.time()\n            return end-start\n        except:\n            return 9999\n\n    def get_future_data_by_ip(ips):\n        start=time.time()\n        try:\n            QA_fetch_get_future_transaction('RBL8',pre_trade_date,pre_trade_date,2,ips['ip'],ips['port'])\n            end=time.time()\n            return end-start\n        except:\n            return 9999\n\n    func,ip_list=0,0\n    if _type=='stock':\n        func,ip_list=get_stock_data_by_ip,stock_ip_list\n    else:\n        func,ip_list=get_future_data_by_ip,future_ip_list  \n    from pathos.multiprocessing import Pool\n    def multiMap(func,sequence):\n        res=[]\n        pool=Pool(4)\n        for i in sequence:\n            res.append(pool.apply_async(func,(i,)))\n        pool.close()\n        pool.join()\n        return list(map(lambda x:x.get(),res))\n   \n    res=multiMap(func,ip_list)\n    index=res.index(min(res))\n    return ip_list[index]", "response": "get best ip by real data fetch"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ip_list_by_multi_process_ping(ip_list=[], n=0, _type='stock'):\n    ''' \u6839\u636eping\u6392\u5e8f\u8fd4\u56de\u53ef\u7528\u7684ip\u5217\u8868\n    2019 03 31 \u53d6\u6d88\u53c2\u6570filename\n\n    :param ip_list: ip\u5217\u8868\n    :param n: \u6700\u591a\u8fd4\u56de\u7684ip\u6570\u91cf\uff0c \u5f53\u53ef\u7528ip\u6570\u91cf\u5c0f\u4e8en\uff0c\u8fd4\u56de\u6240\u6709\u53ef\u7528\u7684ip\uff1bn=0\u65f6\uff0c\u8fd4\u56de\u6240\u6709\u53ef\u7528ip\n    :param _type: ip\u7c7b\u578b\n    :return: \u53ef\u4ee5ping\u901a\u7684ip\u5217\u8868\n    '''\n    cache = QA_util_cache()\n    results = cache.get(_type)\n    if results:\n        # read the data from cache\n        print('loading ip list from {} cache.'.format(_type))\n    else:\n        ips = [(x['ip'], x['port'], _type) for x in ip_list]\n        ps = Parallelism()\n        ps.add(ping, ips)\n        ps.run()\n        data = list(ps.get_results())\n        results = []\n        for i in range(len(data)):\n            # \u5220\u9664ping\u4e0d\u901a\u7684\u6570\u636e\n            if data[i] < datetime.timedelta(0, 9, 0):\n                results.append((data[i], ip_list[i]))\n        # \u6309\u7167ping\u503c\u4ece\u5c0f\u5927\u5927\u6392\u5e8f\n        results = [x[1] for x in sorted(results, key=lambda x: x[0])]\n        if _type:\n                # store the data as binary data stream\n                cache.set(_type, results, age=86400)\n                print('saving ip list to {} cache {}.'.format(_type, len(results)))\n    if len(results) > 0:\n        if n == 0 and len(results) > 0:\n            return results\n        else:\n            return results[:n]\n    else:\n        print('ALL IP PING TIMEOUT!')\n        return [{'ip': None, 'port': None}]", "response": "get ip list by multi - process ping"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the mainmarket ip and port", "response": "def get_mainmarket_ip(ip, port):\n    \"\"\"[summary]\n\n    Arguments:\n        ip {[type]} -- [description]\n        port {[type]} -- [description]\n\n    Returns:\n        [type] -- [description]\n    \"\"\"\n\n    global best_ip\n    if ip is None and port is None and best_ip['stock']['ip'] is None and best_ip['stock']['port'] is None:\n        best_ip = select_best_ip()\n        ip = best_ip['stock']['ip']\n        port = best_ip['stock']['port']\n    elif ip is None and port is None and best_ip['stock']['ip'] is not None and best_ip['stock']['port'] is not None:\n        ip = best_ip['stock']['ip']\n        port = best_ip['stock']['port']\n    else:\n        pass\n    return ip, port"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all security bars from TdxHq", "response": "def QA_fetch_get_security_bars(code, _type, lens, ip=None, port=None):\n    \"\"\"\u6309bar\u957f\u5ea6\u63a8\u7b97\u6570\u636e\n\n    Arguments:\n        code {[type]} -- [description]\n        _type {[type]} -- [description]\n        lens {[type]} -- [description]\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {best_ip})\n        port {[type]} -- [description] (default: {7709})\n\n    Returns:\n        [type] -- [description]\n    \"\"\"\n    ip, port = get_mainmarket_ip(ip, port)\n    api = TdxHq_API()\n    with api.connect(ip, port):\n        data = pd.concat([api.to_df(api.get_security_bars(_select_type(_type), _select_market_code(\n            code), code, (i - 1) * 800, 800)) for i in range(1, int(lens / 800) + 2)], axis=0)\n        data = data \\\n            .drop(['year', 'month', 'day', 'hour', 'minute'], axis=1, inplace=False) \\\n            .assign(datetime=pd.to_datetime(data['datetime']),\n                    date=data['datetime'].apply(lambda x: str(x)[0:10]),\n                    date_stamp=data['datetime'].apply(\n                        lambda x: QA_util_date_stamp(x)),\n                    time_stamp=data['datetime'].apply(\n                        lambda x: QA_util_time_stamp(x)),\n                    type=_type, code=str(code)) \\\n            .set_index('datetime', drop=False, inplace=False).tail(lens)\n        if data is not None:\n            return data\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_fetch_get_stock_day(code, start_date, end_date, if_fq='00', frequence='day', ip=None, port=None):\n    ip, port = get_mainmarket_ip(ip, port)\n    api = TdxHq_API()\n    try:\n        with api.connect(ip, port, time_out=0.7):\n\n            if frequence in ['day', 'd', 'D', 'DAY', 'Day']:\n                frequence = 9\n            elif frequence in ['w', 'W', 'Week', 'week']:\n                frequence = 5\n            elif frequence in ['month', 'M', 'm', 'Month']:\n                frequence = 6\n            elif frequence in ['quarter', 'Q', 'Quarter', 'q']:\n                frequence = 10\n            elif frequence in ['y', 'Y', 'year', 'Year']:\n                frequence = 11\n            start_date = str(start_date)[0:10]\n            today_ = datetime.date.today()\n            lens = QA_util_get_trade_gap(start_date, today_)\n\n            data = pd.concat([api.to_df(api.get_security_bars(frequence, _select_market_code(\n                code), code, (int(lens / 800) - i) * 800, 800)) for i in range(int(lens / 800) + 1)], axis=0)\n\n            # \u8fd9\u91cc\u7684\u95ee\u9898\u662f: \u5982\u679c\u53ea\u53d6\u4e86\u4e00\u5929\u7684\u80a1\u7968,\u800c\u5f53\u5929\u505c\u724c, \u90a3\u4e48\u5c31\u76f4\u63a5\u8fd4\u56deNone\u4e86\n            if len(data) < 1:\n                return None\n            data = data[data['open'] != 0]\n\n            data = data.assign(date=data['datetime'].apply(lambda x: str(x[0:10])),\n                               code=str(code),\n                               date_stamp=data['datetime'].apply(lambda x: QA_util_date_stamp(str(x)[0:10]))) \\\n                .set_index('date', drop=False, inplace=False)\n\n            end_date = str(end_date)[0:10]\n            data = data.drop(['year', 'month', 'day', 'hour', 'minute', 'datetime'], axis=1)[\n                start_date:end_date]\n            if if_fq in ['00', 'bfq']:\n                return data\n            else:\n                print('CURRENTLY NOT SUPPORT REALTIME FUQUAN')\n                return None\n                # xdxr = QA_fetch_get_stock_xdxr(code)\n                # if if_fq in ['01','qfq']:\n                #     return QA_data_make_qfq(data,xdxr)\n                # elif if_fq in ['02','hfq']:\n                #     return QA_data_make_hfq(data,xdxr)\n    except Exception as e:\n        if isinstance(e, TypeError):\n            print('Tushare\u5185\u7f6e\u7684pytdx\u7248\u672c\u548cQUANTAXIS\u4f7f\u7528\u7684pytdx \u7248\u672c\u4e0d\u540c, \u8bf7\u91cd\u65b0\u5b89\u88c5pytdx\u4ee5\u89e3\u51b3\u6b64\u95ee\u9898')\n            print('pip uninstall pytdx')\n            print('pip install pytdx')\n        else:\n            print(e)", "response": "get a single item from the stock"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the name of the current node for the given code", "response": "def for_sz(code):\n    \"\"\"\u6df1\u5e02\u4ee3\u7801\u5206\u7c7b\n\n    Arguments:\n        code {[type]} -- [description]\n\n    Returns:\n        [type] -- [description]\n    \"\"\"\n\n    if str(code)[0:2] in ['00', '30', '02']:\n        return 'stock_cn'\n    elif str(code)[0:2] in ['39']:\n        return 'index_cn'\n    elif str(code)[0:2] in ['15']:\n        return 'etf_cn'\n    elif str(code)[0:2] in ['10', '11', '12', '13']:\n        # 10xxxx \u56fd\u503a\u73b0\u8d27\n        # 11xxxx \u503a\u5238\n        # 12xxxx \u53ef\u8f6c\u6362\u503a\u5238\n        # 12xxxx \u56fd\u503a\u56de\u8d2d\n        return 'bond_cn'\n\n    elif str(code)[0:2] in ['20']:\n        return 'stockB_cn'\n    else:\n        return 'undefined'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the list of all security items", "response": "def QA_fetch_get_index_list(ip=None, port=None):\n    \"\"\"\u83b7\u53d6\u6307\u6570\u5217\u8868\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n    Returns:\n        [type] -- [description]\n    \"\"\"\n\n    ip, port = get_mainmarket_ip(ip, port)\n    api = TdxHq_API()\n    with api.connect(ip, port):\n        data = pd.concat(\n            [pd.concat([api.to_df(api.get_security_list(j, i * 1000)).assign(sse='sz' if j == 0 else 'sh').set_index(\n                ['code', 'sse'], drop=False) for i in range(int(api.get_security_count(j) / 1000) + 1)], axis=0) for j\n                in range(2)], axis=0)\n        # data.code = data.code.apply(int)\n        sz = data.query('sse==\"sz\"')\n        sh = data.query('sse==\"sh\"')\n\n        sz = sz.assign(sec=sz.code.apply(for_sz))\n        sh = sh.assign(sec=sh.code.apply(for_sh))\n        return pd.concat([sz, sh]).query('sec==\"index_cn\"').sort_index().assign(\n            name=data['name'].apply(lambda x: str(x)[0:6]))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_fetch_get_stock_transaction_realtime(code, ip=None, port=None):\n    '\u5b9e\u65f6\u5206\u7b14\u6210\u4ea4 \u5305\u542b\u96c6\u5408\u7ade\u4ef7 buyorsell 1--sell 0--buy 2--\u76d8\u524d'\n    ip, port = get_mainmarket_ip(ip, port)\n    api = TdxHq_API()\n    try:\n        with api.connect(ip, port):\n            data = pd.DataFrame()\n            data = pd.concat([api.to_df(api.get_transaction_data(\n                _select_market_code(str(code)), code, (2 - i) * 2000, 2000)) for i in range(3)], axis=0)\n            if 'value' in data.columns:\n                data = data.drop(['value'], axis=1)\n            data = data.dropna()\n            day = datetime.date.today()\n            return data.assign(date=str(day)).assign(\n                datetime=pd.to_datetime(data['time'].apply(lambda x: str(day) + ' ' + str(x)))) \\\n                .assign(code=str(code)).assign(order=range(len(data.index))).set_index('datetime', drop=False,\n                                                                                       inplace=False)\n    except:\n        return None", "response": "Get stock transaction realtime"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the list of all future items", "response": "def QA_fetch_get_future_list(ip=None, port=None):\n    \"\"\"[summary]\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n    42         3      \u5546\u54c1\u6307\u6570         TI\n    60         3    \u4e3b\u529b\u671f\u8d27\u5408\u7ea6         MA\n    28         3      \u90d1\u5dde\u5546\u54c1         QZ\n    29         3      \u5927\u8fde\u5546\u54c1         QD\n    30         3      \u4e0a\u6d77\u671f\u8d27(\u539f\u6cb9+\u8d35\u91d1\u5c5e)  QS\n    47         3     \u4e2d\u91d1\u6240\u671f\u8d27         CZ\n\n    50         3      \u6e24\u6d77\u5546\u54c1         BH\n    76         3      \u9f50\u9c81\u5546\u54c1         QL\n\n\n    46        11      \u4e0a\u6d77\u9ec4\u91d1(\u4f26\u6566\u91d1T+D)         SG\n    \"\"\"\n\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==42 or market==28 or market==29 or market==30 or market==47')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_fetch_get_globalindex_list(ip=None, port=None):\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==12 or market==37')", "response": "Function to get the global index list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the goods list", "response": "def QA_fetch_get_goods_list(ip=None, port=None):\n    \"\"\"[summary]\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n    42         3      \u5546\u54c1\u6307\u6570         TI\n    60         3    \u4e3b\u529b\u671f\u8d27\u5408\u7ea6         MA\n    28         3      \u90d1\u5dde\u5546\u54c1         QZ\n    29         3      \u5927\u8fde\u5546\u54c1         QD\n    30         3      \u4e0a\u6d77\u671f\u8d27(\u539f\u6cb9+\u8d35\u91d1\u5c5e)  QS\n    47         3     \u4e2d\u91d1\u6240\u671f\u8d27         CZ\n\n    50         3      \u6e24\u6d77\u5546\u54c1         BH\n    76         3      \u9f50\u9c81\u5546\u54c1         QL\n\n\n    46        11      \u4e0a\u6d77\u9ec4\u91d1(\u4f26\u6566\u91d1T+D)         SG\n    \"\"\"\n\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==50 or market==76 or market==46')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QA_fetch_get_globalfuture_list(ip=None, port=None):\n\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query(\n        'market==14 or market==15 or market==16 or market==17 or market==18 or market==19 or market==20 or market==77 or market==39')", "response": "Get global future list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_fetch_get_hkstock_list(ip=None, port=None):\n\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==31 or market==48')", "response": "Get HK stock list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_fetch_get_hkindex_list(ip=None, port=None):\n\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==27')", "response": "Function to get the HK index list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the list of USA s stocks", "response": "def QA_fetch_get_usstock_list(ip=None, port=None):\n    \"\"\"[summary]\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n    ## \u7f8e\u80a1 USA STOCK\n        74        13      \u7f8e\u56fd\u80a1\u7968         US\n        40        11     \u4e2d\u56fd\u6982\u5ff5\u80a1         CH\n        41        11    \u7f8e\u80a1\u77e5\u540d\u516c\u53f8         MG\n\n\n    \"\"\"\n\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==74 or market==40 or market==41')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions to get the list of macro index", "response": "def QA_fetch_get_macroindex_list(ip=None, port=None):\n    \"\"\"\u5b8f\u89c2\u6307\u6807\u5217\u8868\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n        38        10      \u5b8f\u89c2\u6307\u6807         HG\n\n\n    \"\"\"\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==38')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_fetch_get_option_list(ip=None, port=None):\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('category==12 and market!=1')", "response": "Get the list of all available option"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QA_fetch_get_option_contract_time_to_market():\n    '''\n    #\ud83d\udee0todo \u83b7\u53d6\u671f\u6743\u5408\u7ea6\u7684\u4e0a\u5e02\u65e5\u671f \uff1f \u6682\u65f6\u6ca1\u6709\u3002\n    :return: list Series\n    '''\n    result = QA_fetch_get_option_list('tdx')\n    # pprint.pprint(result)\n    #  category  market code name desc  code\n    '''\n    fix here : \n    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n    result['meaningful_name'] = None\n    C:\\work_new\\QUANTAXIS\\QUANTAXIS\\QAFetch\\QATdx.py:1468: SettingWithCopyWarning: \n    A value is trying to be set on a copy of a slice from a DataFrame.\n    Try using .loc[row_indexer,col_indexer] = value instead\n    '''\n    # df = pd.DataFrame()\n    rows = []\n\n    result['meaningful_name'] = None\n    for idx in result.index:\n        # pprint.pprint((idx))\n        strCategory = result.loc[idx, \"category\"]\n        strMarket = result.loc[idx, \"market\"]\n        strCode = result.loc[idx, \"code\"]  # 10001215\n        strName = result.loc[idx, 'name']  # 510050C9M03200\n        strDesc = result.loc[idx, 'desc']  # 10001215\n\n        if strName.startswith(\"510050\"):\n            # print(strCategory,' ', strMarket, ' ', strCode, ' ', strName, ' ', strDesc, )\n\n            if strName.startswith(\"510050C\"):\n                putcall = '50ETF,\u8ba4\u8d2d\u671f\u6743'\n            elif strName.startswith(\"510050P\"):\n                putcall = '50ETF,\u8ba4\u6cbd\u671f\u6743'\n            else:\n                putcall = \"Unkown code name \uff1a \" + strName\n\n            expireMonth = strName[7:8]\n            if expireMonth == 'A':\n                expireMonth = \"10\u6708\"\n            elif expireMonth == 'B':\n                expireMonth = \"11\u6708\"\n            elif expireMonth == 'C':\n                expireMonth = \"12\u6708\"\n            else:\n                expireMonth = expireMonth + '\u6708'\n\n            # \u7b2c12\u4f4d\u671f\u521d\u8bbe\u4e3a\u201cM\u201d\uff0c\u5e76\u6839\u636e\u5408\u7ea6\u8c03\u6574\u6b21\u6570\u6309\u7167\u201cA\u201d\u81f3\u201cZ\u201d\u4f9d\u5e8f\u53d8\u66f4\uff0c\u5982\u53d8\u66f4\u4e3a\u201cA\u201d\u8868\u793a\u671f\u6743\u5408\u7ea6\u53d1\u751f\u9996\u6b21\u8c03\u6574\uff0c\u53d8\u66f4\u4e3a\u201cB\u201d\u8868\u793a\u671f\u6743\u5408\u7ea6\u53d1\u751f\u7b2c\u4e8c\u6b21\u8c03\u6574\uff0c\u4f9d\u6b64\u7c7b\u63a8\uff1b\n            # fix here : M ??\n            if strName[8:9] == \"M\":\n                adjust = \"\u672a\u8c03\u6574\"\n            elif strName[8:9] == 'A':\n                adjust = \" \u7b2c1\u6b21\u8c03\u6574\"\n            elif strName[8:9] == 'B':\n                adjust = \" \u7b2c2\u8c03\u6574\"\n            elif strName[8:9] == 'C':\n                adjust = \" \u7b2c3\u6b21\u8c03\u6574\"\n            elif strName[8:9] == 'D':\n                adjust = \" \u7b2c4\u6b21\u8c03\u6574\"\n            elif strName[8:9] == 'E':\n                adjust = \" \u7b2c5\u6b21\u8c03\u6574\"\n            elif strName[8:9] == 'F':\n                adjust = \" \u7b2c6\u6b21\u8c03\u6574\"\n            elif strName[8:9] == 'G':\n                adjust = \" \u7b2c7\u6b21\u8c03\u6574\"\n            elif strName[8:9] == 'H':\n                adjust = \" \u7b2c8\u6b21\u8c03\u6574\"\n            elif strName[8:9] == 'I':\n                adjust = \" \u7b2c9\u6b21\u8c03\u6574\"\n            elif strName[8:9] == 'J':\n                adjust = \" \u7b2c10\u6b21\u8c03\u6574\"\n            else:\n                adjust = \" \u7b2c10\u6b21\u4ee5\u4e0a\u7684\u8c03\u6574\uff0c\u8c03\u6574\u4ee3\u7801 %s\" + strName[8:9]\n\n            executePrice = strName[9:]\n            result.loc[idx, 'meaningful_name'] = '%s,\u5230\u671f\u6708\u4efd:%s,%s,\u884c\u6743\u4ef7:%s' % (\n                putcall, expireMonth, adjust, executePrice)\n\n            row = result.loc[idx]\n            rows.append(row)\n\n        elif strName.startswith(\"SR\"):\n            # print(\"SR\")\n            # SR1903-P-6500\n            expireYear = strName[2:4]\n            expireMonth = strName[4:6]\n\n            put_or_call = strName[7:8]\n            if put_or_call == \"P\":\n                putcall = \"\u767d\u7cd6,\u8ba4\u6cbd\u671f\u6743\"\n            elif put_or_call == \"C\":\n                putcall = \"\u767d\u7cd6,\u8ba4\u8d2d\u671f\u6743\"\n            else:\n                putcall = \"Unkown code name \uff1a \" + strName\n\n            executePrice = strName[9:]\n            result.loc[idx, 'meaningful_name'] = '%s,\u5230\u671f\u5e74\u6708\u4efd:%s%s,\u884c\u6743\u4ef7:%s' % (\n                putcall, expireYear, expireMonth, executePrice)\n\n            row = result.loc[idx]\n            rows.append(row)\n\n            pass\n        elif strName.startswith(\"CU\"):\n            # print(\"CU\")\n\n            # print(\"SR\")\n            # SR1903-P-6500\n            expireYear = strName[2:4]\n            expireMonth = strName[4:6]\n\n            put_or_call = strName[7:8]\n            if put_or_call == \"P\":\n                putcall = \"\u94dc,\u8ba4\u6cbd\u671f\u6743\"\n            elif put_or_call == \"C\":\n                putcall = \"\u94dc,\u8ba4\u8d2d\u671f\u6743\"\n            else:\n                putcall = \"Unkown code name \uff1a \" + strName\n\n            executePrice = strName[9:]\n            result.loc[idx, 'meaningful_name'] = '%s,\u5230\u671f\u5e74\u6708\u4efd:%s%s,\u884c\u6743\u4ef7:%s' % (\n                putcall, expireYear, expireMonth, executePrice)\n\n            row = result.loc[idx]\n            rows.append(row)\n\n            pass\n        # todo \u65b0\u589e\u671f\u6743\u54c1\u79cd \u68c9\u82b1\uff0c\u7389\u7c73\uff0c \u5929\u7136\u6a61\u80f6\n        elif strName.startswith(\"RU\"):\n            # print(\"M\")\n            # print(strName)\n            ##\n            expireYear = strName[2:4]\n            expireMonth = strName[4:6]\n\n            put_or_call = strName[7:8]\n            if put_or_call == \"P\":\n                putcall = \"\u5929\u7136\u6a61\u80f6,\u8ba4\u6cbd\u671f\u6743\"\n            elif put_or_call == \"C\":\n                putcall = \"\u5929\u7136\u6a61\u80f6,\u8ba4\u8d2d\u671f\u6743\"\n            else:\n                putcall = \"Unkown code name \uff1a \" + strName\n\n            executePrice = strName[9:]\n            result.loc[idx, 'meaningful_name'] = '%s,\u5230\u671f\u5e74\u6708\u4efd:%s%s,\u884c\u6743\u4ef7:%s' % (\n                putcall, expireYear, expireMonth, executePrice)\n\n            row = result.loc[idx]\n            rows.append(row)\n\n            pass\n\n        elif strName.startswith(\"CF\"):\n            # print(\"M\")\n            # print(strName)\n            ##\n            expireYear = strName[2:4]\n            expireMonth = strName[4:6]\n\n            put_or_call = strName[7:8]\n            if put_or_call == \"P\":\n                putcall = \"\u68c9\u82b1,\u8ba4\u6cbd\u671f\u6743\"\n            elif put_or_call == \"C\":\n                putcall = \"\u68c9\u82b1,\u8ba4\u8d2d\u671f\u6743\"\n            else:\n                putcall = \"Unkown code name \uff1a \" + strName\n\n            executePrice = strName[9:]\n            result.loc[idx, 'meaningful_name'] = '%s,\u5230\u671f\u5e74\u6708\u4efd:%s%s,\u884c\u6743\u4ef7:%s' % (\n                putcall, expireYear, expireMonth, executePrice)\n\n            row = result.loc[idx]\n            rows.append(row)\n\n            pass\n\n        elif strName.startswith(\"M\"):\n            # print(\"M\")\n            # print(strName)\n            ##\n            expireYear = strName[1:3]\n            expireMonth = strName[3:5]\n\n            put_or_call = strName[6:7]\n            if put_or_call == \"P\":\n                putcall = \"\u8c46\u7c95,\u8ba4\u6cbd\u671f\u6743\"\n            elif put_or_call == \"C\":\n                putcall = \"\u8c46\u7c95,\u8ba4\u8d2d\u671f\u6743\"\n            else:\n                putcall = \"Unkown code name \uff1a \" + strName\n\n            executePrice = strName[8:]\n            result.loc[idx, 'meaningful_name'] = '%s,\u5230\u671f\u5e74\u6708\u4efd:%s%s,\u884c\u6743\u4ef7:%s' % (\n                putcall, expireYear, expireMonth, executePrice)\n\n            row = result.loc[idx]\n            rows.append(row)\n\n            pass\n        elif strName.startswith(\"C\") and strName[1] != 'F' and strName[1] != 'U':\n            # print(\"M\")\n            # print(strName)\n            ##\n            expireYear = strName[1:3]\n            expireMonth = strName[3:5]\n\n            put_or_call = strName[6:7]\n            if put_or_call == \"P\":\n                putcall = \"\u7389\u7c73,\u8ba4\u6cbd\u671f\u6743\"\n            elif put_or_call == \"C\":\n                putcall = \"\u7389\u7c73,\u8ba4\u8d2d\u671f\u6743\"\n            else:\n                putcall = \"Unkown code name \uff1a \" + strName\n\n            executePrice = strName[8:]\n            result.loc[idx, 'meaningful_name'] = '%s,\u5230\u671f\u5e74\u6708\u4efd:%s%s,\u884c\u6743\u4ef7:%s' % (\n                putcall, expireYear, expireMonth, executePrice)\n\n            row = result.loc[idx]\n            rows.append(row)\n\n            pass\n        else:\n            print(\"\u672a\u77e5\u7c7b\u578b\u5408\u7ea6\")\n            print(strName)\n\n    return rows", "response": "QA_fetch_get_option_contract_time_to_market - get the order of the order of the items in the order of the items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef QA_fetch_get_commodity_option_CF_contract_time_to_market():\n    '''\n    \u94dc\u671f\u6743  CU \u5f00\u5934   \u4e0a\u671f\u8bc1\n    \u8c46\u7c95    M\u5f00\u5934     \u5927\u5546\u6240\n    \u767d\u7cd6    SR\u5f00\u5934    \u90d1\u5546\u6240\n    \u6d4b\u8bd5\u4e2d\u53d1\u73b0\uff0c\u884c\u60c5\u4e0d\u592a\u7a33\u5b9a \uff1f \u662f \u901a\u8fbe\u4fe1 IP \u7684\u95ee\u9898 \uff1f\n    '''\n\n    result = QA_fetch_get_option_list('tdx')\n    # pprint.pprint(result)\n    #  category  market code name desc  code\n\n    # df = pd.DataFrame()\n    rows = []\n    result['meaningful_name'] = None\n    for idx in result.index:\n        # pprint.pprint((idx))\n        strCategory = result.loc[idx, \"category\"]\n        strMarket = result.loc[idx, \"market\"]\n        strCode = result.loc[idx, \"code\"]  #\n        strName = result.loc[idx, 'name']  #\n        strDesc = result.loc[idx, 'desc']  #\n\n        # \u5982\u679c\u540c\u65f6\u83b7\u53d6\uff0c \u4e0d\u540c\u7684 \u671f\u8d27\u4ea4\u6613\u6240\u6570\u636e\uff0c pytdx\u4f1a connection close \u8fde\u63a5\u4e2d\u65ad\uff1f\n        # if strName.startswith(\"CU\") or strName.startswith(\"M\") or strName.startswith('SR'):\n        if strName.startswith(\"CF\"):\n            # print(strCategory,' ', strMarket, ' ', strCode, ' ', strName, ' ', strDesc, )\n            row = result.loc[idx]\n            rows.append(row)\n\n    return rows\n\n    pass", "response": "get the list of commodity option CF contract time to market"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_fetch_get_exchangerate_list(ip=None, port=None):\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==10 or market==11').query('category==4')", "response": "Get the list of exchangerate items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_fetch_get_future_min(code, start, end, frequence='1min', ip=None, port=None):\n    '\u671f\u8d27\u6570\u636e \u5206\u949f\u7ebf'\n    ip, port = get_extensionmarket_ip(ip, port)\n    apix = TdxExHq_API()\n    type_ = ''\n    start_date = str(start)[0:10]\n    today_ = datetime.date.today()\n    lens = QA_util_get_trade_gap(start_date, today_)\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    if str(frequence) in ['5', '5m', '5min', 'five']:\n        frequence, type_ = 0, '5min'\n        lens = 48 * lens * 2.5\n    elif str(frequence) in ['1', '1m', '1min', 'one']:\n        frequence, type_ = 8, '1min'\n        lens = 240 * lens * 2.5\n    elif str(frequence) in ['15', '15m', '15min', 'fifteen']:\n        frequence, type_ = 1, '15min'\n        lens = 16 * lens * 2.5\n    elif str(frequence) in ['30', '30m', '30min', 'half']:\n        frequence, type_ = 2, '30min'\n        lens = 8 * lens * 2.5\n    elif str(frequence) in ['60', '60m', '60min', '1h']:\n        frequence, type_ = 3, '60min'\n        lens = 4 * lens * 2.5\n    if lens > 20800:\n        lens = 20800\n\n    # print(lens)\n    with apix.connect(ip, port):\n\n        code_market = extension_market_list.query(\n            'code==\"{}\"'.format(code)).iloc[0]\n        data = pd.concat([apix.to_df(apix.get_instrument_bars(frequence, int(code_market.market), str(\n            code), (int(lens / 700) - i) * 700, 700)) for i in range(int(lens / 700) + 1)], axis=0)\n        # print(data)\n        # print(data.datetime)\n        data = data \\\n            .assign(tradetime=data['datetime'].apply(str), code=str(code)) \\\n            .assign(datetime=pd.to_datetime(data['datetime'].apply(QA_util_future_to_realdatetime, 1))) \\\n            .drop(['year', 'month', 'day', 'hour', 'minute'], axis=1, inplace=False) \\\n            .assign(date=data['datetime'].apply(lambda x: str(x)[0:10])) \\\n            .assign(date_stamp=data['datetime'].apply(lambda x: QA_util_date_stamp(x))) \\\n            .assign(time_stamp=data['datetime'].apply(lambda x: QA_util_time_stamp(x))) \\\n            .assign(type=type_).set_index('datetime', drop=False, inplace=False)\n        return data.assign(datetime=data['datetime'].apply(lambda x: str(x)))[start:end].sort_index()", "response": "get the future min"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting future transaction realtime", "response": "def QA_fetch_get_future_transaction_realtime(code, ip=None, port=None):\n    '\u671f\u8d27\u5386\u53f2\u6210\u4ea4\u5206\u7b14'\n    ip, port = get_extensionmarket_ip(ip, port)\n    apix = TdxExHq_API()\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    code_market = extension_market_list.query(\n        'code==\"{}\"'.format(code)).iloc[0]\n    with apix.connect(ip, port):\n        data = pd.DataFrame()\n        data = pd.concat([apix.to_df(apix.get_transaction_data(\n            int(code_market.market), code, (30 - i) * 1800)) for i in range(31)], axis=0)\n        return data.assign(datetime=pd.to_datetime(data['date'])).assign(date=lambda x: str(x)[0:10]) \\\n            .assign(code=str(code)).assign(order=range(len(data.index))).set_index('datetime', drop=False,\n                                                                                   inplace=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef datastruct_formater(\n        data,\n        frequence=FREQUENCE.DAY,\n        market_type=MARKET_TYPE.STOCK_CN,\n        default_header=[]\n):\n    \"\"\"\u4e00\u4e2a\u4efb\u610f\u683c\u5f0f\u8f6c\u5316\u4e3aDataStruct\u7684\u65b9\u6cd5\n    \n    Arguments:\n        data {[type]} -- [description]\n    \n    Keyword Arguments:\n        frequence {[type]} -- [description] (default: {FREQUENCE.DAY})\n        market_type {[type]} -- [description] (default: {MARKET_TYPE.STOCK_CN})\n        default_header {list} -- [description] (default: {[]})\n    \n    Returns:\n        [type] -- [description]\n    \"\"\"\n\n    if isinstance(data, list):\n        try:\n            res = pd.DataFrame(data, columns=default_header)\n            if frequence is FREQUENCE.DAY:\n                if market_type is MARKET_TYPE.STOCK_CN:\n                    return QA_DataStruct_Stock_day(\n                        res.assign(date=pd.to_datetime(res.date)\n                                  ).set_index(['date',\n                                               'code'],\n                                              drop=False),\n                        dtype='stock_day'\n                    )\n            elif frequence in [FREQUENCE.ONE_MIN,\n                               FREQUENCE.FIVE_MIN,\n                               FREQUENCE.FIFTEEN_MIN,\n                               FREQUENCE.THIRTY_MIN,\n                               FREQUENCE.SIXTY_MIN]:\n                if market_type is MARKET_TYPE.STOCK_CN:\n                    return QA_DataStruct_Stock_min(\n                        res.assign(datetime=pd.to_datetime(res.datetime)\n                                  ).set_index(['datetime',\n                                               'code'],\n                                              drop=False),\n                        dtype='stock_min'\n                    )\n        except:\n            pass\n    elif isinstance(data, np.ndarray):\n        try:\n            res = pd.DataFrame(data, columns=default_header)\n            if frequence is FREQUENCE.DAY:\n                if market_type is MARKET_TYPE.STOCK_CN:\n                    return QA_DataStruct_Stock_day(\n                        res.assign(date=pd.to_datetime(res.date)\n                                  ).set_index(['date',\n                                               'code'],\n                                              drop=False),\n                        dtype='stock_day'\n                    )\n            elif frequence in [FREQUENCE.ONE_MIN,\n                               FREQUENCE.FIVE_MIN,\n                               FREQUENCE.FIFTEEN_MIN,\n                               FREQUENCE.THIRTY_MIN,\n                               FREQUENCE.SIXTY_MIN]:\n                if market_type is MARKET_TYPE.STOCK_CN:\n                    return QA_DataStruct_Stock_min(\n                        res.assign(datetime=pd.to_datetime(res.datetime)\n                                  ).set_index(['datetime',\n                                               'code'],\n                                              drop=False),\n                        dtype='stock_min'\n                    )\n        except:\n            pass\n\n    elif isinstance(data, pd.DataFrame):\n        index = data.index\n        if isinstance(index, pd.MultiIndex):\n            pass\n        elif isinstance(index, pd.DatetimeIndex):\n            pass\n        elif isinstance(index, pd.Index):\n            pass", "response": "Function to create a DataStruct object from a list of data items."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef QA_fetch_get_stock_adj(code, end=''):\n\n    pro = get_pro()\n    adj = pro.adj_factor(ts_code=code, trade_date=end)\n    return adj", "response": "Function to get the stock adjacency factor"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new object with the given data", "response": "def new(self, data):\n        \"\"\"\u901a\u8fc7data\u65b0\u5efa\u4e00\u4e2astock_block\n\n        Arguments:\n            data {[type]} -- [description]\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n        temp = copy(self)\n        temp.__init__(data)\n        return temp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nviewing code of the block.", "response": "def view_code(self):\n        \"\"\"\u6309\u80a1\u7968\u6392\u5217\u7684\u67e5\u770bblockname\u7684\u89c6\u56fe\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n\n        return self.data.groupby(level=1).apply(\n            lambda x:\n            [item for item in x.index.remove_unused_levels().levels[0]]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_code(self, code):\n        # code= [code] if isinstance(code,str) else\n        return self.new(self.data.loc[(slice(None), code), :])", "response": "getcode \u83b7\u53d6\u67d0\u4e00\u53ea\u80a1\u7968\u7684\u677f\u5757\n\n        Arguments:\n            code {str} -- \u80a1\u7968\u4ee3\u7801\n\n        Returns:\n            DataStruct -- [description]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_both_code(self, code):\n\n        return self.new(self.data.loc[(slice(None), code), :])", "response": "get_both_code - Get a single entry from the list of both key codes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets tick from stock or future", "response": "def QA_get_tick(code, start, end, market):\n    \"\"\"\n    \u7edf\u4e00\u7684\u83b7\u53d6\u671f\u8d27/\u80a1\u7968tick\u7684\u63a5\u53e3\n    \"\"\"\n    res = None\n    if market == MARKET_TYPE.STOCK_CN:\n        res = QATdx.QA_fetch_get_stock_transaction(code, start, end)\n    elif market == MARKET_TYPE.FUTURE_CN:\n        res = QATdx.QA_fetch_get_future_transaction(code, start, end)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget realtime time for a given market", "response": "def QA_get_realtime(code, market):\n    \"\"\"\n    \u7edf\u4e00\u7684\u83b7\u53d6\u671f\u8d27/\u80a1\u7968\u5b9e\u65f6\u884c\u60c5\u7684\u63a5\u53e3\n    \"\"\"\n    res = None\n    if market == MARKET_TYPE.STOCK_CN:\n        res = QATdx.QA_fetch_get_stock_realtime(code)\n    elif market == MARKET_TYPE.FUTURE_CN:\n        res = QATdx.QA_fetch_get_future_realtime(code)\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_util_random_with_zh_stock_code(stockNumber=10):\n    '''\n    \u968f\u673a\u751f\u6210\u80a1\u7968\u4ee3\u7801\n    :param stockNumber: \u751f\u6210\u4e2a\u6570\n    :return:  ['60XXXX', '00XXXX', '300XXX']\n    '''\n    codeList = []\n    pt = 0\n    for i in range(stockNumber):\n        if pt == 0:\n            #print(\"random 60XXXX\")\n            iCode = random.randint(600000, 609999)\n            aCode = \"%06d\" % iCode\n\n        elif pt == 1:\n            #print(\"random 00XXXX\")\n            iCode = random.randint(600000, 600999)\n            aCode = \"%06d\" % iCode\n\n        elif pt == 2:\n            #print(\"random 00XXXX\")\n            iCode = random.randint(2000, 9999)\n            aCode = \"%06d\" % iCode\n\n        elif pt == 3:\n            #print(\"random 300XXX\")\n            iCode = random.randint(300000, 300999)\n            aCode = \"%06d\" % iCode\n\n        elif pt == 4:\n            #print(\"random 00XXXX\")\n            iCode = random.randint(2000, 2999)\n            aCode = \"%06d\" % iCode\n        pt = (pt + 1) % 5\n        codeList.append(aCode)\n    return codeList", "response": "\u968f\u673a\u751f\u6210\u80a1\u7968\u4ee3\u7801\n    :param stockNumber: \u751f\u6210\u4e2a\u6570\n    :return:  ['60XXXX', '00XXXX', '300XXX']"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_pos(self, price, amount, towards):\n        temp_cost = amount*price * \\\n            self.market_preset.get('unit_table', 1)\n        # if towards == ORDER_DIRECTION.SELL_CLOSE:\n        if towards == ORDER_DIRECTION.BUY:\n            # \u80a1\u7968\u6a21\u5f0f/ \u671f\u8d27\u4e70\u5165\u5f00\u4ed3\n            self.volume_long_today += amount\n        elif towards == ORDER_DIRECTION.SELL:\n            # \u80a1\u7968\u5356\u51fa\u6a21\u5f0f:\n            # \u4eca\u65e5\u4e70\u5165\u4ed3\u4f4d\u4e0d\u80fd\u5356\u51fa\n            if self.volume_long_his > amount:\n                self.volume_long_his -= amount\n\n        elif towards == ORDER_DIRECTION.BUY_OPEN:\n\n            # \u589e\u52a0\u4fdd\u8bc1\u91d1\n            self.margin_long += temp_cost * \\\n                self.market_preset['buy_frozen_coeff']\n            # \u91cd\u7b97\u5f00\u4ed3\u5747\u4ef7\n            self.open_price_long = (\n                self.open_price_long * self.volume_long + amount*price) / (amount + self.volume_long)\n            # \u91cd\u7b97\u6301\u4ed3\u5747\u4ef7\n            self.position_price_long = (\n                self.position_price_long * self.volume_long + amount * price) / (amount + self.volume_long)\n            # \u589e\u52a0\u4eca\u4ed3\u6570\u91cf ==> \u4f1a\u81ea\u52a8\u589e\u52a0volume_long\n            self.volume_long_today += amount\n            #\n            self.open_cost_long += temp_cost\n\n        elif towards == ORDER_DIRECTION.SELL_OPEN:\n            # \u589e\u52a0\u4fdd\u8bc1\u91d1\n\n            self.margin_short += temp_cost * \\\n                self.market_preset['sell_frozen_coeff']\n            # \u91cd\u65b0\u8ba1\u7b97\u5f00\u4ed3/\u6301\u4ed3\u6210\u672c\n            self.open_price_short = (\n                self.open_price_short * self.volume_short + amount*price) / (amount + self.volume_short)\n            self.position_price_short = (\n                self.position_price_short * self.volume_short + amount * price) / (amount + self.volume_short)\n            self.open_cost_short += temp_cost\n            self.volume_short_today += amount\n\n        elif towards == ORDER_DIRECTION.BUY_CLOSETODAY:\n            if self.volume_short_today > amount:\n                self.position_cost_short = self.position_cost_short * \\\n                    (self.volume_short-amount)/self.volume_short\n                self.open_cost_short = self.open_cost_short * \\\n                    (self.volume_short-amount)/self.volume_short\n                self.volume_short_today -= amount\n                # close_profit = (self.position_price_short - price) * volume * position->ins->volume_multiple;\n\n                #self.volume_short_frozen_today += amount\n                # \u91ca\u653e\u4fdd\u8bc1\u91d1\n                # TODO\n                # self.margin_short\n                #self.open_cost_short = price* amount\n\n        elif towards == ORDER_DIRECTION.SELL_CLOSETODAY:\n            if self.volume_long_today > amount:\n                self.position_cost_long = self.position_cost_long * \\\n                    (self.volume_long - amount)/self.volume_long\n                self.open_cost_long = self.open_cost_long * \\\n                    (self.volume_long-amount)/self.volume_long\n                self.volume_long_today -= amount\n\n        elif towards == ORDER_DIRECTION.BUY_CLOSE:\n            # \u6709\u6628\u4ed3\u5148\u5e73\u6628\u4ed3\n            self.position_cost_short = self.position_cost_short * \\\n                (self.volume_short-amount)/self.volume_short\n            self.open_cost_short = self.open_cost_short * \\\n                (self.volume_short-amount)/self.volume_short\n            if self.volume_short_his >= amount:\n                self.volume_short_his -= amount\n            else:\n                self.volume_short_today -= (amount - self.volume_short_his)\n                self.volume_short_his = 0\n        elif towards == ORDER_DIRECTION.SELL_CLOSE:\n            # \u6709\u6628\u4ed3\u5148\u5e73\u6628\u4ed3\n            self.position_cost_long = self.position_cost_long * \\\n                (self.volume_long - amount)/self.volume_long\n            self.open_cost_long = self.open_cost_long * \\\n                (self.volume_long-amount)/self.volume_long\n            if self.volume_long_his >= amount:\n                self.volume_long_his -= amount\n            else:\n                self.volume_long_today -= (amount - self.volume_long_his)\n                self.volume_long_his -= amount", "response": "Update position of the current node based on the amount of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsettling the current time - of - day and time - of - day.", "response": "def settle(self):\n        \"\"\"\u6536\u76d8\u540e\u7684\u7ed3\u7b97\u4e8b\u4ef6\n        \"\"\"\n        self.volume_long_his += self.volume_long_today\n        self.volume_long_today = 0\n        self.volume_long_frozen_today = 0\n        self.volume_short_his += self.volume_short_today\n        self.volume_short_today = 0\n        self.volume_short_frozen_today = 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict of available types and descriptions.", "response": "def close_available(self):\n        \"\"\"\u53ef\u5e73\u4ed3\u6570\u91cf\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n        return {\n            'volume_long': self.volume_long - self.volume_long_frozen,\n            'volume_short': self.volume_short - self.volume_short_frozen\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(command, shell=None, working_dir=\".\", echo=False, echo_indent=0):\n    if shell is None:\n        shell = True if isinstance(command, str) else False\n\n    p = Popen(command, stdin=PIPE, stdout=PIPE,\n              stderr=STDOUT, shell=shell, cwd=working_dir)\n\n    if echo:\n        stdout = \"\"\n        while p.poll() is None:\n            # This blocks until it receives a newline.\n            line = p.stdout.readline()\n            print(\" \" * echo_indent, line, end=\"\")\n            stdout += line\n\n        # Read any last bits\n        line = p.stdout.read()\n        print(\" \" * echo_indent, line, end=\"\")\n        print()\n        stdout += line\n    else:\n        stdout, _ = p.communicate()\n\n    return (p.returncode, stdout)", "response": "Execute a command on the command - line."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates market value from data", "response": "def QA_data_calc_marketvalue(data, xdxr):\n    '\u4f7f\u7528\u6570\u636e\u5e93\u6570\u636e\u8ba1\u7b97\u590d\u6743'\n    mv = xdxr.query('category!=6').loc[:,\n                                       ['shares_after',\n                                        'liquidity_after']].dropna()\n    res = pd.concat([data, mv], axis=1)\n\n    res = res.assign(\n        shares=res.shares_after.fillna(method='ffill'),\n        lshares=res.liquidity_after.fillna(method='ffill')\n    )\n    return res.assign(mv=res.close*res.shares*10000, liquidity_mv=res.close*res.lshares*10000).drop(['shares_after', 'liquidity_after'], axis=1)\\\n            .loc[(slice(data.index.remove_unused_levels().levels[0][0],data.index.remove_unused_levels().levels[0][-1]),slice(None)),:]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef MACD_JCSC(dataframe, SHORT=12, LONG=26, M=9):\n    CLOSE = dataframe.close\n    DIFF = QA.EMA(CLOSE, SHORT) - QA.EMA(CLOSE, LONG)\n    DEA = QA.EMA(DIFF, M)\n    MACD = 2*(DIFF-DEA)\n\n    CROSS_JC = QA.CROSS(DIFF, DEA)\n    CROSS_SC = QA.CROSS(DEA, DIFF)\n    ZERO = 0\n    return pd.DataFrame({'DIFF': DIFF, 'DEA': DEA, 'MACD': MACD, 'CROSS_JC': CROSS_JC, 'CROSS_SC': CROSS_SC, 'ZERO': ZERO})", "response": "calculate MACD and CROSS_JC from dataframe"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create(self, cache_file):\n        conn = sqlite3.connect(cache_file)\n        cur = conn.cursor()\n        cur.execute(\"PRAGMA foreign_keys = ON\")\n        cur.execute('''\n            CREATE TABLE jobs(\n                hash TEXT NOT NULL UNIQUE PRIMARY KEY, description TEXT NOT NULL,\n                last_run REAL, next_run REAL, last_run_result INTEGER)''')\n\n        cur.execute('''\n            CREATE TABLE history(\n                hash TEXT, description TEXT, time REAL, result INTEGER,\n                FOREIGN KEY(hash) REFERENCES jobs(hash))''')\n\n        conn.commit()\n        conn.close()", "response": "Create the tables needed to store the information."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the job with the selected ID.", "response": "def get(self, id):\n        \"\"\"Retrieves the job with the selected ID.\n        :param str id: The ID of the job\n        :returns: The dictionary of the job if found, None otherwise\n        \"\"\"\n        self.cur.execute(\"SELECT * FROM jobs WHERE hash=?\", (id,))\n        item = self.cur.fetchone()\n        if item:\n            return dict(zip(\n                (\"id\", \"description\", \"last-run\", \"next-run\", \"last-run-result\"),\n                item))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the last_run next_run and last_run_result for an existing job.", "response": "def update(self, job):\n        \"\"\"Update last_run, next_run, and last_run_result for an existing job.\n        :param dict job: The job dictionary\n        :returns: True\n        \"\"\"\n        self.cur.execute('''UPDATE jobs\n            SET last_run=?,next_run=?,last_run_result=? WHERE hash=?''', (\n            job[\"last-run\"], job[\"next-run\"], job[\"last-run-result\"], job[\"id\"]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a new job into the cache.", "response": "def add_job(self, job):\n        \"\"\"Adds a new job into the cache.\n        :param dict job: The job dictionary\n        :returns: True\n        \"\"\"\n        self.cur.execute(\"INSERT INTO jobs VALUES(?,?,?,?,?)\", (\n            job[\"id\"], job[\"description\"], job[\"last-run\"], job[\"next-run\"], job[\"last-run-result\"]))\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_result(self, job):\n        self.cur.execute(\n            \"INSERT INTO history VALUES(?,?,?,?)\",\n            (job[\"id\"], job[\"description\"], job[\"last-run\"], job[\"last-run-result\"]))\n\n        return True", "response": "Adds a job run result to the history table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_data_tick_resample_1min(tick, type_='1min', if_drop=True):\n    tick = tick.assign(amount=tick.price * tick.vol)\n    resx = pd.DataFrame()\n    _dates = set(tick.date)\n\n    for date in sorted(list(_dates)):\n        _data = tick.loc[tick.date == date]\n        # morning min bar\n        _data1 = _data[time(9,\n                            25):time(11,\n                                     30)].resample(\n                                         type_,\n                                         closed='left',\n                                         base=30,\n                                         loffset=type_\n                                     ).apply(\n                                         {\n                                             'price': 'ohlc',\n                                             'vol': 'sum',\n                                             'code': 'last',\n                                             'amount': 'sum'\n                                         }\n                                     )\n        _data1.columns = _data1.columns.droplevel(0)\n        # do fix on the first and last bar\n        # \u67d0\u4e9b\u80a1\u7968\u67d0\u4e9b\u65e5\u671f\u6ca1\u6709\u96c6\u5408\u7ade\u4ef7\u4fe1\u606f\uff0c\u8b6c\u5982 002468 \u5728 2017 \u5e74 6 \u6708 5 \u65e5\u7684\u6570\u636e\n        if len(_data.loc[time(9, 25):time(9, 25)]) > 0:\n            _data1.loc[time(9,\n                            31):time(9,\n                                     31),\n                       'open'] = _data1.loc[time(9,\n                                                 26):time(9,\n                                                          26),\n                                            'open'].values\n            _data1.loc[time(9,\n                            31):time(9,\n                                     31),\n                       'high'] = _data1.loc[time(9,\n                                                 26):time(9,\n                                                          31),\n                                            'high'].max()\n            _data1.loc[time(9,\n                            31):time(9,\n                                     31),\n                       'low'] = _data1.loc[time(9,\n                                                26):time(9,\n                                                         31),\n                                           'low'].min()\n            _data1.loc[time(9,\n                            31):time(9,\n                                     31),\n                       'vol'] = _data1.loc[time(9,\n                                                26):time(9,\n                                                         31),\n                                           'vol'].sum()\n            _data1.loc[time(9,\n                            31):time(9,\n                                     31),\n                       'amount'] = _data1.loc[time(9,\n                                                   26):time(9,\n                                                            31),\n                                              'amount'].sum()\n        # \u901a\u8fbe\u4fe1\u5206\u7b14\u6570\u636e\u6709\u7684\u6709 11:30 \u6570\u636e\uff0c\u6709\u7684\u6ca1\u6709\n        if len(_data.loc[time(11, 30):time(11, 30)]) > 0:\n            _data1.loc[time(11,\n                            30):time(11,\n                                     30),\n                       'high'] = _data1.loc[time(11,\n                                                 30):time(11,\n                                                          31),\n                                            'high'].max()\n            _data1.loc[time(11,\n                            30):time(11,\n                                     30),\n                       'low'] = _data1.loc[time(11,\n                                                30):time(11,\n                                                         31),\n                                           'low'].min()\n            _data1.loc[time(11,\n                            30):time(11,\n                                     30),\n                       'close'] = _data1.loc[time(11,\n                                                  31):time(11,\n                                                           31),\n                                             'close'].values\n            _data1.loc[time(11,\n                            30):time(11,\n                                     30),\n                       'vol'] = _data1.loc[time(11,\n                                                30):time(11,\n                                                         31),\n                                           'vol'].sum()\n            _data1.loc[time(11,\n                            30):time(11,\n                                     30),\n                       'amount'] = _data1.loc[time(11,\n                                                   30):time(11,\n                                                            31),\n                                              'amount'].sum()\n        _data1 = _data1.loc[time(9, 31):time(11, 30)]\n\n        # afternoon min bar\n        _data2 = _data[time(13,\n                            0):time(15,\n                                    0)].resample(\n                                        type_,\n                                        closed='left',\n                                        base=30,\n                                        loffset=type_\n                                    ).apply(\n                                        {\n                                            'price': 'ohlc',\n                                            'vol': 'sum',\n                                            'code': 'last',\n                                            'amount': 'sum'\n                                        }\n                                    )\n\n        _data2.columns = _data2.columns.droplevel(0)\n        # \u6caa\u5e02\u80a1\u7968\u5728 2018-08-20 \u8d77\uff0c\u5c3e\u76d8 3 \u5206\u949f\u96c6\u5408\u7ade\u4ef7\n        if (pd.Timestamp(date) <\n                pd.Timestamp('2018-08-20')) and (tick.code.iloc[0][0] == '6'):\n            # \u907f\u514d\u51fa\u73b0 tick \u6570\u636e\u6ca1\u6709 1:00 \u7684\u503c\n            if len(_data.loc[time(13, 0):time(13, 0)]) > 0:\n                _data2.loc[time(15,\n                                0):time(15,\n                                        0),\n                           'high'] = _data2.loc[time(15,\n                                                     0):time(15,\n                                                             1),\n                                                'high'].max()\n                _data2.loc[time(15,\n                                0):time(15,\n                                        0),\n                           'low'] = _data2.loc[time(15,\n                                                    0):time(15,\n                                                            1),\n                                               'low'].min()\n                _data2.loc[time(15,\n                                0):time(15,\n                                        0),\n                           'close'] = _data2.loc[time(15,\n                                                      1):time(15,\n                                                              1),\n                                                 'close'].values\n        else:\n            # \u907f\u514d\u51fa\u73b0 tick \u6570\u636e\u6ca1\u6709 15:00 \u7684\u503c\n            if len(_data.loc[time(13, 0):time(13, 0)]) > 0:\n                _data2.loc[time(15,\n                                0):time(15,\n                                        0)] = _data2.loc[time(15,\n                                                              1):time(15,\n                                                                      1)].values\n        _data2 = _data2.loc[time(13, 1):time(15, 0)]\n        resx = resx.append(_data1).append(_data2)\n    resx['vol'] = resx['vol'] * 100.0\n    resx['volume'] = resx['vol']\n    resx['type'] = '1min'\n    if if_drop:\n        resx = resx.dropna()\n    return resx.reset_index().drop_duplicates().set_index(['datetime', 'code'])", "response": "Resample 1min tick to a single level"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_data_ctptick_resample(tick, type_='1min'):\n\n    resx = pd.DataFrame()\n    _temp = set(tick.TradingDay)\n\n    for item in _temp:\n\n        _data = tick.query('TradingDay==\"{}\"'.format(item))\n        try:\n            _data.loc[time(20, 0):time(21, 0), 'volume'] = 0\n        except:\n            pass\n\n        _data.volume = _data.volume.diff()\n        _data = _data.assign(amount=_data.LastPrice * _data.volume)\n        _data0 = _data[time(0,\n                            0):time(2,\n                                    30)].resample(\n                                        type_,\n                                        closed='right',\n                                        base=30,\n                                        loffset=type_\n                                    ).apply(\n                                        {\n                                            'LastPrice': 'ohlc',\n                                            'volume': 'sum',\n                                            'code': 'last',\n                                            'amount': 'sum'\n                                        }\n                                    )\n\n        _data1 = _data[time(9,\n                            0):time(11,\n                                    30)].resample(\n                                        type_,\n                                        closed='right',\n                                        base=30,\n                                        loffset=type_\n                                    ).apply(\n                                        {\n                                            'LastPrice': 'ohlc',\n                                            'volume': 'sum',\n                                            'code': 'last',\n                                            'amount': 'sum'\n                                        }\n                                    )\n\n        _data2 = _data[time(13,\n                            1):time(15,\n                                    0)].resample(\n                                        type_,\n                                        closed='right',\n                                        base=30,\n                                        loffset=type_\n                                    ).apply(\n                                        {\n                                            'LastPrice': 'ohlc',\n                                            'volume': 'sum',\n                                            'code': 'last',\n                                            'amount': 'sum'\n                                        }\n                                    )\n\n        _data3 = _data[time(21,\n                            0):time(23,\n                                    59)].resample(\n                                        type_,\n                                        closed='left',\n                                        loffset=type_\n                                    ).apply(\n                                        {\n                                            'LastPrice': 'ohlc',\n                                            'volume': 'sum',\n                                            'code': 'last',\n                                            'amount': 'sum'\n                                        }\n                                    )\n\n        resx = resx.append(_data0).append(_data1).append(_data2).append(_data3)\n    resx.columns = resx.columns.droplevel(0)\n    return resx.reset_index().drop_duplicates().set_index(['datetime',\n                                                           'code']).sort_index()", "response": "tick\u91c7\u6837\u6210\u4efb\u610f\u7ea7\u522b\u5206\u949f\u7ebf\n\n    Arguments:\n        tick {[type]} -- transaction\n\n    Returns:\n        [type] -- [description]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_data_min_resample(min_data, type_='5min'):\n\n    try:\n        min_data = min_data.reset_index().set_index('datetime', drop=False)\n    except:\n        min_data = min_data.set_index('datetime', drop=False)\n\n    CONVERSION = {\n        'code': 'first',\n        'open': 'first',\n        'high': 'max',\n        'low': 'min',\n        'close': 'last',\n        'vol': 'sum',\n        'amount': 'sum'\n    } if 'vol' in min_data.columns else {\n        'code': 'first',\n        'open': 'first',\n        'high': 'max',\n        'low': 'min',\n        'close': 'last',\n        'volume': 'sum',\n        'amount': 'sum'\n    }\n    resx = pd.DataFrame()\n\n    for item in set(min_data.index.date):\n        min_data_p = min_data.loc[str(item)]\n        n = min_data_p['{} 21:00:00'.format(item):].resample(\n            type_,\n            base=30,\n            closed='right',\n            loffset=type_\n        ).apply(CONVERSION)\n\n        d = min_data_p[:'{} 11:30:00'.format(item)].resample(\n            type_,\n            base=30,\n            closed='right',\n            loffset=type_\n        ).apply(CONVERSION)\n        f = min_data_p['{} 13:00:00'.format(item):].resample(\n            type_,\n            closed='right',\n            loffset=type_\n        ).apply(CONVERSION)\n\n        resx = resx.append(d).append(f)\n\n    return resx.dropna().reset_index().set_index(['datetime', 'code'])", "response": "Resample the data in a series of min_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef QA_data_day_resample(day_data, type_='w'):\n    # return day_data_p.assign(open=day_data.open.resample(type_).first(),high=day_data.high.resample(type_).max(),low=day_data.low.resample(type_).min(),\\\n    #             vol=day_data.vol.resample(type_).sum() if 'vol' in day_data.columns else day_data.volume.resample(type_).sum(),\\\n    #             amount=day_data.amount.resample(type_).sum()).dropna().set_index('date')\n    try:\n        day_data = day_data.reset_index().set_index('date', drop=False)\n    except:\n        day_data = day_data.set_index('date', drop=False)\n\n    CONVERSION = {\n        'code': 'first',\n        'open': 'first',\n        'high': 'max',\n        'low': 'min',\n        'close': 'last',\n        'vol': 'sum',\n        'amount': 'sum'\n    } if 'vol' in day_data.columns else {\n        'code': 'first',\n        'open': 'first',\n        'high': 'max',\n        'low': 'min',\n        'close': 'last',\n        'volume': 'sum',\n        'amount': 'sum'\n    }\n\n    return day_data.resample(\n        type_,\n        closed='right'\n    ).apply(CONVERSION).dropna().reset_index().set_index(['date',\n                                                          'code'])", "response": "Resample a single day data set to a single object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QA_SU_save_stock_info(engine, client=DATABASE):\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_stock_info(client=client)", "response": "save stock info\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_SU_save_stock_list(engine, client=DATABASE):\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_stock_list(client=client)", "response": "save stock_list\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves index_list Arguments: engine {[type]} -- [description] Keyword Arguments: client {[type]} -- [description] (default: {DATABASE})", "response": "def QA_SU_save_index_list(engine, client=DATABASE):\n    \"\"\"save index_list\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_index_list(client=client)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_SU_save_etf_list(engine, client=DATABASE):\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_etf_list(client=client)", "response": "save etf_list\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_SU_save_future_day(engine, client=DATABASE):\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_future_day(client=client)", "response": "save the current date in the future_day"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving all the n - days of the current node", "response": "def QA_SU_save_future_day_all(engine, client=DATABASE):\n    \"\"\"save future_day_all\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_future_day_all(client=client)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave the min version of the current node", "response": "def QA_SU_save_future_min(engine, client=DATABASE):\n    \"\"\"save future_min\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_future_min(client=client)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef QA_SU_save_stock_day(engine, client=DATABASE, paralleled=False):\n\n    engine = select_save_engine(engine, paralleled=paralleled)\n    engine.QA_SU_save_stock_day(client=client)", "response": "save stock_day\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_SU_save_option_commodity_min(engine, client=DATABASE):\n    '''\n    :param engine:\n    :param client:\n    :return:\n    '''\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_option_commodity_min(client=client)", "response": "Save the option commodity minimum"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_SU_save_option_commodity_day(engine, client=DATABASE):\n    '''\n    :param engine:\n    :param client:\n    :return:\n    '''\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_option_commodity_day(client=client)", "response": "Save the option commodity day"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving stock_min Arguments: engine {[type]} -- [description] Keyword Arguments: client {[type]} -- [description] (default: {DATABASE})", "response": "def QA_SU_save_stock_min(engine, client=DATABASE):\n    \"\"\"save stock_min\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_stock_min(client=client)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QA_SU_save_index_day(engine, client=DATABASE):\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_index_day(client=client)", "response": "save index_day\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave index_min Arguments: engine {[type]} -- [description] Keyword Arguments: client {[type]} -- [description] (default: {DATABASE})", "response": "def QA_SU_save_index_min(engine, client=DATABASE):\n    \"\"\"save index_min\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_index_min(client=client)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave etf_day Arguments: engine {[type]} -- [description] Keyword Arguments: client {[type]} -- [description] (default: {DATABASE})", "response": "def QA_SU_save_etf_day(engine, client=DATABASE):\n    \"\"\"save etf_day\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_etf_day(client=client)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave etf_min Arguments: engine {[type]} -- [description] Keyword Arguments: client {[type]} -- [description] (default: {DATABASE})", "response": "def QA_SU_save_etf_min(engine, client=DATABASE):\n    \"\"\"save etf_min\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_etf_min(client=client)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef QA_SU_save_stock_xdxr(engine, client=DATABASE):\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_stock_xdxr(client=client)", "response": "save stock_xdxr\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving stock_block Arguments: engine {[type]} -- [description] Keyword Arguments: client {[type]} -- [description] (default: {DATABASE})", "response": "def QA_SU_save_stock_block(engine, client=DATABASE):\n    \"\"\"save stock_block\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_stock_block(client=client)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nselect save_engine select save_engine thshare or thshare joinquant paralleledQuant or joinquant paralleledQuant or joinquant paralleledQuant or joinquant paralleledQuant or joinquant paralleledQuant or joinquant paralleledQuant or joinquant paralleledQuant or paralleledQuant or thshare.", "response": "def select_save_engine(engine, paralleled=False):\n    '''\n    select save_engine , tushare ts Tushare \u4f7f\u7528 Tushare \u514d\u8d39\u6570\u636e\u63a5\u53e3\uff0c tdx \u4f7f\u7528\u901a\u8fbe\u4fe1\u6570\u636e\u63a5\u53e3\n    :param engine: \u5b57\u7b26\u4e32Str\n    :param paralleled: \u662f\u5426\u5e76\u884c\u5904\u7406\uff1b\u9ed8\u8ba4\u4e3aFalse\n    :return: sts means save_tushare_py  or stdx means save_tdx_py\n    '''\n    if engine in ['tushare', 'ts', 'Tushare']:\n        return sts\n    elif engine in ['tdx']:\n        if paralleled:\n            return stdx_parallelism\n        else:\n            return stdx\n    elif engine in ['gm', 'goldenminer']:\n        return sgm\n    elif engine in ['jq', 'joinquant']:\n        return sjq\n    else:\n        print('QA Error QASU.main.py call select_save_engine with parameter %s is None of  thshare, ts, Thshare, or tdx', engine)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_fetch_stock_day(code, start, end, format='numpy', frequence='day', collections=DATABASE.stock_day):\n\n    start = str(start)[0:10]\n    end = str(end)[0:10]\n    #code= [code] if isinstance(code,str) else code\n\n    # code checking\n    code = QA_util_code_tolist(code)\n\n    if QA_util_date_valid(end):\n\n        cursor = collections.find({\n            'code': {'$in': code}, \"date_stamp\": {\n                \"$lte\": QA_util_date_stamp(end),\n                \"$gte\": QA_util_date_stamp(start)}}, {\"_id\": 0}, batch_size=10000)\n        #res=[QA_util_dict_remove_key(data, '_id') for data in cursor]\n\n        res = pd.DataFrame([item for item in cursor])\n        try:\n            res = res.assign(volume=res.vol, date=pd.to_datetime(\n                res.date)).drop_duplicates((['date', 'code'])).query('volume>1').set_index('date', drop=False)\n            res = res.ix[:, ['code', 'open', 'high', 'low',\n                             'close', 'volume', 'amount', 'date']]\n        except:\n            res = None\n        if format in ['P', 'p', 'pandas', 'pd']:\n            return res\n        elif format in ['json', 'dict']:\n            return QA_util_to_json_from_pandas(res)\n        # \u591a\u79cd\u6570\u636e\u683c\u5f0f\n        elif format in ['n', 'N', 'numpy']:\n            return numpy.asarray(res)\n        elif format in ['list', 'l', 'L']:\n            return numpy.asarray(res).tolist()\n        else:\n            print(\"QA Error QA_fetch_stock_day format parameter %s is none of  \\\"P, p, pandas, pd , json, dict , n, N, numpy, list, l, L, !\\\" \" % format)\n            return None\n    else:\n        QA_util_log_info(\n            'QA Error QA_fetch_stock_day data parameter start=%s end=%s is not right' % (start, end))", "response": "Get the list of items from the stock day"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch stock_min from database", "response": "def QA_fetch_stock_min(code, start, end, format='numpy', frequence='1min', collections=DATABASE.stock_min):\n    '\u83b7\u53d6\u80a1\u7968\u5206\u949f\u7ebf'\n    if frequence in ['1min', '1m']:\n        frequence = '1min'\n    elif frequence in ['5min', '5m']:\n        frequence = '5min'\n    elif frequence in ['15min', '15m']:\n        frequence = '15min'\n    elif frequence in ['30min', '30m']:\n        frequence = '30min'\n    elif frequence in ['60min', '60m']:\n        frequence = '60min'\n    else:\n        print(\"QA Error QA_fetch_stock_min parameter frequence=%s is none of 1min 1m 5min 5m 15min 15m 30min 30m 60min 60m\" % frequence)\n\n    __data = []\n    # code checking\n    code = QA_util_code_tolist(code)\n\n    cursor = collections.find({\n        'code': {'$in': code}, \"time_stamp\": {\n            \"$gte\": QA_util_time_stamp(start),\n            \"$lte\": QA_util_time_stamp(end)\n        }, 'type': frequence\n    }, {\"_id\": 0}, batch_size=10000)\n\n    res = pd.DataFrame([item for item in cursor])\n    try:\n        res = res.assign(volume=res.vol, datetime=pd.to_datetime(\n            res.datetime)).query('volume>1').drop_duplicates(['datetime', 'code']).set_index('datetime', drop=False)\n        # return res\n    except:\n        res = None\n    if format in ['P', 'p', 'pandas', 'pd']:\n        return res\n    elif format in ['json', 'dict']:\n        return QA_util_to_json_from_pandas(res)\n    # \u591a\u79cd\u6570\u636e\u683c\u5f0f\n    elif format in ['n', 'N', 'numpy']:\n        return numpy.asarray(res)\n    elif format in ['list', 'l', 'L']:\n        return numpy.asarray(res).tolist()\n    else:\n        print(\"QA Error QA_fetch_stock_min format parameter %s is none of  \\\"P, p, pandas, pd , json, dict , n, N, numpy, list, l, L, !\\\" \" % format)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of stocks", "response": "def QA_fetch_stock_list(collections=DATABASE.stock_list):\n    '\u83b7\u53d6\u80a1\u7968\u5217\u8868'\n\n    return pd.DataFrame([item for item in collections.find()]).drop('_id', axis=1, inplace=False).set_index('code', drop=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef QA_fetch_etf_list(collections=DATABASE.etf_list):\n    '\u83b7\u53d6ETF\u5217\u8868'\n\n    return pd.DataFrame([item for item in collections.find()]).drop('_id', axis=1, inplace=False).set_index('code', drop=False)", "response": "get the list of all ELF"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting stock basic info tushare", "response": "def QA_fetch_stock_basic_info_tushare(collections=DATABASE.stock_info_tushare):\n    '''\n    purpose:\n        tushare \u80a1\u7968\u5217\u8868\u6570\u636e\u5e93\n\n        code,\u4ee3\u7801\n        name,\u540d\u79f0\n        industry,\u6240\u5c5e\u884c\u4e1a\n        area,\u5730\u533a\n        pe,\u5e02\u76c8\u7387\n        outstanding,\u6d41\u901a\u80a1\u672c(\u4ebf)\n        totals,\u603b\u80a1\u672c(\u4ebf)\n        totalAssets,\u603b\u8d44\u4ea7(\u4e07)\n        liquidAssets,\u6d41\u52a8\u8d44\u4ea7\n        fixedAssets,\u56fa\u5b9a\u8d44\u4ea7\n        reserved,\u516c\u79ef\u91d1\n        reservedPerShare,\u6bcf\u80a1\u516c\u79ef\u91d1\n        esp,\u6bcf\u80a1\u6536\u76ca\n        bvps,\u6bcf\u80a1\u51c0\u8d44\n        pb,\u5e02\u51c0\u7387\n        timeToMarket,\u4e0a\u5e02\u65e5\u671f\n        undp,\u672a\u5206\u5229\u6da6\n        perundp, \u6bcf\u80a1\u672a\u5206\u914d\n        rev,\u6536\u5165\u540c\u6bd4(%)\n        profit,\u5229\u6da6\u540c\u6bd4(%)\n        gpr,\u6bdb\u5229\u7387(%)\n        npr,\u51c0\u5229\u6da6\u7387(%)\n        holders,\u80a1\u4e1c\u4eba\u6570\n\n        add by tauruswang,\n\n    :param collections: stock_info_tushare \u96c6\u5408\n    :return:\n    '''\n    '\u83b7\u53d6\u80a1\u7968\u57fa\u672c\u4fe1\u606f'\n    items = [item for item in collections.find()]\n    # \ud83d\udee0todo  \u8f6c\u53d8\u6210 dataframe \u7c7b\u578b\u6570\u636e\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_fetch_stock_full(date, format='numpy', collections=DATABASE.stock_day):\n    '\u83b7\u53d6\u5168\u5e02\u573a\u7684\u67d0\u4e00\u65e5\u7684\u6570\u636e'\n    Date = str(date)[0:10]\n    if QA_util_date_valid(Date) is True:\n\n        __data = []\n        for item in collections.find({\n                \"date_stamp\": QA_util_date_stamp(Date)}, batch_size=10000):\n            __data.append([str(item['code']), float(item['open']), float(item['high']), float(\n                item['low']), float(item['close']), float(item['vol']), item['date']])\n        # \u591a\u79cd\u6570\u636e\u683c\u5f0f\n        if format in ['n', 'N', 'numpy']:\n            __data = numpy.asarray(__data)\n        elif format in ['list', 'l', 'L']:\n            __data = __data\n        elif format in ['P', 'p', 'pandas', 'pd']:\n            __data = DataFrame(__data, columns=[\n                'code', 'open', 'high', 'low', 'close', 'volume', 'date'])\n            __data['date'] = pd.to_datetime(__data['date'])\n            __data = __data.set_index('date', drop=False)\n        else:\n            print(\"QA Error QA_fetch_stock_full format parameter %s is none of  \\\"P, p, pandas, pd , json, dict , n, N, numpy, list, l, L, !\\\" \" % format)\n\n        return __data\n    else:\n        QA_util_log_info(\n            'QA Error QA_fetch_stock_full data parameter date=%s not right' % date)", "response": "get all stock items from a date"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_fetch_index_min(\n        code,\n        start, end,\n        format='numpy',\n        frequence='1min',\n        collections=DATABASE.index_min):\n    '\u83b7\u53d6\u80a1\u7968\u5206\u949f\u7ebf'\n    if frequence in ['1min', '1m']:\n        frequence = '1min'\n    elif frequence in ['5min', '5m']:\n        frequence = '5min'\n    elif frequence in ['15min', '15m']:\n        frequence = '15min'\n    elif frequence in ['30min', '30m']:\n        frequence = '30min'\n    elif frequence in ['60min', '60m']:\n        frequence = '60min'\n    __data = []\n    code = QA_util_code_tolist(code)\n    cursor = collections.find({\n        'code': {'$in': code}, \"time_stamp\": {\n            \"$gte\": QA_util_time_stamp(start),\n            \"$lte\": QA_util_time_stamp(end)\n        }, 'type': frequence\n    }, {\"_id\": 0}, batch_size=10000)\n    if format in ['dict', 'json']:\n        return [data for data in cursor]\n    # for item in cursor:\n    __data = pd.DataFrame([item for item in cursor])\n    __data = __data.assign(datetime=pd.to_datetime(__data['datetime']))\n    # __data.append([str(item['code']), float(item['open']), float(item['high']), float(\n    #     item['low']), float(item['close']), int(item['up_count']), int(item['down_count']), float(item['vol']), float(item['amount']), item['datetime'], item['time_stamp'], item['date'], item['type']])\n\n    # __data = DataFrame(__data, columns=[\n    #     'code', 'open', 'high', 'low', 'close', 'up_count', 'down_count', 'volume', 'amount', 'datetime', 'time_stamp', 'date', 'type'])\n\n    # __data['datetime'] = pd.to_datetime(__data['datetime'])\n    __data = __data.set_index('datetime', drop=False)\n    if format in ['numpy', 'np', 'n']:\n        return numpy.asarray(__data)\n    elif format in ['list', 'l', 'L']:\n        return numpy.asarray(__data).tolist()\n    elif format in ['P', 'p', 'pandas', 'pd']:\n        return __data", "response": "get the index_min from the database"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_fetch_future_min(\n        code,\n        start, end,\n        format='numpy',\n        frequence='1min',\n        collections=DATABASE.future_min):\n    '\u83b7\u53d6\u80a1\u7968\u5206\u949f\u7ebf'\n    if frequence in ['1min', '1m']:\n        frequence = '1min'\n    elif frequence in ['5min', '5m']:\n        frequence = '5min'\n    elif frequence in ['15min', '15m']:\n        frequence = '15min'\n    elif frequence in ['30min', '30m']:\n        frequence = '30min'\n    elif frequence in ['60min', '60m']:\n        frequence = '60min'\n    __data = []\n    code = QA_util_code_tolist(code, auto_fill=False)\n    cursor = collections.find({\n        'code': {'$in': code}, \"time_stamp\": {\n            \"$gte\": QA_util_time_stamp(start),\n            \"$lte\": QA_util_time_stamp(end)\n        }, 'type': frequence\n    }, batch_size=10000)\n    if format in ['dict', 'json']:\n        return [data for data in cursor]\n    for item in cursor:\n\n        __data.append([str(item['code']), float(item['open']), float(item['high']), float(\n            item['low']), float(item['close']), float(item['position']), float(item['price']), float(item['trade']),\n            item['datetime'], item['tradetime'], item['time_stamp'], item['date'], item['type']])\n\n    __data = DataFrame(__data, columns=[\n        'code', 'open', 'high', 'low', 'close',  'position', 'price', 'trade', 'datetime', 'tradetime', 'time_stamp', 'date', 'type'])\n\n    __data['datetime'] = pd.to_datetime(__data['datetime'])\n    __data = __data.set_index('datetime', drop=False)\n    if format in ['numpy', 'np', 'n']:\n        return numpy.asarray(__data)\n    elif format in ['list', 'l', 'L']:\n        return numpy.asarray(__data).tolist()\n    elif format in ['P', 'p', 'pandas', 'pd']:\n        return __data", "response": "Get the list of future min records from the database"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QA_fetch_future_list(collections=DATABASE.future_list):\n    '\u83b7\u53d6\u671f\u8d27\u5217\u8868'\n    return pd.DataFrame([item for item in collections.find()]).drop('_id', axis=1, inplace=False).set_index('code', drop=False)", "response": "get all future list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_fetch_ctp_tick(code, start, end, frequence, format='pd', collections=DATABASE.ctp_tick):\n\n    code = QA_util_code_tolist(code, auto_fill=False)\n    cursor = collections.find({\n        'InstrumentID': {'$in': code}, \"time_stamp\": {\n            \"$gte\": QA_util_time_stamp(start),\n            \"$lte\": QA_util_time_stamp(end)\n        }, 'type': frequence\n    }, {\"_id\": 0}, batch_size=10000)\n\n    hq = pd.DataFrame([data for data in cursor]).replace(1.7976931348623157e+308,\n                                                         numpy.nan).replace('', numpy.nan).dropna(axis=1)\n    p1 = hq.loc[:, ['ActionDay', 'AskPrice1', 'AskVolume1', 'AveragePrice', 'BidPrice1',\n                    'BidVolume1', 'HighestPrice', 'InstrumentID', 'LastPrice',\n                    'OpenInterest', 'TradingDay', 'UpdateMillisec',\n                    'UpdateTime', 'Volume']]\n    p1 = p1.assign(datetime=p1.ActionDay.apply(QA_util_date_int2str)+' '+p1.UpdateTime + (p1.UpdateMillisec/1000000).apply(lambda x: str('%.6f' % x)[1:]),\n                   code=p1.InstrumentID)\n    p1.datetime = pd.to_datetime(p1.datetime)\n    return p1.set_index(p1.datetime)", "response": "Get a cts_tick record from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_fetch_stock_xdxr(code, format='pd', collections=DATABASE.stock_xdxr):\n    '\u83b7\u53d6\u80a1\u7968\u9664\u6743\u4fe1\u606f/\u6570\u636e\u5e93'\n    code = QA_util_code_tolist(code)\n    data = pd.DataFrame([item for item in collections.find(\n        {'code':  {'$in': code}}, batch_size=10000)]).drop(['_id'], axis=1)\n    data['date'] = pd.to_datetime(data['date'])\n    return data.set_index('date', drop=False)", "response": "get stock_xdxr by code"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_fetch_account(message={}, db=DATABASE):\n    collection = DATABASE.account\n    return [res for res in collection.find(message, {\"_id\": 0})]", "response": "get the account from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_fetch_risk(message={}, params={\"_id\": 0, 'assets': 0, 'timeindex': 0, 'totaltimeindex': 0, 'benchmark_assets': 0, 'month_profit': 0}, db=DATABASE):\n    collection = DATABASE.risk\n    return [res for res in collection.find(message, params)]", "response": "get the risk from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_fetch_user(user_cookie, db=DATABASE):\n    collection = DATABASE.account\n\n    return [res for res in collection.find({'user_cookie': user_cookie}, {\"_id\": 0})]", "response": "get the user by cookie"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_fetch_strategy(message={}, db=DATABASE):\n    collection = DATABASE.strategy\n    return [res for res in collection.find(message, {\"_id\": 0})]", "response": "get the account\n\n    Arguments:\n        query_mes {[type]} -- [description]\n\n    Keyword Arguments:\n        collection {[type]} -- [description] (default: {DATABASE})\n\n    Returns:\n        [type] -- [description]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_fetch_financial_report(code, report_date, ltype='EN', db=DATABASE):\n\n    if isinstance(code, str):\n        code = [code]\n    if isinstance(report_date, str):\n        report_date = [QA_util_date_str2int(report_date)]\n    elif isinstance(report_date, int):\n        report_date = [report_date]\n    elif isinstance(report_date, list):\n        report_date = [QA_util_date_str2int(item) for item in report_date]\n\n    collection = db.financial\n    num_columns = [item[:3] for item in list(financial_dict.keys())]\n    CH_columns = [item[3:] for item in list(financial_dict.keys())]\n    EN_columns = list(financial_dict.values())\n    #num_columns.extend(['283', '_id', 'code', 'report_date'])\n   # CH_columns.extend(['283', '_id', 'code', 'report_date'])\n    #CH_columns = pd.Index(CH_columns)\n    #EN_columns = list(financial_dict.values())\n    #EN_columns.extend(['283', '_id', 'code', 'report_date'])\n    #EN_columns = pd.Index(EN_columns)\n\n    try:\n        if code is not None and report_date is not None:\n            data = [item for item in collection.find(\n                {'code': {'$in': code}, 'report_date': {'$in': report_date}}, {\"_id\": 0}, batch_size=10000)]\n        elif code is None and report_date is not None:\n            data = [item for item in collection.find(\n                {'report_date': {'$in': report_date}}, {\"_id\": 0}, batch_size=10000)]\n        elif code is not None and report_date is None:\n            data = [item for item in collection.find(\n                {'code': {'$in': code}}, {\"_id\": 0}, batch_size=10000)]\n        else:\n            data = [item for item in collection.find({}, {\"_id\": 0})]\n        if len(data) > 0:\n            res_pd = pd.DataFrame(data)\n\n            if ltype in ['CH', 'CN']:\n\n                cndict = dict(zip(num_columns, CH_columns))\n\n                cndict['283'] = '283'\n                try:\n                    cndict['284'] = '284'\n                    cndict['285'] = '285'\n                    cndict['286'] = '286'\n                except:\n                    pass\n\n                cndict['code'] = 'code'\n                cndict['report_date'] = 'report_date'\n                res_pd.columns = res_pd.columns.map(lambda x: cndict[x])\n            elif ltype is 'EN':\n                endict = dict(zip(num_columns, EN_columns))\n                endict['283'] = '283'\n                try:\n                    endict['284'] = '284'\n                    endict['285'] = '285'\n                    endict['286'] = '286'\n                except:\n                    pass\n\n                endict['code'] = 'code'\n                endict['report_date'] = 'report_date'\n                res_pd.columns = res_pd.columns.map(lambda x: endict[x])\n\n            if res_pd.report_date.dtype == numpy.int64:\n                res_pd.report_date = pd.to_datetime(\n                    res_pd.report_date.apply(QA_util_date_int2str))\n            else:\n                res_pd.report_date = pd.to_datetime(res_pd.report_date)\n\n            return res_pd.replace(-4.039810335e+34, numpy.nan).set_index(['report_date', 'code'], drop=False)\n        else:\n            return None\n    except Exception as e:\n        raise e", "response": "Get the financial report for a specific code and date"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_fetch_stock_divyield(code, start, end=None, format='pd', collections=DATABASE.stock_divyield):\n    '\u83b7\u53d6\u80a1\u7968\u65e5\u7ebf'\n    #code= [code] if isinstance(code,str) else code\n    # code checking\n    code = QA_util_code_tolist(code)\n\n    if QA_util_date_valid(end):\n\n        __data = []\n        cursor = collections.find({\n            'a_stockcode': {'$in': code}, \"dir_dcl_date\": {\n                \"$lte\": end,\n                \"$gte\": start}}, {\"_id\": 0}, batch_size=10000)\n        #res=[QA_util_dict_remove_key(data, '_id') for data in cursor]\n\n        res = pd.DataFrame([item for item in cursor])\n        try:\n            res = res.drop_duplicates(\n                (['dir_dcl_date', 'a_stockcode']))\n            res = res.ix[:, ['a_stockcode', 'a_stocksname', 'div_info', 'div_type_code', 'bonus_shr',\n                             'cash_bt', 'cap_shr', 'epsp', 'ps_cr', 'ps_up', 'reg_date', 'dir_dcl_date',\n                             'a_stockcode1', 'ex_divi_date', 'prg']]\n        except:\n            res = None\n        if format in ['P', 'p', 'pandas', 'pd']:\n            return res\n        elif format in ['json', 'dict']:\n            return QA_util_to_json_from_pandas(res)\n        # \u591a\u79cd\u6570\u636e\u683c\u5f0f\n        elif format in ['n', 'N', 'numpy']:\n            return numpy.asarray(res)\n        elif format in ['list', 'l', 'L']:\n            return numpy.asarray(res).tolist()\n        else:\n            print(\"QA Error QA_fetch_stock_divyield format parameter %s is none of  \\\"P, p, pandas, pd , json, dict , n, N, numpy, list, l, L, !\\\" \" % format)\n            return None\n    else:\n        QA_util_log_info(\n            'QA Error QA_fetch_stock_divyield data parameter start=%s end=%s is not right' % (start, end))", "response": "fetch all stock divyields from a stock code"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves stock_day \u4fdd\u5b58\u65e5\u7ebf\u6570\u636e :param client: :param ui_log: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528 :param ui_progress: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528 :param ui_progress_int_value: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528", "response": "def QA_SU_save_stock_day(client=DATABASE, ui_log=None, ui_progress=None):\n    '''\n     save stock_day\n    \u4fdd\u5b58\u65e5\u7ebf\u6570\u636e\n    :param client:\n    :param ui_log:  \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress_int_value: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    '''\n    stock_list = QA_fetch_get_stock_list().code.unique().tolist()\n    coll_stock_day = client.stock_day\n    coll_stock_day.create_index(\n        [(\"code\",\n          pymongo.ASCENDING),\n         (\"date_stamp\",\n          pymongo.ASCENDING)]\n    )\n    err = []\n\n    def __saving_work(code, coll_stock_day):\n        try:\n            QA_util_log_info(\n                '##JOB01 Now Saving STOCK_DAY==== {}'.format(str(code)),\n                ui_log\n            )\n\n            # \u9996\u9009\u67e5\u627e\u6570\u636e\u5e93 \u662f\u5426 \u6709 \u8fd9\u4e2a\u4ee3\u7801\u7684\u6570\u636e\n            ref = coll_stock_day.find({'code': str(code)[0:6]})\n            end_date = str(now_time())[0:10]\n\n            # \u5f53\u524d\u6570\u636e\u5e93\u5df2\u7ecf\u5305\u542b\u4e86\u8fd9\u4e2a\u4ee3\u7801\u7684\u6570\u636e\uff0c \u7ee7\u7eed\u589e\u91cf\u66f4\u65b0\n            # \u52a0\u5165\u8fd9\u4e2a\u5224\u65ad\u7684\u539f\u56e0\u662f\u56e0\u4e3a\u5982\u679c\u80a1\u7968\u662f\u521a\u4e0a\u5e02\u7684 \u6570\u636e\u5e93\u4f1a\u6ca1\u6709\u6570\u636e \u6240\u4ee5\u4f1a\u6709\u8d1f\u7d22\u5f15\u95ee\u9898\u51fa\u73b0\n            if ref.count() > 0:\n\n                # \u63a5\u7740\u4e0a\u6b21\u83b7\u53d6\u7684\u65e5\u671f\u7ee7\u7eed\u66f4\u65b0\n                start_date = ref[ref.count() - 1]['date']\n\n                QA_util_log_info(\n                    'UPDATE_STOCK_DAY \\n Trying updating {} from {} to {}'\n                    .format(code,\n                            start_date,\n                            end_date),\n                    ui_log\n                )\n                if start_date != end_date:\n                    coll_stock_day.insert_many(\n                        QA_util_to_json_from_pandas(\n                            QA_fetch_get_stock_day(\n                                str(code),\n                                QA_util_get_next_day(start_date),\n                                end_date,\n                                '00'\n                            )\n                        )\n                    )\n\n            # \u5f53\u524d\u6570\u636e\u5e93\u4e2d\u6ca1\u6709\u8fd9\u4e2a\u4ee3\u7801\u7684\u80a1\u7968\u6570\u636e\uff0c \u4ece1990-01-01 \u5f00\u59cb\u4e0b\u8f7d\u6240\u6709\u7684\u6570\u636e\n            else:\n                start_date = '1990-01-01'\n                QA_util_log_info(\n                    'UPDATE_STOCK_DAY \\n Trying updating {} from {} to {}'\n                    .format(code,\n                            start_date,\n                            end_date),\n                    ui_log\n                )\n                if start_date != end_date:\n                    coll_stock_day.insert_many(\n                        QA_util_to_json_from_pandas(\n                            QA_fetch_get_stock_day(\n                                str(code),\n                                start_date,\n                                end_date,\n                                '00'\n                            )\n                        )\n                    )\n        except Exception as error0:\n            print(error0)\n            err.append(str(code))\n\n    for item in range(len(stock_list)):\n        QA_util_log_info('The {} of Total {}'.format(item, len(stock_list)))\n\n        strProgressToLog = 'DOWNLOAD PROGRESS {} {}'.format(\n            str(float(item / len(stock_list) * 100))[0:4] + '%',\n            ui_log\n        )\n        intProgressToLog = int(float(item / len(stock_list) * 100))\n        QA_util_log_info(\n            strProgressToLog,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intProgressToLog\n        )\n\n        __saving_work(stock_list[item], coll_stock_day)\n\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS save stock day ^_^', ui_log)\n    else:\n        QA_util_log_info('ERROR CODE \\n ', ui_log)\n        QA_util_log_info(err, ui_log)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves all stock_weeks in database", "response": "def QA_SU_save_stock_week(client=DATABASE, ui_log=None, ui_progress=None):\n    \"\"\"save stock_week\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    stock_list = QA_fetch_get_stock_list().code.unique().tolist()\n    coll_stock_week = client.stock_week\n    coll_stock_week.create_index(\n        [(\"code\",\n          pymongo.ASCENDING),\n         (\"date_stamp\",\n          pymongo.ASCENDING)]\n    )\n    err = []\n\n    def __saving_work(code, coll_stock_week):\n        try:\n            QA_util_log_info(\n                '##JOB01 Now Saving STOCK_WEEK==== {}'.format(str(code)),\n                ui_log=ui_log\n            )\n\n            ref = coll_stock_week.find({'code': str(code)[0:6]})\n            end_date = str(now_time())[0:10]\n            if ref.count() > 0:\n                # \u52a0\u5165\u8fd9\u4e2a\u5224\u65ad\u7684\u539f\u56e0\u662f\u56e0\u4e3a\u5982\u679c\u80a1\u7968\u662f\u521a\u4e0a\u5e02\u7684 \u6570\u636e\u5e93\u4f1a\u6ca1\u6709\u6570\u636e \u6240\u4ee5\u4f1a\u6709\u8d1f\u7d22\u5f15\u95ee\u9898\u51fa\u73b0\n\n                start_date = ref[ref.count() - 1]['date']\n\n                QA_util_log_info(\n                    'UPDATE_STOCK_WEEK \\n Trying updating {} from {} to {}'\n                        .format(code,\n                                start_date,\n                                end_date),\n                    ui_log=ui_log\n                )\n                if start_date != end_date:\n                    coll_stock_week.insert_many(\n                        QA_util_to_json_from_pandas(\n                            QA_fetch_get_stock_day(\n                                str(code),\n                                QA_util_get_next_day(start_date),\n                                end_date,\n                                '00',\n                                frequence='week'\n                            )\n                        )\n                    )\n            else:\n                start_date = '1990-01-01'\n                QA_util_log_info(\n                    'UPDATE_STOCK_WEEK \\n Trying updating {} from {} to {}'\n                        .format(code,\n                                start_date,\n                                end_date),\n                    ui_log=ui_log\n                )\n                if start_date != end_date:\n                    coll_stock_week.insert_many(\n                        QA_util_to_json_from_pandas(\n                            QA_fetch_get_stock_day(\n                                str(code),\n                                start_date,\n                                end_date,\n                                '00',\n                                frequence='week'\n                            )\n                        )\n                    )\n        except:\n            err.append(str(code))\n\n    for item in range(len(stock_list)):\n        QA_util_log_info(\n            'The {} of Total {}'.format(item,\n                                        len(stock_list)),\n            ui_log=ui_log\n        )\n        strProgress = 'DOWNLOAD PROGRESS {} '.format(\n            str(float(item / len(stock_list) * 100))[0:4] + '%'\n        )\n        intProgress = int(float(item / len(stock_list) * 100))\n        QA_util_log_info(\n            strProgress,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intProgress\n        )\n\n        __saving_work(stock_list[item], coll_stock_week)\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS', ui_log=ui_log)\n    else:\n        QA_util_log_info(' ERROR CODE \\n ', ui_log=ui_log)\n        QA_util_log_info(err, ui_log=ui_log)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef QA_SU_save_stock_min(client=DATABASE, ui_log=None, ui_progress=None):\n\n    stock_list = QA_fetch_get_stock_list().code.unique().tolist()\n    coll = client.stock_min\n    coll.create_index(\n        [\n            ('code',\n             pymongo.ASCENDING),\n            ('time_stamp',\n             pymongo.ASCENDING),\n            ('date_stamp',\n             pymongo.ASCENDING)\n        ]\n    )\n    err = []\n\n    def __saving_work(code, coll):\n        QA_util_log_info(\n            '##JOB03 Now Saving STOCK_MIN ==== {}'.format(str(code)),\n            ui_log=ui_log\n        )\n        try:\n            for type in ['1min', '5min', '15min', '30min', '60min']:\n                ref_ = coll.find({'code': str(code)[0:6], 'type': type})\n                end_time = str(now_time())[0:19]\n                if ref_.count() > 0:\n                    start_time = ref_[ref_.count() - 1]['datetime']\n\n                    QA_util_log_info(\n                        '##JOB03.{} Now Saving {} from {} to {} =={} '.format(\n                            ['1min',\n                             '5min',\n                             '15min',\n                             '30min',\n                             '60min'].index(type),\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        ),\n                        ui_log=ui_log\n                    )\n                    if start_time != end_time:\n                        __data = QA_fetch_get_stock_min(\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        )\n                        if len(__data) > 1:\n                            coll.insert_many(\n                                QA_util_to_json_from_pandas(__data)[1::]\n                            )\n                else:\n                    start_time = '2015-01-01'\n                    QA_util_log_info(\n                        '##JOB03.{} Now Saving {} from {} to {} =={} '.format(\n                            ['1min',\n                             '5min',\n                             '15min',\n                             '30min',\n                             '60min'].index(type),\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        ),\n                        ui_log=ui_log\n                    )\n                    if start_time != end_time:\n                        __data = QA_fetch_get_stock_min(\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        )\n                        if len(__data) > 1:\n                            coll.insert_many(\n                                QA_util_to_json_from_pandas(__data)\n                            )\n        except Exception as e:\n            QA_util_log_info(e, ui_log=ui_log)\n            err.append(code)\n            QA_util_log_info(err, ui_log=ui_log)\n\n    executor = ThreadPoolExecutor(max_workers=4)\n    # executor.map((__saving_work,  stock_list[i_], coll),URLS)\n    res = {\n        executor.submit(__saving_work,\n                        stock_list[i_],\n                        coll)\n        for i_ in range(len(stock_list))\n    }\n    count = 0\n    for i_ in concurrent.futures.as_completed(res):\n        QA_util_log_info(\n            'The {} of Total {}'.format(count,\n                                        len(stock_list)),\n            ui_log=ui_log\n        )\n\n        strProgress = 'DOWNLOAD PROGRESS {} '.format(\n            str(float(count / len(stock_list) * 100))[0:4] + '%'\n        )\n        intProgress = int(count / len(stock_list) * 10000.0)\n        QA_util_log_info(\n            strProgress,\n            ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intProgress\n        )\n        count = count + 1\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS', ui_log=ui_log)\n    else:\n        QA_util_log_info(' ERROR CODE \\n ', ui_log=ui_log)\n        QA_util_log_info(err, ui_log=ui_log)", "response": "save stock_min\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the index_day of the current user", "response": "def QA_SU_save_index_day(client=DATABASE, ui_log=None, ui_progress=None):\n    \"\"\"save index_day\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    __index_list = QA_fetch_get_stock_list('index')\n    coll = client.index_day\n    coll.create_index(\n        [('code',\n          pymongo.ASCENDING),\n         ('date_stamp',\n          pymongo.ASCENDING)]\n    )\n    err = []\n\n    def __saving_work(code, coll):\n\n        try:\n            ref_ = coll.find({'code': str(code)[0:6]})\n            end_time = str(now_time())[0:10]\n            if ref_.count() > 0:\n                start_time = ref_[ref_.count() - 1]['date']\n\n                QA_util_log_info(\n                    '##JOB04 Now Saving INDEX_DAY==== \\n Trying updating {} from {} to {}'\n                        .format(code,\n                                start_time,\n                                end_time),\n                    ui_log=ui_log\n                )\n\n                if start_time != end_time:\n                    coll.insert_many(\n                        QA_util_to_json_from_pandas(\n                            QA_fetch_get_index_day(\n                                str(code),\n                                QA_util_get_next_day(start_time),\n                                end_time\n                            )\n                        )\n                    )\n            else:\n                try:\n                    start_time = '1990-01-01'\n                    QA_util_log_info(\n                        '##JOB04 Now Saving INDEX_DAY==== \\n Trying updating {} from {} to {}'\n                            .format(code,\n                                    start_time,\n                                    end_time),\n                        ui_log=ui_log\n                    )\n                    coll.insert_many(\n                        QA_util_to_json_from_pandas(\n                            QA_fetch_get_index_day(\n                                str(code),\n                                start_time,\n                                end_time\n                            )\n                        )\n                    )\n                except:\n                    start_time = '2009-01-01'\n                    QA_util_log_info(\n                        '##JOB04 Now Saving INDEX_DAY==== \\n Trying updating {} from {} to {}'\n                            .format(code,\n                                    start_time,\n                                    end_time),\n                        ui_log=ui_log\n                    )\n                    coll.insert_many(\n                        QA_util_to_json_from_pandas(\n                            QA_fetch_get_index_day(\n                                str(code),\n                                start_time,\n                                end_time\n                            )\n                        )\n                    )\n        except Exception as e:\n            QA_util_log_info(e, ui_log=ui_log)\n            err.append(str(code))\n            QA_util_log_info(err, ui_log=ui_log)\n\n    for i_ in range(len(__index_list)):\n        # __saving_work('000001')\n        QA_util_log_info(\n            'The {} of Total {}'.format(i_,\n                                        len(__index_list)),\n            ui_log=ui_log\n        )\n\n        strLogProgress = 'DOWNLOAD PROGRESS {} '.format(\n            str(float(i_ / len(__index_list) * 100))[0:4] + '%'\n        )\n        intLogProgress = int(float(i_ / len(__index_list) * 10000.0))\n        QA_util_log_info(\n            strLogProgress,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intLogProgress\n        )\n        __saving_work(__index_list.index[i_][0], coll)\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS', ui_log=ui_log)\n    else:\n        QA_util_log_info(' ERROR CODE \\n ', ui_log=ui_log)\n        QA_util_log_info(err, ui_log=ui_log)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the index_min of the current user", "response": "def QA_SU_save_index_min(client=DATABASE, ui_log=None, ui_progress=None):\n    \"\"\"save index_min\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    __index_list = QA_fetch_get_stock_list('index')\n    coll = client.index_min\n    coll.create_index(\n        [\n            ('code',\n             pymongo.ASCENDING),\n            ('time_stamp',\n             pymongo.ASCENDING),\n            ('date_stamp',\n             pymongo.ASCENDING)\n        ]\n    )\n    err = []\n\n    def __saving_work(code, coll):\n\n        QA_util_log_info(\n            '##JOB05 Now Saving Index_MIN ==== {}'.format(str(code)),\n            ui_log=ui_log\n        )\n        try:\n\n            for type in ['1min', '5min', '15min', '30min', '60min']:\n                ref_ = coll.find({'code': str(code)[0:6], 'type': type})\n                end_time = str(now_time())[0:19]\n                if ref_.count() > 0:\n                    start_time = ref_[ref_.count() - 1]['datetime']\n\n                    QA_util_log_info(\n                        '##JOB05.{} Now Saving {} from {} to {} =={} '.format(\n                            ['1min',\n                             '5min',\n                             '15min',\n                             '30min',\n                             '60min'].index(type),\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        ),\n                        ui_log=ui_log\n                    )\n\n                    if start_time != end_time:\n                        __data = QA_fetch_get_index_min(\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        )\n                        if len(__data) > 1:\n                            coll.insert_many(\n                                QA_util_to_json_from_pandas(__data[1::])\n                            )\n                else:\n                    start_time = '2015-01-01'\n\n                    QA_util_log_info(\n                        '##JOB05.{} Now Saving {} from {} to {} =={} '.format(\n                            ['1min',\n                             '5min',\n                             '15min',\n                             '30min',\n                             '60min'].index(type),\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        ),\n                        ui_log=ui_log\n                    )\n\n                    if start_time != end_time:\n                        __data = QA_fetch_get_index_min(\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        )\n                        if len(__data) > 1:\n                            coll.insert_many(\n                                QA_util_to_json_from_pandas(__data)\n                            )\n        except:\n            err.append(code)\n\n    executor = ThreadPoolExecutor(max_workers=4)\n\n    res = {\n        executor.submit(__saving_work,\n                        __index_list.index[i_][0],\n                        coll)\n        for i_ in range(len(__index_list))\n    }  # multi index ./.\n    count = 0\n    for i_ in concurrent.futures.as_completed(res):\n        strLogProgress = 'DOWNLOAD PROGRESS {} '.format(\n            str(float(count / len(__index_list) * 100))[0:4] + '%'\n        )\n        intLogProgress = int(float(count / len(__index_list) * 10000.0))\n        QA_util_log_info(\n            'The {} of Total {}'.format(count,\n                                        len(__index_list)),\n            ui_log=ui_log\n        )\n        QA_util_log_info(\n            strLogProgress,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intLogProgress\n        )\n        count = count + 1\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS', ui_log=ui_log)\n    else:\n        QA_util_log_info(' ERROR CODE \\n ', ui_log=ui_log)\n        QA_util_log_info(err, ui_log=ui_log)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave a stock list", "response": "def QA_SU_save_stock_list(client=DATABASE, ui_log=None, ui_progress=None):\n    \"\"\"save stock_list\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n    client.drop_collection('stock_list')\n    coll = client.stock_list\n    coll.create_index('code')\n\n    try:\n        # \ud83d\udee0todo \u8fd9\u4e2a\u5e94\u8be5\u662f\u7b2c\u4e00\u4e2a\u4efb\u52a1 JOB01\uff0c \u5148\u66f4\u65b0\u80a1\u7968\u5217\u8868\uff01\uff01\n        QA_util_log_info(\n            '##JOB08 Now Saving STOCK_LIST ====',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=5000\n        )\n        stock_list_from_tdx = QA_fetch_get_stock_list()\n        pandas_data = QA_util_to_json_from_pandas(stock_list_from_tdx)\n        coll.insert_many(pandas_data)\n        QA_util_log_info(\n            \"\u5b8c\u6210\u80a1\u7968\u5217\u8868\u83b7\u53d6\",\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=10000\n        )\n    except Exception as e:\n        QA_util_log_info(e, ui_log=ui_log)\n        print(\" Error save_tdx.QA_SU_save_stock_list exception!\")\n\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_SU_save_etf_list(client=DATABASE, ui_log=None, ui_progress=None):\n    try:\n        QA_util_log_info(\n            '##JOB16 Now Saving ETF_LIST ====',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=5000\n        )\n        etf_list_from_tdx = QA_fetch_get_stock_list(type_=\"etf\")\n        pandas_data = QA_util_to_json_from_pandas(etf_list_from_tdx)\n\n        if len(pandas_data) > 0:\n            # \u83b7\u53d6\u5230\u6570\u636e\u540e\u624d\u8fdb\u884cdrop collection \u64cd\u4f5c\n            client.drop_collection('etf_list')\n            coll = client.etf_list\n            coll.create_index('code')\n            coll.insert_many(pandas_data)\n        QA_util_log_info(\n            \"\u5b8c\u6210ETF\u5217\u8868\u83b7\u53d6\",\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=10000\n        )\n    except Exception as e:\n        QA_util_log_info(e, ui_log=ui_log)\n        print(\" Error save_tdx.QA_SU_save_etf_list exception!\")\n        pass", "response": "Save the list of EnergyPi stocks"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef QA_SU_save_stock_block(client=DATABASE, ui_log=None, ui_progress=None):\n\n    client.drop_collection('stock_block')\n    coll = client.stock_block\n    coll.create_index('code')\n\n    try:\n        QA_util_log_info(\n            '##JOB09 Now Saving STOCK_BlOCK ====',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=5000\n        )\n        coll.insert_many(\n            QA_util_to_json_from_pandas(QA_fetch_get_stock_block('tdx'))\n        )\n        QA_util_log_info(\n            'tdx Block ====',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=5000\n        )\n\n        # \ud83d\udee0todo fixhere here \u83b7\u53d6\u540c\u82b1\u987a\u677f\u5757\uff0c \u8fd8\u662f\u8c03\u7528tdx\u7684\n        coll.insert_many(\n            QA_util_to_json_from_pandas(QA_fetch_get_stock_block('ths'))\n        )\n        QA_util_log_info(\n            'ths Block ====',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=8000\n        )\n\n        QA_util_log_info(\n            '\u5b8c\u6210\u80a1\u7968\u677f\u5757\u83b7\u53d6=',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=10000\n        )\n\n    except Exception as e:\n        QA_util_log_info(e, ui_log=ui_log)\n        print(\" Error save_tdx.QA_SU_save_stock_block exception!\")\n        pass", "response": "save stock_block\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave all stock info", "response": "def QA_SU_save_stock_info(client=DATABASE, ui_log=None, ui_progress=None):\n    \"\"\"save stock_info\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    client.drop_collection('stock_info')\n    stock_list = QA_fetch_get_stock_list().code.unique().tolist()\n    coll = client.stock_info\n    coll.create_index('code')\n    err = []\n\n    def __saving_work(code, coll):\n        QA_util_log_info(\n            '##JOB10 Now Saving STOCK INFO ==== {}'.format(str(code)),\n            ui_log=ui_log\n        )\n        try:\n            coll.insert_many(\n                QA_util_to_json_from_pandas(QA_fetch_get_stock_info(str(code)))\n            )\n\n        except:\n            err.append(str(code))\n\n    for i_ in range(len(stock_list)):\n        # __saving_work('000001')\n\n        strLogProgress = 'DOWNLOAD PROGRESS {} '.format(\n            str(float(i_ / len(stock_list) * 100))[0:4] + '%'\n        )\n        intLogProgress = int(float(i_ / len(stock_list) * 10000.0))\n        QA_util_log_info('The {} of Total {}'.format(i_, len(stock_list)))\n        QA_util_log_info(\n            strLogProgress,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intLogProgress\n        )\n\n        __saving_work(stock_list[i_], coll)\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS', ui_log=ui_log)\n    else:\n        QA_util_log_info(' ERROR CODE \\n ', ui_log=ui_log)\n        QA_util_log_info(err, ui_log=ui_log)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves stock_transaction Keyword Arguments: client {[type]} -- [description] (default: {DATABASE})", "response": "def QA_SU_save_stock_transaction(\n        client=DATABASE,\n        ui_log=None,\n        ui_progress=None\n):\n    \"\"\"save stock_transaction\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})\n    \"\"\"\n\n    stock_list = QA_fetch_get_stock_list().code.unique().tolist()\n    coll = client.stock_transaction\n    coll.create_index('code')\n    err = []\n\n    def __saving_work(code):\n        QA_util_log_info(\n            '##JOB11 Now Saving STOCK_TRANSACTION ==== {}'.format(str(code)),\n            ui_log=ui_log\n        )\n        try:\n            coll.insert_many(\n                QA_util_to_json_from_pandas(\n                    # \ud83d\udee0todo  str(stock_list[code]) \u53c2\u6570\u4e0d\u5bf9\uff1f\n                    QA_fetch_get_stock_transaction(\n                        str(code),\n                        '1990-01-01',\n                        str(now_time())[0:10]\n                    )\n                )\n            )\n        except:\n            err.append(str(code))\n\n    for i_ in range(len(stock_list)):\n        # __saving_work('000001')\n        QA_util_log_info(\n            'The {} of Total {}'.format(i_,\n                                        len(stock_list)),\n            ui_log=ui_log\n        )\n\n        strLogProgress = 'DOWNLOAD PROGRESS {} '.format(\n            str(float(i_ / len(stock_list) * 100))[0:4] + '%'\n        )\n        intLogProgress = int(float(i_ / len(stock_list) * 10000.0))\n\n        QA_util_log_info(\n            strLogProgress,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intLogProgress\n        )\n        __saving_work(stock_list[i_])\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS', ui_log=ui_log)\n    else:\n        QA_util_log_info(' ERROR CODE \\n ', ui_log=ui_log)\n        QA_util_log_info(err, ui_log=ui_log)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_SU_save_option_commodity_day(\n        client=DATABASE,\n        ui_log=None,\n        ui_progress=None\n):\n    '''\n        :param client:\n        :return:\n    '''\n    _save_option_commodity_cu_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_m_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_sr_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n\n    _save_option_commodity_ru_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_cf_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_c_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )", "response": "Save all the option commodity days for the current level of the site"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the minimum option commodity of a single item", "response": "def QA_SU_save_option_commodity_min(\n        client=DATABASE,\n        ui_log=None,\n        ui_progress=None\n):\n    '''\n        :param client:\n        :return:\n    '''\n    # \u6d4b\u8bd5\u4e2d\u53d1\u73b0\uff0c \u4e00\u8d77\u56de\u53bb\uff0c\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\uff0c\u6bcf\u6b21\u83b7\u53d6\u4e00\u4e2a\u54c1\u79cd\u540e \uff0c\u66f4\u6362\u670d\u52a1ip\u7ee7\u7eed\u83b7\u53d6 \uff1f\n\n    _save_option_commodity_cu_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_sr_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_m_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n\n    _save_option_commodity_ru_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n\n    _save_option_commodity_cf_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n\n    _save_option_commodity_c_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_SU_save_option_min(client=DATABASE, ui_log=None, ui_progress=None):\n    '''\n    :param client:\n    :return:\n    '''\n    option_contract_list = QA_fetch_get_option_contract_time_to_market()\n    coll_option_min = client.option_day_min\n    coll_option_min.create_index(\n        [(\"code\",\n          pymongo.ASCENDING),\n         (\"date_stamp\",\n          pymongo.ASCENDING)]\n    )\n    err = []\n\n    # \u7d22\u5f15 code\n\n    err = []\n\n    def __saving_work(code, coll):\n\n        QA_util_log_info(\n            '##JOB13 Now Saving Option 50ETF MIN ==== {}'.format(str(code)),\n            ui_log=ui_log\n        )\n        try:\n\n            for type in ['1min', '5min', '15min', '30min', '60min']:\n                ref_ = coll.find({'code': str(code)[0:8], 'type': type})\n\n                end_time = str(now_time())[0:19]\n                if ref_.count() > 0:\n                    start_time = ref_[ref_.count() - 1]['datetime']\n\n                    QA_util_log_info(\n                        '##JOB13.{} Now Saving Option 50ETF {} from {} to {} =={} '\n                            .format(\n                            ['1min',\n                             '5min',\n                             '15min',\n                             '30min',\n                             '60min'].index(type),\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        ),\n                        ui_log=ui_log\n                    )\n\n                    if start_time != end_time:\n                        __data = QA_fetch_get_future_min(\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        )\n                        if len(__data) > 1:\n                            QA_util_log_info(\n                                \" \u5199\u5165 \u65b0\u589e\u5386\u53f2\u5408\u7ea6\u8bb0\u5f55\u6570 {} \".format(len(__data))\n                            )\n                            coll.insert_many(\n                                QA_util_to_json_from_pandas(__data[1::])\n                            )\n                else:\n                    start_time = '2015-01-01'\n\n                    QA_util_log_info(\n                        '##JOB13.{} Now Option 50ETF {} from {} to {} =={} '\n                            .format(\n                            ['1min',\n                             '5min',\n                             '15min',\n                             '30min',\n                             '60min'].index(type),\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        ),\n                        ui_log=ui_log\n                    )\n\n                    if start_time != end_time:\n                        __data = QA_fetch_get_future_min(\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        )\n                        if len(__data) > 1:\n                            QA_util_log_info(\n                                \" \u5199\u5165 \u65b0\u589e\u5408\u7ea6\u8bb0\u5f55\u6570 {} \".format(len(__data))\n                            )\n                            coll.insert_many(\n                                QA_util_to_json_from_pandas(__data)\n                            )\n        except:\n            err.append(code)\n\n    executor = ThreadPoolExecutor(max_workers=4)\n\n    res = {\n        executor.submit(\n            __saving_work,\n            option_contract_list[i_][\"code\"],\n            coll_option_min\n        )\n        for i_ in range(len(option_contract_list))\n    }  # multi index ./.\n    count = 0\n    for i_ in concurrent.futures.as_completed(res):\n        QA_util_log_info(\n            'The {} of Total {}'.format(count,\n                                        len(option_contract_list)),\n            ui_log=ui_log\n        )\n        strLogProgress = 'DOWNLOAD PROGRESS {} '.format(\n            str(float(count / len(option_contract_list) * 100))[0:4] + '%'\n        )\n        intLogProgress = int(float(count / len(option_contract_list) * 10000.0))\n\n        QA_util_log_info(\n            strLogProgress,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intLogProgress\n        )\n        count = count + 1\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS', ui_log=ui_log)\n    else:\n        QA_util_log_info(' ERROR CODE \\n ', ui_log=ui_log)\n        QA_util_log_info(err, ui_log=ui_log)", "response": "Save option_min to database"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_SU_save_future_day(client=DATABASE, ui_log=None, ui_progress=None):\n    '''\n     save future_day\n    \u4fdd\u5b58\u65e5\u7ebf\u6570\u636e\n    :param client:\n    :param ui_log:  \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress_int_value: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :return:\n    '''\n    future_list = [\n        item for item in QA_fetch_get_future_list().code.unique().tolist()\n        if str(item)[-2:] in ['L8',\n                              'L9']\n    ]\n    coll_future_day = client.future_day\n    coll_future_day.create_index(\n        [(\"code\",\n          pymongo.ASCENDING),\n         (\"date_stamp\",\n          pymongo.ASCENDING)]\n    )\n    err = []\n\n    def __saving_work(code, coll_future_day):\n        try:\n            QA_util_log_info(\n                '##JOB12 Now Saving Future_DAY==== {}'.format(str(code)),\n                ui_log\n            )\n\n            # \u9996\u9009\u67e5\u627e\u6570\u636e\u5e93 \u662f\u5426 \u6709 \u8fd9\u4e2a\u4ee3\u7801\u7684\u6570\u636e\n            ref = coll_future_day.find({'code': str(code)[0:4]})\n            end_date = str(now_time())[0:10]\n\n            # \u5f53\u524d\u6570\u636e\u5e93\u5df2\u7ecf\u5305\u542b\u4e86\u8fd9\u4e2a\u4ee3\u7801\u7684\u6570\u636e\uff0c \u7ee7\u7eed\u589e\u91cf\u66f4\u65b0\n            # \u52a0\u5165\u8fd9\u4e2a\u5224\u65ad\u7684\u539f\u56e0\u662f\u56e0\u4e3a\u5982\u679c\u80a1\u7968\u662f\u521a\u4e0a\u5e02\u7684 \u6570\u636e\u5e93\u4f1a\u6ca1\u6709\u6570\u636e \u6240\u4ee5\u4f1a\u6709\u8d1f\u7d22\u5f15\u95ee\u9898\u51fa\u73b0\n            if ref.count() > 0:\n\n                # \u63a5\u7740\u4e0a\u6b21\u83b7\u53d6\u7684\u65e5\u671f\u7ee7\u7eed\u66f4\u65b0\n                start_date = ref[ref.count() - 1]['date']\n\n                QA_util_log_info(\n                    'UPDATE_Future_DAY \\n Trying updating {} from {} to {}'\n                        .format(code,\n                                start_date,\n                                end_date),\n                    ui_log\n                )\n                if start_date != end_date:\n                    coll_future_day.insert_many(\n                        QA_util_to_json_from_pandas(\n                            QA_fetch_get_future_day(\n                                str(code),\n                                QA_util_get_next_day(start_date),\n                                end_date\n                            )\n                        )\n                    )\n\n            # \u5f53\u524d\u6570\u636e\u5e93\u4e2d\u6ca1\u6709\u8fd9\u4e2a\u4ee3\u7801\u7684\u80a1\u7968\u6570\u636e\uff0c \u4ece1990-01-01 \u5f00\u59cb\u4e0b\u8f7d\u6240\u6709\u7684\u6570\u636e\n            else:\n                start_date = '2001-01-01'\n                QA_util_log_info(\n                    'UPDATE_Future_DAY \\n Trying updating {} from {} to {}'\n                        .format(code,\n                                start_date,\n                                end_date),\n                    ui_log\n                )\n                if start_date != end_date:\n                    coll_future_day.insert_many(\n                        QA_util_to_json_from_pandas(\n                            QA_fetch_get_future_day(\n                                str(code),\n                                start_date,\n                                end_date\n                            )\n                        )\n                    )\n        except Exception as error0:\n            print(error0)\n            err.append(str(code))\n\n    for item in range(len(future_list)):\n        QA_util_log_info('The {} of Total {}'.format(item, len(future_list)))\n\n        strProgressToLog = 'DOWNLOAD PROGRESS {} {}'.format(\n            str(float(item / len(future_list) * 100))[0:4] + '%',\n            ui_log\n        )\n        intProgressToLog = int(float(item / len(future_list) * 100))\n        QA_util_log_info(\n            strProgressToLog,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intProgressToLog\n        )\n\n        __saving_work(future_list[item], coll_future_day)\n\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS save future day ^_^', ui_log)\n    else:\n        QA_util_log_info(' ERROR CODE \\n ', ui_log)\n        QA_util_log_info(err, ui_log)", "response": "save future_day\n    \u4fdd\u5b58\u65e5\u7ebf\u6570\u636e\n    :param client:\n    :param ui_log:  \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :param ui_progress_int_value: \u7ed9GUI qt \u754c\u9762\u4f7f\u7528\n    :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_SU_save_future_min(client=DATABASE, ui_log=None, ui_progress=None):\n\n    future_list = [\n        item for item in QA_fetch_get_future_list().code.unique().tolist()\n        if str(item)[-2:] in ['L8',\n                              'L9']\n    ]\n    coll = client.future_min\n    coll.create_index(\n        [\n            ('code',\n             pymongo.ASCENDING),\n            ('time_stamp',\n             pymongo.ASCENDING),\n            ('date_stamp',\n             pymongo.ASCENDING)\n        ]\n    )\n    err = []\n\n    def __saving_work(code, coll):\n\n        QA_util_log_info(\n            '##JOB13 Now Saving Future_MIN ==== {}'.format(str(code)),\n            ui_log=ui_log\n        )\n        try:\n\n            for type in ['1min', '5min', '15min', '30min', '60min']:\n                ref_ = coll.find({'code': str(code)[0:6], 'type': type})\n                end_time = str(now_time())[0:19]\n                if ref_.count() > 0:\n                    start_time = ref_[ref_.count() - 1]['datetime']\n\n                    QA_util_log_info(\n                        '##JOB13.{} Now Saving Future {} from {} to {} =={} '\n                            .format(\n                            ['1min',\n                             '5min',\n                             '15min',\n                             '30min',\n                             '60min'].index(type),\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        ),\n                        ui_log=ui_log\n                    )\n\n                    if start_time != end_time:\n                        __data = QA_fetch_get_future_min(\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        )\n                        if len(__data) > 1:\n                            coll.insert_many(\n                                QA_util_to_json_from_pandas(__data[1::])\n                            )\n                else:\n                    start_time = '2015-01-01'\n\n                    QA_util_log_info(\n                        '##JOB13.{} Now Saving Future {} from {} to {} =={} '\n                            .format(\n                            ['1min',\n                             '5min',\n                             '15min',\n                             '30min',\n                             '60min'].index(type),\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        ),\n                        ui_log=ui_log\n                    )\n\n                    if start_time != end_time:\n                        __data = QA_fetch_get_future_min(\n                            str(code),\n                            start_time,\n                            end_time,\n                            type\n                        )\n                        if len(__data) > 1:\n                            coll.insert_many(\n                                QA_util_to_json_from_pandas(__data)\n                            )\n        except:\n            err.append(code)\n\n    executor = ThreadPoolExecutor(max_workers=4)\n\n    res = {\n        executor.submit(__saving_work,\n                        future_list[i_],\n                        coll)\n        for i_ in range(len(future_list))\n    }  # multi index ./.\n    count = 0\n    for i_ in concurrent.futures.as_completed(res):\n        QA_util_log_info(\n            'The {} of Total {}'.format(count,\n                                        len(future_list)),\n            ui_log=ui_log\n        )\n        strLogProgress = 'DOWNLOAD PROGRESS {} '.format(\n            str(float(count / len(future_list) * 100))[0:4] + '%'\n        )\n        intLogProgress = int(float(count / len(future_list) * 10000.0))\n\n        QA_util_log_info(\n            strLogProgress,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intLogProgress\n        )\n        count = count + 1\n    if len(err) < 1:\n        QA_util_log_info('SUCCESS', ui_log=ui_log)\n    else:\n        QA_util_log_info(' ERROR CODE \\n ', ui_log=ui_log)\n        QA_util_log_info(err, ui_log=ui_log)", "response": "Save the list of all the future_MIN records"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun a shell commad", "response": "def do_shell(self, arg):\n        \"run a shell commad\"\n        print(\">\", arg)\n        sub_cmd = subprocess.Popen(arg, shell=True, stdout=subprocess.PIPE)\n        print(sub_cmd.communicate()[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_fetch_stock_min_adv(\n        code,\n        start, end=None,\n        frequence='1min',\n        if_drop_index=True,\n        # \ud83d\udee0 todo collections \u53c2\u6570\u6ca1\u6709\u7528\u5230\uff0c \u4e14\u6570\u636e\u5e93\u662f\u56fa\u5b9a\u7684\uff0c \u8fd9\u4e2a\u53d8\u91cf\u540e\u671f\u53bb\u6389\n        collections=DATABASE.stock_min):\n    '''\n    '\u83b7\u53d6\u80a1\u7968\u5206\u949f\u7ebf'\n    :param code:  \u5b57\u7b26\u4e32str eg 600085\n    :param start: \u5b57\u7b26\u4e32str \u5f00\u59cb\u65e5\u671f eg 2011-01-01\n    :param end:   \u5b57\u7b26\u4e32str \u7ed3\u675f\u65e5\u671f eg 2011-05-01\n    :param frequence: \u5b57\u7b26\u4e32str \u5206\u949f\u7ebf\u7684\u7c7b\u578b \u652f\u6301 1min 1m 5min 5m 15min 15m 30min 30m 60min 60m \u7c7b\u578b\n    :param if_drop_index: Ture False \uff0c dataframe drop index or not\n    :param collections: mongodb \u6570\u636e\u5e93\n    :return: QA_DataStruct_Stock_min \u7c7b\u578b\n    '''\n    if frequence in ['1min', '1m']:\n        frequence = '1min'\n    elif frequence in ['5min', '5m']:\n        frequence = '5min'\n    elif frequence in ['15min', '15m']:\n        frequence = '15min'\n    elif frequence in ['30min', '30m']:\n        frequence = '30min'\n    elif frequence in ['60min', '60m']:\n        frequence = '60min'\n    else:\n        print(\"QA Error QA_fetch_stock_min_adv parameter frequence=%s is none of 1min 1m 5min 5m 15min 15m 30min 30m 60min 60m\" % frequence)\n        return None\n\n    # __data = [] \u672a\u4f7f\u7528\n\n    end = start if end is None else end\n    if len(start) == 10:\n        start = '{} 09:30:00'.format(start)\n\n    if len(end) == 10:\n        end = '{} 15:00:00'.format(end)\n\n    if start == end:\n        # \ud83d\udee0 todo \u5982\u679c\u76f8\u7b49\uff0c\u6839\u636e frequence \u83b7\u53d6\u5f00\u59cb\u65f6\u95f4\u7684 \u65f6\u95f4\u6bb5 QA_fetch_stock_min\uff0c \u4e0d\u652f\u6301start end\u662f\u76f8\u7b49\u7684\n        print(\"QA Error QA_fetch_stock_min_adv parameter code=%s , start=%s, end=%s is equal, should have time span! \" % (\n            code, start, end))\n        return None\n\n    # \ud83d\udee0 todo \u62a5\u544a\u9519\u8bef \u5982\u679c\u5f00\u59cb\u65f6\u95f4 \u5728 \u7ed3\u675f\u65f6\u95f4\u4e4b\u540e\n\n    res = QA_fetch_stock_min(\n        code, start, end, format='pd', frequence=frequence)\n    if res is None:\n        print(\"QA Error QA_fetch_stock_min_adv parameter code=%s , start=%s, end=%s frequence=%s call QA_fetch_stock_min return None\" % (\n            code, start, end, frequence))\n        return None\n    else:\n        res_set_index = res.set_index(['datetime', 'code'], drop=if_drop_index)\n        # if res_set_index is None:\n        #     print(\"QA Error QA_fetch_stock_min_adv set index 'datetime, code' return None\")\n        #     return None\n        return QA_DataStruct_Stock_min(res_set_index)", "response": "get the stock_min advisory data from a given time range"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef QA_fetch_stock_day_full_adv(date):\n    '''\n    '\u8fd4\u56de\u5168\u5e02\u573a\u67d0\u4e00\u5929\u7684\u6570\u636e'\n    :param date:\n    :return: QA_DataStruct_Stock_day\u7c7b \u578b\u6570\u636e\n    '''\n    # \ud83d\udee0 todo \u68c0\u67e5\u65e5\u671fdata\u53c2\u6570\n    res = QA_fetch_stock_full(date, 'pd')\n    if res is None:\n        print(\"QA Error QA_fetch_stock_day_full_adv parameter date=%s call QA_fetch_stock_full return None\" % (date))\n        return None\n    else:\n        res_set_index = res.set_index(['date', 'code'])\n        # if res_set_index is None:\n        #     print(\"QA Error QA_fetch_stock_day_full set index 'date, code' return None\")\n        return QA_DataStruct_Stock_day(res_set_index)", "response": "QA_fetch_stock_day_full returns a new object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a single entry from the index day", "response": "def QA_fetch_index_day_adv(\n        code,\n        start, end=None,\n        if_drop_index=True,\n        # \ud83d\udee0 todo collections \u53c2\u6570\u6ca1\u6709\u7528\u5230\uff0c \u4e14\u6570\u636e\u5e93\u662f\u56fa\u5b9a\u7684\uff0c \u8fd9\u4e2a\u53d8\u91cf\u540e\u671f\u53bb\u6389\n        collections=DATABASE.index_day):\n    '''\n    :param code: code:  \u5b57\u7b26\u4e32str eg 600085\n    :param start:  \u5b57\u7b26\u4e32str \u5f00\u59cb\u65e5\u671f eg 2011-01-01\n    :param end:  \u5b57\u7b26\u4e32str \u7ed3\u675f\u65e5\u671f eg 2011-05-01\n    :param if_drop_index: Ture False \uff0c dataframe drop index or not\n    :param collections:  mongodb \u6570\u636e\u5e93\n    :return:\n    '''\n    '\u83b7\u53d6\u6307\u6570\u65e5\u7ebf'\n    end = start if end is None else end\n    start = str(start)[0:10]\n    end = str(end)[0:10]\n\n    # \ud83d\udee0 todo \u62a5\u544a\u9519\u8bef \u5982\u679c\u5f00\u59cb\u65f6\u95f4 \u5728 \u7ed3\u675f\u65f6\u95f4\u4e4b\u540e\n    # \ud83d\udee0 todo \u5982\u679c\u76f8\u7b49\n\n    res = QA_fetch_index_day(code, start, end, format='pd')\n    if res is None:\n        print(\"QA Error QA_fetch_index_day_adv parameter code=%s start=%s end=%s call QA_fetch_index_day return None\" % (\n            code, start, end))\n        return None\n    else:\n        res_set_index = res.set_index(['date', 'code'], drop=if_drop_index)\n        # if res_set_index is None:\n        #     print(\"QA Error QA_fetch_index_day_adv set index 'date, code' return None\")\n        #     return None\n        return QA_DataStruct_Index_day(res_set_index)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_fetch_index_min_adv(\n        code,\n        start, end=None,\n        frequence='1min',\n        if_drop_index=True,\n        collections=DATABASE.index_min):\n    '''\n    '\u83b7\u53d6\u80a1\u7968\u5206\u949f\u7ebf'\n    :param code:\n    :param start:\n    :param end:\n    :param frequence:\n    :param if_drop_index:\n    :param collections:\n    :return:\n    '''\n    if frequence in ['1min', '1m']:\n        frequence = '1min'\n    elif frequence in ['5min', '5m']:\n        frequence = '5min'\n    elif frequence in ['15min', '15m']:\n        frequence = '15min'\n    elif frequence in ['30min', '30m']:\n        frequence = '30min'\n    elif frequence in ['60min', '60m']:\n        frequence = '60min'\n\n    # __data = [] \u6ca1\u6709\u4f7f\u7528\n\n    end = start if end is None else end\n    if len(start) == 10:\n        start = '{} 09:30:00'.format(start)\n    if len(end) == 10:\n        end = '{} 15:00:00'.format(end)\n\n    # \ud83d\udee0 todo \u62a5\u544a\u9519\u8bef \u5982\u679c\u5f00\u59cb\u65f6\u95f4 \u5728 \u7ed3\u675f\u65f6\u95f4\u4e4b\u540e\n\n    # if start == end:\n    # \ud83d\udee0 todo \u5982\u679c\u76f8\u7b49\uff0c\u6839\u636e frequence \u83b7\u53d6\u5f00\u59cb\u65f6\u95f4\u7684 \u65f6\u95f4\u6bb5 QA_fetch_index_min_adv\uff0c \u4e0d\u652f\u6301start end\u662f\u76f8\u7b49\u7684\n    #print(\"QA Error QA_fetch_index_min_adv parameter code=%s , start=%s, end=%s is equal, should have time span! \" % (code, start, end))\n    # return None\n\n    res = QA_fetch_index_min(\n        code, start, end, format='pd', frequence=frequence)\n    if res is None:\n        print(\"QA Error QA_fetch_index_min_adv parameter code=%s start=%s end=%s frequence=%s call QA_fetch_index_min return None\" % (\n            code, start, end, frequence))\n    else:\n        res_reset_index = res.set_index(\n            ['datetime', 'code'], drop=if_drop_index)\n        # if res_reset_index is None:\n        #     print(\"QA Error QA_fetch_index_min_adv set index 'date, code' return None\")\n        return QA_DataStruct_Index_min(res_reset_index)", "response": "get the minimum index"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all stock_list items from mongodb", "response": "def QA_fetch_stock_list_adv(collections=DATABASE.stock_list):\n    '''\n    '\u83b7\u53d6\u80a1\u7968\u5217\u8868'\n    :param collections: mongodb \u6570\u636e\u5e93\n    :return: DataFrame\n    '''\n    stock_list_items = QA_fetch_stock_list(collections)\n    if len(stock_list_items) == 0:\n        print(\"QA Error QA_fetch_stock_list_adv call item for item in collections.find() return 0 item, maybe the DATABASE.stock_list is empty!\")\n        return None\n    return stock_list_items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_fetch_index_list_adv(collections=DATABASE.index_list):\n    '''\n    '\u83b7\u53d6\u80a1\u7968\u5217\u8868'\n    :param collections: mongodb \u6570\u636e\u5e93\n    :return: DataFrame\n    '''\n    index_list_items = QA_fetch_index_list(collections)\n    if len(index_list_items) == 0:\n        print(\"QA Error QA_fetch_index_list_adv call item for item in collections.find() return 0 item, maybe the DATABASE.index_list is empty!\")\n        return None\n    return index_list_items", "response": "get all index_list items from mongodb"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QA_fetch_future_min_adv(\n        code,\n        start, end=None,\n        frequence='1min',\n        if_drop_index=True,\n        collections=DATABASE.future_min):\n    '''\n    '\u83b7\u53d6\u80a1\u7968\u5206\u949f\u7ebf'\n    :param code:\n    :param start:\n    :param end:\n    :param frequence:\n    :param if_drop_index:\n    :param collections:\n    :return:\n    '''\n    if frequence in ['1min', '1m']:\n        frequence = '1min'\n    elif frequence in ['5min', '5m']:\n        frequence = '5min'\n    elif frequence in ['15min', '15m']:\n        frequence = '15min'\n    elif frequence in ['30min', '30m']:\n        frequence = '30min'\n    elif frequence in ['60min', '60m']:\n        frequence = '60min'\n\n    # __data = [] \u6ca1\u6709\u4f7f\u7528\n\n    end = start if end is None else end\n    if len(start) == 10:\n        start = '{} 00:00:00'.format(start)\n    if len(end) == 10:\n        end = '{} 15:00:00'.format(end)\n\n    # \ud83d\udee0 todo \u62a5\u544a\u9519\u8bef \u5982\u679c\u5f00\u59cb\u65f6\u95f4 \u5728 \u7ed3\u675f\u65f6\u95f4\u4e4b\u540e\n\n    # if start == end:\n    # \ud83d\udee0 todo \u5982\u679c\u76f8\u7b49\uff0c\u6839\u636e frequence \u83b7\u53d6\u5f00\u59cb\u65f6\u95f4\u7684 \u65f6\u95f4\u6bb5 QA_fetch_index_min_adv\uff0c \u4e0d\u652f\u6301start end\u662f\u76f8\u7b49\u7684\n    #print(\"QA Error QA_fetch_index_min_adv parameter code=%s , start=%s, end=%s is equal, should have time span! \" % (code, start, end))\n    # return None\n\n    res = QA_fetch_future_min(\n        code, start, end, format='pd', frequence=frequence)\n    if res is None:\n        print(\"QA Error QA_fetch_future_min_adv parameter code=%s start=%s end=%s frequence=%s call QA_fetch_future_min return None\" % (\n            code, start, end, frequence))\n    else:\n        res_reset_index = res.set_index(\n            ['datetime', 'code'], drop=if_drop_index)\n        # if res_reset_index is None:\n        #     print(\"QA Error QA_fetch_index_min_adv set index 'date, code' return None\")\n        return QA_DataStruct_Future_min(res_reset_index)", "response": "get the next min frequence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all items from the database", "response": "def QA_fetch_future_list_adv(collections=DATABASE.future_list):\n    '''\n    '\u83b7\u53d6\u80a1\u7968\u5217\u8868'\n    :param collections: mongodb \u6570\u636e\u5e93\n    :return: DataFrame\n    '''\n    future_list_items = QA_fetch_future_list()\n    if len(future_list_items) == 0:\n        print(\"QA Error QA_fetch_future_list_adv call item for item in collections.find() return 0 item, maybe the DATABASE.future_list is empty!\")\n        return None\n    return future_list_items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef QA_fetch_stock_block_adv(code=None, blockname=None, collections=DATABASE.stock_block):\n    '''\n    \u8fd4\u56de\u677f\u5757 \u274c\n    :param code:\n    :param blockname:\n    :param collections: \u9ed8\u8ba4\u6570\u636e\u5e93 stock_block\n    :return: QA_DataStruct_Stock_block\n    '''\n    if code is not None and blockname is None:\n        # \u8fd4\u56de\u8fd9\u4e2a\u80a1\u7968\u4ee3\u7801\u6240\u5c5e\u7684\u677f\u5757\n        data = pd.DataFrame([item for item in collections.find(\n            {'code': {'$in': code}})])\n        data = data.drop(['_id'], axis=1)\n\n        return QA_DataStruct_Stock_block(data.set_index(['blockname', 'code'], drop=True).drop_duplicates())\n    elif blockname is not None and code is None:\n        #\n        # \ud83d\udee0 todo fnished \u8fd4\u56de \u8fd9\u4e2a\u677f\u5757\u6240\u6709\u7684\u80a1\u7968\n        # \u8fd4\u56de\u8be5\u677f\u5757\u6240\u5c5e\u7684\u80a1\u7968\n        # print(\"QA Error blockname is Not none code none, return all code from its block name have not implemented yet !\")\n\n        items_from_collections = [item for item in collections.find(\n            {'blockname': re.compile(blockname)})]\n        data = pd.DataFrame(items_from_collections).drop(['_id'], axis=1)\n        data_set_index = data.set_index(['blockname', 'code'], drop=True)\n        return QA_DataStruct_Stock_block(data_set_index)\n\n    else:\n        # \ud83d\udee0 todo \u8fd4\u56de \u5224\u65ad \u8fd9\u4e2a\u80a1\u7968\u662f\u5426\u548c\u5c5e\u4e8e\u8be5\u677f\u5757\n        data = pd.DataFrame(\n            [item for item in collections.find()]).drop(['_id'], axis=1)\n        data_set_index = data.set_index(['blockname', 'code'], drop=True)\n        return QA_DataStruct_Stock_block(data_set_index)", "response": "get a single item from stock_block"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_fetch_stock_realtime_adv(code=None,\n                                num=1,\n                                collections=DATABASE.get_collection('realtime_{}'.format(datetime.date.today()))):\n    '''\n    \u8fd4\u56de\u5f53\u65e5\u7684\u4e0a\u4e0b\u4e94\u6863, code\u53ef\u4ee5\u662f\u80a1\u7968\u53ef\u4ee5\u662flist, num\u662f\u6bcf\u4e2a\u80a1\u7968\u83b7\u53d6\u7684\u6570\u91cf\n    :param code:\n    :param num:\n    :param collections:  realtime_XXXX-XX-XX \u6bcf\u5929\u5b9e\u65f6\u65f6\u95f4\n    :return: DataFrame\n    '''\n    if code is not None:\n        # code \u5fc5\u987b\u8f6c\u6362\u6210list \u53bb\u67e5\u8be2\u6570\u636e\u5e93\n        if isinstance(code, str):\n            code = [code]\n        elif isinstance(code, list):\n            pass\n        else:\n            print(\n                \"QA Error QA_fetch_stock_realtime_adv parameter code is not List type or String type\")\n\n        items_from_collections = [item for item in collections.find(\n            {'code': {'$in': code}}, limit=num*len(code), sort=[('datetime', pymongo.DESCENDING)])]\n        if items_from_collections is None:\n            print(\"QA Error QA_fetch_stock_realtime_adv find parameter code={} num={} collection={} return NOne\".format(\n                code, num, collections))\n            return\n\n        data = pd.DataFrame(items_from_collections)\n        data_set_index = data.set_index(\n            ['datetime', 'code'], drop=False).drop(['_id'], axis=1)\n        return data_set_index\n    else:\n        print(\"QA Error QA_fetch_stock_realtime_adv parameter code is None\")", "response": "get all items from realtime_XXXX - xx - xx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef QA_fetch_financial_report_adv(code, start, end=None, ltype='EN'):\n\n    if end is None:\n\n        return QA_DataStruct_Financial(QA_fetch_financial_report(code, start, ltype=ltype))\n    else:\n        series = pd.Series(\n            data=month_data, index=pd.to_datetime(month_data), name='date')\n        timerange = series.loc[start:end].tolist()\n        return QA_DataStruct_Financial(QA_fetch_financial_report(code, timerange, ltype=ltype))", "response": "\u9ad8\u7ea7\u8d22\u52a1\u67e5\u8be2\u63a5\u53e3\n    Arguments:\n        code {[type]} -- [description]\n        start {[type]} -- [description]\n    Keyword Arguments:\n        end {[type]} -- [description] (default: {None})"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QA_fetch_stock_financial_calendar_adv(code, start=\"all\", end=None, format='pd', collections=DATABASE.report_calendar):\n    '\u83b7\u53d6\u80a1\u7968\u65e5\u7ebf'\n    #code= [code] if isinstance(code,str) else code\n    end = start if end is None else end\n    start = str(start)[0:10]\n    end = str(end)[0:10]\n\n    # code checking\n    if start == 'all':\n        start = '1990-01-01'\n        end = str(datetime.date.today())\n\n    if end is None:\n\n        return QA_DataStruct_Financial(QA_fetch_stock_financial_calendar(code, start, str(datetime.date.today())))\n    else:\n        series = pd.Series(\n            data=month_data, index=pd.to_datetime(month_data), name='date')\n        timerange = series.loc[start:end].tolist()\n        return QA_DataStruct_Financial(QA_fetch_stock_financial_calendar(code, start, end))", "response": "Get financial calendar adversarial"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the commission coefficient for a given code", "response": "def get_commission_coeff(self, code):\n        \"\"\"\n        \u5f53\u524d\u65e0\u6cd5\u533a\u5206\u662f\u767e\u5206\u6bd4\u8fd8\u662f\u6309\u624b\u6570\u6536\u8d39,\u4e0d\u8fc7\u53ef\u4ee5\u62ff\u5230\u4ee5\u540e\u81ea\u884c\u5224\u65ad\n        \"\"\"\n        return max(self.get_code(code).get('commission_coeff_peramount'),\n                   self.get_code(code).get('commission_coeff_pervol'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets status to failed", "response": "def failed(self, reason=None):\n        \"\"\"\u5931\u8d25\u8ba2\u5355(\u672a\u6210\u529f\u521b\u5efa\u5165broker)\n\n        Arguments:\n            reason {str} -- \u5931\u8d25\u539f\u56e0\n        \"\"\"\n        # \u8ba2\u5355\u521b\u5efa\u5931\u8d25(\u5982\u5e9f\u5355/\u573a\u5916\u5e9f\u5355/\u4ef7\u683c\u9ad8\u4e8e\u6da8\u505c\u4ef7/\u4ef7\u683c\u4f4e\u4e8e\u8dcc\u505c\u4ef7/\u901a\u8baf\u5931\u8d25)\n        self._status = ORDER_STATUS.FAILED\n        self.reason = str(reason)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef trade(self, trade_id, trade_price, trade_amount, trade_time):\n        if self.status in [ORDER_STATUS.SUCCESS_PART, ORDER_STATUS.QUEUED]:\n            trade_amount = int(trade_amount)\n            trade_id = str(trade_id)\n\n            if trade_amount < 1:\n\n                self._status = ORDER_STATUS.NEXT\n            else:\n                if trade_id not in self.trade_id:\n                    trade_price = float(trade_price)\n\n                    trade_time = str(trade_time)\n\n                    self.trade_id.append(trade_id)\n                    self.trade_price = (\n                        self.trade_price * self.trade_amount +\n                        trade_price * trade_amount\n                    ) / (\n                        self.trade_amount + trade_amount\n                    )\n                    self.trade_amount += trade_amount\n                    self.trade_time.append(trade_time)\n                    self.callback(\n                        self.code,\n                        trade_id,\n                        self.order_id,\n                        self.realorder_id,\n                        trade_price,\n                        trade_amount,\n                        self.towards,\n                        trade_time\n                    )\n                else:\n                    pass\n        else:\n            raise RuntimeError(\n                'ORDER STATUS {} CANNNOT TRADE'.format(self.status)\n            )", "response": "This method is used to update the trade amount of the order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict of all the data that can be used to create an OTP.", "response": "def to_otgdict(self):\n        \"\"\"{\n                \"aid\": \"insert_order\",                  # //\u5fc5\u586b, \u4e0b\u5355\u8bf7\u6c42\n                # //\u5fc5\u586b, \u9700\u8981\u4e0e\u767b\u5f55\u7528\u6237\u540d\u4e00\u81f4, \u6216\u4e3a\u767b\u5f55\u7528\u6237\u7684\u5b50\u8d26\u6237(\u4f8b\u5982\u767b\u5f55\u7528\u6237\u4e3auser1, \u5219\u62a5\u5355 user_id \u5e94\u5f53\u4e3a user1 \u6216 user1.some_unit)\n                \"user_id\": account_cookie,\n                # //\u5fc5\u586b, \u59d4\u6258\u5355\u53f7, \u9700\u786e\u4fdd\u5728\u4e00\u4e2a\u8d26\u53f7\u4e2d\u4e0d\u91cd\u590d, \u9650\u957f512\u5b57\u8282\n                \"order_id\": order_id if order_id else QA.QA_util_random_with_topic('QAOTG'),\n                \"exchange_id\": exchange_id,  # //\u5fc5\u586b, \u4e0b\u5355\u5230\u54ea\u4e2a\u4ea4\u6613\u6240\n                \"instrument_id\": code,               # //\u5fc5\u586b, \u4e0b\u5355\u5408\u7ea6\u4ee3\u7801\n                \"direction\": order_direction,                      # //\u5fc5\u586b, \u4e0b\u5355\u4e70\u5356\u65b9\u5411\n                # //\u5fc5\u586b, \u4e0b\u5355\u5f00\u5e73\u65b9\u5411, \u4ec5\u5f53\u6307\u4ee4\u76f8\u5173\u5bf9\u8c61\u4e0d\u652f\u6301\u5f00\u5e73\u673a\u5236(\u4f8b\u5982\u80a1\u7968)\u65f6\u53ef\u4e0d\u586b\u5199\u6b64\u5b57\u6bb5\n                \"offset\":  order_offset,\n                \"volume\":  volume,                             # //\u5fc5\u586b, \u4e0b\u5355\u624b\u6570\n                \"price_type\": \"LIMIT\",  # //\u5fc5\u586b, \u62a5\u5355\u4ef7\u683c\u7c7b\u578b\n                \"limit_price\": price,  # //\u5f53 price_type == LIMIT \u65f6\u9700\u8981\u586b\u5199\u6b64\u5b57\u6bb5, \u62a5\u5355\u4ef7\u683c\n                \"volume_condition\": \"ANY\",\n                \"time_condition\": \"GFD\",\n            }\n        \"\"\"\n        return {\n            \"aid\": \"insert_order\",                  # //\u5fc5\u586b, \u4e0b\u5355\u8bf7\u6c42\n            # //\u5fc5\u586b, \u9700\u8981\u4e0e\u767b\u5f55\u7528\u6237\u540d\u4e00\u81f4, \u6216\u4e3a\u767b\u5f55\u7528\u6237\u7684\u5b50\u8d26\u6237(\u4f8b\u5982\u767b\u5f55\u7528\u6237\u4e3auser1, \u5219\u62a5\u5355 user_id \u5e94\u5f53\u4e3a user1 \u6216 user1.some_unit)\n            \"user_id\": self.account_cookie,\n            # //\u5fc5\u586b, \u59d4\u6258\u5355\u53f7, \u9700\u786e\u4fdd\u5728\u4e00\u4e2a\u8d26\u53f7\u4e2d\u4e0d\u91cd\u590d, \u9650\u957f512\u5b57\u8282\n            \"order_id\": self.order_id,\n            \"exchange_id\": self.exchange_id,  # //\u5fc5\u586b, \u4e0b\u5355\u5230\u54ea\u4e2a\u4ea4\u6613\u6240\n            \"instrument_id\": self.code,               # //\u5fc5\u586b, \u4e0b\u5355\u5408\u7ea6\u4ee3\u7801\n            \"direction\": self.direction,                      # //\u5fc5\u586b, \u4e0b\u5355\u4e70\u5356\u65b9\u5411\n            # //\u5fc5\u586b, \u4e0b\u5355\u5f00\u5e73\u65b9\u5411, \u4ec5\u5f53\u6307\u4ee4\u76f8\u5173\u5bf9\u8c61\u4e0d\u652f\u6301\u5f00\u5e73\u673a\u5236(\u4f8b\u5982\u80a1\u7968)\u65f6\u53ef\u4e0d\u586b\u5199\u6b64\u5b57\u6bb5\n            \"offset\":  self.offset,\n            \"volume\":  self.amount,                             # //\u5fc5\u586b, \u4e0b\u5355\u624b\u6570\n            \"price_type\": self.order_model,  # //\u5fc5\u586b, \u62a5\u5355\u4ef7\u683c\u7c7b\u578b\n            \"limit_price\": self.price,  # //\u5f53 price_type == LIMIT \u65f6\u9700\u8981\u586b\u5199\u6b64\u5b57\u6bb5, \u62a5\u5355\u4ef7\u683c\n            \"volume_condition\": \"ANY\",\n            \"time_condition\": \"GFD\",\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_otgformat(self, otgOrder):\n        self.order_id = otgOrder.get('order_id')\n        self.account_cookie = otgOrder.get('user_id')\n        self.exchange_id = otgOrder.get('exchange_id')\n        self.code = str(otgOrder.get('instrument_id')).upper()\n        self.offset = otgOrder.get('offset')\n        self.direction = otgOrder.get('direction')\n        self.towards = eval('ORDER_DIRECTION.{}_{}'.format(\n            self.direction,\n            self.offset\n        ))\n        self.amount = otgOrder.get('volume_orign')\n        self.trade_amount = self.amount - otgOrder.get('volume_left')\n        self.price = otgOrder.get('limit_price')\n        self.order_model = eval(\n            'ORDER_MODEL.{}'.format(otgOrder.get('price_type'))\n        )\n        self.time_condition = otgOrder.get('time_condition')\n        if otgOrder.get('insert_date_time') == 0:\n            self.datetime = 0\n        else:\n            self.datetime = QA_util_stamp2datetime(\n                int(otgOrder.get('insert_date_time'))\n            )\n        self.sending_time = self.datetime\n        self.volume_condition = otgOrder.get('volume_condition')\n        self.message = otgOrder.get('last_msg')\n\n        self._status = ORDER_STATUS.NEW\n        if '\u5df2\u64a4\u5355' in self.message or '\u62d2\u7edd' in self.message or '\u4ed3\u4f4d\u4e0d\u8db3' in self.message:\n            # \u4ed3\u4f4d\u4e0d\u8db3:  \u4e00\u822c\u662f\u5e73\u4eca/\u5e73\u6628\u4ed3\u4f4d\u4e0d\u8db3\n            self._status = ORDER_STATUS.FAILED\n        self.realorder_id = otgOrder.get('exchange_order_id')\n        return self", "response": "This method is used to create a new object from a given OTP format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_dict(self, order_dict):\n        '''\n        \u4ece\u5b57\u6bb5\u7c7b\u578b\u7684\u5b57\u6bb5 \u586b\u5145 \u5bf9\u8c61\u7684\u5b57\u6bb5\n        :param order_dict:  dict \u7c7b\u578b\n        :return: self QA_Order\n        '''\n        try:\n            # QA_util_log_info('QA_ORDER CHANGE: from {} change to {}'.format(\n            #     self.order_id, order['order_id']))\n            self.price = order_dict['price']\n            self.date = order_dict['date']\n            self.datetime = order_dict['datetime']\n            self.sending_time = order_dict['sending_time'] # \u4e0b\u5355\u65f6\u95f4\n            self.trade_time = order_dict['trade_time']\n            self.amount = order_dict['amount']\n            self.frequence = order_dict['frequence']\n            self.market_type = order_dict['market_type']\n            self.towards = order_dict['towards']\n            self.code = order_dict['code']\n            self.user = order_dict['user']\n            self.account_cookie = order_dict['account_cookie']\n            self.strategy = order_dict['strategy']\n            self.type = order_dict['type']\n            self.order_model = order_dict['order_model']\n            self.amount_model = order_dict['amount_model']\n            self.order_id = order_dict['order_id']\n            self.realorder_id = order_dict['realorder_id']\n            self.trade_id = order_dict['trade_id']\n            self.callback = order_dict['callback']\n            self.commission_coeff = order_dict['commission_coeff']\n            self.tax_coeff = order_dict['tax_coeff']\n\n            self.money = order_dict['money']\n            self._status = order_dict['_status']\n\n            self.cancel_amount = order_dict['cancel_amount']\n            self.trade_amount = order_dict['trade_amount']\n            self.trade_price = order_dict['trade_price']\n            self.reason = order_dict['reason']\n\n            return self\n        except Exception as e:\n            QA_util_log_info('Failed to tran from dict {}'.format(e))", "response": "This method is used to create a new instance of the order object from a dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert_order(self, order):\n        '''\n        :param order: QA_Order\u7c7b\u578b\n        :return:\n        '''\n        #print(\"     *>> QAOrder!insert_order  {}\".format(order))\n        # QUEUED = 300  # queued \u7528\u4e8e\u8868\u793a\u5728order_queue\u4e2d \u5b9e\u9645\u8868\u8fbe\u7684\u610f\u601d\u662f\u8ba2\u5355\u5b58\u6d3b \u5f85\u6210\u4ea4\n        #order.status = ORDER_STATUS.QUEUED\n        # \ud83d\udee0 todo \u662f\u4e3a\u4e86\u901f\u5ea6\u5feb\u628aorder\u5bf9\u8c61\u8f6c\u6362\u6210 df \u5bf9\u8c61\u7684\u5417\uff1f\n        #self.queue_df = self.queue_df.append(order.to_df(), ignore_index=True)\n        #self.queue_df.set_index('order_id', drop=True, inplace=True)\n        if order is not None:\n            self.order_list[order.order_id] = order\n            return order\n        else:\n            print('QAERROR Wrong for get None type while insert order to Queue')", "response": "insert a new order into the queue"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pending(self):\n        '''\n        600 \u5e9f\u5355 \u672a\u59d4\u6258\u6210\u529f\n        200 \u59d4\u6258\u6210\u529f,\u5b8c\u5168\u4ea4\u6613\n        203 \u59d4\u6258\u6210\u529f,\u672a\u5b8c\u5168\u6210\u529f\n        300 \u59d4\u6258\u961f\u5217 \u5f85\u6210\u4ea4\n        400 \u5df2\u64a4\u5355\n        500 \u670d\u52a1\u5668\u64a4\u5355/\u6bcf\u65e5\u7ed3\u7b97\n\n\n        \u8ba2\u5355\u751f\u6210(100) -- \u5e9f\u5355(600)\n        \u8ba2\u5355\u751f\u6210(100) -- \u8fdb\u5165\u5f85\u6210\u4ea4\u961f\u5217(300) -- \u5b8c\u5168\u6210\u4ea4(200) -- \u6bcf\u65e5\u7ed3\u7b97(500)-- \u6b7b\u4ea1\n        \u8ba2\u5355\u751f\u6210(100) -- \u8fdb\u5165\u5f85\u6210\u4ea4\u961f\u5217(300) -- \u90e8\u5206\u6210\u4ea4(203) -- \u672a\u6210\u4ea4(300) -- \u6bcf\u65e5\u7ed3\u7b97(500) -- \u6b7b\u4ea1\n        \u8ba2\u5355\u751f\u6210(100) -- \u8fdb\u5165\u5f85\u6210\u4ea4\u961f\u5217(300) -- \u4e3b\u52a8\u64a4\u5355(400) -- \u6bcf\u65e5\u7ed3\u7b97(500) -- \u6b7b\u4ea1\n        \u9009\u62e9\u5f85\u6210\u4ea4\u5217\u8868\n        :return: dataframe\n        '''\n        try:\n            return [\n                item for item in self.order_list.values() if item.status in [\n                    ORDER_STATUS.QUEUED,\n                    ORDER_STATUS.NEXT,\n                    ORDER_STATUS.SUCCESS_PART\n                ]\n            ]\n        except:\n            return []", "response": "Return a dataframe of all pending entries."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_response(self, statement=None, **kwargs):\n        Statement = self.storage.get_object('statement')\n\n        additional_response_selection_parameters = kwargs.pop('additional_response_selection_parameters', {})\n\n        persist_values_to_response = kwargs.pop('persist_values_to_response', {})\n\n        if isinstance(statement, str):\n            kwargs['text'] = statement\n\n        if isinstance(statement, dict):\n            kwargs.update(statement)\n\n        if statement is None and 'text' not in kwargs:\n            raise self.ChatBotException(\n                'Either a statement object or a \"text\" keyword '\n                'argument is required. Neither was provided.'\n            )\n\n        if hasattr(statement, 'serialize'):\n            kwargs.update(**statement.serialize())\n\n        tags = kwargs.pop('tags', [])\n\n        text = kwargs.pop('text')\n\n        input_statement = Statement(text=text, **kwargs)\n\n        input_statement.add_tags(*tags)\n\n        # Preprocess the input statement\n        for preprocessor in self.preprocessors:\n            input_statement = preprocessor(input_statement)\n\n        # Make sure the input statement has its search text saved\n\n        if not input_statement.search_text:\n            input_statement.search_text = self.storage.tagger.get_bigram_pair_string(input_statement.text)\n\n        if not input_statement.search_in_response_to and input_statement.in_response_to:\n            input_statement.search_in_response_to = self.storage.tagger.get_bigram_pair_string(input_statement.in_response_to)\n\n        response = self.generate_response(input_statement, additional_response_selection_parameters)\n\n        # Update any response data that needs to be changed\n        if persist_values_to_response:\n            for response_key in persist_values_to_response:\n                response_value = persist_values_to_response[response_key]\n                if response_key == 'tags':\n                    input_statement.add_tags(*response_value)\n                    response.add_tags(*response_value)\n                else:\n                    setattr(input_statement, response_key, response_value)\n                    setattr(response, response_key, response_value)\n\n        if not self.read_only:\n            self.learn_response(input_statement)\n\n            # Save the response generated for the input\n            self.storage.create(**response.serialize())\n\n        return response", "response": "Returns a response based on the input statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a response based on a given input statement.", "response": "def generate_response(self, input_statement, additional_response_selection_parameters=None):\n        \"\"\"\n        Return a response based on a given input statement.\n\n        :param input_statement: The input statement to be processed.\n        \"\"\"\n        Statement = self.storage.get_object('statement')\n\n        results = []\n        result = None\n        max_confidence = -1\n\n        for adapter in self.logic_adapters:\n            if adapter.can_process(input_statement):\n\n                output = adapter.process(input_statement, additional_response_selection_parameters)\n                results.append(output)\n\n                self.logger.info(\n                    '{} selected \"{}\" as a response with a confidence of {}'.format(\n                        adapter.class_name, output.text, output.confidence\n                    )\n                )\n\n                if output.confidence > max_confidence:\n                    result = output\n                    max_confidence = output.confidence\n            else:\n                self.logger.info(\n                    'Not processing the statement using {}'.format(adapter.class_name)\n                )\n\n        class ResultOption:\n            def __init__(self, statement, count=1):\n                self.statement = statement\n                self.count = count\n\n        # If multiple adapters agree on the same statement,\n        # then that statement is more likely to be the correct response\n        if len(results) >= 3:\n            result_options = {}\n            for result_option in results:\n                result_string = result_option.text + ':' + (result_option.in_response_to or '')\n\n                if result_string in result_options:\n                    result_options[result_string].count += 1\n                    if result_options[result_string].statement.confidence < result_option.confidence:\n                        result_options[result_string].statement = result_option\n                else:\n                    result_options[result_string] = ResultOption(\n                        result_option\n                    )\n\n            most_common = list(result_options.values())[0]\n\n            for result_option in result_options.values():\n                if result_option.count > most_common.count:\n                    most_common = result_option\n\n            if most_common.count > 1:\n                result = most_common.statement\n\n        response = Statement(\n            text=result.text,\n            in_response_to=input_statement.text,\n            conversation=input_statement.conversation,\n            persona='bot:' + self.name\n        )\n\n        response.confidence = result.confidence\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlearn that the statement provided is a valid response.", "response": "def learn_response(self, statement, previous_statement=None):\n        \"\"\"\n        Learn that the statement provided is a valid response.\n        \"\"\"\n        if not previous_statement:\n            previous_statement = statement.in_response_to\n\n        if not previous_statement:\n            previous_statement = self.get_latest_response(statement.conversation)\n            if previous_statement:\n                previous_statement = previous_statement.text\n\n        previous_statement_text = previous_statement\n\n        if not isinstance(previous_statement, (str, type(None), )):\n            statement.in_response_to = previous_statement.text\n        elif isinstance(previous_statement, str):\n            statement.in_response_to = previous_statement\n\n        self.logger.info('Adding \"{}\" as a response to \"{}\"'.format(\n            statement.text,\n            previous_statement_text\n        ))\n\n        # Save the input statement\n        return self.storage.create(**statement.serialize())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary representation of the statement object.", "response": "def serialize(self):\n        \"\"\"\n        :returns: A dictionary representation of the statement object.\n        :rtype: dict\n        \"\"\"\n        data = {}\n\n        for field_name in self.get_statement_field_names():\n            format_method = getattr(self, 'get_{}'.format(\n                field_name\n            ), None)\n\n            if format_method:\n                data[field_name] = format_method()\n            else:\n                data[field_name] = getattr(self, field_name)\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_module(dotted_path):\n    import importlib\n\n    module_parts = dotted_path.split('.')\n    module_path = '.'.join(module_parts[:-1])\n    module = importlib.import_module(module_path)\n\n    return getattr(module, module_parts[-1])", "response": "Imports the specified module based on the specified dot notated import path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef initialize_class(data, *args, **kwargs):\n    if isinstance(data, dict):\n        import_path = data.get('import_path')\n        data.update(kwargs)\n        Class = import_module(import_path)\n\n        return Class(*args, **data)\n    else:\n        Class = import_module(data)\n\n        return Class(*args, **kwargs)", "response": "Initialize a new object of the correct type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating that the class passed in is a subclass of adapter_class.", "response": "def validate_adapter_class(validate_class, adapter_class):\n    \"\"\"\n    Raises an exception if validate_class is not a\n    subclass of adapter_class.\n\n    :param validate_class: The class to be validated.\n    :type validate_class: class\n\n    :param adapter_class: The class type to check against.\n    :type adapter_class: class\n\n    :raises: Adapter.InvalidAdapterTypeException\n    \"\"\"\n    from chatterbot.adapters import Adapter\n\n    # If a dictionary was passed in, check if it has an import_path attribute\n    if isinstance(validate_class, dict):\n\n        if 'import_path' not in validate_class:\n            raise Adapter.InvalidAdapterTypeException(\n                'The dictionary {} must contain a value for \"import_path\"'.format(\n                    str(validate_class)\n                )\n            )\n\n        # Set the class to the import path for the next check\n        validate_class = validate_class.get('import_path')\n\n    if not issubclass(import_module(validate_class), adapter_class):\n        raise Adapter.InvalidAdapterTypeException(\n            '{} must be a subclass of {}'.format(\n                validate_class,\n                adapter_class.__name__\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_response_time(chatbot, statement='Hello'):\n    import time\n\n    start_time = time.time()\n\n    chatbot.get_response(statement)\n\n    return time.time() - start_time", "response": "Returns the amount of time taken for a given chat bot to return a response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_progress_bar(description, iteration_counter, total_items, progress_bar_length=20):\n    import sys\n\n    percent = float(iteration_counter) / total_items\n    hashes = '#' * int(round(percent * progress_bar_length))\n    spaces = ' ' * (progress_bar_length - len(hashes))\n    sys.stdout.write('\\r{0}: [{1}] {2}%'.format(description, hashes + spaces, int(round(percent * 100))))\n    sys.stdout.flush()\n    if total_items == iteration_counter:\n        print('\\r')", "response": "Print a progress bar for the current iteration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the first match unit metric object supported by pint library", "response": "def get_unit(self, ureg, unit_variations):\n        \"\"\"\n        Get the first match unit metric object supported by pint library\n        given a variation of unit metric names (Ex:['HOUR', 'hour']).\n\n        :param ureg: unit registry which units are defined and handled\n        :type ureg: pint.registry.UnitRegistry object\n\n        :param unit_variations: A list of strings with names of units\n        :type unit_variations: str\n        \"\"\"\n        for unit in unit_variations:\n            try:\n                return getattr(ureg, unit)\n            except Exception:\n                continue\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the firt match pint. unit. Unit object for from_unit and target_unit strings from a possible variation of metric unit names", "response": "def get_valid_units(self, ureg, from_unit, target_unit):\n        \"\"\"\n        Returns the firt match `pint.unit.Unit` object for from_unit and\n        target_unit strings from a possible variation of metric unit names\n        supported by pint library.\n\n        :param ureg: unit registry which units are defined and handled\n        :type ureg: `pint.registry.UnitRegistry`\n\n        :param from_unit: source metric unit\n        :type from_unit: str\n\n        :param from_unit: target metric unit\n        :type from_unit: str\n        \"\"\"\n        from_unit_variations = [from_unit.lower(), from_unit.upper()]\n        target_unit_variations = [target_unit.lower(), target_unit.upper()]\n        from_unit = self.get_unit(ureg, from_unit_variations)\n        target_unit = self.get_unit(ureg, target_unit_variations)\n        return from_unit, target_unit"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_matches(self, match):\n        response = Statement(text='')\n\n        from_parsed = match.group(\"from\")\n        target_parsed = match.group(\"target\")\n        n_statement = match.group(\"number\")\n\n        if n_statement == 'a' or n_statement == 'an':\n            n_statement = '1.0'\n\n        n = mathparse.parse(n_statement, self.language.ISO_639.upper())\n\n        ureg = UnitRegistry()\n        from_parsed, target_parsed = self.get_valid_units(ureg, from_parsed, target_parsed)\n\n        if from_parsed is None or target_parsed is None:\n            response.confidence = 0.0\n        else:\n            from_value = ureg.Quantity(float(n), from_parsed)\n            target_value = from_value.to(target_parsed)\n            response.confidence = 1.0\n            response.text = str(target_value.magnitude)\n\n        return response", "response": "Handles the matches pattern and returns a response statement."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of significant features in the string.", "response": "def time_question_features(self, text):\n        \"\"\"\n        Provide an analysis of significant features in the string.\n        \"\"\"\n        features = {}\n\n        # A list of all words from the known sentences\n        all_words = \" \".join(self.positive + self.negative).split()\n\n        # A list of the first word in each of the known sentence\n        all_first_words = []\n        for sentence in self.positive + self.negative:\n            all_first_words.append(\n                sentence.split(' ', 1)[0]\n            )\n\n        for word in text.split():\n            features['first_word({})'.format(word)] = (word in all_first_words)\n\n        for word in text.split():\n            features['contains({})'.format(word)] = (word in all_words)\n\n        for letter in 'abcdefghijklmnopqrstuvwxyz':\n            features['count({})'.format(letter)] = text.lower().count(letter)\n            features['has({})'.format(letter)] = (letter in text.lower())\n\n        return features"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine whether the user can process the given statement.", "response": "def can_process(self, statement):\n        \"\"\"\n        Determines whether it is appropriate for this\n        adapter to respond to the user input.\n        \"\"\"\n        response = self.process(statement)\n        self.cache[statement.text] = response\n        return response.confidence == 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess a statement string and returns the equation from the statement with the mathematical terms solved.", "response": "def process(self, statement, additional_response_selection_parameters=None):\n        \"\"\"\n        Takes a statement string.\n        Returns the equation from the statement with the mathematical terms solved.\n        \"\"\"\n        from mathparse import mathparse\n\n        input_text = statement.text\n\n        # Use the result cached by the process method if it exists\n        if input_text in self.cache:\n            cached_result = self.cache[input_text]\n            self.cache = {}\n            return cached_result\n\n        # Getting the mathematical terms within the input statement\n        expression = mathparse.extract_expression(input_text, language=self.language.ISO_639.upper())\n\n        response = Statement(text=expression)\n\n        try:\n            response.text += ' = ' + str(\n                mathparse.parse(expression, language=self.language.ISO_639.upper())\n            )\n\n            # The confidence is 1 if the expression could be evaluated\n            response.confidence = 1\n        except mathparse.PostfixTokenEvaluationException:\n            response.confidence = 0\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of recent non - repetitive responses from a chatbot conversation.", "response": "def get_recent_repeated_responses(chatbot, conversation, sample=10, threshold=3, quantity=3):\n    \"\"\"\n    A filter that eliminates possibly repetitive responses to prevent\n    a chat bot from repeating statements that it has recently said.\n    \"\"\"\n    from collections import Counter\n\n    # Get the most recent statements from the conversation\n    conversation_statements = list(chatbot.storage.filter(\n        conversation=conversation,\n        order_by=['id']\n    ))[sample * -1:]\n\n    text_of_recent_responses = [\n        statement.text for statement in conversation_statements\n    ]\n\n    counter = Counter(text_of_recent_responses)\n\n    # Find the n most common responses from the conversation\n    most_common = counter.most_common(quantity)\n\n    return [\n        counted[0] for counted in most_common\n        if counted[1] >= threshold\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_most_frequent_response(input_statement, response_list, storage=None):\n    matching_response = None\n    occurrence_count = -1\n\n    logger = logging.getLogger(__name__)\n    logger.info('Selecting response with greatest number of occurrences.')\n\n    for statement in response_list:\n        count = len(list(storage.filter(\n            text=statement.text,\n            in_response_to=input_statement.text)\n        ))\n\n        # Keep the more common statement\n        if count >= occurrence_count:\n            matching_response = statement\n            occurrence_count = count\n\n    # Choose the most commonly occuring matching response\n    return matching_response", "response": "Returns the most frequent response from a list of response statements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the first response from the list of options.", "response": "def get_first_response(input_statement, response_list, storage=None):\n    \"\"\"\n    :param input_statement: A statement, that closely matches an input to the chat bot.\n    :type input_statement: Statement\n\n    :param response_list: A list of statement options to choose a response from.\n    :type response_list: list\n\n    :param storage: An instance of a storage adapter to allow the response selection\n                    method to access other statements if needed.\n    :type storage: StorageAdapter\n\n    :return: Return the first statement in the response list.\n    :rtype: Statement\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info('Selecting first response from list of {} options.'.format(\n        len(response_list)\n    ))\n    return response_list[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a random response from the list of options.", "response": "def get_random_response(input_statement, response_list, storage=None):\n    \"\"\"\n    :param input_statement: A statement, that closely matches an input to the chat bot.\n    :type input_statement: Statement\n\n    :param response_list: A list of statement options to choose a response from.\n    :type response_list: list\n\n    :param storage: An instance of a storage adapter to allow the response selection\n                    method to access other statements if needed.\n    :type storage: StorageAdapter\n\n    :return: Choose a random response from the selection.\n    :rtype: Statement\n    \"\"\"\n    from random import choice\n    logger = logging.getLogger(__name__)\n    logger.info('Selecting a response from list of {} options.'.format(\n        len(response_list)\n    ))\n    return choice(response_list)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompare two input statements.", "response": "def compare(self, statement_a, statement_b):\n        \"\"\"\n        Compare the two input statements.\n\n        :return: The percent of similarity between the text of the statements.\n        :rtype: float\n        \"\"\"\n\n        # Return 0 if either statement has a falsy text value\n        if not statement_a.text or not statement_b.text:\n            return 0\n\n        # Get the lowercase version of both strings\n        statement_a_text = str(statement_a.text.lower())\n        statement_b_text = str(statement_b.text.lower())\n\n        similarity = SequenceMatcher(\n            None,\n            statement_a_text,\n            statement_b_text\n        )\n\n        # Calculate a decimal percent of the similarity\n        percent = round(similarity.ratio(), 2)\n\n        return percent"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomparing two synset sets and returns the percent of similarity between the closest synset distance between the closest synset distance.", "response": "def compare(self, statement_a, statement_b):\n        \"\"\"\n        Compare the two input statements.\n\n        :return: The percent of similarity between the closest synset distance.\n        :rtype: float\n        \"\"\"\n        document_a = self.nlp(statement_a.text)\n        document_b = self.nlp(statement_b.text)\n\n        return document_a.similarity(document_b)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompare two set of statements based on the Jaccard index.", "response": "def compare(self, statement_a, statement_b):\n        \"\"\"\n        Return the calculated similarity of two\n        statements based on the Jaccard index.\n        \"\"\"\n        # Make both strings lowercase\n        document_a = self.nlp(statement_a.text.lower())\n        document_b = self.nlp(statement_b.text.lower())\n\n        statement_a_lemmas = set([\n            token.lemma_ for token in document_a if not token.is_stop\n        ])\n        statement_b_lemmas = set([\n            token.lemma_ for token in document_b if not token.is_stop\n        ])\n\n        # Calculate Jaccard similarity\n        numerator = len(statement_a_lemmas.intersection(statement_b_lemmas))\n        denominator = float(len(statement_a_lemmas.union(statement_b_lemmas)))\n        ratio = numerator / denominator\n\n        return ratio"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the class for the statement model.", "response": "def get_statement_model(self):\n        \"\"\"\n        Return the class for the statement model.\n        \"\"\"\n        from chatterbot.conversation import Statement\n\n        # Create a storage-aware statement\n        statement = Statement\n        statement.storage = self\n\n        return statement"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mongo_to_object(self, statement_data):\n        Statement = self.get_model('statement')\n\n        statement_data['id'] = statement_data['_id']\n\n        return Statement(**statement_data)", "response": "Return a Statement object when given data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of statements in the database that match the parameters specified.", "response": "def filter(self, **kwargs):\n        \"\"\"\n        Returns a list of statements in the database\n        that match the parameters specified.\n        \"\"\"\n        import pymongo\n\n        page_size = kwargs.pop('page_size', 1000)\n        order_by = kwargs.pop('order_by', None)\n        tags = kwargs.pop('tags', [])\n        exclude_text = kwargs.pop('exclude_text', None)\n        exclude_text_words = kwargs.pop('exclude_text_words', [])\n        persona_not_startswith = kwargs.pop('persona_not_startswith', None)\n        search_text_contains = kwargs.pop('search_text_contains', None)\n\n        if tags:\n            kwargs['tags'] = {\n                '$in': tags\n            }\n\n        if exclude_text:\n            if 'text' not in kwargs:\n                kwargs['text'] = {}\n            elif 'text' in kwargs and isinstance(kwargs['text'], str):\n                text = kwargs.pop('text')\n                kwargs['text'] = {\n                    '$eq': text\n                }\n            kwargs['text']['$nin'] = exclude_text\n\n        if exclude_text_words:\n            if 'text' not in kwargs:\n                kwargs['text'] = {}\n            elif 'text' in kwargs and isinstance(kwargs['text'], str):\n                text = kwargs.pop('text')\n                kwargs['text'] = {\n                    '$eq': text\n                }\n            exclude_word_regex = '|'.join([\n                '.*{}.*'.format(word) for word in exclude_text_words\n            ])\n            kwargs['text']['$not'] = re.compile(exclude_word_regex)\n\n        if persona_not_startswith:\n            if 'persona' not in kwargs:\n                kwargs['persona'] = {}\n            elif 'persona' in kwargs and isinstance(kwargs['persona'], str):\n                persona = kwargs.pop('persona')\n                kwargs['persona'] = {\n                    '$eq': persona\n                }\n            kwargs['persona']['$not'] = re.compile('^bot:*')\n\n        if search_text_contains:\n            or_regex = '|'.join([\n                '{}'.format(word) for word in search_text_contains.split(' ')\n            ])\n            kwargs['search_text'] = re.compile(or_regex)\n\n        mongo_ordering = []\n\n        if order_by:\n\n            # Sort so that newer datetimes appear first\n            if 'created_at' in order_by:\n                order_by.remove('created_at')\n                mongo_ordering.append(('created_at', pymongo.DESCENDING, ))\n\n            for order in order_by:\n                mongo_ordering.append((order, pymongo.ASCENDING))\n\n        total_statements = self.statements.find(kwargs).count()\n\n        for start_index in range(0, total_statements, page_size):\n            if mongo_ordering:\n                for match in self.statements.find(kwargs).sort(mongo_ordering).skip(start_index).limit(page_size):\n                    yield self.mongo_to_object(match)\n            else:\n                for match in self.statements.find(kwargs).skip(start_index).limit(page_size):\n                    yield self.mongo_to_object(match)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new statement matching the keyword arguments specified. Returns the newly created statement.", "response": "def create(self, **kwargs):\n        \"\"\"\n        Creates a new statement matching the keyword arguments specified.\n        Returns the created statement.\n        \"\"\"\n        Statement = self.get_model('statement')\n\n        if 'tags' in kwargs:\n            kwargs['tags'] = list(set(kwargs['tags']))\n\n        if 'search_text' not in kwargs:\n            kwargs['search_text'] = self.tagger.get_bigram_pair_string(kwargs['text'])\n\n        if 'search_in_response_to' not in kwargs:\n            if kwargs.get('in_response_to'):\n                kwargs['search_in_response_to'] = self.tagger.get_bigram_pair_string(kwargs['in_response_to'])\n\n        inserted = self.statements.insert_one(kwargs)\n\n        kwargs['id'] = inserted.inserted_id\n\n        return Statement(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_many(self, statements):\n        create_statements = []\n\n        for statement in statements:\n            statement_data = statement.serialize()\n            tag_data = list(set(statement_data.pop('tags', [])))\n            statement_data['tags'] = tag_data\n\n            if not statement.search_text:\n                statement_data['search_text'] = self.tagger.get_bigram_pair_string(statement.text)\n\n            if not statement.search_in_response_to and statement.in_response_to:\n                statement_data['search_in_response_to'] = self.tagger.get_bigram_pair_string(statement.in_response_to)\n\n            create_statements.append(statement_data)\n\n        self.statements.insert_many(create_statements)", "response": "Creates multiple statement entries."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a random statement from the database", "response": "def get_random(self):\n        \"\"\"\n        Returns a random statement from the database\n        \"\"\"\n        from random import randint\n\n        count = self.count()\n\n        if count < 1:\n            raise self.EmptyDatabaseException()\n\n        random_integer = randint(0, count - 1)\n\n        statements = self.statements.find().limit(1).skip(random_integer)\n\n        return self.mongo_to_object(list(statements)[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a list of strings to the statement as tags.", "response": "def add_tags(self, *tags):\n        \"\"\"\n        Add a list of strings to the statement as tags.\n        \"\"\"\n        self.tags.extend([\n            Tag(name=tag) for tag in tags\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexports the current conversations to a JSON file that can be used to train other chat bots.", "response": "def export_for_training(self, file_path='./export.json'):\n        \"\"\"\n        Create a file from the database that can be used to\n        train other chat bots.\n        \"\"\"\n        import json\n        export = {'conversations': self._generate_export_data()}\n        with open(file_path, 'w+') as jsonfile:\n            json.dump(export, jsonfile, ensure_ascii=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntrains the chat bot based on the provided list of statements that represents a single conversation.", "response": "def train(self, conversation):\n        \"\"\"\n        Train the chat bot based on the provided list of\n        statements that represents a single conversation.\n        \"\"\"\n        previous_statement_text = None\n        previous_statement_search_text = ''\n\n        statements_to_create = []\n\n        for conversation_count, text in enumerate(conversation):\n            if self.show_training_progress:\n                utils.print_progress_bar(\n                    'List Trainer',\n                    conversation_count + 1, len(conversation)\n                )\n\n            statement_search_text = self.chatbot.storage.tagger.get_bigram_pair_string(text)\n\n            statement = self.get_preprocessed_statement(\n                Statement(\n                    text=text,\n                    search_text=statement_search_text,\n                    in_response_to=previous_statement_text,\n                    search_in_response_to=previous_statement_search_text,\n                    conversation='training'\n                )\n            )\n\n            previous_statement_text = statement.text\n            previous_statement_search_text = statement_search_text\n\n            statements_to_create.append(statement)\n\n        self.chatbot.storage.create_many(statements_to_create)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the data file is already downloaded.", "response": "def is_downloaded(self, file_path):\n        \"\"\"\n        Check if the data file is already downloaded.\n        \"\"\"\n        if os.path.exists(file_path):\n            self.chatbot.logger.info('File is already downloaded')\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_extracted(self, file_path):\n\n        if os.path.isdir(file_path):\n            self.chatbot.logger.info('File is already extracted')\n            return True\n        return False", "response": "Check if the data file is already extracted."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads a file from the given url.", "response": "def download(self, url, show_status=True):\n        \"\"\"\n        Download a file from the given url.\n        Show a progress indicator for the download status.\n        Based on: http://stackoverflow.com/a/15645088/1547223\n        \"\"\"\n        import requests\n\n        file_name = url.split('/')[-1]\n        file_path = os.path.join(self.data_directory, file_name)\n\n        # Do not download the data if it already exists\n        if self.is_downloaded(file_path):\n            return file_path\n\n        with open(file_path, 'wb') as open_file:\n            print('Downloading %s' % url)\n            response = requests.get(url, stream=True)\n            total_length = response.headers.get('content-length')\n\n            if total_length is None:\n                # No content length header\n                open_file.write(response.content)\n            else:\n                download = 0\n                total_length = int(total_length)\n                for data in response.iter_content(chunk_size=4096):\n                    download += len(data)\n                    open_file.write(data)\n                    if show_status:\n                        done = int(50 * download / total_length)\n                        sys.stdout.write('\\r[%s%s]' % ('=' * done, ' ' * (50 - done)))\n                        sys.stdout.flush()\n\n            # Add a new line after the download bar\n            sys.stdout.write('\\n')\n\n        print('Download location: %s' % file_path)\n        return file_path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting a tar file at the specified file path.", "response": "def extract(self, file_path):\n        \"\"\"\n        Extract a tar file at the specified file path.\n        \"\"\"\n        import tarfile\n\n        print('Extracting {}'.format(file_path))\n\n        if not os.path.exists(self.extracted_data_directory):\n            os.makedirs(self.extracted_data_directory)\n\n        def track_progress(members):\n            sys.stdout.write('.')\n            for member in members:\n                # This will be the current file being extracted\n                yield member\n\n        with tarfile.open(file_path) as tar:\n            tar.extractall(path=self.extracted_data_directory, members=track_progress(tar))\n\n        self.chatbot.logger.info('File extracted to {}'.format(self.extracted_data_directory))\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef count(self):\n        Statement = self.get_model('statement')\n\n        session = self.Session()\n        statement_count = session.query(Statement).count()\n        session.close()\n        return statement_count", "response": "Return the number of entries in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove(self, statement_text):\n        Statement = self.get_model('statement')\n        session = self.Session()\n\n        query = session.query(Statement).filter_by(text=statement_text)\n        record = query.first()\n\n        session.delete(record)\n\n        self._session_finish(session)", "response": "Removes the statement that matches the input text."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter(self, **kwargs):\n        from sqlalchemy import or_\n\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        session = self.Session()\n\n        page_size = kwargs.pop('page_size', 1000)\n        order_by = kwargs.pop('order_by', None)\n        tags = kwargs.pop('tags', [])\n        exclude_text = kwargs.pop('exclude_text', None)\n        exclude_text_words = kwargs.pop('exclude_text_words', [])\n        persona_not_startswith = kwargs.pop('persona_not_startswith', None)\n        search_text_contains = kwargs.pop('search_text_contains', None)\n\n        # Convert a single sting into a list if only one tag is provided\n        if type(tags) == str:\n            tags = [tags]\n\n        if len(kwargs) == 0:\n            statements = session.query(Statement).filter()\n        else:\n            statements = session.query(Statement).filter_by(**kwargs)\n\n        if tags:\n            statements = statements.join(Statement.tags).filter(\n                Tag.name.in_(tags)\n            )\n\n        if exclude_text:\n            statements = statements.filter(\n                ~Statement.text.in_(exclude_text)\n            )\n\n        if exclude_text_words:\n            or_word_query = [\n                Statement.text.ilike('%' + word + '%') for word in exclude_text_words\n            ]\n            statements = statements.filter(\n                ~or_(*or_word_query)\n            )\n\n        if persona_not_startswith:\n            statements = statements.filter(\n                ~Statement.persona.startswith('bot:')\n            )\n\n        if search_text_contains:\n            or_query = [\n                Statement.search_text.contains(word) for word in search_text_contains.split(' ')\n            ]\n            statements = statements.filter(\n                or_(*or_query)\n            )\n\n        if order_by:\n\n            if 'created_at' in order_by:\n                index = order_by.index('created_at')\n                order_by[index] = Statement.created_at.asc()\n\n            statements = statements.order_by(*order_by)\n\n        total_statements = statements.count()\n\n        for start_index in range(0, total_statements, page_size):\n            for statement in statements.slice(start_index, start_index + page_size):\n                yield self.model_to_object(statement)\n\n        session.close()", "response": "Returns a list of objects from the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new statement matching the keyword arguments specified. Returns the created statement.", "response": "def create(self, **kwargs):\n        \"\"\"\n        Creates a new statement matching the keyword arguments specified.\n        Returns the created statement.\n        \"\"\"\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        session = self.Session()\n\n        tags = set(kwargs.pop('tags', []))\n\n        if 'search_text' not in kwargs:\n            kwargs['search_text'] = self.tagger.get_bigram_pair_string(kwargs['text'])\n\n        if 'search_in_response_to' not in kwargs:\n            in_response_to = kwargs.get('in_response_to')\n            if in_response_to:\n                kwargs['search_in_response_to'] = self.tagger.get_bigram_pair_string(in_response_to)\n\n        statement = Statement(**kwargs)\n\n        for tag_name in tags:\n            tag = session.query(Tag).filter_by(name=tag_name).first()\n\n            if not tag:\n                # Create the tag\n                tag = Tag(name=tag_name)\n\n            statement.tags.append(tag)\n\n        session.add(statement)\n\n        session.flush()\n\n        session.refresh(statement)\n\n        statement_object = self.model_to_object(statement)\n\n        self._session_finish(session)\n\n        return statement_object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates multiple statements and tags.", "response": "def create_many(self, statements):\n        \"\"\"\n        Creates multiple statement entries.\n        \"\"\"\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        session = self.Session()\n\n        create_statements = []\n        create_tags = {}\n\n        for statement in statements:\n\n            statement_data = statement.serialize()\n            tag_data = statement_data.pop('tags', [])\n\n            statement_model_object = Statement(**statement_data)\n\n            if not statement.search_text:\n                statement_model_object.search_text = self.tagger.get_bigram_pair_string(statement.text)\n\n            if not statement.search_in_response_to and statement.in_response_to:\n                statement_model_object.search_in_response_to = self.tagger.get_bigram_pair_string(statement.in_response_to)\n\n            new_tags = set(tag_data) - set(create_tags.keys())\n\n            if new_tags:\n                existing_tags = session.query(Tag).filter(\n                    Tag.name.in_(new_tags)\n                )\n\n                for existing_tag in existing_tags:\n                    create_tags[existing_tag.name] = existing_tag\n\n            for tag_name in tag_data:\n                if tag_name in create_tags:\n                    tag = create_tags[tag_name]\n                else:\n                    # Create the tag if it does not exist\n                    tag = Tag(name=tag_name)\n\n                    create_tags[tag_name] = tag\n\n                statement_model_object.tags.append(tag)\n            create_statements.append(statement_model_object)\n\n        session.add_all(create_statements)\n        session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate an entry in the database.", "response": "def update(self, statement):\n        \"\"\"\n        Modifies an entry in the database.\n        Creates an entry if one does not exist.\n        \"\"\"\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        if statement is not None:\n            session = self.Session()\n            record = None\n\n            if hasattr(statement, 'id') and statement.id is not None:\n                record = session.query(Statement).get(statement.id)\n            else:\n                record = session.query(Statement).filter(\n                    Statement.text == statement.text,\n                    Statement.conversation == statement.conversation,\n                ).first()\n\n                # Create a new statement entry if one does not already exist\n                if not record:\n                    record = Statement(\n                        text=statement.text,\n                        conversation=statement.conversation,\n                        persona=statement.persona\n                    )\n\n            # Update the response value\n            record.in_response_to = statement.in_response_to\n\n            record.created_at = statement.created_at\n\n            record.search_text = self.tagger.get_bigram_pair_string(statement.text)\n\n            if statement.in_response_to:\n                record.search_in_response_to = self.tagger.get_bigram_pair_string(statement.in_response_to)\n\n            for tag_name in statement.get_tags():\n                tag = session.query(Tag).filter_by(name=tag_name).first()\n\n                if not tag:\n                    # Create the record\n                    tag = Tag(name=tag_name)\n\n                record.tags.append(tag)\n\n            session.add(record)\n\n            self._session_finish(session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_random(self):\n        import random\n\n        Statement = self.get_model('statement')\n\n        session = self.Session()\n        count = self.count()\n        if count < 1:\n            raise self.EmptyDatabaseException()\n\n        random_index = random.randrange(0, count)\n        random_statement = session.query(Statement)[random_index]\n\n        statement = self.model_to_object(random_statement)\n\n        session.close()\n        return statement", "response": "Returns a random statement from the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_database(self):\n        from chatterbot.ext.sqlalchemy_app.models import Base\n        Base.metadata.create_all(self.engine)", "response": "Create the database with the tables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a response to the statement in the posted data.", "response": "def post(self, request, *args, **kwargs):\n        \"\"\"\n        Return a response to the statement in the posted data.\n\n        * The JSON data should contain a 'text' attribute.\n        \"\"\"\n        input_data = json.loads(request.body.decode('utf-8'))\n\n        if 'text' not in input_data:\n            return JsonResponse({\n                'text': [\n                    'The attribute \"text\" is required.'\n                ]\n            }, status=400)\n\n        response = self.chatterbot.get_response(input_data)\n\n        response_data = response.serialize()\n\n        return JsonResponse(response_data, status=200)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a dotted file path and returns the file path.", "response": "def get_file_path(dotted_path, extension='json'):\n    \"\"\"\n    Reads a dotted file path and returns the file path.\n    \"\"\"\n    # If the operating system's file path seperator character is in the string\n    if os.sep in dotted_path or '/' in dotted_path:\n        # Assume the path is a valid file path\n        return dotted_path\n\n    parts = dotted_path.split('.')\n    if parts[0] == 'chatterbot':\n        parts.pop(0)\n        parts[0] = DATA_DIRECTORY\n\n    corpus_path = os.path.join(*parts)\n\n    if os.path.exists(corpus_path + '.{}'.format(extension)):\n        corpus_path += '.{}'.format(extension)\n\n    return corpus_path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_corpus(file_name):\n    with io.open(file_name, encoding='utf-8') as data_file:\n        return yaml.load(data_file)", "response": "Read and return the data from a corpus json file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of file paths to each data file in the specified corpus.", "response": "def list_corpus_files(dotted_path):\n    \"\"\"\n    Return a list of file paths to each data file in the specified corpus.\n    \"\"\"\n    corpus_path = get_file_path(dotted_path, extension=CORPUS_EXTENSION)\n    paths = []\n\n    if os.path.isdir(corpus_path):\n        paths = glob.glob(corpus_path + '/**/*.' + CORPUS_EXTENSION, recursive=True)\n    else:\n        paths.append(corpus_path)\n\n    paths.sort()\n    return paths"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the data contained within a specified corpus.", "response": "def load_corpus(*data_file_paths):\n    \"\"\"\n    Return the data contained within a specified corpus.\n    \"\"\"\n    for file_path in data_file_paths:\n        corpus = []\n        corpus_data = read_corpus(file_path)\n\n        conversations = corpus_data.get('conversations', [])\n        corpus.extend(conversations)\n\n        categories = corpus_data.get('categories', [])\n\n        yield corpus, categories, file_path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_bigram_pair_string(self, text):\n        bigram_pairs = []\n\n        if len(text) <= 2:\n            text_without_punctuation = text.translate(self.punctuation_table)\n            if len(text_without_punctuation) >= 1:\n                text = text_without_punctuation\n\n        document = self.nlp(text)\n\n        if len(text) <= 2:\n            bigram_pairs = [\n                token.lemma_.lower() for token in document\n            ]\n        else:\n            tokens = [\n                token for token in document if token.is_alpha and not token.is_stop\n            ]\n\n            if len(tokens) < 2:\n                tokens = [\n                    token for token in document if token.is_alpha\n                ]\n\n            for index in range(1, len(tokens)):\n                bigram_pairs.append('{}:{}'.format(\n                    tokens[index - 1].pos_,\n                    tokens[index].lemma_.lower()\n                ))\n\n        if not bigram_pairs:\n            bigram_pairs = [\n                token.lemma_.lower() for token in document\n            ]\n\n        return ' '.join(bigram_pairs)", "response": "Return a string of text containing part - of - speech lemma pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of statements in the database that match the parameters specified.", "response": "def filter(self, **kwargs):\n        \"\"\"\n        Returns a list of statements in the database\n        that match the parameters specified.\n        \"\"\"\n        from django.db.models import Q\n\n        Statement = self.get_model('statement')\n\n        kwargs.pop('page_size', 1000)\n        order_by = kwargs.pop('order_by', None)\n        tags = kwargs.pop('tags', [])\n        exclude_text = kwargs.pop('exclude_text', None)\n        exclude_text_words = kwargs.pop('exclude_text_words', [])\n        persona_not_startswith = kwargs.pop('persona_not_startswith', None)\n        search_text_contains = kwargs.pop('search_text_contains', None)\n\n        # Convert a single sting into a list if only one tag is provided\n        if type(tags) == str:\n            tags = [tags]\n\n        if tags:\n            kwargs['tags__name__in'] = tags\n\n        statements = Statement.objects.filter(**kwargs)\n\n        if exclude_text:\n            statements = statements.exclude(\n                text__in=exclude_text\n            )\n\n        if exclude_text_words:\n            or_query = [\n                ~Q(text__icontains=word) for word in exclude_text_words\n            ]\n\n            statements = statements.filter(\n                *or_query\n            )\n\n        if persona_not_startswith:\n            statements = statements.exclude(\n                persona__startswith='bot:'\n            )\n\n        if search_text_contains:\n            or_query = Q()\n\n            for word in search_text_contains.split(' '):\n                or_query |= Q(search_text__contains=word)\n\n            statements = statements.filter(\n                or_query\n            )\n\n        if order_by:\n            statements = statements.order_by(*order_by)\n\n        for statement in statements.iterator():\n            yield statement"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new statement matching the keyword arguments specified. Returns the created statement.", "response": "def create(self, **kwargs):\n        \"\"\"\n        Creates a new statement matching the keyword arguments specified.\n        Returns the created statement.\n        \"\"\"\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        tags = kwargs.pop('tags', [])\n\n        if 'search_text' not in kwargs:\n            kwargs['search_text'] = self.tagger.get_bigram_pair_string(kwargs['text'])\n\n        if 'search_in_response_to' not in kwargs:\n            if kwargs.get('in_response_to'):\n                kwargs['search_in_response_to'] = self.tagger.get_bigram_pair_string(kwargs['in_response_to'])\n\n        statement = Statement(**kwargs)\n\n        statement.save()\n\n        tags_to_add = []\n\n        for _tag in tags:\n            tag, _ = Tag.objects.get_or_create(name=_tag)\n            tags_to_add.append(tag)\n\n        statement.tags.add(*tags_to_add)\n\n        return statement"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_many(self, statements):\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        tag_cache = {}\n\n        for statement in statements:\n\n            statement_data = statement.serialize()\n            tag_data = statement_data.pop('tags', [])\n\n            statement_model_object = Statement(**statement_data)\n\n            if not statement.search_text:\n                statement_model_object.search_text = self.tagger.get_bigram_pair_string(statement.text)\n\n            if not statement.search_in_response_to and statement.in_response_to:\n                statement_model_object.search_in_response_to = self.tagger.get_bigram_pair_string(statement.in_response_to)\n\n            statement_model_object.save()\n\n            tags_to_add = []\n\n            for tag_name in tag_data:\n                if tag_name in tag_cache:\n                    tag = tag_cache[tag_name]\n                else:\n                    tag, _ = Tag.objects.get_or_create(name=tag_name)\n                    tag_cache[tag_name] = tag\n                tags_to_add.append(tag)\n\n            statement_model_object.tags.add(*tags_to_add)", "response": "Creates multiple statement entries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, statement):\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        if hasattr(statement, 'id'):\n            statement.save()\n        else:\n            statement = Statement.objects.create(\n                text=statement.text,\n                search_text=self.tagger.get_bigram_pair_string(statement.text),\n                conversation=statement.conversation,\n                in_response_to=statement.in_response_to,\n                search_in_response_to=self.tagger.get_bigram_pair_string(statement.in_response_to),\n                created_at=statement.created_at\n            )\n\n        for _tag in statement.tags.all():\n            tag, _ = Tag.objects.get_or_create(name=_tag)\n\n            statement.tags.add(tag)\n\n        return statement", "response": "Update the provided statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_random(self):\n        Statement = self.get_model('statement')\n\n        statement = Statement.objects.order_by('?').first()\n\n        if statement is None:\n            raise self.EmptyDatabaseException()\n\n        return statement", "response": "Returns a random statement from the database"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove the statement that matches the input text.", "response": "def remove(self, statement_text):\n        \"\"\"\n        Removes the statement that matches the input text.\n        Removes any responses from statements if the response text matches the\n        input text.\n        \"\"\"\n        Statement = self.get_model('statement')\n\n        statements = Statement.objects.filter(text=statement_text)\n\n        statements.delete()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drop(self):\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        Statement.objects.all().delete()\n        Tag.objects.all().delete()", "response": "Drop all data from the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving any consecutive whitespace characters from the statement text.", "response": "def clean_whitespace(statement):\n    \"\"\"\n    Remove any consecutive whitespace characters from the statement text.\n    \"\"\"\n    import re\n\n    # Replace linebreaks and tabs with spaces\n    statement.text = statement.text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n\n    # Remove any leeding or trailing whitespace\n    statement.text = statement.text.strip()\n\n    # Remove consecutive spaces\n    statement.text = re.sub(' +', ' ', statement.text)\n\n    return statement"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert escaped html characters into unescaped html characters. For example: \"&lt;b&gt;\" becomes \"<b>\".", "response": "def unescape_html(statement):\n    \"\"\"\n    Convert escaped html characters into unescaped html characters.\n    For example: \"&lt;b&gt;\" becomes \"<b>\".\n    \"\"\"\n    import html\n\n    statement.text = html.unescape(statement.text)\n\n    return statement"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_to_ascii(statement):\n    import unicodedata\n\n    text = unicodedata.normalize('NFKD', statement.text)\n    text = text.encode('ascii', 'ignore').decode('utf-8')\n\n    statement.text = str(text)\n    return statement", "response": "Converts unicode characters to ASCII character equivalents."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_string_to_number(value):\n    if value is None:\n        return 1\n    if isinstance(value, int):\n        return value\n    if value.isdigit():\n        return int(value)\n    num_list = map(lambda s: NUMBERS[s], re.findall(numbers + '+', value.lower()))\n    return sum(num_list)", "response": "Convert a string to a number"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert time to hour minute", "response": "def convert_time_to_hour_minute(hour, minute, convention):\n    \"\"\"\n    Convert time to hour, minute\n    \"\"\"\n    if hour is None:\n        hour = 0\n    if minute is None:\n        minute = 0\n    if convention is None:\n        convention = 'am'\n\n    hour = int(hour)\n    minute = int(minute)\n\n    if convention.lower() == 'pm':\n        hour += 12\n\n    return {'hours': hour, 'minutes': minute}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract date from quarter of a year", "response": "def date_from_quarter(base_date, ordinal, year):\n    \"\"\"\n    Extract date from quarter of a year\n    \"\"\"\n    interval = 3\n    month_start = interval * (ordinal - 1)\n    if month_start < 0:\n        month_start = 9\n    month_end = month_start + interval\n    if month_start == 0:\n        month_start = 1\n    return [\n        datetime(year, month_start, 1),\n        datetime(year, month_end, calendar.monthrange(year, month_end)[1])\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting relative day to time", "response": "def date_from_relative_day(base_date, time, dow):\n    \"\"\"\n    Converts relative day to time\n    Ex: this tuesday, last tuesday\n    \"\"\"\n    # Reset date to start of the day\n    base_date = datetime(base_date.year, base_date.month, base_date.day)\n    time = time.lower()\n    dow = dow.lower()\n    if time == 'this' or time == 'coming':\n        # Else day of week\n        num = HASHWEEKDAYS[dow]\n        return this_week_day(base_date, num)\n    elif time == 'last' or time == 'previous':\n        # Else day of week\n        num = HASHWEEKDAYS[dow]\n        return previous_week_day(base_date, num)\n    elif time == 'next' or time == 'following':\n        # Else day of week\n        num = HASHWEEKDAYS[dow]\n        return next_week_day(base_date, num)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a relative day to a date in the calendar", "response": "def date_from_relative_week_year(base_date, time, dow, ordinal=1):\n    \"\"\"\n    Converts relative day to time\n    Eg. this tuesday, last tuesday\n    \"\"\"\n    # If there is an ordinal (next 3 weeks) => return a start and end range\n    # Reset date to start of the day\n    relative_date = datetime(base_date.year, base_date.month, base_date.day)\n    ord = convert_string_to_number(ordinal)\n    if dow in year_variations:\n        if time == 'this' or time == 'coming':\n            return datetime(relative_date.year, 1, 1)\n        elif time == 'last' or time == 'previous':\n            return datetime(relative_date.year - 1, relative_date.month, 1)\n        elif time == 'next' or time == 'following':\n            return relative_date + timedelta(ord * 365)\n        elif time == 'end of the':\n            return datetime(relative_date.year, 12, 31)\n    elif dow in month_variations:\n        if time == 'this':\n            return datetime(relative_date.year, relative_date.month, relative_date.day)\n        elif time == 'last' or time == 'previous':\n            return datetime(relative_date.year, relative_date.month - 1, relative_date.day)\n        elif time == 'next' or time == 'following':\n            if relative_date.month + ord >= 12:\n                month = relative_date.month - 1 + ord\n                year = relative_date.year + month // 12\n                month = month % 12 + 1\n                day = min(relative_date.day, calendar.monthrange(year, month)[1])\n                return datetime(year, month, day)\n            else:\n                return datetime(relative_date.year, relative_date.month + ord, relative_date.day)\n        elif time == 'end of the':\n            return datetime(\n                relative_date.year,\n                relative_date.month,\n                calendar.monthrange(relative_date.year, relative_date.month)[1]\n            )\n    elif dow in week_variations:\n        if time == 'this':\n            return relative_date - timedelta(days=relative_date.weekday())\n        elif time == 'last' or time == 'previous':\n            return relative_date - timedelta(weeks=1)\n        elif time == 'next' or time == 'following':\n            return relative_date + timedelta(weeks=ord)\n        elif time == 'end of the':\n            day_of_week = base_date.weekday()\n            return day_of_week + timedelta(days=6 - relative_date.weekday())\n    elif dow in day_variations:\n        if time == 'this':\n            return relative_date\n        elif time == 'last' or time == 'previous':\n            return relative_date - timedelta(days=1)\n        elif time == 'next' or time == 'following':\n            return relative_date + timedelta(days=ord)\n        elif time == 'end of the':\n            return datetime(relative_date.year, relative_date.month, relative_date.day, 23, 59, 59)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts Day adverbs to dates", "response": "def date_from_adverb(base_date, name):\n    \"\"\"\n    Convert Day adverbs to dates\n    Tomorrow => Date\n    Today => Date\n    \"\"\"\n    # Reset date to start of the day\n    adverb_date = datetime(base_date.year, base_date.month, base_date.day)\n    if name == 'today' or name == 'tonite' or name == 'tonight':\n        return adverb_date.today()\n    elif name == 'yesterday':\n        return adverb_date - timedelta(days=1)\n    elif name == 'tomorrow' or name == 'tom':\n        return adverb_date + timedelta(days=1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding dates from duration", "response": "def date_from_duration(base_date, number_as_string, unit, duration, base_time=None):\n    \"\"\"\n    Find dates from duration\n    Eg: 20 days from now\n    Currently does not support strings like \"20 days from last monday\".\n    \"\"\"\n    # Check if query is `2 days before yesterday` or `day before yesterday`\n    if base_time is not None:\n        base_date = date_from_adverb(base_date, base_time)\n    num = convert_string_to_number(number_as_string)\n    if unit in day_variations:\n        args = {'days': num}\n    elif unit in minute_variations:\n        args = {'minutes': num}\n    elif unit in week_variations:\n        args = {'weeks': num}\n    elif unit in month_variations:\n        args = {'days': 365 * num / 12}\n    elif unit in year_variations:\n        args = {'years': num}\n    if duration == 'ago' or duration == 'before' or duration == 'earlier':\n        if 'years' in args:\n            return datetime(base_date.year - args['years'], base_date.month, base_date.day)\n        return base_date - timedelta(**args)\n    elif duration == 'after' or duration == 'later' or duration == 'from now':\n        if 'years' in args:\n            return datetime(base_date.year + args['years'], base_date.month, base_date.day)\n        return base_date + timedelta(**args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding next weekday in base_date", "response": "def next_week_day(base_date, weekday):\n    \"\"\"\n    Finds next weekday\n    \"\"\"\n    day_of_week = base_date.weekday()\n    end_of_this_week = base_date + timedelta(days=6 - day_of_week)\n    day = end_of_this_week + timedelta(days=1)\n    while day.weekday() != weekday:\n        day = day + timedelta(days=1)\n    return day"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract datetime objects from a string of text.", "response": "def datetime_parsing(text, base_date=datetime.now()):\n    \"\"\"\n    Extract datetime objects from a string of text.\n    \"\"\"\n    matches = []\n    found_array = []\n\n    # Find the position in the string\n    for expression, function in regex:\n        for match in expression.finditer(text):\n            matches.append((match.group(), function(match, base_date), match.span()))\n\n    # Wrap the matched text with TAG element to prevent nested selections\n    for match, value, spans in matches:\n        subn = re.subn(\n            '(?!<TAG[^>]*?>)' + match + '(?![^<]*?</TAG>)', '<TAG>' + match + '</TAG>', text\n        )\n        text = subn[0]\n        is_substituted = subn[1]\n        if is_substituted != 0:\n            found_array.append((match, value, spans))\n\n    # To preserve order of the match, sort based on the start position\n    return sorted(found_array, key=lambda match: match and match[2][0])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsearching for close matches to the input statement.", "response": "def search(self, input_statement, **additional_parameters):\n        \"\"\"\n        Search for close matches to the input. Confidence scores for\n        subsequent results will order of increasing value.\n\n        :param input_statement: A statement.\n        :type input_statement: chatterbot.conversation.Statement\n\n        :param **additional_parameters: Additional parameters to be passed\n            to the ``filter`` method of the storage adapter when searching.\n\n        :rtype: Generator yielding one closest matching statement at a time.\n        \"\"\"\n        self.chatbot.logger.info('Beginning search for close text match')\n\n        input_search_text = input_statement.search_text\n\n        if not input_statement.search_text:\n            self.chatbot.logger.warn(\n                'No value for search_text was available on the provided input'\n            )\n\n            input_search_text = self.chatbot.storage.tagger.get_bigram_pair_string(\n                input_statement.text\n            )\n\n        search_parameters = {\n            'search_text_contains': input_search_text,\n            'persona_not_startswith': 'bot:',\n            'page_size': self.search_page_size\n        }\n\n        if additional_parameters:\n            search_parameters.update(additional_parameters)\n\n        statement_list = self.chatbot.storage.filter(**search_parameters)\n\n        closest_match = Statement(text='')\n        closest_match.confidence = 0\n\n        self.chatbot.logger.info('Processing search results')\n\n        # Find the closest matching known statement\n        for statement in statement_list:\n            confidence = self.compare_statements(input_statement, statement)\n\n            if confidence > closest_match.confidence:\n                statement.confidence = confidence\n                closest_match = statement\n\n                self.chatbot.logger.info('Similar text found: {} {}'.format(\n                    closest_match.text, confidence\n                ))\n\n                yield closest_match"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a response from the chatbot and display it.", "response": "def get_response(self):\n        \"\"\"\n        Get a response from the chatbot and display it.\n        \"\"\"\n        user_input = self.usr_input.get()\n        self.usr_input.delete(0, tk.END)\n\n        response = self.chatbot.get_response(user_input)\n\n        self.conversation['state'] = 'normal'\n        self.conversation.insert(\n            tk.END, \"Human: \" + user_input + \"\\n\" + \"ChatBot: \" + str(response.text) + \"\\n\"\n        )\n        self.conversation['state'] = 'disabled'\n\n        time.sleep(0.5)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_tags(self, *tags):\n        for _tag in tags:\n            self.tags.get_or_create(name=_tag)", "response": "Add a list of strings to the statement as tags."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SvelteComponent(name, path):\n  if path[-3:] == \".js\":\n    js_path = path\n  elif path[-5:] == \".html\":\n    print(\"Trying to build svelte component from html...\")\n    js_path = build_svelte(path)\n  js_content = read(js_path, mode='r')\n  def inner(data):\n    id_str = js_id(name)\n    html = _template \\\n        .replace(\"$js\", js_content) \\\n        .replace(\"$name\", name) \\\n        .replace(\"$data\", json.dumps(data)) \\\n        .replace(\"$id\", id_str)\n    _display_html(html)\n  return inner", "response": "Display svelte components in iPython."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving object as json on CNS.", "response": "def save_json(object, handle, indent=2):\n    \"\"\"Save object as json on CNS.\"\"\"\n    obj_json = json.dumps(object, indent=indent, cls=NumpyJSONEncoder)\n    handle.write(obj_json)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave dict of numpy array as npz file.", "response": "def save_npz(object, handle):\n    \"\"\"Save dict of numpy array as npz file.\"\"\"\n    # there is a bug where savez doesn't actually accept a file handle.\n    log.warning(\"Saving npz files currently only works locally. :/\")\n    path = handle.name\n    handle.close()\n    if type(object) is dict:\n        np.savez(path, **object)\n    elif type(object) is list:\n        np.savez(path, *object)\n    else:\n        log.warning(\"Saving non dict or list as npz file, did you maybe want npy?\")\n        np.savez(path, object)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_img(object, handle, **kwargs):\n\n    if isinstance(object, np.ndarray):\n        normalized = _normalize_array(object)\n        object = PIL.Image.fromarray(normalized)\n\n    if isinstance(object, PIL.Image.Image):\n        object.save(handle, **kwargs)  # will infer format from handle's url ext.\n    else:\n        raise ValueError(\"Can only save_img for numpy arrays or PIL.Images!\")", "response": "Save numpy array as image file on CNS."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(thing, url_or_handle, **kwargs):\n    is_handle = hasattr(url_or_handle, \"write\") and hasattr(url_or_handle, \"name\")\n    if is_handle:\n        _, ext = os.path.splitext(url_or_handle.name)\n    else:\n        _, ext = os.path.splitext(url_or_handle)\n    if not ext:\n        raise RuntimeError(\"No extension in URL: \" + url_or_handle)\n\n    if ext in savers:\n        saver = savers[ext]\n        if is_handle:\n            saver(thing, url_or_handle, **kwargs)\n        else:\n            with write_handle(url_or_handle) as handle:\n                saver(thing, handle, **kwargs)\n    else:\n        saver_names = [(key, fn.__name__) for (key, fn) in savers.items()]\n        message = \"Unknown extension '{}', supports {}.\"\n        raise ValueError(message.format(ext, saver_names))", "response": "Save object to file on CNS."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef frustum(left, right, bottom, top, znear, zfar):\n  assert right != left\n  assert bottom != top\n  assert znear != zfar\n\n  M = np.zeros((4, 4), dtype=np.float32)\n  M[0, 0] = +2.0 * znear / (right - left)\n  M[2, 0] = (right + left) / (right - left)\n  M[1, 1] = +2.0 * znear / (top - bottom)\n  M[3, 1] = (top + bottom) / (top - bottom)\n  M[2, 2] = -(zfar + znear) / (zfar - znear)\n  M[3, 2] = -2.0 * znear * zfar / (zfar - znear)\n  M[2, 3] = -1.0\n  return M", "response": "Create view frustum matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing L2 norms alogn specified axes.", "response": "def anorm(x, axis=None, keepdims=False):\n  \"\"\"Compute L2 norms alogn specified axes.\"\"\"\n  return np.sqrt((x*x).sum(axis=axis, keepdims=keepdims))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookat(eye, target=[0, 0, 0], up=[0, 1, 0]):\n  eye = np.float32(eye)\n  forward = normalize(target - eye)\n  side = normalize(np.cross(forward, up))\n  up = np.cross(side, forward)\n  M = np.eye(4, dtype=np.float32)\n  R = M[:3, :3]\n  R[:] = [side, up, -forward]\n  M[:3, 3] = -R.dot(eye)\n  return M", "response": "Generate LookAt modelview matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample_view(min_dist, max_dist=None):\n  '''Sample random camera position.\n  \n  Sample origin directed camera position in given distance\n  range from the origin. ModelView matrix is returned.\n  '''\n  if max_dist is None:\n    max_dist = min_dist\n  dist = np.random.uniform(min_dist, max_dist)\n  eye = np.random.normal(size=3)\n  eye = normalize(eye)*dist\n  return lookat(eye)", "response": "Sample random camera position. A random camera position. A random camera position. A random origin directed camera position is returned."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_vertex_tuple(s):\n  vt = [0, 0, 0]\n  for i, c in enumerate(s.split('/')):\n    if c:\n      vt[i] = int(c)\n  return tuple(vt)", "response": "Parse vertex indices in '/' separated form ( like i j k..."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _unify_rows(a):\n  lens = np.fromiter(map(len, a), np.int32)\n  if not (lens[0] == lens).all():\n    out = np.zeros((len(a), lens.max()), np.float32)\n    for i, row in enumerate(a):\n      out[i, :lens[i]] = row\n  else:\n    out = np.float32(a)\n  return out", "response": "Unify lengths of each row of a."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads 3d mesh form. obj file.", "response": "def load_obj(fn):\n  \"\"\"Load 3d mesh form .obj' file.\n  \n  Args:\n    fn: Input file name or file-like object.\n    \n  Returns:\n    dictionary with the following keys (some of which may be missing):\n      position: np.float32, (n, 3) array, vertex positions\n      uv: np.float32, (n, 2) array, vertex uv coordinates\n      normal: np.float32, (n, 3) array, vertex uv normals\n      face: np.int32, (k*3,) traingular face indices\n  \"\"\"\n  position = [np.zeros(3, dtype=np.float32)]\n  normal = [np.zeros(3, dtype=np.float32)]\n  uv = [np.zeros(2, dtype=np.float32)]\n  \n  tuple2idx = OrderedDict()\n  trinagle_indices = []\n  \n  input_file = open(fn) if isinstance(fn, str) else fn\n  for line in input_file:\n    line = line.strip()\n    if not line or line[0] == '#':\n      continue\n    line = line.split(' ', 1)\n    tag = line[0]\n    if len(line) > 1:\n      line = line[1]\n    else:\n      line = ''\n    if tag == 'v':\n      position.append(np.fromstring(line, sep=' '))\n    elif tag == 'vt':\n      uv.append(np.fromstring(line, sep=' '))\n    elif tag == 'vn':\n      normal.append(np.fromstring(line, sep=' '))\n    elif tag == 'f':\n      output_face_indices = []\n      for chunk in line.split():\n        # tuple order: pos_idx, uv_idx, normal_idx\n        vt = _parse_vertex_tuple(chunk)\n        if vt not in tuple2idx:  # create a new output vertex?\n          tuple2idx[vt] = len(tuple2idx)\n        output_face_indices.append(tuple2idx[vt])\n      # generate face triangles\n      for i in range(1, len(output_face_indices)-1):\n        for vi in [0, i, i+1]:\n          trinagle_indices.append(output_face_indices[vi])\n  \n  outputs = {}\n  outputs['face'] = np.int32(trinagle_indices)\n  pos_idx, uv_idx, normal_idx = np.int32(list(tuple2idx)).T\n  if np.any(pos_idx):\n    outputs['position'] = _unify_rows(position)[pos_idx]\n  if np.any(uv_idx):\n    outputs['uv'] = _unify_rows(uv)[uv_idx]\n  if np.any(normal_idx):\n    outputs['normal'] = _unify_rows(normal)[normal_idx]\n  return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize_mesh(mesh):\n  '''Scale mesh to fit into -1..1 cube'''\n  mesh = dict(mesh)\n  pos = mesh['position'][:,:3].copy()\n  pos -= (pos.max(0)+pos.min(0)) / 2.0\n  pos /= np.abs(pos).max()\n  mesh['position'] = pos\n  return mesh", "response": "Scale mesh to fit into - 1.. 1 cube"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef activations(self):\n    if self._activations is None:\n      self._activations = _get_aligned_activations(self)\n    return self._activations", "response": "Loads sampled activations which requires network access."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nimport model GraphDef into current graph.", "response": "def import_graph(self, t_input=None, scope='import', forget_xy_shape=True):\n    \"\"\"Import model GraphDef into the current graph.\"\"\"\n    graph = tf.get_default_graph()\n    assert graph.unique_name(scope, False) == scope, (\n        'Scope \"%s\" already exists. Provide explicit scope names when '\n        'importing multiple instances of the model.') % scope\n    t_input, t_prep_input = self.create_input(t_input, forget_xy_shape)\n    tf.import_graph_def(\n        self.graph_def, {self.input_name: t_prep_input}, name=scope)\n    self.post_import(scope)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving outliers and scales layout to between [ 0 1 ).", "response": "def normalize_layout(layout, min_percentile=1, max_percentile=99, relative_margin=0.1):\n    \"\"\"Removes outliers and scales layout to between [0,1].\"\"\"\n\n    # compute percentiles\n    mins = np.percentile(layout, min_percentile, axis=(0))\n    maxs = np.percentile(layout, max_percentile, axis=(0))\n\n    # add margins\n    mins -= relative_margin * (maxs - mins)\n    maxs += relative_margin * (maxs - mins)\n\n    # `clip` broadcasts, `[None]`s added only for readability\n    clipped = np.clip(layout, mins, maxs)\n\n    # embed within [0,1] along both axes\n    clipped -= clipped.min(axis=0)\n    clipped /= clipped.max(axis=0)\n\n    return clipped"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nalign the UMAPs of the given activations into a single list of layouts.", "response": "def aligned_umap(activations, umap_options={}, normalize=True, verbose=False):\n    \"\"\"`activations` can be a list of ndarrays. In that case a list of layouts is returned.\"\"\"\n\n    umap_defaults = dict(\n        n_components=2, n_neighbors=50, min_dist=0.05, verbose=verbose, metric=\"cosine\"\n    )\n    umap_defaults.update(umap_options)\n\n    # if passed a list of activations, we combine them and later split the layouts\n    if type(activations) is list or type(activations) is tuple:\n        num_activation_groups = len(activations)\n        combined_activations = np.concatenate(activations)\n    else:\n        num_activation_groups = 1\n        combined_activations = activations\n    try:\n        layout = UMAP(**umap_defaults).fit_transform(combined_activations)\n    except (RecursionError, SystemError) as exception:\n        log.error(\"UMAP failed to fit these activations. We're not yet sure why this sometimes occurs.\")\n        raise ValueError(\"UMAP failed to fit activations: %s\", exception)\n\n    if normalize:\n        layout = normalize_layout(layout)\n\n    if num_activation_groups > 1:\n        layouts = np.split(layout, num_activation_groups, axis=0)\n        return layouts\n    else:\n        return layout"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders each cell in the tile and stitch it into a single image", "response": "def render_tile(cells, ti, tj, render, params, metadata, layout, summary):\n  \"\"\"\n    Render each cell in the tile and stitch it into a single image\n  \"\"\"\n  image_size = params[\"cell_size\"] * params[\"n_tile\"]\n  tile = Image.new(\"RGB\", (image_size, image_size), (255,255,255))\n  keys = cells.keys()\n  for i,key in enumerate(keys):\n    print(\"cell\", i+1, \"/\", len(keys), end='\\r')\n    cell_image = render(cells[key], params, metadata, layout, summary)\n    # stitch this rendering into the tile image\n    ci = key[0] % params[\"n_tile\"]\n    cj = key[1] % params[\"n_tile\"]\n    xmin = ci*params[\"cell_size\"]\n    ymin = cj*params[\"cell_size\"]\n    xmax = (ci+1)*params[\"cell_size\"]\n    ymax = (cj+1)*params[\"cell_size\"]\n\n    if params.get(\"scale_density\", False):\n      density = len(cells[key][\"gi\"])\n      # scale = density/summary[\"max_density\"]\n      scale = math.log(density)/(math.log(summary[\"max_density\"]) or 1)\n      owidth = xmax - xmin\n      width = int(round(owidth * scale))\n      if(width < 1):\n        width = 1\n      offsetL = int(round((owidth - width)/2))\n      offsetR = owidth - width - offsetL # handle odd numbers\n      # print(\"\\n\")\n      # print(\"width\", width, offsetL, offsetR)\n      box = [xmin + offsetL, ymin + offsetL, xmax - offsetR, ymax - offsetR]\n      resample = params.get(\"scale_type\", Image.NEAREST)\n      cell_image = cell_image.resize(size=(width,width), resample=resample)\n      # print(cell_image)\n    else:\n      box = [xmin, ymin, xmax, ymax]\n\n    # print(\"box\", box)\n    tile.paste(cell_image, box)\n  print(\"\\n\")\n  return tile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\naggregating the cells in a single tile.", "response": "def aggregate_tile(cells, ti, tj, aggregate, params, metadata, layout, summary):\n  \"\"\"\n    Call the user defined aggregation function on each cell and combine into a single json object\n  \"\"\"\n  tile = []\n  keys = cells.keys()\n  for i,key in enumerate(keys):\n    print(\"cell\", i+1, \"/\", len(keys), end='\\r')\n    cell_json = aggregate(cells[key], params, metadata, layout, summary)\n    tile.append({\"aggregate\":cell_json, \"i\":int(key[0]), \"j\":int(key[1])})\n  return tile"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_opengl_context(surface_size=(640, 480)):\n  egl_display = egl.eglGetDisplay(egl.EGL_DEFAULT_DISPLAY)\n\n  major, minor = egl.EGLint(), egl.EGLint()\n  egl.eglInitialize(egl_display, pointer(major), pointer(minor))\n\n  config_attribs = [\n      egl.EGL_SURFACE_TYPE, egl.EGL_PBUFFER_BIT, egl.EGL_BLUE_SIZE, 8,\n      egl.EGL_GREEN_SIZE, 8, egl.EGL_RED_SIZE, 8, egl.EGL_DEPTH_SIZE, 24,\n      egl.EGL_RENDERABLE_TYPE, egl.EGL_OPENGL_BIT, egl.EGL_NONE\n  ]\n  config_attribs = (egl.EGLint * len(config_attribs))(*config_attribs)\n\n  num_configs = egl.EGLint()\n  egl_cfg = egl.EGLConfig()\n  egl.eglChooseConfig(egl_display, config_attribs, pointer(egl_cfg), 1,\n                      pointer(num_configs))\n\n  width, height = surface_size\n  pbuffer_attribs = [\n      egl.EGL_WIDTH,\n      width,\n      egl.EGL_HEIGHT,\n      height,\n      egl.EGL_NONE,\n  ]\n  pbuffer_attribs = (egl.EGLint * len(pbuffer_attribs))(*pbuffer_attribs)\n  egl_surf = egl.eglCreatePbufferSurface(egl_display, egl_cfg, pbuffer_attribs)\n\n  egl.eglBindAPI(egl.EGL_OPENGL_API)\n\n  egl_context = egl.eglCreateContext(egl_display, egl_cfg, egl.EGL_NO_CONTEXT,\n                                     None)\n  egl.eglMakeCurrent(egl_display, egl_surf, egl_surf, egl_context)", "response": "Create an OpenGL context and make it current."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collapse_shape(shape, a, b):\n  shape = list(shape)\n  if a < 0:\n    n_pad = -a\n    pad = n_pad * [1]\n    return collapse_shape(pad + shape, a + n_pad, b + n_pad)\n  if b > len(shape):\n    n_pad = b - len(shape)\n    pad = n_pad * [1]\n    return collapse_shape(shape + pad, a, b)\n  return [product(shape[:a])] + shape[a:b] + [product(shape[b:])]", "response": "Collapse a tensor shape by a single dimension."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resize_bilinear_nd(t, target_shape):\n  shape = t.get_shape().as_list()\n  target_shape = list(target_shape)\n  assert len(shape) == len(target_shape)\n\n  # We progressively move through the shape, resizing dimensions...\n  d = 0\n  while d < len(shape):\n\n    # If we don't need to deal with the next dimesnion, step over it\n    if shape[d] == target_shape[d]:\n      d += 1\n      continue\n\n    # Otherwise, we'll resize the next two dimensions...\n    # If d+2 doesn't need to be resized, this will just be a null op for it\n    new_shape = shape[:]\n    new_shape[d : d+2] = target_shape[d : d+2]\n\n    # The helper collapse_shape() makes our shapes 4-dimensional with\n    # the two dimesnions we want to deal with in the middle.\n    shape_ = collapse_shape(shape, d, d+2)\n    new_shape_ = collapse_shape(new_shape, d, d+2)\n\n    # We can then reshape and use the 2d tf.image.resize_bilinear() on the\n    # inner two dimesions.\n    t_ = tf.reshape(t, shape_)\n    t_ = tf.image.resize_bilinear(t_, new_shape_[1:3])\n\n    # And then reshape back to our uncollapsed version, having finished resizing\n    # two more dimensions in our shape.\n    t = tf.reshape(t_, new_shape)\n    shape = new_shape\n    d += 2\n\n  return t", "response": "Bilinear resizes a tensor t to have shape target_shape."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload 100k activations of the specified layer sampled from iterating over ImageNet.", "response": "def get_aligned_activations(layer):\n    \"\"\"Downloads 100k activations of the specified layer sampled from iterating over\n    ImageNet. Activations of all layers where sampled at the same spatial positions for\n    each image, allowing the calculation of correlations.\"\"\"\n    activation_paths = [\n        PATH_TEMPLATE.format(\n            sanitize(layer.model_class.name), sanitize(layer.name), page\n        )\n        for page in range(NUMBER_OF_PAGES)\n    ]\n    activations = np.vstack([load(path) for path in activation_paths])\n    assert np.all(np.isfinite(activations))\n    return activations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the covariance matrix between the neurons of two layers.", "response": "def layer_covariance(layer1, layer2=None):\n    \"\"\"Computes the covariance matrix between the neurons of two layers. If only one\n    layer is passed, computes the symmetric covariance matrix of that layer.\"\"\"\n    layer2 = layer2 or layer1\n    act1, act2 = layer1.activations, layer2.activations\n    num_datapoints = act1.shape[0]  # cast to avoid numpy type promotion during division\n    return np.matmul(act1.T, act2) / float(num_datapoints)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npush activations from one model to another using prerecorded correlations", "response": "def push_activations(activations, from_layer, to_layer):\n    \"\"\"Push activations from one model to another using prerecorded correlations\"\"\"\n    inverse_covariance_matrix = layer_inverse_covariance(from_layer)\n    activations_decorrelated = np.dot(inverse_covariance_matrix, activations.T).T\n    covariance_matrix = layer_covariance(from_layer, to_layer)\n    activation_recorrelated = np.dot(activations_decorrelated, covariance_matrix)\n    return activation_recorrelated"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef multi_interpolation_basis(n_objectives=6, n_interp_steps=5, width=128,\n                              channels=3):\n  \"\"\"A paramaterization for interpolating between each pair of N objectives.\n\n  Sometimes you want to interpolate between optimizing a bunch of objectives,\n  in a paramaterization that encourages images to align.\n\n  Args:\n    n_objectives: number of objectives you want interpolate between\n    n_interp_steps: number of interpolation steps\n    width: width of intepolated images\n    channel\n\n  Returns:\n    A [n_objectives, n_objectives, n_interp_steps, width, width, channel]\n    shaped tensor, t, where the final [width, width, channel] should be\n    seen as images, such that the following properties hold:\n\n     t[a, b]    = t[b, a, ::-1]\n     t[a, i, 0] = t[a, j, 0] for all i, j\n     t[a, a, i] = t[a, a, j] for all i, j\n     t[a, b, i] = t[b, a, -i] for all i\n\n  \"\"\"\n  N, M, W, Ch = n_objectives, n_interp_steps, width, channels\n\n  const_term = sum([lowres_tensor([W, W, Ch], [W//k, W//k, Ch])\n                    for k in [1, 2, 4, 8]])\n  const_term = tf.reshape(const_term, [1, 1, 1, W, W, Ch])\n\n  example_interps = [\n      sum([lowres_tensor([M, W, W, Ch], [2, W//k, W//k, Ch])\n           for k in [1, 2, 4, 8]])\n      for _ in range(N)]\n\n  example_basis = []\n  for n in range(N):\n    col = []\n    for m in range(N):\n      interp = example_interps[n] + example_interps[m][::-1]\n      col.append(interp)\n    example_basis.append(col)\n\n  interp_basis = []\n  for n in range(N):\n    col = [interp_basis[m][N-n][::-1] for m in range(n)]\n    col.append(tf.zeros([M, W, W, 3]))\n    for m in range(n+1, N):\n      interp = sum([lowres_tensor([M, W, W, Ch], [M, W//k, W//k, Ch])\n                    for k in [1, 2]])\n      col.append(interp)\n    interp_basis.append(col)\n\n  basis = []\n  for n in range(N):\n    col_ex = tf.stack(example_basis[n])\n    col_in = tf.stack(interp_basis[n])\n    basis.append(col_ex + col_in)\n  basis = tf.stack(basis)\n\n  return basis + const_term", "response": "A paramaterization for interpolating between each pair of objectives."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters a gradient function to a random string.", "response": "def register_to_random_name(grad_f):\n  \"\"\"Register a gradient function to a random string.\n\n  In order to use a custom gradient in TensorFlow, it must be registered to a\n  string. This is both a hassle, and -- because only one function can every be\n  registered to a string -- annoying to iterate on in an interactive\n  environemnt.\n\n  This function registers a function to a unique random string of the form:\n\n    {FUNCTION_NAME}_{RANDOM_SALT}\n\n  And then returns the random string. This is a helper in creating more\n  convenient gradient overrides.\n\n  Args:\n    grad_f: gradient function to register. Should map (op, grad) -> grad(s)\n\n  Returns:\n    String that gradient function was registered to.\n  \"\"\"\n  grad_f_name = grad_f.__name__ + \"_\" + str(uuid.uuid4())\n  tf.RegisterGradient(grad_f_name)(grad_f)\n  return grad_f_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pixel_image(shape, sd=None, init_val=None):\n    if sd is not None and init_val is not None:\n        warnings.warn(\n            \"`pixel_image` received both an initial value and a sd argument. Ignoring sd in favor of the supplied initial value.\"\n        )\n\n    sd = sd or 0.01\n    init_val = init_val or np.random.normal(size=shape, scale=sd).astype(np.float32)\n    return tf.Variable(init_val)", "response": "A naive pixel - based image parameterization."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rfft2d_freqs(h, w):\n\n    fy = np.fft.fftfreq(h)[:, None]\n    # when we have an odd input dimension we need to keep one additional\n    # frequency and later cut off 1 pixel\n    if w % 2 == 1:\n        fx = np.fft.fftfreq(w)[: w // 2 + 2]\n    else:\n        fx = np.fft.fftfreq(w)[: w // 2 + 1]\n    return np.sqrt(fx * fx + fy * fy)", "response": "Computes 2D spectrum frequencies."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fft_image(shape, sd=None, decay_power=1):\n\n    sd = sd or 0.01\n    batch, h, w, ch = shape\n    freqs = rfft2d_freqs(h, w)\n    init_val_size = (2, ch) + freqs.shape\n\n    images = []\n    for _ in range(batch):\n        # Create a random variable holding the actual 2D fourier coefficients\n        init_val = np.random.normal(size=init_val_size, scale=sd).astype(np.float32)\n        spectrum_real_imag_t = tf.Variable(init_val)\n        spectrum_t = tf.complex(spectrum_real_imag_t[0], spectrum_real_imag_t[1])\n\n        # Scale the spectrum. First normalize energy, then scale by the square-root\n        # of the number of pixels to get a unitary transformation.\n        # This allows to use similar leanring rates to pixel-wise optimisation.\n        scale = 1.0 / np.maximum(freqs, 1.0 / max(w, h)) ** decay_power\n        scale *= np.sqrt(w * h)\n        scaled_spectrum_t = scale * spectrum_t\n\n        # convert complex scaled spectrum to shape (h, w, ch) image tensor\n        # needs to transpose because irfft2d returns channels first\n        image_t = tf.transpose(tf.spectral.irfft2d(scaled_spectrum_t), (1, 2, 0))\n\n        # in case of odd spatial input dimensions we need to crop\n        image_t = image_t[:h, :w, :ch]\n\n        images.append(image_t)\n\n    batched_image_t = tf.stack(images) / 4.0  # TODO: is that a magic constant?\n    return batched_image_t", "response": "An image paramaterization using 2D Fourier coefficients."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef laplacian_pyramid_image(shape, n_levels=4, sd=None):\n    batch_dims = shape[:-3]\n    w, h, ch = shape[-3:]\n    pyramid = 0\n    for n in range(n_levels):\n        k = 2 ** n\n        pyramid += lowres_tensor(shape, batch_dims + (w // k, h // k, ch), sd=sd)\n    return pyramid", "response": "Simple laplacian pyramid paramaterization of an image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bilinearly_sampled_image(texture, uv):\n    h, w = tf.unstack(tf.shape(texture)[:2])\n    u, v = tf.split(uv, 2, axis=-1)\n    v = 1.0 - v  # vertical flip to match GL convention\n    u, v = u * tf.to_float(w) - 0.5, v * tf.to_float(h) - 0.5\n    u0, u1 = tf.floor(u), tf.ceil(u)\n    v0, v1 = tf.floor(v), tf.ceil(v)\n    uf, vf = u - u0, v - v0\n    u0, u1, v0, v1 = map(tf.to_int32, [u0, u1, v0, v1])\n\n    def sample(u, v):\n        vu = tf.concat([v % h, u % w], axis=-1)\n        return tf.gather_nd(texture, vu)\n\n    s00, s01 = sample(u0, v0), sample(u0, v1)\n    s10, s11 = sample(u1, v0), sample(u1, v1)\n    s0 = s00 * (1.0 - vf) + s01 * vf\n    s1 = s10 * (1.0 - vf) + s11 * vf\n    s = s0 * (1.0 - uf) + s1 * uf\n    return s", "response": "Build a bilinear texture sampling graph."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _linear_decorelate_color(t):\n  # check that inner dimension is 3?\n  t_flat = tf.reshape(t, [-1, 3])\n  color_correlation_normalized = color_correlation_svd_sqrt / max_norm_svd_sqrt\n  t_flat = tf.matmul(t_flat, color_correlation_normalized.T)\n  t = tf.reshape(t_flat, tf.shape(t))\n  return t", "response": "Multiply input by sqrt of emperical (ImageNet) color correlation matrix.\n  \n  If you interpret t's innermost dimension as describing colors in a\n  decorrelated version of the color space (which is a very natural way to\n  describe colors -- see discussion in Feature Visualization article) the way\n  to map back to normal colors is multiply the square root of your color\n  correlations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_valid_rgb(t, decorrelate=False, sigmoid=True):\n  if decorrelate:\n    t = _linear_decorelate_color(t)\n  if decorrelate and not sigmoid:\n    t += color_mean\n  if sigmoid:\n    return tf.nn.sigmoid(t)\n  else:\n    return constrain_L_inf(2*t-1)/2 + 0.5", "response": "Transform inner dimension of t to valid rgb colors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _populate_inception_bottlenecks(scope):\n  graph = tf.get_default_graph()\n  for op in graph.get_operations():\n    if op.name.startswith(scope+'/') and 'Concat' in op.type:\n      name = op.name.split('/')[1]\n      pre_relus = []\n      for tower in op.inputs[1:]:\n        if tower.op.type == 'Relu':\n          tower = tower.op.inputs[0]\n        pre_relus.append(tower)\n      concat_name = scope + '/' + name + '_pre_relu'\n      _ = tf.concat(pre_relus, -1, name=concat_name)", "response": "Add Inception bottlenecks and their pre - Relu versions to the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wrap_objective(f, *args, **kwds):\n  objective_func = f(*args, **kwds)\n  objective_name = f.__name__\n  args_str = \" [\" + \", \".join([_make_arg_str(arg) for arg in args]) + \"]\"\n  description = objective_name.title() + args_str\n  return Objective(objective_func, objective_name, description)", "response": "Decorator for creating Objective factories."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvisualizes a single neuron of a single channel.", "response": "def neuron(layer_name, channel_n, x=None, y=None, batch=None):\n  \"\"\"Visualize a single neuron of a single channel.\n\n  Defaults to the center neuron. When width and height are even numbers, we\n  choose the neuron in the bottom right of the center 2x2 neurons.\n\n  Odd width & height:               Even width & height:\n\n  +---+---+---+                     +---+---+---+---+\n  |   |   |   |                     |   |   |   |   |\n  +---+---+---+                     +---+---+---+---+\n  |   | X |   |                     |   |   |   |   |\n  +---+---+---+                     +---+---+---+---+\n  |   |   |   |                     |   |   | X |   |\n  +---+---+---+                     +---+---+---+---+\n                                    |   |   |   |   |\n                                    +---+---+---+---+\n  \"\"\"\n  def inner(T):\n    layer = T(layer_name)\n    shape = tf.shape(layer)\n    x_ = shape[1] // 2 if x is None else x\n    y_ = shape[2] // 2 if y is None else y\n\n    if batch is None:\n      return layer[:, x_, y_, channel_n]\n    else:\n      return layer[batch, x_, y_, channel_n]\n  return inner"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef channel(layer, n_channel, batch=None):\n  if batch is None:\n    return lambda T: tf.reduce_mean(T(layer)[..., n_channel])\n  else:\n    return lambda T: tf.reduce_mean(T(layer)[batch, ..., n_channel])", "response": "Visualize a single channel."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvisualizes a single x y position along the given direction", "response": "def direction_neuron(layer_name, vec, batch=None, x=None, y=None, cossim_pow=0):\n  \"\"\"Visualize a single (x, y) position along the given direction\"\"\"\n  def inner(T):\n    layer = T(layer_name)\n    shape = tf.shape(layer)\n    x_ = shape[1] // 2 if x is None else x\n    y_ = shape[2] // 2 if y is None else y\n    if batch is None:\n      return _dot_cossim(layer[:, x_, y_], vec[None], cossim_pow=cossim_pow)\n    else:\n      return _dot_cossim(layer[batch, x_, y_], vec, cossim_pow=cossim_pow)\n  return inner"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef direction_cossim(layer, vec, batch=None):\n  def inner(T):\n    act_mags = tf.sqrt(tf.reduce_sum(T(layer)**2, -1, keepdims=True))\n    vec_mag = tf.sqrt(tf.reduce_sum(vec**2))\n    mags = act_mags * vec_mag\n    if batch is None:\n      return tf.reduce_mean(T(layer) * vec.reshape([1, 1, 1, -1]) / mags)\n    else:\n      return tf.reduce_mean(T(layer)[batch] * vec.reshape([1, 1, -1]) / mags)\n  return inner", "response": "Visualize a direction ( cossine similarity )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef L2(layer=\"input\", constant=0, epsilon=1e-6, batch=None):\n  if batch is None:\n    return lambda T: tf.sqrt(epsilon + tf.reduce_sum((T(layer) - constant) ** 2))\n  else:\n    return lambda T: tf.sqrt(epsilon + tf.reduce_sum((T(layer)[batch] - constant) ** 2))", "response": "L2 norm of layer. Generally used as penalty."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef blur_input_each_step():\n  def inner(T):\n    t_input = T(\"input\")\n    t_input_blurred = tf.stop_gradient(_tf_blur(t_input))\n    return 0.5*tf.reduce_sum((t_input - t_input_blurred)**2)\n  return inner", "response": "Minimizing this objective is equivelant to blurring input each step."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef channel_interpolate(layer1, n_channel1, layer2, n_channel2):\n  def inner(T):\n    batch_n = T(layer1).get_shape().as_list()[0]\n    arr1 = T(layer1)[..., n_channel1]\n    arr2 = T(layer2)[..., n_channel2]\n    weights = (np.arange(batch_n)/float(batch_n-1))\n    S = 0\n    for n in range(batch_n):\n      S += (1-weights[n]) * tf.reduce_mean(arr1[n])\n      S += weights[n] * tf.reduce_mean(arr2[n])\n    return S\n  return inner", "response": "Interpolate between layer1 n_channel1 and layer2 n_channel2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nenhancing the boundaries of an image to have less variation and of color C.", "response": "def penalize_boundary_complexity(shp, w=20, mask=None, C=0.5):\n  \"\"\"Encourage the boundaries of an image to have less variation and of color C.\n\n  Args:\n    shp: shape of T(\"input\") because this may not be known.\n    w: width of boundary to penalize. Ignored if mask is set.\n    mask: mask describing what area should be penalized.\n\n  Returns:\n    Objective.\n  \"\"\"\n  def inner(T):\n    arr = T(\"input\")\n\n    # print shp\n    if mask is None:\n      mask_ = np.ones(shp)\n      mask_[:, w:-w, w:-w] = 0\n    else:\n      mask_ = mask\n\n    blur = _tf_blur(arr, w=5)\n    diffs = (blur-arr)**2\n    diffs += 0.8*(arr-C)**2\n\n    return -tf.reduce_sum(diffs*mask_)\n  return inner"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencourage neighboring images to be similar. When visualizing the interpolation between two objectives, it's often desireable to encourage analagous boejcts to be drawn in the same position, to make them more comparable. This term penalizes L2 distance between neighboring images, as evaluated at layer. In general, we find this most effective if used with a paramaterization that shares across the batch. (In fact, that works quite well by iteself, so this function may just be obselete.) Args: layer: layer to penalize at. decay_ratio: how much to decay penalty as images move apart in batch. Returns: Objective.", "response": "def alignment(layer, decay_ratio=2):\n  \"\"\"Encourage neighboring images to be similar.\n\n  When visualizing the interpolation between two objectives, it's often\n  desireable to encourage analagous boejcts to be drawn in the same position,\n  to make them more comparable.\n\n  This term penalizes L2 distance between neighboring images, as evaluated at\n  layer.\n\n  In general, we find this most effective if used with a paramaterization that\n  shares across the batch. (In fact, that works quite well by iteself, so this\n  function may just be obselete.)\n\n  Args:\n    layer: layer to penalize at.\n    decay_ratio: how much to decay penalty as images move apart in batch.\n\n  Returns:\n    Objective.\n  \"\"\"\n  def inner(T):\n    batch_n = T(layer).get_shape().as_list()[0]\n    arr = T(layer)\n    accum = 0\n    for d in [1, 2, 3, 4]:\n      for i in range(batch_n - d):\n        a, b = i, i+d\n        arr1, arr2 = arr[a], arr[b]\n        accum += tf.reduce_mean((arr1-arr2)**2) / decay_ratio**float(d)\n    return -accum\n  return inner"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencourages diversity between each batch element.", "response": "def diversity(layer):\n  \"\"\"Encourage diversity between each batch element.\n\n  A neural net feature often responds to multiple things, but naive feature\n  visualization often only shows us one. If you optimize a batch of images,\n  this objective will encourage them all to be different.\n\n  In particular, it caculuates the correlation matrix of activations at layer\n  for each image, and then penalizes cossine similarity between them. This is\n  very similar to ideas in style transfer, except we're *penalizing* style\n  similarity instead of encouraging it.\n\n  Args:\n    layer: layer to evaluate activation correlations on.\n\n  Returns:\n    Objective.\n  \"\"\"\n  def inner(T):\n    layer_t = T(layer)\n    batch_n, _, _, channels = layer_t.get_shape().as_list()\n\n    flattened = tf.reshape(layer_t, [batch_n, -1, channels])\n    grams = tf.matmul(flattened, flattened, transpose_a=True)\n    grams = tf.nn.l2_normalize(grams, axis=[1,2], epsilon=1e-10)\n\n    return sum([ sum([ tf.reduce_sum(grams[i]*grams[j])\n                      for j in range(batch_n) if j != i])\n                for i in range(batch_n)]) / batch_n\n  return inner"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef input_diff(orig_img):\n  def inner(T):\n    diff = T(\"input\") - orig_img\n    return tf.sqrt(tf.reduce_mean(diff**2))\n  return inner", "response": "Average L2 difference between optimized image and orig_img."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlikes channel but for softmax layers.", "response": "def class_logit(layer, label):\n  \"\"\"Like channel, but for softmax layers.\n\n  Args:\n    layer: A layer name string.\n    label: Either a string (refering to a label in model.labels) or an int\n      label position.\n\n  Returns:\n    Objective maximizing a logit.\n  \"\"\"\n  def inner(T):\n    if isinstance(label, int):\n      class_n = label\n    else:\n      class_n = T(\"labels\").index(label)\n    logits = T(layer)\n    logit = tf.reduce_sum(logits[:, class_n])\n    return logit\n  return inner"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef as_objective(obj):\n  if isinstance(obj, Objective):\n    return obj\n  elif callable(obj):\n    return obj\n  elif isinstance(obj, str):\n    layer, n = obj.split(\":\")\n    layer, n = layer.strip(), int(n)\n    return channel(layer, n)", "response": "Convert obj into Objective class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _constrain_L2_grad(op, grad):\n  inp = op.inputs[0]\n  inp_norm = tf.norm(inp)\n  unit_inp = inp / inp_norm\n\n  grad_projection = dot(unit_inp, grad)\n  parallel_grad = unit_inp * grad_projection\n\n  is_in_ball = tf.less_equal(inp_norm, 1)\n  is_pointed_inward = tf.less(grad_projection, 0)\n  allow_grad = tf.logical_or(is_in_ball, is_pointed_inward)\n  clip_grad = tf.logical_not(allow_grad)\n\n  clipped_grad = tf.cond(clip_grad, lambda: grad - parallel_grad, lambda: grad)\n\n  return clipped_grad", "response": "Gradient for constrained optimization on an L2 unit ball."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render_vis(model, objective_f, param_f=None, optimizer=None,\n               transforms=None, thresholds=(512,), print_objectives=None,\n               verbose=True, relu_gradient_override=True, use_fixed_seed=False):\n  \"\"\"Flexible optimization-base feature vis.\n\n  There's a lot of ways one might wish to customize otpimization-based\n  feature visualization. It's hard to create an abstraction that stands up\n  to all the things one might wish to try.\n\n  This function probably can't do *everything* you want, but it's much more\n  flexible than a naive attempt. The basic abstraction is to split the problem\n  into several parts. Consider the rguments:\n\n  Args:\n    model: The model to be visualized, from Alex' modelzoo.\n    objective_f: The objective our visualization maximizes.\n      See the objectives module for more details.\n    param_f: Paramaterization of the image we're optimizing.\n      See the paramaterization module for more details.\n      Defaults to a naively paramaterized [1, 128, 128, 3] image.\n    optimizer: Optimizer to optimize with. Either tf.train.Optimizer instance,\n      or a function from (graph, sess) to such an instance.\n      Defaults to Adam with lr .05.\n    transforms: A list of stochastic transformations that get composed,\n      which our visualization should robustly activate the network against.\n      See the transform module for more details.\n      Defaults to [transform.jitter(8)].\n    thresholds: A list of numbers of optimization steps, at which we should\n      save (and display if verbose=True) the visualization.\n    print_objectives: A list of objectives separate from those being optimized,\n      whose values get logged during the optimization.\n    verbose: Should we display the visualization when we hit a threshold?\n      This should only be used in IPython.\n    relu_gradient_override: Whether to use the gradient override scheme\n      described in lucid/misc/redirected_relu_grad.py. On by default!\n    use_fixed_seed: Seed the RNG with a fixed value so results are reproducible.\n      Off by default. As of tf 1.8 this does not work as intended, see:\n      https://github.com/tensorflow/tensorflow/issues/9171\n  Returns:\n    2D array of optimization results containing of evaluations of supplied\n    param_f snapshotted at specified thresholds. Usually that will mean one or\n    multiple channel visualizations stacked on top of each other.\n  \"\"\"\n\n  with tf.Graph().as_default() as graph, tf.Session() as sess:\n\n    if use_fixed_seed:  # does not mean results are reproducible, see Args doc\n      tf.set_random_seed(0)\n\n    T = make_vis_T(model, objective_f, param_f, optimizer, transforms,\n                   relu_gradient_override)\n    print_objective_func = make_print_objective_func(print_objectives, T)\n    loss, vis_op, t_image = T(\"loss\"), T(\"vis_op\"), T(\"input\")\n    tf.global_variables_initializer().run()\n\n    images = []\n    try:\n      for i in range(max(thresholds)+1):\n        loss_, _ = sess.run([loss, vis_op])\n        if i in thresholds:\n          vis = t_image.eval()\n          images.append(vis)\n          if verbose:\n            print(i, loss_)\n            print_objective_func(sess)\n            show(np.hstack(vis))\n    except KeyboardInterrupt:\n      log.warning(\"Interrupted optimization at step {:d}.\".format(i+1))\n      vis = t_image.eval()\n      show(np.hstack(vis))\n\n    return images", "response": "Flexible optimization - base feature vis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_vis_T(model, objective_f, param_f=None, optimizer=None,\n               transforms=None, relu_gradient_override=False):\n  \"\"\"Even more flexible optimization-base feature vis.\n\n  This function is the inner core of render_vis(), and can be used\n  when render_vis() isn't flexible enough. Unfortunately, it's a bit more\n  tedious to use:\n\n  >  with tf.Graph().as_default() as graph, tf.Session() as sess:\n  >\n  >    T = make_vis_T(model, \"mixed4a_pre_relu:0\")\n  >    tf.initialize_all_variables().run()\n  >\n  >    for i in range(10):\n  >      T(\"vis_op\").run()\n  >      showarray(T(\"input\").eval()[0])\n\n  This approach allows more control over how the visualizaiton is displayed\n  as it renders. It also allows a lot more flexibility in constructing\n  objectives / params because the session is already in scope.\n\n\n  Args:\n    model: The model to be visualized, from Alex' modelzoo.\n    objective_f: The objective our visualization maximizes.\n      See the objectives module for more details.\n    param_f: Paramaterization of the image we're optimizing.\n      See the paramaterization module for more details.\n      Defaults to a naively paramaterized [1, 128, 128, 3] image.\n    optimizer: Optimizer to optimize with. Either tf.train.Optimizer instance,\n      or a function from (graph, sess) to such an instance.\n      Defaults to Adam with lr .05.\n    transforms: A list of stochastic transformations that get composed,\n      which our visualization should robustly activate the network against.\n      See the transform module for more details.\n      Defaults to [transform.jitter(8)].\n\n  Returns:\n    A function T, which allows access to:\n      * T(\"vis_op\") -- the operation for to optimize the visualization\n      * T(\"input\") -- the visualization itself\n      * T(\"loss\") -- the loss for the visualization\n      * T(layer) -- any layer inside the network\n  \"\"\"\n\n  # pylint: disable=unused-variable\n  t_image = make_t_image(param_f)\n  objective_f = objectives.as_objective(objective_f)\n  transform_f = make_transform_f(transforms)\n  optimizer = make_optimizer(optimizer, [])\n\n  global_step = tf.train.get_or_create_global_step()\n  init_global_step = tf.variables_initializer([global_step])\n  init_global_step.run()\n\n  if relu_gradient_override:\n    with gradient_override_map({'Relu': redirected_relu_grad,\n                                'Relu6': redirected_relu6_grad}):\n      T = import_model(model, transform_f(t_image), t_image)\n  else:\n    T = import_model(model, transform_f(t_image), t_image)\n  loss = objective_f(T)\n\n\n  vis_op = optimizer.minimize(-loss, global_step=global_step)\n\n  local_vars = locals()\n  # pylint: enable=unused-variable\n\n  def T2(name):\n    if name in local_vars:\n      return local_vars[name]\n    else: return T(name)\n\n  return T2", "response": "This function creates a function T which allows access to the visualization of the objectives."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grid(metadata, layout, params):\n  x = layout[\"x\"]\n  y = layout[\"y\"]\n  x_min = np.min(x)\n  x_max = np.max(x)\n  y_min = np.min(y)\n  y_max = np.max(y)\n\n  # this creates the grid\n  bins = np.linspace(x_min, x_max, params[\"n_layer\"] - 1)\n  xd = np.digitize(x, bins)\n  bins = np.linspace(y_min, y_max, params[\"n_layer\"] - 1)\n  yd = np.digitize(y, bins)\n\n  # the number of tiles is the number of cells divided by the number of cells in each tile\n  num_tiles = int(params[\"n_layer\"]/params[\"n_tile\"])\n  print(\"num tiles\", num_tiles)\n  # we will save the tiles in an array indexed by the tile coordinates\n  tiles = {}\n  for ti in range(num_tiles):\n    for tj in range(num_tiles):\n      tiles[(ti,tj)] = {\n        \"x\": [],\n        \"y\": [],\n        \"ci\": [], # cell-space x coordinate\n        \"cj\": [], # cell-space y coordinate\n        \"gi\": [], # global index\n      }\n\n  for i,xi in enumerate(x):\n    if(i % 1000 == 0 or i+1 == len(x)):\n      print(\"point\", i+1, \"/\", len(x), end=\"\\r\")\n    # layout-space coordinates\n    yi = y[i]\n    # grid-space cell coordinates\n    ci = xd[i]\n    cj = yd[i]\n    # tile coordinate\n    ti = math.floor(ci / params[\"n_tile\"])\n    tj = math.floor(cj / params[\"n_tile\"])\n\n    # TODO: don't append a point if it doesn't match a filter function provided in params\n    filter = params.get(\"filter\", lambda i,metadata: True)\n    if(filter(i, metadata=metadata)):\n      tiles[(ti,tj)][\"x\"].append(xi)\n      tiles[(ti,tj)][\"y\"].append(yi)\n      tiles[(ti,tj)][\"ci\"].append(ci)\n      tiles[(ti,tj)][\"cj\"].append(cj)\n      tiles[(ti,tj)][\"gi\"].append(i)\n    \n  return tiles", "response": "This function creates a grid of the grids."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_grid_local(tiles, params):\n  # TODO: this isn't being used right now, will need to be\n  # ported to gfile if we want to keep it\n  for ti,tj,tile in enumerate_tiles(tiles):\n    filename = \"{directory}/{name}/tile_{n_layer}_{n_tile}_{ti}_{tj}\".format(ti=ti, tj=tj, **params) #directory=directory, name=name, n_layer=n_layer, n_tile=n_tile, \n    # write out the tile as a npz\n    print(\"saving\", filename + \".npz\")\n    np.savez_compressed(filename + \".npz\", **tile)\n    # write out the tile as a csv\n    print(\"saving\", filename + \".csv\")\n    df = pd.DataFrame(tile)\n    df.to_csv(filename + \".csv\", index=False)", "response": "Write a file for each tile in the grid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_img(handle, target_dtype=np.float32, size=None, **kwargs):\n\n    image_pil = PIL.Image.open(handle, **kwargs)\n\n    # resize the image to the requested size, if one was specified\n    if size is not None:\n        if len(size) > 2:\n            size = size[:2]\n            log.warning(\"`_load_img()` received size: {}, trimming to first two dims!\".format(size))\n        image_pil = image_pil.resize(size, resample=PIL.Image.LANCZOS)\n\n    image_array = np.asarray(image_pil)\n\n    # remove alpha channel if it contains no information\n    # if image_array.shape[-1] > 3 and 'A' not in image_pil.mode:\n    # image_array = image_array[..., :-1]\n\n    image_dtype = image_array.dtype\n    image_max_value = np.iinfo(image_dtype).max  # ...for uint8 that's 255, etc.\n\n    # using np.divide should avoid an extra copy compared to doing division first\n    ndimage = np.divide(image_array, image_max_value, dtype=target_dtype)\n\n    rank = len(ndimage.shape)\n    if rank == 3:\n        return ndimage\n    elif rank == 2:\n        return np.repeat(np.expand_dims(ndimage, axis=2), 3, axis=2)\n    else:\n        message = \"Loaded image has more dimensions than expected: {}\".format(rank)\n        raise NotImplementedError(message)", "response": "Load image file as numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_text(handle, split=False, encoding=\"utf-8\"):\n    string = handle.read().decode(encoding)\n    return string.splitlines() if split else string", "response": "Load and decode a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a graphdef from a binary proto file.", "response": "def _load_graphdef_protobuf(handle, **kwargs):\n    \"\"\"Load GraphDef from a binary proto file.\"\"\"\n    # as_graph_def\n    graph_def = tf.GraphDef.FromString(handle.read())\n\n    # check if this is a lucid-saved model\n    # metadata = modelzoo.util.extract_metadata(graph_def)\n    # if metadata is not None:\n    #   url = handle.name\n    #   return modelzoo.vision_base.Model.load_from_metadata(url, metadata)\n\n    # else return a normal graph_def\n    return graph_def"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(url_or_handle, cache=None, **kwargs):\n\n    ext = get_extension(url_or_handle)\n    try:\n        loader = loaders[ext.lower()]\n        message = \"Using inferred loader '%s' due to passed file extension '%s'.\"\n        log.debug(message, loader.__name__[6:], ext)\n        return load_using_loader(url_or_handle, loader, cache, **kwargs)\n\n    except KeyError:\n\n        log.warning(\"Unknown extension '%s', attempting to load as image.\", ext)\n        try:\n            with read_handle(url_or_handle, cache=cache) as handle:\n                result = _load_img(handle)\n        except Exception as e:\n            message = \"Could not load resource %s as image. Supported extensions: %s\"\n            log.error(message, url_or_handle, list(loaders))\n            raise RuntimeError(message.format(url_or_handle, list(loaders)))\n        else:\n            log.info(\"Unknown extension '%s' successfully loaded as image.\", ext)\n            return result", "response": "Load a file into a new object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef crop_or_pad_to(height, width):\n    def inner(t_image):\n        return tf.image.resize_image_with_crop_or_pad(t_image, height, width)\n    return inner", "response": "Ensures the specified spatial shape by either padding or cropping."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnormalizing an arbitrary rank - 3 NumPy array and returns a PIL. Image .", "response": "def _normalize_array(array, domain=(0, 1)):\n  \"\"\"Given an arbitrary rank-3 NumPy array, produce one representing an image.\n\n  This ensures the resulting array has a dtype of uint8 and a domain of 0-255.\n\n  Args:\n    array: NumPy array representing the image\n    domain: expected range of values in array,\n      defaults to (0, 1), if explicitly set to None will use the array's\n      own range of values and normalize them.\n\n  Returns:\n    normalized PIL.Image\n  \"\"\"\n  # first copy the input so we're never mutating the user's data\n  array = np.array(array)\n  # squeeze helps both with batch=1 and B/W and PIL's mode inference\n  array = np.squeeze(array)\n  assert len(array.shape) <= 3\n  assert np.issubdtype(array.dtype, np.number)\n  assert not np.isnan(array).any()\n\n  low, high = np.min(array), np.max(array)\n  if domain is None:\n    message = \"No domain specified, normalizing from measured (~%.2f, ~%.2f)\"\n    log.debug(message, low, high)\n    domain = (low, high)\n\n  # clip values if domain was specified and array contains values outside of it\n  if low < domain[0] or high > domain[1]:\n    message = \"Clipping domain from (~{:.2f}, ~{:.2f}) to (~{:.2f}, ~{:.2f}).\"\n    log.info(message.format(low, high, domain[0], domain[1]))\n    array = array.clip(*domain)\n\n  min_value, max_value = np.iinfo(np.uint8).min, np.iinfo(np.uint8).max  # 0, 255\n  # convert signed to unsigned if needed\n  if np.issubdtype(array.dtype, np.inexact):\n    offset = domain[0]\n    if offset != 0:\n      array -= offset\n      log.debug(\"Converting inexact array by subtracting -%.2f.\", offset)\n    scalar = max_value / (domain[1] - domain[0])\n    if scalar != 1:\n      array *= scalar\n      log.debug(\"Converting inexact array by scaling by %.2f.\", scalar)\n\n  return array.clip(min_value, max_value).astype(np.uint8)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _serialize_normalized_array(array, fmt='png', quality=70):\n  dtype = array.dtype\n  assert np.issubdtype(dtype, np.unsignedinteger)\n  assert np.max(array) <= np.iinfo(dtype).max\n  assert array.shape[-1] > 1  # array dims must have been squeezed\n\n  image = PIL.Image.fromarray(array)\n  image_bytes = BytesIO()\n  image.save(image_bytes, fmt, quality=quality)\n  # TODO: Python 3 could save a copy here by using `getbuffer()` instead.\n  image_data = image_bytes.getvalue()\n  return image_data", "response": "Given a normalized array returns byte representation of image encoding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing an arbitrary rank - 3 NumPy array into a BytesIO buffer.", "response": "def serialize_array(array, domain=(0, 1), fmt='png', quality=70):\n  \"\"\"Given an arbitrary rank-3 NumPy array,\n  returns the byte representation of the encoded image.\n\n  Args:\n    array: NumPy array of dtype uint8 and range 0 to 255\n    domain: expected range of values in array, see `_normalize_array()`\n    fmt: string describing desired file format, defaults to 'png'\n    quality: specifies compression quality from 0 to 100 for lossy formats\n\n  Returns:\n    image data as BytesIO buffer\n  \"\"\"\n  normalized = _normalize_array(array, domain=domain)\n  return _serialize_normalized_array(normalized, fmt=fmt, quality=quality)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef array_to_jsbuffer(array):\n  if array.ndim != 1:\n    raise TypeError('Only 1d arrays can be converted JS TypedArray.')\n  if array.dtype.name not in JS_ARRAY_TYPES:\n    raise TypeError('Array dtype not supported by JS TypedArray.')\n  js_type_name = array.dtype.name.capitalize() + 'Array'\n  data_base64 = base64.b64encode(array.tobytes()).decode('ascii')\n  code = \"\"\"\n    (function() {\n      const data = atob(\"%s\");\n      const buf = new Uint8Array(data.length);\n      for (var i=0; i<data.length; ++i) {\n        buf[i] = data.charCodeAt(i);\n      }\n      var array_type = %s;\n      if (array_type == Uint8Array) {\n        return buf;\n      }\n      return new array_type(buf.buffer);\n    })()\n  \"\"\" % (data_base64, js_type_name)\n  return code", "response": "Serialize 1d NumPy array to JS TypedArray."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_style(self, input_feeds):\n    sess = tf.get_default_session()\n    computed = sess.run(self.input_grams, input_feeds)\n    for v, g in zip(self.target_vars, computed):\n      v.load(g)", "response": "Set target style variables."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _image_url(array, fmt='png', mode=\"data\", quality=90, domain=None):\n  supported_modes = (\"data\")\n  if mode not in supported_modes:\n    message = \"Unsupported mode '%s', should be one of '%s'.\"\n    raise ValueError(message, mode, supported_modes)\n\n  image_data = serialize_array(array, fmt=fmt, quality=quality)\n  base64_byte_string = base64.b64encode(image_data).decode('ascii')\n  return \"data:image/\" + fmt.upper() + \";base64,\" + base64_byte_string", "response": "Create a data URL representing an image from a PIL. Image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef image(array, domain=None, width=None, format='png', **kwargs):\n\n  image_data = serialize_array(array, fmt=format, domain=domain)\n  image = IPython.display.Image(data=image_data, format=format, width=width)\n  IPython.display.display(image)", "response": "Display an image.\n\n  Args:\n    array: NumPy array representing the image\n    fmt: Image format e.g. png, jpeg\n    domain: Domain of pixel values, inferred from min & max values if None\n    w: width of output image, scaled using nearest neighbor interpolation.\n      size unchanged if None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay a list of images with optional labels.", "response": "def images(arrays, labels=None, domain=None, w=None):\n  \"\"\"Display a list of images with optional labels.\n\n  Args:\n    arrays: A list of NumPy arrays representing images\n    labels: A list of strings to label each image.\n      Defaults to show index if None\n    domain: Domain of pixel values, inferred from min & max values if None\n    w: width of output image, scaled using nearest neighbor interpolation.\n      size unchanged if None\n  \"\"\"\n\n  s = '<div style=\"display: flex; flex-direction: row;\">'\n  for i, array in enumerate(arrays):\n    url = _image_url(array)\n    label = labels[i] if labels is not None else i\n    s += \"\"\"<div style=\"margin-right:10px;\">\n              {label}<br/>\n              <img src=\"{url}\" style=\"margin-top:4px;\">\n            </div>\"\"\".format(label=label, url=url)\n  s += \"</div>\"\n  _display_html(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays a nupmy array without having to specify what it represents.", "response": "def show(thing, domain=(0, 1), **kwargs):\n  \"\"\"Display a nupmy array without having to specify what it represents.\n\n  This module will attempt to infer how to display your tensor based on its\n  rank, shape and dtype. rank 4 tensors will be displayed as image grids, rank\n  2 and 3 tensors as images.\n  \"\"\"\n  if isinstance(thing, np.ndarray):\n    rank = len(thing.shape)\n    if rank == 4:\n      log.debug(\"Show is assuming rank 4 tensor to be a list of images.\")\n      images(thing, domain=domain, **kwargs)\n    elif rank in (2, 3):\n      log.debug(\"Show is assuming rank 2 or 3 tensor to be an image.\")\n      image(thing, domain=domain, **kwargs)\n    else:\n      log.warning(\"Show only supports numpy arrays of rank 2-4. Using repr().\")\n      print(repr(thing))\n  elif isinstance(thing, (list, tuple)):\n    log.debug(\"Show is assuming list or tuple to be a collection of images.\")\n    images(thing, domain=domain, **kwargs)\n  else:\n    log.warning(\"Show only supports numpy arrays so far. Using repr().\")\n    print(repr(thing))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _strip_consts(graph_def, max_const_size=32):\n    strip_def = tf.GraphDef()\n    for n0 in graph_def.node:\n        n = strip_def.node.add()\n        n.MergeFrom(n0)\n        if n.op == 'Const':\n            tensor = n.attr['value'].tensor\n            size = len(tensor.tensor_content)\n            if size > max_const_size:\n                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n    return strip_def", "response": "Strip large constant values from graph_def."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef graph(graph_def, max_const_size=32):\n    if hasattr(graph_def, 'as_graph_def'):\n        graph_def = graph_def.as_graph_def()\n    strip_def = _strip_consts(graph_def, max_const_size=max_const_size)\n    code = \"\"\"\n        <script>\n          function load() {{\n            document.getElementById(\"{id}\").pbtxt = {data};\n          }}\n        </script>\n        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n        <div style=\"height:600px\">\n          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n        </div>\n    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n\n    iframe = \"\"\"\n        <iframe seamless style=\"width:100%; height:620px; border: none;\" srcdoc=\"{}\"></iframe>\n    \"\"\".format(code.replace('\"', '&quot;'))\n    _display_html(iframe)", "response": "Visualize a TensorFlow graph."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef composite(\n    background_image,\n    foreground_image,\n    foreground_width_ratio=0.25,\n    foreground_position=(0.0, 0.0),\n):\n    \"\"\"Takes two images and composites them.\"\"\"\n\n    if foreground_width_ratio <= 0:\n        return background_image\n\n    composite = background_image.copy()\n    width = int(foreground_width_ratio * background_image.shape[1])\n    foreground_resized = resize(foreground_image, width)\n    size = foreground_resized.shape\n\n    x = int(foreground_position[1] * (background_image.shape[1] - size[1]))\n    y = int(foreground_position[0] * (background_image.shape[0] - size[0]))\n\n    # TODO: warn if resulting coordinates are out of bounds?\n    composite[y : y + size[0], x : x + size[1]] = foreground_resized\n\n    return composite", "response": "Takes two images and composites them."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a tensor paramaterized by a lower resolution tensor.", "response": "def lowres_tensor(shape, underlying_shape, offset=None, sd=None):\n    \"\"\"Produces a tensor paramaterized by a interpolated lower resolution tensor.\n\n  This is like what is done in a laplacian pyramid, but a bit more general. It\n  can be a powerful way to describe images.\n\n  Args:\n    shape: desired shape of resulting tensor\n    underlying_shape: shape of the tensor being resized into final tensor\n    offset: Describes how to offset the interpolated vector (like phase in a\n      Fourier transform). If None, apply no offset. If a scalar, apply the same\n      offset to each dimension; if a list use each entry for each dimension.\n      If a int, offset by that much. If False, do not offset. If True, offset by\n      half the ratio between shape and underlying shape (analagous to 90\n      degrees).\n    sd: Standard deviation of initial tensor variable.\n\n  Returns:\n    A tensor paramaterized by a lower resolution tensorflow variable.\n  \"\"\"\n    sd = sd or 0.01\n    init_val = sd * np.random.randn(*underlying_shape).astype(\"float32\")\n    underlying_t = tf.Variable(init_val)\n    t = resize_bilinear_nd(underlying_t, shape)\n    if offset is not None:\n        # Deal with non-list offset\n        if not isinstance(offset, list):\n            offset = len(shape) * [offset]\n\n        # Deal with the non-int offset entries\n        for n in range(len(offset)):\n            if offset[n] is True:\n                offset[n] = shape[n] / underlying_shape[n] / 2\n            if offset[n] is False:\n                offset[n] = 0\n            offset[n] = int(offset[n])\n\n        # Actually apply offset by padding and then croping off the excess.\n        padding = [(pad, 0) for pad in offset]\n        t = tf.pad(t, padding, \"SYMMETRIC\")\n        begin = len(shape) * [0]\n        t = tf.slice(t, begin, shape)\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an intractive TensorFlow session.", "response": "def create_session(target='', timeout_sec=10):\n  '''Create an intractive TensorFlow session.\n\n  Helper function that creates TF session that uses growing GPU memory\n  allocation and opration timeout. 'allow_growth' flag prevents TF\n  from allocating the whole GPU memory an once, which is useful\n  when having multiple python sessions sharing the same GPU.\n  '''\n  graph = tf.Graph()\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth = True\n  config.operation_timeout_in_ms = int(timeout_sec*1000)\n  return tf.InteractiveSession(target=target, graph=graph, config=config)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading from any URL.", "response": "def read(url, encoding=None, cache=None, mode=\"rb\"):\n    \"\"\"Read from any URL.\n\n    Internally differentiates between URLs supported by tf.gfile, such as URLs\n    with the Google Cloud Storage scheme ('gs://...') or local paths, and HTTP\n    URLs. This way users don't need to know about the underlying fetch mechanism.\n\n    Args:\n        url: a URL including scheme or a local path\n        mode: mode in which to open the file. defaults to binary ('rb')\n        encoding: if specified, encoding that should be used to decode read data\n          if mode is specified to be text ('r'), this defaults to 'utf-8'.\n        cache: whether to attempt caching the resource. Defaults to True only if\n          the given URL specifies a remote resource.\n    Returns:\n        All bytes form the specified resource, or a decoded string of those.\n    \"\"\"\n    with read_handle(url, cache, mode=mode) as handle:\n        data = handle.read()\n\n    if encoding:\n        data = data.decode(encoding)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_handle(url, cache=None, mode=\"rb\"):\n    scheme = urlparse(url).scheme\n\n    if cache == 'purge':\n        _purge_cached(url)\n        cache = None\n\n    if _is_remote(scheme) and cache is None:\n        cache = True\n        log.debug(\"Cache not specified, enabling because resource is remote.\")\n\n    if cache:\n        handle = _read_and_cache(url, mode=mode)\n    else:\n        if scheme in (\"http\", \"https\"):\n            handle = _handle_web_url(url, mode=mode)\n        elif scheme in (\"gs\"):\n            handle = _handle_gfile(url, mode=mode)\n        else:\n            handle = open(url, mode=mode)\n\n    yield handle\n    handle.close()", "response": "Read from any URL with a file handle."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the path that remote_url would be cached at locally.", "response": "def local_cache_path(remote_url):\n    \"\"\"Returns the path that remote_url would be cached at locally.\"\"\"\n    local_name = RESERVED_PATH_CHARS.sub(\"_\", remote_url)\n    return os.path.join(gettempdir(), local_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a model instance based on the given name.", "response": "def get_model(name):\n    \"\"\"Returns a model instance such as `model = vision_models.InceptionV1()`.\n    In the future may be expanded to filter by additional criteria, such as\n    architecture, dataset, and task the model was trained on.\n    Args:\n      name: The name of the model, as given by the class name in vision_models.\n    Returns:\n      An instantiated Model class with the requested model. Users still need to\n      manually `load_graphdef` on the return value, and manually import this\n      model's graph into their current graph.\n    Raises:\n      ValueError: If network `name` is not recognized.\n    \"\"\"\n    if name not in models_map:\n        candidates = filter(lambda key: name in key, models_map.keys())\n        candidates_string = \", \".join(candidates)\n        raise ValueError(\n            \"No network named {}. Did you mean one of {}?\".format(\n                name, candidates_string\n            )\n        )\n\n    model_class = models_map[name]\n    model = model_class()\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef activation_atlas(\n    model,\n    layer,\n    grid_size=10,\n    icon_size=96,\n    number_activations=NUMBER_OF_AVAILABLE_SAMPLES,\n    icon_batch_size=32,\n    verbose=False,\n):\n    \"\"\"Renders an Activation Atlas of the given model's layer.\"\"\"\n\n    activations = layer.activations[:number_activations, ...]\n    layout, = aligned_umap(activations, verbose=verbose)\n    directions, coordinates, _ = bin_laid_out_activations(\n        layout, activations, grid_size\n    )\n    icons = []\n    for directions_batch in chunked(directions, icon_batch_size):\n        icon_batch, losses = render_icons(\n            directions_batch, model, layer=layer.name, size=icon_size, num_attempts=1\n        )\n        icons += icon_batch\n    canvas = make_canvas(icons, coordinates, grid_size)\n\n    return canvas", "response": "Renders an Activation Atlas of the given model s layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenders two aligned Activation Atlases of the given models and layers.", "response": "def aligned_activation_atlas(\n    model1,\n    layer1,\n    model2,\n    layer2,\n    grid_size=10,\n    icon_size=80,\n    num_steps=1024,\n    whiten_layers=True,\n    number_activations=NUMBER_OF_AVAILABLE_SAMPLES,\n    icon_batch_size=32,\n    verbose=False,\n):\n    \"\"\"Renders two aligned Activation Atlases of the given models' layers.\n\n    Returns a generator of the two atlasses, and a nested generator for intermediate\n    atlasses while they're being rendered.\n    \"\"\"\n    combined_activations = _combine_activations(\n        layer1, layer2, number_activations=number_activations\n    )\n    layouts = aligned_umap(combined_activations, verbose=verbose)\n\n    for model, layer, layout in zip((model1, model2), (layer1, layer2), layouts):\n        directions, coordinates, densities = bin_laid_out_activations(\n            layout, layer.activations[:number_activations, ...], grid_size, threshold=10\n        )\n\n        def _progressive_canvas_iterator():\n            icons = []\n            for directions_batch in chunked(directions, icon_batch_size):\n                icon_batch, losses = render_icons(\n                    directions_batch,\n                    model,\n                    alpha=False,\n                    layer=layer.name,\n                    size=icon_size,\n                    n_steps=num_steps,\n                    S=layer_inverse_covariance(layer) if whiten_layers else None,\n                )\n                icons += icon_batch\n                yield make_canvas(icons, coordinates, grid_size)\n\n        yield _progressive_canvas_iterator()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _combine_activations(\n    layer1,\n    layer2,\n    activations1=None,\n    activations2=None,\n    mode=ActivationTranslation.BIDIRECTIONAL,\n    number_activations=NUMBER_OF_AVAILABLE_SAMPLES,\n):\n    \"\"\"Given two layers, combines their activations according to mode.\n\n    ActivationTranslation.ONE_TO_TWO:\n      Translate activations of layer1 into the space of layer2, and return a tuple of\n      the translated activations and the original layer2 activations.\n\n    ActivationTranslation.BIDIRECTIONAL:\n      Translate activations of layer1 into the space of layer2, activations of layer2\n      into the space of layer 1, concatenate them along their channels, and returns a\n      tuple of the concatenated activations for each layer.\n    \"\"\"\n    activations1 = activations1 or layer1.activations[:number_activations, ...]\n    activations2 = activations2 or layer2.activations[:number_activations, ...]\n\n    if mode is ActivationTranslation.ONE_TO_TWO:\n\n        acts_1_to_2 = push_activations(activations1, layer1, layer2)\n        return acts_1_to_2, activations2\n\n    elif mode is ActivationTranslation.BIDIRECTIONAL:\n\n        acts_1_to_2 = push_activations(activations1, layer1, layer2)\n        acts_2_to_1 = push_activations(activations2, layer2, layer1)\n\n        activations_model1 = np.concatenate((activations1, acts_1_to_2), axis=1)\n        activations_model2 = np.concatenate((acts_2_to_1, activations2), axis=1)\n\n        return activations_model1, activations_model2", "response": "Given two layers combine their activations into one and two activations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bin_laid_out_activations(layout, activations, grid_size, threshold=5):\n\n    assert layout.shape[0] == activations.shape[0]\n\n    # calculate which grid cells each activation's layout position falls into\n    # first bin stays empty because nothing should be < 0, so we add an extra bin\n    bins = np.linspace(0, 1, num=grid_size + 1)\n    bins[-1] = np.inf  # last bin should include all higher values\n    indices = np.digitize(layout, bins) - 1  # subtract 1 to account for empty first bin\n\n    # because of thresholding we may need to return a variable number of means\n    means, coordinates, counts = [], [], []\n\n    # iterate over all grid cell coordinates to compute their average directions\n    grid_coordinates = np.indices((grid_size, grid_size)).transpose().reshape(-1, 2)\n    for xy_coordinates in grid_coordinates:\n        mask = np.equal(xy_coordinates, indices).all(axis=1)\n        count = np.count_nonzero(mask)\n        if count > threshold:\n            counts.append(count)\n            coordinates.append(xy_coordinates)\n            mean = np.average(activations[mask], axis=0)\n            means.append(mean)\n\n    assert len(means) == len(coordinates) == len(counts)\n    if len(coordinates) == 0:\n        raise RuntimeError(\"Binning activations led to 0 cells containing activations!\")\n\n    return means, coordinates, counts", "response": "Given a layout and activations overlays a grid on the layout and returns a variety of means coordinates counts and counts of activations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading GraphDef from a binary proto file.", "response": "def load_graphdef(model_url, reset_device=True):\n  \"\"\"Load GraphDef from a binary proto file.\"\"\"\n  graph_def = load(model_url)\n\n  if reset_device:\n    for n in graph_def.node:\n      n.device = \"\"\n\n  return graph_def"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef forget_xy(t):\n  shape = (t.shape[0], None, None, t.shape[3])\n  return tf.placeholder_with_default(t, shape)", "response": "Forget the x y of a 4d tensor in shape inference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning frozen and simplified graph_def of default graph.", "response": "def frozen_default_graph_def(input_node_names, output_node_names):\n  \"\"\"Return frozen and simplified graph_def of default graph.\"\"\"\n\n  sess = tf.get_default_session()\n  input_graph_def = tf.get_default_graph().as_graph_def()\n\n  pruned_graph = tf.graph_util.remove_training_nodes(\n      input_graph_def, protected_nodes=(output_node_names + input_node_names)\n  )\n  pruned_graph = tf.graph_util.extract_sub_graph(pruned_graph, output_node_names)\n\n  # remove explicit device assignments\n  for node in pruned_graph.node:\n      node.device = \"\"\n\n  all_variable_names = [v.op.name for v in tf.global_variables()]\n  output_graph_def = tf.graph_util.convert_variables_to_constants(\n      sess=sess,\n      input_graph_def=pruned_graph,\n      output_node_names=output_node_names,\n      variable_names_whitelist=all_variable_names,\n  )\n\n  return output_graph_def"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nembeds meta data as a string constant in a TF graph.", "response": "def infuse_metadata(graph_def, info):\n  \"\"\"Embed meta data as a string constant in a TF graph.\n\n  This function takes info, converts it into json, and embeds\n  it in graph_def as a constant op called `__lucid_metadata_json`.\n  \"\"\"\n  temp_graph = tf.Graph()\n  with temp_graph.as_default():\n    tf.constant(json.dumps(info, cls=NumpyJSONEncoder), name=metadata_node_name)\n  meta_node = temp_graph.as_graph_def().node[0]\n  graph_def.node.extend([meta_node])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nattempt to extract meta data hidden in graph_def. Returns None if not found.", "response": "def extract_metadata(graph_def):\n  \"\"\"Attempt to extract meta data hidden in graph_def.\n\n  Looks for a `__lucid_metadata_json` constant string op.\n  If present, extract it's content and convert it from json to python.\n  If not, returns None.\n  \"\"\"\n  meta_matches = [n for n in graph_def.node if n.name==metadata_node_name]\n  if meta_matches:\n    assert len(meta_matches) == 1, \"found more than 1 lucid metadata node!\"\n    meta_tensor = meta_matches[0].attr['value'].tensor\n    return json.loads(meta_tensor.string_val[0])\n  else:\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef neighborhood(self, node, degree=4):\n    assert self.by_name[node.name] == node\n    already_visited = frontier = set([node.name])\n    for _ in range(degree):\n      neighbor_names = set()\n      for node_name in frontier:\n        outgoing = set(n.name for n in self.by_input[node_name])\n        incoming = set(self.by_name[node_name].input)\n        neighbor_names |= incoming | outgoing\n      frontier = neighbor_names - already_visited\n      already_visited |= neighbor_names\n    return [self.by_name[name] for name in already_visited]", "response": "Returns a list of nodes that are connected to the given node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _retrieve_messages_before_strategy(self, retrieve):\n        before = self.before.id if self.before else None\n        data = await self.logs_from(self.channel.id, retrieve, before=before)\n        if len(data):\n            if self.limit is not None:\n                self.limit -= retrieve\n            self.before = Object(id=int(data[-1]['id']))\n        return data", "response": "Retrieve messages using before parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving messages using after parameter.", "response": "async def _retrieve_messages_after_strategy(self, retrieve):\n        \"\"\"Retrieve messages using after parameter.\"\"\"\n        after = self.after.id if self.after else None\n        data = await self.logs_from(self.channel.id, retrieve, after=after)\n        if len(data):\n            if self.limit is not None:\n                self.limit -= retrieve\n            self.after = Object(id=int(data[0]['id']))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _retrieve_messages_around_strategy(self, retrieve):\n        if self.around:\n            around = self.around.id if self.around else None\n            data = await self.logs_from(self.channel.id, retrieve, around=around)\n            self.around = None\n            return data\n        return []", "response": "Retrieve messages using around parameter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve guilds using before parameter.", "response": "async def _retrieve_guilds_before_strategy(self, retrieve):\n        \"\"\"Retrieve guilds using before parameter.\"\"\"\n        before = self.before.id if self.before else None\n        data = await self.get_guilds(retrieve, before=before)\n        if len(data):\n            if self.limit is not None:\n                self.limit -= retrieve\n            self.before = Object(id=int(data[-1]['id']))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def _retrieve_guilds_after_strategy(self, retrieve):\n        after = self.after.id if self.after else None\n        data = await self.get_guilds(retrieve, after=after)\n        if len(data):\n            if self.limit is not None:\n                self.limit -= retrieve\n            self.after = Object(id=int(data[0]['id']))\n        return data", "response": "Retrieve guilds using after parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_hsv(cls, h, s, v):\n        rgb = colorsys.hsv_to_rgb(h, s, v)\n        return cls.from_rgb(*(int(x * 255) for x in rgb))", "response": "Constructs a : class : Colour from an HSV tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the cog s description typically the cleaned docstring.", "response": "def description(self):\n        \"\"\":class:`str`: Returns the cog's description, typically the cleaned docstring.\"\"\"\n        try:\n            return self.__cog_cleaned_doc__\n        except AttributeError:\n            self.__cog_cleaned_doc__ = cleaned = inspect.getdoc(self)\n            return cleaned"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of ( name function ) listener pairs that are defined in this cog.", "response": "def get_listeners(self):\n        \"\"\"Returns a :class:`list` of (name, function) listener pairs that are defined in this cog.\"\"\"\n        return [(name, getattr(self, method_name)) for name, method_name in self.__cog_listeners__]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_footer(self, *, text=EmptyEmbed, icon_url=EmptyEmbed):\n\n        self._footer = {}\n        if text is not EmptyEmbed:\n            self._footer['text'] = str(text)\n\n        if icon_url is not EmptyEmbed:\n            self._footer['icon_url'] = str(icon_url)\n\n        return self", "response": "Sets the footer for the embed content."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_author(self, *, name, url=EmptyEmbed, icon_url=EmptyEmbed):\n\n        self._author = {\n            'name': str(name)\n        }\n\n        if url is not EmptyEmbed:\n            self._author['url'] = str(url)\n\n        if icon_url is not EmptyEmbed:\n            self._author['icon_url'] = str(icon_url)\n\n        return self", "response": "Sets the author for the embed content."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_field(self, *, name, value, inline=True):\n\n        field = {\n            'inline': inline,\n            'name': str(name),\n            'value': str(value)\n        }\n\n        try:\n            self._fields.append(field)\n        except AttributeError:\n            self._fields = [field]\n\n        return self", "response": "Adds a field to the embed object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmodifying a field to the embed object.", "response": "def set_field_at(self, index, *, name, value, inline=True):\n        \"\"\"Modifies a field to the embed object.\n\n        The index must point to a valid pre-existing field.\n\n        This function returns the class instance to allow for fluent-style\n        chaining.\n\n        Parameters\n        -----------\n        index: :class:`int`\n            The index of the field to modify.\n        name: :class:`str`\n            The name of the field.\n        value: :class:`str`\n            The value of the field.\n        inline: :class:`bool`\n            Whether the field should be displayed inline.\n\n        Raises\n        -------\n        IndexError\n            An invalid index was provided.\n        \"\"\"\n\n        try:\n            field = self._fields[index]\n        except (TypeError, IndexError, AttributeError):\n            raise IndexError('field index out of range')\n\n        field['name'] = str(name)\n        field['value'] = str(value)\n        field['inline'] = inline\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_dict(self):\n\n        # add in the raw data into the dict\n        result = {\n            key[1:]: getattr(self, key)\n            for key in self.__slots__\n            if key[0] == '_' and hasattr(self, key)\n        }\n\n        # deal with basic convenience wrappers\n\n        try:\n            colour = result.pop('colour')\n        except KeyError:\n            pass\n        else:\n            if colour:\n                result['color'] = colour.value\n\n        try:\n            timestamp = result.pop('timestamp')\n        except KeyError:\n            pass\n        else:\n            if timestamp:\n                if timestamp.tzinfo:\n                    result['timestamp'] = timestamp.astimezone(tz=datetime.timezone.utc).isoformat()\n                else:\n                    result['timestamp'] = timestamp.replace(tzinfo=datetime.timezone.utc).isoformat()\n\n        # add in the non raw attribute ones\n        if self.type:\n            result['type'] = self.type\n\n        if self.description:\n            result['description'] = self.description\n\n        if self.url:\n            result['url'] = self.url\n\n        if self.title:\n            result['title'] = self.title\n\n        return result", "response": "Converts this embed object into a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a friendly URL version of the user s avatar.", "response": "def avatar_url_as(self, *, format=None, static_format='webp', size=1024):\n        \"\"\"Returns a friendly URL version of the avatar the user has.\n\n        If the user does not have a traditional avatar, their default\n        avatar URL is returned instead.\n\n        The format must be one of 'webp', 'jpeg', 'jpg', 'png' or 'gif', and\n        'gif' is only valid for animated avatars. The size must be a power of 2\n        between 16 and 1024.\n\n        Parameters\n        -----------\n        format: Optional[:class:`str`]\n            The format to attempt to convert the avatar to.\n            If the format is ``None``, then it is automatically\n            detected into either 'gif' or static_format depending on the\n            avatar being animated or not.\n        static_format: Optional[:class:`str`]\n            Format to attempt to convert only non-animated avatars to.\n            Defaults to 'webp'\n        size: :class:`int`\n            The size of the image to display.\n\n        Raises\n        ------\n        InvalidArgument\n            Bad image format passed to ``format`` or ``static_format``, or\n            invalid ``size``.\n\n        Returns\n        --------\n        :class:`Asset`\n            The resulting CDN asset.\n        \"\"\"\n        return Asset._from_avatar(self._state, self, format=format, static_format=static_format, size=size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mentioned_in(self, message):\n\n        if message.mention_everyone:\n            return True\n\n        for user in message.mentions:\n            if user.id == self.id:\n                return True\n\n        return False", "response": "Checks if the user is mentioned in the specified message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of User objects that the user is friends with.", "response": "def friends(self):\n        r\"\"\"Returns a :class:`list` of :class:`User`\\s that the user is friends with.\n\n        .. note::\n\n            This only applies to non-bot accounts.\n        \"\"\"\n        return [r.user for r in self._relationships.values() if r.type is RelationshipType.friend]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef blocked(self):\n        return [r.user for r in self._relationships.values() if r.type is RelationshipType.blocked]", "response": "Returns a list of User s that the user has blocked."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def edit(self, **fields):\n\n        try:\n            avatar_bytes = fields['avatar']\n        except KeyError:\n            avatar = self.avatar\n        else:\n            if avatar_bytes is not None:\n                avatar = _bytes_to_base64_data(avatar_bytes)\n            else:\n                avatar = None\n\n        not_bot_account = not self.bot\n        password = fields.get('password')\n        if not_bot_account and password is None:\n            raise ClientException('Password is required for non-bot accounts.')\n\n        args = {\n            'password': password,\n            'username': fields.get('username', self.name),\n            'avatar': avatar\n        }\n\n        if not_bot_account:\n            args['email'] = fields.get('email', self.email)\n\n            if 'new_password' in fields:\n                args['new_password'] = fields['new_password']\n\n        http = self._state.http\n\n        if 'house' in fields:\n            house = fields['house']\n            if house is None:\n                await http.leave_hypesquad_house()\n            elif not isinstance(house, HypeSquadHouse):\n                raise ClientException('`house` parameter was not a HypeSquadHouse')\n            else:\n                value = house.value\n\n            await http.change_hypesquad_house(value)\n\n        data = await http.edit_profile(**args)\n        if not_bot_account:\n            self.email = data['email']\n            try:\n                http._token(data['token'], bot=False)\n            except KeyError:\n                pass\n\n        self._update(data)", "response": "Edit the current profile of the client."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new group direct message with the given recipients.", "response": "async def create_group(self, *recipients):\n        r\"\"\"|coro|\n\n        Creates a group direct message with the recipients\n        provided. These recipients must be have a relationship\n        of type :attr:`RelationshipType.friend`.\n\n        .. note::\n\n            This only applies to non-bot accounts.\n\n        Parameters\n        -----------\n        \\*recipients: :class:`User`\n            An argument :class:`list` of :class:`User` to have in\n            your group.\n\n        Raises\n        -------\n        HTTPException\n            Failed to create the group direct message.\n        ClientException\n            Attempted to create a group with only one recipient.\n            This does not include yourself.\n\n        Returns\n        -------\n        :class:`GroupChannel`\n            The new group channel.\n        \"\"\"\n\n        from .channel import GroupChannel\n\n        if len(recipients) < 2:\n            raise ClientException('You must have two or more recipients to create a group.')\n\n        users = [str(u.id) for u in recipients]\n        data = await self._state.http.start_group(self.id, users)\n        return GroupChannel(me=self, data=data, state=self._state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def edit_settings(self, **kwargs):\n        payload = {}\n\n        content_filter = kwargs.pop('explicit_content_filter', None)\n        if content_filter:\n            payload.update({'explicit_content_filter': content_filter.value})\n\n        friend_flags = kwargs.pop('friend_source_flags', None)\n        if friend_flags:\n            dicts = [{}, {'mutual_guilds': True}, {'mutual_friends': True},\n            {'mutual_guilds': True, 'mutual_friends': True}, {'all': True}]\n            payload.update({'friend_source_flags': dicts[friend_flags.value]})\n\n        guild_positions = kwargs.pop('guild_positions', None)\n        if guild_positions:\n            guild_positions = [str(x.id) for x in guild_positions]\n            payload.update({'guild_positions': guild_positions})\n\n        restricted_guilds = kwargs.pop('restricted_guilds', None)\n        if restricted_guilds:\n            restricted_guilds = [str(x.id) for x in restricted_guilds]\n            payload.update({'restricted_guilds': restricted_guilds})\n\n        status = kwargs.pop('status', None)\n        if status:\n            payload.update({'status': status.value})\n\n        theme = kwargs.pop('theme', None)\n        if theme:\n            payload.update({'theme': theme.value})\n\n        payload.update(kwargs)\n\n        data = await self._state.http.edit_settings(**payload)\n        return data", "response": "Edit the settings of a client user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a DMChannel with this user.", "response": "async def create_dm(self):\n        \"\"\"Creates a :class:`DMChannel` with this user.\n\n        This should be rarely called, as this is done transparently for most\n        people.\n        \"\"\"\n        found = self.dm_channel\n        if found is not None:\n            return found\n\n        state = self._state\n        data = await state.http.start_private_message(self.id)\n        return state.add_dm_channel(data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def mutual_friends(self):\n        state = self._state\n        mutuals = await state.http.get_mutual_friends(self.id)\n        return [User(state=state, data=friend) for friend in mutuals]", "response": "Return a list of mutual friends of this user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the user is a friend.", "response": "def is_friend(self):\n        \"\"\":class:`bool`: Checks if the user is your friend.\n\n        .. note::\n\n            This only applies to non-bot accounts.\n        \"\"\"\n        r = self.relationship\n        if r is None:\n            return False\n        return r.type is RelationshipType.friend"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_blocked(self):\n        r = self.relationship\n        if r is None:\n            return False\n        return r.type is RelationshipType.blocked", "response": "Checks if the user is blocked."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def send_friend_request(self):\n        await self._state.http.send_friend_request(username=self.name, discriminator=self.discriminator)", "response": "Send a friend request to the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the user s profile.", "response": "async def profile(self):\n        \"\"\"|coro|\n\n        Gets the user's profile.\n\n        .. note::\n\n            This only applies to non-bot accounts.\n\n        Raises\n        -------\n        Forbidden\n            Not allowed to fetch profiles.\n        HTTPException\n            Fetching the profile failed.\n\n        Returns\n        --------\n        :class:`Profile`\n            The profile of the user.\n        \"\"\"\n\n        state = self._state\n        data = await state.http.get_user_profile(self.id)\n\n        def transform(d):\n            return state._get_guild(int(d['id']))\n\n        since = data.get('premium_since')\n        mutual_guilds = list(filter(None, map(transform, data.get('mutual_guilds', []))))\n        return Profile(flags=data['user'].get('flags', 0),\n                       premium_since=parse_time(since),\n                       mutual_guilds=mutual_guilds,\n                       user=self,\n                       connected_accounts=data['connected_accounts'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a numeric snowflake pretending to be created at the given date.", "response": "def time_snowflake(datetime_obj, high=False):\n    \"\"\"Returns a numeric snowflake pretending to be created at the given date.\n\n    When using as the lower end of a range, use time_snowflake(high=False) - 1 to be inclusive, high=True to be exclusive\n    When using as the higher end of a range, use time_snowflake(high=True) + 1 to be inclusive, high=False to be exclusive\n\n    Parameters\n    -----------\n    datetime_obj\n        A timezone-naive datetime object representing UTC time.\n    high: :class:`bool`\n        Whether or not to set the lower 22 bit to high or low.\n    \"\"\"\n    unix_seconds = (datetime_obj - type(datetime_obj)(1970, 1, 1)).total_seconds()\n    discord_millis = int(unix_seconds * 1000 - DISCORD_EPOCH)\n\n    return (discord_millis << 22) + (2**22-1 if high else 0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the width of a string.", "response": "def _string_width(string, *, _IS_ASCII=_IS_ASCII):\n    \"\"\"Returns string's width.\"\"\"\n    match = _IS_ASCII.match(string)\n    if match:\n        return match.endpos\n\n    UNICODE_WIDE_CHAR_TYPE = 'WFA'\n    width = 0\n    func = unicodedata.east_asian_width\n    for char in string:\n        width += 2 if func(char) in UNICODE_WIDE_CHAR_TYPE else 1\n    return width"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resolve_invite(invite):\n    from .invite import Invite  # circular import\n    if isinstance(invite, Invite) or isinstance(invite, Object):\n        return invite.id\n    else:\n        rx = r'(?:https?\\:\\/\\/)?discord(?:\\.gg|app\\.com\\/invite)\\/(.+)'\n        m = re.match(rx, invite)\n        if m:\n            return m.group(1)\n    return invite", "response": "Resolves an invite from a URL or IDomoInvite."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef escape_markdown(text, *, as_needed=False, ignore_links=True):\n\n    if not as_needed:\n        url_regex = r'(?P<url>(?:https?|steam)://(?:-\\.)?(?:[^\\s/?\\.#-]+\\.?)+(?:/[^\\s]*)?)'\n        def replacement(match):\n            groupdict = match.groupdict()\n            is_url = groupdict.get('url')\n            if is_url:\n                return is_url\n            return '\\\\' + groupdict['markdown']\n\n        regex = r'(?P<markdown>[_\\\\~|\\*`])'\n        if ignore_links:\n            regex = '(?:%s|%s)' % (url_regex, regex)\n        return re.sub(regex, replacement, text)\n    else:\n        text = re.sub(r'\\\\', r'\\\\\\\\', text)\n        return _MARKDOWN_ESCAPE_REGEX.sub(r'\\\\\\1', text)", "response": "A helper function that escapes Discord s markdown."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def add(ctx, left: int, right: int):\n    await ctx.send(left + right)", "response": "Adds two numbers together."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrolls a dice in NdN format.", "response": "async def roll(ctx, dice: str):\n    \"\"\"Rolls a dice in NdN format.\"\"\"\n    try:\n        rolls, limit = map(int, dice.split('d'))\n    except Exception:\n        await ctx.send('Format has to be in NdN!')\n        return\n\n    result = ', '.join(str(random.randint(1, limit)) for r in range(rolls))\n    await ctx.send(result)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def repeat(ctx, times: int, content='repeating...'):\n    for i in range(times):\n        await ctx.send(content)", "response": "Repeats a message multiple times."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def join(self, ctx, *, channel: discord.VoiceChannel):\n\n        if ctx.voice_client is not None:\n            return await ctx.voice_client.move_to(channel)\n\n        await channel.connect()", "response": "Joins a voice channel"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def play(self, ctx, *, query):\n\n        source = discord.PCMVolumeTransformer(discord.FFmpegPCMAudio(query))\n        ctx.voice_client.play(source, after=lambda e: print('Player error: %s' % e) if e else None)\n\n        await ctx.send('Now playing: {}'.format(query))", "response": "Plays a file from the local filesystem"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstreams from a url", "response": "async def stream(self, ctx, *, url):\n        \"\"\"Streams from a url (same as yt, but doesn't predownload)\"\"\"\n\n        async with ctx.typing():\n            player = await YTDLSource.from_url(url, loop=self.bot.loop, stream=True)\n            ctx.voice_client.play(player, after=lambda e: print('Player error: %s' % e) if e else None)\n\n        await ctx.send('Now playing: {}'.format(player.title))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nchange the player s volume", "response": "async def volume(self, ctx, volume: int):\n        \"\"\"Changes the player's volume\"\"\"\n\n        if ctx.voice_client is None:\n            return await ctx.send(\"Not connected to a voice channel.\")\n\n        ctx.voice_client.source.volume = volume / 100\n        await ctx.send(\"Changed volume to {}%\".format(volume))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef duration(self):\n        if self.ended_timestamp is None:\n            return datetime.datetime.utcnow() - self.message.created_at\n        else:\n            return self.ended_timestamp - self.message.created_at", "response": "Queries the duration of the call."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connected(self):\n        ret = [u for u in self.channel.recipients if self.voice_state_for(u) is not None]\n        me = self.channel.me\n        if self.voice_state_for(me) is not None:\n            ret.append(me)\n\n        return ret", "response": "A property that returns the list of User that are currently in this call."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef partial(cls, id, token, *, adapter):\n\n        if not isinstance(adapter, WebhookAdapter):\n            raise TypeError('adapter must be a subclass of WebhookAdapter')\n\n        data = {\n            'id': id,\n            'token': token\n        }\n\n        return cls(data, adapter=adapter)", "response": "Creates a partial webhook object with an ID and a token."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_url(cls, url, *, adapter):\n\n        m = re.search(r'discordapp.com/api/webhooks/(?P<id>[0-9]{17,21})/(?P<token>[A-Za-z0-9\\.\\-\\_]{60,68})', url)\n        if m is None:\n            raise InvalidArgument('Invalid webhook URL given.')\n        return cls(m.groupdict(), adapter=adapter)", "response": "Creates a partial Webhook object from a webhook URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef channel(self):\n        guild = self.guild\n        return guild and guild.get_channel(self.channel_id)", "response": "The text channel this webhook belongs to."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef avatar_url_as(self, *, format=None, size=1024):\n        if self.avatar is None:\n            # Default is always blurple apparently\n            return Asset(self._state, 'https://cdn.discordapp.com/embed/avatars/0.png')\n\n        if not utils.valid_icon_size(size):\n            raise InvalidArgument(\"size must be a power of 2 between 16 and 1024\")\n\n        format = format or 'png'\n\n        if format not in ('png', 'jpg', 'jpeg'):\n            raise InvalidArgument(\"format must be one of 'png', 'jpg', or 'jpeg'.\")\n\n        url = 'https://cdn.discordapp.com/avatars/{0.id}/{0.avatar}.{1}?size={2}'.format(self, format, size)\n        return Asset(self._state, url)", "response": "Returns a friendly URL version of the avatar the webhook has."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(self, content=None, *, wait=False, username=None, avatar_url=None, tts=False,\n                                    file=None, files=None, embed=None, embeds=None):\n        \"\"\"|maybecoro|\n\n        Sends a message using the webhook.\n\n        If the webhook is constructed with a :class:`RequestsWebhookAdapter` then this is\n        not a coroutine.\n\n        The content must be a type that can convert to a string through ``str(content)``.\n\n        To upload a single file, the ``file`` parameter should be used with a\n        single :class:`File` object.\n\n        If the ``embed`` parameter is provided, it must be of type :class:`Embed` and\n        it must be a rich embed type. You cannot mix the ``embed`` parameter with the\n        ``embeds`` parameter, which must be a :class:`list` of :class:`Embed` objects to send.\n\n        Parameters\n        ------------\n        content: :class:`str`\n            The content of the message to send.\n        wait: :class:`bool`\n            Whether the server should wait before sending a response. This essentially\n            means that the return type of this function changes from ``None`` to\n            a :class:`Message` if set to ``True``.\n        username: :class:`str`\n            The username to send with this message. If no username is provided\n            then the default username for the webhook is used.\n        avatar_url: Union[:class:`str`, :class:`Asset`]\n            The avatar URL to send with this message. If no avatar URL is provided\n            then the default avatar for the webhook is used.\n        tts: :class:`bool`\n            Indicates if the message should be sent using text-to-speech.\n        file: :class:`File`\n            The file to upload. This cannot be mixed with ``files`` parameter.\n        files: List[:class:`File`]\n            A list of files to send with the content. This cannot be mixed with the\n            ``file`` parameter.\n        embed: :class:`Embed`\n            The rich embed for the content to send. This cannot be mixed with\n            ``embeds`` parameter.\n        embeds: List[:class:`Embed`]\n            A list of embeds to send with the content. Maximum of 10. This cannot\n            be mixed with the ``embed`` parameter.\n\n        Raises\n        --------\n        HTTPException\n            Sending the message failed.\n        NotFound\n            This webhook was not found.\n        Forbidden\n            The authorization token for the webhook is incorrect.\n        InvalidArgument\n            You specified both ``embed`` and ``embeds`` or the length of\n            ``embeds`` was invalid.\n\n        Returns\n        ---------\n        Optional[:class:`Message`]\n            The message that was sent.\n        \"\"\"\n\n        payload = {}\n\n        if files is not None and file is not None:\n            raise InvalidArgument('Cannot mix file and files keyword arguments.')\n        if embeds is not None and embed is not None:\n            raise InvalidArgument('Cannot mix embed and embeds keyword arguments.')\n\n        if embeds is not None:\n            if len(embeds) > 10:\n                raise InvalidArgument('embeds has a maximum of 10 elements.')\n            payload['embeds'] = [e.to_dict() for e in embeds]\n\n        if embed is not None:\n            payload['embeds'] = [embed.to_dict()]\n\n        if content is not None:\n            payload['content'] = str(content)\n\n        payload['tts'] = tts\n        if avatar_url:\n            payload['avatar_url'] = str(avatar_url)\n        if username:\n            payload['username'] = username\n\n        return self._adapter.execute_webhook(wait=wait, file=file, files=files, payload=payload)", "response": "This method sends a message using the webhook."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef users(self, limit=None, after=None):\n\n        if self.custom_emoji:\n            emoji = '{0.name}:{0.id}'.format(self.emoji)\n        else:\n            emoji = self.emoji\n\n        if limit is None:\n            limit = self.count\n\n        return ReactionIterator(self.message, emoji, limit, after)", "response": "Returns an async iterator that returns the users that have reacted to the message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of members that can see this channel.", "response": "def members(self):\n        \"\"\"Returns a :class:`list` of :class:`Member` that can see this channel.\"\"\"\n        return [m for m in self.guild.members if self.permissions_for(m).read_messages]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbulk delete a list of messages.", "response": "async def delete_messages(self, messages):\n        \"\"\"|coro|\n\n        Deletes a list of messages. This is similar to :meth:`Message.delete`\n        except it bulk deletes multiple messages.\n\n        As a special case, if the number of messages is 0, then nothing\n        is done. If the number of messages is 1 then single message\n        delete is done. If it's more than two, then bulk delete is used.\n\n        You cannot bulk delete more than 100 messages or messages that\n        are older than 14 days old.\n\n        You must have the :attr:`~Permissions.manage_messages` permission to\n        use this.\n\n        Usable only by bot accounts.\n\n        Parameters\n        -----------\n        messages: Iterable[:class:`abc.Snowflake`]\n            An iterable of messages denoting which ones to bulk delete.\n\n        Raises\n        ------\n        ClientException\n            The number of messages to delete was more than 100.\n        Forbidden\n            You do not have proper permissions to delete the messages or\n            you're not using a bot account.\n        HTTPException\n            Deleting the messages failed.\n        \"\"\"\n        if not isinstance(messages, (list, tuple)):\n            messages = list(messages)\n\n        if len(messages) == 0:\n            return # do nothing\n\n        if len(messages) == 1:\n            message_id = messages[0].id\n            await self._state.http.delete_message(self.id, message_id)\n            return\n\n        if len(messages) > 100:\n            raise ClientException('Can only bulk delete messages up to 100 messages')\n\n        message_ids = [m.id for m in messages]\n        await self._state.http.delete_messages(self.id, message_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npurge a list of messages that meet the criteria given by the predicate .", "response": "async def purge(self, *, limit=100, check=None, before=None, after=None, around=None, oldest_first=False, bulk=True):\n        \"\"\"|coro|\n\n        Purges a list of messages that meet the criteria given by the predicate\n        ``check``. If a ``check`` is not provided then all messages are deleted\n        without discrimination.\n\n        You must have the :attr:`~Permissions.manage_messages` permission to\n        delete messages even if they are your own (unless you are a user\n        account). The :attr:`~Permissions.read_message_history` permission is\n        also needed to retrieve message history.\n\n        Internally, this employs a different number of strategies depending\n        on the conditions met such as if a bulk delete is possible or if\n        the account is a user bot or not.\n\n        Examples\n        ---------\n\n        Deleting bot's messages ::\n\n            def is_me(m):\n                return m.author == client.user\n\n            deleted = await channel.purge(limit=100, check=is_me)\n            await channel.send('Deleted {} message(s)'.format(len(deleted)))\n\n        Parameters\n        -----------\n        limit: Optional[:class:`int`]\n            The number of messages to search through. This is not the number\n            of messages that will be deleted, though it can be.\n        check: predicate\n            The function used to check if a message should be deleted.\n            It must take a :class:`Message` as its sole parameter.\n        before\n            Same as ``before`` in :meth:`history`.\n        after\n            Same as ``after`` in :meth:`history`.\n        around\n            Same as ``around`` in :meth:`history`.\n        oldest_first\n            Same as ``oldest_first`` in :meth:`history`.\n        bulk: :class:`bool`\n            If True, use bulk delete. bulk=False is useful for mass-deleting\n            a bot's own messages without manage_messages. When True, will fall\n            back to single delete if current account is a user bot, or if\n            messages are older than two weeks.\n\n        Raises\n        -------\n        Forbidden\n            You do not have proper permissions to do the actions required.\n        HTTPException\n            Purging the messages failed.\n\n        Returns\n        --------\n        List[:class:`.Message`]\n            The list of messages that were deleted.\n        \"\"\"\n\n        if check is None:\n            check = lambda m: True\n\n        iterator = self.history(limit=limit, before=before, after=after, oldest_first=oldest_first, around=around)\n        ret = []\n        count = 0\n\n        minimum_time = int((time.time() - 14 * 24 * 60 * 60) * 1000.0 - 1420070400000) << 22\n        strategy = self.delete_messages if self._state.is_bot and bulk else _single_delete_strategy\n\n        while True:\n            try:\n                msg = await iterator.next()\n            except NoMoreItems:\n                # no more messages to poll\n                if count >= 2:\n                    # more than 2 messages -> bulk delete\n                    to_delete = ret[-count:]\n                    await strategy(to_delete)\n                elif count == 1:\n                    # delete a single message\n                    await ret[-1].delete()\n\n                return ret\n            else:\n                if count == 100:\n                    # we've reached a full 'queue'\n                    to_delete = ret[-100:]\n                    await strategy(to_delete)\n                    count = 0\n                    await asyncio.sleep(1)\n\n                if check(msg):\n                    if msg.id < minimum_time:\n                        # older than 14 days old\n                        if count == 1:\n                            await ret[-1].delete()\n                        elif count >= 2:\n                            to_delete = ret[-count:]\n                            await strategy(to_delete)\n\n                        count = 0\n                        strategy = _single_delete_strategy\n\n                    count += 1\n                    ret.append(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the webhooks for this channel.", "response": "async def webhooks(self):\n        \"\"\"|coro|\n\n        Gets the list of webhooks from this channel.\n\n        Requires :attr:`~.Permissions.manage_webhooks` permissions.\n\n        Raises\n        -------\n        Forbidden\n            You don't have permissions to get the webhooks.\n\n        Returns\n        --------\n        List[:class:`Webhook`]\n            The webhooks for this channel.\n        \"\"\"\n\n        data = await self._state.http.channel_webhooks(self.id)\n        return [Webhook.from_state(d, state=self._state) for d in data]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def create_webhook(self, *, name, avatar=None, reason=None):\n\n        if avatar is not None:\n            avatar = utils._bytes_to_base64_data(avatar)\n\n        data = await self._state.http.create_webhook(self.id, name=str(name), avatar=avatar, reason=reason)\n        return Webhook.from_state(data, state=self._state)", "response": "Create a webhook for this channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef members(self):\n        ret = []\n        for user_id, state in self.guild._voice_states.items():\n            if state.channel.id == self.id:\n                member = self.guild.get_member(user_id)\n                if member is not None:\n                    ret.append(member)\n        return ret", "response": "Returns a list of members that are currently inside this voice channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef channels(self):\n        def comparator(channel):\n            return (not isinstance(channel, TextChannel), channel.position)\n\n        ret = [c for c in self.guild.channels if c.category_id == self.id]\n        ret.sort(key=comparator)\n        return ret", "response": "Returns the list of guild channels under this category."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef text_channels(self):\n        ret = [c for c in self.guild.channels\n            if c.category_id == self.id\n            and isinstance(c, TextChannel)]\n        ret.sort(key=lambda c: (c.position, c.id))\n        return ret", "response": "List of TextChannel objects that are under this category."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef voice_channels(self):\n        ret = [c for c in self.guild.channels\n            if c.category_id == self.id\n            and isinstance(c, VoiceChannel)]\n        ret.sort(key=lambda c: (c.position, c.id))\n        return ret", "response": "List of voice channels that are under this category."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def create_text_channel(self, name, *, overwrites=None, reason=None, **options):\n        return await self.guild.create_text_channel(name, overwrites=overwrites, category=self, reason=reason, **options)", "response": "Create a new TextChannel in the category."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a voice channel in the category.", "response": "async def create_voice_channel(self, name, *, overwrites=None, reason=None, **options):\n        \"\"\"|coro|\n\n        A shortcut method to :meth:`Guild.create_voice_channel` to create a :class:`VoiceChannel` in the category.\n        \"\"\"\n        return await self.guild.create_voice_channel(name, overwrites=overwrites, category=self, reason=reason, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef permissions_for(self, user=None):\n\n        base = Permissions.text()\n        base.send_tts_messages = False\n        base.manage_messages = False\n        return base", "response": "Returns the permissions for a user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef permissions_for(self, user):\n\n        base = Permissions.text()\n        base.send_tts_messages = False\n        base.manage_messages = False\n        base.mention_everyone = True\n\n        if user.id == self.owner.id:\n            base.kick_members = True\n\n        return base", "response": "Returns the permissions for a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds recipients to this group.", "response": "async def add_recipients(self, *recipients):\n        r\"\"\"|coro|\n\n        Adds recipients to this group.\n\n        A group can only have a maximum of 10 members.\n        Attempting to add more ends up in an exception. To\n        add a recipient to the group, you must have a relationship\n        with the user of type :attr:`RelationshipType.friend`.\n\n        Parameters\n        -----------\n        \\*recipients: :class:`User`\n            An argument list of users to add to this group.\n\n        Raises\n        -------\n        HTTPException\n            Adding a recipient to this group failed.\n        \"\"\"\n\n        # TODO: wait for the corresponding WS event\n\n        req = self._state.http.add_group_recipient\n        for recipient in recipients:\n            await req(self.id, recipient.id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def remove_recipients(self, *recipients):\n\n        # TODO: wait for the corresponding WS event\n\n        req = self._state.http.remove_group_recipient\n        for recipient in recipients:\n            await req(self.id, recipient.id)", "response": "|coro| Removes recipients from this group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def edit(self, **fields):\n\n        try:\n            icon_bytes = fields['icon']\n        except KeyError:\n            pass\n        else:\n            if icon_bytes is not None:\n                fields['icon'] = utils._bytes_to_base64_data(icon_bytes)\n\n        data = await self._state.http.edit_group(self.id, **fields)\n        self._update_group(data)", "response": "|coro|\n\n        Edits the group.\n\n        Parameters\n        -----------\n        name: Optional[:class:`str`]\n            The new name to change the group to.\n            Could be ``None`` to remove the name.\n        icon: Optional[:class:`bytes`]\n            A :term:`py:bytes-like object` representing the new icon.\n            Could be ``None`` to remove the icon.\n\n        Raises\n        -------\n        HTTPException\n            Editing the group failed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of roles that have been overridden from their default values in the guild. roles attribute.", "response": "def changed_roles(self):\n        \"\"\"Returns a :class:`list` of :class:`Roles` that have been overridden from\n        their default values in the :attr:`Guild.roles` attribute.\"\"\"\n        ret = []\n        g = self.guild\n        for overwrite in filter(lambda o: o.type == 'role', self._overwrites):\n            role = g.get_role(overwrite.id)\n            if role is None:\n                continue\n\n            role = copy.copy(role)\n            role.permissions.handle_overwrite(overwrite.allow, overwrite.deny)\n            ret.append(role)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef overwrites_for(self, obj):\n\n        if isinstance(obj, User):\n            predicate = lambda p: p.type == 'member'\n        elif isinstance(obj, Role):\n            predicate = lambda p: p.type == 'role'\n        else:\n            predicate = lambda p: True\n\n        for overwrite in filter(predicate, self._overwrites):\n            if overwrite.id == obj.id:\n                allow = Permissions(overwrite.allow)\n                deny = Permissions(overwrite.deny)\n                return PermissionOverwrite.from_pair(allow, deny)\n\n        return PermissionOverwrite()", "response": "Returns the set of channel - specific overwrites for a member or a role."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all of the channel s permission overwrites.", "response": "def overwrites(self):\n        \"\"\"Returns all of the channel's overwrites.\n\n        This is returned as a dictionary where the key contains the target which\n        can be either a :class:`Role` or a :class:`Member` and the key is the\n        overwrite as a :class:`PermissionOverwrite`.\n\n        Returns\n        --------\n        Mapping[Union[:class:`Role`, :class:`Member`], :class:`PermissionOverwrite`]:\n            The channel's permission overwrites.\n        \"\"\"\n        ret = {}\n        for ow in self._overwrites:\n            allow = Permissions(ow.allow)\n            deny = Permissions(ow.deny)\n            overwrite = PermissionOverwrite.from_pair(allow, deny)\n\n            if ow.type == 'role':\n                target = self.guild.get_role(ow.id)\n            elif ow.type == 'member':\n                target = self.guild.get_member(ow.id)\n\n            # TODO: There is potential data loss here in the non-chunked\n            # case, i.e. target is None because get_member returned nothing.\n            # This can be fixed with a slight breaking change to the return type,\n            # i.e. adding discord.Object to the list of it\n            # However, for now this is an acceptable compromise.\n            if target is not None:\n                ret[target] = overwrite\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef permissions_for(self, member):\n\n        # The current cases can be explained as:\n        # Guild owner get all permissions -- no questions asked. Otherwise...\n        # The @everyone role gets the first application.\n        # After that, the applied roles that the user has in the channel\n        # (or otherwise) are then OR'd together.\n        # After the role permissions are resolved, the member permissions\n        # have to take into effect.\n        # After all that is done.. you have to do the following:\n\n        # If manage permissions is True, then all permissions are set to True.\n\n        # The operation first takes into consideration the denied\n        # and then the allowed.\n\n        o = self.guild.owner\n        if o is not None and member.id == o.id:\n            return Permissions.all()\n\n        default = self.guild.default_role\n        base = Permissions(default.permissions.value)\n        roles = member.roles\n\n        # Apply guild roles that the member has.\n        for role in roles:\n            base.value |= role.permissions.value\n\n        # Guild-wide Administrator -> True for everything\n        # Bypass all channel-specific overrides\n        if base.administrator:\n            return Permissions.all()\n\n        # Apply @everyone allow/deny first since it's special\n        try:\n            maybe_everyone = self._overwrites[0]\n            if maybe_everyone.id == self.guild.id:\n                base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)\n                remaining_overwrites = self._overwrites[1:]\n            else:\n                remaining_overwrites = self._overwrites\n        except IndexError:\n            remaining_overwrites = self._overwrites\n\n        # not sure if doing member._roles.get(...) is better than the\n        # set approach. While this is O(N) to re-create into a set for O(1)\n        # the direct approach would just be O(log n) for searching with no\n        # extra memory overhead. For now, I'll keep the set cast\n        # Note that the member.roles accessor up top also creates a\n        # temporary list\n        member_role_ids = {r.id for r in roles}\n        denies = 0\n        allows = 0\n\n        # Apply channel specific role permission overwrites\n        for overwrite in remaining_overwrites:\n            if overwrite.type == 'role' and overwrite.id in member_role_ids:\n                denies |= overwrite.deny\n                allows |= overwrite.allow\n\n        base.handle_overwrite(allow=allows, deny=denies)\n\n        # Apply member specific permission overwrites\n        for overwrite in remaining_overwrites:\n            if overwrite.type == 'member' and overwrite.id == member.id:\n                base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)\n                break\n\n        # if you can't send a message in a channel then you can't have certain\n        # permissions as well\n        if not base.send_messages:\n            base.send_tts_messages = False\n            base.mention_everyone = False\n            base.embed_links = False\n            base.attach_files = False\n\n        # if you can't read a channel then you have no permissions there\n        if not base.read_messages:\n            denied = Permissions.all_channel()\n            base.value &= ~denied.value\n\n        return base", "response": "Returns a list of permissions for the given member."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def delete(self, *, reason=None):\n        await self._state.http.delete_channel(self.id, reason=reason)", "response": "|coro|\n\n        Deletes the channel.\n\n        You must have :attr:`~.Permissions.manage_channels` permission to use this.\n\n        Parameters\n        -----------\n        reason: Optional[:class:`str`]\n            The reason for deleting this channel.\n            Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You do not have proper permissions to delete the channel.\n        NotFound\n            The channel was not found or was already deleted.\n        HTTPException\n            Deleting the channel failed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the permissions for a target in the channel.", "response": "async def set_permissions(self, target, *, overwrite=_undefined, reason=None, **permissions):\n        r\"\"\"|coro|\n\n        Sets the channel specific permission overwrites for a target in the\n        channel.\n\n        The ``target`` parameter should either be a :class:`Member` or a\n        :class:`Role` that belongs to guild.\n\n        The ``overwrite`` parameter, if given, must either be ``None`` or\n        :class:`PermissionOverwrite`. For convenience, you can pass in\n        keyword arguments denoting :class:`Permissions` attributes. If this is\n        done, then you cannot mix the keyword arguments with the ``overwrite``\n        parameter.\n\n        If the ``overwrite`` parameter is ``None``, then the permission\n        overwrites are deleted.\n\n        You must have the :attr:`~Permissions.manage_roles` permission to use this.\n\n        Examples\n        ----------\n\n        Setting allow and deny: ::\n\n            await message.channel.set_permissions(message.author, read_messages=True,\n                                                                  send_messages=False)\n\n        Deleting overwrites ::\n\n            await channel.set_permissions(member, overwrite=None)\n\n        Using :class:`PermissionOverwrite` ::\n\n            overwrite = discord.PermissionOverwrite()\n            overwrite.send_messages = False\n            overwrite.read_messages = True\n            await channel.set_permissions(member, overwrite=overwrite)\n\n        Parameters\n        -----------\n        target\n            The :class:`Member` or :class:`Role` to overwrite permissions for.\n        overwrite: :class:`PermissionOverwrite`\n            The permissions to allow and deny to the target.\n        \\*\\*permissions\n            A keyword argument list of permissions to set for ease of use.\n            Cannot be mixed with ``overwrite``.\n        reason: Optional[:class:`str`]\n            The reason for doing this action. Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to edit channel specific permissions.\n        HTTPException\n            Editing channel specific permissions failed.\n        NotFound\n            The role or member being edited is not part of the guild.\n        InvalidArgument\n            The overwrite parameter invalid or the target type was not\n            :class:`Role` or :class:`Member`.\n        \"\"\"\n\n        http = self._state.http\n\n        if isinstance(target, User):\n            perm_type = 'member'\n        elif isinstance(target, Role):\n            perm_type = 'role'\n        else:\n            raise InvalidArgument('target parameter must be either Member or Role')\n\n        if isinstance(overwrite, _Undefined):\n            if len(permissions) == 0:\n                raise InvalidArgument('No overwrite provided.')\n            try:\n                overwrite = PermissionOverwrite(**permissions)\n            except (ValueError, TypeError):\n                raise InvalidArgument('Invalid permissions given to keyword arguments.')\n        else:\n            if len(permissions) > 0:\n                raise InvalidArgument('Cannot mix overwrite and keyword arguments.')\n\n        # TODO: wait for event\n\n        if overwrite is None:\n            await http.delete_channel_permissions(self.id, target.id, reason=reason)\n        elif isinstance(overwrite, PermissionOverwrite):\n            (allow, deny) = overwrite.pair()\n            await http.edit_channel_permissions(self.id, target.id, allow.value, deny.value, perm_type, reason=reason)\n        else:\n            raise InvalidArgument('Invalid overwrite type provided.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an instant invite for this entry.", "response": "async def create_invite(self, *, reason=None, **fields):\n        \"\"\"|coro|\n\n        Creates an instant invite.\n\n        You must have :attr:`~.Permissions.create_instant_invite` permission to\n        do this.\n\n        Parameters\n        ------------\n        max_age: :class:`int`\n            How long the invite should last. If it's 0 then the invite\n            doesn't expire. Defaults to 0.\n        max_uses: :class:`int`\n            How many uses the invite could be used for. If it's 0 then there\n            are unlimited uses. Defaults to 0.\n        temporary: :class:`bool`\n            Denotes that the invite grants temporary membership\n            (i.e. they get kicked after they disconnect). Defaults to False.\n        unique: :class:`bool`\n            Indicates if a unique invite URL should be created. Defaults to True.\n            If this is set to False then it will return a previously created\n            invite.\n        reason: Optional[:class:`str`]\n            The reason for creating this invite. Shows up on the audit log.\n\n        Raises\n        -------\n        HTTPException\n            Invite creation failed.\n\n        Returns\n        --------\n        :class:`Invite`\n            The invite that was created.\n        \"\"\"\n\n        data = await self._state.http.create_invite(self.id, reason=reason, **fields)\n        return Invite.from_incomplete(data=data, state=self._state)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all active instant invites from this channel.", "response": "async def invites(self):\n        \"\"\"|coro|\n\n        Returns a list of all active instant invites from this channel.\n\n        You must have :attr:`~.Permissions.manage_guild` to get this information.\n\n        Raises\n        -------\n        Forbidden\n            You do not have proper permissions to get the information.\n        HTTPException\n            An error occurred while fetching the information.\n\n        Returns\n        -------\n        List[:class:`Invite`]\n            The list of invites that are currently active.\n        \"\"\"\n\n        state = self._state\n        data = await state.http.invites_from_channel(self.id)\n        result = []\n\n        for invite in data:\n            invite['channel'] = self\n            invite['guild'] = self.guild\n            result.append(Invite(state=state, data=invite))\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a message to the destination.", "response": "async def send(self, content=None, *, tts=False, embed=None, file=None, files=None, delete_after=None, nonce=None):\n        \"\"\"|coro|\n\n        Sends a message to the destination with the content given.\n\n        The content must be a type that can convert to a string through ``str(content)``.\n        If the content is set to ``None`` (the default), then the ``embed`` parameter must\n        be provided.\n\n        To upload a single file, the ``file`` parameter should be used with a\n        single :class:`.File` object. To upload multiple files, the ``files``\n        parameter should be used with a :class:`list` of :class:`.File` objects.\n        **Specifying both parameters will lead to an exception**.\n\n        If the ``embed`` parameter is provided, it must be of type :class:`.Embed` and\n        it must be a rich embed type.\n\n        Parameters\n        ------------\n        content\n            The content of the message to send.\n        tts: :class:`bool`\n            Indicates if the message should be sent using text-to-speech.\n        embed: :class:`.Embed`\n            The rich embed for the content.\n        file: :class:`.File`\n            The file to upload.\n        files: List[:class:`.File`]\n            A list of files to upload. Must be a maximum of 10.\n        nonce: :class:`int`\n            The nonce to use for sending this message. If the message was successfully sent,\n            then the message will have a nonce with this value.\n        delete_after: :class:`float`\n            If provided, the number of seconds to wait in the background\n            before deleting the message we just sent. If the deletion fails,\n            then it is silently ignored.\n\n        Raises\n        --------\n        :exc:`.HTTPException`\n            Sending the message failed.\n        :exc:`.Forbidden`\n            You do not have the proper permissions to send the message.\n        :exc:`.InvalidArgument`\n            The ``files`` list is not of the appropriate size or\n            you specified both ``file`` and ``files``.\n\n        Returns\n        ---------\n        :class:`.Message`\n            The message that was sent.\n        \"\"\"\n\n        channel = await self._get_channel()\n        state = self._state\n        content = str(content) if content is not None else None\n        if embed is not None:\n            embed = embed.to_dict()\n\n        if file is not None and files is not None:\n            raise InvalidArgument('cannot pass both file and files parameter to send()')\n\n        if file is not None:\n            if not isinstance(file, File):\n                raise InvalidArgument('file parameter must be File')\n\n            try:\n                data = await state.http.send_files(channel.id, files=[file],\n                                                   content=content, tts=tts, embed=embed, nonce=nonce)\n            finally:\n                file.close()\n\n        elif files is not None:\n            if len(files) > 10:\n                raise InvalidArgument('files parameter must be a list of up to 10 elements')\n            elif not all(isinstance(file, File) for file in files):\n                raise InvalidArgument('files parameter must be a list of File')\n\n            try:\n                data = await state.http.send_files(channel.id, files=files, content=content, tts=tts,\n                                                   embed=embed, nonce=nonce)\n            finally:\n                for f in files:\n                    f.close()\n        else:\n            data = await state.http.send_message(channel.id, content, tts=tts, embed=embed, nonce=nonce)\n\n        ret = state.create_message(channel=channel, data=data)\n        if delete_after is not None:\n            await ret.delete(delay=delete_after)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntriggers a typing indicator to the destination.", "response": "async def trigger_typing(self):\n        \"\"\"|coro|\n\n        Triggers a *typing* indicator to the destination.\n\n        *Typing* indicator will go away after 10 seconds, or after a message is sent.\n        \"\"\"\n\n        channel = await self._get_channel()\n        await self._state.http.send_typing(channel.id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of messages that are currently pinned.", "response": "async def pins(self):\n        \"\"\"|coro|\n\n        Returns a :class:`list` of :class:`.Message` that are currently pinned.\n\n        Raises\n        -------\n        :exc:`.HTTPException`\n            Retrieving the pinned messages failed.\n        \"\"\"\n\n        channel = await self._get_channel()\n        state = self._state\n        data = await state.http.pins_from(channel.id)\n        return [state.create_message(channel=channel, data=m) for m in data]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef history(self, *, limit=100, before=None, after=None, around=None, oldest_first=None):\n        return HistoryIterator(self, limit=limit, before=before, after=after, around=around, oldest_first=oldest_first)", "response": "Returns an asynchronous iterator that returns the message history for the specified date and time range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def connect(self, *, timeout=60.0, reconnect=True):\n        key_id, _ = self._get_voice_client_key()\n        state = self._state\n\n        if state._get_voice_client(key_id):\n            raise ClientException('Already connected to a voice channel.')\n\n        voice = VoiceClient(state=state, timeout=timeout, channel=self)\n        state._add_voice_client(key_id, voice)\n\n        try:\n            await voice.connect(reconnect=reconnect)\n        except asyncio.TimeoutError:\n            try:\n                await voice.disconnect(force=True)\n            except Exception:\n                # we don't care if disconnect failed because connection failed\n                pass\n            raise # re-raise\n\n        return voice", "response": "Connect to the voice server and create a voice client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an asset of the emoji if it is custom.", "response": "def url(self):\n        \"\"\":class:`Asset`:Returns an asset of the emoji, if it is custom.\"\"\"\n        if self.is_unicode_emoji():\n            return Asset(self._state)\n\n        _format = 'gif' if self.animated else 'png'\n        url = \"https://cdn.discordapp.com/emojis/{0.id}.{1}\".format(self, _format)\n        return Asset(self._state, url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete the custom emoji.", "response": "async def delete(self, *, reason=None):\n        \"\"\"|coro|\n\n        Deletes the custom emoji.\n\n        You must have :attr:`~Permissions.manage_emojis` permission to\n        do this.\n\n        Parameters\n        -----------\n        reason: Optional[:class:`str`]\n            The reason for deleting this emoji. Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You are not allowed to delete emojis.\n        HTTPException\n            An error occurred deleting the emoji.\n        \"\"\"\n\n        await self._state.http.delete_custom_emoji(self.guild.id, self.id, reason=reason)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def edit(self, *, name, roles=None, reason=None):\n\n        if roles:\n            roles = [role.id for role in roles]\n        await self._state.http.edit_custom_emoji(self.guild.id, self.id, name=name, roles=roles, reason=reason)", "response": "Edit the custom emoji."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef loop(*, seconds=0, minutes=0, hours=0, count=None, reconnect=True, loop=None):\n    def decorator(func):\n        return Loop(func, seconds=seconds, minutes=minutes, hours=hours,\n                          count=count, reconnect=reconnect, loop=loop)\n    return decorator", "response": "A decorator that schedules a task in the background for you with a loop."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self, *args, **kwargs):\n\n        if self._task is not None:\n            raise RuntimeError('Task is already launched.')\n\n        if self._injected is not None:\n            args = (self._injected, *args)\n\n        self._task = self.loop.create_task(self._loop(*args, **kwargs))\n        return self._task", "response": "Starts the internal task in the event loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_exception_type(self, exc):\n\n        if not inspect.isclass(exc):\n            raise TypeError('{0!r} must be a class.'.format(exc))\n        if not issubclass(exc, BaseException):\n            raise TypeError('{0!r} must inherit from BaseException.'.format(exc))\n\n        self._valid_exception = (*self._valid_exception, exc)", "response": "r Adds an exception type to be handled during the reconnect logic."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_exception_type(self, exc):\n        old_length = len(self._valid_exception)\n        self._valid_exception = tuple(x for x in self._valid_exception if x is not exc)\n        return len(self._valid_exception) != old_length", "response": "Removes an exception type from being handled during the reconnect logic."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef before_loop(self, coro):\n\n        if not (inspect.iscoroutinefunction(coro) or inspect.isawaitable(coro)):\n            raise TypeError('Expected coroutine or awaitable, received {0.__name__!r}.'.format(type(coro)))\n\n        self._before_loop = coro", "response": "A decorator to register a coroutine to be called before the loop starts."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninvokes a command with the arguments given.", "response": "async def invoke(self, *args, **kwargs):\n        r\"\"\"|coro|\n\n        Calls a command with the arguments given.\n\n        This is useful if you want to just call the callback that a\n        :class:`.Command` holds internally.\n\n        Note\n        ------\n        You do not pass in the context as it is done for you.\n\n        Warning\n        ---------\n        The first parameter passed **must** be the command being invoked.\n\n        Parameters\n        -----------\n        command: :class:`.Command`\n            A command or subclass of a command that is going to be called.\n        \\*args\n            The arguments to to use.\n        \\*\\*kwargs\n            The keyword arguments to use.\n        \"\"\"\n\n        try:\n            command = args[0]\n        except IndexError:\n            raise TypeError('Missing command to invoke.') from None\n\n        arguments = []\n        if command.cog is not None:\n            arguments.append(command.cog)\n\n        arguments.append(self)\n        arguments.extend(args[1:])\n\n        ret = await command.callback(*arguments, **kwargs)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def reinvoke(self, *, call_hooks=False, restart=True):\n        cmd = self.command\n        view = self.view\n        if cmd is None:\n            raise ValueError('This context is not valid.')\n\n        # some state to revert to when we're done\n        index, previous = view.index, view.previous\n        invoked_with = self.invoked_with\n        invoked_subcommand = self.invoked_subcommand\n        subcommand_passed = self.subcommand_passed\n\n        if restart:\n            to_call = cmd.root_parent or cmd\n            view.index = len(self.prefix)\n            view.previous = 0\n            view.get_word() # advance to get the root command\n        else:\n            to_call = cmd\n\n        try:\n            await to_call.reinvoke(self, call_hooks=call_hooks)\n        finally:\n            self.command = cmd\n            view.index = index\n            view.previous = previous\n            self.invoked_with = invoked_with\n            self.invoked_subcommand = invoked_subcommand\n            self.subcommand_passed = subcommand_passed", "response": "Re - invoke the command and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def send_help(self, *args):\n        from .core import Group, Command\n\n        bot = self.bot\n        cmd = bot.help_command\n\n        if cmd is None:\n            return None\n\n        cmd = cmd.copy()\n        cmd.context = self\n        if len(args) == 0:\n            await cmd.prepare_help_command(self, None)\n            mapping = cmd.get_bot_mapping()\n            return await cmd.send_bot_help(mapping)\n\n        entity = args[0]\n        if entity is None:\n            return None\n\n        if isinstance(entity, str):\n            entity = bot.get_cog(entity) or bot.get_command(entity)\n\n        try:\n            qualified_name = entity.qualified_name\n        except AttributeError:\n            # if we're here then it's not a cog, group, or command.\n            return None\n\n        await cmd.prepare_help_command(self, entity.qualified_name)\n\n        if hasattr(entity, '__cog_commands__'):\n            return await cmd.send_cog_help(entity)\n        elif isinstance(entity, Group):\n            return await cmd.send_group_help(entity)\n        elif isinstance(entity, Command):\n            return await cmd.send_command_help(entity)\n        else:\n            return None", "response": "Send the help command for the specified entity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the next delay to wait according to the exponential backoff algorithm.", "response": "def delay(self):\n        \"\"\"Compute the next delay\n\n        Returns the next delay to wait according to the exponential\n        backoff algorithm.  This is a value between 0 and base * 2^exp\n        where exponent starts off at 1 and is incremented at every\n        invocation of this method up to a maximum of 10.\n\n        If a period of more than base * 2^11 has passed since the last\n        retry, the exponent is reset to 1.\n        \"\"\"\n        invocation = time.monotonic()\n        interval = invocation - self._last_invocation\n        self._last_invocation = invocation\n\n        if interval > self._reset_time:\n            self._exp = 0\n\n        self._exp = min(self._exp + 1, self._max)\n        return self._randfunc(0, self._base * 2 ** self._exp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if self has the same or fewer permissions as other.", "response": "def is_subset(self, other):\n        \"\"\"Returns True if self has the same or fewer permissions as other.\"\"\"\n        if isinstance(other, Permissions):\n            return (self.value & other.value) == self.value\n        else:\n            raise TypeError(\"cannot compare {} with {}\".format(self.__class__.__name__, other.__class__.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, **kwargs):\n        for key, value in kwargs.items():\n            try:\n                is_property = isinstance(getattr(self.__class__, key), property)\n            except AttributeError:\n                continue\n\n            if is_property:\n                setattr(self, key, value)", "response": "r Bulk updates this permission object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef changes(self):\n        obj = AuditLogChanges(self, self._changes)\n        del self._changes\n        return obj", "response": "A list of changes this entry has."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef command(name=None, cls=None, **attrs):\n    if cls is None:\n        cls = Command\n\n    def decorator(func):\n        if isinstance(func, Command):\n            raise TypeError('Callback is already a command.')\n        return cls(func, name=name, **attrs)\n\n    return decorator", "response": "A decorator that creates a new command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check(predicate):\n\n    def decorator(func):\n        if isinstance(func, Command):\n            func.checks.append(predicate)\n        else:\n            if not hasattr(func, '__commands_checks__'):\n                func.__commands_checks__ = []\n\n            func.__commands_checks__.append(predicate)\n\n        return func\n    return decorator", "response": "r A decorator that adds a check to the command or its base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_permissions(**perms):\n    def predicate(ctx):\n        ch = ctx.channel\n        permissions = ch.permissions_for(ctx.author)\n\n        missing = [perm for perm, value in perms.items() if getattr(permissions, perm, None) != value]\n\n        if not missing:\n            return True\n\n        raise MissingPermissions(missing)\n\n    return check(predicate)", "response": "A decorator that checks if the member has all of the specified permissions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_owner():\n\n    async def predicate(ctx):\n        if not await ctx.bot.is_owner(ctx.author):\n            raise NotOwner('You do not own this bot.')\n        return True\n\n    return check(predicate)", "response": "A decorator that returns a boolean value that indicates whether the person is the owner of the bot."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_nsfw():\n    def pred(ctx):\n        ch = ctx.channel\n        if ctx.guild is None or (isinstance(ch, discord.TextChannel) and ch.is_nsfw()):\n            return True\n        raise NSFWChannelRequired(ch)\n    return check(pred)", "response": "A : func :. check that checks if the channel is a NSFW channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the current command instance with updated attribute.", "response": "def update(self, **kwargs):\n        \"\"\"Updates :class:`Command` instance with updated attribute.\n\n        This works similarly to the :func:`.command` decorator in terms\n        of parameters in that they are passed to the :class:`Command` or\n        subclass constructors, sans the name and callback.\n        \"\"\"\n        self.__init__(self.callback, **dict(self.__original_kwargs__, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self):\n        ret = self.__class__(self.callback, **self.__original_kwargs__)\n        return self._ensure_assignment_on_copy(ret)", "response": "Creates a copy of this : class : Command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean_params(self):\n        result = self.params.copy()\n        if self.cog is not None:\n            # first parameter is self\n            result.popitem(last=False)\n\n        try:\n            # first/second parameter is context\n            result.popitem(last=False)\n        except Exception:\n            raise ValueError('Missing context parameter') from None\n\n        return result", "response": "Retrieves the OrderedDict without the context or self parameters. Useful for inspecting signature."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the fully qualified parent command name.", "response": "def full_parent_name(self):\n        \"\"\"Retrieves the fully qualified parent command name.\n\n        This the base command name required to execute it. For example,\n        in ``?one two three`` the parent name would be ``one two``.\n        \"\"\"\n        entries = []\n        command = self\n        while command.parent is not None:\n            command = command.parent\n            entries.append(command.name)\n\n        return ' '.join(reversed(entries))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parents(self):\n        entries = []\n        command = self\n        while command.parent is not None:\n            command = command.parent\n            entries.append(command)\n\n        return entries", "response": "Retrieves the parents of this command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the fully qualified name of the command name.", "response": "def qualified_name(self):\n        \"\"\"Retrieves the fully qualified command name.\n\n        This is the full parent name with the command name as well.\n        For example, in ``?one two three`` the qualified name would be\n        ``one two three``.\n        \"\"\"\n\n        parent = self.full_parent_name\n        if parent:\n            return parent + ' ' + self.name\n        else:\n            return self.name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether the command is currently on cooldown.", "response": "def is_on_cooldown(self, ctx):\n        \"\"\"Checks whether the command is currently on cooldown.\n\n        Parameters\n        -----------\n        ctx: :class:`.Context.`\n            The invocation context to use when checking the commands cooldown status.\n\n        Returns\n        --------\n        :class:`bool`\n            A boolean indicating if the command is on cooldown.\n        \"\"\"\n        if not self._buckets.valid:\n            return False\n\n        bucket = self._buckets.get_bucket(ctx.message)\n        return bucket.get_tokens() == 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset_cooldown(self, ctx):\n        if self._buckets.valid:\n            bucket = self._buckets.get_bucket(ctx.message)\n            bucket.reset()", "response": "Resets the cooldown on this command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef before_invoke(self, coro):\n        if not asyncio.iscoroutinefunction(coro):\n            raise TypeError('The pre-invoke hook must be a coroutine.')\n\n        self._before_invoke = coro\n        return coro", "response": "A decorator that registers a coroutine as a pre - invoke hook."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the short documentation of a command.", "response": "def short_doc(self):\n        \"\"\"Gets the \"short\" documentation of a command.\n\n        By default, this is the :attr:`brief` attribute.\n        If that lookup leads to an empty string then the first line of the\n        :attr:`help` attribute is used instead.\n        \"\"\"\n        if self.brief is not None:\n            return self.brief\n        if self.help is not None:\n            return self.help.split('\\n', 1)[0]\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a POSIX - like signature useful for help command output.", "response": "def signature(self):\n        \"\"\"Returns a POSIX-like signature useful for help command output.\"\"\"\n        if self.usage is not None:\n            return self.usage\n\n\n        params = self.clean_params\n        if not params:\n            return ''\n\n        result = []\n        for name, param in params.items():\n            greedy = isinstance(param.annotation, converters._Greedy)\n\n            if param.default is not param.empty:\n                # We don't want None or '' to trigger the [name=value] case and instead it should\n                # do [name] since [name=None] or [name=] are not exactly useful for the user.\n                should_print = param.default if isinstance(param.default, str) else param.default is not None\n                if should_print:\n                    result.append('[%s=%s]' % (name, param.default) if not greedy else\n                                  '[%s=%s]...' % (name, param.default))\n                    continue\n                else:\n                    result.append('[%s]' % name)\n\n            elif param.kind == param.VAR_POSITIONAL:\n                result.append('[%s...]' % name)\n            elif greedy:\n                result.append('[%s]...' % name)\n            elif self._is_typing_optional(param.annotation):\n                result.append('[%s]' % name)\n            else:\n                result.append('<%s>' % name)\n\n        return ' '.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the command can be executed by checking all the predicates in the command s checks attribute.", "response": "async def can_run(self, ctx):\n        \"\"\"|coro|\n\n        Checks if the command can be executed by checking all the predicates\n        inside the :attr:`.checks` attribute.\n\n        Parameters\n        -----------\n        ctx: :class:`.Context`\n            The ctx of the command currently being invoked.\n\n        Raises\n        -------\n        :class:`CommandError`\n            Any command error that was raised during a check call will be propagated\n            by this function.\n\n        Returns\n        --------\n        :class:`bool`\n            A boolean indicating if the command can be invoked.\n        \"\"\"\n\n        original = ctx.command\n        ctx.command = self\n\n        try:\n            if not await ctx.bot.can_run(ctx):\n                raise CheckFailure('The global check functions for command {0.qualified_name} failed.'.format(self))\n\n            cog = self.cog\n            if cog is not None:\n                local_check = Cog._get_overridden_method(cog.cog_check)\n                if local_check is not None:\n                    ret = await discord.utils.maybe_coroutine(local_check, ctx)\n                    if not ret:\n                        return False\n\n            predicates = self.checks\n            if not predicates:\n                # since we have no checks, then we just return True.\n                return True\n\n            return await discord.utils.async_all(predicate(ctx) for predicate in predicates)\n        finally:\n            ctx.command = original"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a command to the internal list of commands.", "response": "def add_command(self, command):\n        \"\"\"Adds a :class:`.Command` or its subclasses into the internal list\n        of commands.\n\n        This is usually not called, instead the :meth:`~.GroupMixin.command` or\n        :meth:`~.GroupMixin.group` shortcut decorators are used instead.\n\n        Parameters\n        -----------\n        command\n            The command to add.\n\n        Raises\n        -------\n        :exc:`.ClientException`\n            If the command is already registered.\n        TypeError\n            If the command passed is not a subclass of :class:`.Command`.\n        \"\"\"\n\n        if not isinstance(command, Command):\n            raise TypeError('The command passed must be a subclass of Command')\n\n        if isinstance(self, Command):\n            command.parent = self\n\n        if command.name in self.all_commands:\n            raise discord.ClientException('Command {0.name} is already registered.'.format(command))\n\n        self.all_commands[command.name] = command\n        for alias in command.aliases:\n            if alias in self.all_commands:\n                raise discord.ClientException('The alias {} is already an existing command or alias.'.format(alias))\n            self.all_commands[alias] = command"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_command(self, name):\n        command = self.all_commands.pop(name, None)\n\n        # does not exist\n        if command is None:\n            return None\n\n        if name in command.aliases:\n            # we're removing an alias so we don't want to remove the rest\n            return command\n\n        # we're not removing the alias so let's delete the rest of them.\n        for alias in command.aliases:\n            self.all_commands.pop(alias, None)\n        return command", "response": "Removes a command from the internal list of commands."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_command(self, name):\n\n        # fast path, no space in name.\n        if ' ' not in name:\n            return self.all_commands.get(name)\n\n        names = name.split()\n        obj = self.all_commands.get(names[0])\n        if not isinstance(obj, GroupMixin):\n            return obj\n\n        for name in names[1:]:\n            try:\n                obj = obj.all_commands[name]\n            except (AttributeError, KeyError):\n                return None\n\n        return obj", "response": "Get a : class :. Command or subclasses from the internal list of commands."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef command(self, *args, **kwargs):\n        def decorator(func):\n            kwargs.setdefault('parent', self)\n            result = command(*args, **kwargs)(func)\n            self.add_command(result)\n            return result\n\n        return decorator", "response": "A decorator that invokes a function and adds it to the internal command list via the GroupMixin. add_command method."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy(self):\n        ret = super().copy()\n        for cmd in self.commands:\n            ret.add_command(cmd.copy())\n        return ret", "response": "Creates a copy of this group."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def read(self):\n        if not self._url:\n            raise DiscordException('Invalid asset (no URL provided)')\n\n        if self._state is None:\n            raise DiscordException('Invalid state (no ConnectionState provided)')\n\n        return await self._state.http.get_from_cdn(self._url)", "response": "Read the content of this asset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def save(self, fp, *, seek_begin=True):\n\n        data = await self.read()\n        if isinstance(fp, io.IOBase) and fp.writable():\n            written = fp.write(data)\n            if seek_begin:\n                fp.seek(0)\n            return written\n        else:\n            with open(fp, 'wb') as f:\n                return f.write(data)", "response": "|coro| Save this asset into a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a main websocket for Discord from a client.", "response": "async def from_client(cls, client, *, shard_id=None, session=None, sequence=None, resume=False):\n        \"\"\"Creates a main websocket for Discord from a :class:`Client`.\n\n        This is for internal use only.\n        \"\"\"\n        gateway = await client.http.get_gateway()\n        ws = await websockets.connect(gateway, loop=client.loop, klass=cls, compression=None)\n\n        # dynamically add attributes needed\n        ws.token = client.http.token\n        ws._connection = client._connection\n        ws._dispatch = client.dispatch\n        ws.gateway = gateway\n        ws.shard_id = shard_id\n        ws.shard_count = client._connection.shard_count\n        ws.session_id = session\n        ws.sequence = sequence\n        ws._max_heartbeat_timeout = client._connection.heartbeat_timeout\n\n        client._connection._update_references(ws)\n\n        log.info('Created websocket connected to %s', gateway)\n\n        # poll event for OP Hello\n        await ws.poll_event()\n\n        if not resume:\n            await ws.identify()\n            return ws\n\n        await ws.resume()\n        try:\n            await ws.ensure_open()\n        except websockets.exceptions.ConnectionClosed:\n            # ws got closed so let's just do a regular IDENTIFY connect.\n            log.info('RESUME failed (the websocket decided to close) for Shard ID %s. Retrying.', shard_id)\n            return await cls.from_client(client, shard_id=shard_id)\n        else:\n            return ws"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits for a DISPATCHD event that meets the predicate.", "response": "def wait_for(self, event, predicate, result=None):\n        \"\"\"Waits for a DISPATCH'd event that meets the predicate.\n\n        Parameters\n        -----------\n        event: :class:`str`\n            The event name in all upper case to wait for.\n        predicate\n            A function that takes a data parameter to check for event\n            properties. The data parameter is the 'd' key in the JSON message.\n        result\n            A function that takes the same data parameter and executes to send\n            the result to the future. If None, returns the data.\n\n        Returns\n        --------\n        asyncio.Future\n            A future to wait for.\n        \"\"\"\n\n        future = self.loop.create_future()\n        entry = EventListener(event=event, predicate=predicate, result=result, future=future)\n        self._dispatch_listeners.append(entry)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends the IDENTIFY packet.", "response": "async def identify(self):\n        \"\"\"Sends the IDENTIFY packet.\"\"\"\n        payload = {\n            'op': self.IDENTIFY,\n            'd': {\n                'token': self.token,\n                'properties': {\n                    '$os': sys.platform,\n                    '$browser': 'discord.py',\n                    '$device': 'discord.py',\n                    '$referrer': '',\n                    '$referring_domain': ''\n                },\n                'compress': True,\n                'large_threshold': 250,\n                'v': 3\n            }\n        }\n\n        if not self._connection.is_bot:\n            payload['d']['synced_guilds'] = []\n\n        if self.shard_id is not None and self.shard_count is not None:\n            payload['d']['shard'] = [self.shard_id, self.shard_count]\n\n        state = self._connection\n        if state._activity is not None or state._status is not None:\n            payload['d']['presence'] = {\n                'status': state._status,\n                'game': state._activity,\n                'since': 0,\n                'afk': False\n            }\n\n        await self.send_as_json(payload)\n        log.info('Shard ID %s has sent the IDENTIFY payload.', self.shard_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def resume(self):\n        payload = {\n            'op': self.RESUME,\n            'd': {\n                'seq': self.sequence,\n                'session_id': self.session_id,\n                'token': self.token\n            }\n        }\n\n        await self.send_as_json(payload)\n        log.info('Shard ID %s has sent the RESUME payload.', self.shard_id)", "response": "Sends the RESUME packet."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def poll_event(self):\n        try:\n            msg = await self.recv()\n            await self.received_message(msg)\n        except websockets.exceptions.ConnectionClosed as exc:\n            if self._can_handle_close(exc.code):\n                log.info('Websocket closed with %s (%s), attempting a reconnect.', exc.code, exc.reason)\n                raise ResumeWebSocket(self.shard_id) from exc\n            else:\n                log.info('Websocket closed with %s (%s), cannot reconnect.', exc.code, exc.reason)\n                raise ConnectionClosed(exc, shard_id=self.shard_id) from exc", "response": "Polls for a DISPATCH event and handles the general gateway loop."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def from_client(cls, client, *, resume=False):\n        gateway = 'wss://' + client.endpoint + '/?v=4'\n        ws = await websockets.connect(gateway, loop=client.loop, klass=cls, compression=None)\n        ws.gateway = gateway\n        ws._connection = client\n        ws._max_heartbeat_timeout = 60.0\n\n        if resume:\n            await ws.resume()\n        else:\n            await ws.identify()\n\n        return ws", "response": "Creates a voice websocket for the given client."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear(self):\n        if self.prefix is not None:\n            self._current_page = [self.prefix]\n            self._count = len(self.prefix) + 1 # prefix + newline\n        else:\n            self._current_page = []\n            self._count = 0\n        self._pages = []", "response": "Clears the paginator to have no pages."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a line to the current page.", "response": "def add_line(self, line='', *, empty=False):\n        \"\"\"Adds a line to the current page.\n\n        If the line exceeds the :attr:`max_size` then an exception\n        is raised.\n\n        Parameters\n        -----------\n        line: :class:`str`\n            The line to add.\n        empty: :class:`bool`\n            Indicates if another empty line should be added.\n\n        Raises\n        ------\n        RuntimeError\n            The line was too big for the current :attr:`max_size`.\n        \"\"\"\n        max_page_size = self.max_size - self._prefix_len - 2\n        if len(line) > max_page_size:\n            raise RuntimeError('Line exceeds maximum page size %s' % (max_page_size))\n\n        if self._count + len(line) + 1 > self.max_size:\n            self.close_page()\n\n        self._count += len(line) + 1\n        self._current_page.append(line)\n\n        if empty:\n            self._current_page.append('')\n            self._count += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close_page(self):\n        if self.suffix is not None:\n            self._current_page.append(self.suffix)\n        self._pages.append('\\n'.join(self._current_page))\n\n        if self.prefix is not None:\n            self._current_page = [self.prefix]\n            self._count = len(self.prefix) + 1 # prefix + newline\n        else:\n            self._current_page = []\n            self._count = 0", "response": "Prematurely terminate a page."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the bot mapping passed to send_bot_help.", "response": "def get_bot_mapping(self):\n        \"\"\"Retrieves the bot mapping passed to :meth:`send_bot_help`.\"\"\"\n        bot = self.context.bot\n        mapping = {\n            cog: cog.get_commands()\n            for cog in bot.cogs.values()\n        }\n        mapping[None] = [c for c in bot.all_commands.values() if c.cog is None]\n        return mapping"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef invoked_with(self):\n        command_name = self._command_impl.name\n        ctx = self.context\n        if ctx is None or ctx.command is None or ctx.command.qualified_name != command_name:\n            return command_name\n        return ctx.invoked_with", "response": "This method returns the internal command name that was invoked by this command."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the signature portion of the help page.", "response": "def get_command_signature(self, command):\n        \"\"\"Retrieves the signature portion of the help page.\n\n        Parameters\n        ------------\n        command: :class:`Command`\n            The command to get the signature of.\n\n        Returns\n        --------\n        :class:`str`\n            The signature for the command.\n        \"\"\"\n\n        parent = command.full_parent_name\n        if len(command.aliases) > 0:\n            aliases = '|'.join(command.aliases)\n            fmt = '[%s|%s]' % (command.name, aliases)\n            if parent:\n                fmt = parent + ' ' + fmt\n            alias = fmt\n        else:\n            alias = command.name if not parent else parent + ' ' + command.name\n\n        return '%s%s %s' % (self.clean_prefix, alias, command.signature)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_mentions(self, string):\n\n        def replace(obj, *, transforms=self.MENTION_TRANSFORMS):\n            return transforms.get(obj.group(0), '@invalid')\n\n        return self.MENTION_PATTERN.sub(replace, string)", "response": "Removes mentions from the string to prevent abuse."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef subcommand_not_found(self, command, string):\n        if isinstance(command, Group) and len(command.all_commands) > 0:\n            return 'Command \"{0.qualified_name}\" has no subcommand named {1}'.format(command, string)\n        return 'Command \"{0.qualified_name}\" has no subcommands.'.format(command)", "response": "A method called when a command does not have a subcommand named string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_max_size(self, commands):\n\n        as_lengths = (\n            discord.utils._string_width(c.name)\n            for c in commands\n        )\n        return max(as_lengths, default=0)", "response": "Returns the largest name length of the specified command list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshortens text to fit into the width.", "response": "def shorten_text(self, text):\n        \"\"\"Shortens text to fit into the :attr:`width`.\"\"\"\n        if len(text) > self.width:\n            return text[:self.width - 3] + '...'\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nindent a list of commands after the specified heading.", "response": "def add_indented_commands(self, commands, *, heading, max_size=None):\n        \"\"\"Indents a list of commands after the specified heading.\n\n        The formatting is added to the :attr:`paginator`.\n\n        The default implementation is the command name indented by\n        :attr:`indent` spaces, padded to ``max_size`` followed by\n        the command's :attr:`Command.short_doc` and then shortened\n        to fit into the :attr:`width`.\n\n        Parameters\n        -----------\n        commands: Sequence[:class:`Command`]\n            A list of commands to indent for output.\n        heading: :class:`str`\n            The heading to add to the output. This is only added\n            if the list of commands is greater than 0.\n        max_size: Optional[:class:`int`]\n            The max size to use for the gap between indents.\n            If unspecified, calls :meth:`get_max_size` on the\n            commands parameter.\n        \"\"\"\n\n        if not commands:\n            return\n\n        self.paginator.add_line(heading)\n        max_size = max_size or self.get_max_size(commands)\n\n        get_width = discord.utils._string_width\n        for command in commands:\n            name = command.name\n            width = max_size - (get_width(name) - len(name))\n            entry = '{0}{1:<{width}} {2}'.format(self.indent * ' ', name, command.short_doc, width=width)\n            self.paginator.add_line(self.shorten_text(entry))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def send_pages(self):\n        destination = self.get_destination()\n        for page in self.paginator.pages:\n            await destination.send(page)", "response": "A helper utility to send the pages from the paginator to the destination."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the minified bot heading with commands to the output.", "response": "def add_bot_commands_formatting(self, commands, heading):\n        \"\"\"Adds the minified bot heading with commands to the output.\n\n        The formatting should be added to the :attr:`paginator`.\n\n        The default implementation is a bold underline heading followed\n        by commands separated by an EN SPACE (U+2002) in the next line.\n\n        Parameters\n        -----------\n        commands: Sequence[:class:`Command`]\n            A list of commands that belong to the heading.\n        heading: :class:`str`\n            The heading to add to the line.\n        \"\"\"\n        if commands:\n            # U+2002 Middle Dot\n            joined = '\\u2002'.join(c.name for c in commands)\n            self.paginator.add_line('__**%s**__' % heading)\n            self.paginator.add_line(joined)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_subcommand_formatting(self, command):\n        fmt = '{0}{1} \\N{EN DASH} {2}' if command.short_doc else '{0}{1}'\n        self.paginator.add_line(fmt.format(self.clean_prefix, command.qualified_name, command.short_doc))", "response": "Adds formatting information on a subcommand."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the formatting information on a command s aliases.", "response": "def add_aliases_formatting(self, aliases):\n        \"\"\"Adds the formatting information on a command's aliases.\n\n        The formatting should be added to the :attr:`paginator`.\n\n        The default implementation is the :attr:`aliases_heading` bolded\n        followed by a comma separated list of aliases.\n\n        This is not called if there are no aliases to format.\n\n        Parameters\n        -----------\n        aliases: Sequence[:class:`str`]\n            A list of aliases to format.\n        \"\"\"\n        self.paginator.add_line('**%s** %s' % (self.aliases_heading, ', '.join(aliases)), empty=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_command_formatting(self, command):\n\n        if command.description:\n            self.paginator.add_line(command.description, empty=True)\n\n        signature = self.get_command_signature(command)\n        if command.aliases:\n            self.paginator.add_line(signature)\n            self.add_aliases_formatting(command.aliases)\n        else:\n            self.paginator.add_line(signature, empty=True)\n\n        if command.help:\n            try:\n                self.paginator.add_line(command.help, empty=True)\n            except RuntimeError:\n                for line in command.help.splitlines():\n                    self.paginator.add_line(line)\n                self.paginator.add_line()", "response": "A utility function to format commands and groups."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def disconnect(self, *, force=False):\n        if not force and not self.is_connected():\n            return\n\n        self.stop()\n        self._connected.clear()\n\n        try:\n            if self.ws:\n                await self.ws.close()\n\n            await self.terminate_handshake(remove=True)\n        finally:\n            if self.socket:\n                self.socket.close()", "response": "Disconnects this voice client from voice."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove you to a different voice channel.", "response": "async def move_to(self, channel):\n        \"\"\"|coro|\n\n        Moves you to a different voice channel.\n\n        Parameters\n        -----------\n        channel: :class:`abc.Snowflake`\n            The channel to move to. Must be a voice channel.\n        \"\"\"\n        guild_id, _ = self.channel._get_voice_state_pair()\n        await self.main_ws.voice_state(guild_id, channel.id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplay an audio source.", "response": "def play(self, source, *, after=None):\n        \"\"\"Plays an :class:`AudioSource`.\n\n        The finalizer, ``after`` is called after the source has been exhausted\n        or an error occurred.\n\n        If an error happens while the audio player is running, the exception is\n        caught and the audio player is then stopped.\n\n        Parameters\n        -----------\n        source: :class:`AudioSource`\n            The audio source we're reading from.\n        after\n            The finalizer that is called after the stream is exhausted.\n            All exceptions it throws are silently discarded. This function\n            must have a single parameter, ``error``, that denotes an\n            optional exception that was raised during playing.\n\n        Raises\n        -------\n        ClientException\n            Already playing audio or not connected.\n        TypeError\n            source is not a :class:`AudioSource` or after is not a callable.\n        \"\"\"\n\n        if not self.is_connected():\n            raise ClientException('Not connected to voice.')\n\n        if self.is_playing():\n            raise ClientException('Already playing audio.')\n\n        if not isinstance(source, AudioSource):\n            raise TypeError('source must an AudioSource not {0.__class__.__name__}'.format(source))\n\n        self._player = AudioPlayer(source, self, after=after)\n        self._player.start()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_audio_packet(self, data, *, encode=True):\n\n        self.checked_add('sequence', 1, 65535)\n        if encode:\n            encoded_data = self.encoder.encode(data, self.encoder.SAMPLES_PER_FRAME)\n        else:\n            encoded_data = data\n        packet = self._get_voice_packet(encoded_data)\n        try:\n            self.socket.sendto(packet, (self.endpoint_ip, self.voice_port))\n        except BlockingIOError:\n            log.warning('A packet has been dropped (seq: %s, timestamp: %s)', self.sequence, self.timestamp)\n\n        self.checked_add('timestamp', self.encoder.SAMPLES_PER_FRAME, 4294967295)", "response": "Sends an audio packet to the connected device."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrequest previously offline members from the given guilds.", "response": "async def request_offline_members(self, *guilds):\n        r\"\"\"|coro|\n\n        Requests previously offline members from the guild to be filled up\n        into the :attr:`.Guild.members` cache. This function is usually not\n        called. It should only be used if you have the ``fetch_offline_members``\n        parameter set to ``False``.\n\n        When the client logs on and connects to the websocket, Discord does\n        not provide the library with offline members if the number of members\n        in the guild is larger than 250. You can check if a guild is large\n        if :attr:`.Guild.large` is ``True``.\n\n        Parameters\n        -----------\n        \\*guilds: :class:`Guild`\n            An argument list of guilds to request offline members for.\n\n        Raises\n        -------\n        InvalidArgument\n            If any guild is unavailable or not large in the collection.\n        \"\"\"\n        if any(not g.large or g.unavailable for g in guilds):\n            raise InvalidArgument('An unavailable or non-large guild was passed.')\n\n        await self._connection.request_offline_members(guilds)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlog in using a user token.", "response": "async def login(self, token, *, bot=True):\n        \"\"\"|coro|\n\n        Logs in the client with the specified credentials.\n\n        This function can be used in two different ways.\n\n        .. warning::\n\n            Logging on with a user token is against the Discord\n            `Terms of Service <https://support.discordapp.com/hc/en-us/articles/115002192352>`_\n            and doing so might potentially get your account banned.\n            Use this at your own risk.\n\n        Parameters\n        -----------\n        token: :class:`str`\n            The authentication token. Do not prefix this token with\n            anything as the library will do it for you.\n        bot: :class:`bool`\n            Keyword argument that specifies if the account logging on is a bot\n            token or not.\n\n        Raises\n        ------\n        LoginFailure\n            The wrong credentials are passed.\n        HTTPException\n            An unknown HTTP related error occurred,\n            usually when it isn't 200 or the known incorrect credentials\n            passing status code.\n        \"\"\"\n\n        log.info('logging in using static token')\n        await self.http.static_login(token, bot=bot)\n        self._connection.is_bot = bot"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def connect(self, *, reconnect=True):\n\n        backoff = ExponentialBackoff()\n        while not self.is_closed():\n            try:\n                await self._connect()\n            except (OSError,\n                    HTTPException,\n                    GatewayNotFound,\n                    ConnectionClosed,\n                    aiohttp.ClientError,\n                    asyncio.TimeoutError,\n                    websockets.InvalidHandshake,\n                    websockets.WebSocketProtocolError) as exc:\n\n                self.dispatch('disconnect')\n                if not reconnect:\n                    await self.close()\n                    if isinstance(exc, ConnectionClosed) and exc.code == 1000:\n                        # clean close, don't re-raise this\n                        return\n                    raise\n\n                if self.is_closed():\n                    return\n\n                # We should only get this when an unhandled close code happens,\n                # such as a clean disconnect (1000) or a bad state (bad token, no sharding, etc)\n                # sometimes, discord sends us 1000 for unknown reasons so we should reconnect\n                # regardless and rely on is_closed instead\n                if isinstance(exc, ConnectionClosed):\n                    if exc.code != 1000:\n                        await self.close()\n                        raise\n\n                retry = backoff.delay()\n                log.exception(\"Attempting a reconnect in %.2fs\", retry)\n                await asyncio.sleep(retry, loop=self.loop)", "response": "Connect to the discord server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def close(self):\n        if self._closed:\n            return\n\n        await self.http.close()\n        self._closed = True\n\n        for voice in self.voice_clients:\n            try:\n                await voice.disconnect()\n            except Exception:\n                # if an error happens during disconnects, disregard it.\n                pass\n\n        if self.ws is not None and self.ws.open:\n            await self.ws.close()\n\n        self._ready.clear()", "response": "Closes the connection to discord."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clear(self):\n        self._closed = False\n        self._ready.clear()\n        self._connection.clear()\n        self.http.recreate()", "response": "Clears the internal state of the bot."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def start(self, *args, **kwargs):\n\n        bot = kwargs.pop('bot', True)\n        reconnect = kwargs.pop('reconnect', True)\n        await self.login(*args, bot=bot)\n        await self.connect(reconnect=reconnect)", "response": "A shorthand for login + connect +."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait_for(self, event, *, check=None, timeout=None):\n\n        future = self.loop.create_future()\n        if check is None:\n            def _check(*args):\n                return True\n            check = _check\n\n        ev = event.lower()\n        try:\n            listeners = self._listeners[ev]\n        except KeyError:\n            listeners = []\n            self._listeners[ev] = listeners\n\n        listeners.append((future, check))\n        return asyncio.wait_for(future, timeout, loop=self.loop)", "response": "A coroutine function that waits for an event to be dispatched."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging presence of a user.", "response": "async def change_presence(self, *, activity=None, status=None, afk=False):\n        \"\"\"|coro|\n\n        Changes the client's presence.\n\n        The activity parameter is a :class:`.Activity` object (not a string) that represents\n        the activity being done currently. This could also be the slimmed down versions,\n        :class:`.Game` and :class:`.Streaming`.\n\n        Example\n        ---------\n\n        .. code-block:: python3\n\n            game = discord.Game(\"with the API\")\n            await client.change_presence(status=discord.Status.idle, activity=game)\n\n        Parameters\n        ----------\n        activity: Optional[Union[:class:`.Game`, :class:`.Streaming`, :class:`.Activity`]]\n            The activity being done. ``None`` if no currently active activity is done.\n        status: Optional[:class:`.Status`]\n            Indicates what status to change to. If None, then\n            :attr:`.Status.online` is used.\n        afk: :class:`bool`\n            Indicates if you are going AFK. This allows the discord\n            client to know how to handle push notifications better\n            for you in case you are actually idle and not lying.\n\n        Raises\n        ------\n        InvalidArgument\n            If the ``activity`` parameter is not the proper type.\n        \"\"\"\n\n        if status is None:\n            status = 'online'\n            status_enum = Status.online\n        elif status is Status.offline:\n            status = 'invisible'\n            status_enum = Status.offline\n        else:\n            status_enum = status\n            status = str(status)\n\n        await self.ws.change_presence(activity=activity, status=status, afk=afk)\n\n        for guild in self._connection.guilds:\n            me = guild.me\n            if me is None:\n                continue\n\n            me.activities = (activity,)\n            me.status = status_enum"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def fetch_guild(self, guild_id):\n        data = await self.http.get_guild(guild_id)\n        return Guild(data=data, state=self._connection)", "response": "Get a guild from an ID."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def create_guild(self, name, region=None, icon=None):\n        if icon is not None:\n            icon = utils._bytes_to_base64_data(icon)\n\n        if region is None:\n            region = VoiceRegion.us_west.value\n        else:\n            region = region.value\n\n        data = await self.http.create_guild(name, region, icon)\n        return Guild(data=data, state=self._connection)", "response": "Create a new guild."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def fetch_invite(self, url, *, with_counts=True):\n\n        invite_id = utils.resolve_invite(url)\n        data = await self.http.get_invite(invite_id, with_counts=with_counts)\n        return Invite.from_incomplete(state=self._connection, data=data)", "response": "Fetch an invite from a discord.gg URL or ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def delete_invite(self, invite):\n\n        invite_id = utils.resolve_invite(invite)\n        await self.http.delete_invite(invite_id)", "response": "|coro|\n\n        Revokes an :class:`.Invite`, URL, or ID to an invite.\n\n        You must have the :attr:`~.Permissions.manage_channels` permission in\n        the associated guild to do this.\n\n        Parameters\n        ----------\n        invite: Union[:class:`.Invite`, :class:`str`]\n            The invite to revoke.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to revoke invites.\n        NotFound\n            The invite is invalid or expired.\n        HTTPException\n            Revoking the invite failed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def fetch_widget(self, guild_id):\n        data = await self.http.get_widget(guild_id)\n\n        return Widget(state=self._connection, data=data)", "response": "Get a widget from a guild."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving s the bot s application information.", "response": "async def application_info(self):\n        \"\"\"|coro|\n\n        Retrieve's the bot's application information.\n\n        Raises\n        -------\n        HTTPException\n            Retrieving the information failed somehow.\n\n        Returns\n        --------\n        :class:`.AppInfo`\n            A namedtuple representing the application info.\n        \"\"\"\n        data = await self.http.application_info()\n        if 'rpc_origins' not in data:\n            data['rpc_origins'] = None\n        return AppInfo(self._connection, data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching a user by its ID.", "response": "async def fetch_user(self, user_id):\n        \"\"\"|coro|\n\n        Retrieves a :class:`~discord.User` based on their ID. This can only\n        be used by bot accounts. You do not have to share any guilds\n        with the user to get this information, however many operations\n        do require that you do.\n\n        .. note::\n\n            This method is an API call. For general usage, consider :meth:`get_user` instead.\n\n        Parameters\n        -----------\n        user_id: :class:`int`\n            The user's ID to fetch from.\n\n        Raises\n        -------\n        NotFound\n            A user with this ID does not exist.\n        HTTPException\n            Fetching the user failed.\n\n        Returns\n        --------\n        :class:`~discord.User`\n            The user you requested.\n        \"\"\"\n        data = await self.http.get_user(user_id)\n        return User(state=self._connection, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def fetch_user_profile(self, user_id):\n\n        state = self._connection\n        data = await self.http.get_user_profile(user_id)\n\n        def transform(d):\n            return state._get_guild(int(d['id']))\n\n        since = data.get('premium_since')\n        mutual_guilds = list(filter(None, map(transform, data.get('mutual_guilds', []))))\n        user = data['user']\n        return Profile(flags=user.get('flags', 0),\n                       premium_since=utils.parse_time(since),\n                       mutual_guilds=mutual_guilds,\n                       user=User(data=user, state=state),\n                       connected_accounts=data['connected_accounts'])", "response": "Fetch a user s profile."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching a webhook from the API.", "response": "async def fetch_webhook(self, webhook_id):\n        \"\"\"|coro|\n\n        Retrieves a :class:`.Webhook` with the specified ID.\n\n        Raises\n        --------\n        HTTPException\n            Retrieving the webhook failed.\n        NotFound\n            Invalid webhook ID.\n        Forbidden\n            You do not have permission to fetch this webhook.\n\n        Returns\n        ---------\n        :class:`.Webhook`\n            The webhook you requested.\n        \"\"\"\n        data = await self.http.get_webhook(webhook_id)\n        return Webhook.from_state(data, state=self._connection)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the URL of the icon for this guild.", "response": "def icon_url_as(self, *, format='webp', size=1024):\n        \"\"\":class:`Asset`: The same operation as :meth:`Guild.icon_url_as`.\"\"\"\n        return Asset._from_guild_image(self._state, self.id, self.icon, 'icons', format=format, size=size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef banner_url_as(self, *, format='webp', size=2048):\n        return Asset._from_guild_image(self._state, self.id, self.banner, 'banners', format=format, size=size)", "response": "Return the URL of the banner of the guild."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef splash_url_as(self, *, format='webp', size=2048):\n        return Asset._from_guild_image(self._state, self.id, self.splash, 'splashes', format=format, size=size)", "response": "Returns the URL of the splash image of the guild."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting this instant invite.", "response": "async def delete(self, *, reason=None):\n        \"\"\"|coro|\n\n        Revokes the instant invite.\n\n        You must have the :attr:`~Permissions.manage_channels` permission to do this.\n\n        Parameters\n        -----------\n        reason: Optional[:class:`str`]\n            The reason for deleting this invite. Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to revoke invites.\n        NotFound\n            The invite is invalid or expired.\n        HTTPException\n            Revoking the invite failed.\n        \"\"\"\n\n        await self._state.http.delete_invite(self.code, reason=reason)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving this attachment into a file - like object.", "response": "async def save(self, fp, *, seek_begin=True, use_cached=False):\n        \"\"\"|coro|\n\n        Saves this attachment into a file-like object.\n\n        Parameters\n        -----------\n        fp: Union[BinaryIO, :class:`os.PathLike`]\n            The file-like object to save this attachment to or the filename\n            to use. If a filename is passed then a file is created with that\n            filename and used instead.\n        seek_begin: :class:`bool`\n            Whether to seek to the beginning of the file after saving is\n            successfully done.\n        use_cached: :class:`bool`\n            Whether to use :attr:`proxy_url` rather than :attr:`url` when downloading\n            the attachment. This will allow attachments to be saved after deletion\n            more often, compared to the regular URL which is generally deleted right\n            after the message is deleted. Note that this can still fail to download\n            deleted attachments if too much time has passed and it does not work\n            on some type of attachments.\n\n        Raises\n        --------\n        HTTPException\n            Saving the attachment failed.\n        NotFound\n            The attachment was deleted.\n\n        Returns\n        --------\n        :class:`int`\n            The number of bytes written.\n        \"\"\"\n        url = self.proxy_url if use_cached else self.url\n        data = await self._http.get_from_cdn(url)\n        if isinstance(fp, io.IOBase) and fp.writable():\n            written = fp.write(data)\n            if seek_begin:\n                fp.seek(0)\n            return written\n        else:\n            with open(fp, 'wb') as f:\n                return f.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raw_mentions(self):\n        return [int(x) for x in re.findall(r'<@!?([0-9]+)>', self.content)]", "response": "A property that returns an array of user IDs matched with\n        the syntax of <@user_id > in the message content."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef raw_role_mentions(self):\n        return [int(x) for x in re.findall(r'<@&([0-9]+)>', self.content)]", "response": "A property that returns an array of role IDs matched with\n        the syntax of <@&role_id > in the message content."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def delete(self, *, delay=None):\n        if delay is not None:\n            async def delete():\n                await asyncio.sleep(delay, loop=self._state.loop)\n                try:\n                    await self._state.http.delete_message(self.channel.id, self.id)\n                except HTTPException:\n                    pass\n\n            asyncio.ensure_future(delete(), loop=self._state.loop)\n        else:\n            await self._state.http.delete_message(self.channel.id, self.id)", "response": "|coro|\n\n        Deletes the message.\n\n        Your own messages could be deleted without any proper permissions. However to\n        delete other people's messages, you need the :attr:`~Permissions.manage_messages`\n        permission.\n\n        .. versionchanged:: 1.1.0\n            Added the new ``delay`` keyword-only parameter.\n\n        Parameters\n        -----------\n        delay: Optional[:class:`float`]\n            If provided, the number of seconds to wait in the background\n            before deleting the message.\n\n        Raises\n        ------\n        Forbidden\n            You do not have proper permissions to delete the message.\n        HTTPException\n            Deleting the message failed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def edit(self, **fields):\n\n        try:\n            content = fields['content']\n        except KeyError:\n            pass\n        else:\n            if content is not None:\n                fields['content'] = str(content)\n\n        try:\n            embed = fields['embed']\n        except KeyError:\n            pass\n        else:\n            if embed is not None:\n                fields['embed'] = embed.to_dict()\n\n        data = await self._state.http.edit_message(self.channel.id, self.id, **fields)\n        self._update(channel=self.channel, data=data)\n\n        try:\n            delete_after = fields['delete_after']\n        except KeyError:\n            pass\n        else:\n            if delete_after is not None:\n                await self.delete(delay=delete_after)", "response": "|coro|\n\n        Edits the message.\n\n        The content must be able to be transformed into a string via ``str(content)``.\n\n        Parameters\n        -----------\n        content: Optional[:class:`str`]\n            The new content to replace the message with.\n            Could be ``None`` to remove the content.\n        embed: Optional[:class:`Embed`]\n            The new embed to replace the original with.\n            Could be ``None`` to remove the embed.\n        delete_after: Optional[:class:`float`]\n            If provided, the number of seconds to wait in the background\n            before deleting the message we just edited. If the deletion fails,\n            then it is silently ignored.\n\n        Raises\n        -------\n        HTTPException\n            Editing the message failed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def pin(self):\n\n        await self._state.http.pin_message(self.channel.id, self.id)\n        self.pinned = True", "response": "|coro|\n\n        Pins the message.\n\n        You must have the :attr:`~Permissions.manage_messages` permission to do\n        this in a non-private channel context.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to pin the message.\n        NotFound\n            The message or channel was not found or deleted.\n        HTTPException\n            Pinning the message failed, probably due to the channel\n            having more than 50 pinned messages."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def unpin(self):\n\n        await self._state.http.unpin_message(self.channel.id, self.id)\n        self.pinned = False", "response": "|coro|\n\n        Unpins the message.\n\n        You must have the :attr:`~Permissions.manage_messages` permission to do\n        this in a non-private channel context.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to unpin the message.\n        NotFound\n            The message or channel was not found or deleted.\n        HTTPException\n            Unpinning the message failed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a reaction to the message.", "response": "async def add_reaction(self, emoji):\n        \"\"\"|coro|\n\n        Add a reaction to the message.\n\n        The emoji may be a unicode emoji or a custom guild :class:`Emoji`.\n\n        You must have the :attr:`~Permissions.read_message_history` permission\n        to use this. If nobody else has reacted to the message using this\n        emoji, the :attr:`~Permissions.add_reactions` permission is required.\n\n        Parameters\n        ------------\n        emoji: Union[:class:`Emoji`, :class:`Reaction`, :class:`PartialEmoji`, :class:`str`]\n            The emoji to react with.\n\n        Raises\n        --------\n        HTTPException\n            Adding the reaction failed.\n        Forbidden\n            You do not have the proper permissions to react to the message.\n        NotFound\n            The emoji you specified was not found.\n        InvalidArgument\n            The emoji parameter is invalid.\n        \"\"\"\n\n        emoji = self._emoji_reaction(emoji)\n        await self._state.http.add_reaction(self.channel.id, self.id, emoji)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def remove_reaction(self, emoji, member):\n\n        emoji = self._emoji_reaction(emoji)\n\n        if member.id == self._state.self_id:\n            await self._state.http.remove_own_reaction(self.channel.id, self.id, emoji)\n        else:\n            await self._state.http.remove_reaction(self.channel.id, self.id, emoji, member.id)", "response": "Remove a reaction by the member."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def clear_reactions(self):\n        await self._state.http.clear_reactions(self.channel.id, self.id)", "response": "Removes all the reactions from the message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef album_cover_url(self):\n        large_image = self._assets.get('large_image', '')\n        if large_image[:8] != 'spotify:':\n            return ''\n        album_image_id = large_image[8:]\n        return 'https://i.scdn.co/image/' + album_image_id", "response": "The album cover image URL from Spotify s CDN."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef when_mentioned_or(*prefixes):\n    def inner(bot, msg):\n        r = list(prefixes)\n        r = when_mentioned(bot, msg) + r\n        return r\n\n    return inner", "response": "A callable that implements when mentioned or other prefixes provided."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall when an exception occurs in the command.", "response": "async def on_command_error(self, context, exception):\n        \"\"\"|coro|\n\n        The default command error handler provided by the bot.\n\n        By default this prints to ``sys.stderr`` however it could be\n        overridden to have a different implementation.\n\n        This only fires if you do not specify any listeners for command error.\n        \"\"\"\n        if self.extra_events.get('on_command_error', None):\n            return\n\n        if hasattr(context.command, 'on_error'):\n            return\n\n        cog = context.cog\n        if cog:\n            if Cog._get_overridden_method(cog.cog_command_error) is not None:\n                return\n\n        print('Ignoring exception in command {}:'.format(context.command), file=sys.stderr)\n        traceback.print_exception(type(exception), exception, exception.__traceback__, file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a global check to the bot.", "response": "def add_check(self, func, *, call_once=False):\n        \"\"\"Adds a global check to the bot.\n\n        This is the non-decorator interface to :meth:`.check`\n        and :meth:`.check_once`.\n\n        Parameters\n        -----------\n        func\n            The function that was used as a global check.\n        call_once: :class:`bool`\n            If the function should only be called once per\n            :meth:`.Command.invoke` call.\n        \"\"\"\n\n        if call_once:\n            self._check_once.append(func)\n        else:\n            self._checks.append(func)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving a global check from the bot.", "response": "def remove_check(self, func, *, call_once=False):\n        \"\"\"Removes a global check from the bot.\n\n        This function is idempotent and will not raise an exception\n        if the function is not in the global checks.\n\n        Parameters\n        -----------\n        func\n            The function to remove from the global checks.\n        call_once: :class:`bool`\n            If the function was added with ``call_once=True`` in\n            the :meth:`.Bot.add_check` call or using :meth:`.check_once`.\n        \"\"\"\n        l = self._check_once if call_once else self._checks\n\n        try:\n            l.remove(func)\n        except ValueError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def is_owner(self, user):\n\n        if self.owner_id is None:\n            app = await self.application_info()\n            self.owner_id = owner_id = app.owner.id\n            return user.id == owner_id\n        return user.id == self.owner_id", "response": "Checks if a user is the owner of this bot."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving a function that was used as a listener to remove.", "response": "def remove_listener(self, func, name=None):\n        \"\"\"Removes a listener from the pool of listeners.\n\n        Parameters\n        -----------\n        func\n            The function that was used as a listener to remove.\n        name: :class:`str`\n            The name of the event we want to remove. Defaults to\n            ``func.__name__``.\n        \"\"\"\n\n        name = func.__name__ if name is None else name\n\n        if name in self.extra_events:\n            try:\n                self.extra_events[name].remove(func)\n            except ValueError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef listen(self, name=None):\n\n        def decorator(func):\n            self.add_listener(func, name)\n            return func\n\n        return decorator", "response": "A decorator that registers another function as an external\n        event listener."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a cog to the bot.", "response": "def add_cog(self, cog):\n        \"\"\"Adds a \"cog\" to the bot.\n\n        A cog is a class that has its own event listeners and commands.\n\n        Parameters\n        -----------\n        cog: :class:`.Cog`\n            The cog to register to the bot.\n\n        Raises\n        -------\n        TypeError\n            The cog does not inherit from :class:`.Cog`.\n        CommandError\n            An error happened during loading.\n        \"\"\"\n\n        if not isinstance(cog, Cog):\n            raise TypeError('cogs must derive from Cog')\n\n        cog = cog._inject(self)\n        self.__cogs[cog.__cog_name__] = cog"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_cog(self, name):\n\n        cog = self.__cogs.pop(name, None)\n        if cog is None:\n            return\n\n        help_command = self._help_command\n        if help_command and help_command.cog is cog:\n            help_command.cog = None\n        cog._eject(self)", "response": "Removes a cog from the bot."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_extension(self, name):\n\n        if name in self.__extensions:\n            raise errors.ExtensionAlreadyLoaded(name)\n\n        try:\n            lib = importlib.import_module(name)\n        except ImportError as e:\n            raise errors.ExtensionNotFound(name, e) from e\n        else:\n            self._load_from_module_spec(lib, name)", "response": "Loads an extension.\n\n        An extension is a python module that contains commands, cogs, or\n        listeners.\n\n        An extension must have a global function, ``setup`` defined as\n        the entry point on what to do when the extension is loaded. This entry\n        point must have a single argument, the ``bot``.\n\n        Parameters\n        ------------\n        name: :class:`str`\n            The extension name to load. It must be dot separated like\n            regular Python imports if accessing a sub-module. e.g.\n            ``foo.test`` if you want to import ``foo/test.py``.\n\n        Raises\n        --------\n        ExtensionNotFound\n            The extension could not be imported.\n        ExtensionAlreadyLoaded\n            The extension is already loaded.\n        NoEntryPointError\n            The extension does not have a setup function.\n        ExtensionFailed\n            The extension setup function had an execution error."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nunload an extension. When the extension is unloaded, all commands, listeners, and cogs are removed from the bot and the module is un-imported. The extension can provide an optional global function, ``teardown``, to do miscellaneous clean-up if necessary. This function takes a single parameter, the ``bot``, similar to ``setup`` from :meth:`~.Bot.load_extension`. Parameters ------------ name: :class:`str` The extension name to unload. It must be dot separated like regular Python imports if accessing a sub-module. e.g. ``foo.test`` if you want to import ``foo/test.py``. Raises ------- ExtensionNotLoaded The extension was not loaded.", "response": "def unload_extension(self, name):\n        \"\"\"Unloads an extension.\n\n        When the extension is unloaded, all commands, listeners, and cogs are\n        removed from the bot and the module is un-imported.\n\n        The extension can provide an optional global function, ``teardown``,\n        to do miscellaneous clean-up if necessary. This function takes a single\n        parameter, the ``bot``, similar to ``setup`` from\n        :meth:`~.Bot.load_extension`.\n\n        Parameters\n        ------------\n        name: :class:`str`\n            The extension name to unload. It must be dot separated like\n            regular Python imports if accessing a sub-module. e.g.\n            ``foo.test`` if you want to import ``foo/test.py``.\n\n        Raises\n        -------\n        ExtensionNotLoaded\n            The extension was not loaded.\n        \"\"\"\n\n        lib = self.__extensions.get(name)\n        if lib is None:\n            raise errors.ExtensionNotLoaded(name)\n\n        self._remove_module_references(lib.__name__)\n        self._call_module_finalizers(lib, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_prefix(self, message):\n        prefix = ret = self.command_prefix\n        if callable(prefix):\n            ret = await discord.utils.maybe_coroutine(prefix, self, message)\n\n        if not isinstance(ret, str):\n            try:\n                ret = list(ret)\n            except TypeError:\n                # It's possible that a generator raised this exception.  Don't\n                # replace it with our own error if that's the case.\n                if isinstance(ret, collections.Iterable):\n                    raise\n\n                raise TypeError(\"command_prefix must be plain string, iterable of strings, or callable \"\n                                \"returning either of these, not {}\".format(ret.__class__.__name__))\n\n            if not ret:\n                raise ValueError(\"Iterable command_prefix must contain at least one prefix\")\n\n        return ret", "response": "Get the prefix of the bot s command_prefix with the message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def get_context(self, message, *, cls=Context):\n\n        view = StringView(message.content)\n        ctx = cls(prefix=None, view=view, bot=self, message=message)\n\n        if self._skip_check(message.author.id, self.user.id):\n            return ctx\n\n        prefix = await self.get_prefix(message)\n        invoked_prefix = prefix\n\n        if isinstance(prefix, str):\n            if not view.skip_string(prefix):\n                return ctx\n        else:\n            try:\n                # if the context class' __init__ consumes something from the view this\n                # will be wrong.  That seems unreasonable though.\n                if message.content.startswith(tuple(prefix)):\n                    invoked_prefix = discord.utils.find(view.skip_string, prefix)\n                else:\n                    return ctx\n\n            except TypeError:\n                if not isinstance(prefix, list):\n                    raise TypeError(\"get_prefix must return either a string or a list of string, \"\n                                    \"not {}\".format(prefix.__class__.__name__))\n\n                # It's possible a bad command_prefix got us here.\n                for value in prefix:\n                    if not isinstance(value, str):\n                        raise TypeError(\"Iterable command_prefix or list returned from get_prefix must \"\n                                        \"contain only strings, not {}\".format(value.__class__.__name__))\n\n                # Getting here shouldn't happen\n                raise\n\n        invoker = view.get_word()\n        ctx.invoked_with = invoker\n        ctx.prefix = invoked_prefix\n        ctx.command = self.all_commands.get(invoker)\n        return ctx", "response": "Return the invocation context from the given message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def invoke(self, ctx):\n        if ctx.command is not None:\n            self.dispatch('command', ctx)\n            try:\n                if await self.can_run(ctx, call_once=True):\n                    await ctx.command.invoke(ctx)\n            except errors.CommandError as exc:\n                await ctx.command.dispatch_error(ctx, exc)\n            else:\n                self.dispatch('command_completion', ctx)\n        elif ctx.invoked_with:\n            exc = errors.CommandNotFound('Command \"{}\" is not found'.format(ctx.invoked_with))\n            self.dispatch('command_error', ctx, exc)", "response": "Invoke the command given under the invocation context and dispatches all the internal event dispatch mechanisms."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef large(self):\n        if self._large is None:\n            try:\n                return self._member_count >= 250\n            except AttributeError:\n                return len(self._members) >= 250\n        return self._large", "response": "Indicates if the guild is a large guild."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef voice_channels(self):\n        r = [ch for ch in self._channels.values() if isinstance(ch, VoiceChannel)]\n        r.sort(key=lambda c: (c.position, c.id))\n        return r", "response": "A list of voice channels that belong to this guild."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef me(self):\n        self_id = self._state.user.id\n        return self.get_member(self_id)", "response": "Similar to Client. user except an instance of Member. This is essentially used to get the member version of yourself."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef text_channels(self):\n        r = [ch for ch in self._channels.values() if isinstance(ch, TextChannel)]\n        r.sort(key=lambda c: (c.position, c.id))\n        return r", "response": "A list of text channels that belong to this guild."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef by_category(self):\n        grouped = defaultdict(list)\n        for channel in self._channels.values():\n            if isinstance(channel, CategoryChannel):\n                continue\n\n            grouped[channel.category_id].append(channel)\n\n        def key(t):\n            k, v = t\n            return ((k.position, k.id) if k else (-1, -1), v)\n\n        _get = self._channels.get\n        as_list = [(_get(k), v) for k, v in grouped.items()]\n        as_list.sort(key=key)\n        for _, channels in as_list:\n            channels.sort(key=lambda c: (c._sorting_bucket, c.position, c.id))\n        return as_list", "response": "Returns every category channel and their associated channels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef system_channel(self):\n        channel_id = self._system_channel_id\n        return channel_id and self._channels.get(channel_id)", "response": "Optional [ class : TextChannel ] : Returns the guild s system channel used for system messages."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef default_role(self):\n        return utils.find(lambda r: r.is_default(), self._roles.values())", "response": "Gets the default role that all members have by default."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a boolean indicating if the guild is chunked.", "response": "def chunked(self):\n        \"\"\"Returns a boolean indicating if the guild is \"chunked\".\n\n        A chunked guild means that :attr:`member_count` is equal to the\n        number of members stored in the internal :attr:`members` cache.\n\n        If this value returns ``False``, then you should request for\n        offline members.\n        \"\"\"\n        count = getattr(self, '_member_count', None)\n        if count is None:\n            return False\n        return count == len(self._members)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the shard ID for this guild if applicable.", "response": "def shard_id(self):\n        \"\"\"Returns the shard ID for this guild if applicable.\"\"\"\n        count = self._state.shard_count\n        if count is None:\n            return None\n        return (self.id >> 22) % count"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the first member that matches the name provided.", "response": "def get_member_named(self, name):\n        \"\"\"Returns the first member found that matches the name provided.\n\n        The name can have an optional discriminator argument, e.g. \"Jake#0001\"\n        or \"Jake\" will both do the lookup. However the former will give a more\n        precise result. Note that the discriminator must have all 4 digits\n        for this to work.\n\n        If a nickname is passed, then it is looked up via the nickname. Note\n        however, that a nickname + discriminator combo will not lookup the nickname\n        but rather the username + discriminator combo due to nickname + discriminator\n        not being unique.\n\n        If no member is found, ``None`` is returned.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The name of the member to lookup with an optional discriminator.\n\n        Returns\n        --------\n        :class:`Member`\n            The member in this guild with the associated name. If not found\n            then ``None`` is returned.\n        \"\"\"\n\n        result = None\n        members = self.members\n        if len(name) > 5 and name[-5] == '#':\n            # The 5 length is checking to see if #0000 is in the string,\n            # as a#0000 has a length of 6, the minimum for a potential\n            # discriminator lookup.\n            potential_discriminator = name[-4:]\n\n            # do the actual lookup and return if found\n            # if it isn't found then we'll do a full name lookup below.\n            result = utils.get(members, name=name[:-5], discriminator=potential_discriminator)\n            if result is not None:\n                return result\n\n        def pred(m):\n            return m.nick == name or m.name == name\n\n        return utils.find(pred, members)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new channel in the guild.", "response": "async def create_text_channel(self, name, *, overwrites=None, category=None, reason=None, **options):\n        \"\"\"|coro|\n\n        Creates a :class:`TextChannel` for the guild.\n\n        Note that you need the :attr:`~Permissions.manage_channels` permission\n        to create the channel.\n\n        The ``overwrites`` parameter can be used to create a 'secret'\n        channel upon creation. This parameter expects a :class:`dict` of\n        overwrites with the target (either a :class:`Member` or a :class:`Role`)\n        as the key and a :class:`PermissionOverwrite` as the value.\n\n        .. note::\n\n            Creating a channel of a specified position will not update the position of\n            other channels to follow suit. A follow-up call to :meth:`~TextChannel.edit`\n            will be required to update the position of the channel in the channel list.\n\n        Examples\n        ----------\n\n        Creating a basic channel:\n\n        .. code-block:: python3\n\n            channel = await guild.create_text_channel('cool-channel')\n\n        Creating a \"secret\" channel:\n\n        .. code-block:: python3\n\n            overwrites = {\n                guild.default_role: discord.PermissionOverwrite(read_messages=False),\n                guild.me: discord.PermissionOverwrite(read_messages=True)\n            }\n\n            channel = await guild.create_text_channel('secret', overwrites=overwrites)\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The channel's name.\n        overwrites\n            A :class:`dict` of target (either a role or a member) to\n            :class:`PermissionOverwrite` to apply upon creation of a channel.\n            Useful for creating secret channels.\n        category: Optional[:class:`CategoryChannel`]\n            The category to place the newly created channel under.\n            The permissions will be automatically synced to category if no\n            overwrites are provided.\n        position: :class:`int`\n            The position in the channel list. This is a number that starts\n            at 0. e.g. the top channel is position 0.\n        topic: Optional[:class:`str`]\n            The new channel's topic.\n        slowmode_delay: :class:`int`\n            Specifies the slowmode rate limit for user in this channel.\n            The maximum value possible is `120`.\n        nsfw: :class:`bool`\n            To mark the channel as NSFW or not.\n        reason: Optional[:class:`str`]\n            The reason for creating this channel. Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You do not have the proper permissions to create this channel.\n        HTTPException\n            Creating the channel failed.\n        InvalidArgument\n            The permission overwrite information is not in proper form.\n\n        Returns\n        -------\n        :class:`TextChannel`\n            The channel that was just created.\n        \"\"\"\n        data = await self._create_channel(name, overwrites, ChannelType.text, category, reason=reason, **options)\n        channel = TextChannel(state=self._state, guild=self, data=data)\n\n        # temporarily add to the cache\n        self._channels[channel.id] = channel\n        return channel"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a voice channel.", "response": "async def create_voice_channel(self, name, *, overwrites=None, category=None, reason=None, **options):\n        \"\"\"|coro|\n\n        This is similar to :meth:`create_text_channel` except makes a :class:`VoiceChannel` instead, in addition\n        to having the following new parameters.\n\n        Parameters\n        -----------\n        bitrate: :class:`int`\n            The channel's preferred audio bitrate in bits per second.\n        user_limit: :class:`int`\n            The channel's limit for number of members that can be in a voice channel.\n        \"\"\"\n        data = await self._create_channel(name, overwrites, ChannelType.voice, category, reason=reason, **options)\n        channel = VoiceChannel(state=self._state, guild=self, data=data)\n\n        # temporarily add to the cache\n        self._channels[channel.id] = channel\n        return channel"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a category channel.", "response": "async def create_category(self, name, *, overwrites=None, reason=None):\n        \"\"\"|coro|\n\n        Same as :meth:`create_text_channel` except makes a :class:`CategoryChannel` instead.\n\n        .. note::\n\n            The ``category`` parameter is not supported in this function since categories\n            cannot have categories.\n        \"\"\"\n        data = await self._create_channel(name, overwrites, ChannelType.category, reason=reason)\n        channel = CategoryChannel(state=self._state, guild=self, data=data)\n\n        # temporarily add to the cache\n        self._channels[channel.id] = channel\n        return channel"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nedit the guild s entry in the system.", "response": "async def edit(self, *, reason=None, **fields):\n        \"\"\"|coro|\n\n        Edits the guild.\n\n        You must have the :attr:`~Permissions.manage_guild` permission\n        to edit the guild.\n\n        Parameters\n        ----------\n        name: :class:`str`\n            The new name of the guild.\n        description: :class:`str`\n            The new description of the guild. This is only available to guilds that\n            contain `VERIFIED` in :attr:`Guild.features`.\n        icon: :class:`bytes`\n            A :term:`py:bytes-like object` representing the icon. Only PNG/JPEG supported.\n            Could be ``None`` to denote removal of the icon.\n        banner: :class:`bytes`\n            A :term:`py:bytes-like object` representing the banner.\n            Could be ``None`` to denote removal of the banner.\n        splash: :class:`bytes`\n            A :term:`py:bytes-like object` representing the invite splash.\n            Only PNG/JPEG supported. Could be ``None`` to denote removing the\n            splash. Only available for partnered guilds with ``INVITE_SPLASH``\n            feature.\n        region: :class:`VoiceRegion`\n            The new region for the guild's voice communication.\n        afk_channel: Optional[:class:`VoiceChannel`]\n            The new channel that is the AFK channel. Could be ``None`` for no AFK channel.\n        afk_timeout: :class:`int`\n            The number of seconds until someone is moved to the AFK channel.\n        owner: :class:`Member`\n            The new owner of the guild to transfer ownership to. Note that you must\n            be owner of the guild to do this.\n        verification_level: :class:`VerificationLevel`\n            The new verification level for the guild.\n        default_notifications: :class:`NotificationLevel`\n            The new default notification level for the guild.\n        explicit_content_filter: :class:`ContentFilter`\n            The new explicit content filter for the guild.\n        vanity_code: :class:`str`\n            The new vanity code for the guild.\n        system_channel: Optional[:class:`TextChannel`]\n            The new channel that is used for the system channel. Could be ``None`` for no system channel.\n        reason: Optional[:class:`str`]\n            The reason for editing this guild. Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to edit the guild.\n        HTTPException\n            Editing the guild failed.\n        InvalidArgument\n            The image format passed in to ``icon`` is invalid. It must be\n            PNG or JPG. This is also raised if you are not the owner of the\n            guild and request an ownership transfer.\n        \"\"\"\n\n        http = self._state.http\n        try:\n            icon_bytes = fields['icon']\n        except KeyError:\n            icon = self.icon\n        else:\n            if icon_bytes is not None:\n                icon = utils._bytes_to_base64_data(icon_bytes)\n            else:\n                icon = None\n\n        try:\n            banner_bytes = fields['banner']\n        except KeyError:\n            banner = self.banner\n        else:\n            if banner_bytes is not None:\n                banner = utils._bytes_to_base64_data(banner_bytes)\n            else:\n                banner = None\n\n        try:\n            vanity_code = fields['vanity_code']\n        except KeyError:\n            pass\n        else:\n            await http.change_vanity_code(self.id, vanity_code, reason=reason)\n\n        try:\n            splash_bytes = fields['splash']\n        except KeyError:\n            splash = self.splash\n        else:\n            if splash_bytes is not None:\n                splash = utils._bytes_to_base64_data(splash_bytes)\n            else:\n                splash = None\n\n        fields['icon'] = icon\n        fields['banner'] = banner\n        fields['splash'] = splash\n\n        try:\n            default_message_notifications = int(fields.pop('default_notifications'))\n        except (TypeError, KeyError):\n            pass\n        else:\n            fields['default_message_notifications'] = default_message_notifications\n\n        try:\n            afk_channel = fields.pop('afk_channel')\n        except KeyError:\n            pass\n        else:\n            if afk_channel is None:\n                fields['afk_channel_id'] = afk_channel\n            else:\n                fields['afk_channel_id'] = afk_channel.id\n\n        try:\n            system_channel = fields.pop('system_channel')\n        except KeyError:\n            pass\n        else:\n            if system_channel is None:\n                fields['system_channel_id'] = system_channel\n            else:\n                fields['system_channel_id'] = system_channel.id\n\n        if 'owner' in fields:\n            if self.owner != self.me:\n                raise InvalidArgument('To transfer ownership you must be the owner of the guild.')\n\n            fields['owner_id'] = fields['owner'].id\n\n        if 'region' in fields:\n            fields['region'] = str(fields['region'])\n\n        level = fields.get('verification_level', self.verification_level)\n        if not isinstance(level, VerificationLevel):\n            raise InvalidArgument('verification_level field must be of type VerificationLevel')\n\n        fields['verification_level'] = level.value\n\n        explicit_content_filter = fields.get('explicit_content_filter', self.explicit_content_filter)\n        if not isinstance(explicit_content_filter, ContentFilter):\n            raise InvalidArgument('explicit_content_filter field must be of type ContentFilter')\n\n        fields['explicit_content_filter'] = explicit_content_filter.value\n        await http.edit_guild(self.id, reason=reason, **fields)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching a member from a guild ID and a member ID.", "response": "async def fetch_member(self, member_id):\n        \"\"\"|coro|\n\n        Retreives a :class:`Member` from a guild ID, and a member ID.\n\n        .. note::\n\n            This method is an API call. For general usage, consider :meth:`get_member` instead.\n\n        Parameters\n        -----------\n        member_id: :class:`int`\n            The member's ID to fetch from.\n\n        Raises\n        -------\n        Forbidden\n            You do not have access to the guild.\n        HTTPException\n            Getting the guild failed.\n\n        Returns\n        --------\n        :class:`Member`\n            The member from the member ID.\n        \"\"\"\n        data = await self._state.http.get_member(self.id, member_id)\n        return Member(data=data, state=self._state, guild=self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch the ban information for a user.", "response": "async def fetch_ban(self, user):\n        \"\"\"|coro|\n\n        Retrieves the :class:`BanEntry` for a user, which is a namedtuple\n        with a ``user`` and ``reason`` field. See :meth:`bans` for more\n        information.\n\n        You must have the :attr:`~Permissions.ban_members` permission\n        to get this information.\n\n        Parameters\n        -----------\n        user: :class:`abc.Snowflake`\n            The user to get ban information from.\n\n        Raises\n        ------\n        Forbidden\n            You do not have proper permissions to get the information.\n        NotFound\n            This user is not banned.\n        HTTPException\n            An error occurred while fetching the information.\n\n        Returns\n        -------\n        BanEntry\n            The BanEntry object for the specified user.\n        \"\"\"\n        data = await self._state.http.get_ban(user.id, self.id)\n        return BanEntry(\n            user=User(state=self._state, data=data['user']),\n            reason=data['reason']\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def bans(self):\n\n        data = await self._state.http.get_bans(self.id)\n        return [BanEntry(user=User(state=self._state, data=e['user']),\n                         reason=e['reason'])\n                for e in data]", "response": "Return a list of bans that have been banned from the guild."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def prune_members(self, *, days, compute_prune_count=True, reason=None):\n\n        if not isinstance(days, int):\n            raise InvalidArgument('Expected int for ``days``, received {0.__class__.__name__} instead.'.format(days))\n\n        data = await self._state.http.prune_members(self.id, days, compute_prune_count=compute_prune_count, reason=reason)\n        return data['pruned']", "response": "Prune the guild from its inactive members."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nestimating the number of members that were pruned in the given number of days.", "response": "async def estimate_pruned_members(self, *, days):\n        \"\"\"|coro|\n\n        Similar to :meth:`prune_members` except instead of actually\n        pruning members, it returns how many members it would prune\n        from the guild had it been called.\n\n        Parameters\n        -----------\n        days: :class:`int`\n            The number of days before counting as inactive.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to prune members.\n        HTTPException\n            An error occurred while fetching the prune members estimate.\n        InvalidArgument\n            An integer was not passed for ``days``.\n\n        Returns\n        ---------\n        :class:`int`\n            The number of members estimated to be pruned.\n        \"\"\"\n\n        if not isinstance(days, int):\n            raise InvalidArgument('Expected int for ``days``, received {0.__class__.__name__} instead.'.format(days))\n\n        data = await self._state.http.estimate_pruned_members(self.id, days)\n        return data['pruned']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all active instant invites from the guild.", "response": "async def invites(self):\n        \"\"\"|coro|\n\n        Returns a list of all active instant invites from the guild.\n\n        You must have the :attr:`~Permissions.manage_guild` permission to get\n        this information.\n\n        Raises\n        -------\n        Forbidden\n            You do not have proper permissions to get the information.\n        HTTPException\n            An error occurred while fetching the information.\n\n        Returns\n        -------\n        List[:class:`Invite`]\n            The list of invites that are currently active.\n        \"\"\"\n\n        data = await self._state.http.invites_from(self.id)\n        result = []\n        for invite in data:\n            channel = self.get_channel(int(invite['channel']['id']))\n            invite['channel'] = channel\n            invite['guild'] = self\n            result.append(Invite(state=self._state, data=invite))\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def fetch_emojis(self):\n        data = await self._state.http.get_all_custom_emojis(self.id)\n        return [Emoji(guild=self, state=self._state, data=d) for d in data]", "response": "Get all custom emojis for this guild."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches a custom emoji from the guild.", "response": "async def fetch_emoji(self, emoji_id):\n        \"\"\"|coro|\n\n        Retrieves a custom :class:`Emoji` from the guild.\n\n        .. note::\n\n            This method is an API call.\n            For general usage, consider iterating over :attr:`emojis` instead.\n\n        Parameters\n        -------------\n        emoji_id: :class:`int`\n            The emoji's ID.\n\n        Raises\n        ---------\n        NotFound\n            The emoji requested could not be found.\n        HTTPException\n            An error occurred fetching the emoji.\n\n        Returns\n        --------\n        :class:`Emoji`\n            The retrieved emoji.\n        \"\"\"\n        data = await self._state.http.get_custom_emoji(self.id, emoji_id)\n        return Emoji(guild=self, state=self._state, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a custom emoji for this guild.", "response": "async def create_custom_emoji(self, *, name, image, roles=None, reason=None):\n        r\"\"\"|coro|\n\n        Creates a custom :class:`Emoji` for the guild.\n\n        There is currently a limit of 50 static and animated emojis respectively per guild,\n        unless the guild has the ``MORE_EMOJI`` feature which extends the limit to 200.\n\n        You must have the :attr:`~Permissions.manage_emojis` permission to\n        do this.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The emoji name. Must be at least 2 characters.\n        image: :class:`bytes`\n            The :term:`py:bytes-like object` representing the image data to use.\n            Only JPG, PNG and GIF images are supported.\n        roles: Optional[List[:class:`Role`]]\n            A :class:`list` of :class:`Role`\\s that can use this emoji. Leave empty to make it available to everyone.\n        reason: Optional[:class:`str`]\n            The reason for creating this emoji. Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You are not allowed to create emojis.\n        HTTPException\n            An error occurred creating an emoji.\n\n        Returns\n        --------\n        :class:`Emoji`\n            The created emoji.\n        \"\"\"\n\n        img = utils._bytes_to_base64_data(image)\n        if roles:\n            roles = [role.id for role in roles]\n        data = await self._state.http.create_custom_emoji(self.id, name, img, roles=roles, reason=reason)\n        return self._state.store_emoji(self, data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def create_role(self, *, reason=None, **fields):\n\n        try:\n            perms = fields.pop('permissions')\n        except KeyError:\n            fields['permissions'] = 0\n        else:\n            fields['permissions'] = perms.value\n\n        try:\n            colour = fields.pop('colour')\n        except KeyError:\n            colour = fields.get('color', Colour.default())\n        finally:\n            fields['color'] = colour.value\n\n        valid_keys = ('name', 'permissions', 'color', 'hoist', 'mentionable')\n        for key in fields:\n            if key not in valid_keys:\n                raise InvalidArgument('%r is not a valid field.' % key)\n\n        data = await self._state.http.create_role(self.id, reason=reason, **fields)\n        role = Role(guild=self, data=data, state=self._state)\n\n        # TODO: add to cache\n        return role", "response": "Create a new role for the guild."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nkicking a user from the guild.", "response": "async def kick(self, user, *, reason=None):\n        \"\"\"|coro|\n\n        Kicks a user from the guild.\n\n        The user must meet the :class:`abc.Snowflake` abc.\n\n        You must have the :attr:`~Permissions.kick_members` permission to\n        do this.\n\n        Parameters\n        -----------\n        user: :class:`abc.Snowflake`\n            The user to kick from their guild.\n        reason: Optional[:class:`str`]\n            The reason the user got kicked.\n\n        Raises\n        -------\n        Forbidden\n            You do not have the proper permissions to kick.\n        HTTPException\n            Kicking failed.\n        \"\"\"\n        await self._state.http.kick(user.id, self.id, reason=reason)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbanning a user from the guild.", "response": "async def ban(self, user, *, reason=None, delete_message_days=1):\n        \"\"\"|coro|\n\n        Bans a user from the guild.\n\n        The user must meet the :class:`abc.Snowflake` abc.\n\n        You must have the :attr:`~Permissions.ban_members` permission to\n        do this.\n\n        Parameters\n        -----------\n        user: :class:`abc.Snowflake`\n            The user to ban from their guild.\n        delete_message_days: :class:`int`\n            The number of days worth of messages to delete from the user\n            in the guild. The minimum is 0 and the maximum is 7.\n        reason: Optional[:class:`str`]\n            The reason the user got banned.\n\n        Raises\n        -------\n        Forbidden\n            You do not have the proper permissions to ban.\n        HTTPException\n            Banning failed.\n        \"\"\"\n        await self._state.http.ban(user.id, self.id, delete_message_days, reason=reason)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def unban(self, user, *, reason=None):\n        await self._state.http.unban(user.id, self.id, reason=reason)", "response": "Unban a user from the guild."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the special vanity invite for this guild.", "response": "async def vanity_invite(self):\n        \"\"\"|coro|\n\n        Returns the guild's special vanity invite.\n\n        The guild must be partnered, i.e. have 'VANITY_URL' in\n        :attr:`~Guild.features`.\n\n        You must have the :attr:`~Permissions.manage_guild` permission to use\n        this as well.\n\n        Raises\n        -------\n        Forbidden\n            You do not have the proper permissions to get this.\n        HTTPException\n            Retrieving the vanity invite failed.\n\n        Returns\n        --------\n        :class:`Invite`\n            The special vanity invite.\n        \"\"\"\n\n        # we start with { code: abc }\n        payload = await self._state.http.get_vanity_code(self.id)\n\n        # get the vanity URL channel since default channels aren't\n        # reliable or a thing anymore\n        data = await self._state.http.get_invite(payload['code'])\n\n        payload['guild'] = self\n        payload['channel'] = self.get_channel(int(data['channel']['id']))\n        payload['revoked'] = False\n        payload['temporary'] = False\n        payload['max_uses'] = 0\n        payload['max_age'] = 0\n        return Invite(state=self._state, data=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ack(self):\n\n        state = self._state\n        if state.is_bot:\n            raise ClientException('Must not be a bot account to ack messages.')\n        return state.http.ack_guild(self.id)", "response": "Acks every message in this guild."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an async iterator that returns the audit logs for the specified user and action.", "response": "def audit_logs(self, *, limit=100, before=None, after=None, oldest_first=None, user=None, action=None):\n        \"\"\"Return an :class:`AsyncIterator` that enables receiving the guild's audit logs.\n\n        You must have the :attr:`~Permissions.view_audit_log` permission to use this.\n\n        Examples\n        ----------\n\n        Getting the first 100 entries: ::\n\n            async for entry in guild.audit_logs(limit=100):\n                print('{0.user} did {0.action} to {0.target}'.format(entry))\n\n        Getting entries for a specific action: ::\n\n            async for entry in guild.audit_logs(action=discord.AuditLogAction.ban):\n                print('{0.user} banned {0.target}'.format(entry))\n\n        Getting entries made by a specific user: ::\n\n            entries = await guild.audit_logs(limit=None, user=guild.me).flatten()\n            await channel.send('I made {} moderation actions.'.format(len(entries)))\n\n        Parameters\n        -----------\n        limit: Optional[:class:`int`]\n            The number of entries to retrieve. If ``None`` retrieve all entries.\n        before: Union[:class:`abc.Snowflake`, datetime]\n            Retrieve entries before this date or entry.\n            If a date is provided it must be a timezone-naive datetime representing UTC time.\n        after: Union[:class:`abc.Snowflake`, datetime]\n            Retrieve entries after this date or entry.\n            If a date is provided it must be a timezone-naive datetime representing UTC time.\n        oldest_first: :class:`bool`\n            If set to true, return entries in oldest->newest order. Defaults to True if\n            ``after`` is specified, otherwise False.\n        user: :class:`abc.Snowflake`\n            The moderator to filter entries from.\n        action: :class:`AuditLogAction`\n            The action to filter with.\n\n        Raises\n        -------\n        Forbidden\n            You are not allowed to fetch audit logs\n        HTTPException\n            An error occurred while fetching the audit logs.\n\n        Yields\n        --------\n        :class:`AuditLogEntry`\n            The audit log entry.\n        \"\"\"\n        if user:\n            user = user.id\n\n        if action:\n            action = action.value\n\n        return AuditLogIterator(self, before=before, after=after, limit=limit,\n                                oldest_first=oldest_first, user_id=user, action_type=action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def widget(self):\n        data = await self._state.http.get_widget(self.id)\n\n        return Widget(state=self._state, data=data)", "response": "Return the widget of the guild."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the average latency between HEARTBEAT and HEARTBEAT_ACK in seconds.", "response": "def latency(self):\n        \"\"\":class:`float`: Measures latency between a HEARTBEAT and a HEARTBEAT_ACK in seconds.\n\n        This operates similarly to :meth:`.Client.latency` except it uses the average\n        latency of every shard's latency. To get a list of shard latency, check the\n        :attr:`latencies` property. Returns ``nan`` if there are no shards ready.\n        \"\"\"\n        if not self.shards:\n            return float('nan')\n        return sum(latency for _, latency in self.latencies) / len(self.shards)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrequests previously - filled members from the given guilds into the internal members cache.", "response": "async def request_offline_members(self, *guilds):\n        r\"\"\"|coro|\n\n        Requests previously offline members from the guild to be filled up\n        into the :attr:`Guild.members` cache. This function is usually not\n        called. It should only be used if you have the ``fetch_offline_members``\n        parameter set to ``False``.\n\n        When the client logs on and connects to the websocket, Discord does\n        not provide the library with offline members if the number of members\n        in the guild is larger than 250. You can check if a guild is large\n        if :attr:`Guild.large` is ``True``.\n\n        Parameters\n        -----------\n        \\*guilds: :class:`Guild`\n            An argument list of guilds to request offline members for.\n\n        Raises\n        -------\n        InvalidArgument\n            If any guild is unavailable or not large in the collection.\n        \"\"\"\n        if any(not g.large or g.unavailable for g in guilds):\n            raise InvalidArgument('An unavailable or non-large guild was passed.')\n\n        _guilds = sorted(guilds, key=lambda g: g.shard_id)\n        for shard_id, sub_guilds in itertools.groupby(_guilds, key=lambda g: g.shard_id):\n            sub_guilds = list(sub_guilds)\n            await self._connection.request_offline_members(sub_guilds, shard_id=shard_id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def close(self):\n        if self.is_closed():\n            return\n\n        self._closed = True\n\n        for vc in self.voice_clients:\n            try:\n                await vc.disconnect()\n            except Exception:\n                pass\n\n        to_close = [shard.ws.close() for shard in self.shards.values()]\n        if to_close:\n            await asyncio.wait(to_close, loop=self.loop)\n\n        await self.http.close()", "response": "Closes the connection to discord."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchanging the presence of a specific node.", "response": "async def change_presence(self, *, activity=None, status=None, afk=False, shard_id=None):\n        \"\"\"|coro|\n\n        Changes the client's presence.\n\n        The activity parameter is a :class:`Activity` object (not a string) that represents\n        the activity being done currently. This could also be the slimmed down versions,\n        :class:`Game` and :class:`Streaming`.\n\n        Example: ::\n\n            game = discord.Game(\"with the API\")\n            await client.change_presence(status=discord.Status.idle, activity=game)\n\n        Parameters\n        ----------\n        activity: Optional[Union[:class:`Game`, :class:`Streaming`, :class:`Activity`]]\n            The activity being done. ``None`` if no currently active activity is done.\n        status: Optional[:class:`Status`]\n            Indicates what status to change to. If None, then\n            :attr:`Status.online` is used.\n        afk: :class:`bool`\n            Indicates if you are going AFK. This allows the discord\n            client to know how to handle push notifications better\n            for you in case you are actually idle and not lying.\n        shard_id: Optional[:class:`int`]\n            The shard_id to change the presence to. If not specified\n            or ``None``, then it will change the presence of every\n            shard the bot can see.\n\n        Raises\n        ------\n        InvalidArgument\n            If the ``activity`` parameter is not of proper type.\n        \"\"\"\n\n        if status is None:\n            status = 'online'\n            status_enum = Status.online\n        elif status is Status.offline:\n            status = 'invisible'\n            status_enum = Status.offline\n        else:\n            status_enum = status\n            status = str(status)\n\n        if shard_id is None:\n            for shard in self.shards.values():\n                await shard.ws.change_presence(activity=activity, status=status, afk=afk)\n\n            guilds = self._connection.guilds\n        else:\n            shard = self.shards[shard_id]\n            await shard.ws.change_presence(activity=activity, status=status, afk=afk)\n            guilds = [g for g in self._connection.guilds if g.shard_id == shard_id]\n\n        for guild in guilds:\n            me = guild.me\n            if me is None:\n                continue\n\n            me.activities = (activity,)\n            me.status = status_enum"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of members with this role.", "response": "def members(self):\n        \"\"\"Returns a :class:`list` of :class:`Member` with this role.\"\"\"\n        all_members = self.guild.members\n        if self.is_default():\n            return all_members\n\n        role_id = self.id\n        return [member for member in all_members if member._roles.has(role_id)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def edit(self, *, reason=None, **fields):\n\n        position = fields.get('position')\n        if position is not None:\n            await self._move(position, reason=reason)\n            self.position = position\n\n        try:\n            colour = fields['colour']\n        except KeyError:\n            colour = fields.get('color', self.colour)\n\n        payload = {\n            'name': fields.get('name', self.name),\n            'permissions': fields.get('permissions', self.permissions).value,\n            'color': colour.value,\n            'hoist': fields.get('hoist', self.hoist),\n            'mentionable': fields.get('mentionable', self.mentionable)\n        }\n\n        data = await self._state.http.edit_role(self.guild.id, self.id, reason=reason, **payload)\n        self._update(data)", "response": "|coro|\n\n        Edits the role.\n\n        You must have the :attr:`~Permissions.manage_roles` permission to\n        use this.\n\n        All fields are optional.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The new role name to change to.\n        permissions: :class:`Permissions`\n            The new permissions to change to.\n        colour: :class:`Colour`\n            The new colour to change to. (aliased to color as well)\n        hoist: :class:`bool`\n            Indicates if the role should be shown separately in the member list.\n        mentionable: :class:`bool`\n            Indicates if the role should be mentionable by others.\n        position: :class:`int`\n            The new role's position. This must be below your top role's\n            position or it will fail.\n        reason: Optional[:class:`str`]\n            The reason for editing this role. Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to change the role.\n        HTTPException\n            Editing the role failed.\n        InvalidArgument\n            An invalid position was given or the default\n            role was asked to be moved."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef group(*blueprints, url_prefix=\"\"):\n\n        def chain(nested):\n            \"\"\"itertools.chain() but leaves strings untouched\"\"\"\n            for i in nested:\n                if isinstance(i, (list, tuple)):\n                    yield from chain(i)\n                elif isinstance(i, BlueprintGroup):\n                    yield from i.blueprints\n                else:\n                    yield i\n\n        bps = BlueprintGroup(url_prefix=url_prefix)\n        for bp in chain(blueprints):\n            if bp.url_prefix is None:\n                bp.url_prefix = \"\"\n            bp.url_prefix = url_prefix + bp.url_prefix\n            bps.append(bp)\n        return bps", "response": "Create a list of blueprints grouped by URL prefix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister the blueprint into the sanic app.", "response": "def register(self, app, options):\n        \"\"\"\n        Register the blueprint to the sanic app.\n\n        :param app: Instance of :class:`sanic.app.Sanic` class\n        :param options: Options to be used while registering the\n            blueprint into the app.\n            *url_prefix* - URL Prefix to override the blueprint prefix\n        \"\"\"\n\n        url_prefix = options.get(\"url_prefix\", self.url_prefix)\n\n        # Routes\n        for future in self.routes:\n            # attach the blueprint name to the handler so that it can be\n            # prefixed properly in the router\n            future.handler.__blueprintname__ = self.name\n            # Prepend the blueprint URI prefix if available\n            uri = url_prefix + future.uri if url_prefix else future.uri\n\n            version = future.version or self.version\n\n            app.route(\n                uri=uri[1:] if uri.startswith(\"//\") else uri,\n                methods=future.methods,\n                host=future.host or self.host,\n                strict_slashes=future.strict_slashes,\n                stream=future.stream,\n                version=version,\n                name=future.name,\n            )(future.handler)\n\n        for future in self.websocket_routes:\n            # attach the blueprint name to the handler so that it can be\n            # prefixed properly in the router\n            future.handler.__blueprintname__ = self.name\n            # Prepend the blueprint URI prefix if available\n            uri = url_prefix + future.uri if url_prefix else future.uri\n            app.websocket(\n                uri=uri,\n                host=future.host or self.host,\n                strict_slashes=future.strict_slashes,\n                name=future.name,\n            )(future.handler)\n\n        # Middleware\n        for future in self.middlewares:\n            if future.args or future.kwargs:\n                app.register_middleware(\n                    future.middleware, *future.args, **future.kwargs\n                )\n            else:\n                app.register_middleware(future.middleware)\n\n        # Exceptions\n        for future in self.exceptions:\n            app.exception(*future.args, **future.kwargs)(future.handler)\n\n        # Static Files\n        for future in self.statics:\n            # Prepend the blueprint URI prefix if available\n            uri = url_prefix + future.uri if url_prefix else future.uri\n            app.static(\n                uri, future.file_or_directory, *future.args, **future.kwargs\n            )\n\n        # Event listeners\n        for event, listeners in self.listeners.items():\n            for listener in listeners:\n                app.listener(event)(listener)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef route(\n        self,\n        uri,\n        methods=frozenset({\"GET\"}),\n        host=None,\n        strict_slashes=None,\n        stream=False,\n        version=None,\n        name=None,\n    ):\n        \"\"\"Create a blueprint route from a decorated function.\n\n        :param uri: endpoint at which the route will be accessible.\n        :param methods: list of acceptable HTTP methods.\n        :param host: IP Address of FQDN for the sanic server to use.\n        :param strict_slashes: Enforce the API urls are requested with a\n            training */*\n        :param stream: If the route should provide a streaming support\n        :param version: Blueprint Version\n        :param name: Unique name to identify the Route\n\n        :return a decorated method that when invoked will return an object\n            of type :class:`FutureRoute`\n        \"\"\"\n        if strict_slashes is None:\n            strict_slashes = self.strict_slashes\n\n        def decorator(handler):\n            route = FutureRoute(\n                handler,\n                uri,\n                methods,\n                host,\n                strict_slashes,\n                stream,\n                version,\n                name,\n            )\n            self.routes.append(route)\n            return handler\n\n        return decorator", "response": "Create a blueprint route from a function that will be invoked when the route is invoked."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a blueprint websocket route from a decorated function. :param uri: endpoint at which the route will be accessible. :param host: IP Address of FQDN for the sanic server to use. :param strict_slashes: Enforce the API urls are requested with a training */* :param version: Blueprint Version :param name: Unique name to identify the Websocket Route", "response": "def websocket(\n        self, uri, host=None, strict_slashes=None, version=None, name=None\n    ):\n        \"\"\"Create a blueprint websocket route from a decorated function.\n\n        :param uri: endpoint at which the route will be accessible.\n        :param host: IP Address of FQDN for the sanic server to use.\n        :param strict_slashes: Enforce the API urls are requested with a\n            training */*\n        :param version: Blueprint Version\n        :param name: Unique name to identify the Websocket Route\n        \"\"\"\n        if strict_slashes is None:\n            strict_slashes = self.strict_slashes\n\n        def decorator(handler):\n            route = FutureRoute(\n                handler, uri, [], host, strict_slashes, False, version, name\n            )\n            self.websocket_routes.append(route)\n            return handler\n\n        return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a blueprint websocket route.", "response": "def add_websocket_route(\n        self, handler, uri, host=None, version=None, name=None\n    ):\n        \"\"\"Create a blueprint websocket route from a function.\n\n        :param handler: function for handling uri requests. Accepts function,\n                        or class instance with a view_class method.\n        :param uri: endpoint at which the route will be accessible.\n        :param host: IP Address of FQDN for the sanic server to use.\n        :param version: Blueprint Version\n        :param name: Unique name to identify the Websocket Route\n        :return: function or class instance\n        \"\"\"\n        self.websocket(uri=uri, host=host, version=version, name=name)(handler)\n        return handler"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a listener from a decorated function.", "response": "def listener(self, event):\n        \"\"\"Create a listener from a decorated function.\n\n        :param event: Event to listen to.\n        \"\"\"\n\n        def decorator(listener):\n            self.listeners[event].append(listener)\n            return listener\n\n        return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a blueprint middleware from a decorated function.", "response": "def middleware(self, *args, **kwargs):\n        \"\"\"\n        Create a blueprint middleware from a decorated function.\n\n        :param args: Positional arguments to be used while invoking the\n            middleware\n        :param kwargs: optional keyword args that can be used with the\n            middleware.\n        \"\"\"\n\n        def register_middleware(_middleware):\n            future_middleware = FutureMiddleware(_middleware, args, kwargs)\n            self.middlewares.append(future_middleware)\n            return _middleware\n\n        # Detect which way this was called, @middleware or @middleware('AT')\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            middleware = args[0]\n            args = []\n            return register_middleware(middleware)\n        else:\n            if kwargs.get(\"bp_group\") and callable(args[0]):\n                middleware = args[0]\n                args = args[1:]\n                kwargs.pop(\"bp_group\")\n                return register_middleware(middleware)\n            else:\n                return register_middleware"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a blueprint static route from a decorated function.", "response": "def static(self, uri, file_or_directory, *args, **kwargs):\n        \"\"\"Create a blueprint static route from a decorated function.\n\n        :param uri: endpoint at which the route will be accessible.\n        :param file_or_directory: Static asset.\n        \"\"\"\n        name = kwargs.pop(\"name\", \"static\")\n        if not name.startswith(self.name + \".\"):\n            name = \"{}.{}\".format(self.name, name)\n        kwargs.update(name=name)\n\n        strict_slashes = kwargs.get(\"strict_slashes\")\n        if strict_slashes is None and self.strict_slashes is not None:\n            kwargs.update(strict_slashes=self.strict_slashes)\n\n        static = FutureStatic(uri, file_or_directory, args, kwargs)\n        self.statics.append(static)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an API URL under the **GET** *HTTP* method :param uri: URL to be tagged to **GET** method of *HTTP* :param host: Host IP or FQDN for the service to use :param strict_slashes: Instruct :class:`sanic.app.Sanic` to check if the request URLs need to terminate with a */* :param version: API Version :param name: Unique name that can be used to identify the Route :return: Object decorated with :func:`route` method", "response": "def get(\n        self, uri, host=None, strict_slashes=None, version=None, name=None\n    ):\n        \"\"\"\n        Add an API URL under the **GET** *HTTP* method\n\n        :param uri: URL to be tagged to **GET** method of *HTTP*\n        :param host: Host IP or FQDN for the service to use\n        :param strict_slashes: Instruct :class:`sanic.app.Sanic` to check\n            if the request URLs need to terminate with a */*\n        :param version: API Version\n        :param name: Unique name that can be used to identify the Route\n        :return: Object decorated with :func:`route` method\n        \"\"\"\n        return self.route(\n            uri,\n            methods=frozenset({\"GET\"}),\n            host=host,\n            strict_slashes=strict_slashes,\n            version=version,\n            name=name,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef abort(status_code, message=None):\n    if message is None:\n        message = STATUS_CODES.get(status_code)\n        # These are stored as bytes in the STATUS_CODES dict\n        message = message.decode(\"utf8\")\n    sanic_exception = _sanic_exceptions.get(status_code, SanicException)\n    raise sanic_exception(message=message, status_code=status_code)", "response": "Raise an exception based on SanicException. Returns the HTTP response containing the given status code."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscheduling a task to run later after the loop has started.", "response": "def add_task(self, task):\n        \"\"\"Schedule a task to run later, after the loop has started.\n        Different from asyncio.ensure_future in that it does not\n        also return a future, and the actual ensure_future call\n        is delayed until before server start.\n\n        :param task: future, couroutine or awaitable\n        \"\"\"\n        try:\n            if callable(task):\n                try:\n                    self.loop.create_task(task(self))\n                except TypeError:\n                    self.loop.create_task(task())\n            else:\n                self.loop.create_task(task)\n        except SanicException:\n\n            @self.listener(\"before_server_start\")\n            def run(app, loop):\n                if callable(task):\n                    try:\n                        loop.create_task(task(self))\n                    except TypeError:\n                        loop.create_task(task())\n                else:\n                    loop.create_task(task)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecorate a function to be registered as a route", "response": "def route(\n        self,\n        uri,\n        methods=frozenset({\"GET\"}),\n        host=None,\n        strict_slashes=None,\n        stream=False,\n        version=None,\n        name=None,\n    ):\n        \"\"\"Decorate a function to be registered as a route\n\n        :param uri: path of the URL\n        :param methods: list or tuple of methods allowed\n        :param host:\n        :param strict_slashes:\n        :param stream:\n        :param version:\n        :param name: user defined route name for url_for\n        :return: decorated function\n        \"\"\"\n\n        # Fix case where the user did not prefix the URL with a /\n        # and will probably get confused as to why it's not working\n        if not uri.startswith(\"/\"):\n            uri = \"/\" + uri\n\n        if stream:\n            self.is_request_stream = True\n\n        if strict_slashes is None:\n            strict_slashes = self.strict_slashes\n\n        def response(handler):\n            args = list(signature(handler).parameters.keys())\n\n            if not args:\n                raise ValueError(\n                    \"Required parameter `request` missing \"\n                    \"in the {0}() route?\".format(handler.__name__)\n                )\n\n            if stream:\n                handler.is_stream = stream\n\n            self.router.add(\n                uri=uri,\n                methods=methods,\n                handler=handler,\n                host=host,\n                strict_slashes=strict_slashes,\n                version=version,\n                name=name,\n            )\n            return handler\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef websocket(\n        self, uri, host=None, strict_slashes=None, subprotocols=None, name=None\n    ):\n        \"\"\"Decorate a function to be registered as a websocket route\n        :param uri: path of the URL\n        :param subprotocols: optional list of str with supported subprotocols\n        :param host:\n        :return: decorated function\n        \"\"\"\n        self.enable_websocket()\n\n        # Fix case where the user did not prefix the URL with a /\n        # and will probably get confused as to why it's not working\n        if not uri.startswith(\"/\"):\n            uri = \"/\" + uri\n\n        if strict_slashes is None:\n            strict_slashes = self.strict_slashes\n\n        def response(handler):\n            async def websocket_handler(request, *args, **kwargs):\n                request.app = self\n                if not getattr(handler, \"__blueprintname__\", False):\n                    request.endpoint = handler.__name__\n                else:\n                    request.endpoint = (\n                        getattr(handler, \"__blueprintname__\", \"\")\n                        + handler.__name__\n                    )\n                try:\n                    protocol = request.transport.get_protocol()\n                except AttributeError:\n                    # On Python3.5 the Transport classes in asyncio do not\n                    # have a get_protocol() method as in uvloop\n                    protocol = request.transport._protocol\n                ws = await protocol.websocket_handshake(request, subprotocols)\n\n                # schedule the application handler\n                # its future is kept in self.websocket_tasks in case it\n                # needs to be cancelled due to the server being stopped\n                fut = ensure_future(handler(request, ws, *args, **kwargs))\n                self.websocket_tasks.add(fut)\n                try:\n                    await fut\n                except (CancelledError, ConnectionClosed):\n                    pass\n                finally:\n                    self.websocket_tasks.remove(fut)\n                await ws.close()\n\n            self.router.add(\n                uri=uri,\n                handler=websocket_handler,\n                methods=frozenset({\"GET\"}),\n                host=host,\n                strict_slashes=strict_slashes,\n                name=name,\n            )\n            return handler\n\n        return response", "response": "Decorate a function to be registered as a websocket route"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable_websocket(self, enable=True):\n        if not self.websocket_enabled:\n            # if the server is stopped, we want to cancel any ongoing\n            # websocket tasks, to allow the server to exit promptly\n            @self.listener(\"before_server_stop\")\n            def cancel_websocket_tasks(app, loop):\n                for task in self.websocket_tasks:\n                    task.cancel()\n\n        self.websocket_enabled = enable", "response": "Enable or disable the support for websocket. Websocket is enabled automatically if routes are added to the application."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_route(self, uri, clean_cache=True, host=None):\n        self.router.remove(uri, clean_cache, host)", "response": "This method is used to remove a route from the object store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exception(self, *exceptions):\n\n        def response(handler):\n            for exception in exceptions:\n                if isinstance(exception, (tuple, list)):\n                    for e in exception:\n                        self.error_handler.add(e, handler)\n                else:\n                    self.error_handler.add(exception, handler)\n            return handler\n\n        return response", "response": "Decorate a function to be registered as a handler for exceptions\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_middleware(self, middleware, attach_to=\"request\"):\n        if attach_to == \"request\":\n            if middleware not in self.request_middleware:\n                self.request_middleware.append(middleware)\n        if attach_to == \"response\":\n            if middleware not in self.response_middleware:\n                self.response_middleware.appendleft(middleware)\n        return middleware", "response": "Register a middleware that will be attached to all the API URLs registered under this application level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef middleware(self, middleware_or_request):\n        # Detect which way this was called, @middleware or @middleware('AT')\n        if callable(middleware_or_request):\n            return self.register_middleware(middleware_or_request)\n\n        else:\n            return partial(\n                self.register_middleware, attach_to=middleware_or_request\n            )", "response": "Decorate and register middleware to be called before a request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef static(\n        self,\n        uri,\n        file_or_directory,\n        pattern=r\"/?.+\",\n        use_modified_since=True,\n        use_content_range=False,\n        stream_large_files=False,\n        name=\"static\",\n        host=None,\n        strict_slashes=None,\n        content_type=None,\n    ):\n        \"\"\"\n        Register a root to serve files from. The input can either be a\n        file or a directory. This method will enable an easy and simple way\n        to setup the :class:`Route` necessary to serve the static files.\n\n        :param uri: URL path to be used for serving static content\n        :param file_or_directory: Path for the Static file/directory with\n            static files\n        :param pattern: Regex Pattern identifying the valid static files\n        :param use_modified_since: If true, send file modified time, and return\n            not modified if the browser's matches the server's\n        :param use_content_range: If true, process header for range requests\n            and sends the file part that is requested\n        :param stream_large_files: If true, use the\n            :func:`StreamingHTTPResponse.file_stream` handler rather\n            than the :func:`HTTPResponse.file` handler to send the file.\n            If this is an integer, this represents the threshold size to\n            switch to :func:`StreamingHTTPResponse.file_stream`\n        :param name: user defined name used for url_for\n        :param host: Host IP or FQDN for the service to use\n        :param strict_slashes: Instruct :class:`Sanic` to check if the request\n            URLs need to terminate with a */*\n        :param content_type: user defined content type for header\n        :return: None\n        \"\"\"\n        static_register(\n            self,\n            uri,\n            file_or_directory,\n            pattern,\n            use_modified_since,\n            use_content_range,\n            stream_large_files,\n            name,\n            host,\n            strict_slashes,\n            content_type,\n        )", "response": "This method registers a new static file for the given aracacore."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef blueprint(self, blueprint, **options):\n        if isinstance(blueprint, (list, tuple, BlueprintGroup)):\n            for item in blueprint:\n                self.blueprint(item, **options)\n            return\n        if blueprint.name in self.blueprints:\n            assert self.blueprints[blueprint.name] is blueprint, (\n                'A blueprint with the name \"%s\" is already registered.  '\n                \"Blueprint names must be unique.\" % (blueprint.name,)\n            )\n        else:\n            self.blueprints[blueprint.name] = blueprint\n            self._blueprint_order.append(blueprint)\n        blueprint.register(self, options)", "response": "Register a blueprint on the application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def handle_request(self, request, write_callback, stream_callback):\n        # Define `response` var here to remove warnings about\n        # allocation before assignment below.\n        response = None\n        cancelled = False\n        try:\n            # -------------------------------------------- #\n            # Request Middleware\n            # -------------------------------------------- #\n            response = await self._run_request_middleware(request)\n            # No middleware results\n            if not response:\n                # -------------------------------------------- #\n                # Execute Handler\n                # -------------------------------------------- #\n\n                # Fetch handler from router\n                handler, args, kwargs, uri = self.router.get(request)\n\n                request.uri_template = uri\n                if handler is None:\n                    raise ServerError(\n                        (\n                            \"'None' was returned while requesting a \"\n                            \"handler from the router\"\n                        )\n                    )\n                else:\n                    if not getattr(handler, \"__blueprintname__\", False):\n                        request.endpoint = self._build_endpoint_name(\n                            handler.__name__\n                        )\n                    else:\n                        request.endpoint = self._build_endpoint_name(\n                            getattr(handler, \"__blueprintname__\", \"\"),\n                            handler.__name__,\n                        )\n\n                # Run response handler\n                response = handler(request, *args, **kwargs)\n                if isawaitable(response):\n                    response = await response\n        except CancelledError:\n            # If response handler times out, the server handles the error\n            # and cancels the handle_request job.\n            # In this case, the transport is already closed and we cannot\n            # issue a response.\n            response = None\n            cancelled = True\n        except Exception as e:\n            # -------------------------------------------- #\n            # Response Generation Failed\n            # -------------------------------------------- #\n\n            try:\n                response = self.error_handler.response(request, e)\n                if isawaitable(response):\n                    response = await response\n            except Exception as e:\n                if isinstance(e, SanicException):\n                    response = self.error_handler.default(\n                        request=request, exception=e\n                    )\n                elif self.debug:\n                    response = HTTPResponse(\n                        \"Error while handling error: {}\\nStack: {}\".format(\n                            e, format_exc()\n                        ),\n                        status=500,\n                    )\n                else:\n                    response = HTTPResponse(\n                        \"An error occurred while handling an error\", status=500\n                    )\n        finally:\n            # -------------------------------------------- #\n            # Response Middleware\n            # -------------------------------------------- #\n            # Don't run response middleware if response is None\n            if response is not None:\n                try:\n                    response = await self._run_response_middleware(\n                        request, response\n                    )\n                except CancelledError:\n                    # Response middleware can timeout too, as above.\n                    response = None\n                    cancelled = True\n                except BaseException:\n                    error_logger.exception(\n                        \"Exception occurred in one of response \"\n                        \"middleware handlers\"\n                    )\n            if cancelled:\n                raise CancelledError()\n\n        # pass the response to the correct callback\n        if isinstance(response, StreamingHTTPResponse):\n            await stream_callback(response)\n        else:\n            write_callback(response)", "response": "This method handles a request from the HTTP Server and returns a response object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the HTTP Server and listen until keyboard interrupt or termterm signal. On termination stop the server and return Nothing.", "response": "def run(\n        self,\n        host: Optional[str] = None,\n        port: Optional[int] = None,\n        debug: bool = False,\n        ssl: Union[dict, SSLContext, None] = None,\n        sock: Optional[socket] = None,\n        workers: int = 1,\n        protocol: Type[Protocol] = None,\n        backlog: int = 100,\n        stop_event: Any = None,\n        register_sys_signals: bool = True,\n        access_log: Optional[bool] = None,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Run the HTTP Server and listen until keyboard interrupt or term\n        signal. On termination, drain connections before closing.\n\n        :param host: Address to host on\n        :type host: str\n        :param port: Port to host on\n        :type port: int\n        :param debug: Enables debug output (slows server)\n        :type debug: bool\n        :param ssl: SSLContext, or location of certificate and key\n            for SSL encryption of worker(s)\n        :type ssl:SSLContext or dict\n        :param sock: Socket for the server to accept connections from\n        :type sock: socket\n        :param workers: Number of processes received before it is respected\n        :type workers: int\n        :param protocol: Subclass of asyncio Protocol class\n        :type protocol: type[Protocol]\n        :param backlog: a number of unaccepted connections that the system\n            will allow before refusing new connections\n        :type backlog: int\n        :param stop_event: event to be triggered\n            before stopping the app - deprecated\n        :type stop_event: None\n        :param register_sys_signals: Register SIG* events\n        :type register_sys_signals: bool\n        :param access_log: Enables writing access logs (slows server)\n        :type access_log: bool\n        :return: Nothing\n        \"\"\"\n        if \"loop\" in kwargs:\n            raise TypeError(\n                \"loop is not a valid argument. To use an existing loop, \"\n                \"change to create_server().\\nSee more: \"\n                \"https://sanic.readthedocs.io/en/latest/sanic/deploying.html\"\n                \"#asynchronous-support\"\n            )\n\n        # Default auto_reload to false\n        auto_reload = False\n        # If debug is set, default it to true (unless on windows)\n        if debug and os.name == \"posix\":\n            auto_reload = True\n        # Allow for overriding either of the defaults\n        auto_reload = kwargs.get(\"auto_reload\", auto_reload)\n\n        if sock is None:\n            host, port = host or \"127.0.0.1\", port or 8000\n\n        if protocol is None:\n            protocol = (\n                WebSocketProtocol if self.websocket_enabled else HttpProtocol\n            )\n        if stop_event is not None:\n            if debug:\n                warnings.simplefilter(\"default\")\n            warnings.warn(\n                \"stop_event will be removed from future versions.\",\n                DeprecationWarning,\n            )\n        # if access_log is passed explicitly change config.ACCESS_LOG\n        if access_log is not None:\n            self.config.ACCESS_LOG = access_log\n\n        server_settings = self._helper(\n            host=host,\n            port=port,\n            debug=debug,\n            ssl=ssl,\n            sock=sock,\n            workers=workers,\n            protocol=protocol,\n            backlog=backlog,\n            register_sys_signals=register_sys_signals,\n            auto_reload=auto_reload,\n        )\n\n        try:\n            self.is_running = True\n            if workers == 1:\n                if auto_reload and os.name != \"posix\":\n                    # This condition must be removed after implementing\n                    # auto reloader for other operating systems.\n                    raise NotImplementedError\n\n                if (\n                    auto_reload\n                    and os.environ.get(\"SANIC_SERVER_RUNNING\") != \"true\"\n                ):\n                    reloader_helpers.watchdog(2)\n                else:\n                    serve(**server_settings)\n            else:\n                serve_multiple(server_settings, workers)\n        except BaseException:\n            error_logger.exception(\n                \"Experienced exception while trying to serve\"\n            )\n            raise\n        finally:\n            self.is_running = False\n        logger.info(\"Server Stopped\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def create_server(\n        self,\n        host: Optional[str] = None,\n        port: Optional[int] = None,\n        debug: bool = False,\n        ssl: Union[dict, SSLContext, None] = None,\n        sock: Optional[socket] = None,\n        protocol: Type[Protocol] = None,\n        backlog: int = 100,\n        stop_event: Any = None,\n        access_log: Optional[bool] = None,\n        return_asyncio_server=False,\n        asyncio_server_kwargs=None,\n    ) -> None:\n        \"\"\"\n        Asynchronous version of :func:`run`.\n\n        This method will take care of the operations necessary to invoke\n        the *before_start* events via :func:`trigger_events` method invocation\n        before starting the *sanic* app in Async mode.\n\n        .. note::\n            This does not support multiprocessing and is not the preferred\n            way to run a :class:`Sanic` application.\n\n        :param host: Address to host on\n        :type host: str\n        :param port: Port to host on\n        :type port: int\n        :param debug: Enables debug output (slows server)\n        :type debug: bool\n        :param ssl: SSLContext, or location of certificate and key\n            for SSL encryption of worker(s)\n        :type ssl:SSLContext or dict\n        :param sock: Socket for the server to accept connections from\n        :type sock: socket\n        :param protocol: Subclass of asyncio Protocol class\n        :type protocol: type[Protocol]\n        :param backlog: a number of unaccepted connections that the system\n            will allow before refusing new connections\n        :type backlog: int\n        :param stop_event: event to be triggered\n            before stopping the app - deprecated\n        :type stop_event: None\n        :param access_log: Enables writing access logs (slows server)\n        :type access_log: bool\n        :param return_asyncio_server: flag that defines whether there's a need\n                                      to return asyncio.Server or\n                                      start it serving right away\n        :type return_asyncio_server: bool\n        :param asyncio_server_kwargs: key-value arguments for\n                                      asyncio/uvloop create_server method\n        :type asyncio_server_kwargs: dict\n        :return: Nothing\n        \"\"\"\n\n        if sock is None:\n            host, port = host or \"127.0.0.1\", port or 8000\n\n        if protocol is None:\n            protocol = (\n                WebSocketProtocol if self.websocket_enabled else HttpProtocol\n            )\n        if stop_event is not None:\n            if debug:\n                warnings.simplefilter(\"default\")\n            warnings.warn(\n                \"stop_event will be removed from future versions.\",\n                DeprecationWarning,\n            )\n        # if access_log is passed explicitly change config.ACCESS_LOG\n        if access_log is not None:\n            self.config.ACCESS_LOG = access_log\n\n        server_settings = self._helper(\n            host=host,\n            port=port,\n            debug=debug,\n            ssl=ssl,\n            sock=sock,\n            loop=get_event_loop(),\n            protocol=protocol,\n            backlog=backlog,\n            run_async=return_asyncio_server,\n        )\n\n        # Trigger before_start events\n        await self.trigger_events(\n            server_settings.get(\"before_start\", []),\n            server_settings.get(\"loop\"),\n        )\n\n        return await serve(\n            asyncio_server_kwargs=asyncio_server_kwargs, **server_settings\n        )", "response": "Asynchronous version of run."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _helper(\n        self,\n        host=None,\n        port=None,\n        debug=False,\n        ssl=None,\n        sock=None,\n        workers=1,\n        loop=None,\n        protocol=HttpProtocol,\n        backlog=100,\n        stop_event=None,\n        register_sys_signals=True,\n        run_async=False,\n        auto_reload=False,\n    ):\n        \"\"\"Helper function used by `run` and `create_server`.\"\"\"\n        if isinstance(ssl, dict):\n            # try common aliaseses\n            cert = ssl.get(\"cert\") or ssl.get(\"certificate\")\n            key = ssl.get(\"key\") or ssl.get(\"keyfile\")\n            if cert is None or key is None:\n                raise ValueError(\"SSLContext or certificate and key required.\")\n            context = create_default_context(purpose=Purpose.CLIENT_AUTH)\n            context.load_cert_chain(cert, keyfile=key)\n            ssl = context\n        if stop_event is not None:\n            if debug:\n                warnings.simplefilter(\"default\")\n            warnings.warn(\n                \"stop_event will be removed from future versions.\",\n                DeprecationWarning,\n            )\n\n        self.error_handler.debug = debug\n        self.debug = debug\n\n        server_settings = {\n            \"protocol\": protocol,\n            \"request_class\": self.request_class,\n            \"is_request_stream\": self.is_request_stream,\n            \"router\": self.router,\n            \"host\": host,\n            \"port\": port,\n            \"sock\": sock,\n            \"ssl\": ssl,\n            \"app\": self,\n            \"signal\": Signal(),\n            \"debug\": debug,\n            \"request_handler\": self.handle_request,\n            \"error_handler\": self.error_handler,\n            \"request_timeout\": self.config.REQUEST_TIMEOUT,\n            \"response_timeout\": self.config.RESPONSE_TIMEOUT,\n            \"keep_alive_timeout\": self.config.KEEP_ALIVE_TIMEOUT,\n            \"request_max_size\": self.config.REQUEST_MAX_SIZE,\n            \"request_buffer_queue_size\": self.config.REQUEST_BUFFER_QUEUE_SIZE,\n            \"keep_alive\": self.config.KEEP_ALIVE,\n            \"loop\": loop,\n            \"register_sys_signals\": register_sys_signals,\n            \"backlog\": backlog,\n            \"access_log\": self.config.ACCESS_LOG,\n            \"websocket_max_size\": self.config.WEBSOCKET_MAX_SIZE,\n            \"websocket_max_queue\": self.config.WEBSOCKET_MAX_QUEUE,\n            \"websocket_read_limit\": self.config.WEBSOCKET_READ_LIMIT,\n            \"websocket_write_limit\": self.config.WEBSOCKET_WRITE_LIMIT,\n            \"graceful_shutdown_timeout\": self.config.GRACEFUL_SHUTDOWN_TIMEOUT,\n        }\n\n        # -------------------------------------------- #\n        # Register start/stop events\n        # -------------------------------------------- #\n\n        for event_name, settings_name, reverse in (\n            (\"before_server_start\", \"before_start\", False),\n            (\"after_server_start\", \"after_start\", False),\n            (\"before_server_stop\", \"before_stop\", True),\n            (\"after_server_stop\", \"after_stop\", True),\n        ):\n            listeners = self.listeners[event_name].copy()\n            if reverse:\n                listeners.reverse()\n            # Prepend sanic to the arguments when listeners are triggered\n            listeners = [partial(listener, self) for listener in listeners]\n            server_settings[settings_name] = listeners\n\n        if self.configure_logging and debug:\n            logger.setLevel(logging.DEBUG)\n\n        if (\n            self.config.LOGO\n            and os.environ.get(\"SANIC_SERVER_RUNNING\") != \"true\"\n        ):\n            logger.debug(\n                self.config.LOGO\n                if isinstance(self.config.LOGO, str)\n                else BASE_LOGO\n            )\n\n        if run_async:\n            server_settings[\"run_async\"] = True\n\n        # Serve\n        if host and port and os.environ.get(\"SANIC_SERVER_RUNNING\") != \"true\":\n            proto = \"http\"\n            if ssl is not None:\n                proto = \"https\"\n            logger.info(\"Goin' Fast @ {}://{}:{}\".format(proto, host, port))\n\n        return server_settings", "response": "This function is the main entry point for the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new HTTPResponse object with body in json format.", "response": "def json(\n    body,\n    status=200,\n    headers=None,\n    content_type=\"application/json\",\n    dumps=json_dumps,\n    **kwargs\n):\n    \"\"\"\n    Returns response object with body in json format.\n\n    :param body: Response data to be serialized.\n    :param status: Response code.\n    :param headers: Custom Headers.\n    :param kwargs: Remaining arguments that are passed to the json encoder.\n    \"\"\"\n    return HTTPResponse(\n        dumps(body, **kwargs),\n        headers=headers,\n        status=status,\n        content_type=content_type,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a HTTPResponse object with body in text format.", "response": "def text(\n    body, status=200, headers=None, content_type=\"text/plain; charset=utf-8\"\n):\n    \"\"\"\n    Returns response object with body in text format.\n\n    :param body: Response data to be encoded.\n    :param status: Response code.\n    :param headers: Custom Headers.\n    :param content_type: the content type (string) of the response\n    \"\"\"\n    return HTTPResponse(\n        body, status=status, headers=headers, content_type=content_type\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def file_stream(\n    location,\n    status=200,\n    chunk_size=4096,\n    mime_type=None,\n    headers=None,\n    filename=None,\n    _range=None,\n):\n    \"\"\"Return a streaming response object with file data.\n\n    :param location: Location of file on system.\n    :param chunk_size: The size of each chunk in the stream (in bytes)\n    :param mime_type: Specific mime_type.\n    :param headers: Custom Headers.\n    :param filename: Override filename.\n    :param _range:\n    \"\"\"\n    headers = headers or {}\n    if filename:\n        headers.setdefault(\n            \"Content-Disposition\", 'attachment; filename=\"{}\"'.format(filename)\n        )\n    filename = filename or path.split(location)[-1]\n\n    _file = await open_async(location, mode=\"rb\")\n\n    async def _streaming_fn(response):\n        nonlocal _file, chunk_size\n        try:\n            if _range:\n                chunk_size = min((_range.size, chunk_size))\n                await _file.seek(_range.start)\n                to_send = _range.size\n                while to_send > 0:\n                    content = await _file.read(chunk_size)\n                    if len(content) < 1:\n                        break\n                    to_send -= len(content)\n                    await response.write(content)\n            else:\n                while True:\n                    content = await _file.read(chunk_size)\n                    if len(content) < 1:\n                        break\n                    await response.write(content)\n        finally:\n            await _file.close()\n        return  # Returning from this fn closes the stream\n\n    mime_type = mime_type or guess_type(filename)[0] or \"text/plain\"\n    if _range:\n        headers[\"Content-Range\"] = \"bytes %s-%s/%s\" % (\n            _range.start,\n            _range.end,\n            _range.total,\n        )\n        status = 206\n    return StreamingHTTPResponse(\n        streaming_fn=_streaming_fn,\n        status=status,\n        headers=headers,\n        content_type=mime_type,\n    )", "response": "Return a streaming HTTP response object with file data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\naccept an coroutine `streaming_fn` which can be used to write chunks to a streaming response. Returns a `StreamingHTTPResponse`. Example usage:: @app.route(\"/\") async def index(request): async def streaming_fn(response): await response.write('foo') await response.write('bar') return stream(streaming_fn, content_type='text/plain') :param streaming_fn: A coroutine accepts a response and writes content to that response. :param mime_type: Specific mime_type. :param headers: Custom Headers.", "response": "def stream(\n    streaming_fn,\n    status=200,\n    headers=None,\n    content_type=\"text/plain; charset=utf-8\",\n):\n    \"\"\"Accepts an coroutine `streaming_fn` which can be used to\n    write chunks to a streaming response. Returns a `StreamingHTTPResponse`.\n\n    Example usage::\n\n        @app.route(\"/\")\n        async def index(request):\n            async def streaming_fn(response):\n                await response.write('foo')\n                await response.write('bar')\n\n            return stream(streaming_fn, content_type='text/plain')\n\n    :param streaming_fn: A coroutine accepts a response and\n        writes content to that response.\n    :param mime_type: Specific mime_type.\n    :param headers: Custom Headers.\n    \"\"\"\n    return StreamingHTTPResponse(\n        streaming_fn, headers=headers, content_type=content_type, status=status\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef redirect(\n    to, headers=None, status=302, content_type=\"text/html; charset=utf-8\"\n):\n    \"\"\"Abort execution and cause a 302 redirect (by default).\n\n    :param to: path or fully qualified URL to redirect to\n    :param headers: optional dict of headers to include in the new request\n    :param status: status code (int) of the new request, defaults to 302\n    :param content_type: the content type (string) of the response\n    :returns: the redirecting Response\n    \"\"\"\n    headers = headers or {}\n\n    # URL Quote the URL before redirecting\n    safe_to = quote_plus(to, safe=\":/%#?&=@[]!$&'()*+,;\")\n\n    # According to RFC 7231, a relative URI is now permitted.\n    headers[\"Location\"] = safe_to\n\n    return HTTPResponse(\n        status=status, headers=headers, content_type=content_type\n    )", "response": "Abort execution and cause a 302 redirect."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a chunk of data to the streaming response.", "response": "async def write(self, data):\n        \"\"\"Writes a chunk of data to the streaming response.\n\n        :param data: bytes-ish data to be written.\n        \"\"\"\n        if type(data) != bytes:\n            data = self._encode_body(data)\n\n        self.protocol.push_data(b\"%x\\r\\n%b\\r\\n\" % (len(data), data))\n        await self.protocol.drain()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstreaming headers runs the streaming_fn callback that writes the response body to the response body then finalizes the response body.", "response": "async def stream(\n        self, version=\"1.1\", keep_alive=False, keep_alive_timeout=None\n    ):\n        \"\"\"Streams headers, runs the `streaming_fn` callback that writes\n        content to the response body, then finalizes the response body.\n        \"\"\"\n        headers = self.get_headers(\n            version,\n            keep_alive=keep_alive,\n            keep_alive_timeout=keep_alive_timeout,\n        )\n        self.protocol.push_data(headers)\n        await self.protocol.drain()\n        await self.streaming_fn(self)\n        self.protocol.push_data(b\"0\\r\\n\\r\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninsert an item into the set at the specified index.", "response": "def insert(self, index: int, item: object) -> None:\n        \"\"\"\n        The Abstract class `MutableSequence` leverages this insert method to\n        perform the `BlueprintGroup.append` operation.\n\n        :param index: Index to use for removing a new Blueprint item\n        :param item: New `Blueprint` object.\n        :return: None\n        \"\"\"\n        self._blueprints.insert(index, item)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfetch and executes an exception handler and returns a response object.", "response": "def response(self, request, exception):\n        \"\"\"Fetches and executes an exception handler and returns a response\n        object\n\n        :param request: Instance of :class:`sanic.request.Request`\n        :param exception: Exception to handle\n\n        :type request: :class:`sanic.request.Request`\n        :type exception: :class:`sanic.exceptions.SanicException` or\n            :class:`Exception`\n\n        :return: Wrap the return value obtained from :func:`default`\n            or registered handler for that type of exception.\n        \"\"\"\n        handler = self.lookup(exception)\n        response = None\n        try:\n            if handler:\n                response = handler(request, exception)\n            if response is None:\n                response = self.default(request, exception)\n        except Exception:\n            self.log(format_exc())\n            try:\n                url = repr(request.url)\n            except AttributeError:\n                url = \"unknown\"\n            response_message = (\n                \"Exception raised in exception handler \" '\"%s\" for uri: %s'\n            )\n            logger.exception(response_message, handler.__name__, url)\n\n            if self.debug:\n                return text(response_message % (handler.__name__, url), 500)\n            else:\n                return text(\"An error occurred while handling an error\", 500)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef default(self, request, exception):\n        self.log(format_exc())\n        try:\n            url = repr(request.url)\n        except AttributeError:\n            url = \"unknown\"\n\n        response_message = \"Exception occurred while handling uri: %s\"\n        logger.exception(response_message, url)\n\n        if issubclass(type(exception), SanicException):\n            return text(\n                \"Error: {}\".format(exception),\n                status=getattr(exception, \"status_code\", 500),\n                headers=getattr(exception, \"headers\", dict()),\n            )\n        elif self.debug:\n            html_output = self._render_traceback_html(exception, request)\n\n            return html(html_output, status=500)\n        else:\n            return html(INTERNAL_SERVER_ERROR_HTML, status=500)", "response": "Default behavior for the objects of ErrorHandler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an SSL context instance for usage in asyncio. create_server.", "response": "def _create_ssl_context(cfg):\n        \"\"\" Creates SSLContext instance for usage in asyncio.create_server.\n        See ssl.SSLSocket.__init__ for more details.\n        \"\"\"\n        ctx = ssl.SSLContext(cfg.ssl_version)\n        ctx.load_cert_chain(cfg.certfile, cfg.keyfile)\n        ctx.verify_mode = cfg.cert_reqs\n        if cfg.ca_certs:\n            ctx.load_verify_locations(cfg.ca_certs)\n        if cfg.ciphers:\n            ctx.set_ciphers(cfg.ciphers)\n        return ctx"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef trigger_events(events, loop):\n    for event in events:\n        result = event(loop)\n        if isawaitable(result):\n            loop.run_until_complete(result)", "response": "Trigger events on the sequence of events in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef serve(\n    host,\n    port,\n    app,\n    request_handler,\n    error_handler,\n    before_start=None,\n    after_start=None,\n    before_stop=None,\n    after_stop=None,\n    debug=False,\n    request_timeout=60,\n    response_timeout=60,\n    keep_alive_timeout=5,\n    ssl=None,\n    sock=None,\n    request_max_size=None,\n    request_buffer_queue_size=100,\n    reuse_port=False,\n    loop=None,\n    protocol=HttpProtocol,\n    backlog=100,\n    register_sys_signals=True,\n    run_multiple=False,\n    run_async=False,\n    connections=None,\n    signal=Signal(),\n    request_class=None,\n    access_log=True,\n    keep_alive=True,\n    is_request_stream=False,\n    router=None,\n    websocket_max_size=None,\n    websocket_max_queue=None,\n    websocket_read_limit=2 ** 16,\n    websocket_write_limit=2 ** 16,\n    state=None,\n    graceful_shutdown_timeout=15.0,\n    asyncio_server_kwargs=None,\n):\n    \"\"\"Start asynchronous HTTP Server on an individual process.\n\n    :param host: Address to host on\n    :param port: Port to host on\n    :param request_handler: Sanic request handler with middleware\n    :param error_handler: Sanic error handler with middleware\n    :param before_start: function to be executed before the server starts\n                         listening. Takes arguments `app` instance and `loop`\n    :param after_start: function to be executed after the server starts\n                        listening. Takes  arguments `app` instance and `loop`\n    :param before_stop: function to be executed when a stop signal is\n                        received before it is respected. Takes arguments\n                        `app` instance and `loop`\n    :param after_stop: function to be executed when a stop signal is\n                       received after it is respected. Takes arguments\n                       `app` instance and `loop`\n    :param debug: enables debug output (slows server)\n    :param request_timeout: time in seconds\n    :param response_timeout: time in seconds\n    :param keep_alive_timeout: time in seconds\n    :param ssl: SSLContext\n    :param sock: Socket for the server to accept connections from\n    :param request_max_size: size in bytes, `None` for no limit\n    :param reuse_port: `True` for multiple workers\n    :param loop: asyncio compatible event loop\n    :param protocol: subclass of asyncio protocol class\n    :param request_class: Request class to use\n    :param access_log: disable/enable access log\n    :param websocket_max_size: enforces the maximum size for\n                               incoming messages in bytes.\n    :param websocket_max_queue: sets the maximum length of the queue\n                                that holds incoming messages.\n    :param websocket_read_limit: sets the high-water limit of the buffer for\n                                 incoming bytes, the low-water limit is half\n                                 the high-water limit.\n    :param websocket_write_limit: sets the high-water limit of the buffer for\n                                  outgoing bytes, the low-water limit is a\n                                  quarter of the high-water limit.\n    :param is_request_stream: disable/enable Request.stream\n    :param request_buffer_queue_size: streaming request buffer queue size\n    :param router: Router object\n    :param graceful_shutdown_timeout: How long take to Force close non-idle\n                                      connection\n    :param asyncio_server_kwargs: key-value args for asyncio/uvloop\n                                  create_server method\n    :return: Nothing\n    \"\"\"\n    if not run_async:\n        # create new event_loop after fork\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n    if debug:\n        loop.set_debug(debug)\n\n    connections = connections if connections is not None else set()\n    server = partial(\n        protocol,\n        loop=loop,\n        connections=connections,\n        signal=signal,\n        app=app,\n        request_handler=request_handler,\n        error_handler=error_handler,\n        request_timeout=request_timeout,\n        response_timeout=response_timeout,\n        keep_alive_timeout=keep_alive_timeout,\n        request_max_size=request_max_size,\n        request_class=request_class,\n        access_log=access_log,\n        keep_alive=keep_alive,\n        is_request_stream=is_request_stream,\n        router=router,\n        websocket_max_size=websocket_max_size,\n        websocket_max_queue=websocket_max_queue,\n        websocket_read_limit=websocket_read_limit,\n        websocket_write_limit=websocket_write_limit,\n        state=state,\n        debug=debug,\n    )\n    asyncio_server_kwargs = (\n        asyncio_server_kwargs if asyncio_server_kwargs else {}\n    )\n    server_coroutine = loop.create_server(\n        server,\n        host,\n        port,\n        ssl=ssl,\n        reuse_port=reuse_port,\n        sock=sock,\n        backlog=backlog,\n        **asyncio_server_kwargs\n    )\n\n    if run_async:\n        return server_coroutine\n\n    trigger_events(before_start, loop)\n\n    try:\n        http_server = loop.run_until_complete(server_coroutine)\n    except BaseException:\n        logger.exception(\"Unable to start server\")\n        return\n\n    trigger_events(after_start, loop)\n\n    # Ignore SIGINT when run_multiple\n    if run_multiple:\n        signal_func(SIGINT, SIG_IGN)\n\n    # Register signals for graceful termination\n    if register_sys_signals:\n        _singals = (SIGTERM,) if run_multiple else (SIGINT, SIGTERM)\n        for _signal in _singals:\n            try:\n                loop.add_signal_handler(_signal, loop.stop)\n            except NotImplementedError:\n                logger.warning(\n                    \"Sanic tried to use loop.add_signal_handler \"\n                    \"but it is not implemented on this platform.\"\n                )\n    pid = os.getpid()\n    try:\n        logger.info(\"Starting worker [%s]\", pid)\n        loop.run_forever()\n    finally:\n        logger.info(\"Stopping worker [%s]\", pid)\n\n        # Run the on_stop function if provided\n        trigger_events(before_stop, loop)\n\n        # Wait for event loop to finish and all connections to drain\n        http_server.close()\n        loop.run_until_complete(http_server.wait_closed())\n\n        # Complete all tasks on the loop\n        signal.stopped = True\n        for connection in connections:\n            connection.close_if_idle()\n\n        # Gracefully shutdown timeout.\n        # We should provide graceful_shutdown_timeout,\n        # instead of letting connection hangs forever.\n        # Let's roughly calcucate time.\n        start_shutdown = 0\n        while connections and (start_shutdown < graceful_shutdown_timeout):\n            loop.run_until_complete(asyncio.sleep(0.1))\n            start_shutdown = start_shutdown + 0.1\n\n        # Force close non-idle connection after waiting for\n        # graceful_shutdown_timeout\n        coros = []\n        for conn in connections:\n            if hasattr(conn, \"websocket\") and conn.websocket:\n                coros.append(conn.websocket.close_connection())\n            else:\n                conn.close()\n\n        _shutdown = asyncio.gather(*coros, loop=loop)\n        loop.run_until_complete(_shutdown)\n\n        trigger_events(after_stop, loop)\n\n        loop.close()", "response": "Start asynchronous HTTP Server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the connection needs to be kept alive based on the params attribute.", "response": "def keep_alive(self):\n        \"\"\"\n        Check if the connection needs to be kept alive based on the params\n        attached to the `_keep_alive` attribute, :attr:`Signal.stopped`\n        and :func:`HttpProtocol.parser.should_keep_alive`\n\n        :return: ``True`` if connection is to be kept alive ``False`` else\n        \"\"\"\n        return (\n            self._keep_alive\n            and not self.signal.stopped\n            and self.parser.should_keep_alive()\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef keep_alive_timeout_callback(self):\n        time_elapsed = time() - self._last_response_time\n        if time_elapsed < self.keep_alive_timeout:\n            time_left = self.keep_alive_timeout - time_elapsed\n            self._keep_alive_timeout_handler = self.loop.call_later(\n                time_left, self.keep_alive_timeout_callback\n            )\n        else:\n            logger.debug(\"KeepAlive Timeout. Closing connection.\")\n            self.transport.close()\n            self.transport = None", "response": "Callback function that checks if elapsed time since last response exceeds our configured keep alive timeout value and closes the transport pipe and let the response writer handle the error."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute_request_handler(self):\n        self._response_timeout_handler = self.loop.call_later(\n            self.response_timeout, self.response_timeout_callback\n        )\n        self._last_request_time = time()\n        self._request_handler_task = self.loop.create_task(\n            self.request_handler(\n                self.request, self.write_response, self.stream_response\n            )\n        )", "response": "Execute the request handler defined by the\n            class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log_response(self, response):\n        if self.access_log:\n            extra = {\"status\": getattr(response, \"status\", 0)}\n\n            if isinstance(response, HTTPResponse):\n                extra[\"byte\"] = len(response.body)\n            else:\n                extra[\"byte\"] = -1\n\n            extra[\"host\"] = \"UNKNOWN\"\n            if self.request is not None:\n                if self.request.ip:\n                    extra[\"host\"] = \"{0}:{1}\".format(\n                        self.request.ip, self.request.port\n                    )\n\n                extra[\"request\"] = \"{0} {1}\".format(\n                    self.request.method, self.request.url\n                )\n            else:\n                extra[\"request\"] = \"nil\"\n\n            access_logger.info(\"\", extra=extra)", "response": "Log the response to the log file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_response(self, response):\n        if self._response_timeout_handler:\n            self._response_timeout_handler.cancel()\n            self._response_timeout_handler = None\n        try:\n            keep_alive = self.keep_alive\n            self.transport.write(\n                response.output(\n                    self.request.version, keep_alive, self.keep_alive_timeout\n                )\n            )\n            self.log_response(response)\n        except AttributeError:\n            logger.error(\n                \"Invalid response object for url %s, \"\n                \"Expected Type: HTTPResponse, Actual Type: %s\",\n                self.url,\n                type(response),\n            )\n            self.write_error(ServerError(\"Invalid response type\"))\n        except RuntimeError:\n            if self._debug:\n                logger.error(\n                    \"Connection lost before response written @ %s\",\n                    self.request.ip,\n                )\n            keep_alive = False\n        except Exception as e:\n            self.bail_out(\n                \"Writing response failed, connection closed {}\".format(repr(e))\n            )\n        finally:\n            if not keep_alive:\n                self.transport.close()\n                self.transport = None\n            else:\n                self._keep_alive_timeout_handler = self.loop.call_later(\n                    self.keep_alive_timeout, self.keep_alive_timeout_callback\n                )\n                self._last_response_time = time()\n                self.cleanup()", "response": "Writes the response to the transport."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bail_out(self, message, from_error=False):\n        if from_error or self.transport is None or self.transport.is_closing():\n            logger.error(\n                \"Transport closed @ %s and exception \"\n                \"experienced during error handling\",\n                (\n                    self.transport.get_extra_info(\"peername\")\n                    if self.transport is not None\n                    else \"N/A\"\n                ),\n            )\n            logger.debug(\"Exception:\", exc_info=True)\n        else:\n            self.write_error(ServerError(message))\n            logger.error(message)", "response": "This method is called when an error occurs while writing data to the transport pipe."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_multipart_form(body, boundary):\n    files = RequestParameters()\n    fields = RequestParameters()\n\n    form_parts = body.split(boundary)\n    for form_part in form_parts[1:-1]:\n        file_name = None\n        content_type = \"text/plain\"\n        content_charset = \"utf-8\"\n        field_name = None\n        line_index = 2\n        line_end_index = 0\n        while not line_end_index == -1:\n            line_end_index = form_part.find(b\"\\r\\n\", line_index)\n            form_line = form_part[line_index:line_end_index].decode(\"utf-8\")\n            line_index = line_end_index + 2\n\n            if not form_line:\n                break\n\n            colon_index = form_line.index(\":\")\n            form_header_field = form_line[0:colon_index].lower()\n            form_header_value, form_parameters = parse_header(\n                form_line[colon_index + 2 :]\n            )\n\n            if form_header_field == \"content-disposition\":\n                field_name = form_parameters.get(\"name\")\n                file_name = form_parameters.get(\"filename\")\n\n                # non-ASCII filenames in RFC2231, \"filename*\" format\n                if file_name is None and form_parameters.get(\"filename*\"):\n                    encoding, _, value = email.utils.decode_rfc2231(\n                        form_parameters[\"filename*\"]\n                    )\n                    file_name = unquote(value, encoding=encoding)\n            elif form_header_field == \"content-type\":\n                content_type = form_header_value\n                content_charset = form_parameters.get(\"charset\", \"utf-8\")\n\n        if field_name:\n            post_data = form_part[line_index:-4]\n            if file_name is None:\n                value = post_data.decode(content_charset)\n                if field_name in fields:\n                    fields[field_name].append(value)\n                else:\n                    fields[field_name] = [value]\n            else:\n                form_file = File(\n                    type=content_type, name=file_name, body=post_data\n                )\n                if field_name in files:\n                    files[field_name].append(form_file)\n                else:\n                    files[field_name] = [form_file]\n        else:\n            logger.debug(\n                \"Form-data field does not have a 'name' parameter \"\n                \"in the Content-Disposition header\"\n            )\n\n    return fields, files", "response": "Parse a multipart request body and return a list of RequestParameters objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a payload from the queue.", "response": "async def read(self):\n        \"\"\" Stop reading when gets None \"\"\"\n        payload = await self._queue.get()\n        self._queue.task_done()\n        return payload"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef token(self):\n        prefixes = (\"Bearer\", \"Token\")\n        auth_header = self.headers.get(\"Authorization\")\n\n        if auth_header is not None:\n            for prefix in prefixes:\n                if prefix in auth_header:\n                    return auth_header.partition(prefix)[-1].strip()\n\n        return auth_header", "response": "Attempt to return the auth header token."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_query_args(\n        self,\n        keep_blank_values: bool = False,\n        strict_parsing: bool = False,\n        encoding: str = \"utf-8\",\n        errors: str = \"replace\",\n    ) -> list:\n        \"\"\"\n        Method to parse `query_string` using `urllib.parse.parse_qsl`.\n        This methods is used by `query_args` property.\n        Can be used directly if you need to change default parameters.\n        :param keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n        :type keep_blank_values: bool\n        :param strict_parsing: flag indicating what to do with parsing errors.\n            If false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception.\n        :type strict_parsing: bool\n        :param encoding: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n        :type encoding: str\n        :param errors: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n        :type errors: str\n        :return: list\n        \"\"\"\n        if not self.parsed_not_grouped_args[\n            (keep_blank_values, strict_parsing, encoding, errors)\n        ]:\n            if self.query_string:\n                self.parsed_not_grouped_args[\n                    (keep_blank_values, strict_parsing, encoding, errors)\n                ] = parse_qsl(\n                    qs=self.query_string,\n                    keep_blank_values=keep_blank_values,\n                    strict_parsing=strict_parsing,\n                    encoding=encoding,\n                    errors=errors,\n                )\n        return self.parsed_not_grouped_args[\n            (keep_blank_values, strict_parsing, encoding, errors)\n        ]", "response": "Method to parse the query_string of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nattempting to return the original client ip based on X - Forwarded - For or X - Real - IP or X - Real - IP.", "response": "def remote_addr(self):\n        \"\"\"Attempt to return the original client ip based on X-Forwarded-For\n        or X-Real-IP. If HTTP headers are unavailable or untrusted, returns\n        an empty string.\n\n        :return: original client ip.\n        \"\"\"\n        if not hasattr(self, \"_remote_addr\"):\n            if self.app.config.PROXIES_COUNT == 0:\n                self._remote_addr = \"\"\n            elif self.app.config.REAL_IP_HEADER and self.headers.get(\n                self.app.config.REAL_IP_HEADER\n            ):\n                self._remote_addr = self.headers[\n                    self.app.config.REAL_IP_HEADER\n                ]\n            elif self.app.config.FORWARDED_FOR_HEADER:\n                forwarded_for = self.headers.get(\n                    self.app.config.FORWARDED_FOR_HEADER, \"\"\n                ).split(\",\")\n                remote_addrs = [\n                    addr\n                    for addr in [addr.strip() for addr in forwarded_for]\n                    if addr\n                ]\n                if self.app.config.PROXIES_COUNT == -1:\n                    self._remote_addr = remote_addrs[0]\n                elif len(remote_addrs) >= self.app.config.PROXIES_COUNT:\n                    self._remote_addr = remote_addrs[\n                        -self.app.config.PROXIES_COUNT\n                    ]\n                else:\n                    self._remote_addr = \"\"\n            else:\n                self._remote_addr = \"\"\n        return self._remote_addr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef strtobool(val):\n    val = val.lower()\n    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n        return True\n    elif val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n        return False\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))", "response": "Convert string to boolean."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a configuration from an environment variable pointing to a configuration file.", "response": "def from_envvar(self, variable_name):\n        \"\"\"Load a configuration from an environment variable pointing to\n        a configuration file.\n\n        :param variable_name: name of the environment variable\n        :return: bool. ``True`` if able to load config, ``False`` otherwise.\n        \"\"\"\n        config_file = os.environ.get(variable_name)\n        if not config_file:\n            raise RuntimeError(\n                \"The environment variable %r is not set and \"\n                \"thus configuration could not be loaded.\" % variable_name\n            )\n        return self.from_pyfile(config_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the values in the configuration object with the values from the given object.", "response": "def from_object(self, obj):\n        \"\"\"Update the values from the given object.\n        Objects are usually either modules or classes.\n\n        Just the uppercase variables in that object are stored in the config.\n        Example usage::\n\n            from yourapplication import default_config\n            app.config.from_object(default_config)\n\n        You should not use this function to load the actual configuration but\n        rather configuration defaults. The actual config should be loaded\n        with :meth:`from_pyfile` and ideally from a location not within the\n        package because the package might be installed system wide.\n\n        :param obj: an object holding the configuration\n        \"\"\"\n        for key in dir(obj):\n            if key.isupper():\n                self[key] = getattr(obj, key)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the environment variables from the environment variables file.", "response": "def load_environment_vars(self, prefix=SANIC_PREFIX):\n        \"\"\"\n        Looks for prefixed environment variables and applies\n        them to the configuration if present.\n        \"\"\"\n        for k, v in os.environ.items():\n            if k.startswith(prefix):\n                _, config_key = k.split(prefix, 1)\n                try:\n                    self[config_key] = int(v)\n                except ValueError:\n                    try:\n                        self[config_key] = float(v)\n                    except ValueError:\n                        try:\n                            self[config_key] = strtobool(v)\n                        except ValueError:\n                            self[config_key] = v"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a string into its constituent name type and pattern.", "response": "def parse_parameter_string(cls, parameter_string):\n        \"\"\"Parse a parameter string into its constituent name, type, and\n        pattern\n\n        For example::\n\n            parse_parameter_string('<param_one:[A-z]>')` ->\n                ('param_one', str, '[A-z]')\n\n        :param parameter_string: String to parse\n        :return: tuple containing\n            (parameter_name, parameter_type, parameter_pattern)\n        \"\"\"\n        # We could receive NAME or NAME:PATTERN\n        name = parameter_string\n        pattern = \"string\"\n        if \":\" in parameter_string:\n            name, pattern = parameter_string.split(\":\", 1)\n            if not name:\n                raise ValueError(\n                    \"Invalid parameter syntax: {}\".format(parameter_string)\n                )\n\n        default = (str, pattern)\n        # Pull from pre-configured types\n        _type, pattern = REGEX_TYPES.get(pattern, default)\n\n        return name, _type, pattern"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(\n        self,\n        uri,\n        methods,\n        handler,\n        host=None,\n        strict_slashes=False,\n        version=None,\n        name=None,\n    ):\n        \"\"\"Add a handler to the route list\n\n        :param uri: path to match\n        :param methods: sequence of accepted method names. If none are\n            provided, any method is allowed\n        :param handler: request handler function.\n            When executed, it should provide a response object.\n        :param strict_slashes: strict to trailing slash\n        :param version: current version of the route or blueprint. See\n            docs for further details.\n        :return: Nothing\n        \"\"\"\n        if version is not None:\n            version = re.escape(str(version).strip(\"/\").lstrip(\"v\"))\n            uri = \"/\".join([\"/v{}\".format(version), uri.lstrip(\"/\")])\n        # add regular version\n        self._add(uri, methods, handler, host, name)\n\n        if strict_slashes:\n            return\n\n        if not isinstance(host, str) and host is not None:\n            # we have gotten back to the top of the recursion tree where the\n            # host was originally a list. By now, we've processed the strict\n            # slashes logic on the leaf nodes (the individual host strings in\n            # the list of host)\n            return\n\n        # Add versions with and without trailing /\n        slashed_methods = self.routes_all.get(uri + \"/\", frozenset({}))\n        unslashed_methods = self.routes_all.get(uri[:-1], frozenset({}))\n        if isinstance(methods, Iterable):\n            _slash_is_missing = all(\n                method in slashed_methods for method in methods\n            )\n            _without_slash_is_missing = all(\n                method in unslashed_methods for method in methods\n            )\n        else:\n            _slash_is_missing = methods in slashed_methods\n            _without_slash_is_missing = methods in unslashed_methods\n\n        slash_is_missing = not uri[-1] == \"/\" and not _slash_is_missing\n        without_slash_is_missing = (\n            uri[-1] == \"/\" and not _without_slash_is_missing and not uri == \"/\"\n        )\n        # add version with trailing slash\n        if slash_is_missing:\n            self._add(uri + \"/\", methods, handler, host, name)\n        # add version without trailing slash\n        elif without_slash_is_missing:\n            self._add(uri[:-1], methods, handler, host, name)", "response": "Add a handler function to the route list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a handler function to the route list.", "response": "def _add(self, uri, methods, handler, host=None, name=None):\n        \"\"\"Add a handler to the route list\n\n        :param uri: path to match\n        :param methods: sequence of accepted method names. If none are\n            provided, any method is allowed\n        :param handler: request handler function.\n            When executed, it should provide a response object.\n        :param name: user defined route name for url_for\n        :return: Nothing\n        \"\"\"\n        if host is not None:\n            if isinstance(host, str):\n                uri = host + uri\n                self.hosts.add(host)\n\n            else:\n                if not isinstance(host, Iterable):\n                    raise ValueError(\n                        \"Expected either string or Iterable of \"\n                        \"host strings, not {!r}\".format(host)\n                    )\n\n                for host_ in host:\n                    self.add(uri, methods, handler, host_, name)\n                return\n\n        # Dict for faster lookups of if method allowed\n        if methods:\n            methods = frozenset(methods)\n\n        parameters = []\n        parameter_names = set()\n        properties = {\"unhashable\": None}\n\n        def add_parameter(match):\n            name = match.group(1)\n            name, _type, pattern = self.parse_parameter_string(name)\n\n            if name in parameter_names:\n                raise ParameterNameConflicts(\n                    \"Multiple parameter named <{name}> \"\n                    \"in route uri {uri}\".format(name=name, uri=uri)\n                )\n            parameter_names.add(name)\n\n            parameter = Parameter(name=name, cast=_type)\n            parameters.append(parameter)\n\n            # Mark the whole route as unhashable if it has the hash key in it\n            if re.search(r\"(^|[^^]){1}/\", pattern):\n                properties[\"unhashable\"] = True\n            # Mark the route as unhashable if it matches the hash key\n            elif re.search(r\"/\", pattern):\n                properties[\"unhashable\"] = True\n\n            return \"({})\".format(pattern)\n\n        pattern_string = re.sub(self.parameter_pattern, add_parameter, uri)\n        pattern = re.compile(r\"^{}$\".format(pattern_string))\n\n        def merge_route(route, methods, handler):\n            # merge to the existing route when possible.\n            if not route.methods or not methods:\n                # method-unspecified routes are not mergeable.\n                raise RouteExists(\"Route already registered: {}\".format(uri))\n            elif route.methods.intersection(methods):\n                # already existing method is not overloadable.\n                duplicated = methods.intersection(route.methods)\n                raise RouteExists(\n                    \"Route already registered: {} [{}]\".format(\n                        uri, \",\".join(list(duplicated))\n                    )\n                )\n            if isinstance(route.handler, CompositionView):\n                view = route.handler\n            else:\n                view = CompositionView()\n                view.add(route.methods, route.handler)\n            view.add(methods, handler)\n            route = route._replace(\n                handler=view, methods=methods.union(route.methods)\n            )\n            return route\n\n        if parameters:\n            # TODO: This is too complex, we need to reduce the complexity\n            if properties[\"unhashable\"]:\n                routes_to_check = self.routes_always_check\n                ndx, route = self.check_dynamic_route_exists(\n                    pattern, routes_to_check, parameters\n                )\n            else:\n                routes_to_check = self.routes_dynamic[url_hash(uri)]\n                ndx, route = self.check_dynamic_route_exists(\n                    pattern, routes_to_check, parameters\n                )\n            if ndx != -1:\n                # Pop the ndx of the route, no dups of the same route\n                routes_to_check.pop(ndx)\n        else:\n            route = self.routes_all.get(uri)\n\n        # prefix the handler name with the blueprint name\n        # if available\n        # special prefix for static files\n        is_static = False\n        if name and name.startswith(\"_static_\"):\n            is_static = True\n            name = name.split(\"_static_\", 1)[-1]\n\n        if hasattr(handler, \"__blueprintname__\"):\n            handler_name = \"{}.{}\".format(\n                handler.__blueprintname__, name or handler.__name__\n            )\n        else:\n            handler_name = name or getattr(handler, \"__name__\", None)\n\n        if route:\n            route = merge_route(route, methods, handler)\n        else:\n            route = Route(\n                handler=handler,\n                methods=methods,\n                pattern=pattern,\n                parameters=parameters,\n                name=handler_name,\n                uri=uri,\n            )\n\n        self.routes_all[uri] = route\n        if is_static:\n            pair = self.routes_static_files.get(handler_name)\n            if not (pair and (pair[0] + \"/\" == uri or uri + \"/\" == pair[0])):\n                self.routes_static_files[handler_name] = (uri, route)\n\n        else:\n            pair = self.routes_names.get(handler_name)\n            if not (pair and (pair[0] + \"/\" == uri or uri + \"/\" == pair[0])):\n                self.routes_names[handler_name] = (uri, route)\n\n        if properties[\"unhashable\"]:\n            self.routes_always_check.append(route)\n        elif parameters:\n            self.routes_dynamic[url_hash(uri)].append(route)\n        else:\n            self.routes_static[uri] = route"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a dynamic route exists in a list of routes provided based on the comparison of pattern and parameters.", "response": "def check_dynamic_route_exists(pattern, routes_to_check, parameters):\n        \"\"\"\n        Check if a URL pattern exists in a list of routes provided based on\n        the comparison of URL pattern and the parameters.\n\n        :param pattern: URL parameter pattern\n        :param routes_to_check: list of dynamic routes either hashable or\n            unhashable routes.\n        :param parameters: List of :class:`Parameter` items\n        :return: Tuple of index and route if matching route exists else\n            -1 for index and None for route\n        \"\"\"\n        for ndx, route in enumerate(routes_to_check):\n            if route.pattern == pattern and route.parameters == parameters:\n                return ndx, route\n        else:\n            return -1, None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind a route in the router based on the view name.", "response": "def find_route_by_view_name(self, view_name, name=None):\n        \"\"\"Find a route in the router based on the specified view name.\n\n        :param view_name: string of view name to search by\n        :param kwargs: additional params, usually for static files\n        :return: tuple containing (uri, Route)\n        \"\"\"\n        if not view_name:\n            return (None, None)\n\n        if view_name == \"static\" or view_name.endswith(\".static\"):\n            return self.routes_static_files.get(name, (None, None))\n\n        return self.routes_names.get(view_name, (None, None))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, request):\n        # No virtual hosts specified; default behavior\n        if not self.hosts:\n            return self._get(request.path, request.method, \"\")\n        # virtual hosts specified; try to match route to the host header\n        try:\n            return self._get(\n                request.path, request.method, request.headers.get(\"Host\", \"\")\n            )\n        # try default hosts\n        except NotFound:\n            return self._get(request.path, request.method, \"\")", "response": "Get a request handler based on the URL of the request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_supported_methods(self, url):\n        route = self.routes_all.get(url)\n        # if methods are None then this logic will prevent an error\n        return getattr(route, \"methods\", None) or frozenset()", "response": "Get a list of supported methods for a given url and optional host."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get(self, url, method, host):\n        url = unquote(host + url)\n        # Check against known static routes\n        route = self.routes_static.get(url)\n        method_not_supported = MethodNotSupported(\n            \"Method {} not allowed for URL {}\".format(method, url),\n            method=method,\n            allowed_methods=self.get_supported_methods(url),\n        )\n        if route:\n            if route.methods and method not in route.methods:\n                raise method_not_supported\n            match = route.pattern.match(url)\n        else:\n            route_found = False\n            # Move on to testing all regex routes\n            for route in self.routes_dynamic[url_hash(url)]:\n                match = route.pattern.match(url)\n                route_found |= match is not None\n                # Do early method checking\n                if match and method in route.methods:\n                    break\n            else:\n                # Lastly, check against all regex routes that cannot be hashed\n                for route in self.routes_always_check:\n                    match = route.pattern.match(url)\n                    route_found |= match is not None\n                    # Do early method checking\n                    if match and method in route.methods:\n                        break\n                else:\n                    # Route was found but the methods didn't match\n                    if route_found:\n                        raise method_not_supported\n                    raise NotFound(\"Requested URL {} not found\".format(url))\n\n        kwargs = {\n            p.name: p.cast(value)\n            for value, p in zip(match.groups(1), route.parameters)\n        }\n        route_handler = route.handler\n        if hasattr(route_handler, \"handlers\"):\n            route_handler = route_handler.handlers[method]\n        return route_handler, [], kwargs, route.uri", "response": "Internal method for caching."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_stream_handler(self, request):\n        try:\n            handler = self.get(request)[0]\n        except (NotFound, MethodNotSupported):\n            return False\n        if hasattr(handler, \"view_class\") and hasattr(\n            handler.view_class, request.method.lower()\n        ):\n            handler = getattr(handler.view_class, request.method.lower())\n        return hasattr(handler, \"is_stream\")", "response": "Returns True if the request is a stream handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the executable to use for reloading.", "response": "def _get_args_for_reloading():\n    \"\"\"Returns the executable.\"\"\"\n    rv = [sys.executable]\n    main_module = sys.modules[\"__main__\"]\n    mod_spec = getattr(main_module, \"__spec__\", None)\n    if mod_spec:\n        # Parent exe was launched as a module rather than a script\n        rv.extend([\"-m\", mod_spec.name])\n        if len(sys.argv) > 1:\n            rv.extend(sys.argv[1:])\n    else:\n        rv.extend(sys.argv)\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef restart_with_reloader():\n    cwd = os.getcwd()\n    args = _get_args_for_reloading()\n    new_environ = os.environ.copy()\n    new_environ[\"SANIC_SERVER_RUNNING\"] = \"true\"\n    cmd = \" \".join(args)\n    worker_process = Process(\n        target=subprocess.call,\n        args=(cmd,),\n        kwargs={\"cwd\": cwd, \"shell\": True, \"env\": new_environ},\n    )\n    worker_process.start()\n    return worker_process", "response": "Create a new process and a subprocess in it with the same arguments as\n    this one."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef kill_process_children_unix(pid):\n    root_process_path = \"/proc/{pid}/task/{pid}/children\".format(pid=pid)\n    if not os.path.isfile(root_process_path):\n        return\n    with open(root_process_path) as children_list_file:\n        children_list_pid = children_list_file.read().split()\n\n    for child_pid in children_list_pid:\n        children_proc_path = \"/proc/%s/task/%s/children\" % (\n            child_pid,\n            child_pid,\n        )\n        if not os.path.isfile(children_proc_path):\n            continue\n        with open(children_proc_path) as children_list_file_2:\n            children_list_pid_2 = children_list_file_2.read().split()\n        for _pid in children_list_pid_2:\n            try:\n                os.kill(int(_pid), signal.SIGTERM)\n            except ProcessLookupError:\n                continue\n        try:\n            os.kill(int(child_pid), signal.SIGTERM)\n        except ProcessLookupError:\n            continue", "response": "Find and kill child processes of a process on Unix systems."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind and kill child processes of a process.", "response": "def kill_process_children(pid):\n    \"\"\"Find and kill child processes of a process.\n\n    :param pid: PID of parent process (process ID)\n    :return: Nothing\n    \"\"\"\n    if sys.platform == \"darwin\":\n        kill_process_children_osx(pid)\n    elif sys.platform == \"linux\":\n        kill_process_children_unix(pid)\n    else:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwatching project files restart worker process if a change happened.", "response": "def watchdog(sleep_interval):\n    \"\"\"Watch project files, restart worker process if a change happened.\n\n    :param sleep_interval: interval in second.\n    :return: Nothing\n    \"\"\"\n    mtimes = {}\n    worker_process = restart_with_reloader()\n    signal.signal(\n        signal.SIGTERM, lambda *args: kill_program_completly(worker_process)\n    )\n    signal.signal(\n        signal.SIGINT, lambda *args: kill_program_completly(worker_process)\n    )\n    while True:\n        for filename in _iter_module_files():\n            try:\n                mtime = os.stat(filename).st_mtime\n            except OSError:\n                continue\n\n            old_time = mtimes.get(filename)\n            if old_time is None:\n                mtimes[filename] = mtime\n                continue\n            elif mtime > old_time:\n                kill_process_children(worker_process.pid)\n                worker_process.terminate()\n                worker_process = restart_with_reloader()\n                mtimes[filename] = mtime\n                break\n\n        sleep(sleep_interval)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a view function for use with the routing system that dispatches request to appropriate handler method.", "response": "def as_view(cls, *class_args, **class_kwargs):\n        \"\"\"Return view function for use with the routing system, that\n        dispatches request to appropriate handler method.\n        \"\"\"\n\n        def view(*args, **kwargs):\n            self = view.view_class(*class_args, **class_kwargs)\n            return self.dispatch_request(*args, **kwargs)\n\n        if cls.decorators:\n            view.__module__ = cls.__module__\n            for decorator in cls.decorators:\n                view = decorator(view)\n\n        view.view_class = cls\n        view.__doc__ = cls.__doc__\n        view.__module__ = cls.__module__\n        view.__name__ = cls.__name__\n        return view"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a single resource from the specified url using the given session object.", "response": "async def bounded_fetch(session, url):\n    \"\"\"\n    Use session object to perform 'get' request on url\n    \"\"\"\n    async with sem, session.get(url) as response:\n        return await response.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_entity_headers(headers, allowed=(\"content-location\", \"expires\")):\n    allowed = set([h.lower() for h in allowed])\n    headers = {\n        header: value\n        for header, value in headers.items()\n        if not is_entity_header(header) or header.lower() in allowed\n    }\n    return headers", "response": "Removes all the entity headers present in the given headers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npreserve shape of the image.", "response": "def preserve_shape(func):\n    \"\"\"Preserve shape of the image.\"\"\"\n    @wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        shape = img.shape\n        result = func(img, *args, **kwargs)\n        result = result.reshape(shape)\n        return result\n\n    return wrapped_function"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef preserve_channel_dim(func):\n    @wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        shape = img.shape\n        result = func(img, *args, **kwargs)\n        if len(shape) == 3 and shape[-1] == 1 and len(result.shape) == 2:\n            result = np.expand_dims(result, axis=-1)\n        return result\n\n    return wrapped_function", "response": "Wrap a function to preserve dummy channel dim."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a random snow to the base image.", "response": "def add_snow(img, snow_point, brightness_coeff):\n    \"\"\"Bleaches out pixels, mitation snow.\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img:\n        snow_point:\n        brightness_coeff:\n\n    Returns:\n\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    snow_point *= 127.5  # = 255 / 2\n    snow_point += 85  # = 255 / 3\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype('uint8'))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError('Unexpected dtype {} for RandomSnow augmentation'.format(input_dtype))\n\n    image_HLS = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    image_HLS = np.array(image_HLS, dtype=np.float32)\n\n    image_HLS[:, :, 1][image_HLS[:, :, 1] < snow_point] *= brightness_coeff\n\n    image_HLS[:, :, 1] = clip(image_HLS[:, :, 1], np.uint8, 255)\n\n    image_HLS = np.array(image_HLS, dtype=np.uint8)\n\n    image_RGB = cv2.cvtColor(image_HLS, cv2.COLOR_HLS2RGB)\n\n    if needs_float:\n        image_RGB = to_float(image_RGB, max_value=255)\n\n    return image_RGB"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding fog to the image.", "response": "def add_fog(img, fog_coef, alpha_coef, haze_list):\n    \"\"\"Add fog to the image.\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img (np.array):\n        fog_coef (float):\n        alpha_coef (float):\n        haze_list (list):\n    Returns:\n\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype('uint8'))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError('Unexpected dtype {} for RandomFog augmentation'.format(input_dtype))\n\n    height, width = img.shape[:2]\n\n    hw = max(int(width // 3 * fog_coef), 10)\n\n    for haze_points in haze_list:\n        x, y = haze_points\n        overlay = img.copy()\n        output = img.copy()\n        alpha = alpha_coef * fog_coef\n        rad = hw // 2\n        point = (x + hw // 2, y + hw // 2)\n        cv2.circle(overlay, point, int(rad), (255, 255, 255), -1)\n        cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n\n        img = output.copy()\n\n    image_rgb = cv2.blur(img, (hw // 10, hw // 10))\n\n    if needs_float:\n        image_rgb = to_float(image_rgb, max_value=255)\n\n    return image_rgb"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd sun flare to the image.", "response": "def add_sun_flare(img, flare_center_x, flare_center_y, src_radius, src_color, circles):\n    \"\"\"Add sun flare.\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img (np.array):\n        flare_center_x (float):\n        flare_center_y (float):\n        src_radius:\n        src_color (int, int, int):\n        circles (list):\n\n    Returns:\n\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype('uint8'))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError('Unexpected dtype {} for RandomSunFlareaugmentation'.format(input_dtype))\n\n    overlay = img.copy()\n    output = img.copy()\n\n    for (alpha, (x, y), rad3, (r_color, g_color, b_color)) in circles:\n        cv2.circle(overlay, (x, y), rad3, (r_color, g_color, b_color), -1)\n\n        cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n\n    point = (int(flare_center_x), int(flare_center_y))\n\n    overlay = output.copy()\n    num_times = src_radius // 10\n    alpha = np.linspace(0.0, 1, num=num_times)\n    rad = np.linspace(1, src_radius, num=num_times)\n    for i in range(num_times):\n        cv2.circle(overlay, point, int(rad[i]), src_color, -1)\n        alp = alpha[num_times - i - 1] * alpha[num_times - i - 1] * alpha[num_times - i - 1]\n        cv2.addWeighted(overlay, alp, output, 1 - alp, 0, output)\n\n    image_rgb = output\n\n    if needs_float:\n        image_rgb = to_float(image_rgb, max_value=255)\n\n    return image_rgb"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding shadows to the image.", "response": "def add_shadow(img, vertices_list):\n    \"\"\"Add shadows to the image.\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img (np.array):\n        vertices_list (list):\n\n    Returns:\n\n    \"\"\"\n    non_rgb_warning(img)\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype('uint8'))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError('Unexpected dtype {} for RandomSnow augmentation'.format(input_dtype))\n\n    image_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    mask = np.zeros_like(img)\n\n    # adding all shadow polygons on empty mask, single 255 denotes only red channel\n    for vertices in vertices_list:\n        cv2.fillPoly(mask, vertices, 255)\n\n    # if red channel is hot, image's \"Lightness\" channel's brightness is lowered\n    red_max_value_ind = mask[:, :, 0] == 255\n    image_hls[:, :, 1][red_max_value_ind] = image_hls[:, :, 1][red_max_value_ind] * 0.5\n\n    image_rgb = cv2.cvtColor(image_hls, cv2.COLOR_HLS2RGB)\n\n    if needs_float:\n        image_rgb = to_float(image_rgb, max_value=255)\n\n    return image_rgb"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef optical_distortion(img, k=0, dx=0, dy=0, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_REFLECT_101,\n                       value=None):\n    \"\"\"Barrel / pincushion distortion. Unconventional augment.\n\n    Reference:\n        |  https://stackoverflow.com/questions/6199636/formulas-for-barrel-pincushion-distortion\n        |  https://stackoverflow.com/questions/10364201/image-transformation-in-opencv\n        |  https://stackoverflow.com/questions/2477774/correcting-fisheye-distortion-programmatically\n        |  http://www.coldvision.io/2017/03/02/advanced-lane-finding-using-opencv/\n    \"\"\"\n    height, width = img.shape[:2]\n\n    fx = width\n    fy = width\n\n    cx = width * 0.5 + dx\n    cy = height * 0.5 + dy\n\n    camera_matrix = np.array([[fx, 0, cx],\n                              [0, fy, cy],\n                              [0, 0, 1]], dtype=np.float32)\n\n    distortion = np.array([k, k, 0, 0, 0], dtype=np.float32)\n    map1, map2 = cv2.initUndistortRectifyMap(camera_matrix, distortion, None, None, (width, height), cv2.CV_32FC1)\n    img = cv2.remap(img, map1, map2, interpolation=interpolation, borderMode=border_mode, borderValue=value)\n    return img", "response": "Barrel - Pincushion distortion."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef grid_distortion(img, num_steps=10, xsteps=[], ysteps=[], interpolation=cv2.INTER_LINEAR,\n                    border_mode=cv2.BORDER_REFLECT_101, value=None):\n    \"\"\"\n    Reference:\n        http://pythology.blogspot.sg/2014/03/interpolation-on-regular-distorted-grid.html\n    \"\"\"\n    height, width = img.shape[:2]\n\n    x_step = width // num_steps\n    xx = np.zeros(width, np.float32)\n    prev = 0\n    for idx, x in enumerate(range(0, width, x_step)):\n        start = x\n        end = x + x_step\n        if end > width:\n            end = width\n            cur = width\n        else:\n            cur = prev + x_step * xsteps[idx]\n\n        xx[start:end] = np.linspace(prev, cur, end - start)\n        prev = cur\n\n    y_step = height // num_steps\n    yy = np.zeros(height, np.float32)\n    prev = 0\n    for idx, y in enumerate(range(0, height, y_step)):\n        start = y\n        end = y + y_step\n        if end > height:\n            end = height\n            cur = height\n        else:\n            cur = prev + y_step * ysteps[idx]\n\n        yy[start:end] = np.linspace(prev, cur, end - start)\n        prev = cur\n\n    map_x, map_y = np.meshgrid(xx, yy)\n    map_x = map_x.astype(np.float32)\n    map_y = map_y.astype(np.float32)\n    img = cv2.remap(img, map_x, map_y, interpolation=interpolation, borderMode=border_mode, borderValue=value)\n    return img", "response": "This function is used to distort the image using regular distortion"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nflip a bounding box vertically around the x - axis.", "response": "def bbox_vflip(bbox, rows, cols):\n    \"\"\"Flip a bounding box vertically around the x-axis.\"\"\"\n    x_min, y_min, x_max, y_max = bbox\n    return [x_min, 1 - y_max, x_max, 1 - y_min]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bbox_hflip(bbox, rows, cols):\n    x_min, y_min, x_max, y_max = bbox\n    return [1 - x_max, y_min, 1 - x_min, y_max]", "response": "Flip a bounding box horizontally around the y - axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bbox_flip(bbox, d, rows, cols):\n    if d == 0:\n        bbox = bbox_vflip(bbox, rows, cols)\n    elif d == 1:\n        bbox = bbox_hflip(bbox, rows, cols)\n    elif d == -1:\n        bbox = bbox_hflip(bbox, rows, cols)\n        bbox = bbox_vflip(bbox, rows, cols)\n    else:\n        raise ValueError('Invalid d value {}. Valid values are -1, 0 and 1'.format(d))\n    return bbox", "response": "Flip a bounding box vertically horizontally or both depending on the value of d."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef crop_bbox_by_coords(bbox, crop_coords, crop_height, crop_width, rows, cols):\n    bbox = denormalize_bbox(bbox, rows, cols)\n    x_min, y_min, x_max, y_max = bbox\n    x1, y1, x2, y2 = crop_coords\n    cropped_bbox = [x_min - x1, y_min - y1, x_max - x1, y_max - y1]\n    return normalize_bbox(cropped_bbox, crop_height, crop_width)", "response": "Crop a bounding box by the provided coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bbox_rot90(bbox, factor, rows, cols):\n    if factor < 0 or factor > 3:\n        raise ValueError('Parameter n must be in range [0;3]')\n    x_min, y_min, x_max, y_max = bbox\n    if factor == 1:\n        bbox = [y_min, 1 - x_max, y_max, 1 - x_min]\n    if factor == 2:\n        bbox = [1 - x_max, 1 - y_max, 1 - x_min, 1 - y_min]\n    if factor == 3:\n        bbox = [1 - y_max, x_min, 1 - y_min, x_max]\n    return bbox", "response": "Rotates a bounding box by 90 degrees CCW."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrotating a bounding box by angle degrees degrees Returns a tuple of x_min y_min x_max y_max x_max y_max x_max y_max x_max y_max x_max y_max x_min y_max x_max y_max x_max y_max x_max y_max x_max y_max x_max y_max x_max y_max x_max y_max", "response": "def bbox_rotate(bbox, angle, rows, cols, interpolation):\n    \"\"\"Rotates a bounding box by angle degrees\n\n    Args:\n        bbox (tuple): A tuple (x_min, y_min, x_max, y_max).\n        angle (int): Angle of rotation in degrees\n        rows (int): Image rows.\n        cols (int): Image cols.\n        interpolation (int): interpolation method.\n\n        return a tuple (x_min, y_min, x_max, y_max)\n    \"\"\"\n    scale = cols / float(rows)\n    x = np.array([bbox[0], bbox[2], bbox[2], bbox[0]])\n    y = np.array([bbox[1], bbox[1], bbox[3], bbox[3]])\n    x = x - 0.5\n    y = y - 0.5\n    angle = np.deg2rad(angle)\n    x_t = (np.cos(angle) * x * scale + np.sin(angle) * y) / scale\n    y_t = (-np.sin(angle) * x * scale + np.cos(angle) * y)\n    x_t = x_t + 0.5\n    y_t = y_t + 0.5\n    return [min(x_t), min(y_t), max(x_t), max(y_t)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bbox_transpose(bbox, axis, rows, cols):\n    x_min, y_min, x_max, y_max = bbox\n    if axis != 0 and axis != 1:\n        raise ValueError('Axis must be either 0 or 1.')\n    if axis == 0:\n        bbox = [y_min, x_min, y_max, x_max]\n    if axis == 1:\n        bbox = [1 - y_max, 1 - x_max, 1 - y_min, 1 - x_min]\n    return bbox", "response": "Transposes a bounding box along given axis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nflip a keypoint vertically around the x - axis.", "response": "def keypoint_vflip(kp, rows, cols):\n    \"\"\"Flip a keypoint vertically around the x-axis.\"\"\"\n    x, y, angle, scale = kp\n    c = math.cos(angle)\n    s = math.sin(angle)\n    angle = math.atan2(-s, c)\n    return [x, (rows - 1) - y, angle, scale]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nflips a keypoint vertically horizontally or both depending on the value of d.", "response": "def keypoint_flip(bbox, d, rows, cols):\n    \"\"\"Flip a keypoint either vertically, horizontally or both depending on the value of `d`.\n\n    Raises:\n        ValueError: if value of `d` is not -1, 0 or 1.\n\n    \"\"\"\n    if d == 0:\n        bbox = keypoint_vflip(bbox, rows, cols)\n    elif d == 1:\n        bbox = keypoint_hflip(bbox, rows, cols)\n    elif d == -1:\n        bbox = keypoint_hflip(bbox, rows, cols)\n        bbox = keypoint_vflip(bbox, rows, cols)\n    else:\n        raise ValueError('Invalid d value {}. Valid values are -1, 0 and 1'.format(d))\n    return bbox"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrotates a keypoint by 90 degrees CCW.", "response": "def keypoint_rot90(keypoint, factor, rows, cols, **params):\n    \"\"\"Rotates a keypoint by 90 degrees CCW (see np.rot90)\n\n    Args:\n        keypoint (tuple): A tuple (x, y, angle, scale).\n        factor (int): Number of CCW rotations. Must be in range [0;3] See np.rot90.\n        rows (int): Image rows.\n        cols (int): Image cols.\n    \"\"\"\n    if factor < 0 or factor > 3:\n        raise ValueError('Parameter n must be in range [0;3]')\n    x, y, angle, scale = keypoint\n    if factor == 1:\n        keypoint = [y, (cols - 1) - x, angle - math.pi / 2, scale]\n    if factor == 2:\n        keypoint = [(cols - 1) - x, (rows - 1) - y, angle - math.pi, scale]\n    if factor == 3:\n        keypoint = [(rows - 1) - y, x, angle + math.pi / 2, scale]\n    return keypoint"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef keypoint_scale(keypoint, scale_x, scale_y, **params):\n    x, y, a, s = keypoint\n    return [x * scale_x, y * scale_y, a, s * max(scale_x, scale_y)]", "response": "Scales a keypoint by scale_x and scale_y."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncropping a keypoint using the provided coordinates of bottom - left and top - right corners in pixels and the required height and width of the crop.", "response": "def crop_keypoint_by_coords(keypoint, crop_coords, crop_height, crop_width, rows, cols):\n    \"\"\"Crop a keypoint using the provided coordinates of bottom-left and top-right corners in pixels and the\n    required height and width of the crop.\n    \"\"\"\n    x, y, a, s = keypoint\n    x1, y1, x2, y2 = crop_coords\n    cropped_keypoint = [x - x1, y - y1, a, s]\n    return cropped_keypoint"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef py3round(number):\n    if abs(round(number) - number) == 0.5:\n        return int(2.0 * round(number / 2.0))\n\n    return int(round(number))", "response": "Unified rounding in all python versions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply(self, img, factor=0, **params):\n        return np.ascontiguousarray(np.rot90(img, factor))", "response": "Applies the rotation factor to the input image."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnormalize coordinates of a bounding box.", "response": "def normalize_bbox(bbox, rows, cols):\n    \"\"\"Normalize coordinates of a bounding box. Divide x-coordinates by image width and y-coordinates\n    by image height.\n    \"\"\"\n    if rows == 0:\n        raise ValueError('Argument rows cannot be zero')\n    if cols == 0:\n        raise ValueError('Argument cols cannot be zero')\n    x_min, y_min, x_max, y_max = bbox[:4]\n    normalized_bbox = [x_min / cols, y_min / rows, x_max / cols, y_max / rows]\n    return normalized_bbox + list(bbox[4:])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef denormalize_bbox(bbox, rows, cols):\n    if rows == 0:\n        raise ValueError('Argument rows cannot be zero')\n    if cols == 0:\n        raise ValueError('Argument cols cannot be zero')\n\n    x_min, y_min, x_max, y_max = bbox[:4]\n    denormalized_bbox = [x_min * cols, y_min * rows, x_max * cols, y_max * rows]\n    return denormalized_bbox + list(bbox[4:])", "response": "Denormalize coordinates of a bounding box."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_bboxes(bboxes, rows, cols):\n    return [normalize_bbox(bbox, rows, cols) for bbox in bboxes]", "response": "Normalize a list of bounding boxes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_bbox_area(bbox, rows, cols):\n    bbox = denormalize_bbox(bbox, rows, cols)\n    x_min, y_min, x_max, y_max = bbox[:4]\n    area = (x_max - x_min) * (y_max - y_min)\n    return area", "response": "Calculate the area of a bounding box in pixels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter_bboxes_by_visibility(original_shape, bboxes, transformed_shape, transformed_bboxes,\n                                threshold=0., min_area=0.):\n    \"\"\"Filter bounding boxes and return only those boxes whose visibility after transformation is above\n    the threshold and minimal area of bounding box in pixels is more then min_area.\n\n    Args:\n        original_shape (tuple): original image shape\n        bboxes (list): original bounding boxes\n        transformed_shape(tuple): transformed image\n        transformed_bboxes (list): transformed bounding boxes\n        threshold (float): visibility threshold. Should be a value in the range [0.0, 1.0].\n        min_area (float): Minimal area threshold.\n    \"\"\"\n    img_height, img_width = original_shape[:2]\n    transformed_img_height, transformed_img_width = transformed_shape[:2]\n\n    visible_bboxes = []\n    for bbox, transformed_bbox in zip(bboxes, transformed_bboxes):\n        if not all(0.0 <= value <= 1.0 for value in transformed_bbox[:4]):\n            continue\n        bbox_area = calculate_bbox_area(bbox, img_height, img_width)\n        transformed_bbox_area = calculate_bbox_area(transformed_bbox, transformed_img_height, transformed_img_width)\n        if transformed_bbox_area < min_area:\n            continue\n        visibility = transformed_bbox_area / bbox_area\n        if visibility >= threshold:\n            visible_bboxes.append(transformed_bbox)\n    return visible_bboxes", "response": "Filter bounding boxes and return only those boxes whose visibility after transformation is above\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity=False):\n    if source_format not in {'coco', 'pascal_voc'}:\n        raise ValueError(\n            \"Unknown source_format {}. Supported formats are: 'coco' and 'pascal_voc'\".format(source_format)\n        )\n    if source_format == 'coco':\n        x_min, y_min, width, height = bbox[:4]\n        x_max = x_min + width\n        y_max = y_min + height\n    else:\n        x_min, y_min, x_max, y_max = bbox[:4]\n    bbox = [x_min, y_min, x_max, y_max] + list(bbox[4:])\n    bbox = normalize_bbox(bbox, rows, cols)\n    if check_validity:\n        check_bbox(bbox)\n    return bbox", "response": "Convert a bounding box from one format specified in source_format to another."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a bounding box from the format used by albumentations to the specified format.", "response": "def convert_bbox_from_albumentations(bbox, target_format, rows, cols, check_validity=False):\n    \"\"\"Convert a bounding box from the format used by albumentations to a format, specified in `target_format`.\n\n    Args:\n        bbox (list): bounding box with coordinates in the format used by albumentations\n        target_format (str): required format of the output bounding box. Should be 'coco' or 'pascal_voc'.\n        rows (int): image height\n        cols (int): image width\n        check_validity (bool): check if all boxes are valid boxes\n\n    Note:\n        The `coco` format of a bounding box looks like `[x_min, y_min, width, height]`, e.g. [97, 12, 150, 200].\n        The `pascal_voc` format of a bounding box looks like `[x_min, y_min, x_max, y_max]`, e.g. [97, 12, 247, 212].\n\n    Raises:\n        ValueError: if `target_format` is not equal to `coco` or `pascal_voc`.\n\n    \"\"\"\n    if target_format not in {'coco', 'pascal_voc'}:\n        raise ValueError(\n            \"Unknown target_format {}. Supported formats are: 'coco' and 'pascal_voc'\".format(target_format)\n        )\n    if check_validity:\n        check_bbox(bbox)\n    bbox = denormalize_bbox(bbox, rows, cols)\n    if target_format == 'coco':\n        x_min, y_min, x_max, y_max = bbox[:4]\n        width = x_max - x_min\n        height = y_max - y_min\n        bbox = [x_min, y_min, width, height] + list(bbox[4:])\n    return bbox"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a list bounding boxes from a format specified in source_format to the format used by albumentations", "response": "def convert_bboxes_to_albumentations(bboxes, source_format, rows, cols, check_validity=False):\n    \"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\n    \"\"\"\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_bboxes_from_albumentations(bboxes, target_format, rows, cols, check_validity=False):\n    return [convert_bbox_from_albumentations(bbox, target_format, rows, cols, check_validity) for bbox in bboxes]", "response": "Convert a list of bounding boxes from the format used by albumentations to a format specified by target_format."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if bbox boundaries are in range 0 1 and minimums are lesser then maximums", "response": "def check_bbox(bbox):\n    \"\"\"Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums\"\"\"\n    for name, value in zip(['x_min', 'y_min', 'x_max', 'y_max'], bbox[:4]):\n        if not 0 <= value <= 1:\n            raise ValueError(\n                'Expected {name} for bbox {bbox} '\n                'to be in the range [0.0, 1.0], got {value}.'.format(\n                    bbox=bbox,\n                    name=name,\n                    value=value,\n                )\n            )\n    x_min, y_min, x_max, y_max = bbox[:4]\n    if x_max <= x_min:\n        raise ValueError('x_max is less than or equal to x_min for bbox {bbox}.'.format(\n            bbox=bbox,\n        ))\n    if y_max <= y_min:\n        raise ValueError('y_max is less than or equal to y_min for bbox {bbox}.'.format(\n            bbox=bbox,\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_bboxes(bboxes, rows, cols, min_area=0., min_visibility=0.):\n    resulting_boxes = []\n    for bbox in bboxes:\n        transformed_box_area = calculate_bbox_area(bbox, rows, cols)\n        bbox[:4] = np.clip(bbox[:4], 0, 1.)\n        clipped_box_area = calculate_bbox_area(bbox, rows, cols)\n        if not transformed_box_area or clipped_box_area / transformed_box_area <= min_visibility:\n            continue\n        else:\n            bbox[:4] = np.clip(bbox[:4], 0, 1.)\n        if calculate_bbox_area(bbox, rows, cols) <= min_area:\n            continue\n        resulting_boxes.append(bbox)\n    return resulting_boxes", "response": "Filter a list of bounding boxes to the final image size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef union_of_bboxes(height, width, bboxes, erosion_rate=0.0, to_int=False):\n    x1, y1 = width, height\n    x2, y2 = 0, 0\n    for b in bboxes:\n        w, h = b[2] - b[0], b[3] - b[1]\n        lim_x1, lim_y1 = b[0] + erosion_rate * w, b[1] + erosion_rate * h\n        lim_x2, lim_y2 = b[2] - erosion_rate * w, b[3] - erosion_rate * h\n        x1, y1 = np.min([x1, lim_x1]), np.min([y1, lim_y1])\n        x2, y2 = np.max([x2, lim_x2]), np.max([y2, lim_y2])\n    return x1, y1, x2, y2", "response": "Calculate the union of the given list of bounding boxes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert input argument to min - max tuple.", "response": "def to_tuple(param, low=None, bias=None):\n    \"\"\"Convert input argument to min-max tuple\n    Args:\n        param (scalar, tuple or list of 2+ elements): Input value.\n            If value is scalar, return value would be (offset - value, offset + value).\n            If value is tuple, return value would be value + offset (broadcasted).\n        low:  Second element of tuple can be passed as optional argument\n        bias: An offset factor added to each element\n    \"\"\"\n    if low is not None and bias is not None:\n        raise ValueError('Arguments low and bias are mutually exclusive')\n\n    if param is None:\n        return param\n\n    if isinstance(param, (int, float)):\n        if low is None:\n            param = - param, + param\n        else:\n            param = (low, param) if low < param else (param, low)\n    elif isinstance(param, (list, tuple)):\n        param = tuple(param)\n    else:\n        raise ValueError('Argument param must be either scalar (int,float) or tuple')\n\n    if bias is not None:\n        return tuple([bias + x for x in param])\n\n    return tuple(param)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if keypoint coordinates are in range [ 0 1 )", "response": "def check_keypoint(kp, rows, cols):\n    \"\"\"Check if keypoint coordinates are in range [0, 1)\"\"\"\n    for name, value, size in zip(['x', 'y'], kp[:2], [cols, rows]):\n        if not 0 <= value < size:\n            raise ValueError(\n                'Expected {name} for keypoint {kp} '\n                'to be in the range [0.0, {size}], got {value}.'.format(\n                    kp=kp,\n                    name=name,\n                    value=value,\n                    size=size\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_keypoints(keypoints, rows, cols):\n    for kp in keypoints:\n        check_keypoint(kp, rows, cols)", "response": "Check if keypoints boundaries are in range [ 0 1 )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts watching source files for changes.", "response": "def start(check_time: int = 500) -> None:\n    \"\"\"Begins watching source files for changes.\n\n    .. versionchanged:: 5.0\n       The ``io_loop`` argument (deprecated since version 4.1) has been removed.\n    \"\"\"\n    io_loop = ioloop.IOLoop.current()\n    if io_loop in _io_loops:\n        return\n    _io_loops[io_loop] = True\n    if len(_io_loops) > 1:\n        gen_log.warning(\"tornado.autoreload started more than once in the same process\")\n    modify_times = {}  # type: Dict[str, float]\n    callback = functools.partial(_reload_on_update, modify_times)\n    scheduler = ioloop.PeriodicCallback(callback, check_time)\n    scheduler.start()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main() -> None:\n    # Remember that we were launched with autoreload as main.\n    # The main module can be tricky; set the variables both in our globals\n    # (which may be __main__) and the real importable version.\n    import tornado.autoreload\n\n    global _autoreload_is_main\n    global _original_argv, _original_spec\n    tornado.autoreload._autoreload_is_main = _autoreload_is_main = True\n    original_argv = sys.argv\n    tornado.autoreload._original_argv = _original_argv = original_argv\n    original_spec = getattr(sys.modules[\"__main__\"], \"__spec__\", None)\n    tornado.autoreload._original_spec = _original_spec = original_spec\n    sys.argv = sys.argv[:]\n    if len(sys.argv) >= 3 and sys.argv[1] == \"-m\":\n        mode = \"module\"\n        module = sys.argv[2]\n        del sys.argv[1:3]\n    elif len(sys.argv) >= 2:\n        mode = \"script\"\n        script = sys.argv[1]\n        sys.argv = sys.argv[1:]\n    else:\n        print(_USAGE, file=sys.stderr)\n        sys.exit(1)\n\n    try:\n        if mode == \"module\":\n            import runpy\n\n            runpy.run_module(module, run_name=\"__main__\", alter_sys=True)\n        elif mode == \"script\":\n            with open(script) as f:\n                # Execute the script in our namespace instead of creating\n                # a new one so that something that tries to import __main__\n                # (e.g. the unittest module) will see names defined in the\n                # script instead of just those defined in this module.\n                global __file__\n                __file__ = script\n                # If __package__ is defined, imports may be incorrectly\n                # interpreted as relative to this module.\n                global __package__\n                del __package__\n                exec_in(f.read(), globals(), globals())\n    except SystemExit as e:\n        logging.basicConfig()\n        gen_log.info(\"Script exited with status %s\", e.code)\n    except Exception as e:\n        logging.basicConfig()\n        gen_log.warning(\"Script exited with uncaught exception\", exc_info=True)\n        # If an exception occurred at import time, the file with the error\n        # never made it into sys.modules and so we won't know to watch it.\n        # Just to make sure we've covered everything, walk the stack trace\n        # from the exception and watch every file.\n        for (filename, lineno, name, line) in traceback.extract_tb(sys.exc_info()[2]):\n            watch(filename)\n        if isinstance(e, SyntaxError):\n            # SyntaxErrors are special:  their innermost stack frame is fake\n            # so extract_tb won't see it and we have to get the filename\n            # from the exception object.\n            watch(e.filename)\n    else:\n        logging.basicConfig()\n        gen_log.info(\"Script exited normally\")\n    # restore sys.argv so subsequent executions will include autoreload\n    sys.argv = original_argv\n\n    if mode == \"module\":\n        # runpy did a fake import of the module as __main__, but now it's\n        # no longer in sys.modules.  Figure out where it is and watch it.\n        loader = pkgutil.get_loader(module)\n        if loader is not None:\n            watch(loader.get_filename())  # type: ignore\n\n    wait()", "response": "Main function for the command - line wrapper."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split(\n        addrinfo: List[Tuple]\n    ) -> Tuple[\n        List[Tuple[socket.AddressFamily, Tuple]],\n        List[Tuple[socket.AddressFamily, Tuple]],\n    ]:\n        \"\"\"Partition the ``addrinfo`` list by address family.\n\n        Returns two lists.  The first list contains the first entry from\n        ``addrinfo`` and all others with the same family, and the\n        second list contains all other addresses (normally one list will\n        be AF_INET and the other AF_INET6, although non-standard resolvers\n        may return additional families).\n        \"\"\"\n        primary = []\n        secondary = []\n        primary_af = addrinfo[0][0]\n        for af, addr in addrinfo:\n            if af == primary_af:\n                primary.append((af, addr))\n            else:\n                secondary.append((af, addr))\n        return primary, secondary", "response": "Partition the addrinfo list by address family."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect to the given host and port.", "response": "async def connect(\n        self,\n        host: str,\n        port: int,\n        af: socket.AddressFamily = socket.AF_UNSPEC,\n        ssl_options: Union[Dict[str, Any], ssl.SSLContext] = None,\n        max_buffer_size: int = None,\n        source_ip: str = None,\n        source_port: int = None,\n        timeout: Union[float, datetime.timedelta] = None,\n    ) -> IOStream:\n        \"\"\"Connect to the given host and port.\n\n        Asynchronously returns an `.IOStream` (or `.SSLIOStream` if\n        ``ssl_options`` is not None).\n\n        Using the ``source_ip`` kwarg, one can specify the source\n        IP address to use when establishing the connection.\n        In case the user needs to resolve and\n        use a specific interface, it has to be handled outside\n        of Tornado as this depends very much on the platform.\n\n        Raises `TimeoutError` if the input future does not complete before\n        ``timeout``, which may be specified in any form allowed by\n        `.IOLoop.add_timeout` (i.e. a `datetime.timedelta` or an absolute time\n        relative to `.IOLoop.time`)\n\n        Similarly, when the user requires a certain source port, it can\n        be specified using the ``source_port`` arg.\n\n        .. versionchanged:: 4.5\n           Added the ``source_ip`` and ``source_port`` arguments.\n\n        .. versionchanged:: 5.0\n           Added the ``timeout`` argument.\n        \"\"\"\n        if timeout is not None:\n            if isinstance(timeout, numbers.Real):\n                timeout = IOLoop.current().time() + timeout\n            elif isinstance(timeout, datetime.timedelta):\n                timeout = IOLoop.current().time() + timeout.total_seconds()\n            else:\n                raise TypeError(\"Unsupported timeout %r\" % timeout)\n        if timeout is not None:\n            addrinfo = await gen.with_timeout(\n                timeout, self.resolver.resolve(host, port, af)\n            )\n        else:\n            addrinfo = await self.resolver.resolve(host, port, af)\n        connector = _Connector(\n            addrinfo,\n            functools.partial(\n                self._create_stream,\n                max_buffer_size,\n                source_ip=source_ip,\n                source_port=source_port,\n            ),\n        )\n        af, addr, stream = await connector.start(connect_timeout=timeout)\n        # TODO: For better performance we could cache the (af, addr)\n        # information here and re-use it on subsequent connections to\n        # the same host. (http://tools.ietf.org/html/rfc6555#section-4.2)\n        if ssl_options is not None:\n            if timeout is not None:\n                stream = await gen.with_timeout(\n                    timeout,\n                    stream.start_tls(\n                        False, ssl_options=ssl_options, server_hostname=host\n                    ),\n                )\n            else:\n                stream = await stream.start_tls(\n                    False, ssl_options=ssl_options, server_hostname=host\n                )\n        return stream"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def close_all_connections(self) -> None:\n        while self._connections:\n            # Peek at an arbitrary element of the set\n            conn = next(iter(self._connections))\n            await conn.close()", "response": "Close all open connections and asynchronously wait for them to finish."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _apply_xheaders(self, headers: httputil.HTTPHeaders) -> None:\n        # Squid uses X-Forwarded-For, others use X-Real-Ip\n        ip = headers.get(\"X-Forwarded-For\", self.remote_ip)\n        # Skip trusted downstream hosts in X-Forwarded-For list\n        for ip in (cand.strip() for cand in reversed(ip.split(\",\"))):\n            if ip not in self.trusted_downstream:\n                break\n        ip = headers.get(\"X-Real-Ip\", ip)\n        if netutil.is_valid_ip(ip):\n            self.remote_ip = ip\n        # AWS uses X-Forwarded-Proto\n        proto_header = headers.get(\n            \"X-Scheme\", headers.get(\"X-Forwarded-Proto\", self.protocol)\n        )\n        if proto_header:\n            # use only the last proto entry if there is more than one\n            # TODO: support trusting mutiple layers of proxied protocol\n            proto_header = proto_header.split(\",\")[-1].strip()\n        if proto_header in (\"http\", \"https\"):\n            self.protocol = proto_header", "response": "Rewrite the remote_ip and protocol fields based on the X - Forwarded - For and X - Real - Ip headers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _unapply_xheaders(self) -> None:\n        self.remote_ip = self._orig_remote_ip\n        self.protocol = self._orig_protocol", "response": "Undo changes from _apply_xheaders."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_default_locale(code: str) -> None:\n    global _default_locale\n    global _supported_locales\n    _default_locale = code\n    _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])", "response": "Sets the default locale."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_translations(directory: str, encoding: str = None) -> None:\n    global _translations\n    global _supported_locales\n    _translations = {}\n    for path in os.listdir(directory):\n        if not path.endswith(\".csv\"):\n            continue\n        locale, extension = path.split(\".\")\n        if not re.match(\"[a-z]+(_[A-Z]+)?$\", locale):\n            gen_log.error(\n                \"Unrecognized locale %r (path: %s)\",\n                locale,\n                os.path.join(directory, path),\n            )\n            continue\n        full_path = os.path.join(directory, path)\n        if encoding is None:\n            # Try to autodetect encoding based on the BOM.\n            with open(full_path, \"rb\") as bf:\n                data = bf.read(len(codecs.BOM_UTF16_LE))\n            if data in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n                encoding = \"utf-16\"\n            else:\n                # utf-8-sig is \"utf-8 with optional BOM\". It's discouraged\n                # in most cases but is common with CSV files because Excel\n                # cannot read utf-8 files without a BOM.\n                encoding = \"utf-8-sig\"\n        # python 3: csv.reader requires a file open in text mode.\n        # Specify an encoding to avoid dependence on $LANG environment variable.\n        with open(full_path, encoding=encoding) as f:\n            _translations[locale] = {}\n            for i, row in enumerate(csv.reader(f)):\n                if not row or len(row) < 2:\n                    continue\n                row = [escape.to_unicode(c).strip() for c in row]\n                english, translation = row[:2]\n                if len(row) > 2:\n                    plural = row[2] or \"unknown\"\n                else:\n                    plural = \"unknown\"\n                if plural not in (\"plural\", \"singular\", \"unknown\"):\n                    gen_log.error(\n                        \"Unrecognized plural indicator %r in %s line %d\",\n                        plural,\n                        path,\n                        i + 1,\n                    )\n                    continue\n                _translations[locale].setdefault(plural, {})[english] = translation\n    _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])\n    gen_log.debug(\"Supported locales: %s\", sorted(_supported_locales))", "response": "Loads translations from CSV files in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading translations from gettext s locale tree", "response": "def load_gettext_translations(directory: str, domain: str) -> None:\n    \"\"\"Loads translations from `gettext`'s locale tree\n\n    Locale tree is similar to system's ``/usr/share/locale``, like::\n\n        {directory}/{lang}/LC_MESSAGES/{domain}.mo\n\n    Three steps are required to have your app translated:\n\n    1. Generate POT translation file::\n\n        xgettext --language=Python --keyword=_:1,2 -d mydomain file1.py file2.html etc\n\n    2. Merge against existing POT file::\n\n        msgmerge old.po mydomain.po > new.po\n\n    3. Compile::\n\n        msgfmt mydomain.po -o {directory}/pt_BR/LC_MESSAGES/mydomain.mo\n    \"\"\"\n    global _translations\n    global _supported_locales\n    global _use_gettext\n    _translations = {}\n    for lang in os.listdir(directory):\n        if lang.startswith(\".\"):\n            continue  # skip .svn, etc\n        if os.path.isfile(os.path.join(directory, lang)):\n            continue\n        try:\n            os.stat(os.path.join(directory, lang, \"LC_MESSAGES\", domain + \".mo\"))\n            _translations[lang] = gettext.translation(\n                domain, directory, languages=[lang]\n            )\n        except Exception as e:\n            gen_log.error(\"Cannot load translation for '%s': %s\", lang, str(e))\n            continue\n    _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])\n    _use_gettext = True\n    gen_log.debug(\"Supported locales: %s\", sorted(_supported_locales))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the closest match for the given locale code.", "response": "def get_closest(cls, *locale_codes: str) -> \"Locale\":\n        \"\"\"Returns the closest match for the given locale code.\"\"\"\n        for code in locale_codes:\n            if not code:\n                continue\n            code = code.replace(\"-\", \"_\")\n            parts = code.split(\"_\")\n            if len(parts) > 2:\n                continue\n            elif len(parts) == 2:\n                code = parts[0].lower() + \"_\" + parts[1].upper()\n            if code in _supported_locales:\n                return cls.get(code)\n            if parts[0].lower() in _supported_locales:\n                return cls.get(parts[0].lower())\n        return cls.get(_default_locale)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(cls, code: str) -> \"Locale\":\n        if code not in cls._cache:\n            assert code in _supported_locales\n            translations = _translations.get(code, None)\n            if translations is None:\n                locale = CSVLocale(code, {})  # type: Locale\n            elif _use_gettext:\n                locale = GettextLocale(code, translations)\n            else:\n                locale = CSVLocale(code, translations)\n            cls._cache[code] = locale\n        return cls._cache[code]", "response": "Returns the Locale for the given locale code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the translation for the given message for this locale.", "response": "def translate(\n        self, message: str, plural_message: str = None, count: int = None\n    ) -> str:\n        \"\"\"Returns the translation for the given message for this locale.\n\n        If ``plural_message`` is given, you must also provide\n        ``count``. We return ``plural_message`` when ``count != 1``,\n        and we return the singular form for the given message when\n        ``count == 1``.\n        \"\"\"\n        raise NotImplementedError()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_date(\n        self,\n        date: Union[int, float, datetime.datetime],\n        gmt_offset: int = 0,\n        relative: bool = True,\n        shorter: bool = False,\n        full_format: bool = False,\n    ) -> str:\n        \"\"\"Formats the given date (which should be GMT).\n\n        By default, we return a relative time (e.g., \"2 minutes ago\"). You\n        can return an absolute date string with ``relative=False``.\n\n        You can force a full format date (\"July 10, 1980\") with\n        ``full_format=True``.\n\n        This method is primarily intended for dates in the past.\n        For dates in the future, we fall back to full format.\n        \"\"\"\n        if isinstance(date, (int, float)):\n            date = datetime.datetime.utcfromtimestamp(date)\n        now = datetime.datetime.utcnow()\n        if date > now:\n            if relative and (date - now).seconds < 60:\n                # Due to click skew, things are some things slightly\n                # in the future. Round timestamps in the immediate\n                # future down to now in relative mode.\n                date = now\n            else:\n                # Otherwise, future dates always use the full format.\n                full_format = True\n        local_date = date - datetime.timedelta(minutes=gmt_offset)\n        local_now = now - datetime.timedelta(minutes=gmt_offset)\n        local_yesterday = local_now - datetime.timedelta(hours=24)\n        difference = now - date\n        seconds = difference.seconds\n        days = difference.days\n\n        _ = self.translate\n        format = None\n        if not full_format:\n            if relative and days == 0:\n                if seconds < 50:\n                    return _(\"1 second ago\", \"%(seconds)d seconds ago\", seconds) % {\n                        \"seconds\": seconds\n                    }\n\n                if seconds < 50 * 60:\n                    minutes = round(seconds / 60.0)\n                    return _(\"1 minute ago\", \"%(minutes)d minutes ago\", minutes) % {\n                        \"minutes\": minutes\n                    }\n\n                hours = round(seconds / (60.0 * 60))\n                return _(\"1 hour ago\", \"%(hours)d hours ago\", hours) % {\"hours\": hours}\n\n            if days == 0:\n                format = _(\"%(time)s\")\n            elif days == 1 and local_date.day == local_yesterday.day and relative:\n                format = _(\"yesterday\") if shorter else _(\"yesterday at %(time)s\")\n            elif days < 5:\n                format = _(\"%(weekday)s\") if shorter else _(\"%(weekday)s at %(time)s\")\n            elif days < 334:  # 11mo, since confusing for same month last year\n                format = (\n                    _(\"%(month_name)s %(day)s\")\n                    if shorter\n                    else _(\"%(month_name)s %(day)s at %(time)s\")\n                )\n\n        if format is None:\n            format = (\n                _(\"%(month_name)s %(day)s, %(year)s\")\n                if shorter\n                else _(\"%(month_name)s %(day)s, %(year)s at %(time)s\")\n            )\n\n        tfhour_clock = self.code not in (\"en\", \"en_US\", \"zh_CN\")\n        if tfhour_clock:\n            str_time = \"%d:%02d\" % (local_date.hour, local_date.minute)\n        elif self.code == \"zh_CN\":\n            str_time = \"%s%d:%02d\" % (\n                (u\"\\u4e0a\\u5348\", u\"\\u4e0b\\u5348\")[local_date.hour >= 12],\n                local_date.hour % 12 or 12,\n                local_date.minute,\n            )\n        else:\n            str_time = \"%d:%02d %s\" % (\n                local_date.hour % 12 or 12,\n                local_date.minute,\n                (\"am\", \"pm\")[local_date.hour >= 12],\n            )\n\n        return format % {\n            \"month_name\": self._months[local_date.month - 1],\n            \"weekday\": self._weekdays[local_date.weekday()],\n            \"day\": str(local_date.day),\n            \"year\": str(local_date.year),\n            \"time\": str_time,\n        }", "response": "Formats the given date in the current language."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_day(\n        self, date: datetime.datetime, gmt_offset: int = 0, dow: bool = True\n    ) -> bool:\n        \"\"\"Formats the given date as a day of week.\n\n        Example: \"Monday, January 22\". You can remove the day of week with\n        ``dow=False``.\n        \"\"\"\n        local_date = date - datetime.timedelta(minutes=gmt_offset)\n        _ = self.translate\n        if dow:\n            return _(\"%(weekday)s, %(month_name)s %(day)s\") % {\n                \"month_name\": self._months[local_date.month - 1],\n                \"weekday\": self._weekdays[local_date.weekday()],\n                \"day\": str(local_date.day),\n            }\n        else:\n            return _(\"%(month_name)s %(day)s\") % {\n                \"month_name\": self._months[local_date.month - 1],\n                \"day\": str(local_date.day),\n            }", "response": "Formats the given date as a day of week."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a comma - separated list for the given list of parts.", "response": "def list(self, parts: Any) -> str:\n        \"\"\"Returns a comma-separated list for the given list of parts.\n\n        The format is, e.g., \"A, B and C\", \"A and B\" or just \"A\" for lists\n        of size 1.\n        \"\"\"\n        _ = self.translate\n        if len(parts) == 0:\n            return \"\"\n        if len(parts) == 1:\n            return parts[0]\n        comma = u\" \\u0648 \" if self.code.startswith(\"fa\") else u\", \"\n        return _(\"%(commas)s and %(last)s\") % {\n            \"commas\": comma.join(parts[:-1]),\n            \"last\": parts[len(parts) - 1],\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef friendly_number(self, value: int) -> str:\n        if self.code not in (\"en\", \"en_US\"):\n            return str(value)\n        s = str(value)\n        parts = []\n        while s:\n            parts.append(s[-3:])\n            s = s[:-3]\n        return \",\".join(reversed(parts))", "response": "Returns a comma - separated number for the given integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pgettext(\n        self, context: str, message: str, plural_message: str = None, count: int = None\n    ) -> str:\n        \"\"\"Allows to set context for translation, accepts plural forms.\n\n        Usage example::\n\n            pgettext(\"law\", \"right\")\n            pgettext(\"good\", \"right\")\n\n        Plural message example::\n\n            pgettext(\"organization\", \"club\", \"clubs\", len(clubs))\n            pgettext(\"stick\", \"club\", \"clubs\", len(clubs))\n\n        To generate POT file with context, add following options to step 1\n        of `load_gettext_translations` sequence::\n\n            xgettext [basic options] --keyword=pgettext:1c,2 --keyword=pgettext:1c,2,3\n\n        .. versionadded:: 4.2\n        \"\"\"\n        if plural_message is not None:\n            assert count is not None\n            msgs_with_ctxt = (\n                \"%s%s%s\" % (context, CONTEXT_SEPARATOR, message),\n                \"%s%s%s\" % (context, CONTEXT_SEPARATOR, plural_message),\n                count,\n            )\n            result = self.ngettext(*msgs_with_ctxt)\n            if CONTEXT_SEPARATOR in result:\n                # Translation not found\n                result = self.ngettext(message, plural_message, count)\n            return result\n        else:\n            msg_with_ctxt = \"%s%s%s\" % (context, CONTEXT_SEPARATOR, message)\n            result = self.gettext(msg_with_ctxt)\n            if CONTEXT_SEPARATOR in result:\n                # Translation not found\n                result = message\n            return result", "response": "Returns the translated version of the given message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _unquote_or_none(s: Optional[str]) -> Optional[bytes]:  # noqa: F811\n    if s is None:\n        return s\n    return url_unescape(s, encoding=None, plus=False)", "response": "Unquote or return None if s is None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the appropriate handler for the given request.", "response": "def find_handler(\n        self, request: httputil.HTTPServerRequest, **kwargs: Any\n    ) -> Optional[httputil.HTTPMessageDelegate]:\n        \"\"\"Must be implemented to return an appropriate instance of `~.httputil.HTTPMessageDelegate`\n        that can serve the request.\n        Routing implementations may pass additional kwargs to extend the routing logic.\n\n        :arg httputil.HTTPServerRequest request: current HTTP request.\n        :arg kwargs: additional keyword arguments passed by routing implementation.\n        :returns: an instance of `~.httputil.HTTPMessageDelegate` that will be used to\n            process the request.\n        \"\"\"\n        raise NotImplementedError()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_rules(self, rules: _RuleList) -> None:\n        for rule in rules:\n            if isinstance(rule, (tuple, list)):\n                assert len(rule) in (2, 3, 4)\n                if isinstance(rule[0], basestring_type):\n                    rule = Rule(PathMatches(rule[0]), *rule[1:])\n                else:\n                    rule = Rule(*rule)\n\n            self.rules.append(self.process_rule(rule))", "response": "Appends new rules to the router."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an instance of httputil. HTTPMessageDelegate for the given target.", "response": "def get_target_delegate(\n        self, target: Any, request: httputil.HTTPServerRequest, **target_params: Any\n    ) -> Optional[httputil.HTTPMessageDelegate]:\n        \"\"\"Returns an instance of `~.httputil.HTTPMessageDelegate` for a\n        Rule's target. This method is called by `~.find_handler` and can be\n        extended to provide additional target types.\n\n        :arg target: a Rule's target.\n        :arg httputil.HTTPServerRequest request: current request.\n        :arg target_params: additional parameters that can be useful\n            for `~.httputil.HTTPMessageDelegate` creation.\n        \"\"\"\n        if isinstance(target, Router):\n            return target.find_handler(request, **target_params)\n\n        elif isinstance(target, httputil.HTTPServerConnectionDelegate):\n            assert request.connection is not None\n            return target.start_request(request.server_connection, request.connection)\n\n        elif callable(target):\n            assert request.connection is not None\n            return _CallableAdapter(\n                partial(target, **target_params), request.connection\n            )\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmatching current instance against the request.", "response": "def match(self, request: httputil.HTTPServerRequest) -> Optional[Dict[str, Any]]:\n        \"\"\"Matches current instance against the request.\n\n        :arg httputil.HTTPServerRequest request: current HTTP request\n        :returns: a dict of parameters to be passed to the target handler\n            (for example, ``handler_kwargs``, ``path_args``, ``path_kwargs``\n            can be passed for proper `~.web.RequestHandler` instantiation).\n            An empty dict is a valid (and common) return value to indicate a match\n            when the argument-passing features are not used.\n            ``None`` must be returned to indicate that there is no match.\"\"\"\n        raise NotImplementedError()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the groups of the url.", "response": "def _find_groups(self) -> Tuple[Optional[str], Optional[int]]:\n        \"\"\"Returns a tuple (reverse string, group count) for a url.\n\n        For example: Given the url pattern /([0-9]{4})/([a-z-]+)/, this method\n        would return ('/%s/%s/', 2).\n        \"\"\"\n        pattern = self.regex.pattern\n        if pattern.startswith(\"^\"):\n            pattern = pattern[1:]\n        if pattern.endswith(\"$\"):\n            pattern = pattern[:-1]\n\n        if self.regex.groups != pattern.count(\"(\"):\n            # The pattern is too complicated for our simplistic matching,\n            # so we can't support reversing it.\n            return None, None\n\n        pieces = []\n        for fragment in pattern.split(\"(\"):\n            if \")\" in fragment:\n                paren_loc = fragment.index(\")\")\n                if paren_loc >= 0:\n                    pieces.append(\"%s\" + fragment[paren_loc + 1 :])\n            else:\n                try:\n                    unescaped_fragment = re_unescape(fragment)\n                except ValueError:\n                    # If we can't unescape part of it, we can't\n                    # reverse this url.\n                    return (None, None)\n                pieces.append(unescaped_fragment)\n\n        return \"\".join(pieces), self.regex.groups"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def get_links_from_url(url):\n    response = await httpclient.AsyncHTTPClient().fetch(url)\n    print(\"fetched %s\" % url)\n\n    html = response.body.decode(errors=\"ignore\")\n    return [urljoin(url, remove_fragment(new_url)) for new_url in get_links(html)]", "response": "Download the page at url and parse it for links."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_messages_since(self, cursor):\n        results = []\n        for msg in reversed(self.cache):\n            if msg[\"id\"] == cursor:\n                break\n            results.append(msg)\n        results.reverse()\n        return results", "response": "Returns a list of messages newer than the given cursor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports an object by name.", "response": "def import_object(name: str) -> Any:\n    \"\"\"Imports an object by name.\n\n    ``import_object('x')`` is equivalent to ``import x``.\n    ``import_object('x.y.z')`` is equivalent to ``from x.y import z``.\n\n    >>> import tornado.escape\n    >>> import_object('tornado.escape') is tornado.escape\n    True\n    >>> import_object('tornado.escape.utf8') is tornado.escape.utf8\n    True\n    >>> import_object('tornado') is tornado\n    True\n    >>> import_object('tornado.missing_module')\n    Traceback (most recent call last):\n        ...\n    ImportError: No module named missing_module\n    \"\"\"\n    if name.count(\".\") == 0:\n        return __import__(name)\n\n    parts = name.split(\".\")\n    obj = __import__(\".\".join(parts[:-1]), fromlist=[parts[-1]])\n    try:\n        return getattr(obj, parts[-1])\n    except AttributeError:\n        raise ImportError(\"No module named %s\" % parts[-1])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef errno_from_exception(e: BaseException) -> Optional[int]:\n\n    if hasattr(e, \"errno\"):\n        return e.errno  # type: ignore\n    elif e.args:\n        return e.args[0]\n    else:\n        return None", "response": "Provides the errno attribute from an exception object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _websocket_mask_python(mask: bytes, data: bytes) -> bytes:\n    mask_arr = array.array(\"B\", mask)\n    unmasked_arr = array.array(\"B\", data)\n    for i in range(len(data)):\n        unmasked_arr[i] = unmasked_arr[i] ^ mask_arr[i % 4]\n    return unmasked_arr.tobytes()", "response": "Websocket masking function.\n\n    `mask` is a `bytes` object of length 4; `data` is a `bytes` object of any length.\n    Returns a `bytes` object of the same length as `data` with the mask applied\n    as specified in section 5.3 of RFC 6455.\n\n    This pure-python implementation may be replaced by an optimized version when available."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decompress(self, value: bytes, max_length: int = 0) -> bytes:\n        return self.decompressobj.decompress(value, max_length)", "response": "Decompress a chunk of data returning newly - available data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the class to use when the base class is instantiated.", "response": "def configure(cls, impl, **kwargs):\n        # type: (Union[None, str, Type[Configurable]], Any) -> None\n        \"\"\"Sets the class to use when the base class is instantiated.\n\n        Keyword arguments will be saved and added to the arguments passed\n        to the constructor.  This can be used to set global defaults for\n        some parameters.\n        \"\"\"\n        base = cls.configurable_base()\n        if isinstance(impl, str):\n            impl = typing.cast(Type[Configurable], import_object(impl))\n        if impl is not None and not issubclass(impl, cls):\n            raise ValueError(\"Invalid subclass of %s\" % cls)\n        base.__impl_class = impl\n        base.__impl_kwargs = kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the old value of the named argument without replacing it.", "response": "def get_old_value(\n        self, args: Sequence[Any], kwargs: Dict[str, Any], default: Any = None\n    ) -> Any:\n        \"\"\"Returns the old value of the named argument without replacing it.\n\n        Returns ``default`` if the argument is not present.\n        \"\"\"\n        if self.arg_pos is not None and len(args) > self.arg_pos:\n            return args[self.arg_pos]\n        else:\n            return kwargs.get(self.name, default)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreplacing the named argument in args and kwargs with new_value.", "response": "def replace(\n        self, new_value: Any, args: Sequence[Any], kwargs: Dict[str, Any]\n    ) -> Tuple[Any, Sequence[Any], Dict[str, Any]]:\n        \"\"\"Replace the named argument in ``args, kwargs`` with ``new_value``.\n\n        Returns ``(old_value, args, kwargs)``.  The returned ``args`` and\n        ``kwargs`` objects may not be the same as the input objects, or\n        the input objects may be mutated.\n\n        If the named argument was not found, ``new_value`` will be added\n        to ``kwargs`` and None will be returned as ``old_value``.\n        \"\"\"\n        if self.arg_pos is not None and len(args) > self.arg_pos:\n            # The arg to replace is passed positionally\n            old_value = args[self.arg_pos]\n            args = list(args)  # *args is normally a tuple\n            args[self.arg_pos] = new_value\n        else:\n            # The arg to replace is either omitted or passed by keyword.\n            old_value = kwargs.get(self.name)\n            kwargs[self.name] = new_value\n        return old_value, args, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef environ(request: httputil.HTTPServerRequest) -> Dict[Text, Any]:\n        hostport = request.host.split(\":\")\n        if len(hostport) == 2:\n            host = hostport[0]\n            port = int(hostport[1])\n        else:\n            host = request.host\n            port = 443 if request.protocol == \"https\" else 80\n        environ = {\n            \"REQUEST_METHOD\": request.method,\n            \"SCRIPT_NAME\": \"\",\n            \"PATH_INFO\": to_wsgi_str(\n                escape.url_unescape(request.path, encoding=None, plus=False)\n            ),\n            \"QUERY_STRING\": request.query,\n            \"REMOTE_ADDR\": request.remote_ip,\n            \"SERVER_NAME\": host,\n            \"SERVER_PORT\": str(port),\n            \"SERVER_PROTOCOL\": request.version,\n            \"wsgi.version\": (1, 0),\n            \"wsgi.url_scheme\": request.protocol,\n            \"wsgi.input\": BytesIO(escape.utf8(request.body)),\n            \"wsgi.errors\": sys.stderr,\n            \"wsgi.multithread\": False,\n            \"wsgi.multiprocess\": True,\n            \"wsgi.run_once\": False,\n        }\n        if \"Content-Type\" in request.headers:\n            environ[\"CONTENT_TYPE\"] = request.headers.pop(\"Content-Type\")\n        if \"Content-Length\" in request.headers:\n            environ[\"CONTENT_LENGTH\"] = request.headers.pop(\"Content-Length\")\n        for key, value in request.headers.items():\n            environ[\"HTTP_\" + key.replace(\"-\", \"_\").upper()] = value\n        return environ", "response": "Converts a tornado. httputil. HTTPServerRequest to a WSGI environment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stream_request_body(cls: Type[RequestHandler]) -> Type[RequestHandler]:\n    if not issubclass(cls, RequestHandler):\n        raise TypeError(\"expected subclass of RequestHandler, got %r\", cls)\n    cls._stream_request_body = True\n    return cls", "response": "Decorator to enable streaming body support for the given RequestHandler."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses this decorator to remove trailing slashes from the request path. For example, a request to ``/foo/`` would redirect to ``/foo`` with this decorator. Your request handler mapping should use a regular expression like ``r'/foo/*'`` in conjunction with using the decorator.", "response": "def removeslash(\n    method: Callable[..., Optional[Awaitable[None]]]\n) -> Callable[..., Optional[Awaitable[None]]]:\n    \"\"\"Use this decorator to remove trailing slashes from the request path.\n\n    For example, a request to ``/foo/`` would redirect to ``/foo`` with this\n    decorator. Your request handler mapping should use a regular expression\n    like ``r'/foo/*'`` in conjunction with using the decorator.\n    \"\"\"\n\n    @functools.wraps(method)\n    def wrapper(  # type: ignore\n        self: RequestHandler, *args, **kwargs\n    ) -> Optional[Awaitable[None]]:\n        if self.request.path.endswith(\"/\"):\n            if self.request.method in (\"GET\", \"HEAD\"):\n                uri = self.request.path.rstrip(\"/\")\n                if uri:  # don't try to redirect '/' to ''\n                    if self.request.query:\n                        uri += \"?\" + self.request.query\n                    self.redirect(uri, permanent=True)\n                    return None\n            else:\n                raise HTTPError(404)\n        return method(self, *args, **kwargs)\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authenticated(\n    method: Callable[..., Optional[Awaitable[None]]]\n) -> Callable[..., Optional[Awaitable[None]]]:\n    \"\"\"Decorate methods with this to require that the user be logged in.\n\n    If the user is not logged in, they will be redirected to the configured\n    `login url <RequestHandler.get_login_url>`.\n\n    If you configure a login url with a query parameter, Tornado will\n    assume you know what you're doing and use it as-is.  If not, it\n    will add a `next` parameter so the login page knows where to send\n    you once you're logged in.\n    \"\"\"\n\n    @functools.wraps(method)\n    def wrapper(  # type: ignore\n        self: RequestHandler, *args, **kwargs\n    ) -> Optional[Awaitable[None]]:\n        if not self.current_user:\n            if self.request.method in (\"GET\", \"HEAD\"):\n                url = self.get_login_url()\n                if \"?\" not in url:\n                    if urllib.parse.urlsplit(url).scheme:\n                        # if login url is absolute, make next absolute too\n                        next_url = self.request.full_url()\n                    else:\n                        assert self.request.uri is not None\n                        next_url = self.request.uri\n                    url += \"?\" + urlencode(dict(next=next_url))\n                self.redirect(url)\n                return None\n            raise HTTPError(403)\n        return method(self, *args, **kwargs)\n\n    return wrapper", "response": "Decorate methods with this to require that the user be logged in."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls by the client when the connection is closed.", "response": "def on_connection_close(self) -> None:\n        \"\"\"Called in async handlers if the client closed the connection.\n\n        Override this to clean up resources associated with\n        long-lived connections.  Note that this method is called only if\n        the connection was closed during asynchronous processing; if you\n        need to do cleanup after every request override `on_finish`\n        instead.\n\n        Proxies may keep a connection open for a time (perhaps\n        indefinitely) after the client has gone away, so this method\n        may not be called promptly after the end user closes their\n        connection.\n        \"\"\"\n        if _has_stream_request_body(self.__class__):\n            if not self.request._body_future.done():\n                self.request._body_future.set_exception(iostream.StreamClosedError())\n                self.request._body_future.exception()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets all headers and content for this response.", "response": "def clear(self) -> None:\n        \"\"\"Resets all headers and content for this response.\"\"\"\n        self._headers = httputil.HTTPHeaders(\n            {\n                \"Server\": \"TornadoServer/%s\" % tornado.version,\n                \"Content-Type\": \"text/html; charset=UTF-8\",\n                \"Date\": httputil.format_timestamp(time.time()),\n            }\n        )\n        self.set_default_headers()\n        self._write_buffer = []  # type: List[bytes]\n        self._status_code = 200\n        self._reason = httputil.responses[200]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the status code for our response.", "response": "def set_status(self, status_code: int, reason: str = None) -> None:\n        \"\"\"Sets the status code for our response.\n\n        :arg int status_code: Response status code.\n        :arg str reason: Human-readable reason phrase describing the status\n            code. If ``None``, it will be filled in from\n            `http.client.responses` or \"Unknown\".\n\n        .. versionchanged:: 5.0\n\n           No longer validates that the response code is in\n           `http.client.responses`.\n        \"\"\"\n        self._status_code = status_code\n        if reason is not None:\n            self._reason = escape.native_str(reason)\n        else:\n            self._reason = httputil.responses.get(status_code, \"Unknown\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the given response header name and value.", "response": "def set_header(self, name: str, value: _HeaderTypes) -> None:\n        \"\"\"Sets the given response header name and value.\n\n        All header values are converted to strings (`datetime` objects\n        are formatted according to the HTTP specification for the\n        ``Date`` header).\n\n        \"\"\"\n        self._headers[name] = self._convert_header_value(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_header(self, name: str, value: _HeaderTypes) -> None:\n        self._headers.add(name, self._convert_header_value(value))", "response": "Adds the given response header and value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear_header(self, name: str) -> None:\n        if name in self._headers:\n            del self._headers[name]", "response": "Clears an outgoing header."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the value of the argument with the given name.", "response": "def get_argument(  # noqa: F811\n        self,\n        name: str,\n        default: Union[None, str, _ArgDefaultMarker] = _ARG_DEFAULT,\n        strip: bool = True,\n    ) -> Optional[str]:\n        \"\"\"Returns the value of the argument with the given name.\n\n        If default is not provided, the argument is considered to be\n        required, and we raise a `MissingArgumentError` if it is missing.\n\n        If the argument appears in the request more than once, we return the\n        last value.\n\n        This method searches both the query and body arguments.\n        \"\"\"\n        return self._get_argument(name, default, self.request.arguments, strip)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_arguments(self, name: str, strip: bool = True) -> List[str]:\n\n        # Make sure `get_arguments` isn't accidentally being called with a\n        # positional argument that's assumed to be a default (like in\n        # `get_argument`.)\n        assert isinstance(strip, bool)\n\n        return self._get_arguments(name, self.request.arguments, strip)", "response": "Returns a list of the arguments with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_body_argument(\n        self,\n        name: str,\n        default: Union[None, str, _ArgDefaultMarker] = _ARG_DEFAULT,\n        strip: bool = True,\n    ) -> Optional[str]:\n        \"\"\"Returns the value of the argument with the given name\n        from the request body.\n\n        If default is not provided, the argument is considered to be\n        required, and we raise a `MissingArgumentError` if it is missing.\n\n        If the argument appears in the url more than once, we return the\n        last value.\n\n        .. versionadded:: 3.2\n        \"\"\"\n        return self._get_argument(name, default, self.request.body_arguments, strip)", "response": "Returns the value of the given name from the request body."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_body_arguments(self, name: str, strip: bool = True) -> List[str]:\n        return self._get_arguments(name, self.request.body_arguments, strip)", "response": "Returns a list of the body arguments with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the value of the given name from the request query string.", "response": "def get_query_argument(\n        self,\n        name: str,\n        default: Union[None, str, _ArgDefaultMarker] = _ARG_DEFAULT,\n        strip: bool = True,\n    ) -> Optional[str]:\n        \"\"\"Returns the value of the argument with the given name\n        from the request query string.\n\n        If default is not provided, the argument is considered to be\n        required, and we raise a `MissingArgumentError` if it is missing.\n\n        If the argument appears in the url more than once, we return the\n        last value.\n\n        .. versionadded:: 3.2\n        \"\"\"\n        return self._get_argument(name, default, self.request.query_arguments, strip)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of the query arguments with the given name.", "response": "def get_query_arguments(self, name: str, strip: bool = True) -> List[str]:\n        \"\"\"Returns a list of the query arguments with the given name.\n\n        If the argument is not present, returns an empty list.\n\n        .. versionadded:: 3.2\n        \"\"\"\n        return self._get_arguments(name, self.request.query_arguments, strip)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode_argument(self, value: bytes, name: str = None) -> str:\n        try:\n            return _unicode(value)\n        except UnicodeDecodeError:\n            raise HTTPError(\n                400, \"Invalid unicode in %s: %r\" % (name or \"url\", value[:40])\n            )", "response": "Decodes an argument from the request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cookie(self, name: str, default: str = None) -> Optional[str]:\n        if self.request.cookies is not None and name in self.request.cookies:\n            return self.request.cookies[name].value\n        return default", "response": "Returns the value of the request cookie with the given name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_cookie(\n        self,\n        name: str,\n        value: Union[str, bytes],\n        domain: str = None,\n        expires: Union[float, Tuple, datetime.datetime] = None,\n        path: str = \"/\",\n        expires_days: int = None,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Sets an outgoing cookie name/value with the given options.\n\n        Newly-set cookies are not immediately visible via `get_cookie`;\n        they are not present until the next request.\n\n        expires may be a numeric timestamp as returned by `time.time`,\n        a time tuple as returned by `time.gmtime`, or a\n        `datetime.datetime` object.\n\n        Additional keyword arguments are set on the cookies.Morsel\n        directly.\n        See https://docs.python.org/3/library/http.cookies.html#http.cookies.Morsel\n        for available attributes.\n        \"\"\"\n        # The cookie library only accepts type str, in both python 2 and 3\n        name = escape.native_str(name)\n        value = escape.native_str(value)\n        if re.search(r\"[\\x00-\\x20]\", name + value):\n            # Don't let us accidentally inject bad stuff\n            raise ValueError(\"Invalid cookie %r: %r\" % (name, value))\n        if not hasattr(self, \"_new_cookie\"):\n            self._new_cookie = http.cookies.SimpleCookie()\n        if name in self._new_cookie:\n            del self._new_cookie[name]\n        self._new_cookie[name] = value\n        morsel = self._new_cookie[name]\n        if domain:\n            morsel[\"domain\"] = domain\n        if expires_days is not None and not expires:\n            expires = datetime.datetime.utcnow() + datetime.timedelta(days=expires_days)\n        if expires:\n            morsel[\"expires\"] = httputil.format_timestamp(expires)\n        if path:\n            morsel[\"path\"] = path\n        for k, v in kwargs.items():\n            if k == \"max_age\":\n                k = \"max-age\"\n\n            # skip falsy values for httponly and secure flags because\n            # SimpleCookie sets them regardless\n            if k in [\"httponly\", \"secure\"] and not v:\n                continue\n\n            morsel[k] = v", "response": "Sets an outgoing cookie name value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes the cookie with the given name.", "response": "def clear_cookie(self, name: str, path: str = \"/\", domain: str = None) -> None:\n        \"\"\"Deletes the cookie with the given name.\n\n        Due to limitations of the cookie protocol, you must pass the same\n        path and domain to clear a cookie as were used when that cookie\n        was set (but there is no way to find out on the server side\n        which values were used for a given cookie).\n\n        Similar to `set_cookie`, the effect of this method will not be\n        seen until the following request.\n        \"\"\"\n        expires = datetime.datetime.utcnow() - datetime.timedelta(days=365)\n        self.set_cookie(name, value=\"\", path=path, expires=expires, domain=domain)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes all the cookies that have been sent with this request.", "response": "def clear_all_cookies(self, path: str = \"/\", domain: str = None) -> None:\n        \"\"\"Deletes all the cookies the user sent with this request.\n\n        See `clear_cookie` for more information on the path and domain\n        parameters.\n\n        Similar to `set_cookie`, the effect of this method will not be\n        seen until the following request.\n\n        .. versionchanged:: 3.2\n\n           Added the ``path`` and ``domain`` parameters.\n        \"\"\"\n        for name in self.request.cookies:\n            self.clear_cookie(name, path=path, domain=domain)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsign and timestamps a cookie so it cannot be forged.", "response": "def set_secure_cookie(\n        self,\n        name: str,\n        value: Union[str, bytes],\n        expires_days: int = 30,\n        version: int = None,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Signs and timestamps a cookie so it cannot be forged.\n\n        You must specify the ``cookie_secret`` setting in your Application\n        to use this method. It should be a long, random sequence of bytes\n        to be used as the HMAC secret for the signature.\n\n        To read a cookie set with this method, use `get_secure_cookie()`.\n\n        Note that the ``expires_days`` parameter sets the lifetime of the\n        cookie in the browser, but is independent of the ``max_age_days``\n        parameter to `get_secure_cookie`.\n\n        Secure cookies may contain arbitrary byte values, not just unicode\n        strings (unlike regular cookies)\n\n        Similar to `set_cookie`, the effect of this method will not be\n        seen until the following request.\n\n        .. versionchanged:: 3.2.1\n\n           Added the ``version`` argument.  Introduced cookie version 2\n           and made it the default.\n        \"\"\"\n        self.set_cookie(\n            name,\n            self.create_signed_value(name, value, version=version),\n            expires_days=expires_days,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a signed value for a key in the application.", "response": "def create_signed_value(\n        self, name: str, value: Union[str, bytes], version: int = None\n    ) -> bytes:\n        \"\"\"Signs and timestamps a string so it cannot be forged.\n\n        Normally used via set_secure_cookie, but provided as a separate\n        method for non-cookie uses.  To decode a value not stored\n        as a cookie use the optional value argument to get_secure_cookie.\n\n        .. versionchanged:: 3.2.1\n\n           Added the ``version`` argument.  Introduced cookie version 2\n           and made it the default.\n        \"\"\"\n        self.require_setting(\"cookie_secret\", \"secure cookies\")\n        secret = self.application.settings[\"cookie_secret\"]\n        key_version = None\n        if isinstance(secret, dict):\n            if self.application.settings.get(\"key_version\") is None:\n                raise Exception(\"key_version setting must be used for secret_key dicts\")\n            key_version = self.application.settings[\"key_version\"]\n\n        return create_signed_value(\n            secret, name, value, version=version, key_version=key_version\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_secure_cookie(\n        self,\n        name: str,\n        value: str = None,\n        max_age_days: int = 31,\n        min_version: int = None,\n    ) -> Optional[bytes]:\n        \"\"\"Returns the given signed cookie if it validates, or None.\n\n        The decoded cookie value is returned as a byte string (unlike\n        `get_cookie`).\n\n        Similar to `get_cookie`, this method only returns cookies that\n        were present in the request. It does not see outgoing cookies set by\n        `set_secure_cookie` in this handler.\n\n        .. versionchanged:: 3.2.1\n\n           Added the ``min_version`` argument.  Introduced cookie version 2;\n           both versions 1 and 2 are accepted by default.\n        \"\"\"\n        self.require_setting(\"cookie_secret\", \"secure cookies\")\n        if value is None:\n            value = self.get_cookie(name)\n        return decode_signed_value(\n            self.application.settings[\"cookie_secret\"],\n            name,\n            value,\n            max_age_days=max_age_days,\n            min_version=min_version,\n        )", "response": "Returns the given signed cookie if it validates or None."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_secure_cookie_key_version(\n        self, name: str, value: str = None\n    ) -> Optional[int]:\n        \"\"\"Returns the signing key version of the secure cookie.\n\n        The version is returned as int.\n        \"\"\"\n        self.require_setting(\"cookie_secret\", \"secure cookies\")\n        if value is None:\n            value = self.get_cookie(name)\n        if value is None:\n            return None\n        return get_signature_key_version(value)", "response": "Returns the signing key version of the secure cookie."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef redirect(self, url: str, permanent: bool = False, status: int = None) -> None:\n        if self._headers_written:\n            raise Exception(\"Cannot redirect after headers have been written\")\n        if status is None:\n            status = 301 if permanent else 302\n        else:\n            assert isinstance(status, int) and 300 <= status <= 399\n        self.set_status(status)\n        self.set_header(\"Location\", utf8(url))\n        self.finish()", "response": "Sends a redirect to the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the given chunk to the output buffer.", "response": "def write(self, chunk: Union[str, bytes, dict]) -> None:\n        \"\"\"Writes the given chunk to the output buffer.\n\n        To write the output to the network, use the `flush()` method below.\n\n        If the given chunk is a dictionary, we write it as JSON and set\n        the Content-Type of the response to be ``application/json``.\n        (if you want to send JSON as a different ``Content-Type``, call\n        ``set_header`` *after* calling ``write()``).\n\n        Note that lists are not converted to JSON because of a potential\n        cross-site security vulnerability.  All JSON output should be\n        wrapped in a dictionary.  More details at\n        http://haacked.com/archive/2009/06/25/json-hijacking.aspx/ and\n        https://github.com/facebook/tornado/issues/1009\n        \"\"\"\n        if self._finished:\n            raise RuntimeError(\"Cannot write() after finish()\")\n        if not isinstance(chunk, (bytes, unicode_type, dict)):\n            message = \"write() only accepts bytes, unicode, and dict objects\"\n            if isinstance(chunk, list):\n                message += (\n                    \". Lists not accepted for security reasons; see \"\n                    + \"http://www.tornadoweb.org/en/stable/web.html#tornado.web.RequestHandler.write\"  # noqa: E501\n                )\n            raise TypeError(message)\n        if isinstance(chunk, dict):\n            chunk = escape.json_encode(chunk)\n            self.set_header(\"Content-Type\", \"application/json; charset=UTF-8\")\n        chunk = utf8(chunk)\n        self._write_buffer.append(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrenders the given template_name and returns a Future that is returned.", "response": "def render(self, template_name: str, **kwargs: Any) -> \"Future[None]\":\n        \"\"\"Renders the template with the given arguments as the response.\n\n        ``render()`` calls ``finish()``, so no other output methods can be called\n        after it.\n\n        Returns a `.Future` with the same semantics as the one returned by `finish`.\n        Awaiting this `.Future` is optional.\n\n        .. versionchanged:: 5.1\n\n           Now returns a `.Future` instead of ``None``.\n        \"\"\"\n        if self._finished:\n            raise RuntimeError(\"Cannot render() after finish()\")\n        html = self.render_string(template_name, **kwargs)\n\n        # Insert the additional JS and CSS added by the modules on the page\n        js_embed = []\n        js_files = []\n        css_embed = []\n        css_files = []\n        html_heads = []\n        html_bodies = []\n        for module in getattr(self, \"_active_modules\", {}).values():\n            embed_part = module.embedded_javascript()\n            if embed_part:\n                js_embed.append(utf8(embed_part))\n            file_part = module.javascript_files()\n            if file_part:\n                if isinstance(file_part, (unicode_type, bytes)):\n                    js_files.append(_unicode(file_part))\n                else:\n                    js_files.extend(file_part)\n            embed_part = module.embedded_css()\n            if embed_part:\n                css_embed.append(utf8(embed_part))\n            file_part = module.css_files()\n            if file_part:\n                if isinstance(file_part, (unicode_type, bytes)):\n                    css_files.append(_unicode(file_part))\n                else:\n                    css_files.extend(file_part)\n            head_part = module.html_head()\n            if head_part:\n                html_heads.append(utf8(head_part))\n            body_part = module.html_body()\n            if body_part:\n                html_bodies.append(utf8(body_part))\n\n        if js_files:\n            # Maintain order of JavaScript files given by modules\n            js = self.render_linked_js(js_files)\n            sloc = html.rindex(b\"</body>\")\n            html = html[:sloc] + utf8(js) + b\"\\n\" + html[sloc:]\n        if js_embed:\n            js_bytes = self.render_embed_js(js_embed)\n            sloc = html.rindex(b\"</body>\")\n            html = html[:sloc] + js_bytes + b\"\\n\" + html[sloc:]\n        if css_files:\n            css = self.render_linked_css(css_files)\n            hloc = html.index(b\"</head>\")\n            html = html[:hloc] + utf8(css) + b\"\\n\" + html[hloc:]\n        if css_embed:\n            css_bytes = self.render_embed_css(css_embed)\n            hloc = html.index(b\"</head>\")\n            html = html[:hloc] + css_bytes + b\"\\n\" + html[hloc:]\n        if html_heads:\n            hloc = html.index(b\"</head>\")\n            html = html[:hloc] + b\"\".join(html_heads) + b\"\\n\" + html[hloc:]\n        if html_bodies:\n            hloc = html.index(b\"</body>\")\n            html = html[:hloc] + b\"\".join(html_bodies) + b\"\\n\" + html[hloc:]\n        return self.finish(html)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndefaulting method used to render the final js links for the current locale.", "response": "def render_linked_js(self, js_files: Iterable[str]) -> str:\n        \"\"\"Default method used to render the final js links for the\n        rendered webpage.\n\n        Override this method in a sub-classed controller to change the output.\n        \"\"\"\n        paths = []\n        unique_paths = set()  # type: Set[str]\n\n        for path in js_files:\n            if not is_absolute(path):\n                path = self.static_url(path)\n            if path not in unique_paths:\n                paths.append(path)\n                unique_paths.add(path)\n\n        return \"\".join(\n            '<script src=\"'\n            + escape.xhtml_escape(p)\n            + '\" type=\"text/javascript\"></script>'\n            for p in paths\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_embed_js(self, js_embed: Iterable[bytes]) -> bytes:\n        return (\n            b'<script type=\"text/javascript\">\\n//<![CDATA[\\n'\n            + b\"\\n\".join(js_embed)\n            + b\"\\n//]]>\\n</script>\"\n        )", "response": "Default method used to render the final embedded js for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render_linked_css(self, css_files: Iterable[str]) -> str:\n        paths = []\n        unique_paths = set()  # type: Set[str]\n\n        for path in css_files:\n            if not is_absolute(path):\n                path = self.static_url(path)\n            if path not in unique_paths:\n                paths.append(path)\n                unique_paths.add(path)\n\n        return \"\".join(\n            '<link href=\"' + escape.xhtml_escape(p) + '\" '\n            'type=\"text/css\" rel=\"stylesheet\"/>'\n            for p in paths\n        )", "response": "Default method used to render the final css links for the current page."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndefaulting method used to render the final embedded css for the current webpage.", "response": "def render_embed_css(self, css_embed: Iterable[bytes]) -> bytes:\n        \"\"\"Default method used to render the final embedded css for the\n        rendered webpage.\n\n        Override this method in a sub-classed controller to change the output.\n        \"\"\"\n        return b'<style type=\"text/css\">\\n' + b\"\\n\".join(css_embed) + b\"\\n</style>\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate the given template with the given arguments.", "response": "def render_string(self, template_name: str, **kwargs: Any) -> bytes:\n        \"\"\"Generate the given template with the given arguments.\n\n        We return the generated byte string (in utf8). To generate and\n        write a template as a response, use render() above.\n        \"\"\"\n        # If no template_path is specified, use the path of the calling file\n        template_path = self.get_template_path()\n        if not template_path:\n            frame = sys._getframe(0)\n            web_file = frame.f_code.co_filename\n            while frame.f_code.co_filename == web_file:\n                frame = frame.f_back\n            assert frame.f_code.co_filename is not None\n            template_path = os.path.dirname(frame.f_code.co_filename)\n        with RequestHandler._template_loader_lock:\n            if template_path not in RequestHandler._template_loaders:\n                loader = self.create_template_loader(template_path)\n                RequestHandler._template_loaders[template_path] = loader\n            else:\n                loader = RequestHandler._template_loaders[template_path]\n        t = loader.load(template_name)\n        namespace = self.get_template_namespace()\n        namespace.update(kwargs)\n        return t.generate(**namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_template_namespace(self) -> Dict[str, Any]:\n        namespace = dict(\n            handler=self,\n            request=self.request,\n            current_user=self.current_user,\n            locale=self.locale,\n            _=self.locale.translate,\n            pgettext=self.locale.pgettext,\n            static_url=self.static_url,\n            xsrf_form_html=self.xsrf_form_html,\n            reverse_url=self.reverse_url,\n        )\n        namespace.update(self.ui)\n        return namespace", "response": "Returns a dictionary to be used as the default template namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new template loader for the given path.", "response": "def create_template_loader(self, template_path: str) -> template.BaseLoader:\n        \"\"\"Returns a new template loader for the given path.\n\n        May be overridden by subclasses.  By default returns a\n        directory-based loader on the given path, using the\n        ``autoescape`` and ``template_whitespace`` application\n        settings.  If a ``template_loader`` application setting is\n        supplied, uses that instead.\n        \"\"\"\n        settings = self.application.settings\n        if \"template_loader\" in settings:\n            return settings[\"template_loader\"]\n        kwargs = {}\n        if \"autoescape\" in settings:\n            # autoescape=None means \"no escaping\", so we have to be sure\n            # to only pass this kwarg if the user asked for it.\n            kwargs[\"autoescape\"] = settings[\"autoescape\"]\n        if \"template_whitespace\" in settings:\n            kwargs[\"whitespace\"] = settings[\"template_whitespace\"]\n        return template.Loader(template_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nflushes the current output buffer to the network.", "response": "def flush(self, include_footers: bool = False) -> \"Future[None]\":\n        \"\"\"Flushes the current output buffer to the network.\n\n        The ``callback`` argument, if given, can be used for flow control:\n        it will be run when all flushed data has been written to the socket.\n        Note that only one flush callback can be outstanding at a time;\n        if another flush occurs before the previous flush's callback\n        has been run, the previous callback will be discarded.\n\n        .. versionchanged:: 4.0\n           Now returns a `.Future` if no callback is given.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed.\n        \"\"\"\n        assert self.request.connection is not None\n        chunk = b\"\".join(self._write_buffer)\n        self._write_buffer = []\n        if not self._headers_written:\n            self._headers_written = True\n            for transform in self._transforms:\n                assert chunk is not None\n                self._status_code, self._headers, chunk = transform.transform_first_chunk(\n                    self._status_code, self._headers, chunk, include_footers\n                )\n            # Ignore the chunk and only write the headers for HEAD requests\n            if self.request.method == \"HEAD\":\n                chunk = b\"\"\n\n            # Finalize the cookie headers (which have been stored in a side\n            # object so an outgoing cookie could be overwritten before it\n            # is sent).\n            if hasattr(self, \"_new_cookie\"):\n                for cookie in self._new_cookie.values():\n                    self.add_header(\"Set-Cookie\", cookie.OutputString(None))\n\n            start_line = httputil.ResponseStartLine(\"\", self._status_code, self._reason)\n            return self.request.connection.write_headers(\n                start_line, self._headers, chunk\n            )\n        else:\n            for transform in self._transforms:\n                chunk = transform.transform_chunk(chunk, include_footers)\n            # Ignore the chunk and only write the headers for HEAD requests\n            if self.request.method != \"HEAD\":\n                return self.request.connection.write(chunk)\n            else:\n                future = Future()  # type: Future[None]\n                future.set_result(None)\n                return future"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinishes the HTTP request and returns a Future that resolves when all the data has been sent.", "response": "def finish(self, chunk: Union[str, bytes, dict] = None) -> \"Future[None]\":\n        \"\"\"Finishes this response, ending the HTTP request.\n\n        Passing a ``chunk`` to ``finish()`` is equivalent to passing that\n        chunk to ``write()`` and then calling ``finish()`` with no arguments.\n\n        Returns a `.Future` which may optionally be awaited to track the sending\n        of the response to the client. This `.Future` resolves when all the response\n        data has been sent, and raises an error if the connection is closed before all\n        data can be sent.\n\n        .. versionchanged:: 5.1\n\n           Now returns a `.Future` instead of ``None``.\n        \"\"\"\n        if self._finished:\n            raise RuntimeError(\"finish() called twice\")\n\n        if chunk is not None:\n            self.write(chunk)\n\n        # Automatically support ETags and add the Content-Length header if\n        # we have not flushed any content yet.\n        if not self._headers_written:\n            if (\n                self._status_code == 200\n                and self.request.method in (\"GET\", \"HEAD\")\n                and \"Etag\" not in self._headers\n            ):\n                self.set_etag_header()\n                if self.check_etag_header():\n                    self._write_buffer = []\n                    self.set_status(304)\n            if self._status_code in (204, 304) or (\n                self._status_code >= 100 and self._status_code < 200\n            ):\n                assert not self._write_buffer, (\n                    \"Cannot send body with %s\" % self._status_code\n                )\n                self._clear_headers_for_304()\n            elif \"Content-Length\" not in self._headers:\n                content_length = sum(len(part) for part in self._write_buffer)\n                self.set_header(\"Content-Length\", content_length)\n\n        assert self.request.connection is not None\n        # Now that the request is finished, clear the callback we\n        # set on the HTTPConnection (which would otherwise prevent the\n        # garbage collection of the RequestHandler when there\n        # are keepalive connections)\n        self.request.connection.set_close_callback(None)  # type: ignore\n\n        future = self.flush(include_footers=True)\n        self.request.connection.finish()\n        self._log()\n        self._finished = True\n        self.on_finish()\n        self._break_cycles()\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef detach(self) -> iostream.IOStream:\n        self._finished = True\n        # TODO: add detach to HTTPConnection?\n        return self.request.connection.detach()", "response": "Detach the underlying stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending an error response to the browser.", "response": "def send_error(self, status_code: int = 500, **kwargs: Any) -> None:\n        \"\"\"Sends the given HTTP error code to the browser.\n\n        If `flush()` has already been called, it is not possible to send\n        an error, so this method will simply terminate the response.\n        If output has been written but not yet flushed, it will be discarded\n        and replaced with the error page.\n\n        Override `write_error()` to customize the error page that is returned.\n        Additional keyword arguments are passed through to `write_error`.\n        \"\"\"\n        if self._headers_written:\n            gen_log.error(\"Cannot send error response after headers written\")\n            if not self._finished:\n                # If we get an error between writing headers and finishing,\n                # we are unlikely to be able to finish due to a\n                # Content-Length mismatch. Try anyway to release the\n                # socket.\n                try:\n                    self.finish()\n                except Exception:\n                    gen_log.error(\"Failed to flush partial response\", exc_info=True)\n            return\n        self.clear()\n\n        reason = kwargs.get(\"reason\")\n        if \"exc_info\" in kwargs:\n            exception = kwargs[\"exc_info\"][1]\n            if isinstance(exception, HTTPError) and exception.reason:\n                reason = exception.reason\n        self.set_status(status_code, reason=reason)\n        try:\n            self.write_error(status_code, **kwargs)\n        except Exception:\n            app_log.error(\"Uncaught exception in write_error\", exc_info=True)\n        if not self._finished:\n            self.finish()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noverride to implement custom error pages.", "response": "def write_error(self, status_code: int, **kwargs: Any) -> None:\n        \"\"\"Override to implement custom error pages.\n\n        ``write_error`` may call `write`, `render`, `set_header`, etc\n        to produce output as usual.\n\n        If this error was caused by an uncaught exception (including\n        HTTPError), an ``exc_info`` triple will be available as\n        ``kwargs[\"exc_info\"]``.  Note that this exception may not be\n        the \"current\" exception for purposes of methods like\n        ``sys.exc_info()`` or ``traceback.format_exc``.\n        \"\"\"\n        if self.settings.get(\"serve_traceback\") and \"exc_info\" in kwargs:\n            # in debug mode, try to send a traceback\n            self.set_header(\"Content-Type\", \"text/plain\")\n            for line in traceback.format_exception(*kwargs[\"exc_info\"]):\n                self.write(line)\n            self.finish()\n        else:\n            self.finish(\n                \"<html><title>%(code)d: %(message)s</title>\"\n                \"<body>%(code)d: %(message)s</body></html>\"\n                % {\"code\": status_code, \"message\": self._reason}\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef locale(self) -> tornado.locale.Locale:\n        if not hasattr(self, \"_locale\"):\n            loc = self.get_user_locale()\n            if loc is not None:\n                self._locale = loc\n            else:\n                self._locale = self.get_browser_locale()\n                assert self._locale\n        return self._locale", "response": "The locale for the current session."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_browser_locale(self, default: str = \"en_US\") -> tornado.locale.Locale:\n        if \"Accept-Language\" in self.request.headers:\n            languages = self.request.headers[\"Accept-Language\"].split(\",\")\n            locales = []\n            for language in languages:\n                parts = language.strip().split(\";\")\n                if len(parts) > 1 and parts[1].startswith(\"q=\"):\n                    try:\n                        score = float(parts[1][2:])\n                    except (ValueError, TypeError):\n                        score = 0.0\n                else:\n                    score = 1.0\n                locales.append((parts[0], score))\n            if locales:\n                locales.sort(key=lambda pair: pair[1], reverse=True)\n                codes = [l[0] for l in locales]\n                return locale.get(*codes)\n        return locale.get(default)", "response": "Determines the user s locale from Accept - Language header."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef current_user(self) -> Any:\n        if not hasattr(self, \"_current_user\"):\n            self._current_user = self.get_current_user()\n        return self._current_user", "response": "The authenticated user for this request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the XSRF - prevention token for the current user or session.", "response": "def xsrf_token(self) -> bytes:\n        \"\"\"The XSRF-prevention token for the current user/session.\n\n        To prevent cross-site request forgery, we set an '_xsrf' cookie\n        and include the same '_xsrf' value as an argument with all POST\n        requests. If the two do not match, we reject the form submission\n        as a potential forgery.\n\n        See http://en.wikipedia.org/wiki/Cross-site_request_forgery\n\n        This property is of type `bytes`, but it contains only ASCII\n        characters. If a character string is required, there is no\n        need to base64-encode it; just decode the byte string as\n        UTF-8.\n\n        .. versionchanged:: 3.2.2\n           The xsrf token will now be have a random mask applied in every\n           request, which makes it safe to include the token in pages\n           that are compressed.  See http://breachattack.com for more\n           information on the issue fixed by this change.  Old (version 1)\n           cookies will be converted to version 2 when this method is called\n           unless the ``xsrf_cookie_version`` `Application` setting is\n           set to 1.\n\n        .. versionchanged:: 4.3\n           The ``xsrf_cookie_kwargs`` `Application` setting may be\n           used to supply additional cookie options (which will be\n           passed directly to `set_cookie`). For example,\n           ``xsrf_cookie_kwargs=dict(httponly=True, secure=True)``\n           will set the ``secure`` and ``httponly`` flags on the\n           ``_xsrf`` cookie.\n        \"\"\"\n        if not hasattr(self, \"_xsrf_token\"):\n            version, token, timestamp = self._get_raw_xsrf_token()\n            output_version = self.settings.get(\"xsrf_cookie_version\", 2)\n            cookie_kwargs = self.settings.get(\"xsrf_cookie_kwargs\", {})\n            if output_version == 1:\n                self._xsrf_token = binascii.b2a_hex(token)\n            elif output_version == 2:\n                mask = os.urandom(4)\n                self._xsrf_token = b\"|\".join(\n                    [\n                        b\"2\",\n                        binascii.b2a_hex(mask),\n                        binascii.b2a_hex(_websocket_mask(mask, token)),\n                        utf8(str(int(timestamp))),\n                    ]\n                )\n            else:\n                raise ValueError(\"unknown xsrf cookie version %d\", output_version)\n            if version is None:\n                if self.current_user and \"expires_days\" not in cookie_kwargs:\n                    cookie_kwargs[\"expires_days\"] = 30\n                self.set_cookie(\"_xsrf\", self._xsrf_token, **cookie_kwargs)\n        return self._xsrf_token"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_raw_xsrf_token(self) -> Tuple[Optional[int], bytes, float]:\n        if not hasattr(self, \"_raw_xsrf_token\"):\n            cookie = self.get_cookie(\"_xsrf\")\n            if cookie:\n                version, token, timestamp = self._decode_xsrf_token(cookie)\n            else:\n                version, token, timestamp = None, None, None\n            if token is None:\n                version = None\n                token = os.urandom(16)\n                timestamp = time.time()\n            assert token is not None\n            assert timestamp is not None\n            self._raw_xsrf_token = (version, token, timestamp)\n        return self._raw_xsrf_token", "response": "Read or generate the xsrf token in its raw form."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a string into a tuple form returned by _get_raw_xsrf_token.", "response": "def _decode_xsrf_token(\n        self, cookie: str\n    ) -> Tuple[Optional[int], Optional[bytes], Optional[float]]:\n        \"\"\"Convert a cookie string into a the tuple form returned by\n        _get_raw_xsrf_token.\n        \"\"\"\n\n        try:\n            m = _signed_value_version_re.match(utf8(cookie))\n\n            if m:\n                version = int(m.group(1))\n                if version == 2:\n                    _, mask_str, masked_token, timestamp_str = cookie.split(\"|\")\n\n                    mask = binascii.a2b_hex(utf8(mask_str))\n                    token = _websocket_mask(mask, binascii.a2b_hex(utf8(masked_token)))\n                    timestamp = int(timestamp_str)\n                    return version, token, timestamp\n                else:\n                    # Treat unknown versions as not present instead of failing.\n                    raise Exception(\"Unknown xsrf cookie version\")\n            else:\n                version = 1\n                try:\n                    token = binascii.a2b_hex(utf8(cookie))\n                except (binascii.Error, TypeError):\n                    token = utf8(cookie)\n                # We don't have a usable timestamp in older versions.\n                timestamp = int(time.time())\n                return (version, token, timestamp)\n        except Exception:\n            # Catch exceptions and return nothing instead of failing.\n            gen_log.debug(\"Uncaught exception in _decode_xsrf_token\", exc_info=True)\n            return None, None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies that the _xsrf cookie matches the _xsrf argument.", "response": "def check_xsrf_cookie(self) -> None:\n        \"\"\"Verifies that the ``_xsrf`` cookie matches the ``_xsrf`` argument.\n\n        To prevent cross-site request forgery, we set an ``_xsrf``\n        cookie and include the same value as a non-cookie\n        field with all ``POST`` requests. If the two do not match, we\n        reject the form submission as a potential forgery.\n\n        The ``_xsrf`` value may be set as either a form field named ``_xsrf``\n        or in a custom HTTP header named ``X-XSRFToken`` or ``X-CSRFToken``\n        (the latter is accepted for compatibility with Django).\n\n        See http://en.wikipedia.org/wiki/Cross-site_request_forgery\n\n        .. versionchanged:: 3.2.2\n           Added support for cookie version 2.  Both versions 1 and 2 are\n           supported.\n        \"\"\"\n        # Prior to release 1.1.1, this check was ignored if the HTTP header\n        # ``X-Requested-With: XMLHTTPRequest`` was present.  This exception\n        # has been shown to be insecure and has been removed.  For more\n        # information please see\n        # http://www.djangoproject.com/weblog/2011/feb/08/security/\n        # http://weblog.rubyonrails.org/2011/2/8/csrf-protection-bypass-in-ruby-on-rails\n        token = (\n            self.get_argument(\"_xsrf\", None)\n            or self.request.headers.get(\"X-Xsrftoken\")\n            or self.request.headers.get(\"X-Csrftoken\")\n        )\n        if not token:\n            raise HTTPError(403, \"'_xsrf' argument missing from POST\")\n        _, token, _ = self._decode_xsrf_token(token)\n        _, expected_token, _ = self._get_raw_xsrf_token()\n        if not token:\n            raise HTTPError(403, \"'_xsrf' argument has invalid format\")\n        if not hmac.compare_digest(utf8(token), utf8(expected_token)):\n            raise HTTPError(403, \"XSRF cookie does not match POST argument\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a static URL for the given relative static file path.", "response": "def static_url(self, path: str, include_host: bool = None, **kwargs: Any) -> str:\n        \"\"\"Returns a static URL for the given relative static file path.\n\n        This method requires you set the ``static_path`` setting in your\n        application (which specifies the root directory of your static\n        files).\n\n        This method returns a versioned url (by default appending\n        ``?v=<signature>``), which allows the static files to be\n        cached indefinitely.  This can be disabled by passing\n        ``include_version=False`` (in the default implementation;\n        other static file implementations are not required to support\n        this, but they may support other options).\n\n        By default this method returns URLs relative to the current\n        host, but if ``include_host`` is true the URL returned will be\n        absolute.  If this handler has an ``include_host`` attribute,\n        that value will be used as the default for all `static_url`\n        calls that do not pass ``include_host`` as a keyword argument.\n\n        \"\"\"\n        self.require_setting(\"static_path\", \"static_url\")\n        get_url = self.settings.get(\n            \"static_handler_class\", StaticFileHandler\n        ).make_static_url\n\n        if include_host is None:\n            include_host = getattr(self, \"include_host\", False)\n\n        if include_host:\n            base = self.request.protocol + \"://\" + self.request.host\n        else:\n            base = \"\"\n\n        return base + get_url(self.settings, path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef require_setting(self, name: str, feature: str = \"this feature\") -> None:\n        if not self.application.settings.get(name):\n            raise Exception(\n                \"You must define the '%s' setting in your \"\n                \"application to use %s\" % (name, feature)\n            )", "response": "Raises an exception if the given app setting is not defined."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\naliasing for Application. reverse_url.", "response": "def reverse_url(self, name: str, *args: Any) -> str:\n        \"\"\"Alias for `Application.reverse_url`.\"\"\"\n        return self.application.reverse_url(name, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the etag header to be used for this request.", "response": "def compute_etag(self) -> Optional[str]:\n        \"\"\"Computes the etag header to be used for this request.\n\n        By default uses a hash of the content written so far.\n\n        May be overridden to provide custom etag implementations,\n        or may return None to disable tornado's default etag support.\n        \"\"\"\n        hasher = hashlib.sha1()\n        for part in self._write_buffer:\n            hasher.update(part)\n        return '\"%s\"' % hasher.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_etag_header(self) -> bool:\n        computed_etag = utf8(self._headers.get(\"Etag\", \"\"))\n        # Find all weak and strong etag values from If-None-Match header\n        # because RFC 7232 allows multiple etag values in a single header.\n        etags = re.findall(\n            br'\\*|(?:W/)?\"[^\"]*\"', utf8(self.request.headers.get(\"If-None-Match\", \"\"))\n        )\n        if not computed_etag or not etags:\n            return False\n\n        match = False\n        if etags[0] == b\"*\":\n            match = True\n        else:\n            # Use a weak comparison when comparing entity-tags.\n            def val(x: bytes) -> bytes:\n                return x[2:] if x.startswith(b\"W/\") else x\n\n            for etag in etags:\n                if val(etag) == val(computed_etag):\n                    match = True\n                    break\n        return match", "response": "Checks the ETag header against the request s If - None - Match header and returns True if the ETag matches and a 304 should be returned."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute the request with the given output transforms.", "response": "async def _execute(\n        self, transforms: List[\"OutputTransform\"], *args: bytes, **kwargs: bytes\n    ) -> None:\n        \"\"\"Executes this request with the given output transforms.\"\"\"\n        self._transforms = transforms\n        try:\n            if self.request.method not in self.SUPPORTED_METHODS:\n                raise HTTPError(405)\n            self.path_args = [self.decode_argument(arg) for arg in args]\n            self.path_kwargs = dict(\n                (k, self.decode_argument(v, name=k)) for (k, v) in kwargs.items()\n            )\n            # If XSRF cookies are turned on, reject form submissions without\n            # the proper cookie\n            if self.request.method not in (\n                \"GET\",\n                \"HEAD\",\n                \"OPTIONS\",\n            ) and self.application.settings.get(\"xsrf_cookies\"):\n                self.check_xsrf_cookie()\n\n            result = self.prepare()\n            if result is not None:\n                result = await result\n            if self._prepared_future is not None:\n                # Tell the Application we've finished with prepare()\n                # and are ready for the body to arrive.\n                future_set_result_unless_cancelled(self._prepared_future, None)\n            if self._finished:\n                return\n\n            if _has_stream_request_body(self.__class__):\n                # In streaming mode request.body is a Future that signals\n                # the body has been completely received.  The Future has no\n                # result; the data has been passed to self.data_received\n                # instead.\n                try:\n                    await self.request._body_future\n                except iostream.StreamClosedError:\n                    return\n\n            method = getattr(self, self.request.method.lower())\n            result = method(*self.path_args, **self.path_kwargs)\n            if result is not None:\n                result = await result\n            if self._auto_finish and not self._finished:\n                self.finish()\n        except Exception as e:\n            try:\n                self._handle_request_exception(e)\n            except Exception:\n                app_log.error(\"Exception in exception handler\", exc_info=True)\n            finally:\n                # Unset result to avoid circular references\n                result = None\n            if self._prepared_future is not None and not self._prepared_future.done():\n                # In case we failed before setting _prepared_future, do it\n                # now (to unblock the HTTP server).  Note that this is not\n                # in a finally block to avoid GC issues prior to Python 3.4.\n                self._prepared_future.set_result(None)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noverriding to customize logging of uncaught exceptions.", "response": "def log_exception(\n        self,\n        typ: \"Optional[Type[BaseException]]\",\n        value: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        \"\"\"Override to customize logging of uncaught exceptions.\n\n        By default logs instances of `HTTPError` as warnings without\n        stack traces (on the ``tornado.general`` logger), and all\n        other exceptions as errors with stack traces (on the\n        ``tornado.application`` logger).\n\n        .. versionadded:: 3.1\n        \"\"\"\n        if isinstance(value, HTTPError):\n            if value.log_message:\n                format = \"%d %s: \" + value.log_message\n                args = [value.status_code, self._request_summary()] + list(value.args)\n                gen_log.warning(format, *args)\n        else:\n            app_log.error(  # type: ignore\n                \"Uncaught exception %s\\n%r\",\n                self._request_summary(),\n                self.request,\n                exc_info=(typ, value, tb),\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef listen(self, port: int, address: str = \"\", **kwargs: Any) -> HTTPServer:\n        server = HTTPServer(self, **kwargs)\n        server.listen(port, address)\n        return server", "response": "Starts an HTTP server for this application on the given port."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend the given handlers to our handler list.", "response": "def add_handlers(self, host_pattern: str, host_handlers: _RuleList) -> None:\n        \"\"\"Appends the given handlers to our handler list.\n\n        Host patterns are processed sequentially in the order they were\n        added. All matching patterns will be considered.\n        \"\"\"\n        host_matcher = HostMatches(host_pattern)\n        rule = Rule(host_matcher, _ApplicationRouter(self, host_handlers))\n\n        self.default_router.rules.insert(-1, rule)\n\n        if self.default_host is not None:\n            self.wildcard_router.add_rules(\n                [(DefaultHostMatches(self, host_matcher.host_pattern), host_handlers)]\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_handler_delegate(\n        self,\n        request: httputil.HTTPServerRequest,\n        target_class: Type[RequestHandler],\n        target_kwargs: Dict[str, Any] = None,\n        path_args: List[bytes] = None,\n        path_kwargs: Dict[str, bytes] = None,\n    ) -> \"_HandlerDelegate\":\n        \"\"\"Returns `~.httputil.HTTPMessageDelegate` that can serve a request\n        for application and `RequestHandler` subclass.\n\n        :arg httputil.HTTPServerRequest request: current HTTP request.\n        :arg RequestHandler target_class: a `RequestHandler` class.\n        :arg dict target_kwargs: keyword arguments for ``target_class`` constructor.\n        :arg list path_args: positional arguments for ``target_class`` HTTP method that\n            will be executed while handling a request (``get``, ``post`` or any other).\n        :arg dict path_kwargs: keyword arguments for ``target_class`` HTTP method.\n        \"\"\"\n        return _HandlerDelegate(\n            self, request, target_class, target_kwargs, path_args, path_kwargs\n        )", "response": "Returns a new instance of _HandlerDelegate that can serve a request for application and handler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reverse_url(self, name: str, *args: Any) -> str:\n        reversed_url = self.default_router.reverse_url(name, *args)\n        if reversed_url is not None:\n            return reversed_url\n\n        raise KeyError(\"%s not found in named urls\" % name)", "response": "Returns a URL path for handler named name with optional arguments substituted for capturing groups in the URLSpec regex."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef log_request(self, handler: RequestHandler) -> None:\n        if \"log_function\" in self.settings:\n            self.settings[\"log_function\"](handler)\n            return\n        if handler.get_status() < 400:\n            log_method = access_log.info\n        elif handler.get_status() < 500:\n            log_method = access_log.warning\n        else:\n            log_method = access_log.error\n        request_time = 1000.0 * handler.request.request_time()\n        log_method(\n            \"%d %s %.2fms\",\n            handler.get_status(),\n            handler._request_summary(),\n            request_time,\n        )", "response": "Writes a completed HTTP request to the logs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the etag header based on the static url version.", "response": "def compute_etag(self) -> Optional[str]:\n        \"\"\"Sets the ``Etag`` header based on static url version.\n\n        This allows efficient ``If-None-Match`` checks against cached\n        versions, and sends the correct ``Etag`` for a partial response\n        (i.e. the same ``Etag`` as the full file).\n\n        .. versionadded:: 3.1\n        \"\"\"\n        assert self.absolute_path is not None\n        version_hash = self._get_cached_version(self.absolute_path)\n        if not version_hash:\n            return None\n        return '\"%s\"' % (version_hash,)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the content and caching headers on the response.", "response": "def set_headers(self) -> None:\n        \"\"\"Sets the content and caching headers on the response.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        self.set_header(\"Accept-Ranges\", \"bytes\")\n        self.set_etag_header()\n\n        if self.modified is not None:\n            self.set_header(\"Last-Modified\", self.modified)\n\n        content_type = self.get_content_type()\n        if content_type:\n            self.set_header(\"Content-Type\", content_type)\n\n        cache_time = self.get_cache_time(self.path, self.modified, content_type)\n        if cache_time > 0:\n            self.set_header(\n                \"Expires\",\n                datetime.datetime.utcnow() + datetime.timedelta(seconds=cache_time),\n            )\n            self.set_header(\"Cache-Control\", \"max-age=\" + str(cache_time))\n\n        self.set_extra_headers(self.path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef should_return_304(self) -> bool:\n        # If client sent If-None-Match, use it, ignore If-Modified-Since\n        if self.request.headers.get(\"If-None-Match\"):\n            return self.check_etag_header()\n\n        # Check the If-Modified-Since, and don't send the result if the\n        # content has not been modified\n        ims_value = self.request.headers.get(\"If-Modified-Since\")\n        if ims_value is not None:\n            date_tuple = email.utils.parsedate(ims_value)\n            if date_tuple is not None:\n                if_since = datetime.datetime(*date_tuple[:6])\n                assert self.modified is not None\n                if if_since >= self.modified:\n                    return True\n\n        return False", "response": "Returns True if the headers indicate that we should return 304."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_absolute_path(cls, root: str, path: str) -> str:\n        abspath = os.path.abspath(os.path.join(root, path))\n        return abspath", "response": "Returns the absolute location of path relative to root."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_absolute_path(self, root: str, absolute_path: str) -> Optional[str]:\n        # os.path.abspath strips a trailing /.\n        # We must add it back to `root` so that we only match files\n        # in a directory named `root` instead of files starting with\n        # that prefix.\n        root = os.path.abspath(root)\n        if not root.endswith(os.path.sep):\n            # abspath always removes a trailing slash, except when\n            # root is '/'. This is an unusual case, but several projects\n            # have independently discovered this technique to disable\n            # Tornado's path validation and (hopefully) do their own,\n            # so we need to support it.\n            root += os.path.sep\n        # The trailing slash also needs to be temporarily added back\n        # the requested path so a request to root/ will match.\n        if not (absolute_path + os.path.sep).startswith(root):\n            raise HTTPError(403, \"%s is not in root static directory\", self.path)\n        if os.path.isdir(absolute_path) and self.default_filename is not None:\n            # need to look at the request.path here for when path is empty\n            # but there is some prefix to the path that was already\n            # trimmed by the routing\n            if not self.request.path.endswith(\"/\"):\n                self.redirect(self.request.path + \"/\", permanent=True)\n                return None\n            absolute_path = os.path.join(absolute_path, self.default_filename)\n        if not os.path.exists(absolute_path):\n            raise HTTPError(404)\n        if not os.path.isfile(absolute_path):\n            raise HTTPError(403, \"%s is not a file\", self.path)\n        return absolute_path", "response": "Validate and return the absolute path for the static file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_content(\n        cls, abspath: str, start: int = None, end: int = None\n    ) -> Generator[bytes, None, None]:\n        \"\"\"Retrieve the content of the requested resource which is located\n        at the given absolute path.\n\n        This class method may be overridden by subclasses.  Note that its\n        signature is different from other overridable class methods\n        (no ``settings`` argument); this is deliberate to ensure that\n        ``abspath`` is able to stand on its own as a cache key.\n\n        This method should either return a byte string or an iterator\n        of byte strings.  The latter is preferred for large files\n        as it helps reduce memory fragmentation.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        with open(abspath, \"rb\") as file:\n            if start is not None:\n                file.seek(start)\n            if end is not None:\n                remaining = end - (start or 0)  # type: Optional[int]\n            else:\n                remaining = None\n            while True:\n                chunk_size = 64 * 1024\n                if remaining is not None and remaining < chunk_size:\n                    chunk_size = remaining\n                chunk = file.read(chunk_size)\n                if chunk:\n                    if remaining is not None:\n                        remaining -= len(chunk)\n                    yield chunk\n                else:\n                    if remaining is not None:\n                        assert remaining == 0\n                    return", "response": "Returns the content of the requested resource which is located\n        at the given absolute path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a version string for the resource at the given path.", "response": "def get_content_version(cls, abspath: str) -> str:\n        \"\"\"Returns a version string for the resource at the given path.\n\n        This class method may be overridden by subclasses.  The\n        default implementation is a hash of the file's contents.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        data = cls.get_content(abspath)\n        hasher = hashlib.md5()\n        if isinstance(data, bytes):\n            hasher.update(data)\n        else:\n            for chunk in data:\n                hasher.update(chunk)\n        return hasher.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_modified_time(self) -> Optional[datetime.datetime]:\n        stat_result = self._stat()\n        # NOTE: Historically, this used stat_result[stat.ST_MTIME],\n        # which truncates the fractional portion of the timestamp. It\n        # was changed from that form to stat_result.st_mtime to\n        # satisfy mypy (which disallows the bracket operator), but the\n        # latter form returns a float instead of an int. For\n        # consistency with the past (and because we have a unit test\n        # that relies on this), we truncate the float here, although\n        # I'm not sure that's the right thing to do.\n        modified = datetime.datetime.utcfromtimestamp(int(stat_result.st_mtime))\n        return modified", "response": "Returns the time that this file was last modified."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_content_type(self) -> str:\n        assert self.absolute_path is not None\n        mime_type, encoding = mimetypes.guess_type(self.absolute_path)\n        # per RFC 6713, use the appropriate type for a gzip compressed file\n        if encoding == \"gzip\":\n            return \"application/gzip\"\n        # As of 2015-07-21 there is no bzip2 encoding defined at\n        # http://www.iana.org/assignments/media-types/media-types.xhtml\n        # So for that (and any other encoding), use octet-stream.\n        elif encoding is not None:\n            return \"application/octet-stream\"\n        elif mime_type is not None:\n            return mime_type\n        # if mime_type not detected, use application/octet-stream\n        else:\n            return \"application/octet-stream\"", "response": "Returns the content - type header to be used for this request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_cache_time(\n        self, path: str, modified: Optional[datetime.datetime], mime_type: str\n    ) -> int:\n        \"\"\"Override to customize cache control behavior.\n\n        Return a positive number of seconds to make the result\n        cacheable for that amount of time or 0 to mark resource as\n        cacheable for an unspecified amount of time (subject to\n        browser heuristics).\n\n        By default returns cache expiry of 10 years for resources requested\n        with ``v`` argument.\n        \"\"\"\n        return self.CACHE_MAX_AGE if \"v\" in self.request.arguments else 0", "response": "Override to customize cache control behavior."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a versioned url for the given path.", "response": "def make_static_url(\n        cls, settings: Dict[str, Any], path: str, include_version: bool = True\n    ) -> str:\n        \"\"\"Constructs a versioned url for the given path.\n\n        This method may be overridden in subclasses (but note that it\n        is a class method rather than an instance method).  Subclasses\n        are only required to implement the signature\n        ``make_static_url(cls, settings, path)``; other keyword\n        arguments may be passed through `~RequestHandler.static_url`\n        but are not standard.\n\n        ``settings`` is the `Application.settings` dictionary.  ``path``\n        is the static path being requested.  The url returned should be\n        relative to the current host.\n\n        ``include_version`` determines whether the generated URL should\n        include the query string containing the version hash of the\n        file corresponding to the given ``path``.\n\n        \"\"\"\n        url = settings.get(\"static_url_prefix\", \"/static/\") + path\n        if not include_version:\n            return url\n\n        version_hash = cls.get_version(settings, path)\n        if not version_hash:\n            return url\n\n        return \"%s?v=%s\" % (url, version_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_url_path(self, url_path: str) -> str:\n        if os.path.sep != \"/\":\n            url_path = url_path.replace(\"/\", os.path.sep)\n        return url_path", "response": "Converts a static URL path into a filesystem path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the version string to be used in static URLs.", "response": "def get_version(cls, settings: Dict[str, Any], path: str) -> Optional[str]:\n        \"\"\"Generate the version string to be used in static URLs.\n\n        ``settings`` is the `Application.settings` dictionary and ``path``\n        is the relative location of the requested asset on the filesystem.\n        The returned value should be a string, or ``None`` if no version\n        could be determined.\n\n        .. versionchanged:: 3.1\n           This method was previously recommended for subclasses to override;\n           `get_content_version` is now preferred as it allows the base\n           class to handle caching of the result.\n        \"\"\"\n        abs_path = cls.get_absolute_path(settings[\"static_path\"], path)\n        return cls._get_cached_version(abs_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrendering a template and returns it as a string.", "response": "def render_string(self, path: str, **kwargs: Any) -> bytes:\n        \"\"\"Renders a template and returns it as a string.\"\"\"\n        return self.handler.render_string(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nescaping a string so it is valid within HTML or XML.", "response": "def xhtml_escape(value: Union[str, bytes]) -> str:\n    \"\"\"Escapes a string so it is valid within HTML or XML.\n\n    Escapes the characters ``<``, ``>``, ``\"``, ``'``, and ``&``.\n    When used in attribute values the escaped strings must be enclosed\n    in quotes.\n\n    .. versionchanged:: 3.2\n\n       Added the single quote to the list of escaped characters.\n    \"\"\"\n    return _XHTML_ESCAPE_RE.sub(\n        lambda match: _XHTML_ESCAPE_DICT[match.group(0)], to_basestring(value)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef xhtml_unescape(value: Union[str, bytes]) -> str:\n    return re.sub(r\"&(#?)(\\w+?);\", _convert_entity, _unicode(value))", "response": "Un - escapes an XML - escaped string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn Python objects for the given JSON string. Supports both str and bytes inputs.", "response": "def json_decode(value: Union[str, bytes]) -> Any:\n    \"\"\"Returns Python objects for the given JSON string.\n\n    Supports both `str` and `bytes` inputs.\n    \"\"\"\n    return json.loads(to_basestring(value))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a URL - encoded version of the given value.", "response": "def url_escape(value: Union[str, bytes], plus: bool = True) -> str:\n    \"\"\"Returns a URL-encoded version of the given value.\n\n    If ``plus`` is true (the default), spaces will be represented\n    as \"+\" instead of \"%20\".  This is appropriate for query strings\n    but not for the path component of a URL.  Note that this default\n    is the reverse of Python's urllib module.\n\n    .. versionadded:: 3.1\n        The ``plus`` argument\n    \"\"\"\n    quote = urllib.parse.quote_plus if plus else urllib.parse.quote\n    return quote(utf8(value))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding the given value from a URL.", "response": "def url_unescape(  # noqa: F811\n    value: Union[str, bytes], encoding: Optional[str] = \"utf-8\", plus: bool = True\n) -> Union[str, bytes]:\n    \"\"\"Decodes the given value from a URL.\n\n    The argument may be either a byte or unicode string.\n\n    If encoding is None, the result will be a byte string.  Otherwise,\n    the result is a unicode string in the specified encoding.\n\n    If ``plus`` is true (the default), plus signs will be interpreted\n    as spaces (literal plus signs must be represented as \"%2B\").  This\n    is appropriate for query strings and form-encoded values but not\n    for the path component of a URL.  Note that this default is the\n    reverse of Python's urllib module.\n\n    .. versionadded:: 3.1\n       The ``plus`` argument\n    \"\"\"\n    if encoding is None:\n        if plus:\n            # unquote_to_bytes doesn't have a _plus variant\n            value = to_basestring(value).replace(\"+\", \" \")\n        return urllib.parse.unquote_to_bytes(value)\n    else:\n        unquote = urllib.parse.unquote_plus if plus else urllib.parse.unquote\n        return unquote(to_basestring(value), encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_qs_bytes(\n    qs: str, keep_blank_values: bool = False, strict_parsing: bool = False\n) -> Dict[str, List[bytes]]:\n    \"\"\"Parses a query string like urlparse.parse_qs, but returns the\n    values as byte strings.\n\n    Keys still become type str (interpreted as latin1 in python3!)\n    because it's too painful to keep them as byte strings in\n    python3 and in practice they're nearly always ascii anyway.\n    \"\"\"\n    # This is gross, but python3 doesn't give us another way.\n    # Latin1 is the universal donor of character encodings.\n    result = urllib.parse.parse_qs(\n        qs, keep_blank_values, strict_parsing, encoding=\"latin1\", errors=\"strict\"\n    )\n    encoded = {}\n    for k, v in result.items():\n        encoded[k] = [i.encode(\"latin1\") for i in v]\n    return encoded", "response": "Parses a query string like urlparse. parse_qs but returns the values as byte strings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a string argument to a byte string.", "response": "def utf8(value: Union[None, str, bytes]) -> Optional[bytes]:  # noqa: F811\n    \"\"\"Converts a string argument to a byte string.\n\n    If the argument is already a byte string or None, it is returned unchanged.\n    Otherwise it must be a unicode string and is encoded as utf8.\n    \"\"\"\n    if isinstance(value, _UTF8_TYPES):\n        return value\n    if not isinstance(value, unicode_type):\n        raise TypeError(\"Expected bytes, unicode, or None; got %r\" % type(value))\n    return value.encode(\"utf-8\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a string argument to a unicode string.", "response": "def to_unicode(value: Union[None, str, bytes]) -> Optional[str]:  # noqa: F811\n    \"\"\"Converts a string argument to a unicode string.\n\n    If the argument is already a unicode string or None, it is returned\n    unchanged.  Otherwise it must be a byte string and is decoded as utf8.\n    \"\"\"\n    if isinstance(value, _TO_UNICODE_TYPES):\n        return value\n    if not isinstance(value, bytes):\n        raise TypeError(\"Expected bytes, unicode, or None; got %r\" % type(value))\n    return value.decode(\"utf-8\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwalks a simple data structure, converting byte strings to unicode. Supports lists, tuples, and dictionaries.", "response": "def recursive_unicode(obj: Any) -> Any:\n    \"\"\"Walks a simple data structure, converting byte strings to unicode.\n\n    Supports lists, tuples, and dictionaries.\n    \"\"\"\n    if isinstance(obj, dict):\n        return dict(\n            (recursive_unicode(k), recursive_unicode(v)) for (k, v) in obj.items()\n        )\n    elif isinstance(obj, list):\n        return list(recursive_unicode(i) for i in obj)\n    elif isinstance(obj, tuple):\n        return tuple(recursive_unicode(i) for i in obj)\n    elif isinstance(obj, bytes):\n        return to_unicode(obj)\n    else:\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting plain text into HTML with links.", "response": "def linkify(\n    text: Union[str, bytes],\n    shorten: bool = False,\n    extra_params: Union[str, Callable[[str], str]] = \"\",\n    require_protocol: bool = False,\n    permitted_protocols: List[str] = [\"http\", \"https\"],\n) -> str:\n    \"\"\"Converts plain text into HTML with links.\n\n    For example: ``linkify(\"Hello http://tornadoweb.org!\")`` would return\n    ``Hello <a href=\"http://tornadoweb.org\">http://tornadoweb.org</a>!``\n\n    Parameters:\n\n    * ``shorten``: Long urls will be shortened for display.\n\n    * ``extra_params``: Extra text to include in the link tag, or a callable\n      taking the link as an argument and returning the extra text\n      e.g. ``linkify(text, extra_params='rel=\"nofollow\" class=\"external\"')``,\n      or::\n\n          def extra_params_cb(url):\n              if url.startswith(\"http://example.com\"):\n                  return 'class=\"internal\"'\n              else:\n                  return 'class=\"external\" rel=\"nofollow\"'\n          linkify(text, extra_params=extra_params_cb)\n\n    * ``require_protocol``: Only linkify urls which include a protocol. If\n      this is False, urls such as www.facebook.com will also be linkified.\n\n    * ``permitted_protocols``: List (or set) of protocols which should be\n      linkified, e.g. ``linkify(text, permitted_protocols=[\"http\", \"ftp\",\n      \"mailto\"])``. It is very unsafe to include protocols such as\n      ``javascript``.\n    \"\"\"\n    if extra_params and not callable(extra_params):\n        extra_params = \" \" + extra_params.strip()\n\n    def make_link(m: typing.Match) -> str:\n        url = m.group(1)\n        proto = m.group(2)\n        if require_protocol and not proto:\n            return url  # not protocol, no linkify\n\n        if proto and proto not in permitted_protocols:\n            return url  # bad protocol, no linkify\n\n        href = m.group(1)\n        if not proto:\n            href = \"http://\" + href  # no proto specified, use http\n\n        if callable(extra_params):\n            params = \" \" + extra_params(href).strip()\n        else:\n            params = extra_params\n\n        # clip long urls. max_len is just an approximation\n        max_len = 30\n        if shorten and len(url) > max_len:\n            before_clip = url\n            if proto:\n                proto_len = len(proto) + 1 + len(m.group(3) or \"\")  # +1 for :\n            else:\n                proto_len = 0\n\n            parts = url[proto_len:].split(\"/\")\n            if len(parts) > 1:\n                # Grab the whole host part plus the first bit of the path\n                # The path is usually not that interesting once shortened\n                # (no more slug, etc), so it really just provides a little\n                # extra indication of shortening.\n                url = (\n                    url[:proto_len]\n                    + parts[0]\n                    + \"/\"\n                    + parts[1][:8].split(\"?\")[0].split(\".\")[0]\n                )\n\n            if len(url) > max_len * 1.5:  # still too long\n                url = url[:max_len]\n\n            if url != before_clip:\n                amp = url.rfind(\"&\")\n                # avoid splitting html char entities\n                if amp > max_len - 5:\n                    url = url[:amp]\n                url += \"...\"\n\n                if len(url) >= len(before_clip):\n                    url = before_clip\n                else:\n                    # full url is visible on mouse-over (for those who don't\n                    # have a status bar, such as Safari by default)\n                    params += ' title=\"%s\"' % href\n\n        return u'<a href=\"%s\"%s>%s</a>' % (href, params, url)\n\n    # First HTML-escape so that our strings are all safe.\n    # The regex is modified to avoid character entites other than &amp; so\n    # that we won't pick up &quot;, etc.\n    text = _unicode(xhtml_escape(text))\n    return _URL_RE.sub(make_link, text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the current thread s IOLoop.", "response": "def current(instance: bool = True) -> Optional[\"IOLoop\"]:\n        \"\"\"Returns the current thread's `IOLoop`.\n\n        If an `IOLoop` is currently running or has been marked as\n        current by `make_current`, returns that instance.  If there is\n        no current `IOLoop` and ``instance`` is true, creates one.\n\n        .. versionchanged:: 4.1\n           Added ``instance`` argument to control the fallback to\n           `IOLoop.instance()`.\n        .. versionchanged:: 5.0\n           On Python 3, control of the current `IOLoop` is delegated\n           to `asyncio`, with this and other methods as pass-through accessors.\n           The ``instance`` argument now controls whether an `IOLoop`\n           is created automatically when there is none, instead of\n           whether we fall back to `IOLoop.instance()` (which is now\n           an alias for this method). ``instance=False`` is deprecated,\n           since even if we do not create an `IOLoop`, this method\n           may initialize the asyncio loop.\n        \"\"\"\n        try:\n            loop = asyncio.get_event_loop()\n        except (RuntimeError, AssertionError):\n            if not instance:\n                return None\n            raise\n        try:\n            return IOLoop._ioloop_for_asyncio[loop]\n        except KeyError:\n            if instance:\n                from tornado.platform.asyncio import AsyncIOMainLoop\n\n                current = AsyncIOMainLoop(make_current=True)  # type: Optional[IOLoop]\n            else:\n                current = None\n        return current"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear the current thread s IOLoop.", "response": "def clear_current() -> None:\n        \"\"\"Clears the `IOLoop` for the current thread.\n\n        Intended primarily for use by test frameworks in between tests.\n\n        .. versionchanged:: 5.0\n           This method also clears the current `asyncio` event loop.\n        \"\"\"\n        old = IOLoop.current(instance=False)\n        if old is not None:\n            old._clear_current_hook()\n        if asyncio is None:\n            IOLoop._current.instance = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_handler(  # noqa: F811\n        self, fd: Union[int, _Selectable], handler: Callable[..., None], events: int\n    ) -> None:\n        \"\"\"Registers the given handler to receive the given events for ``fd``.\n\n        The ``fd`` argument may either be an integer file descriptor or\n        a file-like object with a ``fileno()`` and ``close()`` method.\n\n        The ``events`` argument is a bitwise or of the constants\n        ``IOLoop.READ``, ``IOLoop.WRITE``, and ``IOLoop.ERROR``.\n\n        When an event occurs, ``handler(fd, events)`` will be run.\n\n        .. versionchanged:: 4.0\n           Added the ability to pass file-like objects in addition to\n           raw file descriptors.\n        \"\"\"\n        raise NotImplementedError()", "response": "Registers the given handler to receive the given events for fd."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _setup_logging(self) -> None:\n        if not any(\n            [\n                logging.getLogger().handlers,\n                logging.getLogger(\"tornado\").handlers,\n                logging.getLogger(\"tornado.application\").handlers,\n            ]\n        ):\n            logging.basicConfig()", "response": "Configure logging for this IOLoop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_sync(self, func: Callable, timeout: float = None) -> Any:\n        future_cell = [None]  # type: List[Optional[Future]]\n\n        def run() -> None:\n            try:\n                result = func()\n                if result is not None:\n                    from tornado.gen import convert_yielded\n\n                    result = convert_yielded(result)\n            except Exception:\n                fut = Future()  # type: Future[Any]\n                future_cell[0] = fut\n                future_set_exc_info(fut, sys.exc_info())\n            else:\n                if is_future(result):\n                    future_cell[0] = result\n                else:\n                    fut = Future()\n                    future_cell[0] = fut\n                    fut.set_result(result)\n            assert future_cell[0] is not None\n            self.add_future(future_cell[0], lambda future: self.stop())\n\n        self.add_callback(run)\n        if timeout is not None:\n\n            def timeout_callback() -> None:\n                # If we can cancel the future, do so and wait on it. If not,\n                # Just stop the loop and return with the task still pending.\n                # (If we neither cancel nor wait for the task, a warning\n                # will be logged).\n                assert future_cell[0] is not None\n                if not future_cell[0].cancel():\n                    self.stop()\n\n            timeout_handle = self.add_timeout(self.time() + timeout, timeout_callback)\n        self.start()\n        if timeout is not None:\n            self.remove_timeout(timeout_handle)\n        assert future_cell[0] is not None\n        if future_cell[0].cancelled() or not future_cell[0].done():\n            raise TimeoutError(\"Operation timed out after %s seconds\" % timeout)\n        return future_cell[0].result()", "response": "Starts the IOLoop runs the given function and stops the IOLoop."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a callback to the current time for the specified deadline.", "response": "def add_timeout(\n        self,\n        deadline: Union[float, datetime.timedelta],\n        callback: Callable[..., None],\n        *args: Any,\n        **kwargs: Any\n    ) -> object:\n        \"\"\"Runs the ``callback`` at the time ``deadline`` from the I/O loop.\n\n        Returns an opaque handle that may be passed to\n        `remove_timeout` to cancel.\n\n        ``deadline`` may be a number denoting a time (on the same\n        scale as `IOLoop.time`, normally `time.time`), or a\n        `datetime.timedelta` object for a deadline relative to the\n        current time.  Since Tornado 4.0, `call_later` is a more\n        convenient alternative for the relative case since it does not\n        require a timedelta object.\n\n        Note that it is not safe to call `add_timeout` from other threads.\n        Instead, you must use `add_callback` to transfer control to the\n        `IOLoop`'s thread, and then call `add_timeout` from there.\n\n        Subclasses of IOLoop must implement either `add_timeout` or\n        `call_at`; the default implementations of each will call\n        the other.  `call_at` is usually easier to implement, but\n        subclasses that wish to maintain compatibility with Tornado\n        versions prior to 4.0 must use `add_timeout` instead.\n\n        .. versionchanged:: 4.0\n           Now passes through ``*args`` and ``**kwargs`` to the callback.\n        \"\"\"\n        if isinstance(deadline, numbers.Real):\n            return self.call_at(deadline, callback, *args, **kwargs)\n        elif isinstance(deadline, datetime.timedelta):\n            return self.call_at(\n                self.time() + deadline.total_seconds(), callback, *args, **kwargs\n            )\n        else:\n            raise TypeError(\"Unsupported deadline %r\" % deadline)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef call_later(\n        self, delay: float, callback: Callable[..., None], *args: Any, **kwargs: Any\n    ) -> object:\n        \"\"\"Runs the ``callback`` after ``delay`` seconds have passed.\n\n        Returns an opaque handle that may be passed to `remove_timeout`\n        to cancel.  Note that unlike the `asyncio` method of the same\n        name, the returned object does not have a ``cancel()`` method.\n\n        See `add_timeout` for comments on thread-safety and subclassing.\n\n        .. versionadded:: 4.0\n        \"\"\"\n        return self.call_at(self.time() + delay, callback, *args, **kwargs)", "response": "Runs the callback at the time of the given delay seconds. Returns an opaque handle that can be passed to remove_timeout."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef call_at(\n        self, when: float, callback: Callable[..., None], *args: Any, **kwargs: Any\n    ) -> object:\n        \"\"\"Runs the ``callback`` at the absolute time designated by ``when``.\n\n        ``when`` must be a number using the same reference point as\n        `IOLoop.time`.\n\n        Returns an opaque handle that may be passed to `remove_timeout`\n        to cancel.  Note that unlike the `asyncio` method of the same\n        name, the returned object does not have a ``cancel()`` method.\n\n        See `add_timeout` for comments on thread-safety and subclassing.\n\n        .. versionadded:: 4.0\n        \"\"\"\n        return self.add_timeout(when, callback, *args, **kwargs)", "response": "Calls the callback at the absolute time designated by when. Returns an opaque handle that can be used to cancel the call."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nspawn a callback on the next IOLoop iteration.", "response": "def spawn_callback(self, callback: Callable, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Calls the given callback on the next IOLoop iteration.\n\n        As of Tornado 6.0, this method is equivalent to `add_callback`.\n\n        .. versionadded:: 4.0\n        \"\"\"\n        self.add_callback(callback, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nschedule a callback on the IOLoop when the given nanosite. Future is finished.", "response": "def add_future(\n        self,\n        future: \"Union[Future[_T], concurrent.futures.Future[_T]]\",\n        callback: Callable[[\"Future[_T]\"], None],\n    ) -> None:\n        \"\"\"Schedules a callback on the ``IOLoop`` when the given\n        `.Future` is finished.\n\n        The callback is invoked with one argument, the\n        `.Future`.\n\n        This method only accepts `.Future` objects and not other\n        awaitables (unlike most of Tornado where the two are\n        interchangeable).\n        \"\"\"\n        if isinstance(future, Future):\n            # Note that we specifically do not want the inline behavior of\n            # tornado.concurrent.future_add_done_callback. We always want\n            # this callback scheduled on the next IOLoop iteration (which\n            # asyncio.Future always does).\n            #\n            # Wrap the callback in self._run_callback so we control\n            # the error logging (i.e. it goes to tornado.log.app_log\n            # instead of asyncio's log).\n            future.add_done_callback(\n                lambda f: self._run_callback(functools.partial(callback, future))\n            )\n        else:\n            assert is_future(future)\n            # For concurrent futures, we use self.add_callback, so\n            # it's fine if future_add_done_callback inlines that call.\n            future_add_done_callback(\n                future, lambda f: self.add_callback(callback, future)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_in_executor(\n        self,\n        executor: Optional[concurrent.futures.Executor],\n        func: Callable[..., _T],\n        *args: Any\n    ) -> Awaitable[_T]:\n        \"\"\"Runs a function in a ``concurrent.futures.Executor``. If\n        ``executor`` is ``None``, the IO loop's default executor will be used.\n\n        Use `functools.partial` to pass keyword arguments to ``func``.\n\n        .. versionadded:: 5.0\n        \"\"\"\n        if executor is None:\n            if not hasattr(self, \"_executor\"):\n                from tornado.process import cpu_count\n\n                self._executor = concurrent.futures.ThreadPoolExecutor(\n                    max_workers=(cpu_count() * 5)\n                )  # type: concurrent.futures.Executor\n            executor = self._executor\n        c_future = executor.submit(func, *args)\n        # Concurrent Futures are not usable with await. Wrap this in a\n        # Tornado Future instead, using self.add_future for thread-safety.\n        t_future = Future()  # type: Future[_T]\n        self.add_future(c_future, lambda f: chain_future(f, t_future))\n        return t_future", "response": "Runs a function in an executor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_callback(self, callback: Callable[[], Any]) -> None:\n        try:\n            ret = callback()\n            if ret is not None:\n                from tornado import gen\n\n                # Functions that return Futures typically swallow all\n                # exceptions and store them in the Future.  If a Future\n                # makes it out to the IOLoop, ensure its exception (if any)\n                # gets logged too.\n                try:\n                    ret = gen.convert_yielded(ret)\n                except gen.BadYieldError:\n                    # It's not unusual for add_callback to be used with\n                    # methods returning a non-None and non-yieldable\n                    # result, which should just be ignored.\n                    pass\n                else:\n                    self.add_future(ret, self._discard_future_result)\n        except asyncio.CancelledError:\n            pass\n        except Exception:\n            app_log.error(\"Exception in callback %r\", callback, exc_info=True)", "response": "Runs a callback with error handling."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconcatenates url and arguments regardless of whether the url has existing query parameters.", "response": "def url_concat(\n    url: str,\n    args: Union[\n        None, Dict[str, str], List[Tuple[str, str]], Tuple[Tuple[str, str], ...]\n    ],\n) -> str:\n    \"\"\"Concatenate url and arguments regardless of whether\n    url has existing query parameters.\n\n    ``args`` may be either a dictionary or a list of key-value pairs\n    (the latter allows for multiple values with the same key.\n\n    >>> url_concat(\"http://example.com/foo\", dict(c=\"d\"))\n    'http://example.com/foo?c=d'\n    >>> url_concat(\"http://example.com/foo?a=b\", dict(c=\"d\"))\n    'http://example.com/foo?a=b&c=d'\n    >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])\n    'http://example.com/foo?a=b&c=d&c=d2'\n    \"\"\"\n    if args is None:\n        return url\n    parsed_url = urlparse(url)\n    if isinstance(args, dict):\n        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)\n        parsed_query.extend(args.items())\n    elif isinstance(args, list) or isinstance(args, tuple):\n        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)\n        parsed_query.extend(args)\n    else:\n        err = \"'args' parameter should be dict, list or tuple. Not {0}\".format(\n            type(args)\n        )\n        raise TypeError(err)\n    final_query = urlencode(parsed_query)\n    url = urlunparse(\n        (\n            parsed_url[0],\n            parsed_url[1],\n            parsed_url[2],\n            parsed_url[3],\n            final_query,\n            parsed_url[5],\n        )\n    )\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a Range header.", "response": "def _parse_request_range(\n    range_header: str\n) -> Optional[Tuple[Optional[int], Optional[int]]]:\n    \"\"\"Parses a Range header.\n\n    Returns either ``None`` or tuple ``(start, end)``.\n    Note that while the HTTP headers use inclusive byte positions,\n    this method returns indexes suitable for use in slices.\n\n    >>> start, end = _parse_request_range(\"bytes=1-2\")\n    >>> start, end\n    (1, 3)\n    >>> [0, 1, 2, 3, 4][start:end]\n    [1, 2]\n    >>> _parse_request_range(\"bytes=6-\")\n    (6, None)\n    >>> _parse_request_range(\"bytes=-6\")\n    (-6, None)\n    >>> _parse_request_range(\"bytes=-0\")\n    (None, 0)\n    >>> _parse_request_range(\"bytes=\")\n    (None, None)\n    >>> _parse_request_range(\"foo=42\")\n    >>> _parse_request_range(\"bytes=1-2,6-10\")\n\n    Note: only supports one range (ex, ``bytes=1-2,6-10`` is not allowed).\n\n    See [0] for the details of the range header.\n\n    [0]: http://greenbytes.de/tech/webdav/draft-ietf-httpbis-p5-range-latest.html#byte.ranges\n    \"\"\"\n    unit, _, value = range_header.partition(\"=\")\n    unit, value = unit.strip(), value.strip()\n    if unit != \"bytes\":\n        return None\n    start_b, _, end_b = value.partition(\"-\")\n    try:\n        start = _int_or_none(start_b)\n        end = _int_or_none(end_b)\n    except ValueError:\n        return None\n    if end is not None:\n        if start is None:\n            if end != 0:\n                start = -end\n                end = None\n        else:\n            end += 1\n    return (start, end)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_content_range(start: Optional[int], end: Optional[int], total: int) -> str:\n    start = start or 0\n    end = (end or total) - 1\n    return \"bytes %s-%s/%s\" % (start, end, total)", "response": "Returns a suitable Content - Range header."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a form request body.", "response": "def parse_body_arguments(\n    content_type: str,\n    body: bytes,\n    arguments: Dict[str, List[bytes]],\n    files: Dict[str, List[HTTPFile]],\n    headers: HTTPHeaders = None,\n) -> None:\n    \"\"\"Parses a form request body.\n\n    Supports ``application/x-www-form-urlencoded`` and\n    ``multipart/form-data``.  The ``content_type`` parameter should be\n    a string and ``body`` should be a byte string.  The ``arguments``\n    and ``files`` parameters are dictionaries that will be updated\n    with the parsed contents.\n    \"\"\"\n    if content_type.startswith(\"application/x-www-form-urlencoded\"):\n        if headers and \"Content-Encoding\" in headers:\n            gen_log.warning(\n                \"Unsupported Content-Encoding: %s\", headers[\"Content-Encoding\"]\n            )\n            return\n        try:\n            uri_arguments = parse_qs_bytes(native_str(body), keep_blank_values=True)\n        except Exception as e:\n            gen_log.warning(\"Invalid x-www-form-urlencoded body: %s\", e)\n            uri_arguments = {}\n        for name, values in uri_arguments.items():\n            if values:\n                arguments.setdefault(name, []).extend(values)\n    elif content_type.startswith(\"multipart/form-data\"):\n        if headers and \"Content-Encoding\" in headers:\n            gen_log.warning(\n                \"Unsupported Content-Encoding: %s\", headers[\"Content-Encoding\"]\n            )\n            return\n        try:\n            fields = content_type.split(\";\")\n            for field in fields:\n                k, sep, v = field.strip().partition(\"=\")\n                if k == \"boundary\" and v:\n                    parse_multipart_form_data(utf8(v), body, arguments, files)\n                    break\n            else:\n                raise ValueError(\"multipart boundary not found\")\n        except Exception as e:\n            gen_log.warning(\"Invalid multipart/form-data: %s\", e)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_multipart_form_data(\n    boundary: bytes,\n    data: bytes,\n    arguments: Dict[str, List[bytes]],\n    files: Dict[str, List[HTTPFile]],\n) -> None:\n    \"\"\"Parses a ``multipart/form-data`` body.\n\n    The ``boundary`` and ``data`` parameters are both byte strings.\n    The dictionaries given in the arguments and files parameters\n    will be updated with the contents of the body.\n\n    .. versionchanged:: 5.1\n\n       Now recognizes non-ASCII filenames in RFC 2231/5987\n       (``filename*=``) format.\n    \"\"\"\n    # The standard allows for the boundary to be quoted in the header,\n    # although it's rare (it happens at least for google app engine\n    # xmpp).  I think we're also supposed to handle backslash-escapes\n    # here but I'll save that until we see a client that uses them\n    # in the wild.\n    if boundary.startswith(b'\"') and boundary.endswith(b'\"'):\n        boundary = boundary[1:-1]\n    final_boundary_index = data.rfind(b\"--\" + boundary + b\"--\")\n    if final_boundary_index == -1:\n        gen_log.warning(\"Invalid multipart/form-data: no final boundary\")\n        return\n    parts = data[:final_boundary_index].split(b\"--\" + boundary + b\"\\r\\n\")\n    for part in parts:\n        if not part:\n            continue\n        eoh = part.find(b\"\\r\\n\\r\\n\")\n        if eoh == -1:\n            gen_log.warning(\"multipart/form-data missing headers\")\n            continue\n        headers = HTTPHeaders.parse(part[:eoh].decode(\"utf-8\"))\n        disp_header = headers.get(\"Content-Disposition\", \"\")\n        disposition, disp_params = _parse_header(disp_header)\n        if disposition != \"form-data\" or not part.endswith(b\"\\r\\n\"):\n            gen_log.warning(\"Invalid multipart/form-data\")\n            continue\n        value = part[eoh + 4 : -2]\n        if not disp_params.get(\"name\"):\n            gen_log.warning(\"multipart/form-data value missing name\")\n            continue\n        name = disp_params[\"name\"]\n        if disp_params.get(\"filename\"):\n            ctype = headers.get(\"Content-Type\", \"application/unknown\")\n            files.setdefault(name, []).append(\n                HTTPFile(\n                    filename=disp_params[\"filename\"], body=value, content_type=ctype\n                )\n            )\n        else:\n            arguments.setdefault(name, []).append(value)", "response": "Parses a multipart - form - data response."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats a timestamp in the format used by HTTP.", "response": "def format_timestamp(\n    ts: Union[int, float, tuple, time.struct_time, datetime.datetime]\n) -> str:\n    \"\"\"Formats a timestamp in the format used by HTTP.\n\n    The argument may be a numeric timestamp as returned by `time.time`,\n    a time tuple as returned by `time.gmtime`, or a `datetime.datetime`\n    object.\n\n    >>> format_timestamp(1359312200)\n    'Sun, 27 Jan 2013 18:43:20 GMT'\n    \"\"\"\n    if isinstance(ts, (int, float)):\n        time_num = ts\n    elif isinstance(ts, (tuple, time.struct_time)):\n        time_num = calendar.timegm(ts)\n    elif isinstance(ts, datetime.datetime):\n        time_num = calendar.timegm(ts.utctimetuple())\n    else:\n        raise TypeError(\"unknown timestamp type: %r\" % ts)\n    return email.utils.formatdate(time_num, usegmt=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a HTTP 1. x request line.", "response": "def parse_request_start_line(line: str) -> RequestStartLine:\n    \"\"\"Returns a (method, path, version) tuple for an HTTP 1.x request line.\n\n    The response is a `collections.namedtuple`.\n\n    >>> parse_request_start_line(\"GET /foo HTTP/1.1\")\n    RequestStartLine(method='GET', path='/foo', version='HTTP/1.1')\n    \"\"\"\n    try:\n        method, path, version = line.split(\" \")\n    except ValueError:\n        # https://tools.ietf.org/html/rfc7230#section-3.1.1\n        # invalid request-line SHOULD respond with a 400 (Bad Request)\n        raise HTTPInputError(\"Malformed HTTP request line\")\n    if not re.match(r\"^HTTP/1\\.[0-9]$\", version):\n        raise HTTPInputError(\n            \"Malformed HTTP version in HTTP Request-Line: %r\" % version\n        )\n    return RequestStartLine(method, path, version)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a HTTP 1. x response line.", "response": "def parse_response_start_line(line: str) -> ResponseStartLine:\n    \"\"\"Returns a (version, code, reason) tuple for an HTTP 1.x response line.\n\n    The response is a `collections.namedtuple`.\n\n    >>> parse_response_start_line(\"HTTP/1.1 200 OK\")\n    ResponseStartLine(version='HTTP/1.1', code=200, reason='OK')\n    \"\"\"\n    line = native_str(line)\n    match = re.match(\"(HTTP/1.[0-9]) ([0-9]+) ([^\\r]*)\", line)\n    if not match:\n        raise HTTPInputError(\"Error parsing response start line\")\n    return ResponseStartLine(match.group(1), int(match.group(2)), match.group(3))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_header(line: str) -> Tuple[str, Dict[str, str]]:\n    parts = _parseparam(\";\" + line)\n    key = next(parts)\n    # decode_params treats first argument special, but we already stripped key\n    params = [(\"Dummy\", \"value\")]\n    for p in parts:\n        i = p.find(\"=\")\n        if i >= 0:\n            name = p[:i].strip().lower()\n            value = p[i + 1 :].strip()\n            params.append((name, native_str(value)))\n    decoded_params = email.utils.decode_params(params)\n    decoded_params.pop(0)  # get rid of the dummy again\n    pdict = {}\n    for name, decoded_value in decoded_params:\n        value = email.utils.collapse_rfc2231_value(decoded_value)\n        if len(value) >= 2 and value[0] == '\"' and value[-1] == '\"':\n            value = value[1:-1]\n        pdict[name] = value\n    return key, pdict", "response": "r Parse a Content - Type - like header."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _encode_header(key: str, pdict: Dict[str, str]) -> str:\n    if not pdict:\n        return key\n    out = [key]\n    # Sort the parameters just to make it easy to test.\n    for k, v in sorted(pdict.items()):\n        if v is None:\n            out.append(k)\n        else:\n            # TODO: quote if necessary.\n            out.append(\"%s=%s\" % (k, v))\n    return \"; \".join(out)", "response": "Encode the header for the given key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode_username_password(\n    username: Union[str, bytes], password: Union[str, bytes]\n) -> bytes:\n    \"\"\"Encodes a username/password pair in the format used by HTTP auth.\n\n    The return value is a byte string in the form ``username:password``.\n\n    .. versionadded:: 5.1\n    \"\"\"\n    if isinstance(username, unicode_type):\n        username = unicodedata.normalize(\"NFC\", username)\n    if isinstance(password, unicode_type):\n        password = unicodedata.normalize(\"NFC\", password)\n    return utf8(username) + b\":\" + utf8(password)", "response": "Encodes a username and password pair in the format used by HTTP auth.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit a netloc into a host and port tuple.", "response": "def split_host_and_port(netloc: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Returns ``(host, port)`` tuple from ``netloc``.\n\n    Returned ``port`` will be ``None`` if not present.\n\n    .. versionadded:: 4.1\n    \"\"\"\n    match = re.match(r\"^(.+):(\\d+)$\", netloc)\n    if match:\n        host = match.group(1)\n        port = int(match.group(2))  # type: Optional[int]\n    else:\n        host = netloc\n        port = None\n    return (host, port)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle double quotes and escaping in cookie values.", "response": "def _unquote_cookie(s: str) -> str:\n    \"\"\"Handle double quotes and escaping in cookie values.\n\n    This method is copied verbatim from the Python 3.5 standard\n    library (http.cookies._unquote) so we don't have to depend on\n    non-public interfaces.\n    \"\"\"\n    # If there aren't any doublequotes,\n    # then there can't be any special characters.  See RFC 2109.\n    if s is None or len(s) < 2:\n        return s\n    if s[0] != '\"' or s[-1] != '\"':\n        return s\n\n    # We have to assume that we must decode this string.\n    # Down to work.\n\n    # Remove the \"s\n    s = s[1:-1]\n\n    # Check for special sequences.  Examples:\n    #    \\012 --> \\n\n    #    \\\"   --> \"\n    #\n    i = 0\n    n = len(s)\n    res = []\n    while 0 <= i < n:\n        o_match = _OctalPatt.search(s, i)\n        q_match = _QuotePatt.search(s, i)\n        if not o_match and not q_match:  # Neither matched\n            res.append(s[i:])\n            break\n        # else:\n        j = k = -1\n        if o_match:\n            j = o_match.start(0)\n        if q_match:\n            k = q_match.start(0)\n        if q_match and (not o_match or k < j):  # QuotePatt matched\n            res.append(s[i:k])\n            res.append(s[k + 1])\n            i = k + 2\n        else:  # OctalPatt matched\n            res.append(s[i:j])\n            res.append(chr(int(s[j + 1 : j + 4], 8)))\n            i = j + 4\n    return _nulljoin(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_cookie(cookie: str) -> Dict[str, str]:\n    cookiedict = {}\n    for chunk in cookie.split(str(\";\")):\n        if str(\"=\") in chunk:\n            key, val = chunk.split(str(\"=\"), 1)\n        else:\n            # Assume an empty name per\n            # https://bugzilla.mozilla.org/show_bug.cgi?id=169091\n            key, val = str(\"\"), chunk\n        key, val = key.strip(), val.strip()\n        if key or val:\n            # unquote using Python's algorithm.\n            cookiedict[key] = _unquote_cookie(val)\n    return cookiedict", "response": "Parse a cookie string into a dict of name - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new value for the given key.", "response": "def add(self, name: str, value: str) -> None:\n        \"\"\"Adds a new value for the given key.\"\"\"\n        norm_name = _normalized_headers[name]\n        self._last_key = norm_name\n        if norm_name in self:\n            self._dict[norm_name] = (\n                native_str(self[norm_name]) + \",\" + native_str(value)\n            )\n            self._as_list[norm_name].append(value)\n        else:\n            self[norm_name] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_list(self, name: str) -> List[str]:\n        norm_name = _normalized_headers[name]\n        return self._as_list.get(norm_name, [])", "response": "Returns all values for the given header as a list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_all(self) -> Iterable[Tuple[str, str]]:\n        for name, values in self._as_list.items():\n            for value in values:\n                yield (name, value)", "response": "Returns an iterable of all ( name value ) pairs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_line(self, line: str) -> None:\n        if line[0].isspace():\n            # continuation of a multi-line header\n            if self._last_key is None:\n                raise HTTPInputError(\"first header line cannot start with whitespace\")\n            new_part = \" \" + line.lstrip()\n            self._as_list[self._last_key][-1] += new_part\n            self._dict[self._last_key] += new_part\n        else:\n            try:\n                name, value = line.split(\":\", 1)\n            except ValueError:\n                raise HTTPInputError(\"no colon in header line\")\n            self.add(name, value.strip())", "response": "Parse a single line of HTTP headers into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(cls, headers: str) -> \"HTTPHeaders\":\n        h = cls()\n        for line in _CRLF_RE.split(headers):\n            if line:\n                h.parse_line(line)\n        return h", "response": "Parses the HTTP headers text into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cookies(self) -> Dict[str, http.cookies.Morsel]:\n        if not hasattr(self, \"_cookies\"):\n            self._cookies = http.cookies.SimpleCookie()\n            if \"Cookie\" in self.headers:\n                try:\n                    parsed = parse_cookie(self.headers[\"Cookie\"])\n                except Exception:\n                    pass\n                else:\n                    for k, v in parsed.items():\n                        try:\n                            self._cookies[k] = v\n                        except Exception:\n                            # SimpleCookie imposes some restrictions on keys;\n                            # parse_cookie does not. Discard any cookies\n                            # with disallowed keys.\n                            pass\n        return self._cookies", "response": "A dictionary of http. cookies. Morsel objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef request_time(self) -> float:\n        if self._finish_time is None:\n            return time.time() - self._start_time\n        else:\n            return self._finish_time - self._start_time", "response": "Returns the amount of time it took for this request to execute."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the client s SSL certificate if any.", "response": "def get_ssl_certificate(\n        self, binary_form: bool = False\n    ) -> Union[None, Dict, bytes]:\n        \"\"\"Returns the client's SSL certificate, if any.\n\n        To use client certificates, the HTTPServer's\n        `ssl.SSLContext.verify_mode` field must be set, e.g.::\n\n            ssl_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n            ssl_ctx.load_cert_chain(\"foo.crt\", \"foo.key\")\n            ssl_ctx.load_verify_locations(\"cacerts.pem\")\n            ssl_ctx.verify_mode = ssl.CERT_REQUIRED\n            server = HTTPServer(app, ssl_options=ssl_ctx)\n\n        By default, the return value is a dictionary (or None, if no\n        client certificate is present).  If ``binary_form`` is true, a\n        DER-encoded form of the certificate is returned instead.  See\n        SSLSocket.getpeercert() in the standard library for more\n        details.\n        http://docs.python.org/library/ssl.html#sslsocket-objects\n        \"\"\"\n        try:\n            if self.connection is None:\n                return None\n            # TODO: add a method to HTTPConnection for this so it can work with HTTP/2\n            return self.connection.stream.socket.getpeercert(  # type: ignore\n                binary_form=binary_form\n            )\n        except SSLError:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate listening sockets bound to the given port and address.", "response": "def bind_sockets(\n    port: int,\n    address: str = None,\n    family: socket.AddressFamily = socket.AF_UNSPEC,\n    backlog: int = _DEFAULT_BACKLOG,\n    flags: int = None,\n    reuse_port: bool = False,\n) -> List[socket.socket]:\n    \"\"\"Creates listening sockets bound to the given port and address.\n\n    Returns a list of socket objects (multiple sockets are returned if\n    the given address maps to multiple IP addresses, which is most common\n    for mixed IPv4 and IPv6 use).\n\n    Address may be either an IP address or hostname.  If it's a hostname,\n    the server will listen on all IP addresses associated with the\n    name.  Address may be an empty string or None to listen on all\n    available interfaces.  Family may be set to either `socket.AF_INET`\n    or `socket.AF_INET6` to restrict to IPv4 or IPv6 addresses, otherwise\n    both will be used if available.\n\n    The ``backlog`` argument has the same meaning as for\n    `socket.listen() <socket.socket.listen>`.\n\n    ``flags`` is a bitmask of AI_* flags to `~socket.getaddrinfo`, like\n    ``socket.AI_PASSIVE | socket.AI_NUMERICHOST``.\n\n    ``reuse_port`` option sets ``SO_REUSEPORT`` option for every socket\n    in the list. If your platform doesn't support this option ValueError will\n    be raised.\n    \"\"\"\n    if reuse_port and not hasattr(socket, \"SO_REUSEPORT\"):\n        raise ValueError(\"the platform doesn't support SO_REUSEPORT\")\n\n    sockets = []\n    if address == \"\":\n        address = None\n    if not socket.has_ipv6 and family == socket.AF_UNSPEC:\n        # Python can be compiled with --disable-ipv6, which causes\n        # operations on AF_INET6 sockets to fail, but does not\n        # automatically exclude those results from getaddrinfo\n        # results.\n        # http://bugs.python.org/issue16208\n        family = socket.AF_INET\n    if flags is None:\n        flags = socket.AI_PASSIVE\n    bound_port = None\n    unique_addresses = set()  # type: set\n    for res in sorted(\n        socket.getaddrinfo(address, port, family, socket.SOCK_STREAM, 0, flags),\n        key=lambda x: x[0],\n    ):\n        if res in unique_addresses:\n            continue\n\n        unique_addresses.add(res)\n\n        af, socktype, proto, canonname, sockaddr = res\n        if (\n            sys.platform == \"darwin\"\n            and address == \"localhost\"\n            and af == socket.AF_INET6\n            and sockaddr[3] != 0\n        ):\n            # Mac OS X includes a link-local address fe80::1%lo0 in the\n            # getaddrinfo results for 'localhost'.  However, the firewall\n            # doesn't understand that this is a local address and will\n            # prompt for access (often repeatedly, due to an apparent\n            # bug in its ability to remember granting access to an\n            # application). Skip these addresses.\n            continue\n        try:\n            sock = socket.socket(af, socktype, proto)\n        except socket.error as e:\n            if errno_from_exception(e) == errno.EAFNOSUPPORT:\n                continue\n            raise\n        set_close_exec(sock.fileno())\n        if os.name != \"nt\":\n            try:\n                sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            except socket.error as e:\n                if errno_from_exception(e) != errno.ENOPROTOOPT:\n                    # Hurd doesn't support SO_REUSEADDR.\n                    raise\n        if reuse_port:\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n        if af == socket.AF_INET6:\n            # On linux, ipv6 sockets accept ipv4 too by default,\n            # but this makes it impossible to bind to both\n            # 0.0.0.0 in ipv4 and :: in ipv6.  On other systems,\n            # separate sockets *must* be used to listen for both ipv4\n            # and ipv6.  For consistency, always disable ipv4 on our\n            # ipv6 sockets and use a separate ipv4 socket when needed.\n            #\n            # Python 2.x on windows doesn't have IPPROTO_IPV6.\n            if hasattr(socket, \"IPPROTO_IPV6\"):\n                sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)\n\n        # automatic port allocation with port=None\n        # should bind on the same port on IPv4 and IPv6\n        host, requested_port = sockaddr[:2]\n        if requested_port == 0 and bound_port is not None:\n            sockaddr = tuple([host, bound_port] + list(sockaddr[2:]))\n\n        sock.setblocking(False)\n        sock.bind(sockaddr)\n        bound_port = sock.getsockname()[1]\n        sock.listen(backlog)\n        sockets.append(sock)\n    return sockets"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_accept_handler(\n    sock: socket.socket, callback: Callable[[socket.socket, Any], None]\n) -> Callable[[], None]:\n    \"\"\"Adds an `.IOLoop` event handler to accept new connections on ``sock``.\n\n    When a connection is accepted, ``callback(connection, address)`` will\n    be run (``connection`` is a socket object, and ``address`` is the\n    address of the other end of the connection).  Note that this signature\n    is different from the ``callback(fd, events)`` signature used for\n    `.IOLoop` handlers.\n\n    A callable is returned which, when called, will remove the `.IOLoop`\n    event handler and stop processing further incoming connections.\n\n    .. versionchanged:: 5.0\n       The ``io_loop`` argument (deprecated since version 4.1) has been removed.\n\n    .. versionchanged:: 5.0\n       A callable is returned (``None`` was returned before).\n    \"\"\"\n    io_loop = IOLoop.current()\n    removed = [False]\n\n    def accept_handler(fd: socket.socket, events: int) -> None:\n        # More connections may come in while we're handling callbacks;\n        # to prevent starvation of other tasks we must limit the number\n        # of connections we accept at a time.  Ideally we would accept\n        # up to the number of connections that were waiting when we\n        # entered this method, but this information is not available\n        # (and rearranging this method to call accept() as many times\n        # as possible before running any callbacks would have adverse\n        # effects on load balancing in multiprocess configurations).\n        # Instead, we use the (default) listen backlog as a rough\n        # heuristic for the number of connections we can reasonably\n        # accept at once.\n        for i in range(_DEFAULT_BACKLOG):\n            if removed[0]:\n                # The socket was probably closed\n                return\n            try:\n                connection, address = sock.accept()\n            except socket.error as e:\n                # _ERRNO_WOULDBLOCK indicate we have accepted every\n                # connection that is available.\n                if errno_from_exception(e) in _ERRNO_WOULDBLOCK:\n                    return\n                # ECONNABORTED indicates that there was a connection\n                # but it was closed while still in the accept queue.\n                # (observed on FreeBSD).\n                if errno_from_exception(e) == errno.ECONNABORTED:\n                    continue\n                raise\n            set_close_exec(connection.fileno())\n            callback(connection, address)\n\n    def remove_handler() -> None:\n        io_loop.remove_handler(sock)\n        removed[0] = True\n\n    io_loop.add_handler(sock, accept_handler, IOLoop.READ)\n    return remove_handler", "response": "Adds an event handler to accept new connections on sock."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_valid_ip(ip: str) -> bool:\n    if not ip or \"\\x00\" in ip:\n        # getaddrinfo resolves empty strings to localhost, and truncates\n        # on zero bytes.\n        return False\n    try:\n        res = socket.getaddrinfo(\n            ip, 0, socket.AF_UNSPEC, socket.SOCK_STREAM, 0, socket.AI_NUMERICHOST\n        )\n        return bool(res)\n    except socket.gaierror as e:\n        if e.args[0] == socket.EAI_NONAME:\n            return False\n        raise\n    return True", "response": "Returns True if the given string is a well - formed IP address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ssl_options_to_context(\n    ssl_options: Union[Dict[str, Any], ssl.SSLContext]\n) -> ssl.SSLContext:\n    \"\"\"Try to convert an ``ssl_options`` dictionary to an\n    `~ssl.SSLContext` object.\n\n    The ``ssl_options`` dictionary contains keywords to be passed to\n    `ssl.wrap_socket`.  In Python 2.7.9+, `ssl.SSLContext` objects can\n    be used instead.  This function converts the dict form to its\n    `~ssl.SSLContext` equivalent, and may be used when a component which\n    accepts both forms needs to upgrade to the `~ssl.SSLContext` version\n    to use features like SNI or NPN.\n    \"\"\"\n    if isinstance(ssl_options, ssl.SSLContext):\n        return ssl_options\n    assert isinstance(ssl_options, dict)\n    assert all(k in _SSL_CONTEXT_KEYWORDS for k in ssl_options), ssl_options\n    # Can't use create_default_context since this interface doesn't\n    # tell us client vs server.\n    context = ssl.SSLContext(ssl_options.get(\"ssl_version\", ssl.PROTOCOL_SSLv23))\n    if \"certfile\" in ssl_options:\n        context.load_cert_chain(\n            ssl_options[\"certfile\"], ssl_options.get(\"keyfile\", None)\n        )\n    if \"cert_reqs\" in ssl_options:\n        context.verify_mode = ssl_options[\"cert_reqs\"]\n    if \"ca_certs\" in ssl_options:\n        context.load_verify_locations(ssl_options[\"ca_certs\"])\n    if \"ciphers\" in ssl_options:\n        context.set_ciphers(ssl_options[\"ciphers\"])\n    if hasattr(ssl, \"OP_NO_COMPRESSION\"):\n        # Disable TLS compression to avoid CRIME and related attacks.\n        # This constant depends on openssl version 1.0.\n        # TODO: Do we need to do this ourselves or can we trust\n        # the defaults?\n        context.options |= ssl.OP_NO_COMPRESSION\n    return context", "response": "Try to convert an ssl. SSLContext dictionary to an ssl. SSLContext object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an ssl. SSLSocket wrapping the given socket.", "response": "def ssl_wrap_socket(\n    socket: socket.socket,\n    ssl_options: Union[Dict[str, Any], ssl.SSLContext],\n    server_hostname: str = None,\n    **kwargs: Any\n) -> ssl.SSLSocket:\n    \"\"\"Returns an ``ssl.SSLSocket`` wrapping the given socket.\n\n    ``ssl_options`` may be either an `ssl.SSLContext` object or a\n    dictionary (as accepted by `ssl_options_to_context`).  Additional\n    keyword arguments are passed to ``wrap_socket`` (either the\n    `~ssl.SSLContext` method or the `ssl` module function as\n    appropriate).\n    \"\"\"\n    context = ssl_options_to_context(ssl_options)\n    if ssl.HAS_SNI:\n        # In python 3.4, wrap_socket only accepts the server_hostname\n        # argument if HAS_SNI is true.\n        # TODO: add a unittest (python added server-side SNI support in 3.4)\n        # In the meantime it can be manually tested with\n        # python3 -m tornado.httpclient https://sni.velox.ch\n        return context.wrap_socket(socket, server_hostname=server_hostname, **kwargs)\n    else:\n        return context.wrap_socket(socket, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_on_executor(*args: Any, **kwargs: Any) -> Callable:\n    # Fully type-checking decorators is tricky, and this one is\n    # discouraged anyway so it doesn't have all the generic magic.\n    def run_on_executor_decorator(fn: Callable) -> Callable[..., Future]:\n        executor = kwargs.get(\"executor\", \"executor\")\n\n        @functools.wraps(fn)\n        def wrapper(self: Any, *args: Any, **kwargs: Any) -> Future:\n            async_future = Future()  # type: Future\n            conc_future = getattr(self, executor).submit(fn, self, *args, **kwargs)\n            chain_future(conc_future, async_future)\n            return async_future\n\n        return wrapper\n\n    if args and kwargs:\n        raise ValueError(\"cannot combine positional and keyword args\")\n    if len(args) == 1:\n        return run_on_executor_decorator(args[0])\n    elif len(args) != 0:\n        raise ValueError(\"expected 1 argument, got %d\", len(args))\n    return run_on_executor_decorator", "response": "Decorator to run a synchronous method asynchronously on an executor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchaining two futures together so that when one completes it will be copied to b.", "response": "def chain_future(a: \"Future[_T]\", b: \"Future[_T]\") -> None:\n    \"\"\"Chain two futures together so that when one completes, so does the other.\n\n    The result (success or failure) of ``a`` will be copied to ``b``, unless\n    ``b`` has already been completed or cancelled by the time ``a`` finishes.\n\n    .. versionchanged:: 5.0\n\n       Now accepts both Tornado/asyncio `Future` objects and\n       `concurrent.futures.Future`.\n\n    \"\"\"\n\n    def copy(future: \"Future[_T]\") -> None:\n        assert future is a\n        if b.done():\n            return\n        if hasattr(a, \"exc_info\") and a.exc_info() is not None:  # type: ignore\n            future_set_exc_info(b, a.exc_info())  # type: ignore\n        elif a.exception() is not None:\n            b.set_exception(a.exception())\n        else:\n            b.set_result(a.result())\n\n    if isinstance(a, Future):\n        future_add_done_callback(a, copy)\n    else:\n        # concurrent.futures.Future\n        from tornado.ioloop import IOLoop\n\n        IOLoop.current().add_future(a, copy)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef future_set_exception_unless_cancelled(\n    future: \"Union[futures.Future[_T], Future[_T]]\", exc: BaseException\n) -> None:\n    \"\"\"Set the given ``exc`` as the `Future`'s exception.\n\n    If the Future is already canceled, logs the exception instead. If\n    this logging is not desired, the caller should explicitly check\n    the state of the Future and call ``Future.set_exception`` instead of\n    this wrapper.\n\n    Avoids ``asyncio.InvalidStateError`` when calling ``set_exception()`` on\n    a cancelled `asyncio.Future`.\n\n    .. versionadded:: 6.0\n\n    \"\"\"\n    if not future.cancelled():\n        future.set_exception(exc)\n    else:\n        app_log.error(\"Exception after Future was cancelled\", exc_info=exc)", "response": "Sets the given exc as the Future s exception."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the given exc_info as the future s exception.", "response": "def future_set_exc_info(\n    future: \"Union[futures.Future[_T], Future[_T]]\",\n    exc_info: Tuple[\n        Optional[type], Optional[BaseException], Optional[types.TracebackType]\n    ],\n) -> None:\n    \"\"\"Set the given ``exc_info`` as the `Future`'s exception.\n\n    Understands both `asyncio.Future` and the extensions in older\n    versions of Tornado to enable better tracebacks on Python 2.\n\n    .. versionadded:: 5.0\n\n    .. versionchanged:: 6.0\n\n       If the future is already cancelled, this function is a no-op.\n       (previously ``asyncio.InvalidStateError`` would be raised)\n\n    \"\"\"\n    if exc_info[1] is None:\n        raise Exception(\"future_set_exc_info called with no exception\")\n    future_set_exception_unless_cancelled(future, exc_info[1])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\narrange to call callback when future is complete.", "response": "def future_add_done_callback(  # noqa: F811\n    future: \"Union[futures.Future[_T], Future[_T]]\", callback: Callable[..., None]\n) -> None:\n    \"\"\"Arrange to call ``callback`` when ``future`` is complete.\n\n    ``callback`` is invoked with one argument, the ``future``.\n\n    If ``future`` is already done, ``callback`` is invoked immediately.\n    This may differ from the behavior of ``Future.add_done_callback``,\n    which makes no such guarantee.\n\n    .. versionadded:: 5.0\n    \"\"\"\n    if future.done():\n        callback(future)\n    else:\n        future.add_done_callback(callback)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform whitespace in text according to mode.", "response": "def filter_whitespace(mode: str, text: str) -> str:\n    \"\"\"Transform whitespace in ``text`` according to ``mode``.\n\n    Available modes are:\n\n    * ``all``: Return all whitespace unmodified.\n    * ``single``: Collapse consecutive whitespace with a single whitespace\n      character, preserving newlines.\n    * ``oneline``: Collapse all runs of whitespace into a single space\n      character, removing all newlines in the process.\n\n    .. versionadded:: 4.3\n    \"\"\"\n    if mode == \"all\":\n        return text\n    elif mode == \"single\":\n        text = re.sub(r\"([\\t ]+)\", \" \", text)\n        text = re.sub(r\"(\\s*\\n\\s*)\", \"\\n\", text)\n        return text\n    elif mode == \"oneline\":\n        return re.sub(r\"(\\s+)\", \" \", text)\n    else:\n        raise Exception(\"invalid whitespace mode %s\" % mode)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate(self, **kwargs: Any) -> bytes:\n        namespace = {\n            \"escape\": escape.xhtml_escape,\n            \"xhtml_escape\": escape.xhtml_escape,\n            \"url_escape\": escape.url_escape,\n            \"json_encode\": escape.json_encode,\n            \"squeeze\": escape.squeeze,\n            \"linkify\": escape.linkify,\n            \"datetime\": datetime,\n            \"_tt_utf8\": escape.utf8,  # for internal use\n            \"_tt_string_types\": (unicode_type, bytes),\n            # __name__ and __loader__ allow the traceback mechanism to find\n            # the generated source code.\n            \"__name__\": self.name.replace(\".\", \"_\"),\n            \"__loader__\": ObjectDict(get_source=lambda name: self.code),\n        }\n        namespace.update(self.namespace)\n        namespace.update(kwargs)\n        exec_in(self.compiled, namespace)\n        execute = typing.cast(Callable[[], bytes], namespace[\"_tt_execute\"])\n        # Clear the traceback module's cache of source data now that\n        # we've generated a new template (mainly for this module's\n        # unittests, where different tests reuse the same name).\n        linecache.clearcache()\n        return execute()", "response": "Generate this template with the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun multiple asynchronous operations in parallel.", "response": "def multi(\n    children: Union[List[_Yieldable], Dict[Any, _Yieldable]],\n    quiet_exceptions: \"Union[Type[Exception], Tuple[Type[Exception], ...]]\" = (),\n) -> \"Union[Future[List], Future[Dict]]\":\n    \"\"\"Runs multiple asynchronous operations in parallel.\n\n    ``children`` may either be a list or a dict whose values are\n    yieldable objects. ``multi()`` returns a new yieldable\n    object that resolves to a parallel structure containing their\n    results. If ``children`` is a list, the result is a list of\n    results in the same order; if it is a dict, the result is a dict\n    with the same keys.\n\n    That is, ``results = yield multi(list_of_futures)`` is equivalent\n    to::\n\n        results = []\n        for future in list_of_futures:\n            results.append(yield future)\n\n    If any children raise exceptions, ``multi()`` will raise the first\n    one. All others will be logged, unless they are of types\n    contained in the ``quiet_exceptions`` argument.\n\n    In a ``yield``-based coroutine, it is not normally necessary to\n    call this function directly, since the coroutine runner will\n    do it automatically when a list or dict is yielded. However,\n    it is necessary in ``await``-based coroutines, or to pass\n    the ``quiet_exceptions`` argument.\n\n    This function is available under the names ``multi()`` and ``Multi()``\n    for historical reasons.\n\n    Cancelling a `.Future` returned by ``multi()`` does not cancel its\n    children. `asyncio.gather` is similar to ``multi()``, but it does\n    cancel its children.\n\n    .. versionchanged:: 4.2\n       If multiple yieldables fail, any exceptions after the first\n       (which is raised) will be logged. Added the ``quiet_exceptions``\n       argument to suppress this logging for selected exception types.\n\n    .. versionchanged:: 4.3\n       Replaced the class ``Multi`` and the function ``multi_future``\n       with a unified function ``multi``. Added support for yieldables\n       other than ``YieldPoint`` and `.Future`.\n\n    \"\"\"\n    return multi_future(children, quiet_exceptions=quiet_exceptions)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef multi_future(\n    children: Union[List[_Yieldable], Dict[Any, _Yieldable]],\n    quiet_exceptions: \"Union[Type[Exception], Tuple[Type[Exception], ...]]\" = (),\n) -> \"Union[Future[List], Future[Dict]]\":\n    \"\"\"Wait for multiple asynchronous futures in parallel.\n\n    Since Tornado 6.0, this function is exactly the same as `multi`.\n\n    .. versionadded:: 4.0\n\n    .. versionchanged:: 4.2\n       If multiple ``Futures`` fail, any exceptions after the first (which is\n       raised) will be logged. Added the ``quiet_exceptions``\n       argument to suppress this logging for selected exception types.\n\n    .. deprecated:: 4.3\n       Use `multi` instead.\n    \"\"\"\n    if isinstance(children, dict):\n        keys = list(children.keys())  # type: Optional[List]\n        children_seq = children.values()  # type: Iterable\n    else:\n        keys = None\n        children_seq = children\n    children_futs = list(map(convert_yielded, children_seq))\n    assert all(is_future(i) or isinstance(i, _NullFuture) for i in children_futs)\n    unfinished_children = set(children_futs)\n\n    future = _create_future()\n    if not children_futs:\n        future_set_result_unless_cancelled(future, {} if keys is not None else [])\n\n    def callback(fut: Future) -> None:\n        unfinished_children.remove(fut)\n        if not unfinished_children:\n            result_list = []\n            for f in children_futs:\n                try:\n                    result_list.append(f.result())\n                except Exception as e:\n                    if future.done():\n                        if not isinstance(e, quiet_exceptions):\n                            app_log.error(\n                                \"Multiple exceptions in yield list\", exc_info=True\n                            )\n                    else:\n                        future_set_exc_info(future, sys.exc_info())\n            if not future.done():\n                if keys is not None:\n                    future_set_result_unless_cancelled(\n                        future, dict(zip(keys, result_list))\n                    )\n                else:\n                    future_set_result_unless_cancelled(future, result_list)\n\n    listening = set()  # type: Set[Future]\n    for f in children_futs:\n        if f not in listening:\n            listening.add(f)\n            future_add_done_callback(f, callback)\n    return future", "response": "Wait for multiple asynchronous futures in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef maybe_future(x: Any) -> Future:\n    if is_future(x):\n        return x\n    else:\n        fut = _create_future()\n        fut.set_result(x)\n        return fut", "response": "Converts x into a Future."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps a coroutine that will be cancelled when the timeout expires.", "response": "def with_timeout(\n    timeout: Union[float, datetime.timedelta],\n    future: _Yieldable,\n    quiet_exceptions: \"Union[Type[Exception], Tuple[Type[Exception], ...]]\" = (),\n) -> Future:\n    \"\"\"Wraps a `.Future` (or other yieldable object) in a timeout.\n\n    Raises `tornado.util.TimeoutError` if the input future does not\n    complete before ``timeout``, which may be specified in any form\n    allowed by `.IOLoop.add_timeout` (i.e. a `datetime.timedelta` or\n    an absolute time relative to `.IOLoop.time`)\n\n    If the wrapped `.Future` fails after it has timed out, the exception\n    will be logged unless it is of a type contained in ``quiet_exceptions``\n    (which may be an exception type or a sequence of types).\n\n    The wrapped `.Future` is not canceled when the timeout expires,\n    permitting it to be reused. `asyncio.wait_for` is similar to this\n    function but it does cancel the wrapped `.Future` on timeout.\n\n    .. versionadded:: 4.0\n\n    .. versionchanged:: 4.1\n       Added the ``quiet_exceptions`` argument and the logging of unhandled\n       exceptions.\n\n    .. versionchanged:: 4.4\n       Added support for yieldable objects other than `.Future`.\n\n    \"\"\"\n    # It's tempting to optimize this by cancelling the input future on timeout\n    # instead of creating a new one, but A) we can't know if we are the only\n    # one waiting on the input future, so cancelling it might disrupt other\n    # callers and B) concurrent futures can only be cancelled while they are\n    # in the queue, so cancellation cannot reliably bound our waiting time.\n    future_converted = convert_yielded(future)\n    result = _create_future()\n    chain_future(future_converted, result)\n    io_loop = IOLoop.current()\n\n    def error_callback(future: Future) -> None:\n        try:\n            future.result()\n        except Exception as e:\n            if not isinstance(e, quiet_exceptions):\n                app_log.error(\n                    \"Exception in Future %r after timeout\", future, exc_info=True\n                )\n\n    def timeout_callback() -> None:\n        if not result.done():\n            result.set_exception(TimeoutError(\"Timeout\"))\n        # In case the wrapped future goes on to fail, log it.\n        future_add_done_callback(future_converted, error_callback)\n\n    timeout_handle = io_loop.add_timeout(timeout, timeout_callback)\n    if isinstance(future_converted, Future):\n        # We know this future will resolve on the IOLoop, so we don't\n        # need the extra thread-safety of IOLoop.add_future (and we also\n        # don't care about StackContext here.\n        future_add_done_callback(\n            future_converted, lambda future: io_loop.remove_timeout(timeout_handle)\n        )\n    else:\n        # concurrent.futures.Futures may resolve on any thread, so we\n        # need to route them back to the IOLoop.\n        io_loop.add_future(\n            future_converted, lambda future: io_loop.remove_timeout(timeout_handle)\n        )\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new Future that resolves after the given number of seconds.", "response": "def sleep(duration: float) -> \"Future[None]\":\n    \"\"\"Return a `.Future` that resolves after the given number of seconds.\n\n    When used with ``yield`` in a coroutine, this is a non-blocking\n    analogue to `time.sleep` (which should not be used in coroutines\n    because it is blocking)::\n\n        yield gen.sleep(0.5)\n\n    Note that calling this function on its own does nothing; you must\n    wait on the `.Future` it returns (usually by yielding it).\n\n    .. versionadded:: 4.1\n    \"\"\"\n    f = _create_future()\n    IOLoop.current().call_later(\n        duration, lambda: future_set_result_unless_cancelled(f, None)\n    )\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a yielded object into a tornado. Future.", "response": "def convert_yielded(yielded: _Yieldable) -> Future:\n    \"\"\"Convert a yielded object into a `.Future`.\n\n    The default implementation accepts lists, dictionaries, and\n    Futures. This has the side effect of starting any coroutines that\n    did not start themselves, similar to `asyncio.ensure_future`.\n\n    If the `~functools.singledispatch` library is available, this function\n    may be extended to support additional types. For example::\n\n        @convert_yielded.register(asyncio.Future)\n        def _(asyncio_future):\n            return tornado.platform.asyncio.to_tornado_future(asyncio_future)\n\n    .. versionadded:: 4.1\n\n    \"\"\"\n    if yielded is None or yielded is moment:\n        return moment\n    elif yielded is _null_future:\n        return _null_future\n    elif isinstance(yielded, (list, dict)):\n        return multi(yielded)  # type: ignore\n    elif is_future(yielded):\n        return typing.cast(Future, yielded)\n    elif isawaitable(yielded):\n        return _wrap_awaitable(yielded)  # type: ignore\n    else:\n        raise BadYieldError(\"yielded unknown object %r\" % (yielded,))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef next(self) -> Future:\n        self._running_future = Future()\n\n        if self._finished:\n            self._return_result(self._finished.popleft())\n\n        return self._running_future", "response": "Returns a Future that will yield the next available result."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self) -> None:\n        if self.running or self.finished:\n            return\n        try:\n            self.running = True\n            while True:\n                future = self.future\n                if future is None:\n                    raise Exception(\"No pending future\")\n                if not future.done():\n                    return\n                self.future = None\n                try:\n                    exc_info = None\n\n                    try:\n                        value = future.result()\n                    except Exception:\n                        exc_info = sys.exc_info()\n                    future = None\n\n                    if exc_info is not None:\n                        try:\n                            yielded = self.gen.throw(*exc_info)  # type: ignore\n                        finally:\n                            # Break up a reference to itself\n                            # for faster GC on CPython.\n                            exc_info = None\n                    else:\n                        yielded = self.gen.send(value)\n\n                except (StopIteration, Return) as e:\n                    self.finished = True\n                    self.future = _null_future\n                    future_set_result_unless_cancelled(\n                        self.result_future, _value_from_stopiteration(e)\n                    )\n                    self.result_future = None  # type: ignore\n                    return\n                except Exception:\n                    self.finished = True\n                    self.future = _null_future\n                    future_set_exc_info(self.result_future, sys.exc_info())\n                    self.result_future = None  # type: ignore\n                    return\n                if not self.handle_yield(yielded):\n                    return\n                yielded = None\n        finally:\n            self.running = False", "response": "Starts or resumes the generator until it reaches a yield point that is not ready."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending the given piece of data to the end of the internal buffer.", "response": "def append(self, data: Union[bytes, bytearray, memoryview]) -> None:\n        \"\"\"\n        Append the given piece of data (should be a buffer-compatible object).\n        \"\"\"\n        size = len(data)\n        if size > self._large_buf_threshold:\n            if not isinstance(data, memoryview):\n                data = memoryview(data)\n            self._buffers.append((True, data))\n        elif size > 0:\n            if self._buffers:\n                is_memview, b = self._buffers[-1]\n                new_buf = is_memview or len(b) >= self._large_buf_threshold\n            else:\n                new_buf = True\n            if new_buf:\n                self._buffers.append((False, bytearray(data)))\n            else:\n                b += data  # type: ignore\n\n        self._size += size"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef peek(self, size: int) -> memoryview:\n        assert size > 0\n        try:\n            is_memview, b = self._buffers[0]\n        except IndexError:\n            return memoryview(b\"\")\n\n        pos = self._first_pos\n        if is_memview:\n            return typing.cast(memoryview, b[pos : pos + size])\n        else:\n            return memoryview(b)[pos : pos + size]", "response": "Get a view of at most size bytes at the current position."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadvances the current buffer position by size bytes.", "response": "def advance(self, size: int) -> None:\n        \"\"\"\n        Advance the current buffer position by ``size`` bytes.\n        \"\"\"\n        assert 0 < size <= self._size\n        self._size -= size\n        pos = self._first_pos\n\n        buffers = self._buffers\n        while buffers and size > 0:\n            is_large, b = buffers[0]\n            b_remain = len(b) - size - pos\n            if b_remain <= 0:\n                buffers.popleft()\n                size -= len(b) - pos\n                pos = 0\n            elif is_large:\n                pos += size\n                size = 0\n            else:\n                # Amortized O(1) shrink for Python 2\n                pos += size\n                if len(b) <= 2 * pos:\n                    del typing.cast(bytearray, b)[:pos]\n                    pos = 0\n                size = 0\n\n        assert size == 0\n        self._first_pos = pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_until_regex(self, regex: bytes, max_bytes: int = None) -> Awaitable[bytes]:\n        future = self._start_read()\n        self._read_regex = re.compile(regex)\n        self._read_max_bytes = max_bytes\n        try:\n            self._try_inline_read()\n        except UnsatisfiableReadError as e:\n            # Handle this the same way as in _handle_events.\n            gen_log.info(\"Unsatisfiable read, closing connection: %s\" % e)\n            self.close(exc_info=e)\n            return future\n        except:\n            # Ensure that the future doesn't log an error because its\n            # failure was never examined.\n            future.add_done_callback(lambda f: f.exception())\n            raise\n        return future", "response": "Asynchronously read until we have matched the given regex."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_into(self, buf: bytearray, partial: bool = False) -> Awaitable[int]:\n        future = self._start_read()\n\n        # First copy data already in read buffer\n        available_bytes = self._read_buffer_size\n        n = len(buf)\n        if available_bytes >= n:\n            end = self._read_buffer_pos + n\n            buf[:] = memoryview(self._read_buffer)[self._read_buffer_pos : end]\n            del self._read_buffer[:end]\n            self._after_user_read_buffer = self._read_buffer\n        elif available_bytes > 0:\n            buf[:available_bytes] = memoryview(self._read_buffer)[\n                self._read_buffer_pos :\n            ]\n\n        # Set up the supplied buffer as our temporary read buffer.\n        # The original (if it had any data remaining) has been\n        # saved for later.\n        self._user_read_buffer = True\n        self._read_buffer = buf\n        self._read_buffer_pos = 0\n        self._read_buffer_size = available_bytes\n        self._read_bytes = n\n        self._read_partial = partial\n\n        try:\n            self._try_inline_read()\n        except:\n            future.add_done_callback(lambda f: f.exception())\n            raise\n        return future", "response": "Asynchronously read a number of bytes into a buffer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_until_close(self) -> Awaitable[bytes]:\n        future = self._start_read()\n        if self.closed():\n            self._finish_read(self._read_buffer_size, False)\n            return future\n        self._read_until_close = True\n        try:\n            self._try_inline_read()\n        except:\n            future.add_done_callback(lambda f: f.exception())\n            raise\n        return future", "response": "Asynchronously reads all data from the socket until it is closed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_close_callback(self, callback: Optional[Callable[[], None]]) -> None:\n        self._close_callback = callback\n        self._maybe_add_error_listener()", "response": "Call the given callback when the stream is closed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose the internal state of the internal state of the object.", "response": "def close(\n        self,\n        exc_info: Union[\n            None,\n            bool,\n            BaseException,\n            Tuple[\n                \"Optional[Type[BaseException]]\",\n                Optional[BaseException],\n                Optional[TracebackType],\n            ],\n        ] = False,\n    ) -> None:\n        \"\"\"Close this stream.\n\n        If ``exc_info`` is true, set the ``error`` attribute to the current\n        exception from `sys.exc_info` (or if ``exc_info`` is a tuple,\n        use that instead of `sys.exc_info`).\n        \"\"\"\n        if not self.closed():\n            if exc_info:\n                if isinstance(exc_info, tuple):\n                    self.error = exc_info[1]\n                elif isinstance(exc_info, BaseException):\n                    self.error = exc_info\n                else:\n                    exc_info = sys.exc_info()\n                    if any(exc_info):\n                        self.error = exc_info[1]\n            if self._read_until_close:\n                self._read_until_close = False\n                self._finish_read(self._read_buffer_size, False)\n            if self._state is not None:\n                self.io_loop.remove_handler(self.fileno())\n                self._state = None\n            self.close_fd()\n            self._closed = True\n        self._signal_closed()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntries to complete the current read operation from buffered data.", "response": "def _try_inline_read(self) -> None:\n        \"\"\"Attempt to complete the current read operation from buffered data.\n\n        If the read can be completed without blocking, schedules the\n        read callback on the next IOLoop iteration; otherwise starts\n        listening for reads on the socket.\n        \"\"\"\n        # See if we've already got the data from a previous read\n        pos = self._find_read_pos()\n        if pos is not None:\n            self._read_from_buffer(pos)\n            return\n        self._check_closed()\n        pos = self._read_to_buffer_loop()\n        if pos is not None:\n            self._read_from_buffer(pos)\n            return\n        # We couldn't satisfy the read inline, so make sure we're\n        # listening for new data unless the stream is closed.\n        if not self.closed():\n            self._add_io_state(ioloop.IOLoop.READ)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_to_buffer(self) -> Optional[int]:\n        try:\n            while True:\n                try:\n                    if self._user_read_buffer:\n                        buf = memoryview(self._read_buffer)[\n                            self._read_buffer_size :\n                        ]  # type: Union[memoryview, bytearray]\n                    else:\n                        buf = bytearray(self.read_chunk_size)\n                    bytes_read = self.read_from_fd(buf)\n                except (socket.error, IOError, OSError) as e:\n                    if errno_from_exception(e) == errno.EINTR:\n                        continue\n                    # ssl.SSLError is a subclass of socket.error\n                    if self._is_connreset(e):\n                        # Treat ECONNRESET as a connection close rather than\n                        # an error to minimize log spam  (the exception will\n                        # be available on self.error for apps that care).\n                        self.close(exc_info=e)\n                        return None\n                    self.close(exc_info=e)\n                    raise\n                break\n            if bytes_read is None:\n                return 0\n            elif bytes_read == 0:\n                self.close()\n                return 0\n            if not self._user_read_buffer:\n                self._read_buffer += memoryview(buf)[:bytes_read]\n            self._read_buffer_size += bytes_read\n        finally:\n            # Break the reference to buf so we don't waste a chunk's worth of\n            # memory in case an exception hangs on to our stack frame.\n            del buf\n        if self._read_buffer_size > self.max_buffer_size:\n            gen_log.error(\"Reached maximum read buffer size\")\n            self.close()\n            raise StreamBufferFullError(\"Reached maximum read buffer size\")\n        return bytes_read", "response": "Reads from the socket and appends the result to the read buffer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_from_buffer(self, pos: int) -> None:\n        self._read_bytes = self._read_delimiter = self._read_regex = None\n        self._read_partial = False\n        self._finish_read(pos, False)", "response": "Attempts to complete the currently - pending read from the buffer."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nattempt to find a position in the read buffer that satisfies the currently - pending read. Returns None if the current read cannot be satisfied.", "response": "def _find_read_pos(self) -> Optional[int]:\n        \"\"\"Attempts to find a position in the read buffer that satisfies\n        the currently-pending read.\n\n        Returns a position in the buffer if the current read can be satisfied,\n        or None if it cannot.\n        \"\"\"\n        if self._read_bytes is not None and (\n            self._read_buffer_size >= self._read_bytes\n            or (self._read_partial and self._read_buffer_size > 0)\n        ):\n            num_bytes = min(self._read_bytes, self._read_buffer_size)\n            return num_bytes\n        elif self._read_delimiter is not None:\n            # Multi-byte delimiters (e.g. '\\r\\n') may straddle two\n            # chunks in the read buffer, so we can't easily find them\n            # without collapsing the buffer.  However, since protocols\n            # using delimited reads (as opposed to reads of a known\n            # length) tend to be \"line\" oriented, the delimiter is likely\n            # to be in the first few chunks.  Merge the buffer gradually\n            # since large merges are relatively expensive and get undone in\n            # _consume().\n            if self._read_buffer:\n                loc = self._read_buffer.find(\n                    self._read_delimiter, self._read_buffer_pos\n                )\n                if loc != -1:\n                    loc -= self._read_buffer_pos\n                    delimiter_len = len(self._read_delimiter)\n                    self._check_max_bytes(self._read_delimiter, loc + delimiter_len)\n                    return loc + delimiter_len\n                self._check_max_bytes(self._read_delimiter, self._read_buffer_size)\n        elif self._read_regex is not None:\n            if self._read_buffer:\n                m = self._read_regex.search(self._read_buffer, self._read_buffer_pos)\n                if m is not None:\n                    loc = m.end() - self._read_buffer_pos\n                    self._check_max_bytes(self._read_regex, loc)\n                    return loc\n                self._check_max_bytes(self._read_regex, self._read_buffer_size)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_io_state(self, state: int) -> None:\n        if self.closed():\n            # connection has been closed, so there can be no future events\n            return\n        if self._state is None:\n            self._state = ioloop.IOLoop.ERROR | state\n            self.io_loop.add_handler(self.fileno(), self._handle_events, self._state)\n        elif not self._state & state:\n            self._state = self._state | state\n            self.io_loop.update_handler(self.fileno(), self._state)", "response": "Add the given state to our event handler."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if exc is an ECONNRESET.", "response": "def _is_connreset(self, exc: BaseException) -> bool:\n        \"\"\"Return ``True`` if exc is ECONNRESET or equivalent.\n\n        May be overridden in subclasses.\n        \"\"\"\n        return (\n            isinstance(exc, (socket.error, IOError))\n            and errno_from_exception(exc) in _ERRNO_CONNRESET\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect the socket to a remote address without blocking.", "response": "def connect(\n        self: _IOStreamType, address: tuple, server_hostname: str = None\n    ) -> \"Future[_IOStreamType]\":\n        \"\"\"Connects the socket to a remote address without blocking.\n\n        May only be called if the socket passed to the constructor was\n        not previously connected.  The address parameter is in the\n        same format as for `socket.connect <socket.socket.connect>` for\n        the type of socket passed to the IOStream constructor,\n        e.g. an ``(ip, port)`` tuple.  Hostnames are accepted here,\n        but will be resolved synchronously and block the IOLoop.\n        If you have a hostname instead of an IP address, the `.TCPClient`\n        class is recommended instead of calling this method directly.\n        `.TCPClient` will do asynchronous DNS resolution and handle\n        both IPv4 and IPv6.\n\n        If ``callback`` is specified, it will be called with no\n        arguments when the connection is completed; if not this method\n        returns a `.Future` (whose result after a successful\n        connection will be the stream itself).\n\n        In SSL mode, the ``server_hostname`` parameter will be used\n        for certificate validation (unless disabled in the\n        ``ssl_options``) and SNI (if supported; requires Python\n        2.7.9+).\n\n        Note that it is safe to call `IOStream.write\n        <BaseIOStream.write>` while the connection is pending, in\n        which case the data will be written as soon as the connection\n        is ready.  Calling `IOStream` read methods before the socket is\n        connected works on some platforms but is non-portable.\n\n        .. versionchanged:: 4.0\n            If no callback is given, returns a `.Future`.\n\n        .. versionchanged:: 4.2\n           SSL certificates are validated by default; pass\n           ``ssl_options=dict(cert_reqs=ssl.CERT_NONE)`` or a\n           suitably-configured `ssl.SSLContext` to the\n           `SSLIOStream` constructor to disable.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned\n           `.Future` instead.\n\n        \"\"\"\n        self._connecting = True\n        future = Future()  # type: Future[_IOStreamType]\n        self._connect_future = typing.cast(\"Future[IOStream]\", future)\n        try:\n            self.socket.connect(address)\n        except socket.error as e:\n            # In non-blocking mode we expect connect() to raise an\n            # exception with EINPROGRESS or EWOULDBLOCK.\n            #\n            # On freebsd, other errors such as ECONNREFUSED may be\n            # returned immediately when attempting to connect to\n            # localhost, so handle them the same way as an error\n            # reported later in _handle_connect.\n            if (\n                errno_from_exception(e) not in _ERRNO_INPROGRESS\n                and errno_from_exception(e) not in _ERRNO_WOULDBLOCK\n            ):\n                if future is None:\n                    gen_log.warning(\n                        \"Connect error on fd %s: %s\", self.socket.fileno(), e\n                    )\n                self.close(exc_info=e)\n                return future\n        self._add_io_state(self.io_loop.WRITE)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_tls(\n        self,\n        server_side: bool,\n        ssl_options: Union[Dict[str, Any], ssl.SSLContext] = None,\n        server_hostname: str = None,\n    ) -> Awaitable[\"SSLIOStream\"]:\n        \"\"\"Convert this `IOStream` to an `SSLIOStream`.\n\n        This enables protocols that begin in clear-text mode and\n        switch to SSL after some initial negotiation (such as the\n        ``STARTTLS`` extension to SMTP and IMAP).\n\n        This method cannot be used if there are outstanding reads\n        or writes on the stream, or if there is any data in the\n        IOStream's buffer (data in the operating system's socket\n        buffer is allowed).  This means it must generally be used\n        immediately after reading or writing the last clear-text\n        data.  It can also be used immediately after connecting,\n        before any reads or writes.\n\n        The ``ssl_options`` argument may be either an `ssl.SSLContext`\n        object or a dictionary of keyword arguments for the\n        `ssl.wrap_socket` function.  The ``server_hostname`` argument\n        will be used for certificate validation unless disabled\n        in the ``ssl_options``.\n\n        This method returns a `.Future` whose result is the new\n        `SSLIOStream`.  After this method has been called,\n        any other operation on the original stream is undefined.\n\n        If a close callback is defined on this stream, it will be\n        transferred to the new stream.\n\n        .. versionadded:: 4.0\n\n        .. versionchanged:: 4.2\n           SSL certificates are validated by default; pass\n           ``ssl_options=dict(cert_reqs=ssl.CERT_NONE)`` or a\n           suitably-configured `ssl.SSLContext` to disable.\n        \"\"\"\n        if (\n            self._read_future\n            or self._write_futures\n            or self._connect_future\n            or self._closed\n            or self._read_buffer\n            or self._write_buffer\n        ):\n            raise ValueError(\"IOStream is not idle; cannot convert to SSL\")\n        if ssl_options is None:\n            if server_side:\n                ssl_options = _server_ssl_defaults\n            else:\n                ssl_options = _client_ssl_defaults\n\n        socket = self.socket\n        self.io_loop.remove_handler(socket)\n        self.socket = None  # type: ignore\n        socket = ssl_wrap_socket(\n            socket,\n            ssl_options,\n            server_hostname=server_hostname,\n            server_side=server_side,\n            do_handshake_on_connect=False,\n        )\n        orig_close_callback = self._close_callback\n        self._close_callback = None\n\n        future = Future()  # type: Future[SSLIOStream]\n        ssl_stream = SSLIOStream(socket, ssl_options=ssl_options)\n        ssl_stream.set_close_callback(orig_close_callback)\n        ssl_stream._ssl_connect_future = future\n        ssl_stream.max_buffer_size = self.max_buffer_size\n        ssl_stream.read_chunk_size = self.read_chunk_size\n        return future", "response": "Convert this IOStream to an SSLIOStream."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if peercert is valid according to the configured SSL certificate validation mode and hostname.", "response": "def _verify_cert(self, peercert: Any) -> bool:\n        \"\"\"Returns ``True`` if peercert is valid according to the configured\n        validation mode and hostname.\n\n        The ssl handshake already tested the certificate for a valid\n        CA signature; the only thing that remains is to check\n        the hostname.\n        \"\"\"\n        if isinstance(self._ssl_options, dict):\n            verify_mode = self._ssl_options.get(\"cert_reqs\", ssl.CERT_NONE)\n        elif isinstance(self._ssl_options, ssl.SSLContext):\n            verify_mode = self._ssl_options.verify_mode\n        assert verify_mode in (ssl.CERT_NONE, ssl.CERT_REQUIRED, ssl.CERT_OPTIONAL)\n        if verify_mode == ssl.CERT_NONE or self._server_hostname is None:\n            return True\n        cert = self.socket.getpeercert()\n        if cert is None and verify_mode == ssl.CERT_REQUIRED:\n            gen_log.warning(\"No SSL certificate given\")\n            return False\n        try:\n            ssl.match_hostname(peercert, self._server_hostname)\n        except ssl.CertificateError as e:\n            gen_log.warning(\"Invalid SSL certificate: %s\" % e)\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting for the SSL handshake to complete.", "response": "def wait_for_handshake(self) -> \"Future[SSLIOStream]\":\n        \"\"\"Wait for the initial SSL handshake to complete.\n\n        If a ``callback`` is given, it will be called with no\n        arguments once the handshake is complete; otherwise this\n        method returns a `.Future` which will resolve to the\n        stream itself after the handshake is complete.\n\n        Once the handshake is complete, information such as\n        the peer's certificate and NPN/ALPN selections may be\n        accessed on ``self.socket``.\n\n        This method is intended for use on server-side streams\n        or after using `IOStream.start_tls`; it should not be used\n        with `IOStream.connect` (which already waits for the\n        handshake to complete). It may only be called once per stream.\n\n        .. versionadded:: 4.2\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned\n           `.Future` instead.\n\n        \"\"\"\n        if self._ssl_connect_future is not None:\n            raise RuntimeError(\"Already waiting\")\n        future = self._ssl_connect_future = Future()\n        if not self._ssl_accepting:\n            self._finish_ssl_connect()\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef enable_pretty_logging(options: Any = None, logger: logging.Logger = None) -> None:\n    if options is None:\n        import tornado.options\n\n        options = tornado.options.options\n    if options.logging is None or options.logging.lower() == \"none\":\n        return\n    if logger is None:\n        logger = logging.getLogger()\n    logger.setLevel(getattr(logging, options.logging.upper()))\n    if options.log_file_prefix:\n        rotate_mode = options.log_rotate_mode\n        if rotate_mode == \"size\":\n            channel = logging.handlers.RotatingFileHandler(\n                filename=options.log_file_prefix,\n                maxBytes=options.log_file_max_size,\n                backupCount=options.log_file_num_backups,\n                encoding=\"utf-8\",\n            )  # type: logging.Handler\n        elif rotate_mode == \"time\":\n            channel = logging.handlers.TimedRotatingFileHandler(\n                filename=options.log_file_prefix,\n                when=options.log_rotate_when,\n                interval=options.log_rotate_interval,\n                backupCount=options.log_file_num_backups,\n                encoding=\"utf-8\",\n            )\n        else:\n            error_message = (\n                \"The value of log_rotate_mode option should be \"\n                + '\"size\" or \"time\", not \"%s\".' % rotate_mode\n            )\n            raise ValueError(error_message)\n        channel.setFormatter(LogFormatter(color=False))\n        logger.addHandler(channel)\n\n    if options.log_to_stderr or (options.log_to_stderr is None and not logger.handlers):\n        # Set up color if we are in a tty and curses is installed\n        channel = logging.StreamHandler()\n        channel.setFormatter(LogFormatter())\n        logger.addHandler(channel)", "response": "Turns on formatted logging output as configured."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndefines logging - related flags to options instance.", "response": "def define_logging_options(options: Any = None) -> None:\n    \"\"\"Add logging-related flags to ``options``.\n\n    These options are present automatically on the default options instance;\n    this method is only necessary if you have created your own `.OptionParser`.\n\n    .. versionadded:: 4.2\n        This function existed in prior versions but was broken and undocumented until 4.2.\n    \"\"\"\n    if options is None:\n        # late import to prevent cycle\n        import tornado.options\n\n        options = tornado.options.options\n    options.define(\n        \"logging\",\n        default=\"info\",\n        help=(\n            \"Set the Python log level. If 'none', tornado won't touch the \"\n            \"logging configuration.\"\n        ),\n        metavar=\"debug|info|warning|error|none\",\n    )\n    options.define(\n        \"log_to_stderr\",\n        type=bool,\n        default=None,\n        help=(\n            \"Send log output to stderr (colorized if possible). \"\n            \"By default use stderr if --log_file_prefix is not set and \"\n            \"no other logging is configured.\"\n        ),\n    )\n    options.define(\n        \"log_file_prefix\",\n        type=str,\n        default=None,\n        metavar=\"PATH\",\n        help=(\n            \"Path prefix for log files. \"\n            \"Note that if you are running multiple tornado processes, \"\n            \"log_file_prefix must be different for each of them (e.g. \"\n            \"include the port number)\"\n        ),\n    )\n    options.define(\n        \"log_file_max_size\",\n        type=int,\n        default=100 * 1000 * 1000,\n        help=\"max size of log files before rollover\",\n    )\n    options.define(\n        \"log_file_num_backups\", type=int, default=10, help=\"number of log files to keep\"\n    )\n\n    options.define(\n        \"log_rotate_when\",\n        type=str,\n        default=\"midnight\",\n        help=(\n            \"specify the type of TimedRotatingFileHandler interval \"\n            \"other options:('S', 'M', 'H', 'D', 'W0'-'W6')\"\n        ),\n    )\n    options.define(\n        \"log_rotate_interval\",\n        type=int,\n        default=1,\n        help=\"The interval value of timed rotating\",\n    )\n\n    options.define(\n        \"log_rotate_mode\",\n        type=str,\n        default=\"size\",\n        help=\"The mode of rotating files(time or size)\",\n    )\n\n    options.add_parse_callback(lambda: enable_pretty_logging(options))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initialize(  # type: ignore\n        self,\n        max_clients: int = 10,\n        hostname_mapping: Dict[str, str] = None,\n        max_buffer_size: int = 104857600,\n        resolver: Resolver = None,\n        defaults: Dict[str, Any] = None,\n        max_header_size: int = None,\n        max_body_size: int = None,\n    ) -> None:\n        \"\"\"Creates a AsyncHTTPClient.\n\n        Only a single AsyncHTTPClient instance exists per IOLoop\n        in order to provide limitations on the number of pending connections.\n        ``force_instance=True`` may be used to suppress this behavior.\n\n        Note that because of this implicit reuse, unless ``force_instance``\n        is used, only the first call to the constructor actually uses\n        its arguments. It is recommended to use the ``configure`` method\n        instead of the constructor to ensure that arguments take effect.\n\n        ``max_clients`` is the number of concurrent requests that can be\n        in progress; when this limit is reached additional requests will be\n        queued. Note that time spent waiting in this queue still counts\n        against the ``request_timeout``.\n\n        ``hostname_mapping`` is a dictionary mapping hostnames to IP addresses.\n        It can be used to make local DNS changes when modifying system-wide\n        settings like ``/etc/hosts`` is not possible or desirable (e.g. in\n        unittests).\n\n        ``max_buffer_size`` (default 100MB) is the number of bytes\n        that can be read into memory at once. ``max_body_size``\n        (defaults to ``max_buffer_size``) is the largest response body\n        that the client will accept.  Without a\n        ``streaming_callback``, the smaller of these two limits\n        applies; with a ``streaming_callback`` only ``max_body_size``\n        does.\n\n        .. versionchanged:: 4.2\n           Added the ``max_body_size`` argument.\n        \"\"\"\n        super(SimpleAsyncHTTPClient, self).initialize(defaults=defaults)\n        self.max_clients = max_clients\n        self.queue = (\n            collections.deque()\n        )  # type: Deque[Tuple[object, HTTPRequest, Callable[[HTTPResponse], None]]]\n        self.active = (\n            {}\n        )  # type: Dict[object, Tuple[HTTPRequest, Callable[[HTTPResponse], None]]]\n        self.waiting = (\n            {}\n        )  # type: Dict[object, Tuple[HTTPRequest, Callable[[HTTPResponse], None], object]]\n        self.max_buffer_size = max_buffer_size\n        self.max_header_size = max_header_size\n        self.max_body_size = max_body_size\n        # TCPClient could create a Resolver for us, but we have to do it\n        # ourselves to support hostname_mapping.\n        if resolver:\n            self.resolver = resolver\n            self.own_resolver = False\n        else:\n            self.resolver = Resolver()\n            self.own_resolver = True\n        if hostname_mapping is not None:\n            self.resolver = OverrideResolver(\n                resolver=self.resolver, mapping=hostname_mapping\n            )\n        self.tcp_client = TCPClient(resolver=self.resolver)", "response": "Initializes the AsyncHTTPClient instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _on_timeout(self, key: object, info: str = None) -> None:\n        request, callback, timeout_handle = self.waiting[key]\n        self.queue.remove((key, request, callback))\n\n        error_message = \"Timeout {0}\".format(info) if info else \"Timeout\"\n        timeout_response = HTTPResponse(\n            request,\n            599,\n            error=HTTPTimeoutError(error_message),\n            request_time=self.io_loop.time() - request.start_time,\n        )\n        self.io_loop.add_callback(callback, timeout_response)\n        del self.waiting[key]", "response": "Timeout callback of request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _oauth10a_signature(\n    consumer_token: Dict[str, Any],\n    method: str,\n    url: str,\n    parameters: Dict[str, Any] = {},\n    token: Dict[str, Any] = None,\n) -> bytes:\n    \"\"\"Calculates the HMAC-SHA1 OAuth 1.0a signature for the given request.\n\n    See http://oauth.net/core/1.0a/#signing_process\n    \"\"\"\n    parts = urllib.parse.urlparse(url)\n    scheme, netloc, path = parts[:3]\n    normalized_url = scheme.lower() + \"://\" + netloc.lower() + path\n\n    base_elems = []\n    base_elems.append(method.upper())\n    base_elems.append(normalized_url)\n    base_elems.append(\n        \"&\".join(\n            \"%s=%s\" % (k, _oauth_escape(str(v))) for k, v in sorted(parameters.items())\n        )\n    )\n\n    base_string = \"&\".join(_oauth_escape(e) for e in base_elems)\n    key_elems = [escape.utf8(urllib.parse.quote(consumer_token[\"secret\"], safe=\"~\"))]\n    key_elems.append(\n        escape.utf8(urllib.parse.quote(token[\"secret\"], safe=\"~\") if token else \"\")\n    )\n    key = b\"&\".join(key_elems)\n\n    hash = hmac.new(key, escape.utf8(base_string), hashlib.sha1)\n    return binascii.b2a_base64(hash.digest())[:-1]", "response": "Calculates the HMAC - SHA1 signature for the given request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nredirect to the authentication URL for this service.", "response": "def authenticate_redirect(\n        self,\n        callback_uri: str = None,\n        ax_attrs: List[str] = [\"name\", \"email\", \"language\", \"username\"],\n    ) -> None:\n        \"\"\"Redirects to the authentication URL for this service.\n\n        After authentication, the service will redirect back to the given\n        callback URI with additional parameters including ``openid.mode``.\n\n        We request the given attributes for the authenticated user by\n        default (name, email, language, and username). If you don't need\n        all those attributes for your app, you can request fewer with\n        the ax_attrs keyword argument.\n\n        .. versionchanged:: 6.0\n\n            The ``callback`` argument was removed and this method no\n            longer returns an awaitable object. It is now an ordinary\n            synchronous function.\n        \"\"\"\n        handler = cast(RequestHandler, self)\n        callback_uri = callback_uri or handler.request.uri\n        assert callback_uri is not None\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)\n        endpoint = self._OPENID_ENDPOINT  # type: ignore\n        handler.redirect(endpoint + \"?\" + urllib.parse.urlencode(args))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches the authenticated user data upon redirect.", "response": "async def get_authenticated_user(\n        self, http_client: httpclient.AsyncHTTPClient = None\n    ) -> Dict[str, Any]:\n        \"\"\"Fetches the authenticated user data upon redirect.\n\n        This method should be called by the handler that receives the\n        redirect from the `authenticate_redirect()` method (which is\n        often the same as the one that calls it; in that case you would\n        call `get_authenticated_user` if the ``openid.mode`` parameter\n        is present and `authenticate_redirect` if it is not).\n\n        The result of this method will generally be used to set a cookie.\n\n        .. versionchanged:: 6.0\n\n            The ``callback`` argument was removed. Use the returned\n            awaitable object instead.\n        \"\"\"\n        handler = cast(RequestHandler, self)\n        # Verify the OpenID response via direct request to the OP\n        args = dict(\n            (k, v[-1]) for k, v in handler.request.arguments.items()\n        )  # type: Dict[str, Union[str, bytes]]\n        args[\"openid.mode\"] = u\"check_authentication\"\n        url = self._OPENID_ENDPOINT  # type: ignore\n        if http_client is None:\n            http_client = self.get_auth_http_client()\n        resp = await http_client.fetch(\n            url, method=\"POST\", body=urllib.parse.urlencode(args)\n        )\n        return self._on_authentication_verified(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def authorize_redirect(\n        self,\n        callback_uri: str = None,\n        extra_params: Dict[str, Any] = None,\n        http_client: httpclient.AsyncHTTPClient = None,\n    ) -> None:\n        \"\"\"Redirects the user to obtain OAuth authorization for this service.\n\n        The ``callback_uri`` may be omitted if you have previously\n        registered a callback URI with the third-party service. For\n        some services, you must use a previously-registered callback\n        URI and cannot specify a callback via this method.\n\n        This method sets a cookie called ``_oauth_request_token`` which is\n        subsequently used (and cleared) in `get_authenticated_user` for\n        security purposes.\n\n        This method is asynchronous and must be called with ``await``\n        or ``yield`` (This is different from other ``auth*_redirect``\n        methods defined in this module). It calls\n        `.RequestHandler.finish` for you so you should not write any\n        other response after it returns.\n\n        .. versionchanged:: 3.1\n           Now returns a `.Future` and takes an optional callback, for\n           compatibility with `.gen.coroutine`.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned\n           awaitable object instead.\n\n        \"\"\"\n        if callback_uri and getattr(self, \"_OAUTH_NO_CALLBACKS\", False):\n            raise Exception(\"This service does not support oauth_callback\")\n        if http_client is None:\n            http_client = self.get_auth_http_client()\n        assert http_client is not None\n        if getattr(self, \"_OAUTH_VERSION\", \"1.0a\") == \"1.0a\":\n            response = await http_client.fetch(\n                self._oauth_request_token_url(\n                    callback_uri=callback_uri, extra_params=extra_params\n                )\n            )\n        else:\n            response = await http_client.fetch(self._oauth_request_token_url())\n        url = self._OAUTH_AUTHORIZE_URL  # type: ignore\n        self._on_request_token(url, callback_uri, response)", "response": "Redirects the user to obtain OAuth authorization for this service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def get_authenticated_user(\n        self, http_client: httpclient.AsyncHTTPClient = None\n    ) -> Dict[str, Any]:\n        \"\"\"Gets the OAuth authorized user and access token.\n\n        This method should be called from the handler for your\n        OAuth callback URL to complete the registration process. We run the\n        callback with the authenticated user dictionary.  This dictionary\n        will contain an ``access_key`` which can be used to make authorized\n        requests to this service on behalf of the user.  The dictionary will\n        also contain other fields such as ``name``, depending on the service\n        used.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned\n           awaitable object instead.\n        \"\"\"\n        handler = cast(RequestHandler, self)\n        request_key = escape.utf8(handler.get_argument(\"oauth_token\"))\n        oauth_verifier = handler.get_argument(\"oauth_verifier\", None)\n        request_cookie = handler.get_cookie(\"_oauth_request_token\")\n        if not request_cookie:\n            raise AuthError(\"Missing OAuth request token cookie\")\n        handler.clear_cookie(\"_oauth_request_token\")\n        cookie_key, cookie_secret = [\n            base64.b64decode(escape.utf8(i)) for i in request_cookie.split(\"|\")\n        ]\n        if cookie_key != request_key:\n            raise AuthError(\"Request token does not match cookie\")\n        token = dict(\n            key=cookie_key, secret=cookie_secret\n        )  # type: Dict[str, Union[str, bytes]]\n        if oauth_verifier:\n            token[\"verifier\"] = oauth_verifier\n        if http_client is None:\n            http_client = self.get_auth_http_client()\n        assert http_client is not None\n        response = await http_client.fetch(self._oauth_access_token_url(token))\n        access_token = _oauth_parse_response(response.body)\n        user = await self._oauth_get_user_future(access_token)\n        if not user:\n            raise AuthError(\"Error getting user\")\n        user[\"access_token\"] = access_token\n        return user", "response": "Gets the authenticated user and access token."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef authorize_redirect(\n        self,\n        redirect_uri: str = None,\n        client_id: str = None,\n        client_secret: str = None,\n        extra_params: Dict[str, Any] = None,\n        scope: str = None,\n        response_type: str = \"code\",\n    ) -> None:\n        \"\"\"Redirects the user to obtain OAuth authorization for this service.\n\n        Some providers require that you register a redirect URL with\n        your application instead of passing one via this method. You\n        should call this method to log the user in, and then call\n        ``get_authenticated_user`` in the handler for your\n        redirect URL to complete the authorization process.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument and returned awaitable were removed;\n           this is now an ordinary synchronous function.\n        \"\"\"\n        handler = cast(RequestHandler, self)\n        args = {\"response_type\": response_type}\n        if redirect_uri is not None:\n            args[\"redirect_uri\"] = redirect_uri\n        if client_id is not None:\n            args[\"client_id\"] = client_id\n        if extra_params:\n            args.update(extra_params)\n        if scope:\n            args[\"scope\"] = \" \".join(scope)\n        url = self._OAUTH_AUTHORIZE_URL  # type: ignore\n        handler.redirect(url_concat(url, args))", "response": "Redirects the user to obtain OAuth authorization for this service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def oauth2_request(\n        self,\n        url: str,\n        access_token: str = None,\n        post_args: Dict[str, Any] = None,\n        **args: Any\n    ) -> Any:\n        \"\"\"Fetches the given URL auth an OAuth2 access token.\n\n        If the request is a POST, ``post_args`` should be provided. Query\n        string arguments should be given as keyword arguments.\n\n        Example usage:\n\n        ..testcode::\n\n            class MainHandler(tornado.web.RequestHandler,\n                              tornado.auth.FacebookGraphMixin):\n                @tornado.web.authenticated\n                async def get(self):\n                    new_entry = await self.oauth2_request(\n                        \"https://graph.facebook.com/me/feed\",\n                        post_args={\"message\": \"I am posting from my Tornado application!\"},\n                        access_token=self.current_user[\"access_token\"])\n\n                    if not new_entry:\n                        # Call failed; perhaps missing permission?\n                        await self.authorize_redirect()\n                        return\n                    self.finish(\"Posted a message!\")\n\n        .. testoutput::\n           :hide:\n\n        .. versionadded:: 4.3\n\n        .. versionchanged::: 6.0\n\n           The ``callback`` argument was removed. Use the returned awaitable object instead.\n        \"\"\"\n        all_args = {}\n        if access_token:\n            all_args[\"access_token\"] = access_token\n            all_args.update(args)\n\n        if all_args:\n            url += \"?\" + urllib.parse.urlencode(all_args)\n        http = self.get_auth_http_client()\n        if post_args is not None:\n            response = await http.fetch(\n                url, method=\"POST\", body=urllib.parse.urlencode(post_args)\n            )\n        else:\n            response = await http.fetch(url)\n        return escape.json_decode(response.body)", "response": "Fetches the given URL auth an OAuth2 access token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch the given API path and returns the response.", "response": "async def twitter_request(\n        self,\n        path: str,\n        access_token: Dict[str, Any],\n        post_args: Dict[str, Any] = None,\n        **args: Any\n    ) -> Any:\n        \"\"\"Fetches the given API path, e.g., ``statuses/user_timeline/btaylor``\n\n        The path should not include the format or API version number.\n        (we automatically use JSON format and API version 1).\n\n        If the request is a POST, ``post_args`` should be provided. Query\n        string arguments should be given as keyword arguments.\n\n        All the Twitter methods are documented at http://dev.twitter.com/\n\n        Many methods require an OAuth access token which you can\n        obtain through `~OAuthMixin.authorize_redirect` and\n        `~OAuthMixin.get_authenticated_user`. The user returned through that\n        process includes an 'access_token' attribute that can be used\n        to make authenticated requests via this method. Example\n        usage:\n\n        .. testcode::\n\n            class MainHandler(tornado.web.RequestHandler,\n                              tornado.auth.TwitterMixin):\n                @tornado.web.authenticated\n                async def get(self):\n                    new_entry = await self.twitter_request(\n                        \"/statuses/update\",\n                        post_args={\"status\": \"Testing Tornado Web Server\"},\n                        access_token=self.current_user[\"access_token\"])\n                    if not new_entry:\n                        # Call failed; perhaps missing permission?\n                        yield self.authorize_redirect()\n                        return\n                    self.finish(\"Posted a message!\")\n\n        .. testoutput::\n           :hide:\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned\n           awaitable object instead.\n        \"\"\"\n        if path.startswith(\"http:\") or path.startswith(\"https:\"):\n            # Raw urls are useful for e.g. search which doesn't follow the\n            # usual pattern: http://search.twitter.com/search.json\n            url = path\n        else:\n            url = self._TWITTER_BASE_URL + path + \".json\"\n        # Add the OAuth resource request signature if we have credentials\n        if access_token:\n            all_args = {}\n            all_args.update(args)\n            all_args.update(post_args or {})\n            method = \"POST\" if post_args is not None else \"GET\"\n            oauth = self._oauth_request_parameters(\n                url, access_token, all_args, method=method\n            )\n            args.update(oauth)\n        if args:\n            url += \"?\" + urllib.parse.urlencode(args)\n        http = self.get_auth_http_client()\n        if post_args is not None:\n            response = await http.fetch(\n                url, method=\"POST\", body=urllib.parse.urlencode(post_args)\n            )\n        else:\n            response = await http.fetch(url)\n        return escape.json_decode(response.body)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle the login for the Google user and returns an access token.", "response": "async def get_authenticated_user(\n        self, redirect_uri: str, code: str\n    ) -> Dict[str, Any]:\n        \"\"\"Handles the login for the Google user, returning an access token.\n\n        The result is a dictionary containing an ``access_token`` field\n        ([among others](https://developers.google.com/identity/protocols/OAuth2WebServer#handlingtheresponse)).\n        Unlike other ``get_authenticated_user`` methods in this package,\n        this method does not return any additional information about the user.\n        The returned access token can be used with `OAuth2Mixin.oauth2_request`\n        to request additional information (perhaps from\n        ``https://www.googleapis.com/oauth2/v2/userinfo``)\n\n        Example usage:\n\n        .. testcode::\n\n            class GoogleOAuth2LoginHandler(tornado.web.RequestHandler,\n                                           tornado.auth.GoogleOAuth2Mixin):\n                async def get(self):\n                    if self.get_argument('code', False):\n                        access = await self.get_authenticated_user(\n                            redirect_uri='http://your.site.com/auth/google',\n                            code=self.get_argument('code'))\n                        user = await self.oauth2_request(\n                            \"https://www.googleapis.com/oauth2/v1/userinfo\",\n                            access_token=access[\"access_token\"])\n                        # Save the user and access token with\n                        # e.g. set_secure_cookie.\n                    else:\n                        await self.authorize_redirect(\n                            redirect_uri='http://your.site.com/auth/google',\n                            client_id=self.settings['google_oauth']['key'],\n                            scope=['profile', 'email'],\n                            response_type='code',\n                            extra_params={'approval_prompt': 'auto'})\n\n        .. testoutput::\n           :hide:\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned awaitable object instead.\n        \"\"\"  # noqa: E501\n        handler = cast(RequestHandler, self)\n        http = self.get_auth_http_client()\n        body = urllib.parse.urlencode(\n            {\n                \"redirect_uri\": redirect_uri,\n                \"code\": code,\n                \"client_id\": handler.settings[self._OAUTH_SETTINGS_KEY][\"key\"],\n                \"client_secret\": handler.settings[self._OAUTH_SETTINGS_KEY][\"secret\"],\n                \"grant_type\": \"authorization_code\",\n            }\n        )\n\n        response = await http.fetch(\n            self._OAUTH_ACCESS_TOKEN_URL,\n            method=\"POST\",\n            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n            body=body,\n        )\n        return escape.json_decode(response.body)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle the login for the Facebook user.", "response": "async def get_authenticated_user(\n        self,\n        redirect_uri: str,\n        client_id: str,\n        client_secret: str,\n        code: str,\n        extra_fields: Dict[str, Any] = None,\n    ) -> Optional[Dict[str, Any]]:\n        \"\"\"Handles the login for the Facebook user, returning a user object.\n\n        Example usage:\n\n        .. testcode::\n\n            class FacebookGraphLoginHandler(tornado.web.RequestHandler,\n                                            tornado.auth.FacebookGraphMixin):\n              async def get(self):\n                  if self.get_argument(\"code\", False):\n                      user = await self.get_authenticated_user(\n                          redirect_uri='/auth/facebookgraph/',\n                          client_id=self.settings[\"facebook_api_key\"],\n                          client_secret=self.settings[\"facebook_secret\"],\n                          code=self.get_argument(\"code\"))\n                      # Save the user with e.g. set_secure_cookie\n                  else:\n                      await self.authorize_redirect(\n                          redirect_uri='/auth/facebookgraph/',\n                          client_id=self.settings[\"facebook_api_key\"],\n                          extra_params={\"scope\": \"read_stream,offline_access\"})\n\n        .. testoutput::\n           :hide:\n\n        This method returns a dictionary which may contain the following fields:\n\n        * ``access_token``, a string which may be passed to `facebook_request`\n        * ``session_expires``, an integer encoded as a string representing\n          the time until the access token expires in seconds. This field should\n          be used like ``int(user['session_expires'])``; in a future version of\n          Tornado it will change from a string to an integer.\n        * ``id``, ``name``, ``first_name``, ``last_name``, ``locale``, ``picture``,\n          ``link``, plus any fields named in the ``extra_fields`` argument. These\n          fields are copied from the Facebook graph API\n          `user object <https://developers.facebook.com/docs/graph-api/reference/user>`_\n\n        .. versionchanged:: 4.5\n           The ``session_expires`` field was updated to support changes made to the\n           Facebook API in March 2017.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned awaitable object instead.\n        \"\"\"\n        http = self.get_auth_http_client()\n        args = {\n            \"redirect_uri\": redirect_uri,\n            \"code\": code,\n            \"client_id\": client_id,\n            \"client_secret\": client_secret,\n        }\n\n        fields = set(\n            [\"id\", \"name\", \"first_name\", \"last_name\", \"locale\", \"picture\", \"link\"]\n        )\n        if extra_fields:\n            fields.update(extra_fields)\n\n        response = await http.fetch(\n            self._oauth_request_token_url(**args)  # type: ignore\n        )\n        args = escape.json_decode(response.body)\n        session = {\n            \"access_token\": args.get(\"access_token\"),\n            \"expires_in\": args.get(\"expires_in\"),\n        }\n        assert session[\"access_token\"] is not None\n\n        user = await self.facebook_request(\n            path=\"/me\",\n            access_token=session[\"access_token\"],\n            appsecret_proof=hmac.new(\n                key=client_secret.encode(\"utf8\"),\n                msg=session[\"access_token\"].encode(\"utf8\"),\n                digestmod=hashlib.sha256,\n            ).hexdigest(),\n            fields=\",\".join(fields),\n        )\n\n        if user is None:\n            return None\n\n        fieldmap = {}\n        for field in fields:\n            fieldmap[field] = user.get(field)\n\n        # session_expires is converted to str for compatibility with\n        # older versions in which the server used url-encoding and\n        # this code simply returned the string verbatim.\n        # This should change in Tornado 5.0.\n        fieldmap.update(\n            {\n                \"access_token\": session[\"access_token\"],\n                \"session_expires\": str(session.get(\"expires_in\")),\n            }\n        )\n        return fieldmap"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching the given relative path from the facebook API.", "response": "async def facebook_request(\n        self,\n        path: str,\n        access_token: str = None,\n        post_args: Dict[str, Any] = None,\n        **args: Any\n    ) -> Any:\n        \"\"\"Fetches the given relative API path, e.g., \"/btaylor/picture\"\n\n        If the request is a POST, ``post_args`` should be provided. Query\n        string arguments should be given as keyword arguments.\n\n        An introduction to the Facebook Graph API can be found at\n        http://developers.facebook.com/docs/api\n\n        Many methods require an OAuth access token which you can\n        obtain through `~OAuth2Mixin.authorize_redirect` and\n        `get_authenticated_user`. The user returned through that\n        process includes an ``access_token`` attribute that can be\n        used to make authenticated requests via this method.\n\n        Example usage:\n\n        .. testcode::\n\n            class MainHandler(tornado.web.RequestHandler,\n                              tornado.auth.FacebookGraphMixin):\n                @tornado.web.authenticated\n                async def get(self):\n                    new_entry = await self.facebook_request(\n                        \"/me/feed\",\n                        post_args={\"message\": \"I am posting from my Tornado application!\"},\n                        access_token=self.current_user[\"access_token\"])\n\n                    if not new_entry:\n                        # Call failed; perhaps missing permission?\n                        yield self.authorize_redirect()\n                        return\n                    self.finish(\"Posted a message!\")\n\n        .. testoutput::\n           :hide:\n\n        The given path is relative to ``self._FACEBOOK_BASE_URL``,\n        by default \"https://graph.facebook.com\".\n\n        This method is a wrapper around `OAuth2Mixin.oauth2_request`;\n        the only difference is that this method takes a relative path,\n        while ``oauth2_request`` takes a complete url.\n\n        .. versionchanged:: 3.1\n           Added the ability to override ``self._FACEBOOK_BASE_URL``.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned awaitable object instead.\n        \"\"\"\n        url = self._FACEBOOK_BASE_URL + path\n        return await self.oauth2_request(\n            url, access_token=access_token, post_args=post_args, **args\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwait for `.notify`. Returns a `.Future` that resolves ``True`` if the condition is notified, or ``False`` after a timeout.", "response": "def wait(self, timeout: Union[float, datetime.timedelta] = None) -> Awaitable[bool]:\n        \"\"\"Wait for `.notify`.\n\n        Returns a `.Future` that resolves ``True`` if the condition is notified,\n        or ``False`` after a timeout.\n        \"\"\"\n        waiter = Future()  # type: Future[bool]\n        self._waiters.append(waiter)\n        if timeout:\n\n            def on_timeout() -> None:\n                if not waiter.done():\n                    future_set_result_unless_cancelled(waiter, False)\n                self._garbage_collect()\n\n            io_loop = ioloop.IOLoop.current()\n            timeout_handle = io_loop.add_timeout(timeout, on_timeout)\n            waiter.add_done_callback(lambda _: io_loop.remove_timeout(timeout_handle))\n        return waiter"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set(self) -> None:\n        if not self._value:\n            self._value = True\n\n            for fut in self._waiters:\n                if not fut.done():\n                    fut.set_result(None)", "response": "Sets the internal flag to True. All waiters are awakened."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait(self, timeout: Union[float, datetime.timedelta] = None) -> Awaitable[None]:\n        fut = Future()  # type: Future[None]\n        if self._value:\n            fut.set_result(None)\n            return fut\n        self._waiters.add(fut)\n        fut.add_done_callback(lambda fut: self._waiters.remove(fut))\n        if timeout is None:\n            return fut\n        else:\n            timeout_fut = gen.with_timeout(\n                timeout, fut, quiet_exceptions=(CancelledError,)\n            )\n            # This is a slightly clumsy workaround for the fact that\n            # gen.with_timeout doesn't cancel its futures. Cancelling\n            # fut will remove it from the waiters list.\n            timeout_fut.add_done_callback(\n                lambda tf: fut.cancel() if not fut.done() else None\n            )\n            return timeout_fut", "response": "Block until the internal flag is true."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nincrementing the counter and wake one waiter.", "response": "def release(self) -> None:\n        \"\"\"Increment the counter and wake one waiter.\"\"\"\n        self._value += 1\n        while self._waiters:\n            waiter = self._waiters.popleft()\n            if not waiter.done():\n                self._value -= 1\n\n                # If the waiter is a coroutine paused at\n                #\n                #     with (yield semaphore.acquire()):\n                #\n                # then the context manager's __exit__ calls release() at the end\n                # of the \"with\" block.\n                waiter.set_result(_ReleasingContextManager(self))\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nincrementing the counter and wake one waiter.", "response": "def release(self) -> None:\n        \"\"\"Increment the counter and wake one waiter.\"\"\"\n        if self._value >= self._initial_value:\n            raise ValueError(\"Semaphore released too many times\")\n        super(BoundedSemaphore, self).release()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef acquire(\n        self, timeout: Union[float, datetime.timedelta] = None\n    ) -> Awaitable[_ReleasingContextManager]:\n        \"\"\"Attempt to lock. Returns an awaitable.\n\n        Returns an awaitable, which raises `tornado.util.TimeoutError` after a\n        timeout.\n        \"\"\"\n        return self._block.acquire(timeout)", "response": "Attempt to acquire a lock. Returns an awaitable which raises aTimeoutError if the lock is not acquired."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_response(self, delegate: httputil.HTTPMessageDelegate) -> Awaitable[bool]:\n        if self.params.decompress:\n            delegate = _GzipMessageDelegate(delegate, self.params.chunk_size)\n        return self._read_message(delegate)", "response": "Read a single HTTP response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _clear_callbacks(self) -> None:\n        self._write_callback = None\n        self._write_future = None  # type: Optional[Future[None]]\n        self._close_callback = None  # type: Optional[Callable[[], None]]\n        if self.stream is not None:\n            self.stream.set_close_callback(None)", "response": "Clears the callback attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetaches the underlying stream.", "response": "def detach(self) -> iostream.IOStream:\n        \"\"\"Take control of the underlying stream.\n\n        Returns the underlying `.IOStream` object and stops all further\n        HTTP processing.  May only be called during\n        `.HTTPMessageDelegate.headers_received`.  Intended for implementing\n        protocols like websockets that tunnel over an HTTP handshake.\n        \"\"\"\n        self._clear_callbacks()\n        stream = self.stream\n        self.stream = None  # type: ignore\n        if not self._finish_future.done():\n            future_set_result_unless_cancelled(self._finish_future, None)\n        return stream"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimplement `.HTTPConnection.write`. For backwards compatibility it is allowed but deprecated to skip `write_headers` and instead call `write()` with a pre-encoded header block.", "response": "def write(self, chunk: bytes) -> \"Future[None]\":\n        \"\"\"Implements `.HTTPConnection.write`.\n\n        For backwards compatibility it is allowed but deprecated to\n        skip `write_headers` and instead call `write()` with a\n        pre-encoded header block.\n        \"\"\"\n        future = None\n        if self.stream.closed():\n            future = self._write_future = Future()\n            self._write_future.set_exception(iostream.StreamClosedError())\n            self._write_future.exception()\n        else:\n            future = self._write_future = Future()\n            self._pending_write = self.stream.write(self._format_chunk(chunk))\n            future_add_done_callback(self._pending_write, self._on_write_complete)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclose the connection. Returns a. Future that resolves after the serving loop has exited.", "response": "async def close(self) -> None:\n        \"\"\"Closes the connection.\n\n        Returns a `.Future` that resolves after the serving loop has exited.\n        \"\"\"\n        self.stream.close()\n        # Block until the serving loop is done, but ignore any exceptions\n        # (start_serving is already responsible for logging them).\n        assert self._serving_future is not None\n        try:\n            await self._serving_future\n        except Exception:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_serving(self, delegate: httputil.HTTPServerConnectionDelegate) -> None:\n        assert isinstance(delegate, httputil.HTTPServerConnectionDelegate)\n        fut = gen.convert_yielded(self._server_request_loop(delegate))\n        self._serving_future = fut\n        # Register the future on the IOLoop so its errors get logged.\n        self.stream.io_loop.add_future(fut, lambda f: f.result())", "response": "Starts serving requests on this connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a message to the client of this Web Socket.", "response": "def write_message(\n        self, message: Union[bytes, str, Dict[str, Any]], binary: bool = False\n    ) -> \"Future[None]\":\n        \"\"\"Sends the given message to the client of this Web Socket.\n\n        The message may be either a string or a dict (which will be\n        encoded as json).  If the ``binary`` argument is false, the\n        message will be sent as utf8; in binary mode any byte string\n        is allowed.\n\n        If the connection is already closed, raises `WebSocketClosedError`.\n        Returns a `.Future` which can be used for flow control.\n\n        .. versionchanged:: 3.2\n           `WebSocketClosedError` was added (previously a closed connection\n           would raise an `AttributeError`)\n\n        .. versionchanged:: 4.3\n           Returns a `.Future` which can be used for flow control.\n\n        .. versionchanged:: 5.0\n           Consistently raises `WebSocketClosedError`. Previously could\n           sometimes raise `.StreamClosedError`.\n        \"\"\"\n        if self.ws_connection is None or self.ws_connection.is_closing():\n            raise WebSocketClosedError()\n        if isinstance(message, dict):\n            message = tornado.escape.json_encode(message)\n        return self.ws_connection.write_message(message, binary=binary)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a ping frame to the remote end.", "response": "def ping(self, data: Union[str, bytes] = b\"\") -> None:\n        \"\"\"Send ping frame to the remote end.\n\n        The data argument allows a small amount of data (up to 125\n        bytes) to be sent as a part of the ping message. Note that not\n        all websocket implementations expose this data to\n        applications.\n\n        Consider using the ``websocket_ping_interval`` application\n        setting instead of sending pings manually.\n\n        .. versionchanged:: 5.1\n\n           The data argument is now optional.\n\n        \"\"\"\n        data = utf8(data)\n        if self.ws_connection is None or self.ws_connection.is_closing():\n            raise WebSocketClosedError()\n        self.ws_connection.write_ping(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose this Web Socket.", "response": "def close(self, code: int = None, reason: str = None) -> None:\n        \"\"\"Closes this Web Socket.\n\n        Once the close handshake is successful the socket will be closed.\n\n        ``code`` may be a numeric status code, taken from the values\n        defined in `RFC 6455 section 7.4.1\n        <https://tools.ietf.org/html/rfc6455#section-7.4.1>`_.\n        ``reason`` may be a textual message about why the connection is\n        closing.  These values are made available to the client, but are\n        not otherwise interpreted by the websocket protocol.\n\n        .. versionchanged:: 4.0\n\n           Added the ``code`` and ``reason`` arguments.\n        \"\"\"\n        if self.ws_connection:\n            self.ws_connection.close(code, reason)\n            self.ws_connection = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noverrides to enable support for allowing alternate origins.", "response": "def check_origin(self, origin: str) -> bool:\n        \"\"\"Override to enable support for allowing alternate origins.\n\n        The ``origin`` argument is the value of the ``Origin`` HTTP\n        header, the url responsible for initiating this request.  This\n        method is not called for clients that do not send this header;\n        such requests are always allowed (because all browsers that\n        implement WebSockets support this header, and non-browser\n        clients do not have the same cross-site security concerns).\n\n        Should return ``True`` to accept the request or ``False`` to\n        reject it. By default, rejects all requests with an origin on\n        a host other than this one.\n\n        This is a security protection against cross site scripting attacks on\n        browsers, since WebSockets are allowed to bypass the usual same-origin\n        policies and don't use CORS headers.\n\n        .. warning::\n\n           This is an important security measure; don't disable it\n           without understanding the security implications. In\n           particular, if your authentication is cookie-based, you\n           must either restrict the origins allowed by\n           ``check_origin()`` or implement your own XSRF-like\n           protection for websocket connections. See `these\n           <https://www.christian-schneider.net/CrossSiteWebSocketHijacking.html>`_\n           `articles\n           <https://devcenter.heroku.com/articles/websocket-security>`_\n           for more.\n\n        To accept all cross-origin traffic (which was the default prior to\n        Tornado 4.0), simply override this method to always return ``True``::\n\n            def check_origin(self, origin):\n                return True\n\n        To allow connections from any subdomain of your site, you might\n        do something like::\n\n            def check_origin(self, origin):\n                parsed_origin = urllib.parse.urlparse(origin)\n                return parsed_origin.netloc.endswith(\".mydomain.com\")\n\n        .. versionadded:: 4.0\n\n        \"\"\"\n        parsed_origin = urlparse(origin)\n        origin = parsed_origin.netloc\n        origin = origin.lower()\n\n        host = self.request.headers.get(\"Host\")\n\n        # Check to see that origin matches host directly, including ports\n        return origin == host"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_nodelay(self, value: bool) -> None:\n        assert self.ws_connection is not None\n        self.ws_connection.set_nodelay(value)", "response": "Sets the no - delay flag for this stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns the given callback with exception handling.", "response": "def _run_callback(\n        self, callback: Callable, *args: Any, **kwargs: Any\n    ) -> \"Optional[Future[Any]]\":\n        \"\"\"Runs the given callback with exception handling.\n\n        If the callback is a coroutine, returns its Future. On error, aborts the\n        websocket connection and returns None.\n        \"\"\"\n        try:\n            result = callback(*args, **kwargs)\n        except Exception:\n            self.handler.log_exception(*sys.exc_info())\n            self._abort()\n            return None\n        else:\n            if result is not None:\n                result = gen.convert_yielded(result)\n                assert self.stream is not None\n                self.stream.io_loop.add_future(result, lambda f: f.result())\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nverifying all invariant - and required headers - and required headers - and raises ValueError if they are not.", "response": "def _handle_websocket_headers(self, handler: WebSocketHandler) -> None:\n        \"\"\"Verifies all invariant- and required headers\n\n        If a header is missing or have an incorrect value ValueError will be\n        raised\n        \"\"\"\n        fields = (\"Host\", \"Sec-Websocket-Key\", \"Sec-Websocket-Version\")\n        if not all(map(lambda f: handler.request.headers.get(f), fields)):\n            raise ValueError(\"Missing/Invalid WebSocket headers\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the value for the Sec - WebSocket - Accept header given the Sec - WebSocket - Key.", "response": "def compute_accept_value(key: Union[str, bytes]) -> str:\n        \"\"\"Computes the value for the Sec-WebSocket-Accept header,\n        given the value for Sec-WebSocket-Key.\n        \"\"\"\n        sha1 = hashlib.sha1()\n        sha1.update(utf8(key))\n        sha1.update(b\"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\")  # Magic value\n        return native_str(base64.b64encode(sha1.digest()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing the server headers.", "response": "def _process_server_headers(\n        self, key: Union[str, bytes], headers: httputil.HTTPHeaders\n    ) -> None:\n        \"\"\"Process the headers sent by the server to this client connection.\n\n        'key' is the websocket handshake challenge/response key.\n        \"\"\"\n        assert headers[\"Upgrade\"].lower() == \"websocket\"\n        assert headers[\"Connection\"].lower() == \"upgrade\"\n        accept = self.compute_accept_value(key)\n        assert headers[\"Sec-Websocket-Accept\"] == accept\n\n        extensions = self._parse_extensions_header(headers)\n        for ext in extensions:\n            if ext[0] == \"permessage-deflate\" and self._compression_options is not None:\n                self._create_compressors(\"client\", ext[1])\n            else:\n                raise ValueError(\"unsupported extension %r\", ext)\n\n        self.selected_subprotocol = headers.get(\"Sec-WebSocket-Protocol\", None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a websocket agreed_parameters to keyword arguments", "response": "def _get_compressor_options(\n        self,\n        side: str,\n        agreed_parameters: Dict[str, Any],\n        compression_options: Dict[str, Any] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Converts a websocket agreed_parameters set to keyword arguments\n        for our compressor objects.\n        \"\"\"\n        options = dict(\n            persistent=(side + \"_no_context_takeover\") not in agreed_parameters\n        )  # type: Dict[str, Any]\n        wbits_header = agreed_parameters.get(side + \"_max_window_bits\", None)\n        if wbits_header is None:\n            options[\"max_wbits\"] = zlib.MAX_WBITS\n        else:\n            options[\"max_wbits\"] = int(wbits_header)\n        options[\"compression_options\"] = compression_options\n        return options"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_message(\n        self, message: Union[str, bytes], binary: bool = False\n    ) -> \"Future[None]\":\n        \"\"\"Sends the given message to the client of this Web Socket.\"\"\"\n        if binary:\n            opcode = 0x2\n        else:\n            opcode = 0x1\n        message = tornado.escape.utf8(message)\n        assert isinstance(message, bytes)\n        self._message_bytes_out += len(message)\n        flags = 0\n        if self._compressor:\n            message = self._compressor.compress(message)\n            flags |= self.RSV1\n        # For historical reasons, write methods in Tornado operate in a semi-synchronous\n        # mode in which awaiting the Future they return is optional (But errors can\n        # still be raised). This requires us to go through an awkward dance here\n        # to transform the errors that may be returned while presenting the same\n        # semi-synchronous interface.\n        try:\n            fut = self._write_frame(True, opcode, message, flags=flags)\n        except StreamClosedError:\n            raise WebSocketClosedError()\n\n        async def wrapper() -> None:\n            try:\n                await fut\n            except StreamClosedError:\n                raise WebSocketClosedError()\n\n        return asyncio.ensure_future(wrapper())", "response": "Sends the given message to the client of this Web Socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a ping frame.", "response": "def write_ping(self, data: bytes) -> None:\n        \"\"\"Send ping frame.\"\"\"\n        assert isinstance(data, bytes)\n        self._write_frame(True, 0x9, data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute on_message returning its Future if it is a coroutine.", "response": "def _handle_message(self, opcode: int, data: bytes) -> \"Optional[Future[None]]\":\n        \"\"\"Execute on_message, returning its Future if it is a coroutine.\"\"\"\n        if self.client_terminated:\n            return None\n\n        if self._frame_compressed:\n            assert self._decompressor is not None\n            try:\n                data = self._decompressor.decompress(data)\n            except _DecompressTooLargeError:\n                self.close(1009, \"message too big after decompression\")\n                self._abort()\n                return None\n\n        if opcode == 0x1:\n            # UTF-8 data\n            self._message_bytes_in += len(data)\n            try:\n                decoded = data.decode(\"utf-8\")\n            except UnicodeDecodeError:\n                self._abort()\n                return None\n            return self._run_callback(self.handler.on_message, decoded)\n        elif opcode == 0x2:\n            # Binary data\n            self._message_bytes_in += len(data)\n            return self._run_callback(self.handler.on_message, data)\n        elif opcode == 0x8:\n            # Close\n            self.client_terminated = True\n            if len(data) >= 2:\n                self.close_code = struct.unpack(\">H\", data[:2])[0]\n            if len(data) > 2:\n                self.close_reason = to_unicode(data[2:])\n            # Echo the received close code, if any (RFC 6455 section 5.5.1).\n            self.close(self.close_code)\n        elif opcode == 0x9:\n            # Ping\n            try:\n                self._write_frame(True, 0xA, data)\n            except StreamClosedError:\n                self._abort()\n            self._run_callback(self.handler.on_ping, data)\n        elif opcode == 0xA:\n            # Pong\n            self.last_pong = IOLoop.current().time()\n            return self._run_callback(self.handler.on_pong, data)\n        else:\n            self._abort()\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncloses the WebSocket connection.", "response": "def close(self, code: int = None, reason: str = None) -> None:\n        \"\"\"Closes the WebSocket connection.\"\"\"\n        if not self.server_terminated:\n            if not self.stream.closed():\n                if code is None and reason is not None:\n                    code = 1000  # \"normal closure\" status code\n                if code is None:\n                    close_data = b\"\"\n                else:\n                    close_data = struct.pack(\">H\", code)\n                if reason is not None:\n                    close_data += utf8(reason)\n                try:\n                    self._write_frame(True, 0x8, close_data)\n                except StreamClosedError:\n                    self._abort()\n            self.server_terminated = True\n        if self.client_terminated:\n            if self._waiting is not None:\n                self.stream.io_loop.remove_timeout(self._waiting)\n                self._waiting = None\n            self.stream.close()\n        elif self._waiting is None:\n            # Give the client a few seconds to complete a clean shutdown,\n            # otherwise just close the connection.\n            self._waiting = self.stream.io_loop.add_timeout(\n                self.stream.io_loop.time() + 5, self._abort\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if this connection is closing.", "response": "def is_closing(self) -> bool:\n        \"\"\"Return ``True`` if this connection is closing.\n\n        The connection is considered closing if either side has\n        initiated its closing handshake or if the stream has been\n        shut down uncleanly.\n        \"\"\"\n        return self.stream.closed() or self.client_terminated or self.server_terminated"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart sending periodic pings to keep the connection alive.", "response": "def start_pinging(self) -> None:\n        \"\"\"Start sending periodic pings to keep the connection alive\"\"\"\n        assert self.ping_interval is not None\n        if self.ping_interval > 0:\n            self.last_ping = self.last_pong = IOLoop.current().time()\n            self.ping_callback = PeriodicCallback(\n                self.periodic_ping, self.ping_interval * 1000\n            )\n            self.ping_callback.start()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef periodic_ping(self) -> None:\n        if self.is_closing() and self.ping_callback is not None:\n            self.ping_callback.stop()\n            return\n\n        # Check for timeout on pong. Make sure that we really have\n        # sent a recent ping in case the machine with both server and\n        # client has been suspended since the last ping.\n        now = IOLoop.current().time()\n        since_last_pong = now - self.last_pong\n        since_last_ping = now - self.last_ping\n        assert self.ping_interval is not None\n        assert self.ping_timeout is not None\n        if (\n            since_last_ping < 2 * self.ping_interval\n            and since_last_pong > self.ping_timeout\n        ):\n            self.close()\n            return\n\n        self.write_ping(b\"\")\n        self.last_ping = now", "response": "Send a ping to keep the websocket alive."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self, code: int = None, reason: str = None) -> None:\n        if self.protocol is not None:\n            self.protocol.close(code, reason)\n            self.protocol = None", "response": "Closes the websocket connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_message(\n        self, message: Union[str, bytes], binary: bool = False\n    ) -> \"Future[None]\":\n        \"\"\"Sends a message to the WebSocket server.\n\n        If the stream is closed, raises `WebSocketClosedError`.\n        Returns a `.Future` which can be used for flow control.\n\n        .. versionchanged:: 5.0\n           Exception raised on a closed stream changed from `.StreamClosedError`\n           to `WebSocketClosedError`.\n        \"\"\"\n        return self.protocol.write_message(message, binary=binary)", "response": "Sends a message to the WebSocket server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a message from the WebSocket server.", "response": "def read_message(\n        self, callback: Callable[[\"Future[Union[None, str, bytes]]\"], None] = None\n    ) -> Awaitable[Union[None, str, bytes]]:\n        \"\"\"Reads a message from the WebSocket server.\n\n        If on_message_callback was specified at WebSocket\n        initialization, this function will never return messages\n\n        Returns a future whose result is the message, or None\n        if the connection is closed.  If a callback argument\n        is given it will be called with the future when it is\n        ready.\n        \"\"\"\n\n        awaitable = self.read_queue.get()\n        if callback is not None:\n            self.io_loop.add_future(asyncio.ensure_future(awaitable), callback)\n        return awaitable"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a ping frame to the remote end.", "response": "def ping(self, data: bytes = b\"\") -> None:\n        \"\"\"Send ping frame to the remote end.\n\n        The data argument allows a small amount of data (up to 125\n        bytes) to be sent as a part of the ping message. Note that not\n        all websocket implementations expose this data to\n        applications.\n\n        Consider using the ``ping_interval`` argument to\n        `websocket_connect` instead of sending pings manually.\n\n        .. versionadded:: 5.1\n\n        \"\"\"\n        data = utf8(data)\n        if self.protocol is None:\n            raise WebSocketClosedError()\n        self.protocol.write_ping(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndefines an option in the global namespace.", "response": "def define(\n    name: str,\n    default: Any = None,\n    type: type = None,\n    help: str = None,\n    metavar: str = None,\n    multiple: bool = False,\n    group: str = None,\n    callback: Callable[[Any], None] = None,\n) -> None:\n    \"\"\"Defines an option in the global namespace.\n\n    See `OptionParser.define`.\n    \"\"\"\n    return options.define(\n        name,\n        default=default,\n        type=type,\n        help=help,\n        metavar=metavar,\n        multiple=multiple,\n        group=group,\n        callback=callback,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing global options from the command line.", "response": "def parse_command_line(args: List[str] = None, final: bool = True) -> List[str]:\n    \"\"\"Parses global options from the command line.\n\n    See `OptionParser.parse_command_line`.\n    \"\"\"\n    return options.parse_command_line(args, final=final)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_config_file(path: str, final: bool = True) -> None:\n    return options.parse_config_file(path, final=final)", "response": "Parses global options from a config file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef items(self) -> Iterable[Tuple[str, Any]]:\n        return [(opt.name, opt.value()) for name, opt in self._options.items()]", "response": "An iterable of ( name value ) pairs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef group_dict(self, group: str) -> Dict[str, Any]:\n        return dict(\n            (opt.name, opt.value())\n            for name, opt in self._options.items()\n            if not group or group == opt.group_name\n        )", "response": "The names and values of options in a group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_dict(self) -> Dict[str, Any]:\n        return dict((opt.name, opt.value()) for name, opt in self._options.items())", "response": "The names and values of all options.\n       .. versionadded :: 3. 1\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndefine a new command line option.", "response": "def define(\n        self,\n        name: str,\n        default: Any = None,\n        type: type = None,\n        help: str = None,\n        metavar: str = None,\n        multiple: bool = False,\n        group: str = None,\n        callback: Callable[[Any], None] = None,\n    ) -> None:\n        \"\"\"Defines a new command line option.\n\n        ``type`` can be any of `str`, `int`, `float`, `bool`,\n        `~datetime.datetime`, or `~datetime.timedelta`. If no ``type``\n        is given but a ``default`` is, ``type`` is the type of\n        ``default``. Otherwise, ``type`` defaults to `str`.\n\n        If ``multiple`` is True, the option value is a list of ``type``\n        instead of an instance of ``type``.\n\n        ``help`` and ``metavar`` are used to construct the\n        automatically generated command line help string. The help\n        message is formatted like::\n\n           --name=METAVAR      help string\n\n        ``group`` is used to group the defined options in logical\n        groups. By default, command line options are grouped by the\n        file in which they are defined.\n\n        Command line option names must be unique globally.\n\n        If a ``callback`` is given, it will be run with the new value whenever\n        the option is changed.  This can be used to combine command-line\n        and file-based options::\n\n            define(\"config\", type=str, help=\"path to config file\",\n                   callback=lambda path: parse_config_file(path, final=False))\n\n        With this definition, options in the file specified by ``--config`` will\n        override options set earlier on the command line, but can be overridden\n        by later flags.\n\n        \"\"\"\n        normalized = self._normalize_name(name)\n        if normalized in self._options:\n            raise Error(\n                \"Option %r already defined in %s\"\n                % (normalized, self._options[normalized].file_name)\n            )\n        frame = sys._getframe(0)\n        options_file = frame.f_code.co_filename\n\n        # Can be called directly, or through top level define() fn, in which\n        # case, step up above that frame to look for real caller.\n        if (\n            frame.f_back.f_code.co_filename == options_file\n            and frame.f_back.f_code.co_name == \"define\"\n        ):\n            frame = frame.f_back\n\n        file_name = frame.f_back.f_code.co_filename\n        if file_name == options_file:\n            file_name = \"\"\n        if type is None:\n            if not multiple and default is not None:\n                type = default.__class__\n            else:\n                type = str\n        if group:\n            group_name = group  # type: Optional[str]\n        else:\n            group_name = file_name\n        option = _Option(\n            name,\n            file_name=file_name,\n            default=default,\n            type=type,\n            help=help,\n            metavar=metavar,\n            multiple=multiple,\n            group_name=group_name,\n            callback=callback,\n        )\n        self._options[normalized] = option"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses all options given on the command line.", "response": "def parse_command_line(\n        self, args: List[str] = None, final: bool = True\n    ) -> List[str]:\n        \"\"\"Parses all options given on the command line (defaults to\n        `sys.argv`).\n\n        Options look like ``--option=value`` and are parsed according\n        to their ``type``. For boolean options, ``--option`` is\n        equivalent to ``--option=true``\n\n        If the option has ``multiple=True``, comma-separated values\n        are accepted. For multi-value integer options, the syntax\n        ``x:y`` is also accepted and equivalent to ``range(x, y)``.\n\n        Note that ``args[0]`` is ignored since it is the program name\n        in `sys.argv`.\n\n        We return a list of all arguments that are not parsed as options.\n\n        If ``final`` is ``False``, parse callbacks will not be run.\n        This is useful for applications that wish to combine configurations\n        from multiple sources.\n\n        \"\"\"\n        if args is None:\n            args = sys.argv\n        remaining = []  # type: List[str]\n        for i in range(1, len(args)):\n            # All things after the last option are command line arguments\n            if not args[i].startswith(\"-\"):\n                remaining = args[i:]\n                break\n            if args[i] == \"--\":\n                remaining = args[i + 1 :]\n                break\n            arg = args[i].lstrip(\"-\")\n            name, equals, value = arg.partition(\"=\")\n            name = self._normalize_name(name)\n            if name not in self._options:\n                self.print_help()\n                raise Error(\"Unrecognized command line option: %r\" % name)\n            option = self._options[name]\n            if not equals:\n                if option.type == bool:\n                    value = \"true\"\n                else:\n                    raise Error(\"Option %r requires a value\" % name)\n            option.parse(value)\n\n        if final:\n            self.run_parse_callbacks()\n\n        return remaining"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse and loads the config file at the given path.", "response": "def parse_config_file(self, path: str, final: bool = True) -> None:\n        \"\"\"Parses and loads the config file at the given path.\n\n        The config file contains Python code that will be executed (so\n        it is **not safe** to use untrusted config files). Anything in\n        the global namespace that matches a defined option will be\n        used to set that option's value.\n\n        Options may either be the specified type for the option or\n        strings (in which case they will be parsed the same way as in\n        `.parse_command_line`)\n\n        Example (using the options defined in the top-level docs of\n        this module)::\n\n            port = 80\n            mysql_host = 'mydb.example.com:3306'\n            # Both lists and comma-separated strings are allowed for\n            # multiple=True.\n            memcache_hosts = ['cache1.example.com:11011',\n                              'cache2.example.com:11011']\n            memcache_hosts = 'cache1.example.com:11011,cache2.example.com:11011'\n\n        If ``final`` is ``False``, parse callbacks will not be run.\n        This is useful for applications that wish to combine configurations\n        from multiple sources.\n\n        .. note::\n\n            `tornado.options` is primarily a command-line library.\n            Config file support is provided for applications that wish\n            to use it, but applications that prefer config files may\n            wish to look at other libraries instead.\n\n        .. versionchanged:: 4.1\n           Config files are now always interpreted as utf-8 instead of\n           the system default encoding.\n\n        .. versionchanged:: 4.4\n           The special variable ``__file__`` is available inside config\n           files, specifying the absolute path to the config file itself.\n\n        .. versionchanged:: 5.1\n           Added the ability to set options via strings in config files.\n\n        \"\"\"\n        config = {\"__file__\": os.path.abspath(path)}\n        with open(path, \"rb\") as f:\n            exec_in(native_str(f.read()), config, config)\n        for name in config:\n            normalized = self._normalize_name(name)\n            if normalized in self._options:\n                option = self._options[normalized]\n                if option.multiple:\n                    if not isinstance(config[name], (list, str)):\n                        raise Error(\n                            \"Option %r is required to be a list of %s \"\n                            \"or a comma-separated string\"\n                            % (option.name, option.type.__name__)\n                        )\n\n                if type(config[name]) == str and option.type != str:\n                    option.parse(config[name])\n                else:\n                    option.set(config[name])\n\n        if final:\n            self.run_parse_callbacks()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint all the command line options to stderr or another file.", "response": "def print_help(self, file: TextIO = None) -> None:\n        \"\"\"Prints all the command line options to stderr (or another file).\"\"\"\n        if file is None:\n            file = sys.stderr\n        print(\"Usage: %s [OPTIONS]\" % sys.argv[0], file=file)\n        print(\"\\nOptions:\\n\", file=file)\n        by_group = {}  # type: Dict[str, List[_Option]]\n        for option in self._options.values():\n            by_group.setdefault(option.group_name, []).append(option)\n\n        for filename, o in sorted(by_group.items()):\n            if filename:\n                print(\"\\n%s options:\\n\" % os.path.normpath(filename), file=file)\n            o.sort(key=lambda option: option.name)\n            for option in o:\n                # Always print names with dashes in a CLI context.\n                prefix = self._normalize_name(option.name)\n                if option.metavar:\n                    prefix += \"=\" + option.metavar\n                description = option.help or \"\"\n                if option.default is not None and option.default != \"\":\n                    description += \" (default %s)\" % option.default\n                lines = textwrap.wrap(description, 79 - 35)\n                if len(prefix) > 30 or len(lines) == 0:\n                    lines.insert(0, \"\")\n                print(\"  --%-30s %s\" % (prefix, lines[0]), file=file)\n                for line in lines[1:]:\n                    print(\"%-34s %s\" % (\" \", line), file=file)\n        print(file=file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a SQL row to an object supporting dict and attribute access.", "response": "def row_to_obj(self, row, cur):\n        \"\"\"Convert a SQL row to an object supporting dict and attribute access.\"\"\"\n        obj = tornado.util.ObjectDict()\n        for val, desc in zip(row, cur.description):\n            obj[desc.name] = val\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a SQL statement.", "response": "async def execute(self, stmt, *args):\n        \"\"\"Execute a SQL statement.\n\n        Must be called with ``await self.execute(...)``\n        \"\"\"\n        with (await self.application.db.cursor()) as cur:\n            await cur.execute(stmt, args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries for a list of results. Typical usage ::", "response": "async def query(self, stmt, *args):\n        \"\"\"Query for a list of results.\n\n        Typical usage::\n\n            results = await self.query(...)\n\n        Or::\n\n            for row in await self.query(...)\n        \"\"\"\n        with (await self.application.db.cursor()) as cur:\n            await cur.execute(stmt, args)\n            return [self.row_to_obj(row, cur) for row in await cur.fetchall()]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries for exactly one result. Raises NoResultError Raises ValueError", "response": "async def queryone(self, stmt, *args):\n        \"\"\"Query for exactly one result.\n\n        Raises NoResultError if there are no results, or ValueError if\n        there are more than one.\n        \"\"\"\n        results = await self.query(stmt, *args)\n        if len(results) == 0:\n            raise NoResultError()\n        elif len(results) > 1:\n            raise ValueError(\"Expected 1 result, got %d\" % len(results))\n        return results[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef listen(self, port: int, address: str = \"\") -> None:\n        sockets = bind_sockets(port, address=address)\n        self.add_sockets(sockets)", "response": "Starts accepting connections on the given port."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_sockets(self, sockets: Iterable[socket.socket]) -> None:\n        for sock in sockets:\n            self._sockets[sock.fileno()] = sock\n            self._handlers[sock.fileno()] = add_accept_handler(\n                sock, self._handle_connection\n            )", "response": "Makes this server start accepting connections on the given sockets."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bind(\n        self,\n        port: int,\n        address: str = None,\n        family: socket.AddressFamily = socket.AF_UNSPEC,\n        backlog: int = 128,\n        reuse_port: bool = False,\n    ) -> None:\n        \"\"\"Binds this server to the given port on the given address.\n\n        To start the server, call `start`. If you want to run this server\n        in a single process, you can call `listen` as a shortcut to the\n        sequence of `bind` and `start` calls.\n\n        Address may be either an IP address or hostname.  If it's a hostname,\n        the server will listen on all IP addresses associated with the\n        name.  Address may be an empty string or None to listen on all\n        available interfaces.  Family may be set to either `socket.AF_INET`\n        or `socket.AF_INET6` to restrict to IPv4 or IPv6 addresses, otherwise\n        both will be used if available.\n\n        The ``backlog`` argument has the same meaning as for\n        `socket.listen <socket.socket.listen>`. The ``reuse_port`` argument\n        has the same meaning as for `.bind_sockets`.\n\n        This method may be called multiple times prior to `start` to listen\n        on multiple ports or interfaces.\n\n        .. versionchanged:: 4.4\n           Added the ``reuse_port`` argument.\n        \"\"\"\n        sockets = bind_sockets(\n            port, address=address, family=family, backlog=backlog, reuse_port=reuse_port\n        )\n        if self._started:\n            self.add_sockets(sockets)\n        else:\n            self._pending_sockets.extend(sockets)", "response": "Binds this server to the given port on the given address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting this server in the. IOLoop.", "response": "def start(self, num_processes: Optional[int] = 1, max_restarts: int = None) -> None:\n        \"\"\"Starts this server in the `.IOLoop`.\n\n        By default, we run the server in this process and do not fork any\n        additional child process.\n\n        If num_processes is ``None`` or <= 0, we detect the number of cores\n        available on this machine and fork that number of child\n        processes. If num_processes is given and > 1, we fork that\n        specific number of sub-processes.\n\n        Since we use processes and not threads, there is no shared memory\n        between any server code.\n\n        Note that multiple processes are not compatible with the autoreload\n        module (or the ``autoreload=True`` option to `tornado.web.Application`\n        which defaults to True when ``debug=True``).\n        When using multiple processes, no IOLoops can be created or\n        referenced until after the call to ``TCPServer.start(n)``.\n\n        Values of ``num_processes`` other than 1 are not supported on Windows.\n\n        The ``max_restarts`` argument is passed to `.fork_processes`.\n\n        .. versionchanged:: 6.0\n\n           Added ``max_restarts`` argument.\n        \"\"\"\n        assert not self._started\n        self._started = True\n        if num_processes != 1:\n            process.fork_processes(num_processes, max_restarts)\n        sockets = self._pending_sockets\n        self._pending_sockets = []\n        self.add_sockets(sockets)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstopping listening for new connections.", "response": "def stop(self) -> None:\n        \"\"\"Stops listening for new connections.\n\n        Requests currently in progress may still continue after the\n        server is stopped.\n        \"\"\"\n        if self._stopped:\n            return\n        self._stopped = True\n        for fd, sock in self._sockets.items():\n            assert sock.fileno() == fd\n            # Unregister socket from IOLoop\n            self._handlers.pop(fd)()\n            sock.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nputs an item into the queue.", "response": "def put(\n        self, item: _T, timeout: Union[float, datetime.timedelta] = None\n    ) -> \"Future[None]\":\n        \"\"\"Put an item into the queue, perhaps waiting until there is room.\n\n        Returns a Future, which raises `tornado.util.TimeoutError` after a\n        timeout.\n\n        ``timeout`` may be a number denoting a time (on the same\n        scale as `tornado.ioloop.IOLoop.time`, normally `time.time`), or a\n        `datetime.timedelta` object for a deadline relative to the\n        current time.\n        \"\"\"\n        future = Future()  # type: Future[None]\n        try:\n            self.put_nowait(item)\n        except QueueFull:\n            self._putters.append((item, future))\n            _set_timeout(future, timeout)\n        else:\n            future.set_result(None)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put_nowait(self, item: _T) -> None:\n        self._consume_expired()\n        if self._getters:\n            assert self.empty(), \"queue non-empty, why are getters waiting?\"\n            getter = self._getters.popleft()\n            self.__put_internal(item)\n            future_set_result_unless_cancelled(getter, self._get())\n        elif self.full():\n            raise QueueFull\n        else:\n            self.__put_internal(item)", "response": "Put an item into the queue without blocking."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, timeout: Union[float, datetime.timedelta] = None) -> Awaitable[_T]:\n        future = Future()  # type: Future[_T]\n        try:\n            future.set_result(self.get_nowait())\n        except QueueEmpty:\n            self._getters.append(future)\n            _set_timeout(future, timeout)\n        return future", "response": "Remove and return an item from the queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_nowait(self) -> _T:\n        self._consume_expired()\n        if self._putters:\n            assert self.full(), \"queue not full, why are putters waiting?\"\n            item, putter = self._putters.popleft()\n            self.__put_internal(item)\n            future_set_result_unless_cancelled(putter, None)\n            return self._get()\n        elif self.qsize():\n            return self._get()\n        else:\n            raise QueueEmpty", "response": "Remove and return an item from the queue without blocking."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef task_done(self) -> None:\n        if self._unfinished_tasks <= 0:\n            raise ValueError(\"task_done() called too many times\")\n        self._unfinished_tasks -= 1\n        if self._unfinished_tasks == 0:\n            self._finished.set()", "response": "Indicate that a task is complete."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nblocks until all items in the queue are processed.", "response": "def join(self, timeout: Union[float, datetime.timedelta] = None) -> Awaitable[None]:\n        \"\"\"Block until all items in the queue are processed.\n\n        Returns an awaitable, which raises `tornado.util.TimeoutError` after a\n        timeout.\n        \"\"\"\n        return self._finished.wait(timeout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cpu_count() -> int:\n    if multiprocessing is None:\n        return 1\n    try:\n        return multiprocessing.cpu_count()\n    except NotImplementedError:\n        pass\n    try:\n        return os.sysconf(\"SC_NPROCESSORS_CONF\")\n    except (AttributeError, ValueError):\n        pass\n    gen_log.error(\"Could not detect number of processors; assuming 1\")\n    return 1", "response": "Returns the number of processors on this machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts multiple worker processes.", "response": "def fork_processes(num_processes: Optional[int], max_restarts: int = None) -> int:\n    \"\"\"Starts multiple worker processes.\n\n    If ``num_processes`` is None or <= 0, we detect the number of cores\n    available on this machine and fork that number of child\n    processes. If ``num_processes`` is given and > 0, we fork that\n    specific number of sub-processes.\n\n    Since we use processes and not threads, there is no shared memory\n    between any server code.\n\n    Note that multiple processes are not compatible with the autoreload\n    module (or the ``autoreload=True`` option to `tornado.web.Application`\n    which defaults to True when ``debug=True``).\n    When using multiple processes, no IOLoops can be created or\n    referenced until after the call to ``fork_processes``.\n\n    In each child process, ``fork_processes`` returns its *task id*, a\n    number between 0 and ``num_processes``.  Processes that exit\n    abnormally (due to a signal or non-zero exit status) are restarted\n    with the same id (up to ``max_restarts`` times).  In the parent\n    process, ``fork_processes`` returns None if all child processes\n    have exited normally, but will otherwise only exit by throwing an\n    exception.\n\n    max_restarts defaults to 100.\n\n    Availability: Unix\n    \"\"\"\n    if max_restarts is None:\n        max_restarts = 100\n\n    global _task_id\n    assert _task_id is None\n    if num_processes is None or num_processes <= 0:\n        num_processes = cpu_count()\n    gen_log.info(\"Starting %d processes\", num_processes)\n    children = {}\n\n    def start_child(i: int) -> Optional[int]:\n        pid = os.fork()\n        if pid == 0:\n            # child process\n            _reseed_random()\n            global _task_id\n            _task_id = i\n            return i\n        else:\n            children[pid] = i\n            return None\n\n    for i in range(num_processes):\n        id = start_child(i)\n        if id is not None:\n            return id\n    num_restarts = 0\n    while children:\n        try:\n            pid, status = os.wait()\n        except OSError as e:\n            if errno_from_exception(e) == errno.EINTR:\n                continue\n            raise\n        if pid not in children:\n            continue\n        id = children.pop(pid)\n        if os.WIFSIGNALED(status):\n            gen_log.warning(\n                \"child %d (pid %d) killed by signal %d, restarting\",\n                id,\n                pid,\n                os.WTERMSIG(status),\n            )\n        elif os.WEXITSTATUS(status) != 0:\n            gen_log.warning(\n                \"child %d (pid %d) exited with status %d, restarting\",\n                id,\n                pid,\n                os.WEXITSTATUS(status),\n            )\n        else:\n            gen_log.info(\"child %d (pid %d) exited normally\", id, pid)\n            continue\n        num_restarts += 1\n        if num_restarts > max_restarts:\n            raise RuntimeError(\"Too many child restarts, giving up\")\n        new_id = start_child(id)\n        if new_id is not None:\n            return new_id\n    # All child processes exited cleanly, so exit the master process\n    # instead of just returning to right after the call to\n    # fork_processes (which will probably just start up another IOLoop\n    # unless the caller checks the return value).\n    sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_exit_callback(self, callback: Callable[[int], None]) -> None:\n        self._exit_callback = callback\n        Subprocess.initialize()\n        Subprocess._waiting[self.pid] = self\n        Subprocess._try_cleanup_process(self.pid)", "response": "Sets the exit callback for this process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wait_for_exit(self, raise_error: bool = True) -> \"Future[int]\":\n        future = Future()  # type: Future[int]\n\n        def callback(ret: int) -> None:\n            if ret != 0 and raise_error:\n                # Unfortunately we don't have the original args any more.\n                future_set_exception_unless_cancelled(\n                    future, CalledProcessError(ret, \"unknown\")\n                )\n            else:\n                future_set_result_unless_cancelled(future, ret)\n\n        self.set_exit_callback(callback)\n        return future", "response": "A context manager which resolves when the process exits."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the SIGCHLD handler.", "response": "def initialize(cls) -> None:\n        \"\"\"Initializes the ``SIGCHLD`` handler.\n\n        The signal handler is run on an `.IOLoop` to avoid locking issues.\n        Note that the `.IOLoop` used for signal handling need not be the\n        same one used by individual Subprocess objects (as long as the\n        ``IOLoops`` are each running in separate threads).\n\n        .. versionchanged:: 5.0\n           The ``io_loop`` argument (deprecated since version 4.1) has been\n           removed.\n\n        Availability: Unix\n        \"\"\"\n        if cls._initialized:\n            return\n        io_loop = ioloop.IOLoop.current()\n        cls._old_sigchld = signal.signal(\n            signal.SIGCHLD,\n            lambda sig, frame: io_loop.add_callback_from_signal(cls._cleanup),\n        )\n        cls._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef uninitialize(cls) -> None:\n        if not cls._initialized:\n            return\n        signal.signal(signal.SIGCHLD, cls._old_sigchld)\n        cls._initialized = False", "response": "Removes the SIGCHLD handler from the signal list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_socket(self, event: int, fd: int, multi: Any, data: bytes) -> None:\n        event_map = {\n            pycurl.POLL_NONE: ioloop.IOLoop.NONE,\n            pycurl.POLL_IN: ioloop.IOLoop.READ,\n            pycurl.POLL_OUT: ioloop.IOLoop.WRITE,\n            pycurl.POLL_INOUT: ioloop.IOLoop.READ | ioloop.IOLoop.WRITE,\n        }\n        if event == pycurl.POLL_REMOVE:\n            if fd in self._fds:\n                self.io_loop.remove_handler(fd)\n                del self._fds[fd]\n        else:\n            ioloop_event = event_map[event]\n            # libcurl sometimes closes a socket and then opens a new\n            # one using the same FD without giving us a POLL_NONE in\n            # between.  This is a problem with the epoll IOLoop,\n            # because the kernel can tell when a socket is closed and\n            # removes it from the epoll automatically, causing future\n            # update_handler calls to fail.  Since we can't tell when\n            # this has happened, always use remove and re-add\n            # instead of update.\n            if fd in self._fds:\n                self.io_loop.remove_handler(fd)\n            self.io_loop.add_handler(fd, self._handle_events, ioloop_event)\n            self._fds[fd] = ioloop_event", "response": "Called by libcurl when it wants to change the file descriptors\n            it cares about."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling by libcurl to schedule a timeout.", "response": "def _set_timeout(self, msecs: int) -> None:\n        \"\"\"Called by libcurl to schedule a timeout.\"\"\"\n        if self._timeout is not None:\n            self.io_loop.remove_timeout(self._timeout)\n        self._timeout = self.io_loop.add_timeout(\n            self.io_loop.time() + msecs / 1000.0, self._handle_timeout\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall by IOLoop when there is activity on one of our file descriptors.", "response": "def _handle_events(self, fd: int, events: int) -> None:\n        \"\"\"Called by IOLoop when there is activity on one of our\n        file descriptors.\n        \"\"\"\n        action = 0\n        if events & ioloop.IOLoop.READ:\n            action |= pycurl.CSELECT_IN\n        if events & ioloop.IOLoop.WRITE:\n            action |= pycurl.CSELECT_OUT\n        while True:\n            try:\n                ret, num_handles = self._multi.socket_action(fd, action)\n            except pycurl.error as e:\n                ret = e.args[0]\n            if ret != pycurl.E_CALL_MULTI_PERFORM:\n                break\n        self._finish_pending_requests()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls by IOLoop when the timeout has passed.", "response": "def _handle_timeout(self) -> None:\n        \"\"\"Called by IOLoop when the requested timeout has passed.\"\"\"\n        self._timeout = None\n        while True:\n            try:\n                ret, num_handles = self._multi.socket_action(pycurl.SOCKET_TIMEOUT, 0)\n            except pycurl.error as e:\n                ret = e.args[0]\n            if ret != pycurl.E_CALL_MULTI_PERFORM:\n                break\n        self._finish_pending_requests()\n\n        # In theory, we shouldn't have to do this because curl will\n        # call _set_timeout whenever the timeout changes.  However,\n        # sometimes after _handle_timeout we will need to reschedule\n        # immediately even though nothing has changed from curl's\n        # perspective.  This is because when socket_action is\n        # called with SOCKET_TIMEOUT, libcurl decides internally which\n        # timeouts need to be processed by using a monotonic clock\n        # (where available) while tornado uses python's time.time()\n        # to decide when timeouts have occurred.  When those clocks\n        # disagree on elapsed time (as they will whenever there is an\n        # NTP adjustment), tornado might call _handle_timeout before\n        # libcurl is ready.  After each timeout, resync the scheduled\n        # timeout with libcurl's current state.\n        new_timeout = self._multi.timeout()\n        if new_timeout >= 0:\n            self._set_timeout(new_timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling by IOLoop periodically to process any events that may have forgotten about.", "response": "def _handle_force_timeout(self) -> None:\n        \"\"\"Called by IOLoop periodically to ask libcurl to process any\n        events it may have forgotten about.\n        \"\"\"\n        while True:\n            try:\n                ret, num_handles = self._multi.socket_all()\n            except pycurl.error as e:\n                ret = e.args[0]\n            if ret != pycurl.E_CALL_MULTI_PERFORM:\n                break\n        self._finish_pending_requests()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _finish_pending_requests(self) -> None:\n        while True:\n            num_q, ok_list, err_list = self._multi.info_read()\n            for curl in ok_list:\n                self._finish(curl)\n            for curl, errnum, errmsg in err_list:\n                self._finish(curl, errnum, errmsg)\n            if num_q == 0:\n                break\n        self._process_queue()", "response": "Process any pending requests that were completed by the last\n        call to multi. socket_action."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting the mock S3 server on the given port at the given path.", "response": "def start(port, root_directory, bucket_depth):\n    \"\"\"Starts the mock S3 server on the given port at the given path.\"\"\"\n    application = S3Application(root_directory, bucket_depth)\n    http_server = httpserver.HTTPServer(application)\n    http_server.listen(port)\n    ioloop.IOLoop.current().start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close(self) -> None:\n        if not self._closed:\n            self._async_client.close()\n            self._io_loop.close()\n            self._closed = True", "response": "Closes the HTTPClient and closes the IO Loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetch(\n        self, request: Union[\"HTTPRequest\", str], **kwargs: Any\n    ) -> \"HTTPResponse\":\n        \"\"\"Executes a request, returning an `HTTPResponse`.\n\n        The request may be either a string URL or an `HTTPRequest` object.\n        If it is a string, we construct an `HTTPRequest` using any additional\n        kwargs: ``HTTPRequest(request, **kwargs)``\n\n        If an error occurs during the fetch, we raise an `HTTPError` unless\n        the ``raise_error`` keyword argument is set to False.\n        \"\"\"\n        response = self._io_loop.run_sync(\n            functools.partial(self._async_client.fetch, request, **kwargs)\n        )\n        return response", "response": "Executes a request returning an HTTPResponse."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self) -> None:\n        if self._closed:\n            return\n        self._closed = True\n        if self._instance_cache is not None:\n            cached_val = self._instance_cache.pop(self.io_loop, None)\n            # If there's an object other than self in the instance\n            # cache for our IOLoop, something has gotten mixed up. A\n            # value of None appears to be possible when this is called\n            # from a destructor (HTTPClient.__del__) as the weakref\n            # gets cleared before the destructor runs.\n            if cached_val is not None and cached_val is not self:\n                raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")", "response": "Destroys this HTTP client and cleans up any file descriptors used by the IOLoop."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute a request asynchronously returning an HTTPResponse.", "response": "def fetch(\n        self,\n        request: Union[str, \"HTTPRequest\"],\n        raise_error: bool = True,\n        **kwargs: Any\n    ) -> Awaitable[\"HTTPResponse\"]:\n        \"\"\"Executes a request, asynchronously returning an `HTTPResponse`.\n\n        The request may be either a string URL or an `HTTPRequest` object.\n        If it is a string, we construct an `HTTPRequest` using any additional\n        kwargs: ``HTTPRequest(request, **kwargs)``\n\n        This method returns a `.Future` whose result is an\n        `HTTPResponse`. By default, the ``Future`` will raise an\n        `HTTPError` if the request returned a non-200 response code\n        (other errors may also be raised if the server could not be\n        contacted). Instead, if ``raise_error`` is set to False, the\n        response will always be returned regardless of the response\n        code.\n\n        If a ``callback`` is given, it will be invoked with the `HTTPResponse`.\n        In the callback interface, `HTTPError` is not automatically raised.\n        Instead, you must check the response's ``error`` attribute or\n        call its `~HTTPResponse.rethrow` method.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned\n           `.Future` instead.\n\n           The ``raise_error=False`` argument only affects the\n           `HTTPError` raised when a non-200 response code is used,\n           instead of suppressing all errors.\n        \"\"\"\n        if self._closed:\n            raise RuntimeError(\"fetch() called on closed AsyncHTTPClient\")\n        if not isinstance(request, HTTPRequest):\n            request = HTTPRequest(url=request, **kwargs)\n        else:\n            if kwargs:\n                raise ValueError(\n                    \"kwargs can't be used if request is an HTTPRequest object\"\n                )\n        # We may modify this (to add Host, Accept-Encoding, etc),\n        # so make sure we don't modify the caller's object.  This is also\n        # where normal dicts get converted to HTTPHeaders objects.\n        request.headers = httputil.HTTPHeaders(request.headers)\n        request_proxy = _RequestProxy(request, self.defaults)\n        future = Future()  # type: Future[HTTPResponse]\n\n        def handle_response(response: \"HTTPResponse\") -> None:\n            if response.error:\n                if raise_error or not response._error_is_response_code:\n                    future_set_exception_unless_cancelled(future, response.error)\n                    return\n            future_set_result_unless_cancelled(future, response)\n\n        self.fetch_impl(cast(HTTPRequest, request_proxy), handle_response)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef configure(\n        cls, impl: \"Union[None, str, Type[Configurable]]\", **kwargs: Any\n    ) -> None:\n        \"\"\"Configures the `AsyncHTTPClient` subclass to use.\n\n        ``AsyncHTTPClient()`` actually creates an instance of a subclass.\n        This method may be called with either a class object or the\n        fully-qualified name of such a class (or ``None`` to use the default,\n        ``SimpleAsyncHTTPClient``)\n\n        If additional keyword arguments are given, they will be passed\n        to the constructor of each subclass instance created.  The\n        keyword argument ``max_clients`` determines the maximum number\n        of simultaneous `~AsyncHTTPClient.fetch()` operations that can\n        execute in parallel on each `.IOLoop`.  Additional arguments\n        may be supported depending on the implementation class in use.\n\n        Example::\n\n           AsyncHTTPClient.configure(\"tornado.curl_httpclient.CurlAsyncHTTPClient\")\n        \"\"\"\n        super(AsyncHTTPClient, cls).configure(impl, **kwargs)", "response": "Configures the AsyncHTTPClient subclass to use."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _cleanup_closed(self) -> None:\n        if self._cleanup_closed_handle:\n            self._cleanup_closed_handle.cancel()\n\n        for transport in self._cleanup_closed_transports:\n            if transport is not None:\n                transport.abort()\n\n        self._cleanup_closed_transports = []\n\n        if not self._cleanup_closed_disabled:\n            self._cleanup_closed_handle = helpers.weakref_handle(\n                self, '_cleanup_closed',\n                self._cleanup_closed_period, self._loop)", "response": "Double confirmation for transport close."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _available_connections(self, key: 'ConnectionKey') -> int:\n\n        if self._limit:\n            # total calc available connections\n            available = self._limit - len(self._acquired)\n\n            # check limit per host\n            if (self._limit_per_host and available > 0 and\n                    key in self._acquired_per_host):\n                acquired = self._acquired_per_host.get(key)\n                assert acquired is not None\n                available = self._limit_per_host - len(acquired)\n\n        elif self._limit_per_host and key in self._acquired_per_host:\n            # check limit per host\n            acquired = self._acquired_per_host.get(key)\n            assert acquired is not None\n            available = self._limit_per_host - len(acquired)\n        else:\n            available = 1\n\n        return available", "response": "Return the number of available connections taking into account the limit limit_per_host and the connection key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect to the specified connection.", "response": "async def connect(self, req: 'ClientRequest',\n                      traces: List['Trace'],\n                      timeout: 'ClientTimeout') -> Connection:\n        \"\"\"Get from pool or create new connection.\"\"\"\n        key = req.connection_key\n        available = self._available_connections(key)\n\n        # Wait if there are no available connections.\n        if available <= 0:\n            fut = self._loop.create_future()\n\n            # This connection will now count towards the limit.\n            waiters = self._waiters[key]\n            waiters.append(fut)\n\n            if traces:\n                for trace in traces:\n                    await trace.send_connection_queued_start()\n\n            try:\n                await fut\n            except BaseException as e:\n                # remove a waiter even if it was cancelled, normally it's\n                #  removed when it's notified\n                try:\n                    waiters.remove(fut)\n                except ValueError:  # fut may no longer be in list\n                    pass\n\n                raise e\n            finally:\n                if not waiters:\n                    try:\n                        del self._waiters[key]\n                    except KeyError:\n                        # the key was evicted before.\n                        pass\n\n            if traces:\n                for trace in traces:\n                    await trace.send_connection_queued_end()\n\n        proto = self._get(key)\n        if proto is None:\n            placeholder = cast(ResponseHandler, _TransportPlaceholder())\n            self._acquired.add(placeholder)\n            self._acquired_per_host[key].add(placeholder)\n\n            if traces:\n                for trace in traces:\n                    await trace.send_connection_create_start()\n\n            try:\n                proto = await self._create_connection(req, traces, timeout)\n                if self._closed:\n                    proto.close()\n                    raise ClientConnectionError(\"Connector is closed.\")\n            except BaseException:\n                if not self._closed:\n                    self._acquired.remove(placeholder)\n                    self._drop_acquired_per_host(key, placeholder)\n                    self._release_waiter()\n                raise\n            else:\n                if not self._closed:\n                    self._acquired.remove(placeholder)\n                    self._drop_acquired_per_host(key, placeholder)\n\n            if traces:\n                for trace in traces:\n                    await trace.send_connection_create_end()\n        else:\n            if traces:\n                for trace in traces:\n                    await trace.send_connection_reuseconn()\n\n        self._acquired.add(proto)\n        self._acquired_per_host[key].add(proto)\n        return Connection(self, key, proto, self._loop)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over all waiters till found one that is not finsihed and belongs to a host that has available connections.", "response": "def _release_waiter(self) -> None:\n        \"\"\"\n        Iterates over all waiters till found one that is not finsihed and\n        belongs to a host that has available connections.\n        \"\"\"\n        if not self._waiters:\n            return\n\n        # Having the dict keys ordered this avoids to iterate\n        # at the same order at each call.\n        queues = list(self._waiters.keys())\n        random.shuffle(queues)\n\n        for key in queues:\n            if self._available_connections(key) < 1:\n                continue\n\n            waiters = self._waiters[key]\n            while waiters:\n                waiter = waiters.popleft()\n                if not waiter.done():\n                    waiter.set_result(None)\n                    return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose all ongoing DNS calls.", "response": "def close(self) -> Awaitable[None]:\n        \"\"\"Close all ongoing DNS calls.\"\"\"\n        for ev in self._throttle_dns_events.values():\n            ev.cancel()\n\n        return super().close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove specified host or clear all dns local cache.", "response": "def clear_dns_cache(self,\n                        host: Optional[str]=None,\n                        port: Optional[int]=None) -> None:\n        \"\"\"Remove specified host/port or clear all dns local cache.\"\"\"\n        if host is not None and port is not None:\n            self._cached_hosts.remove((host, port))\n        elif host is not None or port is not None:\n            raise ValueError(\"either both host and port \"\n                             \"or none of them are allowed\")\n        else:\n            self._cached_hosts.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _create_connection(self, req: 'ClientRequest',\n                                 traces: List['Trace'],\n                                 timeout: 'ClientTimeout') -> ResponseHandler:\n        \"\"\"Create connection.\n\n        Has same keyword arguments as BaseEventLoop.create_connection.\n        \"\"\"\n        if req.proxy:\n            _, proto = await self._create_proxy_connection(\n                req, traces, timeout)\n        else:\n            _, proto = await self._create_direct_connection(\n                req, traces, timeout)\n\n        return proto", "response": "Create connection.\n\n        Has same keyword arguments as BaseEventLoop.create_connection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_ssl_context(self, req: 'ClientRequest') -> Optional[SSLContext]:\n        if req.is_ssl():\n            if ssl is None:  # pragma: no cover\n                raise RuntimeError('SSL is not supported.')\n            sslcontext = req.ssl\n            if isinstance(sslcontext, ssl.SSLContext):\n                return sslcontext\n            if sslcontext is not None:\n                # not verified or fingerprinted\n                return self._make_ssl_context(False)\n            sslcontext = self._ssl\n            if isinstance(sslcontext, ssl.SSLContext):\n                return sslcontext\n            if sslcontext is not None:\n                # not verified or fingerprinted\n                return self._make_ssl_context(False)\n            return self._make_ssl_context(True)\n        else:\n            return None", "response": "Logic to get the correct SSL context for the given client request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmasking the data according to the Python version.", "response": "def _websocket_mask_python(mask: bytes, data: bytearray) -> None:\n    \"\"\"Websocket masking function.\n\n    `mask` is a `bytes` object of length 4; `data` is a `bytearray`\n    object of any length. The contents of `data` are masked with `mask`,\n    as specified in section 5.3 of RFC 6455.\n\n    Note that this function mutates the `data` argument.\n\n    This pure-python implementation may be replaced by an optimized\n    version when available.\n\n    \"\"\"\n    assert isinstance(data, bytearray), data\n    assert len(mask) == 4, mask\n\n    if data:\n        a, b, c, d = (_XOR_TABLE[n] for n in mask)\n        data[::4] = data[::4].translate(a)\n        data[1::4] = data[1::4].translate(b)\n        data[2::4] = data[2::4].translate(c)\n        data[3::4] = data[3::4].translate(d)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef json(self, *,  # type: ignore\n             loads: Callable[[Any], Any]=json.loads) -> None:\n        \"\"\"Return parsed JSON data.\n\n        .. versionadded:: 0.22\n        \"\"\"\n        return loads(self.data)", "response": "Return parsed JSON data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_frame(self, buf: bytes) -> List[Tuple[bool, Optional[int],\n                                                    bytearray,\n                                                    Optional[bool]]]:\n        \"\"\"Return the next frame from the socket.\"\"\"\n        frames = []\n        if self._tail:\n            buf, self._tail = self._tail + buf, b''\n\n        start_pos = 0\n        buf_length = len(buf)\n\n        while True:\n            # read header\n            if self._state == WSParserState.READ_HEADER:\n                if buf_length - start_pos >= 2:\n                    data = buf[start_pos:start_pos+2]\n                    start_pos += 2\n                    first_byte, second_byte = data\n\n                    fin = (first_byte >> 7) & 1\n                    rsv1 = (first_byte >> 6) & 1\n                    rsv2 = (first_byte >> 5) & 1\n                    rsv3 = (first_byte >> 4) & 1\n                    opcode = first_byte & 0xf\n\n                    # frame-fin = %x0 ; more frames of this message follow\n                    #           / %x1 ; final frame of this message\n                    # frame-rsv1 = %x0 ;\n                    #    1 bit, MUST be 0 unless negotiated otherwise\n                    # frame-rsv2 = %x0 ;\n                    #    1 bit, MUST be 0 unless negotiated otherwise\n                    # frame-rsv3 = %x0 ;\n                    #    1 bit, MUST be 0 unless negotiated otherwise\n                    #\n                    # Remove rsv1 from this test for deflate development\n                    if rsv2 or rsv3 or (rsv1 and not self._compress):\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            'Received frame with non-zero reserved bits')\n\n                    if opcode > 0x7 and fin == 0:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            'Received fragmented control frame')\n\n                    has_mask = (second_byte >> 7) & 1\n                    length = second_byte & 0x7f\n\n                    # Control frames MUST have a payload\n                    # length of 125 bytes or less\n                    if opcode > 0x7 and length > 125:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            'Control frame payload cannot be '\n                            'larger than 125 bytes')\n\n                    # Set compress status if last package is FIN\n                    # OR set compress status if this is first fragment\n                    # Raise error if not first fragment with rsv1 = 0x1\n                    if self._frame_fin or self._compressed is None:\n                        self._compressed = True if rsv1 else False\n                    elif rsv1:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            'Received frame with non-zero reserved bits')\n\n                    self._frame_fin = bool(fin)\n                    self._frame_opcode = opcode\n                    self._has_mask = bool(has_mask)\n                    self._payload_length_flag = length\n                    self._state = WSParserState.READ_PAYLOAD_LENGTH\n                else:\n                    break\n\n            # read payload length\n            if self._state == WSParserState.READ_PAYLOAD_LENGTH:\n                length = self._payload_length_flag\n                if length == 126:\n                    if buf_length - start_pos >= 2:\n                        data = buf[start_pos:start_pos+2]\n                        start_pos += 2\n                        length = UNPACK_LEN2(data)[0]\n                        self._payload_length = length\n                        self._state = (\n                            WSParserState.READ_PAYLOAD_MASK\n                            if self._has_mask\n                            else WSParserState.READ_PAYLOAD)\n                    else:\n                        break\n                elif length > 126:\n                    if buf_length - start_pos >= 8:\n                        data = buf[start_pos:start_pos+8]\n                        start_pos += 8\n                        length = UNPACK_LEN3(data)[0]\n                        self._payload_length = length\n                        self._state = (\n                            WSParserState.READ_PAYLOAD_MASK\n                            if self._has_mask\n                            else WSParserState.READ_PAYLOAD)\n                    else:\n                        break\n                else:\n                    self._payload_length = length\n                    self._state = (\n                        WSParserState.READ_PAYLOAD_MASK\n                        if self._has_mask\n                        else WSParserState.READ_PAYLOAD)\n\n            # read payload mask\n            if self._state == WSParserState.READ_PAYLOAD_MASK:\n                if buf_length - start_pos >= 4:\n                    self._frame_mask = buf[start_pos:start_pos+4]\n                    start_pos += 4\n                    self._state = WSParserState.READ_PAYLOAD\n                else:\n                    break\n\n            if self._state == WSParserState.READ_PAYLOAD:\n                length = self._payload_length\n                payload = self._frame_payload\n\n                chunk_len = buf_length - start_pos\n                if length >= chunk_len:\n                    self._payload_length = length - chunk_len\n                    payload.extend(buf[start_pos:])\n                    start_pos = buf_length\n                else:\n                    self._payload_length = 0\n                    payload.extend(buf[start_pos:start_pos+length])\n                    start_pos = start_pos + length\n\n                if self._payload_length == 0:\n                    if self._has_mask:\n                        assert self._frame_mask is not None\n                        _websocket_mask(self._frame_mask, payload)\n\n                    frames.append((\n                        self._frame_fin,\n                        self._frame_opcode,\n                        payload,\n                        self._compressed))\n\n                    self._frame_payload = bytearray()\n                    self._state = WSParserState.READ_HEADER\n                else:\n                    break\n\n        self._tail = buf[start_pos:]\n\n        return frames", "response": "Parse a single frame from the socket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a frame over the websocket.", "response": "async def _send_frame(self, message: bytes, opcode: int,\n                          compress: Optional[int]=None) -> None:\n        \"\"\"Send a frame over the websocket with message as its payload.\"\"\"\n        if self._closing:\n            ws_logger.warning('websocket connection is closing.')\n\n        rsv = 0\n\n        # Only compress larger packets (disabled)\n        # Does small packet needs to be compressed?\n        # if self.compress and opcode < 8 and len(message) > 124:\n        if (compress or self.compress) and opcode < 8:\n            if compress:\n                # Do not set self._compress if compressing is for this frame\n                compressobj = zlib.compressobj(wbits=-compress)\n            else:  # self.compress\n                if not self._compressobj:\n                    self._compressobj = zlib.compressobj(wbits=-self.compress)\n                compressobj = self._compressobj\n\n            message = compressobj.compress(message)\n            message = message + compressobj.flush(\n                zlib.Z_FULL_FLUSH if self.notakeover else zlib.Z_SYNC_FLUSH)\n            if message.endswith(_WS_DEFLATE_TRAILING):\n                message = message[:-4]\n            rsv = rsv | 0x40\n\n        msg_length = len(message)\n\n        use_mask = self.use_mask\n        if use_mask:\n            mask_bit = 0x80\n        else:\n            mask_bit = 0\n\n        if msg_length < 126:\n            header = PACK_LEN1(0x80 | rsv | opcode, msg_length | mask_bit)\n        elif msg_length < (1 << 16):\n            header = PACK_LEN2(0x80 | rsv | opcode, 126 | mask_bit, msg_length)\n        else:\n            header = PACK_LEN3(0x80 | rsv | opcode, 127 | mask_bit, msg_length)\n        if use_mask:\n            mask = self.randrange(0, 0xffffffff)\n            mask = mask.to_bytes(4, 'big')\n            message = bytearray(message)\n            _websocket_mask(mask, message)\n            self.transport.write(header + mask + message)\n            self._output_size += len(header) + len(mask) + len(message)\n        else:\n            if len(message) > MSG_SIZE:\n                self.transport.write(header)\n                self.transport.write(message)\n            else:\n                self.transport.write(header + message)\n\n            self._output_size += len(header) + len(message)\n\n        if self._output_size > self._limit:\n            self._output_size = 0\n            await self.protocol._drain_helper()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a pong message.", "response": "async def pong(self, message: bytes=b'') -> None:\n        \"\"\"Send pong message.\"\"\"\n        if isinstance(message, str):\n            message = message.encode('utf-8')\n        await self._send_frame(message, WSMsgType.PONG)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a frame over the websocket with message as its payload.", "response": "async def send(self, message: Union[str, bytes],\n                   binary: bool=False,\n                   compress: Optional[int]=None) -> None:\n        \"\"\"Send a frame over the websocket with message as its payload.\"\"\"\n        if isinstance(message, str):\n            message = message.encode('utf-8')\n        if binary:\n            await self._send_frame(message, WSMsgType.BINARY, compress)\n        else:\n            await self._send_frame(message, WSMsgType.TEXT, compress)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def close(self, code: int=1000, message: bytes=b'') -> None:\n        if isinstance(message, str):\n            message = message.encode('utf-8')\n        try:\n            await self._send_frame(\n                PACK_CLOSE_CODE(code) + message, opcode=WSMsgType.CLOSE)\n        finally:\n            self._closing = True", "response": "Close the websocket sending the specified code and message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter_cookies(self, request_url: URL=URL()) -> 'BaseCookie[str]':\n        self._do_expiration()\n        request_url = URL(request_url)\n        filtered = SimpleCookie()\n        hostname = request_url.raw_host or \"\"\n        is_not_secure = request_url.scheme not in (\"https\", \"wss\")\n\n        for cookie in self:\n            name = cookie.key\n            domain = cookie[\"domain\"]\n\n            # Send shared cookies\n            if not domain:\n                filtered[name] = cookie.value\n                continue\n\n            if not self._unsafe and is_ip_address(hostname):\n                continue\n\n            if (domain, name) in self._host_only_cookies:\n                if domain != hostname:\n                    continue\n            elif not self._is_domain_match(domain, hostname):\n                continue\n\n            if not self._is_path_match(request_url.path, cookie[\"path\"]):\n                continue\n\n            if is_not_secure and cookie[\"secure\"]:\n                continue\n\n            # It's critical we use the Morsel so the coded_value\n            # (based on cookie version) is preserved\n            mrsl_val = cast('Morsel[str]', cookie.get(cookie.key, Morsel()))\n            mrsl_val.set(cookie.key, cookie.value, cookie.coded_value)\n            filtered[name] = mrsl_val\n\n        return filtered", "response": "Returns this jar s cookies filtered by their attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _is_domain_match(domain: str, hostname: str) -> bool:\n        if hostname == domain:\n            return True\n\n        if not hostname.endswith(domain):\n            return False\n\n        non_matching = hostname[:-len(domain)]\n\n        if not non_matching.endswith(\".\"):\n            return False\n\n        return not is_ip_address(hostname)", "response": "Implements domain matching adhering to RFC 6265."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_path_match(req_path: str, cookie_path: str) -> bool:\n        if not req_path.startswith(\"/\"):\n            req_path = \"/\"\n\n        if req_path == cookie_path:\n            return True\n\n        if not req_path.startswith(cookie_path):\n            return False\n\n        if cookie_path.endswith(\"/\"):\n            return True\n\n        non_matching = req_path[len(cookie_path):]\n\n        return non_matching.startswith(\"/\")", "response": "Implements path matching adhering to RFC 6265."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_date(cls, date_str: str) -> Optional[datetime.datetime]:\n        if not date_str:\n            return None\n\n        found_time = False\n        found_day = False\n        found_month = False\n        found_year = False\n\n        hour = minute = second = 0\n        day = 0\n        month = 0\n        year = 0\n\n        for token_match in cls.DATE_TOKENS_RE.finditer(date_str):\n\n            token = token_match.group(\"token\")\n\n            if not found_time:\n                time_match = cls.DATE_HMS_TIME_RE.match(token)\n                if time_match:\n                    found_time = True\n                    hour, minute, second = [\n                        int(s) for s in time_match.groups()]\n                    continue\n\n            if not found_day:\n                day_match = cls.DATE_DAY_OF_MONTH_RE.match(token)\n                if day_match:\n                    found_day = True\n                    day = int(day_match.group())\n                    continue\n\n            if not found_month:\n                month_match = cls.DATE_MONTH_RE.match(token)\n                if month_match:\n                    found_month = True\n                    month = month_match.lastindex\n                    continue\n\n            if not found_year:\n                year_match = cls.DATE_YEAR_RE.match(token)\n                if year_match:\n                    found_year = True\n                    year = int(year_match.group())\n\n        if 70 <= year <= 99:\n            year += 1900\n        elif 0 <= year <= 69:\n            year += 2000\n\n        if False in (found_day, found_month, found_year, found_time):\n            return None\n\n        if not 1 <= day <= 31:\n            return None\n\n        if year < 1601 or hour > 23 or minute > 59 or second > 59:\n            return None\n\n        return datetime.datetime(year, month, day,\n                                 hour, minute, second,\n                                 tzinfo=datetime.timezone.utc)", "response": "Implements date string parsing adhering to RFC 6265."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_content_disposition(self,\n                                disptype: str,\n                                quote_fields: bool=True,\n                                **params: Any) -> None:\n        \"\"\"Sets ``Content-Disposition`` header.\"\"\"\n        self._headers[hdrs.CONTENT_DISPOSITION] = content_disposition_header(\n            disptype, quote_fields=quote_fields, **params)", "response": "Sets the Content - Disposition header."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clone(self, *, method: str=sentinel, rel_url: StrOrURL=sentinel,\n              headers: LooseHeaders=sentinel, scheme: str=sentinel,\n              host: str=sentinel,\n              remote: str=sentinel) -> 'BaseRequest':\n        \"\"\"Clone itself with replacement some attributes.\n\n        Creates and returns a new instance of Request object. If no parameters\n        are given, an exact copy is returned. If a parameter is not passed, it\n        will reuse the one from the current request object.\n\n        \"\"\"\n\n        if self._read_bytes:\n            raise RuntimeError(\"Cannot clone request \"\n                               \"after reading its content\")\n\n        dct = {}  # type: Dict[str, Any]\n        if method is not sentinel:\n            dct['method'] = method\n        if rel_url is not sentinel:\n            new_url = URL(rel_url)\n            dct['url'] = new_url\n            dct['path'] = str(new_url)\n        if headers is not sentinel:\n            # a copy semantic\n            dct['headers'] = CIMultiDictProxy(CIMultiDict(headers))\n            dct['raw_headers'] = tuple((k.encode('utf-8'), v.encode('utf-8'))\n                                       for k, v in headers.items())\n\n        message = self._message._replace(**dct)\n\n        kwargs = {}\n        if scheme is not sentinel:\n            kwargs['scheme'] = scheme\n        if host is not sentinel:\n            kwargs['host'] = host\n        if remote is not sentinel:\n            kwargs['remote'] = remote\n\n        return self.__class__(\n            message,\n            self._payload,\n            self._protocol,\n            self._payload_writer,\n            self._task,\n            self._loop,\n            client_max_size=self._client_max_size,\n            state=self._state.copy(),\n            **kwargs)", "response": "Creates and returns a new instance of this Request object with replacement some attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef host(self) -> str:\n        host = self._message.headers.get(hdrs.HOST)\n        if host is not None:\n            return host\n        else:\n            return socket.getfqdn()", "response": "Hostname of the request."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remote(self) -> Optional[str]:\n        if isinstance(self._transport_peername, (list, tuple)):\n            return self._transport_peername[0]\n        else:\n            return self._transport_peername", "response": "Remote IP of client initiated HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a date string and return a datetime object", "response": "def _http_date(_date_str: str) -> Optional[datetime.datetime]:\n        \"\"\"Process a date string, return a datetime object\n        \"\"\"\n        if _date_str is not None:\n            timetuple = parsedate(_date_str)\n            if timetuple is not None:\n                return datetime.datetime(*timetuple[:6],\n                                         tzinfo=datetime.timezone.utc)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef if_modified_since(self) -> Optional[datetime.datetime]:\n        return self._http_date(self.headers.get(hdrs.IF_MODIFIED_SINCE))", "response": "The value of If - Modified - Since HTTP header or None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef if_unmodified_since(self) -> Optional[datetime.datetime]:\n        return self._http_date(self.headers.get(hdrs.IF_UNMODIFIED_SINCE))", "response": "The value of If - Unmodified - Since HTTP header or None."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef if_range(self) -> Optional[datetime.datetime]:\n        return self._http_date(self.headers.get(hdrs.IF_RANGE))", "response": "The value of If - Range HTTP header or None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn request cookies. A read - only dictionary - like object.", "response": "def cookies(self) -> Mapping[str, str]:\n        \"\"\"Return request cookies.\n\n        A read-only dictionary-like object.\n        \"\"\"\n        raw = self.headers.get(hdrs.COOKIE, '')\n        parsed = SimpleCookie(raw)\n        return MappingProxyType(\n            {key: val.value for key, val in parsed.items()})"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef http_range(self) -> slice:\n        rng = self._headers.get(hdrs.RANGE)\n        start, end = None, None\n        if rng is not None:\n            try:\n                pattern = r'^bytes=(\\d*)-(\\d*)$'\n                start, end = re.findall(pattern, rng)[0]\n            except IndexError:  # pattern was not found in header\n                raise ValueError(\"range not in acceptable format\")\n\n            end = int(end) if end else None\n            start = int(start) if start else None\n\n            if start is None and end is not None:\n                # end with no start is to return tail of content\n                start = -end\n                end = None\n\n            if start is not None and end is not None:\n                # end is inclusive in range header, exclusive for slice\n                end += 1\n\n                if start >= end:\n                    raise ValueError('start cannot be after end')\n\n            if start is end is None:  # No valid range supplied\n                raise ValueError('No start or end of range specified')\n\n        return slice(start, end, 1)", "response": "The content of Range HTTP header."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the HTTP BODY can be read False otherwise.", "response": "def has_body(self) -> bool:\n        \"\"\"Return True if request's HTTP BODY can be read, False otherwise.\"\"\"\n        warnings.warn(\n            \"Deprecated, use .can_read_body #2005\",\n            DeprecationWarning, stacklevel=2)\n        return not self._payload.at_eof()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading request body if present. Returns bytes object with full request content.", "response": "async def read(self) -> bytes:\n        \"\"\"Read request body if present.\n\n        Returns bytes object with full request content.\n        \"\"\"\n        if self._read_bytes is None:\n            body = bytearray()\n            while True:\n                chunk = await self._payload.readany()\n                body.extend(chunk)\n                if self._client_max_size:\n                    body_size = len(body)\n                    if body_size >= self._client_max_size:\n                        raise HTTPRequestEntityTooLarge(\n                            max_size=self._client_max_size,\n                            actual_size=body_size\n                        )\n                if not chunk:\n                    break\n            self._read_bytes = bytes(body)\n        return self._read_bytes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning BODY as text using encoding from. charset.", "response": "async def text(self) -> str:\n        \"\"\"Return BODY as text using encoding from .charset.\"\"\"\n        bytes_body = await self.read()\n        encoding = self.charset or 'utf-8'\n        return bytes_body.decode(encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def json(self, *, loads: JSONDecoder=DEFAULT_JSON_DECODER) -> Any:\n        body = await self.text()\n        return loads(body)", "response": "Return BODY as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets keep - alive connection mode.", "response": "def keep_alive(self, val: bool) -> None:\n        \"\"\"Set keep-alive connection mode.\n\n        :param bool val: new state.\n        \"\"\"\n        self._keepalive = val\n        if self._keepalive_handle:\n            self._keepalive_handle.cancel()\n            self._keepalive_handle = None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self) -> None:\n        self._close = True\n        if self._waiter:\n            self._waiter.cancel()", "response": "Stop accepting new pipelinig messages and close the connection when handlers done processing messages."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def start(self) -> None:\n        loop = self._loop\n        handler = self._task_handler\n        assert handler is not None\n        manager = self._manager\n        assert manager is not None\n        keepalive_timeout = self._keepalive_timeout\n        resp = None\n        assert self._request_factory is not None\n        assert self._request_handler is not None\n\n        while not self._force_close:\n            if not self._messages:\n                try:\n                    # wait for next request\n                    self._waiter = loop.create_future()\n                    await self._waiter\n                except asyncio.CancelledError:\n                    break\n                finally:\n                    self._waiter = None\n\n            message, payload = self._messages.popleft()\n\n            if self.access_log:\n                now = loop.time()\n\n            manager.requests_count += 1\n            writer = StreamWriter(self, loop)\n            request = self._request_factory(\n                message, payload, self, writer, handler)\n            try:\n                try:\n                    # a new task is used for copy context vars (#3406)\n                    task = self._loop.create_task(\n                        self._request_handler(request))\n                    resp = await task\n                except HTTPException as exc:\n                    resp = Response(status=exc.status,\n                                    reason=exc.reason,\n                                    text=exc.text,\n                                    headers=exc.headers)\n                except asyncio.CancelledError:\n                    self.log_debug('Ignored premature client disconnection')\n                    break\n                except asyncio.TimeoutError as exc:\n                    self.log_debug('Request handler timed out.', exc_info=exc)\n                    resp = self.handle_error(request, 504)\n                except Exception as exc:\n                    resp = self.handle_error(request, 500, exc)\n\n                try:\n                    prepare_meth = resp.prepare\n                except AttributeError:\n                    if resp is None:\n                        raise RuntimeError(\"Missing return \"\n                                           \"statement on request handler\")\n                    else:\n                        raise RuntimeError(\"Web-handler should return \"\n                                           \"a response instance, \"\n                                           \"got {!r}\".format(resp))\n                await prepare_meth(request)\n                await resp.write_eof()\n\n                # notify server about keep-alive\n                self._keepalive = bool(resp.keep_alive)\n\n                # log access\n                if self.access_log:\n                    self.log_access(request, resp, loop.time() - now)\n\n                # check payload\n                if not payload.is_eof():\n                    lingering_time = self._lingering_time\n                    if not self._force_close and lingering_time:\n                        self.log_debug(\n                            'Start lingering close timer for %s sec.',\n                            lingering_time)\n\n                        now = loop.time()\n                        end_t = now + lingering_time\n\n                        with suppress(\n                                asyncio.TimeoutError, asyncio.CancelledError):\n                            while not payload.is_eof() and now < end_t:\n                                with CeilTimeout(end_t - now, loop=loop):\n                                    # read and ignore\n                                    await payload.readany()\n                                now = loop.time()\n\n                    # if payload still uncompleted\n                    if not payload.is_eof() and not self._force_close:\n                        self.log_debug('Uncompleted request.')\n                        self.close()\n\n                payload.set_exception(PayloadAccessError())\n\n            except asyncio.CancelledError:\n                self.log_debug('Ignored premature client disconnection ')\n                break\n            except RuntimeError as exc:\n                if self.debug:\n                    self.log_exception(\n                        'Unhandled runtime exception', exc_info=exc)\n                self.force_close()\n            except Exception as exc:\n                self.log_exception('Unhandled exception', exc_info=exc)\n                self.force_close()\n            finally:\n                if self.transport is None and resp is not None:\n                    self.log_debug('Ignored premature client disconnection.')\n                elif not self._force_close:\n                    if self._keepalive and not self._close:\n                        # start keep-alive timer\n                        if keepalive_timeout is not None:\n                            now = self._loop.time()\n                            self._keepalive_time = now\n                            if self._keepalive_handle is None:\n                                self._keepalive_handle = loop.call_at(\n                                    now + keepalive_timeout,\n                                    self._process_keepalive)\n                    else:\n                        break\n\n        # remove handler, close transport if no handlers left\n        if not self._force_close:\n            self._task_handler = None\n            if self.transport is not None and self._error_handler is None:\n                self.transport.close()", "response": "Start processing incoming requests."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_error(self,\n                     request: BaseRequest,\n                     status: int=500,\n                     exc: Optional[BaseException]=None,\n                     message: Optional[str]=None) -> StreamResponse:\n        \"\"\"Handle errors.\n\n        Returns HTTP response with specific status code. Logs additional\n        information. It always closes current connection.\"\"\"\n        self.log_exception(\"Error handling request\", exc_info=exc)\n\n        ct = 'text/plain'\n        if status == HTTPStatus.INTERNAL_SERVER_ERROR:\n            title = '{0.value} {0.phrase}'.format(\n                HTTPStatus.INTERNAL_SERVER_ERROR\n            )\n            msg = HTTPStatus.INTERNAL_SERVER_ERROR.description\n            tb = None\n            if self.debug:\n                with suppress(Exception):\n                    tb = traceback.format_exc()\n\n            if 'text/html' in request.headers.get('Accept', ''):\n                if tb:\n                    tb = html_escape(tb)\n                    msg = '<h2>Traceback:</h2>\\n<pre>{}</pre>'.format(tb)\n                message = (\n                    \"<html><head>\"\n                    \"<title>{title}</title>\"\n                    \"</head><body>\\n<h1>{title}</h1>\"\n                    \"\\n{msg}\\n</body></html>\\n\"\n                ).format(title=title, msg=msg)\n                ct = 'text/html'\n            else:\n                if tb:\n                    msg = tb\n                message = title + '\\n\\n' + msg\n\n        resp = Response(status=status, text=message, content_type=ct)\n        resp.force_close()\n\n        # some data already got sent, connection is broken\n        if request.writer.output_size > 0 or self.transport is None:\n            self.force_close()\n\n        return resp", "response": "Handles errors. Returns HTTP response with specific status code. Logs additional information. It always closes current connection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_app(app: Union[Application, Awaitable[Application]], *,\n            host: Optional[str]=None,\n            port: Optional[int]=None,\n            path: Optional[str]=None,\n            sock: Optional[socket.socket]=None,\n            shutdown_timeout: float=60.0,\n            ssl_context: Optional[SSLContext]=None,\n            print: Optional[Callable[..., None]]=print,\n            backlog: int=128,\n            access_log_class: Type[AbstractAccessLogger]=AccessLogger,\n            access_log_format: str=AccessLogger.LOG_FORMAT,\n            access_log: Optional[logging.Logger]=access_logger,\n            handle_signals: bool=True,\n            reuse_address: Optional[bool]=None,\n            reuse_port: Optional[bool]=None) -> None:\n    \"\"\"Run an app locally\"\"\"\n    loop = asyncio.get_event_loop()\n\n    # Configure if and only if in debugging mode and using the default logger\n    if loop.get_debug() and access_log and access_log.name == 'aiohttp.access':\n        if access_log.level == logging.NOTSET:\n            access_log.setLevel(logging.DEBUG)\n        if not access_log.hasHandlers():\n            access_log.addHandler(logging.StreamHandler())\n\n    try:\n        loop.run_until_complete(_run_app(app,\n                                         host=host,\n                                         port=port,\n                                         path=path,\n                                         sock=sock,\n                                         shutdown_timeout=shutdown_timeout,\n                                         ssl_context=ssl_context,\n                                         print=print,\n                                         backlog=backlog,\n                                         access_log_class=access_log_class,\n                                         access_log_format=access_log_format,\n                                         access_log=access_log,\n                                         handle_signals=handle_signals,\n                                         reuse_address=reuse_address,\n                                         reuse_port=reuse_port))\n    except (GracefulExit, KeyboardInterrupt):  # pragma: no cover\n        pass\n    finally:\n        _cancel_all_tasks(loop)\n        if sys.version_info >= (3, 6):  # don't use PY_36 to pass mypy\n            loop.run_until_complete(loop.shutdown_asyncgens())\n        loop.close()", "response": "Run an application locally."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_chunked(self, n: int) -> AsyncStreamIterator[bytes]:\n        return AsyncStreamIterator(lambda: self.read(n))", "response": "Returns an asynchronous iterator that yields chunks of size n."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a chunk of HTTP data from the HTTP server.", "response": "async def readchunk(self) -> Tuple[bytes, bool]:\n        \"\"\"Returns a tuple of (data, end_of_http_chunk). When chunked transfer\n        encoding is used, end_of_http_chunk is a boolean indicating if the end\n        of the data corresponds to the end of a HTTP chunk , otherwise it is\n        always False.\n        \"\"\"\n        while True:\n            if self._exception is not None:\n                raise self._exception\n\n            while self._http_chunk_splits:\n                pos = self._http_chunk_splits.pop(0)\n                if pos == self._cursor:\n                    return (b\"\", True)\n                if pos > self._cursor:\n                    return (self._read_nowait(pos-self._cursor), True)\n                internal_logger.warning('Skipping HTTP chunk end due to data '\n                                        'consumption beyond chunk boundary')\n\n            if self._buffer:\n                return (self._read_nowait_chunk(-1), False)\n                # return (self._read_nowait(-1), False)\n\n            if self._eof:\n                # Special case for signifying EOF.\n                # (b'', True) is not a final return value actually.\n                return (b'', False)\n\n            await self._wait('readchunk')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading n bytes from the buffer and return them as a string.", "response": "def _read_nowait(self, n: int) -> bytes:\n        \"\"\" Read not more than n bytes, or whole buffer is n == -1 \"\"\"\n        chunks = []\n\n        while self._buffer:\n            chunk = self._read_nowait_chunk(n)\n            chunks.append(chunk)\n            if n != -1:\n                n -= len(chunk)\n                if n == 0:\n                    break\n\n        return b''.join(chunks) if chunks else b''"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def send(self, *args, **kwargs):\n        if not self.frozen:\n            raise RuntimeError(\"Cannot send non-frozen signal.\")\n\n        for receiver in self:\n            await receiver(*args, **kwargs)", "response": "Sends data to all registered receivers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntranslate log_format into form usable by modulo formatting Jobs.", "response": "def compile_format(self, log_format: str) -> Tuple[str, List[KeyMethod]]:\n        \"\"\"Translate log_format into form usable by modulo formatting\n\n        All known atoms will be replaced with %s\n        Also methods for formatting of those atoms will be added to\n        _methods in appropriate order\n\n        For example we have log_format = \"%a %t\"\n        This format will be translated to \"%s %s\"\n        Also contents of _methods will be\n        [self._format_a, self._format_t]\n        These method will be called and results will be passed\n        to translated string format.\n\n        Each _format_* method receive 'args' which is list of arguments\n        given to self.log\n\n        Exceptions are _format_e, _format_i and _format_o methods which\n        also receive key name (by functools.partial)\n\n        \"\"\"\n        # list of (key, method) tuples, we don't use an OrderedDict as users\n        # can repeat the same key more than once\n        methods = list()\n\n        for atom in self.FORMAT_RE.findall(log_format):\n            if atom[1] == '':\n                format_key1 = self.LOG_FORMAT_MAP[atom[0]]\n                m = getattr(AccessLogger, '_format_%s' % atom[0])\n                key_method = KeyMethod(format_key1, m)\n            else:\n                format_key2 = (self.LOG_FORMAT_MAP[atom[2]], atom[1])\n                m = getattr(AccessLogger, '_format_%s' % atom[2])\n                key_method = KeyMethod(format_key2,\n                                       functools.partial(m, atom[1]))\n\n            methods.append(key_method)\n\n        log_format = self.FORMAT_RE.sub(r'%s', log_format)\n        log_format = self.CLEANUP_RE.sub(r'%\\1', log_format)\n        return log_format, methods"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize_path_middleware(\n        *, append_slash: bool=True, remove_slash: bool=False,\n        merge_slashes: bool=True,\n        redirect_class: Type[HTTPMove]=HTTPMovedPermanently) -> _Middleware:\n    \"\"\"\n    Middleware factory which produces a middleware that normalizes\n    the path of a request. By normalizing it means:\n\n        - Add or remove a trailing slash to the path.\n        - Double slashes are replaced by one.\n\n    The middleware returns as soon as it finds a path that resolves\n    correctly. The order if both merge and append/remove are enabled is\n        1) merge slashes\n        2) append/remove slash\n        3) both merge slashes and append/remove slash.\n    If the path resolves with at least one of those conditions, it will\n    redirect to the new path.\n\n    Only one of `append_slash` and `remove_slash` can be enabled. If both\n    are `True` the factory will raise an assertion error\n\n    If `append_slash` is `True` the middleware will append a slash when\n    needed. If a resource is defined with trailing slash and the request\n    comes without it, it will append it automatically.\n\n    If `remove_slash` is `True`, `append_slash` must be `False`. When enabled\n    the middleware will remove trailing slashes and redirect if the resource\n    is defined\n\n    If merge_slashes is True, merge multiple consecutive slashes in the\n    path into one.\n    \"\"\"\n\n    correct_configuration = not (append_slash and remove_slash)\n    assert correct_configuration, \"Cannot both remove and append slash\"\n\n    @middleware\n    async def impl(request: Request, handler: _Handler) -> StreamResponse:\n        if isinstance(request.match_info.route, SystemRoute):\n            paths_to_check = []\n            if '?' in request.raw_path:\n                path, query = request.raw_path.split('?', 1)\n                query = '?' + query\n            else:\n                query = ''\n                path = request.raw_path\n\n            if merge_slashes:\n                paths_to_check.append(re.sub('//+', '/', path))\n            if append_slash and not request.path.endswith('/'):\n                paths_to_check.append(path + '/')\n            if remove_slash and request.path.endswith('/'):\n                paths_to_check.append(path[:-1])\n            if merge_slashes and append_slash:\n                paths_to_check.append(\n                    re.sub('//+', '/', path + '/'))\n            if merge_slashes and remove_slash and path.endswith('/'):\n                merged_slashes = re.sub('//+', '/', path)\n                paths_to_check.append(merged_slashes[:-1])\n\n            for path in paths_to_check:\n                resolves, request = await _check_request_resolves(\n                    request, path)\n                if resolves:\n                    raise redirect_class(request.raw_path + query)\n\n        return await handler(request)\n\n    return impl", "response": "This function returns a middleware that normalizes the path of a request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _gen_form_data(self) -> multipart.MultipartWriter:\n        for dispparams, headers, value in self._fields:\n            try:\n                if hdrs.CONTENT_TYPE in headers:\n                    part = payload.get_payload(\n                        value, content_type=headers[hdrs.CONTENT_TYPE],\n                        headers=headers, encoding=self._charset)\n                else:\n                    part = payload.get_payload(\n                        value, headers=headers, encoding=self._charset)\n            except Exception as exc:\n                raise TypeError(\n                    'Can not serialize value type: %r\\n '\n                    'headers: %r\\n value: %r' % (\n                        type(value), headers, value)) from exc\n\n            if dispparams:\n                part.set_content_disposition(\n                    'form-data', quote_fields=self._quote_fields, **dispparams\n                )\n                # FIXME cgi.FieldStorage doesn't likes body parts with\n                # Content-Length which were sent via chunked transfer encoding\n                assert part.headers is not None\n                part.headers.popall(hdrs.CONTENT_LENGTH, None)\n\n            self._writer.append_payload(part)\n\n        return self._writer", "response": "Generate a list of fields using the multipart - data MIME format"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a chunk of data to a file.", "response": "async def write(self, chunk: bytes,\n                    *, drain: bool=True, LIMIT: int=0x10000) -> None:\n        \"\"\"Writes chunk of data to a stream.\n\n        write_eof() indicates end of stream.\n        writer can't be used after write_eof() method being called.\n        write() return drain future.\n        \"\"\"\n        if self._on_chunk_sent is not None:\n            await self._on_chunk_sent(chunk)\n\n        if self._compress is not None:\n            chunk = self._compress.compress(chunk)\n            if not chunk:\n                return\n\n        if self.length is not None:\n            chunk_len = len(chunk)\n            if self.length >= chunk_len:\n                self.length = self.length - chunk_len\n            else:\n                chunk = chunk[:self.length]\n                self.length = 0\n                if not chunk:\n                    return\n\n        if chunk:\n            if self.chunked:\n                chunk_len_pre = ('%x\\r\\n' % len(chunk)).encode('ascii')\n                chunk = chunk_len_pre + chunk + b'\\r\\n'\n\n            self._write(chunk)\n\n            if self.buffer_size > LIMIT and drain:\n                self.buffer_size = 0\n                await self.drain()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite status and headers.", "response": "async def write_headers(self, status_line: str,\n                            headers: 'CIMultiDict[str]') -> None:\n        \"\"\"Write request/response status and headers.\"\"\"\n        # status + headers\n        buf = _serialize_headers(status_line, headers)\n        self._write(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattempting to load the netrc file from the path specified by the env - varNETRC NETRC or in the default location.", "response": "def netrc_from_env() -> Optional[netrc.netrc]:\n    \"\"\"Attempt to load the netrc file from the path specified by the env-var\n    NETRC or in the default location in the user's home directory.\n\n    Returns None if it couldn't be found or fails to parse.\n    \"\"\"\n    netrc_env = os.environ.get('NETRC')\n\n    if netrc_env is not None:\n        netrc_path = Path(netrc_env)\n    else:\n        try:\n            home_dir = Path.home()\n        except RuntimeError as e:  # pragma: no cover\n            # if pathlib can't resolve home, it may raise a RuntimeError\n            client_logger.debug('Could not resolve home directory when '\n                                'trying to look for .netrc file: %s', e)\n            return None\n\n        netrc_path = home_dir / (\n            '_netrc' if platform.system() == 'Windows' else '.netrc')\n\n    try:\n        return netrc.netrc(str(netrc_path))\n    except netrc.NetrcParseError as e:\n        client_logger.warning('Could not parse .netrc file: %s', e)\n    except OSError as e:\n        # we couldn't read the file (doesn't exist, permissions, etc.)\n        if netrc_env or netrc_path.is_file():\n            # only warn if the environment wanted us to load it,\n            # or it appears like the default file does actually exist\n            client_logger.warning('Could not read .netrc file: %s', e)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a MIME type string into its components.", "response": "def parse_mimetype(mimetype: str) -> MimeType:\n    \"\"\"Parses a MIME type into its components.\n\n    mimetype is a MIME type string.\n\n    Returns a MimeType object.\n\n    Example:\n\n    >>> parse_mimetype('text/html; charset=utf-8')\n    MimeType(type='text', subtype='html', suffix='',\n             parameters={'charset': 'utf-8'})\n\n    \"\"\"\n    if not mimetype:\n        return MimeType(type='', subtype='', suffix='',\n                        parameters=MultiDictProxy(MultiDict()))\n\n    parts = mimetype.split(';')\n    params = MultiDict()  # type: MultiDict[str]\n    for item in parts[1:]:\n        if not item:\n            continue\n        key, value = cast(Tuple[str, str],\n                          item.split('=', 1) if '=' in item else (item, ''))\n        params.add(key.lower().strip(), value.strip(' \"'))\n\n    fulltype = parts[0].strip().lower()\n    if fulltype == '*':\n        fulltype = '*/*'\n\n    mtype, stype = (cast(Tuple[str, str], fulltype.split('/', 1))\n                    if '/' in fulltype else (fulltype, ''))\n    stype, suffix = (cast(Tuple[str, str], stype.split('+', 1))\n                     if '+' in stype else (stype, ''))\n\n    return MimeType(type=mtype, subtype=stype, suffix=suffix,\n                    parameters=MultiDictProxy(params))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the Content - Disposition header.", "response": "def content_disposition_header(disptype: str,\n                               quote_fields: bool=True,\n                               **params: str) -> str:\n    \"\"\"Sets ``Content-Disposition`` header.\n\n    disptype is a disposition type: inline, attachment, form-data.\n    Should be valid extension token (see RFC 2183)\n\n    params is a dict with disposition params.\n    \"\"\"\n    if not disptype or not (TOKEN > set(disptype)):\n        raise ValueError('bad content disposition type {!r}'\n                         ''.format(disptype))\n\n    value = disptype\n    if params:\n        lparams = []\n        for key, val in params.items():\n            if not key or not (TOKEN > set(key)):\n                raise ValueError('bad content disposition parameter'\n                                 ' {!r}={!r}'.format(key, val))\n            qval = quote(val, '') if quote_fields else val\n            lparams.append((key, '\"%s\"' % qval))\n            if key == 'filename':\n                lparams.append(('filename*', \"utf-8''\" + qval))\n        sparams = '; '.join('='.join(pair) for pair in lparams)\n        value = '; '.join((value, sparams))\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode(cls, auth_header: str, encoding: str='latin1') -> 'BasicAuth':\n        try:\n            auth_type, encoded_credentials = auth_header.split(' ', 1)\n        except ValueError:\n            raise ValueError('Could not parse authorization header.')\n\n        if auth_type.lower() != 'basic':\n            raise ValueError('Unknown authorization method %s' % auth_type)\n\n        try:\n            decoded = base64.b64decode(\n                encoded_credentials.encode('ascii'), validate=True\n            ).decode(encoding)\n        except binascii.Error:\n            raise ValueError('Invalid base64 encoding.')\n\n        try:\n            # RFC 2617 HTTP Authentication\n            # https://www.ietf.org/rfc/rfc2617.txt\n            # the colon must be present, but the username and password may be\n            # otherwise blank.\n            username, password = decoded.split(':', 1)\n        except ValueError:\n            raise ValueError('Invalid credentials.')\n\n        return cls(username, password, encoding=encoding)", "response": "Create a BasicAuth object from an Authorization HTTP header."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate BasicAuth instance from url.", "response": "def from_url(cls, url: URL,\n                 *, encoding: str='latin1') -> Optional['BasicAuth']:\n        \"\"\"Create BasicAuth from url.\"\"\"\n        if not isinstance(url, URL):\n            raise TypeError(\"url should be yarl.URL instance\")\n        if url.user is None:\n            return None\n        return cls(url.user, url.password or '', encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef content_type(self) -> str:\n        raw = self._headers.get(hdrs.CONTENT_TYPE)  # type: ignore\n        if self._stored_content_type != raw:\n            self._parse_content_type(raw)\n        return self._content_type", "response": "The value of content part for Content - Type HTTP header."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request(\n        method: str,\n        url: StrOrURL, *,\n        params: Optional[Mapping[str, str]]=None,\n        data: Any=None,\n        json: Any=None,\n        headers: LooseHeaders=None,\n        skip_auto_headers: Optional[Iterable[str]]=None,\n        auth: Optional[BasicAuth]=None,\n        allow_redirects: bool=True,\n        max_redirects: int=10,\n        compress: Optional[str]=None,\n        chunked: Optional[bool]=None,\n        expect100: bool=False,\n        raise_for_status: Optional[bool]=None,\n        read_until_eof: bool=True,\n        proxy: Optional[StrOrURL]=None,\n        proxy_auth: Optional[BasicAuth]=None,\n        timeout: Union[ClientTimeout, object]=sentinel,\n        cookies: Optional[LooseCookies]=None,\n        version: HttpVersion=http.HttpVersion11,\n        connector: Optional[BaseConnector]=None,\n        loop: Optional[asyncio.AbstractEventLoop]=None\n) -> _SessionRequestContextManager:\n    \"\"\"Constructs and sends a request. Returns response object.\n    method - HTTP method\n    url - request url\n    params - (optional) Dictionary or bytes to be sent in the query\n      string of the new request\n    data - (optional) Dictionary, bytes, or file-like object to\n      send in the body of the request\n    json - (optional) Any json compatible python object\n    headers - (optional) Dictionary of HTTP Headers to send with\n      the request\n    cookies - (optional) Dict object to send with the request\n    auth - (optional) BasicAuth named tuple represent HTTP Basic Auth\n    auth - aiohttp.helpers.BasicAuth\n    allow_redirects - (optional) If set to False, do not follow\n      redirects\n    version - Request HTTP version.\n    compress - Set to True if request has to be compressed\n       with deflate encoding.\n    chunked - Set to chunk size for chunked transfer encoding.\n    expect100 - Expect 100-continue response from server.\n    connector - BaseConnector sub-class instance to support\n       connection pooling.\n    read_until_eof - Read response until eof if response\n       does not have Content-Length header.\n    loop - Optional event loop.\n    timeout - Optional ClientTimeout settings structure, 5min\n       total timeout by default.\n    Usage::\n      >>> import aiohttp\n      >>> resp = await aiohttp.request('GET', 'http://python.org/')\n      >>> resp\n      <ClientResponse(python.org/) [200]>\n      >>> data = await resp.read()\n    \"\"\"\n    connector_owner = False\n    if connector is None:\n        connector_owner = True\n        connector = TCPConnector(loop=loop, force_close=True)\n\n    session = ClientSession(\n        loop=loop, cookies=cookies, version=version, timeout=timeout,\n        connector=connector, connector_owner=connector_owner)\n\n    return _SessionRequestContextManager(\n        session._request(method, url,\n                         params=params,\n                         data=data,\n                         json=json,\n                         headers=headers,\n                         skip_auto_headers=skip_auto_headers,\n                         auth=auth,\n                         allow_redirects=allow_redirects,\n                         max_redirects=max_redirects,\n                         compress=compress,\n                         chunked=chunked,\n                         expect100=expect100,\n                         raise_for_status=raise_for_status,\n                         read_until_eof=read_until_eof,\n                         proxy=proxy,\n                         proxy_auth=proxy_auth,),\n        session)", "response": "Constructs and sends a request to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _prepare_headers(\n            self,\n            headers: Optional[LooseHeaders]) -> 'CIMultiDict[str]':\n        \"\"\" Add default headers and transform it to CIMultiDict\n        \"\"\"\n        # Convert headers to MultiDict\n        result = CIMultiDict(self._default_headers)\n        if headers:\n            if not isinstance(headers, (MultiDictProxy, MultiDict)):\n                headers = CIMultiDict(headers)\n            added_names = set()  # type: Set[str]\n            for key, value in headers.items():\n                if key in added_names:\n                    result.add(key, value)\n                else:\n                    result[key] = value\n                    added_names.add(key)\n        return result", "response": "Add default headers and transform it to MultiDict."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms HTTP GET request.", "response": "def get(self, url: StrOrURL, *, allow_redirects: bool=True,\n            **kwargs: Any) -> '_RequestContextManager':\n        \"\"\"Perform HTTP GET request.\"\"\"\n        return _RequestContextManager(\n            self._request(hdrs.METH_GET, url,\n                          allow_redirects=allow_redirects,\n                          **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform an HTTP OPTIONS request.", "response": "def options(self, url: StrOrURL, *, allow_redirects: bool=True,\n                **kwargs: Any) -> '_RequestContextManager':\n        \"\"\"Perform HTTP OPTIONS request.\"\"\"\n        return _RequestContextManager(\n            self._request(hdrs.METH_OPTIONS, url,\n                          allow_redirects=allow_redirects,\n                          **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform HTTP HEAD request.", "response": "def head(self, url: StrOrURL, *, allow_redirects: bool=False,\n             **kwargs: Any) -> '_RequestContextManager':\n        \"\"\"Perform HTTP HEAD request.\"\"\"\n        return _RequestContextManager(\n            self._request(hdrs.METH_HEAD, url,\n                          allow_redirects=allow_redirects,\n                          **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(self, url: StrOrURL,\n             *, data: Any=None, **kwargs: Any) -> '_RequestContextManager':\n        \"\"\"Perform HTTP POST request.\"\"\"\n        return _RequestContextManager(\n            self._request(hdrs.METH_POST, url,\n                          data=data,\n                          **kwargs))", "response": "Perform HTTP POST request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put(self, url: StrOrURL,\n            *, data: Any=None, **kwargs: Any) -> '_RequestContextManager':\n        \"\"\"Perform HTTP PUT request.\"\"\"\n        return _RequestContextManager(\n            self._request(hdrs.METH_PUT, url,\n                          data=data,\n                          **kwargs))", "response": "Perform HTTP PUT request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch(self, url: StrOrURL,\n              *, data: Any=None, **kwargs: Any) -> '_RequestContextManager':\n        \"\"\"Perform HTTP PATCH request.\"\"\"\n        return _RequestContextManager(\n            self._request(hdrs.METH_PATCH, url,\n                          data=data,\n                          **kwargs))", "response": "Perform HTTP PATCH request."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, url: StrOrURL, **kwargs: Any) -> '_RequestContextManager':\n        return _RequestContextManager(\n            self._request(hdrs.METH_DELETE, url,\n                          **kwargs))", "response": "Perform HTTP DELETE request."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncloses underlying connector. Release all acquired resources.", "response": "async def close(self) -> None:\n        \"\"\"Close underlying connector.\n\n        Release all acquired resources.\n        \"\"\"\n        if not self.closed:\n            if self._connector is not None and self._connector_owner:\n                await self._connector.close()\n            self._connector = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndo URL requoting on redirection handling.", "response": "def requote_redirect_url(self, val: bool) -> None:\n        \"\"\"Do URL requoting on redirection handling.\"\"\"\n        warnings.warn(\"session.requote_redirect_url modification \"\n                      \"is deprecated #2778\",\n                      DeprecationWarning,\n                      stacklevel=2)\n        self._requote_redirect_url = val"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def resolve(self, host: str,\n                      port: int, family: int) -> List[Dict[str, Any]]:\n        \"\"\"Return IP address for given hostname\"\"\"", "response": "Resolve hostname to IP address"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nemits next multipart reader object.", "response": "async def next(self) -> Any:\n        \"\"\"Emits next multipart reader object.\"\"\"\n        item = await self.stream.next()\n        if self.stream.at_eof():\n            await self.release()\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads body part data.", "response": "async def read(self, *, decode: bool=False) -> Any:\n        \"\"\"Reads body part data.\n\n        decode: Decodes data following by encoding\n                method from Content-Encoding header. If it missed\n                data remains untouched\n        \"\"\"\n        if self._at_eof:\n            return b''\n        data = bytearray()\n        while not self._at_eof:\n            data.extend((await self.read_chunk(self.chunk_size)))\n        if decode:\n            return self.decode(data)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def read_chunk(self, size: int=chunk_size) -> bytes:\n        if self._at_eof:\n            return b''\n        if self._length:\n            chunk = await self._read_chunk_from_length(size)\n        else:\n            chunk = await self._read_chunk_from_stream(size)\n\n        self._read_bytes += len(chunk)\n        if self._read_bytes == self._length:\n            self._at_eof = True\n        if self._at_eof:\n            newline = await self._content.readline()\n            assert newline == self._newline, \\\n                'reader did not read all the data or it is malformed'\n        return chunk", "response": "Reads a chunk of the specified size."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the next line of the content of the file.", "response": "async def readline(self) -> bytes:\n        \"\"\"Reads body part by line by line.\"\"\"\n        if self._at_eof:\n            return b''\n\n        if self._unread:\n            line = self._unread.popleft()\n        else:\n            line = await self._content.readline()\n\n        if line.startswith(self._boundary):\n            # the very last boundary may not come with \\r\\n,\n            # so set single rules for everyone\n            sline = line.rstrip(b'\\r\\n')\n            boundary = self._boundary\n            last_boundary = self._boundary + b'--'\n            # ensure that we read exactly the boundary, not something alike\n            if sline == boundary or sline == last_boundary:\n                self._at_eof = True\n                self._unread.append(line)\n                return b''\n        else:\n            next_line = await self._content.readline()\n            if next_line.startswith(self._boundary):\n                # strip newline but only once\n                line = line[:-len(self._newline)]\n            self._unread.append(next_line)\n\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def release(self) -> None:\n        if self._at_eof:\n            return\n        while not self._at_eof:\n            await self.read_chunk(self.chunk_size)", "response": "Like read but reads all the data to the void."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlike read but assumes that body part contains text data.", "response": "async def text(self, *, encoding: Optional[str]=None) -> str:\n        \"\"\"Like read(), but assumes that body part contains text data.\"\"\"\n        data = await self.read(decode=True)\n        # see https://www.w3.org/TR/html5/forms.html#multipart/form-data-encoding-algorithm # NOQA\n        # and https://dvcs.w3.org/hg/xhr/raw-file/tip/Overview.html#dom-xmlhttprequest-send # NOQA\n        encoding = encoding or self.get_charset(default='utf-8')\n        return data.decode(encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking read but assumes that body parts contains JSON data.", "response": "async def json(self, *, encoding: Optional[str]=None) -> Any:\n        \"\"\"Like read(), but assumes that body parts contains JSON data.\"\"\"\n        data = await self.read(decode=True)\n        if not data:\n            return None\n        encoding = encoding or self.get_charset(default='utf-8')\n        return json.loads(data.decode(encoding))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def form(self, *,\n                   encoding: Optional[str]=None) -> List[Tuple[str, str]]:\n        \"\"\"Like read(), but assumes that body parts contains form\n        urlencoded data.\n        \"\"\"\n        data = await self.read(decode=True)\n        if not data:\n            return []\n        if encoding is not None:\n            real_encoding = encoding\n        else:\n            real_encoding = self.get_charset(default='utf-8')\n        return parse_qsl(data.rstrip().decode(real_encoding),\n                         keep_blank_values=True,\n                         encoding=real_encoding)", "response": "Like read but assumes that body parts contains form\nAttributeNames urlencoded data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode(self, data: bytes) -> bytes:\n        if CONTENT_TRANSFER_ENCODING in self.headers:\n            data = self._decode_content_transfer(data)\n        if CONTENT_ENCODING in self.headers:\n            return self._decode_content(data)\n        return data", "response": "Decodes data according the specified Content - Encoding - Encoding headers value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_charset(self, default: str) -> str:\n        ctype = self.headers.get(CONTENT_TYPE, '')\n        mimetype = parse_mimetype(ctype)\n        return mimetype.parameters.get('charset', default)", "response": "Returns charset parameter from Content - Type header or default."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns name specified in Content - Disposition header or None if header is malformed.", "response": "def name(self) -> Optional[str]:\n        \"\"\"Returns name specified in Content-Disposition header or None\n        if missed or header is malformed.\n        \"\"\"\n\n        _, params = parse_content_disposition(\n            self.headers.get(CONTENT_DISPOSITION))\n        return content_disposition_filename(params, 'name')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing reader instance from HTTP response.", "response": "def from_response(cls, response: 'ClientResponse') -> Any:\n        \"\"\"Constructs reader instance from HTTP response.\n\n        :param response: :class:`~aiohttp.client.ClientResponse` instance\n        \"\"\"\n        obj = cls.response_wrapper_cls(response, cls(response.headers,\n                                                     response.content))\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def next(self) -> Any:\n        # So, if we're at BOF, we need to skip till the boundary.\n        if self._at_eof:\n            return\n        await self._maybe_release_last_part()\n        if self._at_bof:\n            await self._read_until_first_boundary()\n            self._at_bof = False\n        else:\n            await self._read_boundary()\n        if self._at_eof:  # we just read the last boundary, nothing to do there\n            return\n        self._last_part = await self.fetch_next_part()\n        return self._last_part", "response": "Emits the next multipart body part."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread all the body parts to the void till the final boundary.", "response": "async def release(self) -> None:\n        \"\"\"Reads all the body parts to the void till the final boundary.\"\"\"\n        while not self._at_eof:\n            item = await self.next()\n            if item is None:\n                break\n            await item.release()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndispatching the response by the Content - Type header returning instance.", "response": "def _get_part_reader(self, headers: 'CIMultiDictProxy[str]') -> Any:\n        \"\"\"Dispatches the response by the `Content-Type` header, returning\n        suitable reader instance.\n\n        :param dict headers: Response headers\n        \"\"\"\n        ctype = headers.get(CONTENT_TYPE, '')\n        mimetype = parse_mimetype(ctype)\n\n        if mimetype.type == 'multipart':\n            if self.multipart_reader_cls is None:\n                return type(self)(headers, self._content)\n            return self.multipart_reader_cls(\n                headers, self._content, _newline=self._newline\n            )\n        else:\n            return self.part_reader_cls(\n                self._boundary, headers, self._content, _newline=self._newline\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nensures that the last read body part is read completely.", "response": "async def _maybe_release_last_part(self) -> None:\n        \"\"\"Ensures that the last read body part is read completely.\"\"\"\n        if self._last_part is not None:\n            if not self._last_part.at_eof():\n                await self._last_part.release()\n            self._unread.extend(self._last_part._unread)\n            self._last_part = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps boundary parameter value in quotes if necessary.", "response": "def _boundary_value(self) -> str:\n        \"\"\"Wrap boundary parameter value in quotes, if necessary.\n\n        Reads self.boundary and returns a unicode sting.\n        \"\"\"\n        # Refer to RFCs 7231, 7230, 5234.\n        #\n        # parameter      = token \"=\" ( token / quoted-string )\n        # token          = 1*tchar\n        # quoted-string  = DQUOTE *( qdtext / quoted-pair ) DQUOTE\n        # qdtext         = HTAB / SP / %x21 / %x23-5B / %x5D-7E / obs-text\n        # obs-text       = %x80-FF\n        # quoted-pair    = \"\\\" ( HTAB / SP / VCHAR / obs-text )\n        # tchar          = \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\"\n        #                  / \"+\" / \"-\" / \".\" / \"^\" / \"_\" / \"`\" / \"|\" / \"~\"\n        #                  / DIGIT / ALPHA\n        #                  ; any VCHAR, except delimiters\n        # VCHAR           = %x21-7E\n        value = self._boundary\n        if re.match(self._valid_tchar_regex, value):\n            return value.decode('ascii')  # cannot fail\n\n        if re.search(self._invalid_qdtext_char_regex, value):\n            raise ValueError(\"boundary value contains invalid characters\")\n\n        # escape %x5C and %x22\n        quoted_value_content = value.replace(b'\\\\', b'\\\\\\\\')\n        quoted_value_content = quoted_value_content.replace(b'\"', b'\\\\\"')\n\n        return '\"' + quoted_value_content.decode('ascii') + '\"'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef append_payload(self, payload: Payload) -> Payload:\n        # compression\n        encoding = payload.headers.get(CONTENT_ENCODING, '').lower()  # type: Optional[str]  # noqa\n        if encoding and encoding not in ('deflate', 'gzip', 'identity'):\n            raise RuntimeError('unknown content encoding: {}'.format(encoding))\n        if encoding == 'identity':\n            encoding = None\n\n        # te encoding\n        te_encoding = payload.headers.get(\n            CONTENT_TRANSFER_ENCODING, '').lower()  # type: Optional[str]  # noqa\n        if te_encoding not in ('', 'base64', 'quoted-printable', 'binary'):\n            raise RuntimeError('unknown content transfer encoding: {}'\n                               ''.format(te_encoding))\n        if te_encoding == 'binary':\n            te_encoding = None\n\n        # size\n        size = payload.size\n        if size is not None and not (encoding or te_encoding):\n            payload.headers[CONTENT_LENGTH] = str(size)\n\n        self._parts.append((payload, encoding, te_encoding))  # type: ignore\n        return payload", "response": "Adds a new body part to the multipart writer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef append_json(\n            self,\n            obj: Any,\n            headers: Optional['MultiMapping[str]']=None\n    ) -> Payload:\n        \"\"\"Helper to append JSON part.\"\"\"\n        if headers is None:\n            headers = CIMultiDict()\n\n        return self.append_payload(JsonPayload(obj, headers=headers))", "response": "Helper to append JSON part."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append_form(\n            self,\n            obj: Union[Sequence[Tuple[str, str]],\n                       Mapping[str, str]],\n            headers: Optional['MultiMapping[str]']=None\n    ) -> Payload:\n        \"\"\"Helper to append form urlencoded part.\"\"\"\n        assert isinstance(obj, (Sequence, Mapping))\n\n        if headers is None:\n            headers = CIMultiDict()\n\n        if isinstance(obj, Mapping):\n            obj = list(obj.items())\n        data = urlencode(obj, doseq=True)\n\n        return self.append_payload(\n            StringPayload(data, headers=headers,\n                          content_type='application/x-www-form-urlencoded'))", "response": "Helper to append form urlencoded part."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef size(self) -> Optional[int]:\n        if not self._parts:\n            return 0\n\n        total = 0\n        for part, encoding, te_encoding in self._parts:\n            if encoding or te_encoding or part.size is None:\n                return None\n\n            total += int(\n                2 + len(self._boundary) + 2 +  # b'--'+self._boundary+b'\\r\\n'\n                part.size + len(part._binary_headers) +\n                2  # b'\\r\\n'\n            )\n\n        total += 2 + len(self._boundary) + 4  # b'--'+self._boundary+b'--\\r\\n'\n        return total", "response": "Size of the payload."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def write(self, writer: Any,\n                    close_boundary: bool=True) -> None:\n        \"\"\"Write body.\"\"\"\n        if not self._parts:\n            return\n\n        for part, encoding, te_encoding in self._parts:\n            await writer.write(b'--' + self._boundary + b'\\r\\n')\n            await writer.write(part._binary_headers)\n\n            if encoding or te_encoding:\n                w = MultipartPayloadWriter(writer)\n                if encoding:\n                    w.enable_compression(encoding)\n                if te_encoding:\n                    w.enable_encoding(te_encoding)\n                await part.write(w)  # type: ignore\n                await w.write_eof()\n            else:\n                await part.write(writer)\n\n            await writer.write(b'\\r\\n')\n\n        if close_boundary:\n            await writer.write(b'--' + self._boundary + b'--\\r\\n')", "response": "Write the message body."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate destination host port and connection type.", "response": "def update_host(self, url: URL) -> None:\n        \"\"\"Update destination host, port and connection type (ssl).\"\"\"\n        # get host/port\n        if not url.host:\n            raise InvalidURL(url)\n\n        # basic auth info\n        username, password = url.user, url.password\n        if username:\n            self.auth = helpers.BasicAuth(username, password or '')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert request version to two elements tuple.", "response": "def update_version(self, version: Union[http.HttpVersion, str]) -> None:\n        \"\"\"Convert request version to two elements tuple.\n\n        parser HTTP version '1.1' => (1, 1)\n        \"\"\"\n        if isinstance(version, str):\n            v = [l.strip() for l in version.split('.', 1)]\n            try:\n                version = http.HttpVersion(int(v[0]), int(v[1]))\n            except ValueError:\n                raise ValueError(\n                    'Can not parse http version number: {}'\n                    .format(version)) from None\n        self.version = version"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_headers(self, headers: Optional[LooseHeaders]) -> None:\n        self.headers = CIMultiDict()  # type: CIMultiDict[str]\n\n        # add host\n        netloc = cast(str, self.url.raw_host)\n        if helpers.is_ipv6_address(netloc):\n            netloc = '[{}]'.format(netloc)\n        if not self.url.is_default_port():\n            netloc += ':' + str(self.url.port)\n        self.headers[hdrs.HOST] = netloc\n\n        if headers:\n            if isinstance(headers, (dict, MultiDictProxy, MultiDict)):\n                headers = headers.items()  # type: ignore\n\n            for key, value in headers:\n                # A special case for Host header\n                if key.lower() == 'host':\n                    self.headers[key] = value\n                else:\n                    self.headers.add(key, value)", "response": "Update the request headers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate request cookies header.", "response": "def update_cookies(self, cookies: Optional[LooseCookies]) -> None:\n        \"\"\"Update request cookies header.\"\"\"\n        if not cookies:\n            return\n\n        c = SimpleCookie()\n        if hdrs.COOKIE in self.headers:\n            c.load(self.headers.get(hdrs.COOKIE, ''))\n            del self.headers[hdrs.COOKIE]\n\n        if isinstance(cookies, Mapping):\n            iter_cookies = cookies.items()\n        else:\n            iter_cookies = cookies  # type: ignore\n        for name, value in iter_cookies:\n            if isinstance(value, Morsel):\n                # Preserve coded_value\n                mrsl_val = value.get(value.key, Morsel())\n                mrsl_val.set(value.key, value.value, value.coded_value)  # type: ignore  # noqa\n                c[name] = mrsl_val\n            else:\n                c[name] = value  # type: ignore\n\n        self.headers[hdrs.COOKIE] = c.output(header='', sep=';').strip()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the content encoding of the current request.", "response": "def update_content_encoding(self, data: Any) -> None:\n        \"\"\"Set request content encoding.\"\"\"\n        if not data:\n            return\n\n        enc = self.headers.get(hdrs.CONTENT_ENCODING, '').lower()\n        if enc:\n            if self.compress:\n                raise ValueError(\n                    'compress can not be set '\n                    'if Content-Encoding header is set')\n        elif self.compress:\n            if not isinstance(self.compress, str):\n                self.compress = 'deflate'\n            self.headers[hdrs.CONTENT_ENCODING] = self.compress\n            self.chunked = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_transfer_encoding(self) -> None:\n        te = self.headers.get(hdrs.TRANSFER_ENCODING, '').lower()\n\n        if 'chunked' in te:\n            if self.chunked:\n                raise ValueError(\n                    'chunked can not be set '\n                    'if \"Transfer-Encoding: chunked\" header is set')\n\n        elif self.chunked:\n            if hdrs.CONTENT_LENGTH in self.headers:\n                raise ValueError(\n                    'chunked can not be set '\n                    'if Content-Length header is set')\n\n            self.headers[hdrs.TRANSFER_ENCODING] = 'chunked'\n        else:\n            if hdrs.CONTENT_LENGTH not in self.headers:\n                self.headers[hdrs.CONTENT_LENGTH] = str(len(self.body))", "response": "Update the Transfer - Encoding header."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsupporting coroutines that yields bytes objects.", "response": "async def write_bytes(self, writer: AbstractStreamWriter,\n                          conn: 'Connection') -> None:\n        \"\"\"Support coroutines that yields bytes objects.\"\"\"\n        # 100 response\n        if self._continue is not None:\n            await writer.drain()\n            await self._continue\n\n        protocol = conn.protocol\n        assert protocol is not None\n        try:\n            if isinstance(self.body, payload.Payload):\n                await self.body.write(writer)\n            else:\n                if isinstance(self.body, (bytes, bytearray)):\n                    self.body = (self.body,)  # type: ignore\n\n                for chunk in self.body:\n                    await writer.write(chunk)  # type: ignore\n\n            await writer.write_eof()\n        except OSError as exc:\n            new_exc = ClientOSError(\n                exc.errno,\n                'Can not write request body for %s' % self.url)\n            new_exc.__context__ = exc\n            new_exc.__cause__ = exc\n            protocol.set_exception(new_exc)\n        except asyncio.CancelledError as exc:\n            if not conn.closed:\n                protocol.set_exception(exc)\n        except Exception as exc:\n            protocol.set_exception(exc)\n        finally:\n            self._writer = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads response payload and decode.", "response": "async def text(self,\n                   encoding: Optional[str]=None, errors: str='strict') -> str:\n        \"\"\"Read response payload and decode.\"\"\"\n        if self._body is None:\n            await self.read()\n\n        if encoding is None:\n            encoding = self.get_encoding()\n\n        return self._body.decode(encoding, errors=errors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads and decodes JSON response.", "response": "async def json(self, *, encoding: str=None,\n                   loads: JSONDecoder=DEFAULT_JSON_DECODER,\n                   content_type: Optional[str]='application/json') -> Any:\n        \"\"\"Read and decodes JSON response.\"\"\"\n        if self._body is None:\n            await self.read()\n\n        if content_type:\n            ctype = self.headers.get(hdrs.CONTENT_TYPE, '').lower()\n            if not _is_expected_content_type(ctype, content_type):\n                raise ContentTypeError(\n                    self.request_info,\n                    self.history,\n                    message=('Attempt to decode JSON with '\n                             'unexpected mimetype: %s' % ctype),\n                    headers=self.headers)\n\n        if encoding is None:\n            encoding = self.get_encoding()\n\n        return loads(self._body.decode(encoding))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling automatic chunked transfer encoding.", "response": "def enable_chunked_encoding(self, chunk_size: Optional[int]=None) -> None:\n        \"\"\"Enables automatic chunked transfer encoding.\"\"\"\n        self._chunked = True\n\n        if hdrs.CONTENT_LENGTH in self._headers:\n            raise RuntimeError(\"You can't enable chunked encoding when \"\n                               \"a content length is set\")\n        if chunk_size is not None:\n            warnings.warn('Chunk size is deprecated #1615', DeprecationWarning)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enable_compression(self,\n                           force: Optional[Union[bool, ContentCoding]]=None\n                           ) -> None:\n        \"\"\"Enables response compression encoding.\"\"\"\n        # Backwards compatibility for when force was a bool <0.17.\n        if type(force) == bool:\n            force = ContentCoding.deflate if force else ContentCoding.identity\n            warnings.warn(\"Using boolean for force is deprecated #3318\",\n                          DeprecationWarning)\n        elif force is not None:\n            assert isinstance(force, ContentCoding), (\"force should one of \"\n                                                      \"None, bool or \"\n                                                      \"ContentEncoding\")\n\n        self._compression = True\n        self._compression_force = force", "response": "Enables response compression encoding."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets or update a cookie.", "response": "def set_cookie(self, name: str, value: str, *,\n                   expires: Optional[str]=None,\n                   domain: Optional[str]=None,\n                   max_age: Optional[Union[int, str]]=None,\n                   path: str='/',\n                   secure: Optional[str]=None,\n                   httponly: Optional[str]=None,\n                   version: Optional[str]=None) -> None:\n        \"\"\"Set or update response cookie.\n\n        Sets new cookie or updates existent with new value.\n        Also updates only those params which are not None.\n        \"\"\"\n\n        old = self._cookies.get(name)\n        if old is not None and old.coded_value == '':\n            # deleted cookie\n            self._cookies.pop(name, None)\n\n        self._cookies[name] = value\n        c = self._cookies[name]\n\n        if expires is not None:\n            c['expires'] = expires\n        elif c.get('expires') == 'Thu, 01 Jan 1970 00:00:00 GMT':\n            del c['expires']\n\n        if domain is not None:\n            c['domain'] = domain\n\n        if max_age is not None:\n            c['max-age'] = str(max_age)\n        elif 'max-age' in c:\n            del c['max-age']\n\n        c['path'] = path\n\n        if secure is not None:\n            c['secure'] = secure\n        if httponly is not None:\n            c['httponly'] = httponly\n        if version is not None:\n            c['version'] = version"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes cookie. Creates new empty expired cookie.", "response": "def del_cookie(self, name: str, *,\n                   domain: Optional[str]=None,\n                   path: str='/') -> None:\n        \"\"\"Delete cookie.\n\n        Creates new empty expired cookie.\n        \"\"\"\n        # TODO: do we need domain/path here?\n        self._cookies.pop(name, None)\n        self.set_cookie(name, '', max_age=0,\n                        expires=\"Thu, 01 Jan 1970 00:00:00 GMT\",\n                        domain=domain, path=path)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndefaulting handler for Expect header.", "response": "async def _default_expect_handler(request: Request) -> None:\n    \"\"\"Default handler for Expect header.\n\n    Just send \"100 Continue\" to client.\n    raise HTTPExpectationFailed if value of header is not \"100-continue\"\n    \"\"\"\n    expect = request.headers.get(hdrs.EXPECT)\n    if request.version == HttpVersion11:\n        if expect.lower() == \"100-continue\":\n            await request.writer.write(b\"HTTP/1.1 100 Continue\\r\\n\\r\\n\")\n        else:\n            raise HTTPExpectationFailed(text=\"Unknown Expect: %s\" % expect)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs url for route with additional params.", "response": "def url_for(self, *args: str, **kwargs: str) -> URL:\n        \"\"\"Construct url for route with additional params.\"\"\"\n        return self._resource.url_for(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_static(self, prefix: str, path: PathLike, *,\n                   name: Optional[str]=None,\n                   expect_handler: Optional[_ExpectHandler]=None,\n                   chunk_size: int=256 * 1024,\n                   show_index: bool=False, follow_symlinks: bool=False,\n                   append_version: bool=False) -> AbstractResource:\n        \"\"\"Add static files view.\n\n        prefix - url prefix\n        path - folder with files\n\n        \"\"\"\n        assert prefix.startswith('/')\n        if prefix.endswith('/'):\n            prefix = prefix[:-1]\n        resource = StaticResource(prefix, path,\n                                  name=name,\n                                  expect_handler=expect_handler,\n                                  chunk_size=chunk_size,\n                                  show_index=show_index,\n                                  follow_symlinks=follow_symlinks,\n                                  append_version=append_version)\n        self.register_resource(resource)\n        return resource", "response": "Add static files view."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_patch(self, path: str, handler: _WebHandler,\n                  **kwargs: Any) -> AbstractRoute:\n        \"\"\"\n        Shortcut for add_route with method PATCH\n        \"\"\"\n        return self.add_route(hdrs.METH_PATCH, path, handler, **kwargs)", "response": "Shortcut for add_route with method PATCH"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_view(self, path: str, handler: AbstractView,\n                 **kwargs: Any) -> AbstractRoute:\n        \"\"\"\n        Shortcut for add_route with ANY methods for a class-based view\n        \"\"\"\n        return self.add_route(hdrs.METH_ANY, path, handler, **kwargs)", "response": "Shortcut for add_route with ANY methods for a class - based view"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_routes(self, routes: Iterable[AbstractRouteDef]) -> None:\n        for route_def in routes:\n            route_def.register(self)", "response": "Append routes to route table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing RFC 5322 headers from a stream.", "response": "def parse_headers(\n            self,\n            lines: List[bytes]\n    ) -> Tuple['CIMultiDictProxy[str]',\n               RawHeaders,\n               Optional[bool],\n               Optional[str],\n               bool,\n               bool]:\n        \"\"\"Parses RFC 5322 headers from a stream.\n\n        Line continuations are supported. Returns list of header name\n        and value pairs. Header name is in upper case.\n        \"\"\"\n        headers, raw_headers = self._headers_parser.parse_headers(lines)\n        close_conn = None\n        encoding = None\n        upgrade = False\n        chunked = False\n\n        # keep-alive\n        conn = headers.get(hdrs.CONNECTION)\n        if conn:\n            v = conn.lower()\n            if v == 'close':\n                close_conn = True\n            elif v == 'keep-alive':\n                close_conn = False\n            elif v == 'upgrade':\n                upgrade = True\n\n        # encoding\n        enc = headers.get(hdrs.CONTENT_ENCODING)\n        if enc:\n            enc = enc.lower()\n            if enc in ('gzip', 'deflate', 'br'):\n                encoding = enc\n\n        # chunking\n        te = headers.get(hdrs.TRANSFER_ENCODING)\n        if te and 'chunked' in te.lower():\n            chunked = True\n\n        return (headers, raw_headers, close_conn, encoding, upgrade, chunked)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget extra info from connection transport", "response": "def get_extra_info(self, name: str, default: Any=None) -> Any:\n        \"\"\"extra info from connection transport\"\"\"\n        conn = self._response.connection\n        if conn is None:\n            return default\n        transport = conn.transport\n        if transport is None:\n            return default\n        return transport.get_extra_info(name, default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef user_agent(style=None) -> _UserAgent:\n    global useragent\n    if (not useragent) and style:\n        useragent = UserAgent()\n\n    return useragent[style] if style else DEFAULT_USER_AGENT", "response": "Returns an apparently legit user - agent if not requested one of a specific style."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the raw HTML representation of the HTML.", "response": "def raw_html(self) -> _RawHTML:\n        \"\"\"Bytes representation of the HTML content.\n        (`learn more <http://www.diveintopython3.net/strings.html>`_).\n        \"\"\"\n        if self._html:\n            return self._html\n        else:\n            return etree.tostring(self.element, encoding='unicode').strip().encode(self.encoding)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef html(self) -> _BaseHTML:\n        if self._html:\n            return self.raw_html.decode(self.encoding, errors='replace')\n        else:\n            return etree.tostring(self.element, encoding='unicode').strip()", "response": "Unicode representation of the HTML content."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the PyQuery representation of the XML document.", "response": "def pq(self) -> PyQuery:\n        \"\"\"`PyQuery <https://pythonhosted.org/pyquery/>`_ representation\n        of the :class:`Element <Element>` or :class:`HTML <HTML>`.\n        \"\"\"\n        if self._pq is None:\n            self._pq = PyQuery(self.lxml)\n\n        return self._pq"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the lxml representation of the page.", "response": "def lxml(self) -> HtmlElement:\n        \"\"\"`lxml <http://lxml.de>`_ representation of the\n        :class:`Element <Element>` or :class:`HTML <HTML>`.\n        \"\"\"\n        if self._lxml is None:\n            try:\n                self._lxml = soup_parse(self.html, features='html.parser')\n            except ValueError:\n                self._lxml = lxml.html.fromstring(self.raw_html)\n\n        return self._lxml"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find(self, selector: str = \"*\", *, containing: _Containing = None, clean: bool = False, first: bool = False, _encoding: str = None) -> _Find:\n\n        # Convert a single containing into a list.\n        if isinstance(containing, str):\n            containing = [containing]\n\n        encoding = _encoding or self.encoding\n        elements = [\n            Element(element=found, url=self.url, default_encoding=encoding)\n            for found in self.pq(selector)\n        ]\n\n        if containing:\n            elements_copy = elements.copy()\n            elements = []\n\n            for element in elements_copy:\n                if any([c.lower() in element.full_text.lower() for c in containing]):\n                    elements.append(element)\n\n            elements.reverse()\n\n        # Sanitize the found HTML.\n        if clean:\n            elements_copy = elements.copy()\n            elements = []\n\n            for element in elements_copy:\n                element.raw_html = lxml_html_tostring(cleaner.clean_html(element.lxml))\n                elements.append(element)\n\n        return _get_first_or_list(elements, first)", "response": "Find elements by CSS Selector."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving an XPath selector returns a list of Element objects or a single one.", "response": "def xpath(self, selector: str, *, clean: bool = False, first: bool = False, _encoding: str = None) -> _XPath:\n        \"\"\"Given an XPath selector, returns a list of\n        :class:`Element <Element>` objects or a single one.\n\n        :param selector: XPath Selector to use.\n        :param clean: Whether or not to sanitize the found HTML of ``<script>`` and ``<style>`` tags.\n        :param first: Whether or not to return just the first result.\n        :param _encoding: The encoding format.\n\n        If a sub-selector is specified (e.g. ``//a/@href``), a simple\n        list of results is returned.\n\n        See W3School's `XPath Examples\n        <https://www.w3schools.com/xml/xpath_examples.asp>`_\n        for more details.\n\n        If ``first`` is ``True``, only returns the first\n        :class:`Element <Element>` found.\n        \"\"\"\n        selected = self.lxml.xpath(selector)\n\n        elements = [\n            Element(element=selection, url=self.url, default_encoding=_encoding or self.encoding)\n            if not isinstance(selection, etree._ElementUnicodeResult) else str(selection)\n            for selection in selected\n        ]\n\n        # Sanitize the found HTML.\n        if clean:\n            elements_copy = elements.copy()\n            elements = []\n\n            for element in elements_copy:\n                element.raw_html = lxml_html_tostring(cleaner.clean_html(element.lxml))\n                elements.append(element)\n\n        return _get_first_or_list(elements, first)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search_all(self, template: str) -> _Result:\n        return [r for r in findall(template, self.html)]", "response": "Search the list of elements that match the given template."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef links(self) -> _Links:\n\n        def gen():\n            for link in self.find('a'):\n\n                try:\n                    href = link.attrs['href'].strip()\n                    if href and not (href.startswith('#') and self.skip_anchors) and not href.startswith(('javascript:', 'mailto:')):\n                        yield href\n                except KeyError:\n                    pass\n\n        return set(gen())", "response": "All found links on page in as\u2013is form."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_absolute(self, link):\n\n        # Parse the link with stdlib.\n        parsed = urlparse(link)._asdict()\n\n        # If link is relative, then join it with base_url.\n        if not parsed['netloc']:\n            return urljoin(self.base_url, link)\n\n        # Link is absolute; if it lacks a scheme, add one from base_url.\n        if not parsed['scheme']:\n            parsed['scheme'] = urlparse(self.base_url).scheme\n\n            # Reconstruct the URL to incorporate the new scheme.\n            parsed = (v for v in parsed.values())\n            return urlunparse(parsed)\n\n        # Link is absolute and complete with scheme; nothing to be done here.\n        return link", "response": "Makes a given link absolute."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef base_url(self) -> _URL:\n\n        # Support for <base> tag.\n        base = self.find('base', first=True)\n        if base:\n            result = base.attrs.get('href', '').strip()\n            if result:\n                return result\n\n        # Parse the url to separate out the path\n        parsed = urlparse(self.url)._asdict()\n\n        # Remove any part of the path after the last '/'\n        parsed['path'] = '/'.join(parsed['path'].split('/')[:-1]) + '/'\n\n        # Reconstruct the url with the modified path\n        parsed = (v for v in parsed.values())\n        url = urlunparse(parsed)\n\n        return url", "response": "The base URL for the page. Supports the base tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary of the attributes of the class and rel elements of the class and rel elements of the class element.", "response": "def attrs(self) -> _Attrs:\n        \"\"\"Returns a dictionary of the attributes of the :class:`Element <Element>`\n        (`learn more <https://www.w3schools.com/tags/ref_attributes.asp>`_).\n        \"\"\"\n        if self._attrs is None:\n            self._attrs = {k: v for k, v in self.element.items()}\n\n            # Split class and rel up, as there are ussually many of them:\n            for attr in ['class', 'rel']:\n                if attr in self._attrs:\n                    self._attrs[attr] = tuple(self._attrs[attr].split())\n\n        return self._attrs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef next(self, fetch: bool = False, next_symbol: _NextSymbol = DEFAULT_NEXT_SYMBOL) -> _Next:\n\n        def get_next():\n            candidates = self.find('a', containing=next_symbol)\n\n            for candidate in candidates:\n                if candidate.attrs.get('href'):\n                    # Support 'next' rel (e.g. reddit).\n                    if 'next' in candidate.attrs.get('rel', []):\n                        return candidate.attrs['href']\n\n                    # Support 'next' in classnames.\n                    for _class in candidate.attrs.get('class', []):\n                        if 'next' in _class:\n                            return candidate.attrs['href']\n\n                    if 'page' in candidate.attrs['href']:\n                        return candidate.attrs['href']\n\n            try:\n                # Resort to the last candidate.\n                return candidates[-1].attrs['href']\n            except IndexError:\n                return None\n\n        __next = get_next()\n        if __next:\n            url = self._make_absolute(__next)\n        else:\n            return None\n\n        if fetch:\n            return self.session.get(url)\n        else:\n            return url", "response": "Attempts to find the next page if there is one. If fetch is True returns the next URL. Otherwise returns None."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _async_render(self, *, url: str, script: str = None, scrolldown, sleep: int, wait: float, reload, content: Optional[str], timeout: Union[float, int], keep_page: bool):\n        try:\n            page = await self.browser.newPage()\n\n            # Wait before rendering the page, to prevent timeouts.\n            await asyncio.sleep(wait)\n\n            # Load the given page (GET request, obviously.)\n            if reload:\n                await page.goto(url, options={'timeout': int(timeout * 1000)})\n            else:\n                await page.goto(f'data:text/html,{self.html}', options={'timeout': int(timeout * 1000)})\n\n            result = None\n            if script:\n                result = await page.evaluate(script)\n\n            if scrolldown:\n                for _ in range(scrolldown):\n                    await page._keyboard.down('PageDown')\n                    await asyncio.sleep(sleep)\n            else:\n                await asyncio.sleep(sleep)\n\n            if scrolldown:\n                await page._keyboard.up('PageDown')\n\n            # Return the content of the page, JavaScript evaluated.\n            content = await page.content()\n            if not keep_page:\n                await page.close()\n                page = None\n            return content, result, page\n        except TimeoutError:\n            await page.close()\n            page = None\n            return None", "response": "Internal method for rendering a single page."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render(self, retries: int = 8, script: str = None, wait: float = 0.2, scrolldown=False, sleep: int = 0, reload: bool = True, timeout: Union[float, int] = 8.0, keep_page: bool = False):\n\n        self.browser = self.session.browser  # Automatically create a event loop and browser\n        content = None\n\n        # Automatically set Reload to False, if example URL is being used.\n        if self.url == DEFAULT_URL:\n            reload = False\n\n\n        for i in range(retries):\n            if not content:\n                try:\n\n                    content, result, page = self.session.loop.run_until_complete(self._async_render(url=self.url, script=script, sleep=sleep, wait=wait, content=self.html, reload=reload, scrolldown=scrolldown, timeout=timeout, keep_page=keep_page))\n                except TypeError:\n                    pass\n            else:\n                break\n\n        if not content:\n            raise MaxRetries(\"Unable to render the page. Try increasing timeout\")\n\n        html = HTML(url=self.url, html=content.encode(DEFAULT_ENCODING), default_encoding=DEFAULT_ENCODING)\n        self.__dict__.update(html.__dict__)\n        self.page = page\n        return result", "response": "Renders the HTML content in Chromium and replaces it with an updated version with JavaScript executed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging response enconding and replace it by a HTMLResponse.", "response": "def response_hook(self, response, **kwargs) -> HTMLResponse:\n        \"\"\" Change response enconding and replace it by a HTMLResponse. \"\"\"\n        if not response.encoding:\n            response.encoding = DEFAULT_ENCODING\n        return HTMLResponse._from_response(response, self)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        if hasattr(self, \"_browser\"):\n            self.loop.run_until_complete(self._browser.close())\n        super().close()", "response": "If a browser was created close it first."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request(self, *args, **kwargs):\n        func = partial(super().request, *args, **kwargs)\n        return self.loop.run_in_executor(self.thread_pool, func)", "response": "Partial request func and run it in a thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns all the coroutines in the loop and wait for the result.", "response": "def run(self, *coros):\n        \"\"\" Pass in all the coroutines you want to run, it will wrap each one\n            in a task, run it and wait for the result. Return a list with all\n            results, this is returned in the same order coros are passed in. \"\"\"\n        tasks = [\n            asyncio.ensure_future(coro()) for coro in coros\n        ]\n        done, _ = self.loop.run_until_complete(asyncio.wait(tasks))\n        return [t.result() for t in done]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a depth channel to the image tensor.", "response": "def add_depth_channel(img_tensor, pad_mode):\n    '''\n    img_tensor: N, C, H, W\n    '''\n    img_tensor[:, 1] = get_depth_tensor(pad_mode)\n    img_tensor[:, 2] = img_tensor[:, 0] * get_depth_tensor(pad_mode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_pre_compute(self, s):\n        '''\n        :param s: [src_sequence, batch_size, src_dim]\n        :return: [src_sequence, batch_size. hidden_dim]\n        '''\n        hidden_dim = self.hidden_dim\n        src_dim = s.get_shape().as_list()[-1]\n        assert src_dim is not None, 'src dim must be defined'\n        W = self._get_var('W', shape=[src_dim, hidden_dim])\n        b = self._get_var('b', shape=[1, hidden_dim])\n        return tf.tensordot(s, W, [[2], [0]]) + b", "response": "Compute the pre - compute tensor for the source sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_prob(self, src, tgt, mask, pre_compute, return_logits=False):\n        '''\n        :param s: [src_sequence_length, batch_size, src_dim]\n        :param h: [batch_size, tgt_dim] or [tgt_sequence_length, batch_size, tgt_dim]\n        :param mask: [src_sequence_length, batch_size]\\\n             or [tgt_sequence_length, src_sequence_length, batch_sizse]\n        :param pre_compute: [src_sequence_length, batch_size, hidden_dim]\n        :return: [src_sequence_length, batch_size]\\\n             or [tgt_sequence_length, src_sequence_length, batch_size]\n        '''\n        s_shape = src.get_shape().as_list()\n        h_shape = tgt.get_shape().as_list()\n        src_dim = s_shape[-1]\n        tgt_dim = h_shape[-1]\n        assert src_dim is not None, 'src dimension must be defined'\n        assert tgt_dim is not None, 'tgt dimension must be defined'\n\n        self._define_params(src_dim, tgt_dim)\n\n        if len(h_shape) == 2:\n            tgt = tf.expand_dims(tgt, 0)\n        if pre_compute is None:\n            pre_compute = self.get_pre_compute(src)\n\n        buf0 = pre_compute\n        buf1 = tf.tensordot(tgt, self.var['U'], axes=[[2], [0]])\n        buf2 = tf.tanh(tf.expand_dims(buf0, 0) + tf.expand_dims(buf1, 1))\n\n        if not self.is_vanilla:\n            xh1 = tgt\n            xh2 = tgt\n            s1 = src\n            if self.need_padding:\n                xh1 = tf.tensordot(xh1, self.var['V_t'], 1)\n                xh2 = tf.tensordot(xh2, self.var['S_t'], 1)\n                s1 = tf.tensordot(s1, self.var['V_s'], 1)\n            if not self.is_identity_transform:\n                xh1 = tf.tensordot(xh1, self.var['T'], 1)\n                xh2 = tf.tensordot(xh2, self.var['T'], 1)\n            buf3 = tf.expand_dims(s1, 0) * tf.expand_dims(xh1, 1)\n            buf3 = tf.tanh(tf.tensordot(buf3, self.var['V'], axes=[[3], [0]]))\n            buf = tf.reshape(tf.tanh(buf2 + buf3), shape=tf.shape(buf3))\n        else:\n            buf = buf2\n        v = self.var['v']\n        e = tf.tensordot(buf, v, [[3], [0]])\n        e = tf.squeeze(e, axis=[3])\n        tmp = tf.reshape(e + (mask - 1) * 10000.0, shape=tf.shape(e))\n        prob = tf.nn.softmax(tmp, 1)\n        if len(h_shape) == 2:\n            prob = tf.squeeze(prob, axis=[0])\n            tmp = tf.squeeze(tmp, axis=[0])\n        if return_logits:\n            return prob, tmp\n        return prob", "response": "Get the probability of a source sequence."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_att(self, s, prob):\n        '''\n        :param s: [src_sequence_length, batch_size, src_dim]\n        :param prob: [src_sequence_length, batch_size]\\\n            or [tgt_sequence_length, src_sequence_length, batch_size]\n        :return: [batch_size, src_dim] or [tgt_sequence_length, batch_size, src_dim]\n        '''\n        buf = s * tf.expand_dims(prob, axis=-1)\n        att = tf.reduce_sum(buf, axis=-3)\n        return att", "response": "Get att for the n - tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets shape of variable.", "response": "def shape(tensor):\n    '''\n    Get shape of variable.\n    Return type is tuple.\n    '''\n    temp_s = tensor.get_shape()\n    return tuple([temp_s[i].value for i in range(0, len(temp_s))])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_variable(name, temp_s):\n    '''\n    Get variable by name.\n    '''\n    return tf.Variable(tf.zeros(temp_s), name=name)", "response": "Get variable by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_tta_predict(args, model, ckp_path, tta_num=4):\n    '''\n    return 18000x128x128 np array\n    '''\n    model.eval()\n    preds = []\n    meta = None\n\n    # i is tta index, 0: no change, 1: horizon flip, 2: vertical flip, 3: do both\n    for flip_index in range(tta_num):\n        print('flip_index:', flip_index)\n        test_loader = get_test_loader(args.batch_size, index=flip_index, dev_mode=False, pad_mode=args.pad_mode)\n        meta = test_loader.meta\n        outputs = None\n        with torch.no_grad():\n            for i, img in enumerate(test_loader):\n                add_depth_channel(img, args.pad_mode)\n                img = img.cuda()\n                output, _ = model(img)\n                output = torch.sigmoid(output)\n                if outputs is None:\n                    outputs = output.squeeze()\n                else:\n                    outputs = torch.cat([outputs, output.squeeze()], 0)\n\n                print('{} / {}'.format(args.batch_size*(i+1), test_loader.num), end='\\r')\n        outputs = outputs.cpu().numpy()\n        # flip back masks\n        if flip_index == 1:\n            outputs = np.flip(outputs, 2)\n        elif flip_index == 2:\n            outputs = np.flip(outputs, 1)\n        elif flip_index == 3:\n            outputs = np.flip(outputs, 2)\n            outputs = np.flip(outputs, 1)\n        #print(outputs.shape)\n        preds.append(outputs)\n    \n    parent_dir = ckp_path+'_out'\n    if not os.path.exists(parent_dir):\n        os.makedirs(parent_dir)\n    np_file = os.path.join(parent_dir, 'pred.npy')\n\n    model_pred_result = np.mean(preds, 0)\n    np.save(np_file, model_pred_result)\n\n    return model_pred_result, meta", "response": "predict the tta of the current dataset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndistributing Synchronous SGD Example", "response": "def run(params):\n    \"\"\" Distributed Synchronous SGD Example \"\"\"\n    rank = dist.get_rank()\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n    model = model\n    optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], momentum=params['momentum'])\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    total_loss = 0.0\n    for epoch in range(3):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            data, target = Variable(data), Variable(target)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        #logger.debug('Rank: ', rank, ', epoch: ', epoch, ': ', epoch_loss / num_batches)\n        if rank == 0:\n            nni.report_intermediate_result(epoch_loss / num_batches)\n        total_loss += (epoch_loss / num_batches)\n    total_loss /= 3\n    logger.debug('Final loss: {}'.format(total_loss))\n    if rank == 0:\n        nni.report_final_result(total_loss)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef graph_loads(graph_json):\n    '''\n    Load graph\n    '''\n    layers = []\n    for layer in graph_json['layers']:\n        layer_info = Layer(layer['type'], layer['input'], layer['output'], layer['size'])\n        layer_info.is_delete = layer['is_delete']\n        layers.append(layer_info)\n    graph = Graph(graph_json['max_layer_num'], [], [], [])\n    graph.layers = layers\n    return graph", "response": "Load graph from json format"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_size(self, graph_id, size):\n        '''\n        Set size.\n        '''\n        if self.graph_type == LayerType.attention.value:\n            if self.input[0] == graph_id:\n                self.size = size\n        if self.graph_type == LayerType.rnn.value:\n            self.size = size\n        if self.graph_type == LayerType.self_attention.value:\n            self.size = size\n        if self.graph_type == LayerType.output.value:\n            if self.size != size:\n                return False\n        return True", "response": "Set size of the node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclearing size of the resource.", "response": "def clear_size(self):\n        '''\n        Clear size\n        '''\n        if self.graph_type == LayerType.attention.value or \\\n            LayerType.rnn.value or LayerType.self_attention.value:\n            self.size = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\njudge whether the current state of the cluster is legal for the given layers.", "response": "def is_legal(self, layers=None):\n        '''\n        Judge whether is legal for layers\n        '''\n        if layers is None:\n            layers = self.layers\n\n        for layer in layers:\n            if layer.is_delete is False:\n                if len(layer.input) != layer.input_size:\n                    return False\n                if len(layer.output) < layer.output_size:\n                    return False\n\n        # layer_num <= max_layer_num\n        if self.layer_num(layers) > self.max_layer_num:\n            return False\n\n        # There is loop in graph || some layers can't to arrive\n        if self.is_topology(layers) is False:\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmutate the graph with a random number of edge - level layers.", "response": "def mutation(self, only_add=False):\n        '''\n        Mutation for a graph\n        '''\n        types = []\n        if self.layer_num() < self.max_layer_num:\n            types.append(0)\n            types.append(1)\n        if self.layer_num() > 5 and only_add is False:\n            types.append(2)\n            types.append(3)\n        # 0 : add a layer , delete a edge\n        # 1 : add a layer , change a edge\n        # 2 : delete a layer, delete a edge\n        # 3 : delete a layer, change a edge\n        graph_type = random.choice(types)\n        layer_type = random.choice([LayerType.attention.value,\\\n            LayerType.self_attention.value, LayerType.rnn.value])\n        layers = copy.deepcopy(self.layers)\n        cnt_try = 0\n        while True:\n            layers_in = []\n            layers_out = []\n            layers_del = []\n            for i, layer in enumerate(layers):\n                if layer.is_delete is False:\n                    if layer.graph_type != LayerType.output.value:\n                        layers_in.append(i)\n                    if layer.graph_type != LayerType.input.value:\n                        layers_out.append(i)\n                    if layer.graph_type != LayerType.output.value\\\n                            and layer.graph_type != LayerType.input.value:\n                        layers_del.append(i)\n            if graph_type <= 1:\n                new_id = len(layers)\n                out = random.choice(layers_out)\n                inputs = []\n                output = [out]\n                pos = random.randint(0, len(layers[out].input) - 1)\n                last_in = layers[out].input[pos]\n                layers[out].input[pos] = new_id\n                if graph_type == 0:\n                    layers[last_in].output.remove(out)\n                if graph_type == 1:\n                    layers[last_in].output.remove(out)\n                    layers[last_in].output.append(new_id)\n                    inputs = [last_in]\n                lay = Layer(graph_type=layer_type, inputs=inputs, output=output)\n                while len(inputs) < lay.input_size:\n                    layer1 = random.choice(layers_in)\n                    inputs.append(layer1)\n                    layers[layer1].output.append(new_id)\n                lay.input = inputs\n                layers.append(lay)\n            else:\n                layer1 = random.choice(layers_del)\n                for layer2 in layers[layer1].output:\n                    layers[layer2].input.remove(layer1)\n                    if graph_type == 2:\n                        random_in = random.choice(layers_in)\n                    else:\n                        random_in = random.choice(layers[layer1].input)\n                    layers[layer2].input.append(random_in)\n                    layers[random_in].output.append(layer2)\n                for layer2 in layers[layer1].input:\n                    layers[layer2].output.remove(layer1)\n                layers[layer1].is_delete = True\n\n            if self.is_legal(layers):\n                self.layers = layers\n                break\n            else:\n                layers = copy.deepcopy(self.layers)\n                cnt_try += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the internal state of the internal state of the object with the search space.", "response": "def update_search_space(self, search_space):\n        \"\"\"TODO: this is urgly, we put all the initialization work in this method, because initialization relies\n        on search space, also because update_search_space is called at the beginning.\n        NOTE: updating search space is not supported.\n\n        Parameters\n        ----------\n        search_space:\n            search space\n        \"\"\"\n        if not self.update_ss_done:\n            self.categorical_dict = generate_scenario(search_space)\n            if self.categorical_dict is None:\n                raise RuntimeError('categorical dict is not correctly returned after parsing search space.')\n            self.optimizer = self._main_cli()\n            self.smbo_solver = self.optimizer.solver\n            self.loguniform_key = {key for key in search_space.keys() if search_space[key]['_type'] == 'loguniform'}\n            self.update_ss_done = True\n        else:\n            self.logger.warning('update search space is not supported.')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef receive_trial_result(self, parameter_id, parameters, value):\n        reward = extract_scalar_reward(value)\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -reward\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        if self.first_one:\n            self.smbo_solver.nni_smac_receive_first_run(self.total_data[parameter_id], reward)\n            self.first_one = False\n        else:\n            self.smbo_solver.nni_smac_receive_runs(self.total_data[parameter_id], reward)", "response": "This function is called by the trial result thread when the parameter value is received."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_loguniform_categorical(self, challenger_dict):\n        converted_dict = {}\n        for key, value in challenger_dict.items():\n            # convert to loguniform\n            if key in self.loguniform_key:\n                converted_dict[key] = np.exp(challenger_dict[key])\n            # convert categorical back to original value\n            elif key in self.categorical_dict:\n                idx = challenger_dict[key]\n                converted_dict[key] = self.categorical_dict[key][idx]\n            else:\n                converted_dict[key] = value\n        return converted_dict", "response": "Convert loguniform values back to their initial range\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates one instance of hyperparameters", "response": "def generate_parameters(self, parameter_id):\n        \"\"\"generate one instance of hyperparameters\n        \n        Parameters\n        ----------\n        parameter_id: int\n            parameter id\n        \n        Returns\n        -------\n        list\n            new generated parameters\n        \"\"\"\n        if self.first_one:\n            init_challenger = self.smbo_solver.nni_smac_start()\n            self.total_data[parameter_id] = init_challenger\n            return self.convert_loguniform_categorical(init_challenger.get_dictionary())\n        else:\n            challengers = self.smbo_solver.nni_smac_request_challengers()\n            for challenger in challengers:\n                self.total_data[parameter_id] = challenger\n                return self.convert_loguniform_categorical(challenger.get_dictionary())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_multiple_parameters(self, parameter_id_list):\n        if self.first_one:\n            params = []\n            for one_id in parameter_id_list:\n                init_challenger = self.smbo_solver.nni_smac_start()\n                self.total_data[one_id] = init_challenger\n                params.append(self.convert_loguniform_categorical(init_challenger.get_dictionary()))\n        else:\n            challengers = self.smbo_solver.nni_smac_request_challengers()\n            cnt = 0\n            params = []\n            for challenger in challengers:\n                if cnt >= len(parameter_id_list):\n                    break\n                self.total_data[parameter_id_list[cnt]] = challenger\n                params.append(self.convert_loguniform_categorical(challenger.get_dictionary()))\n                cnt += 1\n        return params", "response": "generate multiple parameters for a single hyperparameter"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the gradient of the Lovasz extension w. r. t sorted errors", "response": "def lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the IoU of the class based on the prediction and labels.", "response": "def iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    \"\"\"\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    \"\"\"\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / union\n        ious.append(iou)\n    iou = mean(ious)    # mean accross images if per_image\n    return 100 * iou"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []    \n        for i in range(C):\n            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(mean, zip(*ious)) # mean accross images if per_image\n    return 100 * np.array(ious)", "response": "Returns the ious estimates of the class in the hierarchy of predicted classes and labels."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss", "response": "Binary Lovasz hinge loss."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lovasz_hinge_flat(logits, labels):\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.elu(errors_sorted)+1, Variable(grad))\n    #loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n    \n    return loss", "response": "Binary Lovasz hinge loss."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flatten_binary_scores(scores, labels, ignore=None):\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels", "response": "Flattens scores in the batch and labels in the batch"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef binary_xloss(logits, labels, ignore=None):\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss", "response": "Binary Cross entropy loss."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n    return loss", "response": "Compute the Lovasz - Softmax loss for a set of class probabilities."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lovasz_softmax_flat(probas, labels, only_present=False):\n    C = probas.size(1)\n    losses = []\n    for c in range(C):\n        fg = (labels == c).float() # foreground for class c\n        if only_present and fg.sum() == 0:\n            continue\n        errors = (Variable(fg) - probas[:, c]).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)", "response": "Compute the Lovasz - Softmax loss for a given set of class probabilities."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nflattening predictions in the batch", "response": "def flatten_probas(probas, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch\n    \"\"\"\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mean(l, ignore_nan=False, empty=0):\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(np.isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == 'raise':\n            raise ValueError('Empty mean')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n", "response": "mean compatible with generators.\n    nanmean compatible with generators.\n    emptymean compatible with generators.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef forward(self, x):\n        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''\n        N,C,H,W = x.size()\n        g = self.groups\n        return x.view(N,g,C/g,H,W).permute(0,2,1,3,4).contiguous().view(N,C,H,W)", "response": "Forward method for the channel shuffle."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_embedding(path):\n    '''\n    return embedding for a specific file by given file path.\n    '''\n    EMBEDDING_DIM = 300\n    embedding_dict = {}\n    with open(path, 'r', encoding='utf-8') as file:\n        pairs = [line.strip('\\r\\n').split() for line in file.readlines()]\n        for pair in pairs:\n            if len(pair) == EMBEDDING_DIM + 1:\n                embedding_dict[pair[0]] = [float(x) for x in pair[1:]]\n    logger.debug('embedding_dict size: %d', len(embedding_dict))\n    return embedding_dict", "response": "load embedding for a specific file by given path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_predict_json(position1_result, position2_result, ids, passage_tokens):\n    '''\n    Generate json by prediction.\n    '''\n    predict_len = len(position1_result)\n    logger.debug('total prediction num is %s', str(predict_len))\n\n    answers = {}\n    for i in range(predict_len):\n        sample_id = ids[i]\n        passage, tokens = passage_tokens[i]\n        kbest = find_best_answer_span(\n            position1_result[i], position2_result[i], len(tokens), 23)\n        _, start, end = kbest[0]\n        answer = passage[tokens[start]['char_begin']:tokens[end]['char_end']]\n        answers[sample_id] = answer\n    logger.debug('generate predict done.')\n    return answers", "response": "Generate json by prediction."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_data(path, tokenizer, char_vcb, word_vcb, is_training=False):\n    '''\n    Generate data\n    '''\n    global root_path\n    qp_pairs = data.load_from_file(path=path, is_training=is_training)\n\n    tokenized_sent = 0\n    # qp_pairs = qp_pairs[:1000]1\n    for qp_pair in qp_pairs:\n        tokenized_sent += 1\n        data.tokenize(qp_pair, tokenizer, is_training)\n        for word in qp_pair['question_tokens']:\n            word_vcb.add(word['word'])\n            for char in word['word']:\n                char_vcb.add(char)\n        for word in qp_pair['passage_tokens']:\n            word_vcb.add(word['word'])\n            for char in word['word']:\n                char_vcb.add(char)\n\n    max_query_length = max(len(x['question_tokens']) for x in qp_pairs)\n    max_passage_length = max(len(x['passage_tokens']) for x in qp_pairs)\n    #min_passage_length = min(len(x['passage_tokens']) for x in qp_pairs)\n    cfg.max_query_length = max_query_length\n    cfg.max_passage_length = max_passage_length\n\n    return qp_pairs", "response": "Generate data for the given file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef f1_score(prediction, ground_truth):\n    '''\n    Calculate the f1 score.\n    '''\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1_result = (2 * precision * recall) / (precision + recall)\n    return f1_result", "response": "Calculate the f1 score."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nevaluates the dataset and predictions.", "response": "def _evaluate(dataset, predictions):\n    '''\n    Evaluate function.\n    '''\n    f1_result = exact_match = total = 0\n    count = 0\n    for article in dataset:\n        for paragraph in article['paragraphs']:\n            for qa_pair in paragraph['qas']:\n                total += 1\n                if qa_pair['id'] not in predictions:\n                    count += 1\n                    continue\n                ground_truths = list(map(lambda x: x['text'], qa_pair['answers']))\n                prediction = predictions[qa_pair['id']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1_result += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n    print('total', total, 'exact_match', exact_match, 'unanswer_question ', count)\n    exact_match = 100.0 * exact_match / total\n    f1_result = 100.0 * f1_result / total\n    return {'exact_match': exact_match, 'f1': f1_result}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nevaluates the dataset and prediction.", "response": "def evaluate(data_file, pred_file):\n    '''\n    Evaluate.\n    '''\n    expected_version = '1.1'\n    with open(data_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if dataset_json['version'] != expected_version:\n            print('Evaluation expects v-' + expected_version +\n                  ', but got dataset with v-' + dataset_json['version'],\n                  file=sys.stderr)\n        dataset = dataset_json['data']\n    with open(pred_file) as prediction_file:\n        predictions = json.load(prediction_file)\n    # print(json.dumps(evaluate(dataset, predictions)))\n    result = _evaluate(dataset, predictions)\n    # print('em:', result['exact_match'], 'f1:', result['f1'])\n    return result['exact_match']"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend command to Training Service.", "response": "def send(command, data):\n    \"\"\"Send command to Training Service.\n    command: CommandType object.\n    data: string payload.\n    \"\"\"\n    global _lock\n    try:\n        _lock.acquire()\n        data = data.encode('utf8')\n        assert len(data) < 1000000, 'Command too long'\n        msg = b'%b%06d%b' % (command.value, len(data), data)\n        logging.getLogger(__name__).debug('Sending command, data: [%s]' % msg)\n        _out_file.write(msg)\n        _out_file.flush()\n    finally:\n        _lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreceives a command from Training Service.", "response": "def receive():\n    \"\"\"Receive a command from Training Service.\n    Returns a tuple of command (CommandType) and payload (str)\n    \"\"\"\n    header = _in_file.read(8)\n    logging.getLogger(__name__).debug('Received command, header: [%s]' % header)\n    if header is None or len(header) < 8:\n        # Pipe EOF encountered\n        logging.getLogger(__name__).debug('Pipe EOF encountered')\n        return None, None\n    length = int(header[2:])\n    data = _in_file.read(length)\n    command = CommandType(header[:2])\n    data = data.decode('utf8')\n    logging.getLogger(__name__).debug('Received command, data: [%s]' % data)\n    return command, data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef json2space(in_x, name=ROOT):\n    out_y = copy.deepcopy(in_x)\n    if isinstance(in_x, dict):\n        if TYPE in in_x.keys():\n            _type = in_x[TYPE]\n            name = name + '-' + _type\n            _value = json2space(in_x[VALUE], name=name)\n            if _type == 'choice':\n                out_y = eval('hp.hp.'+_type)(name, _value)\n            else:\n                if _type in ['loguniform', 'qloguniform']:\n                    _value[:2] = np.log(_value[:2])\n                out_y = eval('hp.hp.' + _type)(name, *_value)\n        else:\n            out_y = dict()\n            for key in in_x.keys():\n                out_y[key] = json2space(in_x[key], name+'[%s]' % str(key))\n    elif isinstance(in_x, list):\n        out_y = list()\n        for i, x_i in enumerate(in_x):\n            out_y.append(json2space(x_i, name+'[%d]' % i))\n    else:\n        logger.info('in_x is not a dict or a list in json2space fuinction %s', str(in_x))\n    return out_y", "response": "Change json to search space in hyperopt."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json2parameter(in_x, parameter, name=ROOT):\n    out_y = copy.deepcopy(in_x)\n    if isinstance(in_x, dict):\n        if TYPE in in_x.keys():\n            _type = in_x[TYPE]\n            name = name + '-' + _type\n            if _type == 'choice':\n                _index = parameter[name]\n                out_y = {\n                    INDEX: _index,\n                    VALUE: json2parameter(in_x[VALUE][_index], parameter, name=name+'[%d]' % _index)\n                }\n            else:\n                out_y = parameter[name]\n        else:\n            out_y = dict()\n            for key in in_x.keys():\n                out_y[key] = json2parameter(\n                    in_x[key], parameter, name + '[%s]' % str(key))\n    elif isinstance(in_x, list):\n        out_y = list()\n        for i, x_i in enumerate(in_x):\n            out_y.append(json2parameter(x_i, parameter, name + '[%d]' % i))\n    else:\n        logger.info('in_x is not a dict or a list in json2space fuinction %s', str(in_x))\n    return out_y", "response": "Change json to parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_index(in_x, parameter):\n    if TYPE not in in_x: # if at the top level\n        out_y = dict()\n        for key, value in parameter.items():\n            out_y[key] = _add_index(in_x[key], value)\n        return out_y\n    elif isinstance(in_x, dict):\n        value_type = in_x[TYPE]\n        value_format = in_x[VALUE]\n        if value_type == \"choice\":\n            choice_name = parameter[0] if isinstance(parameter, list) else parameter\n            for pos, item in enumerate(value_format): # here value_format is a list\n                if isinstance(item, list): # this format is [\"choice_key\", format_dict]\n                    choice_key = item[0]\n                    choice_value_format = item[1]\n                    if choice_key == choice_name:\n                        return {INDEX: pos, VALUE: [choice_name, _add_index(choice_value_format, parameter[1])]}\n                elif choice_name == item:\n                    return {INDEX: pos, VALUE: item}\n        else:\n            return parameter", "response": "Add index to the parameter dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes index infromation from params Delete index infromation from params", "response": "def _split_index(params):\n    \"\"\"\n    Delete index infromation from params\n    \"\"\"\n    if isinstance(params, list):\n        return [params[0], _split_index(params[1])]\n    elif isinstance(params, dict):\n        if INDEX in params.keys():\n            return _split_index(params[VALUE])\n        result = dict()\n        for key in params:\n            result[key] = _split_index(params[key])\n        return result\n    else:\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the tuner class corresponding to the given algorithm name.", "response": "def _choose_tuner(self, algorithm_name):\n        \"\"\"\n        Parameters\n        ----------\n        algorithm_name : str\n            algorithm_name includes \"tpe\", \"random_search\" and anneal\"\n        \"\"\"\n        if algorithm_name == 'tpe':\n            return hp.tpe.suggest\n        if algorithm_name == 'random_search':\n            return hp.rand.suggest\n        if algorithm_name == 'anneal':\n            return hp.anneal.suggest\n        raise RuntimeError('Not support tuner algorithm in hyperopt.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate search space definition in tuner by search_space.", "response": "def update_search_space(self, search_space):\n        \"\"\"\n        Update search space definition in tuner by search_space in parameters.\n\n        Will called when first setup experiemnt or update search space in WebUI.\n\n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        self.json = search_space\n        search_space_instance = json2space(self.json)\n        rstate = np.random.RandomState()\n        trials = hp.Trials()\n        domain = hp.Domain(None, search_space_instance,\n                           pass_expr_memo_ctrl=None)\n        algorithm = self._choose_tuner(self.algorithm_name)\n        self.rval = hp.FMinIter(algorithm, domain, trials,\n                                max_evals=-1, rstate=rstate, verbose=0)\n        self.rval.catch_eval_exceptions = False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_parameters(self, parameter_id):\n        total_params = self.get_suggestion(random_search=False)\n        # avoid generating same parameter with concurrent trials because hyperopt doesn't support parallel mode\n        if total_params in self.total_data.values():\n            # but it can cause deplicate parameter rarely\n            total_params = self.get_suggestion(random_search=True)\n        self.total_data[parameter_id] = total_params\n        params = _split_index(total_params)\n        return params", "response": "Generates a set of trial ( hyper - ) parameters for the given parameter_id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrecord an observation of the objective function Parameters ---------- parameter_id : int parameters : dict value : dict/float if value is dict, it should have \"default\" key. value is final metrics of the trial.", "response": "def receive_trial_result(self, parameter_id, parameters, value):\n        \"\"\"\n        Record an observation of the objective function\n\n        Parameters\n        ----------\n        parameter_id : int\n        parameters : dict\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n            value is final metrics of the trial.\n        \"\"\"\n        reward = extract_scalar_reward(value)\n        # restore the paramsters contains '_index'\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        params = self.total_data[parameter_id]\n\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -reward\n\n        rval = self.rval\n        domain = rval.domain\n        trials = rval.trials\n\n        new_id = len(trials)\n\n        rval_specs = [None]\n        rval_results = [domain.new_result()]\n        rval_miscs = [dict(tid=new_id, cmd=domain.cmd, workdir=domain.workdir)]\n\n        vals = params\n        idxs = dict()\n\n        out_y = dict()\n        json2vals(self.json, vals, out_y)\n        vals = out_y\n        for key in domain.params:\n            if key in [VALUE, INDEX]:\n                continue\n            if key not in vals or vals[key] is None or vals[key] == []:\n                idxs[key] = vals[key] = []\n            else:\n                idxs[key] = [new_id]\n                vals[key] = [vals[key]]\n\n        self.miscs_update_idxs_vals(rval_miscs, idxs, vals,\n                                    idxs_map={new_id: new_id},\n                                    assert_all_vals_used=False)\n\n        trial = trials.new_trial_docs([new_id], rval_specs, rval_results, rval_miscs)[0]\n        trial['result'] = {'loss': reward, 'status': 'ok'}\n        trial['state'] = hp.JOB_STATE_DONE\n        trials.insert_trial_docs([trial])\n        trials.refresh()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nunpack the idxs - vals format into the list of dictionaries that is can have the same number of values.", "response": "def miscs_update_idxs_vals(self, miscs, idxs, vals,\n                               assert_all_vals_used=True,\n                               idxs_map=None):\n        \"\"\"\n        Unpack the idxs-vals format into the list of dictionaries that is\n        `misc`.\n\n        Parameters\n        ----------\n        idxs_map : dict\n            idxs_map is a dictionary of id->id mappings so that the misc['idxs'] can\n        contain different numbers than the idxs argument.\n        \"\"\"\n        if idxs_map is None:\n            idxs_map = {}\n\n        assert set(idxs.keys()) == set(vals.keys())\n\n        misc_by_id = {m['tid']: m for m in miscs}\n        for m in miscs:\n            m['idxs'] = dict([(key, []) for key in idxs])\n            m['vals'] = dict([(key, []) for key in idxs])\n\n        for key in idxs:\n            assert len(idxs[key]) == len(vals[key])\n            for tid, val in zip(idxs[key], vals[key]):\n                tid = idxs_map.get(tid, tid)\n                if assert_all_vals_used or tid in misc_by_id:\n                    misc_by_id[tid]['idxs'][key] = [tid]\n                    misc_by_id[tid]['vals'][key] = [val]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget suggestion from hyperopt", "response": "def get_suggestion(self, random_search=False):\n        \"\"\"get suggestion from hyperopt\n\n        Parameters\n        ----------\n        random_search : bool\n            flag to indicate random search or not (default: {False})\n\n        Returns\n        ----------\n        total_params : dict\n            parameter suggestion\n        \"\"\"\n\n        rval = self.rval\n        trials = rval.trials\n        algorithm = rval.algo\n        new_ids = rval.trials.new_trial_ids(1)\n        rval.trials.refresh()\n        random_state = rval.rstate.randint(2**31-1)\n        if random_search:\n            new_trials = hp.rand.suggest(new_ids, rval.domain, trials, random_state)\n        else:\n            new_trials = algorithm(new_ids, rval.domain, trials, random_state)\n        rval.trials.refresh()\n        vals = new_trials[0]['misc']['vals']\n        parameter = dict()\n        for key in vals:\n            try:\n                parameter[key] = vals[key][0].item()\n            except (KeyError, IndexError):\n                parameter[key] = None\n\n        # remove '_index' from json2parameter and save params-id\n        total_params = json2parameter(self.json, parameter)\n        return total_params"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_data(self, data):\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\" %(_completed_num, len(data)))\n            _completed_num += 1\n            if self.algorithm_name == 'random_search':\n                return\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\" %_value)\n                continue\n            self.supplement_data_num += 1\n            _parameter_id = '_'.join([\"ImportData\", str(self.supplement_data_num)])\n            self.total_data[_parameter_id] = _add_index(in_x=self.json, parameter=_params)\n            self.receive_trial_result(parameter_id=_parameter_id, parameters=_params, value=_value)\n        logger.info(\"Successfully import data to TPE/Anneal tuner.\")", "response": "Import additional data for tuning"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the lowest mu of a node.", "response": "def _lowest_mu(x, fun_prediction, fun_prediction_args,\n               x_bounds, x_types, minimize_constraints_fun):\n    '''\n    Calculate the lowest mu\n    '''\n    # This is only for step-wise optimization\n    x = lib_data.match_val_type(x, x_bounds, x_types)\n\n    mu = sys.maxsize\n    if (minimize_constraints_fun is None) or (minimize_constraints_fun(x) is True):\n        mu, _ = fun_prediction(x, *fun_prediction_args)\n    return mu"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the char embedding network for the QA model.", "response": "def build_char_states(self, char_embed, is_training, reuse, char_ids, char_lengths):\n        \"\"\"Build char embedding network for the QA model.\"\"\"\n        max_char_length = self.cfg.max_char_length\n\n        inputs = dropout(tf.nn.embedding_lookup(char_embed, char_ids),\n                         self.cfg.dropout, is_training)\n        inputs = tf.reshape(\n            inputs, shape=[max_char_length, -1, self.cfg.char_embed_dim])\n        char_lengths = tf.reshape(char_lengths, shape=[-1])\n        with tf.variable_scope('char_encoding', reuse=reuse):\n            cell_fw = XGRUCell(hidden_dim=self.cfg.char_embed_dim)\n            cell_bw = XGRUCell(hidden_dim=self.cfg.char_embed_dim)\n            _, (left_right, right_left) = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=cell_fw,\n                cell_bw=cell_bw,\n                sequence_length=char_lengths,\n                inputs=inputs,\n                time_major=True,\n                dtype=tf.float32\n            )\n\n        left_right = tf.reshape(left_right, shape=[-1, self.cfg.char_embed_dim])\n\n        right_left = tf.reshape(right_left, shape=[-1, self.cfg.char_embed_dim])\n\n        states = tf.concat([left_right, right_left], axis=1)\n        out_shape = tf.shape(char_ids)[1:3]\n        out_shape = tf.concat([out_shape, tf.constant(\n            value=[self.cfg.char_embed_dim * 2], dtype=tf.int32)], axis=0)\n        return tf.reshape(states, shape=out_shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_report_metric_data(self, data):\n        if data['type'] == 'FINAL':\n            self._handle_final_metric_data(data)\n        elif data['type'] == 'PERIODICAL':\n            if self.assessor is not None:\n                self._handle_intermediate_metric_data(data)\n            else:\n                pass\n        else:\n            raise ValueError('Data type not supported: {}'.format(data['type']))", "response": "Handle report metric data from the nni_manager"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the trial end event.", "response": "def handle_trial_end(self, data):\n        \"\"\"\n        data: it has three keys: trial_job_id, event, hyper_params\n             - trial_job_id: the id generated by training service\n             - event: the job's state\n             - hyper_params: the hyperparameters generated and returned by tuner\n        \"\"\"\n        trial_job_id = data['trial_job_id']\n        _ended_trials.add(trial_job_id)\n        if trial_job_id in _trial_history:\n            _trial_history.pop(trial_job_id)\n            if self.assessor is not None:\n                self.assessor.trial_end(trial_job_id, data['event'] == 'SUCCEEDED')\n        if self.tuner is not None:\n            self.tuner.trial_end(json_tricks.loads(data['hyper_params'])['parameter_id'], data['event'] == 'SUCCEEDED')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handle_final_metric_data(self, data):\n        id_ = data['parameter_id']\n        value = data['value']\n        if id_ in _customized_parameter_ids:\n            self.tuner.receive_customized_trial_result(id_, _trial_params[id_], value)\n        else:\n            self.tuner.receive_trial_result(id_, _trial_params[id_], value)", "response": "Handle final metric data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_intermediate_metric_data(self, data):\n        if data['type'] != 'PERIODICAL':\n            return\n        if self.assessor is None:\n            return\n\n        trial_job_id = data['trial_job_id']\n        if trial_job_id in _ended_trials:\n            return\n\n        history = _trial_history[trial_job_id]\n        history[data['sequence']] = data['value']\n        ordered_history = _sort_history(history)\n        if len(ordered_history) < data['sequence']:  # no user-visible update since last time\n            return\n\n        try:\n            result = self.assessor.assess_trial(trial_job_id, ordered_history)\n        except Exception as e:\n            _logger.exception('Assessor error')\n\n        if isinstance(result, bool):\n            result = AssessResult.Good if result else AssessResult.Bad\n        elif not isinstance(result, AssessResult):\n            msg = 'Result of Assessor.assess_trial must be an object of AssessResult, not %s'\n            raise RuntimeError(msg % type(result))\n\n        if result is AssessResult.Bad:\n            _logger.debug('BAD, kill %s', trial_job_id)\n            send(CommandType.KillTrialJob, json_tricks.dumps(trial_job_id))\n            # notify tuner\n            _logger.debug('env var: NNI_INCLUDE_INTERMEDIATE_RESULTS: [%s]', dispatcher_env_vars.NNI_INCLUDE_INTERMEDIATE_RESULTS)\n            if dispatcher_env_vars.NNI_INCLUDE_INTERMEDIATE_RESULTS == 'true':\n                self._earlystop_notify_tuner(data)\n        else:\n            _logger.debug('GOOD')", "response": "Process intermediate metric data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends last intermediate result as final result to tuner in case early stopped.", "response": "def _earlystop_notify_tuner(self, data):\n        \"\"\"Send last intermediate result as final result to tuner in case the\n        trial is early stopped.\n        \"\"\"\n        _logger.debug('Early stop notify tuner data: [%s]', data)\n        data['type'] = 'FINAL'\n        if multi_thread_enabled():\n            self._handle_final_metric_data(data)\n        else:\n            self.enqueue_command(CommandType.ReportMetricData, data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the reveive msgs to global variable", "response": "def parse_rev_args(receive_msg):\n    \"\"\" parse reveive msgs to global variable\n    \"\"\"\n    global trainloader\n    global testloader\n    global net\n\n    # Loading Data\n    logger.debug(\"Preparing data..\")\n\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n    y_train = to_categorical(y_train, 10)\n    y_test = to_categorical(y_test, 10)\n    x_train = x_train.reshape(x_train.shape+(1,)).astype(\"float32\")\n    x_test = x_test.reshape(x_test.shape+(1,)).astype(\"float32\")\n    x_train /= 255.0\n    x_test /= 255.0\n    trainloader = (x_train, y_train)\n    testloader = (x_test, y_test)\n\n    # Model\n    logger.debug(\"Building model..\")\n    net = build_graph_from_json(receive_msg)\n\n    # parallel model\n    try:\n        available_devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n        gpus = len(available_devices.split(\",\"))\n        if gpus > 1:\n            net = multi_gpu_model(net, gpus)\n    except KeyError:\n        logger.debug(\"parallel model not support in this config settings\")\n\n    if args.optimizer == \"SGD\":\n        optimizer = SGD(lr=args.learning_rate, momentum=0.9, decay=args.weight_decay)\n    if args.optimizer == \"Adadelta\":\n        optimizer = Adadelta(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == \"Adagrad\":\n        optimizer = Adagrad(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == \"Adam\":\n        optimizer = Adam(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == \"Adamax\":\n        optimizer = Adamax(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == \"RMSprop\":\n        optimizer = RMSprop(lr=args.learning_rate, decay=args.weight_decay)\n\n    # Compile the model\n    net.compile(\n        loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n    )\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef train_eval():\n\n    global trainloader\n    global testloader\n    global net\n\n    (x_train, y_train) = trainloader\n    (x_test, y_test) = testloader\n\n    # train procedure\n    net.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=args.batch_size,\n        validation_data=(x_test, y_test),\n        epochs=args.epochs,\n        shuffle=True,\n        callbacks=[\n            SendMetrics(),\n            EarlyStopping(min_delta=0.001, patience=10),\n            TensorBoard(log_dir=TENSORBOARD_DIR),\n        ],\n    )\n\n    # trial report final acc to tuner\n    _, acc = net.evaluate(x_test, y_test)\n    logger.debug(\"Final result is: %.3f\", acc)\n    nni.report_final_result(acc)", "response": "train and eval the model\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning on end of each epoch", "response": "def on_epoch_end(self, epoch, logs=None):\n        \"\"\"\n        Run on end of each epoch\n        \"\"\"\n        if logs is None:\n            logs = dict()\n        logger.debug(logs)\n        nni.report_intermediate_result(logs[\"val_acc\"])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_bracket_parameter_id(brackets_id, brackets_curr_decay, increased_id=-1):\n    if increased_id == -1:\n        increased_id = str(create_parameter_id())\n    params_id = '_'.join([str(brackets_id),\n                          str(brackets_curr_decay),\n                          increased_id])\n    return params_id", "response": "Create a full id for a specific bracket s hyperparameter configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates values for hyperparameters from hyperparameter space i. e. x.", "response": "def json2paramater(ss_spec, random_state):\n    \"\"\"Randomly generate values for hyperparameters from hyperparameter space i.e., x.\n    \n    Parameters\n    ----------\n    ss_spec:\n        hyperparameter space\n    random_state:\n        random operator to generate random values\n\n    Returns\n    -------\n    Parameter:\n        Parameters in this experiment\n    \"\"\"\n    if isinstance(ss_spec, dict):\n        if '_type' in ss_spec.keys():\n            _type = ss_spec['_type']\n            _value = ss_spec['_value']\n            if _type == 'choice':\n                _index = random_state.randint(len(_value))\n                chosen_params = json2paramater(ss_spec['_value'][_index], random_state)\n            else:\n                chosen_params = eval('parameter_expressions.' + # pylint: disable=eval-used\n                                     _type)(*(_value + [random_state]))\n        else:\n            chosen_params = dict()\n            for key in ss_spec.keys():\n                chosen_params[key] = json2paramater(ss_spec[key], random_state)\n    elif isinstance(ss_spec, list):\n        chosen_params = list()\n        for _, subspec in enumerate(ss_spec):\n            chosen_params.append(json2paramater(subspec, random_state))\n    else:\n        chosen_params = copy.deepcopy(ss_spec)\n    return chosen_params"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the values of n and r for the next round", "response": "def get_n_r(self):\n        \"\"\"return the values of n and r for the next round\"\"\"\n        return math.floor(self.n / self.eta**self.i + _epsilon), math.floor(self.r * self.eta**self.i + _epsilon)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nincreasing i by 1.", "response": "def increase_i(self):\n        \"\"\"i means the ith round. Increase i by 1\"\"\"\n        self.i += 1\n        if self.i > self.bracket_id:\n            self.no_more_trial = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating trial s latest result with its sequence number or batch number", "response": "def set_config_perf(self, i, parameter_id, seq, value):\n        \"\"\"update trial's latest result with its sequence number, e.g., epoch number or batch number\n        \n        Parameters\n        ----------\n        i: int\n            the ith round\n        parameter_id: int\n            the id of the trial/parameter\n        seq: int\n            sequence number, e.g., epoch number or batch number\n        value: int\n            latest result with sequence number seq\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if parameter_id in self.configs_perf[i]:\n            if self.configs_perf[i][parameter_id][0] < seq:\n                self.configs_perf[i][parameter_id] = [seq, value]\n        else:\n            self.configs_perf[i][parameter_id] = [seq, value]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_hyperparameter_configurations(self, num, r, searchspace_json, random_state): # pylint: disable=invalid-name\n        global _KEY # pylint: disable=global-statement\n        assert self.i == 0\n        hyperparameter_configs = dict()\n        for _ in range(num):\n            params_id = create_bracket_parameter_id(self.bracket_id, self.i)\n            params = json2paramater(searchspace_json, random_state)\n            params[_KEY] = r\n            hyperparameter_configs[params_id] = params\n        self._record_hyper_configs(hyperparameter_configs)\n        return [[key, value] for key, value in hyperparameter_configs.items()]", "response": "Generate num hyperparameter configurations from search space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _record_hyper_configs(self, hyper_configs):\n        self.hyper_configs.append(hyper_configs)\n        self.configs_perf.append(dict())\n        self.num_finished_configs.append(0)\n        self.num_configs_to_run.append(len(hyper_configs))\n        self.increase_i()", "response": "record the generated hyperconfigs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _request_one_trial_job(self):\n        if not self.generated_hyper_configs:\n            if self.curr_s < 0:\n                self.curr_s = self.s_max\n            _logger.debug('create a new bracket, self.curr_s=%d', self.curr_s)\n            self.brackets[self.curr_s] = Bracket(self.curr_s, self.s_max, self.eta, self.R, self.optimize_mode)\n            next_n, next_r = self.brackets[self.curr_s].get_n_r()\n            _logger.debug('new bracket, next_n=%d, next_r=%d', next_n, next_r)\n            assert self.searchspace_json is not None and self.random_state is not None\n            generated_hyper_configs = self.brackets[self.curr_s].get_hyperparameter_configurations(next_n, next_r,\n                                                                                                   self.searchspace_json,\n                                                                                                   self.random_state)\n            self.generated_hyper_configs = generated_hyper_configs.copy()\n            self.curr_s -= 1\n\n        assert self.generated_hyper_configs\n        params = self.generated_hyper_configs.pop()\n        ret = {\n            'parameter_id': params[0],\n            'parameter_source': 'algorithm',\n            'parameters': params[1]\n        }\n        send(CommandType.NewTrialJob, json_tricks.dumps(ret))", "response": "get one trial job i. e. one hyperparameter configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the search space with new data", "response": "def handle_update_search_space(self, data):\n        \"\"\"data: JSON object, which is search space\n        \n        Parameters\n        ----------\n        data: int\n            number of trial jobs\n        \"\"\"\n        self.searchspace_json = data\n        self.random_state = np.random.RandomState()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_trial_end(self, data):\n        hyper_params = json_tricks.loads(data['hyper_params'])\n        bracket_id, i, _ = hyper_params['parameter_id'].split('_')\n        hyper_configs = self.brackets[int(bracket_id)].inform_trial_end(int(i))\n        if hyper_configs is not None:\n            _logger.debug('bracket %s next round %s, hyper_configs: %s', bracket_id, i, hyper_configs)\n            self.generated_hyper_configs = self.generated_hyper_configs + hyper_configs\n            for _ in range(self.credit):\n                if not self.generated_hyper_configs:\n                    break\n                params = self.generated_hyper_configs.pop()\n                ret = {\n                    'parameter_id': params[0],\n                    'parameter_source': 'algorithm',\n                    'parameters': params[1]\n                }\n                send(CommandType.NewTrialJob, json_tricks.dumps(ret))\n                self.credit -= 1", "response": "Handle trial end command."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_report_metric_data(self, data):\n        value = extract_scalar_reward(data['value'])\n        bracket_id, i, _ = data['parameter_id'].split('_')\n        bracket_id = int(bracket_id)\n        if data['type'] == 'FINAL':\n            # sys.maxsize indicates this value is from FINAL metric data, because data['sequence'] from FINAL metric\n            # and PERIODICAL metric are independent, thus, not comparable.\n            self.brackets[bracket_id].set_config_perf(int(i), data['parameter_id'], sys.maxsize, value)\n            self.completed_hyper_configs.append(data)\n        elif data['type'] == 'PERIODICAL':\n            self.brackets[bracket_id].set_config_perf(int(i), data['parameter_id'], data['sequence'], value)\n        else:\n            raise ValueError('Data type not supported: {}'.format(data['type']))", "response": "Handles the report metric data from the hypervisor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_parameters(self, parameter_id):\n        if len(self.population) <= 0:\n            logger.debug(\"the len of poplution lower than zero.\")\n            raise Exception('The population is empty')\n        pos = -1\n        for i in range(len(self.population)):\n            if self.population[i].result == None:\n                pos = i\n                break\n        if pos != -1:\n            indiv = copy.deepcopy(self.population[pos])\n            self.population.pop(pos)\n            temp = json.loads(graph_dumps(indiv.config))\n        else:\n            random.shuffle(self.population)\n            if self.population[0].result < self.population[1].result:\n                self.population[0] = self.population[1]\n            indiv = copy.deepcopy(self.population[0])\n            self.population.pop(1)\n            indiv.mutation()\n            graph = indiv.config\n            temp =  json.loads(graph_dumps(graph))\n        logger.debug('generate_parameter return value is:')\n        logger.debug(temp)\n        return temp", "response": "Returns a set of trial graph config as a serializable object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef receive_trial_result(self, parameter_id, parameters, value):\n        '''\n        Record an observation of the objective function\n        parameter_id : int\n        parameters : dict of parameters\n        value: final metrics of the trial, including reward\n        '''\n        reward = extract_scalar_reward(value)\n        if self.optimize_mode is OptimizeMode.Minimize:\n            reward = -reward\n\n        logger.debug('receive trial result is:\\n')\n        logger.debug(str(parameters))\n        logger.debug(str(reward))\n\n        indiv = Individual(graph_loads(parameters), result=reward)\n        self.population.append(indiv)\n        return", "response": "Record an observation of the objective function and the trial function and the final metrics of the trial."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a CNN. Args: model_len: An integer. Number of convolutional layers. model_width: An integer. Number of filters for the convolutional layers. Returns: An instance of the class Graph. Represents the neural architecture graph of the generated model.", "response": "def generate(self, model_len=None, model_width=None):\n        \"\"\"Generates a CNN.\n        Args:\n            model_len: An integer. Number of convolutional layers.\n            model_width: An integer. Number of filters for the convolutional layers.\n        Returns:\n            An instance of the class Graph. Represents the neural architecture graph of the generated model.\n        \"\"\"\n\n        if model_len is None:\n            model_len = Constant.MODEL_LEN\n        if model_width is None:\n            model_width = Constant.MODEL_WIDTH\n        pooling_len = int(model_len / 4)\n        graph = Graph(self.input_shape, False)\n        temp_input_channel = self.input_shape[-1]\n        output_node_id = 0\n        stride = 1\n        for i in range(model_len):\n            output_node_id = graph.add_layer(StubReLU(), output_node_id)\n            output_node_id = graph.add_layer(\n                self.batch_norm(graph.node_list[output_node_id].shape[-1]), output_node_id\n            )\n            output_node_id = graph.add_layer(\n                self.conv(temp_input_channel, model_width, kernel_size=3, stride=stride),\n                output_node_id,\n            )\n            temp_input_channel = model_width\n            if pooling_len == 0 or ((i + 1) % pooling_len == 0 and i != model_len - 1):\n                output_node_id = graph.add_layer(self.pooling(), output_node_id)\n\n        output_node_id = graph.add_layer(self.global_avg_pooling(), output_node_id)\n        output_node_id = graph.add_layer(\n            self.dropout(Constant.CONV_DROPOUT_RATE), output_node_id\n        )\n        output_node_id = graph.add_layer(\n            StubDense(graph.node_list[output_node_id].shape[0], model_width),\n            output_node_id,\n        )\n        output_node_id = graph.add_layer(StubReLU(), output_node_id)\n        graph.add_layer(StubDense(model_width, self.n_output_node), output_node_id)\n        return graph"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate(self, model_len=None, model_width=None):\n        if model_len is None:\n            model_len = Constant.MODEL_LEN\n        if model_width is None:\n            model_width = Constant.MODEL_WIDTH\n        if isinstance(model_width, list) and not len(model_width) == model_len:\n            raise ValueError(\"The length of 'model_width' does not match 'model_len'\")\n        elif isinstance(model_width, int):\n            model_width = [model_width] * model_len\n\n        graph = Graph(self.input_shape, False)\n        output_node_id = 0\n        n_nodes_prev_layer = self.input_shape[0]\n        for width in model_width:\n            output_node_id = graph.add_layer(\n                StubDense(n_nodes_prev_layer, width), output_node_id\n            )\n            output_node_id = graph.add_layer(\n                StubDropout1d(Constant.MLP_DROPOUT_RATE), output_node_id\n            )\n            output_node_id = graph.add_layer(StubReLU(), output_node_id)\n            n_nodes_prev_layer = width\n\n        graph.add_layer(StubDense(n_nodes_prev_layer, self.n_output_node), output_node_id)\n        return graph", "response": "Generates a multi - layer Perceptron."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_search_space(code_dir):\n    search_space = {}\n    \n    if code_dir.endswith(slash):\n        code_dir = code_dir[:-1]\n\n    for subdir, _, files in os.walk(code_dir):\n        # generate module name from path\n        if subdir == code_dir:\n            package = ''\n        else:\n            assert subdir.startswith(code_dir + slash), subdir\n            prefix_len = len(code_dir) + 1\n            package = subdir[prefix_len:].replace(slash, '.') + '.'\n\n        for file_name in files:\n            if file_name.endswith('.py'):\n                path = os.path.join(subdir, file_name)\n                module = package + file_name[:-3]\n                search_space.update(_generate_file_search_space(path, module))\n\n    return search_space", "response": "Generate search space from Python source code."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpand annotations in user code.", "response": "def expand_annotations(src_dir, dst_dir):\n    \"\"\"Expand annotations in user code.\n    Return dst_dir if annotation detected; return src_dir if not.\n    src_dir: directory path of user code (str)\n    dst_dir: directory to place generated files (str)\n    \"\"\"\n    if src_dir[-1] == slash:\n        src_dir = src_dir[:-1]\n    \n    if dst_dir[-1] == slash:\n        dst_dir = dst_dir[:-1]\n\n    annotated = False\n\n    for src_subdir, dirs, files in os.walk(src_dir):\n        assert src_subdir.startswith(src_dir)\n        dst_subdir = src_subdir.replace(src_dir, dst_dir, 1)\n        os.makedirs(dst_subdir, exist_ok=True)\n\n        for file_name in files:\n            src_path = os.path.join(src_subdir, file_name)\n            dst_path = os.path.join(dst_subdir, file_name)\n            if file_name.endswith('.py'):\n                annotated |= _expand_file_annotations(src_path, dst_path)\n            else:\n                shutil.copyfile(src_path, dst_path)\n\n        for dir_name in dirs:\n            os.makedirs(os.path.join(dst_subdir, dir_name), exist_ok=True)\n\n    return dst_dir if annotated else src_dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gen_send_stdout_url(ip, port):\n    '''Generate send stdout url'''\n    return '{0}:{1}{2}{3}/{4}/{5}'.format(BASE_URL.format(ip), port, API_ROOT_URL, STDOUT_API, NNI_EXP_ID, NNI_TRIAL_JOB_ID)", "response": "Generate send stdout url"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_send_version_url(ip, port):\n    '''Generate send error url'''\n    return '{0}:{1}{2}{3}/{4}/{5}'.format(BASE_URL.format(ip), port, API_ROOT_URL, VERSION_API, NNI_EXP_ID, NNI_TRIAL_JOB_ID)", "response": "Generate send error url"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_digit(value, start, end):\n    '''validate if a digit is valid'''\n    if not str(value).isdigit() or int(value) < start or int(value) > end:\n        raise ValueError('%s must be a digit from %s to %s' % (value, start, end))", "response": "validate if a digit is valid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_dispatcher(args):\n    '''validate if the dispatcher of the experiment supports importing data'''\n    nni_config = Config(get_config_filename(args)).get_config('experimentConfig')\n    if nni_config.get('tuner') and nni_config['tuner'].get('builtinTunerName'):\n        dispatcher_name = nni_config['tuner']['builtinTunerName']\n    elif nni_config.get('advisor') and nni_config['advisor'].get('builtinAdvisorName'):\n        dispatcher_name = nni_config['advisor']['builtinAdvisorName']\n    else: # otherwise it should be a customized one\n        return\n    if dispatcher_name not in TUNERS_SUPPORTING_IMPORT_DATA:\n        if dispatcher_name in TUNERS_NO_NEED_TO_IMPORT_DATA:\n            print_warning(\"There is no need to import data for %s\" % dispatcher_name)\n            exit(0)\n        else:\n            print_error(\"%s does not support importing addtional data\" % dispatcher_name)\n            exit(1)", "response": "validate if the dispatcher of the experiment supports importing data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_search_space(path):\n    '''load search space content'''\n    content = json.dumps(get_json_content(path))\n    if not content:\n        raise ValueError('searchSpace file should not be empty')\n    return content", "response": "load search space content"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_experiment_profile(args, key, value):\n    '''call restful server to update experiment profile'''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    running, _ = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(experiment_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            experiment_profile = json.loads(response.text)\n            experiment_profile['params'][key] = value\n            response = rest_put(experiment_url(rest_port)+get_query_type(key), json.dumps(experiment_profile), REST_TIME_OUT)\n            if response and check_response(response):\n                return response\n    else:\n        print_error('Restful server is not running...')\n    return None", "response": "call restful server to update experiment profile"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef import_data(args):\n    '''import additional data to the experiment'''\n    validate_file(args.filename)\n    validate_dispatcher(args)\n    content = load_search_space(args.filename)\n    args.port = get_experiment_port(args)\n    if args.port is not None:\n        if import_data_to_restful_server(args, content):\n            pass\n        else:\n            print_error('Import data failed!')", "response": "import additional data to the experiment"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_data_to_restful_server(args, content):\n    '''call restful server to import data to the experiment'''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    running, _ = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_post(import_data_url(rest_port), content, REST_TIME_OUT)\n        if response and check_response(response):\n            return response\n    else:\n        print_error('Restful server is not running...')\n    return None", "response": "call restful server to import data to the experiment"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a keras layer into a real keras layer.", "response": "def to_real_keras_layer(layer):\n    ''' real keras layer.\n    '''\n    from keras import layers\n\n    if is_layer(layer, \"Dense\"):\n        return layers.Dense(layer.units, input_shape=(layer.input_units,))\n    if is_layer(layer, \"Conv\"):\n        return layers.Conv2D(\n            layer.filters,\n            layer.kernel_size,\n            input_shape=layer.input.shape,\n            padding=\"same\",\n        )  # padding\n    if is_layer(layer, \"Pooling\"):\n        return layers.MaxPool2D(2)\n    if is_layer(layer, \"BatchNormalization\"):\n        return layers.BatchNormalization(input_shape=layer.input.shape)\n    if is_layer(layer, \"Concatenate\"):\n        return layers.Concatenate()\n    if is_layer(layer, \"Add\"):\n        return layers.Add()\n    if is_layer(layer, \"Dropout\"):\n        return keras_dropout(layer, layer.rate)\n    if is_layer(layer, \"ReLU\"):\n        return layers.Activation(\"relu\")\n    if is_layer(layer, \"Softmax\"):\n        return layers.Activation(\"softmax\")\n    if is_layer(layer, \"Flatten\"):\n        return layers.Flatten()\n    if is_layer(layer, \"GlobalAveragePooling\"):\n        return layers.GlobalAveragePooling2D()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_layer(layer, layer_type):\n    '''judge the layer type.\n    Returns:\n        boolean -- True or False\n    '''\n\n    if layer_type == \"Input\":\n        return isinstance(layer, StubInput)\n    elif layer_type == \"Conv\":\n        return isinstance(layer, StubConv)\n    elif layer_type == \"Dense\":\n        return isinstance(layer, (StubDense,))\n    elif layer_type == \"BatchNormalization\":\n        return isinstance(layer, (StubBatchNormalization,))\n    elif layer_type == \"Concatenate\":\n        return isinstance(layer, (StubConcatenate,))\n    elif layer_type == \"Add\":\n        return isinstance(layer, (StubAdd,))\n    elif layer_type == \"Pooling\":\n        return isinstance(layer, StubPooling)\n    elif layer_type == \"Dropout\":\n        return isinstance(layer, (StubDropout,))\n    elif layer_type == \"Softmax\":\n        return isinstance(layer, (StubSoftmax,))\n    elif layer_type == \"ReLU\":\n        return isinstance(layer, (StubReLU,))\n    elif layer_type == \"Flatten\":\n        return isinstance(layer, (StubFlatten,))\n    elif layer_type == \"GlobalAveragePooling\":\n        return isinstance(layer, StubGlobalPooling)", "response": "judge the layer type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the layer description from the node_to_id dictionary", "response": "def layer_description_extractor(layer, node_to_id):\n    '''get layer description.\n    '''\n\n    layer_input = layer.input\n    layer_output = layer.output\n    if layer_input is not None:\n        if isinstance(layer_input, Iterable):\n            layer_input = list(map(lambda x: node_to_id[x], layer_input))\n        else:\n            layer_input = node_to_id[layer_input]\n\n    if layer_output is not None:\n        layer_output = node_to_id[layer_output]\n\n    if isinstance(layer, StubConv):\n        return (\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.input_channel,\n            layer.filters,\n            layer.kernel_size,\n            layer.stride,\n            layer.padding,\n        )\n    elif isinstance(layer, (StubDense,)):\n        return [\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.input_units,\n            layer.units,\n        ]\n    elif isinstance(layer, (StubBatchNormalization,)):\n        return (type(layer).__name__, layer_input, layer_output, layer.num_features)\n    elif isinstance(layer, (StubDropout,)):\n        return (type(layer).__name__, layer_input, layer_output, layer.rate)\n    elif isinstance(layer, StubPooling):\n        return (\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.kernel_size,\n            layer.stride,\n            layer.padding,\n        )\n    else:\n        return (type(layer).__name__, layer_input, layer_output)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef layer_description_builder(layer_information, id_to_node):\n    '''build layer from description.\n    '''\n    # pylint: disable=W0123\n    layer_type = layer_information[0]\n\n    layer_input_ids = layer_information[1]\n    if isinstance(layer_input_ids, Iterable):\n        layer_input = list(map(lambda x: id_to_node[x], layer_input_ids))\n    else:\n        layer_input = id_to_node[layer_input_ids]\n    layer_output = id_to_node[layer_information[2]]\n    if layer_type.startswith(\"StubConv\"):\n        input_channel = layer_information[3]\n        filters = layer_information[4]\n        kernel_size = layer_information[5]\n        stride = layer_information[6]\n        return eval(layer_type)(\n            input_channel, filters, kernel_size, stride, layer_input, layer_output\n        )\n    elif layer_type.startswith(\"StubDense\"):\n        input_units = layer_information[3]\n        units = layer_information[4]\n        return eval(layer_type)(input_units, units, layer_input, layer_output)\n    elif layer_type.startswith(\"StubBatchNormalization\"):\n        num_features = layer_information[3]\n        return eval(layer_type)(num_features, layer_input, layer_output)\n    elif layer_type.startswith(\"StubDropout\"):\n        rate = layer_information[3]\n        return eval(layer_type)(rate, layer_input, layer_output)\n    elif layer_type.startswith(\"StubPooling\"):\n        kernel_size = layer_information[3]\n        stride = layer_information[4]\n        padding = layer_information[5]\n        return eval(layer_type)(kernel_size, stride, padding, layer_input, layer_output)\n    else:\n        return eval(layer_type)(layer_input, layer_output)", "response": "build layer from description."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the GRU cell.", "response": "def build(self, x, h, mask=None):\n        '''\n        Build the GRU cell.\n        '''\n        xw = tf.split(tf.matmul(x, self.w_matrix) + self.bias, 3, 1)\n        hu = tf.split(tf.matmul(h, self.U), 3, 1)\n        r = tf.sigmoid(xw[0] + hu[0])\n        z = tf.sigmoid(xw[1] + hu[1])\n        h1 = tf.tanh(xw[2] + r * hu[2])\n        next_h = h1 * (1 - z) + h * z\n        if mask is not None:\n            next_h = next_h * mask + h * (1 - mask)\n        return next_h"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef conv2d(x_input, w_matrix):\n    return tf.nn.conv2d(x_input, w_matrix, strides=[1, 1, 1, 1], padding='SAME')", "response": "conv2d returns a 2d convolution layer with full stride."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef max_pool(x_input, pool_size):\n    return tf.nn.max_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n                          strides=[1, pool_size, pool_size, 1], padding='SAME')", "response": "max_pool downsamples a feature map by 2X."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the whole neural network for the QA model.", "response": "def build_net(self, is_training):\n        \"\"\"Build the whole neural network for the QA model.\"\"\"\n        cfg = self.cfg\n        with tf.device('/cpu:0'):\n            word_embed = tf.get_variable(\n                name='word_embed', initializer=self.embed, dtype=tf.float32, trainable=False)\n            char_embed = tf.get_variable(name='char_embed',\n                                         shape=[cfg.char_vcb_size,\n                                                cfg.char_embed_dim],\n                                         dtype=tf.float32)\n\n        # [query_length, batch_size]\n        self.query_word = tf.placeholder(dtype=tf.int32,\n                                         shape=[None, None],\n                                         name='query_word')\n        self.query_mask = tf.placeholder(dtype=tf.float32,\n                                         shape=[None, None],\n                                         name='query_mask')\n        # [batch_size]\n        self.query_lengths = tf.placeholder(\n            dtype=tf.int32, shape=[None], name='query_lengths')\n\n        # [passage_length, batch_size]\n        self.passage_word = tf.placeholder(\n            dtype=tf.int32, shape=[None, None], name='passage_word')\n        self.passage_mask = tf.placeholder(\n            dtype=tf.float32, shape=[None, None], name='passage_mask')\n        # [batch_size]\n        self.passage_lengths = tf.placeholder(\n            dtype=tf.int32, shape=[None], name='passage_lengths')\n\n        if is_training:\n            self.answer_begin = tf.placeholder(\n                dtype=tf.int32, shape=[None], name='answer_begin')\n            self.answer_end = tf.placeholder(\n                dtype=tf.int32, shape=[None], name='answer_end')\n\n        self.query_char_ids = tf.placeholder(dtype=tf.int32,\n                                             shape=[\n                                                 self.cfg.max_char_length, None, None],\n                                             name='query_char_ids')\n        # sequence_length, batch_size\n        self.query_char_lengths = tf.placeholder(\n            dtype=tf.int32, shape=[None, None], name='query_char_lengths')\n\n        self.passage_char_ids = tf.placeholder(dtype=tf.int32,\n                                               shape=[\n                                                   self.cfg.max_char_length, None, None],\n                                               name='passage_char_ids')\n        # sequence_length, batch_size\n        self.passage_char_lengths = tf.placeholder(dtype=tf.int32,\n                                                   shape=[None, None],\n                                                   name='passage_char_lengths')\n\n        query_char_states = self.build_char_states(char_embed=char_embed,\n                                                   is_training=is_training,\n                                                   reuse=False,\n                                                   char_ids=self.query_char_ids,\n                                                   char_lengths=self.query_char_lengths)\n\n        passage_char_states = self.build_char_states(char_embed=char_embed,\n                                                     is_training=is_training,\n                                                     reuse=True,\n                                                     char_ids=self.passage_char_ids,\n                                                     char_lengths=self.passage_char_lengths)\n\n        with tf.variable_scope(\"encoding\") as scope:\n            query_states = tf.concat([tf.nn.embedding_lookup(\n                word_embed, self.query_word), query_char_states], axis=2)\n            scope.reuse_variables()\n            passage_states = tf.concat([tf.nn.embedding_lookup(\n                word_embed, self.passage_word), passage_char_states], axis=2)\n        passage_states = tf.transpose(passage_states, perm=[1, 0, 2])\n        query_states = tf.transpose(query_states, perm=[1, 0, 2])\n        self.passage_states = passage_states\n        self.query_states = query_states\n\n        output, output2 = graph_to_network(passage_states, query_states,\n                                           self.passage_lengths, self.query_lengths,\n                                           self.graph, self.cfg.dropout,\n                                           is_training, num_heads=cfg.num_heads,\n                                           rnn_units=cfg.rnn_units)\n\n        passage_att_mask = self.passage_mask\n        batch_size_x = tf.shape(self.query_lengths)\n        answer_h = tf.zeros(\n            tf.concat([batch_size_x, tf.constant([cfg.ptr_dim], dtype=tf.int32)], axis=0))\n\n        answer_context = tf.reduce_mean(output2, axis=1)\n\n        query_init_w = tf.get_variable(\n            'query_init_w', shape=[output2.get_shape().as_list()[-1], cfg.ptr_dim])\n        self.query_init = query_init_w\n        answer_context = tf.matmul(answer_context, query_init_w)\n\n        output = tf.transpose(output, perm=[1, 0, 2])\n\n        with tf.variable_scope('answer_ptr_layer'):\n            ptr_att = DotAttention('ptr',\n                                   hidden_dim=cfg.ptr_dim,\n                                   is_vanilla=self.cfg.att_is_vanilla,\n                                   is_identity_transform=self.cfg.att_is_id,\n                                   need_padding=self.cfg.att_need_padding)\n            answer_pre_compute = ptr_att.get_pre_compute(output)\n            ptr_gru = XGRUCell(hidden_dim=cfg.ptr_dim)\n            begin_prob, begin_logits = ptr_att.get_prob(output, answer_context, passage_att_mask,\n                                                        answer_pre_compute, True)\n            att_state = ptr_att.get_att(output, begin_prob)\n            (_, answer_h) = ptr_gru.call(inputs=att_state, state=answer_h)\n            answer_context = answer_h\n            end_prob, end_logits = ptr_att.get_prob(output, answer_context,\n                                                    passage_att_mask, answer_pre_compute,\n                                                    True)\n\n        self.begin_prob = tf.transpose(begin_prob, perm=[1, 0])\n        self.end_prob = tf.transpose(end_prob, perm=[1, 0])\n        begin_logits = tf.transpose(begin_logits, perm=[1, 0])\n        end_logits = tf.transpose(end_logits, perm=[1, 0])\n\n        if is_training:\n            def label_smoothing(inputs, masks, epsilon=0.1):\n                \"\"\"Modify target for label smoothing.\"\"\"\n                epsilon = cfg.labelsmoothing\n                num_of_channel = tf.shape(inputs)[-1]  # number of channels\n                inputs = tf.cast(inputs, tf.float32)\n                return (((1 - epsilon) * inputs) + (epsilon /\n                                                    tf.cast(num_of_channel, tf.float32))) * masks\n            cost1 = tf.reduce_mean(\n                tf.losses.softmax_cross_entropy(label_smoothing(\n                    tf.one_hot(self.answer_begin,\n                               depth=tf.shape(self.passage_word)[0]),\n                    tf.transpose(self.passage_mask, perm=[1, 0])), begin_logits))\n            cost2 = tf.reduce_mean(\n                tf.losses.softmax_cross_entropy(\n                    label_smoothing(tf.one_hot(self.answer_end,\n                                               depth=tf.shape(self.passage_word)[0]),\n                                    tf.transpose(self.passage_mask, perm=[1, 0])), end_logits))\n\n            reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            l2_loss = tf.reduce_sum(reg_ws)\n            loss = cost1 + cost2 + l2_loss\n            self.loss = loss\n\n            optimizer = tf.train.AdamOptimizer(learning_rate=cfg.learning_rate)\n            self.train_op = optimizer.minimize(self.loss)\n\n        return tf.stack([self.begin_prob, self.end_prob])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling check_output command to read content from a file", "response": "def check_output_command(file_path, head=None, tail=None):\n    '''call check_output command to read content from a file'''\n    if os.path.exists(file_path):\n        if sys.platform == 'win32':\n            cmds = ['powershell.exe', 'type', file_path]\n            if head:\n                cmds += ['|', 'select', '-first', str(head)]\n            elif tail:\n                cmds += ['|', 'select', '-last', str(tail)]\n            return check_output(cmds, shell=True).decode('utf-8')\n        else:\n            cmds = ['cat', file_path]\n            if head:\n                cmds = ['head', '-' + str(head), file_path]\n            elif tail:\n                cmds = ['tail', '-' + str(tail), file_path]\n            return check_output(cmds, shell=False).decode('utf-8')\n    else:\n        print_error('{0} does not exist!'.format(file_path))\n        exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninstall python package from pip", "response": "def install_package_command(package_name):\n    '''install python package from pip'''\n    #TODO refactor python logic\n    if sys.platform == \"win32\":\n        cmds = 'python -m pip install --user {0}'.format(package_name)\n    else:\n        cmds = 'python3 -m pip install --user {0}'.format(package_name)\n    call(cmds, shell=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_params():\n    ''' Get parameters from command line '''\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_dir\", type=str, default='/tmp/tensorflow/mnist/input_data', help=\"data directory\")\n    parser.add_argument(\"--dropout_rate\", type=float, default=0.5, help=\"dropout rate\")\n    parser.add_argument(\"--channel_1_num\", type=int, default=32)\n    parser.add_argument(\"--channel_2_num\", type=int, default=64)\n    parser.add_argument(\"--conv_size\", type=int, default=5)\n    parser.add_argument(\"--pool_size\", type=int, default=2)\n    parser.add_argument(\"--hidden_size\", type=int, default=1024)\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n    parser.add_argument(\"--batch_num\", type=int, default=2700)\n    parser.add_argument(\"--batch_size\", type=int, default=32)\n\n    args, _ = parser.parse_known_args()\n    return args", "response": "Get parameters from command line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild the neural network for the master image.", "response": "def build_network(self):\n        '''\n        Building network for mnist\n        '''\n\n        # Reshape to use within a convolutional neural net.\n        # Last dimension is for \"features\" - there is only one here, since images are\n        # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n        with tf.name_scope('reshape'):\n            try:\n                input_dim = int(math.sqrt(self.x_dim))\n            except:\n                print(\n                    'input dim cannot be sqrt and reshape. input dim: ' + str(self.x_dim))\n                logger.debug(\n                    'input dim cannot be sqrt and reshape. input dim: %s', str(self.x_dim))\n                raise\n            x_image = tf.reshape(self.images, [-1, input_dim, input_dim, 1])\n\n        # First convolutional layer - maps one grayscale image to 32 feature maps.\n        with tf.name_scope('conv1'):\n            w_conv1 = weight_variable(\n                [self.conv_size, self.conv_size, 1, self.channel_1_num])\n            b_conv1 = bias_variable([self.channel_1_num])\n            h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n\n        # Pooling layer - downsamples by 2X.\n        with tf.name_scope('pool1'):\n            h_pool1 = max_pool(h_conv1, self.pool_size)\n\n        # Second convolutional layer -- maps 32 feature maps to 64.\n        with tf.name_scope('conv2'):\n            w_conv2 = weight_variable([self.conv_size, self.conv_size,\n                                       self.channel_1_num, self.channel_2_num])\n            b_conv2 = bias_variable([self.channel_2_num])\n            h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n\n        # Second pooling layer.\n        with tf.name_scope('pool2'):\n            h_pool2 = max_pool(h_conv2, self.pool_size)\n\n        # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n        # is down to 7x7x64 feature maps -- maps this to 1024 features.\n        last_dim = int(input_dim / (self.pool_size * self.pool_size))\n        with tf.name_scope('fc1'):\n            w_fc1 = weight_variable(\n                [last_dim * last_dim * self.channel_2_num, self.hidden_size])\n            b_fc1 = bias_variable([self.hidden_size])\n\n        h_pool2_flat = tf.reshape(\n            h_pool2, [-1, last_dim * last_dim * self.channel_2_num])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n\n        # Dropout - controls the complexity of the model, prevents co-adaptation of features.\n        with tf.name_scope('dropout'):\n            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n\n        # Map the 1024 features to 10 classes, one for each digit\n        with tf.name_scope('fc2'):\n            w_fc2 = weight_variable([self.hidden_size, self.y_dim])\n            b_fc2 = bias_variable([self.y_dim])\n            y_conv = tf.matmul(h_fc1_drop, w_fc2) + b_fc2\n\n        with tf.name_scope('loss'):\n            cross_entropy = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=y_conv))\n        with tf.name_scope('adam_optimizer'):\n            self.train_step = tf.train.AdamOptimizer(\n                self.learning_rate).minimize(cross_entropy)\n\n        with tf.name_scope('accuracy'):\n            correct_prediction = tf.equal(\n                tf.argmax(y_conv, 1), tf.argmax(self.labels, 1))\n            self.accuracy = tf.reduce_mean(\n                tf.cast(correct_prediction, tf.float32))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_experiment_time(port):\n    '''get the startTime and endTime of an experiment'''\n    response = rest_get(experiment_url(port), REST_TIME_OUT)\n    if response and check_response(response):\n        content = convert_time_stamp_to_date(json.loads(response.text))\n        return content.get('startTime'), content.get('endTime')\n    return None, None", "response": "get the startTime and endTime of an experiment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_experiment_status(port):\n    '''get the status of an experiment'''\n    result, response = check_rest_server_quick(port)\n    if result:\n        return json.loads(response.text).get('status')\n    return None", "response": "get the status of an experiment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_experiment():\n    '''Update the experiment status in config file'''\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        return None\n    for key in experiment_dict.keys():\n        if isinstance(experiment_dict[key], dict):\n            if experiment_dict[key].get('status') != 'STOPPED':\n                nni_config = Config(experiment_dict[key]['fileName'])\n                rest_pid = nni_config.get_config('restServerPid')\n                if not detect_process(rest_pid):\n                    experiment_config.update_experiment(key, 'status', 'STOPPED')\n                    continue\n                rest_port = nni_config.get_config('restServerPort')\n                startTime, endTime = get_experiment_time(rest_port)\n                if startTime:\n                    experiment_config.update_experiment(key, 'startTime', startTime)\n                if endTime:\n                    experiment_config.update_experiment(key, 'endTime', endTime)\n                status = get_experiment_status(rest_port)\n                if status:\n                    experiment_config.update_experiment(key, 'status', status)", "response": "Update the status of all experiments in config file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the id is valid", "response": "def check_experiment_id(args):\n    '''check if the id is valid\n    '''\n    update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print_normal('There is no experiment running...')\n        return None\n    if not args.id:\n        running_experiment_list = []\n        for key in experiment_dict.keys():\n            if isinstance(experiment_dict[key], dict):\n                if experiment_dict[key].get('status') != 'STOPPED':\n                    running_experiment_list.append(key)\n            elif isinstance(experiment_dict[key], list):\n                # if the config file is old version, remove the configuration from file\n                experiment_config.remove_experiment(key)\n        if len(running_experiment_list) > 1:\n            print_error('There are multiple experiments, please set the experiment id...')\n            experiment_information = \"\"\n            for key in running_experiment_list:\n                experiment_information += (EXPERIMENT_DETAIL_FORMAT % (key, experiment_dict[key]['status'], \\\n                experiment_dict[key]['port'], experiment_dict[key].get('platform'), experiment_dict[key]['startTime'], experiment_dict[key]['endTime']))\n            print(EXPERIMENT_INFORMATION_FORMAT % experiment_information)\n            exit(1)\n        elif not running_experiment_list:\n            print_error('There is no experiment running!')\n            return None\n        else:\n            return running_experiment_list[0]\n    if experiment_dict.get(args.id):\n        return args.id\n    else:\n        print_error('Id not correct!')\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the arguments for nnictl stop", "response": "def parse_ids(args):\n    '''Parse the arguments for nnictl stop\n    1.If there is an id specified, return the corresponding id\n    2.If there is no id specified, and there is an experiment running, return the id, or return Error\n    3.If the id matches an experiment, nnictl will return the id.\n    4.If the id ends with *, nnictl will match all ids matchs the regular\n    5.If the id does not exist but match the prefix of an experiment id, nnictl will return the matched id\n    6.If the id does not exist but match multiple prefix of the experiment ids, nnictl will give id information\n    '''\n    update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print_normal('Experiment is not running...')\n        return None\n    result_list = []\n    running_experiment_list = []\n    for key in experiment_dict.keys():\n        if isinstance(experiment_dict[key], dict):\n            if experiment_dict[key].get('status') != 'STOPPED':\n                running_experiment_list.append(key)\n        elif isinstance(experiment_dict[key], list):\n            # if the config file is old version, remove the configuration from file\n            experiment_config.remove_experiment(key)\n    if not args.id:\n        if len(running_experiment_list) > 1:\n            print_error('There are multiple experiments, please set the experiment id...')\n            experiment_information = \"\"\n            for key in running_experiment_list:\n                experiment_information += (EXPERIMENT_DETAIL_FORMAT % (key, experiment_dict[key]['status'], \\\n                experiment_dict[key]['port'], experiment_dict[key].get('platform'), experiment_dict[key]['startTime'], experiment_dict[key]['endTime']))\n            print(EXPERIMENT_INFORMATION_FORMAT % experiment_information)\n            exit(1)\n        else:\n            result_list = running_experiment_list\n    elif args.id == 'all':\n        result_list = running_experiment_list\n    elif args.id.endswith('*'):\n        for id in running_experiment_list:\n            if id.startswith(args.id[:-1]):\n                result_list.append(id)\n    elif args.id in running_experiment_list:\n        result_list.append(args.id)\n    else:\n        for id in running_experiment_list:\n            if id.startswith(args.id):\n                result_list.append(id)\n        if len(result_list) > 1:\n            print_error(args.id + ' is ambiguous, please choose ' + ' '.join(result_list) )\n            return None\n    if not result_list and args.id:\n        print_error('There are no experiments matched, please set correct experiment id...')\n    elif not result_list:\n        print_error('There is no experiment running...')\n    return result_list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_config_filename(args):\n    '''get the file name of config file'''\n    experiment_id = check_experiment_id(args)\n    if experiment_id is None:\n        print_error('Please set the experiment id!')\n        exit(1)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    return experiment_dict[experiment_id]['fileName']", "response": "get the file name of config file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert time stamp to date time format", "response": "def convert_time_stamp_to_date(content):\n    '''Convert time stamp to date time format'''\n    start_time_stamp = content.get('startTime')\n    end_time_stamp = content.get('endTime')\n    if start_time_stamp:\n        start_time = datetime.datetime.utcfromtimestamp(start_time_stamp // 1000).strftime(\"%Y/%m/%d %H:%M:%S\")\n        content['startTime'] = str(start_time)\n    if end_time_stamp:\n        end_time = datetime.datetime.utcfromtimestamp(end_time_stamp // 1000).strftime(\"%Y/%m/%d %H:%M:%S\")\n        content['endTime'] = str(end_time)\n    return content"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if restful server is running", "response": "def check_rest(args):\n    '''check if restful server is running'''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    running, _ = check_rest_server_quick(rest_port)\n    if not running:\n        print_normal('Restful server is running...')\n    else:\n        print_normal('Restful server is not running...')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstop the experiment which is running", "response": "def stop_experiment(args):\n    '''Stop the experiment which is running'''\n    experiment_id_list = parse_ids(args)\n    if experiment_id_list:\n        experiment_config = Experiments()\n        experiment_dict = experiment_config.get_all_experiments()\n        for experiment_id in experiment_id_list:\n            print_normal('Stoping experiment %s' % experiment_id)\n            nni_config = Config(experiment_dict[experiment_id]['fileName'])\n            rest_port = nni_config.get_config('restServerPort')\n            rest_pid = nni_config.get_config('restServerPid')\n            if rest_pid:\n                kill_command(rest_pid)\n                tensorboard_pid_list = nni_config.get_config('tensorboardPidList')\n                if tensorboard_pid_list:\n                    for tensorboard_pid in tensorboard_pid_list:\n                        try:\n                            kill_command(tensorboard_pid)\n                        except Exception as exception:\n                            print_error(exception)\n                    nni_config.set_config('tensorboardPidList', [])\n            print_normal('Stop experiment success!')\n            experiment_config.update_experiment(experiment_id, 'status', 'STOPPED')\n            time_now = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))\n            experiment_config.update_experiment(experiment_id, 'endTime', str(time_now))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef experiment_status(args):\n    '''Show the status of experiment'''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    result, response = check_rest_server_quick(rest_port)\n    if not result:\n        print_normal('Restful server is not running...')\n    else:\n        print(json.dumps(json.loads(response.text), indent=4, sort_keys=True, separators=(',', ':')))", "response": "Show the status of experiment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef log_internal(args, filetype):\n    '''internal function to call get_log_content'''\n    file_name = get_config_filename(args)\n    if filetype == 'stdout':\n        file_full_path = os.path.join(NNICTL_HOME_DIR, file_name, 'stdout')\n    else:\n        file_full_path = os.path.join(NNICTL_HOME_DIR, file_name, 'stderr')\n    print(check_output_command(file_full_path, head=args.head, tail=args.tail))", "response": "internal function to call get_log_content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting trial log path", "response": "def log_trial(args):\n    ''''get trial log path'''\n    trial_id_path_dict = {}\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    rest_pid = nni_config.get_config('restServerPid')\n    if not detect_process(rest_pid):\n        print_error('Experiment is not running...')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(trial_jobs_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            content = json.loads(response.text)\n            for trial in content:\n                trial_id_path_dict[trial['id']] = trial['logPath']\n    else:\n        print_error('Restful server is not running...')\n        exit(1)\n    if args.id:\n        if args.trial_id:\n            if trial_id_path_dict.get(args.trial_id):\n                print_normal('id:' + args.trial_id + ' path:' + trial_id_path_dict[args.trial_id])\n            else:\n                print_error('trial id is not valid!')\n                exit(1)\n        else:\n            print_error('please specific the trial id!')\n            exit(1)\n    else:\n        for key in trial_id_path_dict:\n            print('id:' + key + ' path:' + trial_id_path_dict[key])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef webui_url(args):\n    '''show the url of web ui'''\n    nni_config = Config(get_config_filename(args))\n    print_normal('{0} {1}'.format('Web UI url:', ' '.join(nni_config.get_config('webuiUrl'))))", "response": "show the url of web ui"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the information of all experiments", "response": "def experiment_list(args):\n    '''get the information of all experiments'''\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print('There is no experiment running...')\n        exit(1)\n    update_experiment()\n    experiment_id_list = []\n    if args.all and args.all == 'all':\n        for key in experiment_dict.keys():\n            experiment_id_list.append(key)\n    else:\n        for key in experiment_dict.keys():\n            if experiment_dict[key]['status'] != 'STOPPED':\n                experiment_id_list.append(key)\n        if not experiment_id_list:\n            print_warning('There is no experiment running...\\nYou can use \\'nnictl experiment list all\\' to list all stopped experiments!')\n    experiment_information = \"\"\n    for key in experiment_id_list:\n        \n        experiment_information += (EXPERIMENT_DETAIL_FORMAT % (key, experiment_dict[key]['status'], experiment_dict[key]['port'],\\\n        experiment_dict[key].get('platform'), experiment_dict[key]['startTime'], experiment_dict[key]['endTime']))\n    print(EXPERIMENT_INFORMATION_FORMAT % experiment_information)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the interval of two times", "response": "def get_time_interval(time1, time2):\n    '''get the interval of two times'''\n    try:\n        #convert time to timestamp\n        time1 = time.mktime(time.strptime(time1, '%Y/%m/%d %H:%M:%S'))\n        time2 = time.mktime(time.strptime(time2, '%Y/%m/%d %H:%M:%S'))\n        seconds = (datetime.datetime.fromtimestamp(time2) - datetime.datetime.fromtimestamp(time1)).seconds\n        #convert seconds to day:hour:minute:second\n        days = seconds / 86400\n        seconds %= 86400\n        hours = seconds / 3600\n        seconds %= 3600\n        minutes = seconds / 60\n        seconds %= 60\n        return '%dd %dh %dm %ds' % (days, hours, minutes, seconds)\n    except:\n        return 'N/A'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_experiment_info():\n    '''show experiment information in monitor'''\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print('There is no experiment running...')\n        exit(1)\n    update_experiment()\n    experiment_id_list = []\n    for key in experiment_dict.keys():\n        if experiment_dict[key]['status'] != 'STOPPED':\n            experiment_id_list.append(key)\n    if not experiment_id_list:\n        print_warning('There is no experiment running...')\n        return\n    for key in experiment_id_list:\n        print(EXPERIMENT_MONITOR_INFO % (key, experiment_dict[key]['status'], experiment_dict[key]['port'], \\\n             experiment_dict[key].get('platform'), experiment_dict[key]['startTime'], get_time_interval(experiment_dict[key]['startTime'], experiment_dict[key]['endTime'])))\n        print(TRIAL_MONITOR_HEAD)\n        running, response = check_rest_server_quick(experiment_dict[key]['port'])\n        if running:\n            response = rest_get(trial_jobs_url(experiment_dict[key]['port']), REST_TIME_OUT)\n            if response and check_response(response):\n                content = json.loads(response.text)\n                for index, value in enumerate(content):\n                    content[index] = convert_time_stamp_to_date(value)\n                    print(TRIAL_MONITOR_CONTENT % (content[index].get('id'), content[index].get('startTime'), content[index].get('endTime'), content[index].get('status')))\n        print(TRIAL_MONITOR_TAIL)", "response": "show experiment information in monitor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_trial_data(content):\n    trial_records = []\n    for trial_data in content:\n        for phase_i in range(len(trial_data['hyperParameters'])):\n            hparam = json.loads(trial_data['hyperParameters'][phase_i])['parameters']\n            hparam['id'] = trial_data['id']\n            if 'finalMetricData' in trial_data.keys() and phase_i < len(trial_data['finalMetricData']):\n                reward = json.loads(trial_data['finalMetricData'][phase_i]['data'])\n                if isinstance(reward, (float, int)):\n                    dict_tmp = {**hparam, **{'reward': reward}}\n                elif isinstance(reward, dict):\n                    dict_tmp = {**hparam, **reward}\n                else:\n                    raise ValueError(\"Invalid finalMetricsData format: {}/{}\".format(type(reward), reward))\n            else:\n                dict_tmp = hparam\n            trial_records.append(dict_tmp)\n    return trial_records", "response": "parse trial data into list of dicts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexporting experiment metadata to csv", "response": "def export_trials_data(args):\n    \"\"\"export experiment metadata to csv\n    \"\"\"\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    rest_pid = nni_config.get_config('restServerPid')\n    if not detect_process(rest_pid):\n        print_error('Experiment is not running...')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(trial_jobs_url(rest_port), 20)\n        if response is not None and check_response(response):\n            content = json.loads(response.text)\n            # dframe = pd.DataFrame.from_records([parse_trial_data(t_data) for t_data in content])\n            # dframe.to_csv(args.csv_path, sep='\\t')\n            records = parse_trial_data(content)\n            if args.type == 'json':\n                json_records = []\n                for trial in records:\n                    value = trial.pop('reward', None)\n                    trial_id =  trial.pop('id', None)\n                    json_records.append({'parameter': trial, 'value': value, 'id': trial_id})\n            with open(args.path, 'w') as file:\n                if args.type == 'csv':\n                    writer = csv.DictWriter(file, set.union(*[set(r.keys()) for r in records]))\n                    writer.writeheader()\n                    writer.writerows(records)\n                else:\n                    json.dump(json_records, file)\n        else:\n            print_error('Export failed...')\n    else:\n        print_error('Restful server is not Running')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy_remote_directory_to_local(sftp, remote_path, local_path):\n    '''copy remote directory to local machine'''\n    try:\n        os.makedirs(local_path, exist_ok=True)\n        files = sftp.listdir(remote_path)\n        for file in files:\n            remote_full_path = os.path.join(remote_path, file)\n            local_full_path = os.path.join(local_path, file)\n            try:\n                if sftp.listdir(remote_full_path):\n                    copy_remote_directory_to_local(sftp, remote_full_path, local_full_path)\n            except:\n                sftp.get(remote_full_path, local_full_path)\n    except Exception:\n        pass", "response": "copy remote directory to local machine"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges search space from json format to hyperopt format", "response": "def json2space(x, oldy=None, name=NodeType.Root.value):\n    \"\"\"Change search space from json format to hyperopt format\n    \"\"\"\n    y = list()\n    if isinstance(x, dict):\n        if NodeType.Type.value in x.keys():\n            _type = x[NodeType.Type.value]\n            name = name + '-' + _type\n            if _type == 'choice':\n                if oldy != None:\n                    _index = oldy[NodeType.Index.value]\n                    y += json2space(x[NodeType.Value.value][_index],\n                                    oldy[NodeType.Value.value], name=name+'[%d]' % _index)\n                else:\n                    y += json2space(x[NodeType.Value.value], None, name=name)\n            y.append(name)\n        else:\n            for key in x.keys():\n                y += json2space(x[key], (oldy[key] if oldy !=\n                                         None else None), name+\"[%s]\" % str(key))\n    elif isinstance(x, list):\n        for i, x_i in enumerate(x):\n            y += json2space(x_i, (oldy[i] if oldy !=\n                                  None else None), name+\"[%d]\" % i)\n    else:\n        pass\n    return y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _split_index(params):\n    result = {}\n    for key in params:\n        if isinstance(params[key], dict):\n            value = params[key]['_value']\n        else:\n            value = params[key]\n        result[key] = value\n    return result", "response": "Delete index information from params into a dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mutation(self, config=None, info=None, save_dir=None):\n        self.result = None\n        self.config = config\n        self.restore_dir = self.save_dir\n        self.save_dir = save_dir\n        self.info = info", "response": "This method is used to set the result of the current session to None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_search_space(self, search_space):\n        self.searchspace_json = search_space\n        self.space = json2space(self.searchspace_json)\n\n        self.random_state = np.random.RandomState()\n        self.population = []\n        is_rand = dict()\n        for item in self.space:\n            is_rand[item] = True\n        for _ in range(self.population_size):\n            config = json2paramater(\n                self.searchspace_json, is_rand, self.random_state)\n            self.population.append(Individual(config=config))", "response": "Update the search space."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a dict of trial - hyper - parameters as a serializable object.", "response": "def generate_parameters(self, parameter_id):\n        \"\"\"Returns a dict of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n    \n        Returns\n        -------\n        config : dict\n        \"\"\"\n        if not self.population:\n            raise RuntimeError('The population is empty')\n        pos = -1\n        for i in range(len(self.population)):\n            if self.population[i].result is None:\n                pos = i\n                break\n        if pos != -1:\n            indiv = copy.deepcopy(self.population[pos])\n            self.population.pop(pos)\n            total_config = indiv.config\n        else:\n            random.shuffle(self.population)\n            if self.population[0].result < self.population[1].result:\n                self.population[0] = self.population[1]\n\n            # mutation\n            space = json2space(self.searchspace_json,\n                               self.population[0].config)\n            is_rand = dict()\n            mutation_pos = space[random.randint(0, len(space)-1)]\n            for i in range(len(self.space)):\n                is_rand[self.space[i]] = (self.space[i] == mutation_pos)\n            config = json2paramater(\n                self.searchspace_json, is_rand, self.random_state, self.population[0].config)\n            self.population.pop(1)\n            # remove \"_index\" from config and save params-id\n\n            total_config = config\n        self.total_data[parameter_id] = total_config\n        config = _split_index(total_config)\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef receive_trial_result(self, parameter_id, parameters, value):\n        '''Record the result from a trial\n\n        Parameters\n        ----------\n        parameters: dict\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n            value is final metrics of the trial.\n        '''\n        reward = extract_scalar_reward(value)\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        # restore the paramsters contains \"_index\"\n        params = self.total_data[parameter_id]\n\n        if self.optimize_mode == OptimizeMode.Minimize:\n            reward = -reward\n\n        indiv = Individual(config=params, result=reward)\n        self.population.append(indiv)", "response": "Record the result from a trial."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_json_content(file_path):\n    try:\n        with open(file_path, 'r') as file:\n            return json.load(file)\n    except TypeError as err:\n        print('Error: ', err)\n        return None", "response": "Load the json file content\n    \n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_pcs(nni_search_space_content):\n    categorical_dict = {}\n    search_space = nni_search_space_content\n    with open('param_config_space.pcs', 'w') as pcs_fd:\n        if isinstance(search_space, dict):\n            for key in search_space.keys():\n                if isinstance(search_space[key], dict):\n                    try:\n                        if search_space[key]['_type'] == 'choice':\n                            choice_len = len(search_space[key]['_value'])\n                            pcs_fd.write('%s categorical {%s} [%s]\\n' % (\n                                key, \n                                json.dumps(list(range(choice_len)))[1:-1], \n                                json.dumps(0)))\n                            if key in categorical_dict:\n                                raise RuntimeError('%s has already existed, please make sure search space has no duplicate key.' % key)\n                            categorical_dict[key] = search_space[key]['_value']\n                        elif search_space[key]['_type'] == 'randint':\n                            # TODO: support lower bound in randint\n                            pcs_fd.write('%s integer [0, %d] [%d]\\n' % (\n                                key, \n                                search_space[key]['_value'][0], \n                                search_space[key]['_value'][0]))\n                        elif search_space[key]['_type'] == 'uniform':\n                            pcs_fd.write('%s real %s [%s]\\n' % (\n                                key, \n                                json.dumps(search_space[key]['_value']),\n                                json.dumps(search_space[key]['_value'][0])))\n                        elif search_space[key]['_type'] == 'loguniform':\n                            # use np.round here to ensure that the rounded defaut value is in the range, which will be rounded in configure_space package\n                            search_space[key]['_value'] = list(np.round(np.log(search_space[key]['_value']), 10))\n                            pcs_fd.write('%s real %s [%s]\\n' % (\n                                key, \n                                json.dumps(search_space[key]['_value']),\n                                json.dumps(search_space[key]['_value'][0])))\n                        elif search_space[key]['_type'] == 'quniform' \\\n                            and search_space[key]['_value'][2] == 1:\n                            pcs_fd.write('%s integer [%d, %d] [%d]\\n' % (\n                                key,\n                                search_space[key]['_value'][0],\n                                search_space[key]['_value'][1],\n                                search_space[key]['_value'][0]))\n                        else:\n                            raise RuntimeError('unsupported _type %s' % search_space[key]['_type'])\n                    except:\n                        raise RuntimeError('_type or _value error.')\n        else:\n            raise RuntimeError('incorrect search space.')\n        return categorical_dict\n    return None", "response": "Generate the Parameter Configuration Space for the given search space content."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_scenario(ss_content):\n    with open('scenario.txt', 'w') as sce_fd:\n        sce_fd.write('deterministic = 0\\n')\n        #sce_fd.write('output_dir = \\n')\n        sce_fd.write('paramfile = param_config_space.pcs\\n')\n        sce_fd.write('run_obj = quality\\n')\n\n    return generate_pcs(ss_content)", "response": "Generate the scenario file for the given SMAC and target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_data(train_path='./data/regression.train', test_path='./data/regression.test'):\n    '''\n    Load or create dataset\n    '''\n    print('Load data...')\n    df_train = pd.read_csv(train_path, header=None, sep='\\t')\n    df_test = pd.read_csv(test_path, header=None, sep='\\t')\n    num = len(df_train)\n    split_num = int(0.9 * num)\n\n    y_train = df_train[0].values\n    y_test = df_test[0].values\n    y_eval = y_train[split_num:]\n    y_train = y_train[:split_num]\n\n    X_train = df_train.drop(0, axis=1).values\n    X_test = df_test.drop(0, axis=1).values\n    X_eval = X_train[split_num:, :]\n    X_train = X_train[:split_num, :]\n\n    # create dataset for lightgbm\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_eval, y_eval, reference=lgb_train)\n\n    return lgb_train, lgb_eval, X_test, y_test", "response": "Load or create dataset\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef layer_distance(a, b):\n    # pylint: disable=unidiomatic-typecheck\n    if type(a) != type(b):\n        return 1.0\n    if is_layer(a, \"Conv\"):\n        att_diff = [\n            (a.filters, b.filters),\n            (a.kernel_size, b.kernel_size),\n            (a.stride, b.stride),\n        ]\n        return attribute_difference(att_diff)\n    if is_layer(a, \"Pooling\"):\n        att_diff = [\n            (a.padding, b.padding),\n            (a.kernel_size, b.kernel_size),\n            (a.stride, b.stride),\n        ]\n        return attribute_difference(att_diff)\n    return 0.0", "response": "The distance between two layers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef edit_distance(x, y):\n\n    ret = layers_distance(x.layers, y.layers)\n    ret += Constant.KERNEL_LAMBDA * skip_connections_distance(\n        x.skip_connections, y.skip_connections\n    )\n    return ret", "response": "The edit - distance between two neural networks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the edit distance matrix.", "response": "def edit_distance_matrix(train_x, train_y=None):\n    \"\"\"Calculate the edit distance.\n    Args:\n        train_x: A list of neural architectures.\n        train_y: A list of neural architectures.\n    Returns:\n        An edit-distance matrix.\n    \"\"\"\n    if train_y is None:\n        ret = np.zeros((train_x.shape[0], train_x.shape[0]))\n        for x_index, x in enumerate(train_x):\n            for y_index, y in enumerate(train_x):\n                if x_index == y_index:\n                    ret[x_index][y_index] = 0\n                elif x_index < y_index:\n                    ret[x_index][y_index] = edit_distance(x, y)\n                else:\n                    ret[x_index][y_index] = ret[y_index][x_index]\n        return ret\n    ret = np.zeros((train_x.shape[0], train_y.shape[0]))\n    for x_index, x in enumerate(train_x):\n        for y_index, y in enumerate(train_y):\n            ret[x_index][y_index] = edit_distance(x, y)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef vector_distance(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    return np.linalg.norm(a - b)", "response": "The Euclidean distance between two vectors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bourgain_embedding_matrix(distance_matrix):\n    distance_matrix = np.array(distance_matrix)\n    n = len(distance_matrix)\n    if n == 1:\n        return distance_matrix\n    np.random.seed(123)\n    distort_elements = []\n    r = range(n)\n    k = int(math.ceil(math.log(n) / math.log(2) - 1))\n    t = int(math.ceil(math.log(n)))\n    counter = 0\n    for i in range(0, k + 1):\n        for t in range(t):\n            s = np.random.choice(r, 2 ** i)\n            for j in r:\n                d = min([distance_matrix[j][s] for s in s])\n                counter += len(s)\n                if i == 0 and t == 0:\n                    distort_elements.append([d])\n                else:\n                    distort_elements[j].append(d)\n    return rbf_kernel(distort_elements, distort_elements)", "response": "Use Bourgain algorithm to embed the neural architectures based on their edit - distance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef contain(descriptors, target_descriptor):\n    for descriptor in descriptors:\n        if edit_distance(descriptor, target_descriptor) < 1e-5:\n            return True\n    return False", "response": "Check if the target descriptor is in the descriptors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfitting the regressor with more data.", "response": "def fit(self, train_x, train_y):\n        \"\"\" Fit the regressor with more data.\n        Args:\n            train_x: A list of NetworkDescriptor.\n            train_y: A list of metric values.\n        \"\"\"\n        if self.first_fitted:\n            self.incremental_fit(train_x, train_y)\n        else:\n            self.first_fit(train_x, train_y)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfit the regressor for the first time.", "response": "def first_fit(self, train_x, train_y):\n        \"\"\" Fit the regressor for the first time. \"\"\"\n        train_x, train_y = np.array(train_x), np.array(train_y)\n\n        self._x = np.copy(train_x)\n        self._y = np.copy(train_y)\n\n        self._distance_matrix = edit_distance_matrix(self._x)\n        k_matrix = bourgain_embedding_matrix(self._distance_matrix)\n        k_matrix[np.diag_indices_from(k_matrix)] += self.alpha\n\n        self._l_matrix = cholesky(k_matrix, lower=True)  # Line 2\n\n        self._alpha_vector = cho_solve((self._l_matrix, True), self._y)  # Line 3\n\n        self._first_fitted = True\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef predict(self, train_x):\n        k_trans = np.exp(-np.power(edit_distance_matrix(train_x, self._x), 2))\n        y_mean = k_trans.dot(self._alpha_vector)  # Line 4 (y_mean = f_star)\n\n        # compute inverse K_inv of K based on its Cholesky\n        # decomposition L and its inverse L_inv\n        l_inv = solve_triangular(self._l_matrix.T, np.eye(self._l_matrix.shape[0]))\n        k_inv = l_inv.dot(l_inv.T)\n        # Compute variance of predictive distribution\n        y_var = np.ones(len(train_x), dtype=np.float)\n        y_var -= np.einsum(\"ij,ij->i\", np.dot(k_trans, k_inv), k_trans)\n\n        # Check if any of the variances is negative because of\n        # numerical issues. If yes: set the variance to 0.\n        y_var_negative = y_var < 0\n        if np.any(y_var_negative):\n            y_var[y_var_negative] = 0.0\n        return y_mean, np.sqrt(y_var)", "response": "Predict the result of the predictive distribution of the current network."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a new neural network with weights.", "response": "def generate(self, descriptors):\n        \"\"\"Generate new architecture.\n        Args:\n            descriptors: All the searched neural architectures.\n        Returns:\n            graph: An instance of Graph. A morphed neural network with weights.\n            father_id: The father node ID in the search tree.\n        \"\"\"\n        model_ids = self.search_tree.adj_list.keys()\n\n        target_graph = None\n        father_id = None\n        descriptors = deepcopy(descriptors)\n        elem_class = Elem\n        if self.optimizemode is OptimizeMode.Maximize:\n            elem_class = ReverseElem\n\n        # Initialize the priority queue.\n        pq = PriorityQueue()\n        temp_list = []\n        for model_id in model_ids:\n            metric_value = self.searcher.get_metric_value_by_id(model_id)\n            temp_list.append((metric_value, model_id))\n        temp_list = sorted(temp_list)\n        for metric_value, model_id in temp_list:\n            graph = self.searcher.load_model_by_id(model_id)\n            graph.clear_operation_history()\n            graph.clear_weights()\n            pq.put(elem_class(metric_value, model_id, graph))\n\n        t = 1.0\n        t_min = self.t_min\n        alpha = 0.9\n        opt_acq = self._get_init_opt_acq_value()\n        while not pq.empty() and t > t_min:\n            elem = pq.get()\n            if self.optimizemode is OptimizeMode.Maximize:\n                temp_exp = min((elem.metric_value - opt_acq) / t, 1.0)\n            else:\n                temp_exp = min((opt_acq - elem.metric_value) / t, 1.0)\n            ap = math.exp(temp_exp)\n            if ap >= random.uniform(0, 1):\n                for temp_graph in transform(elem.graph):\n                    if contain(descriptors, temp_graph.extract_descriptor()):\n                        continue\n\n                    temp_acq_value = self.acq(temp_graph)\n                    pq.put(elem_class(temp_acq_value, elem.father_id, temp_graph))\n                    descriptors.append(temp_graph.extract_descriptor())\n                    if self._accept_new_acq_value(opt_acq, temp_acq_value):\n                        opt_acq = temp_acq_value\n                        father_id = elem.father_id\n                        target_graph = deepcopy(temp_graph)\n            t *= alpha\n\n        # Did not found a not duplicated architecture\n        if father_id is None:\n            return None, None\n        nm_graph = self.searcher.load_model_by_id(father_id)\n        for args in target_graph.operation_history:\n            getattr(nm_graph, args[0])(*list(args[1:]))\n        return nm_graph, father_id"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef acq(self, graph):\n        ''' estimate the value of generated graph\n        '''\n        mean, std = self.gpr.predict(np.array([graph.extract_descriptor()]))\n        if self.optimizemode is OptimizeMode.Maximize:\n            return mean + self.beta * std\n        return mean - self.beta * std", "response": "estimate the value of generated graph\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd child to search tree itself.", "response": "def add_child(self, u, v):\n        ''' add child to search tree itself.\n        Arguments:\n            u {int} -- father id\n            v {int} --  child id\n        '''\n\n        if u == -1:\n            self.root = v\n            self.adj_list[v] = []\n            return\n        if v not in self.adj_list[u]:\n            self.adj_list[u].append(v)\n        if v not in self.adj_list:\n            self.adj_list[v] = []"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntrains a network from a specific graph.", "response": "def train_with_graph(p_graph, qp_pairs, dev_qp_pairs):\n    '''\n    Train a network from a specific graph.\n    '''\n    global sess\n    with tf.Graph().as_default():\n        train_model = GAG(cfg, embed, p_graph)\n        train_model.build_net(is_training=True)\n        tf.get_variable_scope().reuse_variables()\n        dev_model = GAG(cfg, embed, p_graph)\n        dev_model.build_net(is_training=False)\n        with tf.Session() as sess:\n            if restore_path is not None:\n                restore_mapping = dict(zip(restore_shared, restore_shared))\n                logger.debug('init shared variables from {}, restore_scopes: {}'.format(restore_path, restore_shared))\n                init_from_checkpoint(restore_path, restore_mapping)\n            logger.debug('init variables')\n            logger.debug(sess.run(tf.report_uninitialized_variables()))\n            init = tf.global_variables_initializer()\n            sess.run(init)\n            # writer = tf.summary.FileWriter('%s/graph/'%execution_path, sess.graph)\n            logger.debug('assign to graph')\n\n            saver = tf.train.Saver()\n            train_loss = None\n            bestacc = 0\n            patience = 5\n            patience_increase = 2\n            improvement_threshold = 0.995\n\n            for epoch in range(max_epoch):\n                logger.debug('begin to train')\n                train_batches = data.get_batches(qp_pairs, cfg.batch_size)\n                train_loss = run_epoch(train_batches, train_model, True)\n                logger.debug('epoch ' + str(epoch) +\n                             ' loss: ' + str(train_loss))\n                dev_batches = list(data.get_batches(\n                    dev_qp_pairs, cfg.batch_size))\n                _, position1, position2, ids, contexts = run_epoch(\n                    dev_batches, dev_model, False)\n\n                answers = generate_predict_json(\n                    position1, position2, ids, contexts)\n                if save_path is not None:\n                    logger.info('save prediction file to {}'.format(save_path))\n                    with open(os.path.join(save_path, 'epoch%d.prediction' % epoch), 'w') as file:\n                        json.dump(answers, file)\n                else:\n                    answers = json.dumps(answers)\n                    answers = json.loads(answers)\n                iter = epoch + 1\n\n                acc = evaluate.evaluate_with_predictions(\n                    args.dev_file, answers)\n\n                logger.debug('Send intermediate acc: %s', str(acc))\n                nni.report_intermediate_result(acc)\n\n                logger.debug('Send intermediate result done.')\n\n                if acc > bestacc:\n                    if acc * improvement_threshold > bestacc:\n                        patience = max(patience, iter * patience_increase)\n                    bestacc = acc\n\n                    if save_path is not None:\n                        logger.info('save model & prediction to {}'.format(save_path))\n                        saver.save(sess, os.path.join(save_path, 'epoch%d.model' % epoch))\n                        with open(os.path.join(save_path, 'epoch%d.score' % epoch), 'wb') as file:\n                            pickle.dump(\n                                (position1, position2, ids, contexts), file)\n                logger.debug('epoch %d acc %g bestacc %g' %\n                             (epoch, acc, bestacc))\n                if patience <= iter:\n                    break\n            logger.debug('save done.')\n    return train_loss, bestacc"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns multiple sets of trial ( hyper - ) parameters as iterable of serializable objects.", "response": "def generate_multiple_parameters(self, parameter_id_list):\n        \"\"\"Returns multiple sets of trial (hyper-)parameters, as iterable of serializable objects.\n        Call 'generate_parameters()' by 'count' times by default.\n        User code must override either this function or 'generate_parameters()'.\n        If there's no more trial, user should raise nni.NoMoreTrialError exception in generate_parameters().\n        If so, this function will only return sets of trial (hyper-)parameters that have already been collected.\n        parameter_id_list: list of int\n        \"\"\"\n        result = []\n        for parameter_id in parameter_id_list:\n            try:\n                _logger.debug(\"generating param for {}\".format(parameter_id))\n                res = self.generate_parameters(parameter_id)\n            except nni.NoMoreTrialError:\n                return result\n            result.append(res)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads graph from json format.", "response": "def graph_loads(graph_json):\n    '''\n    Load graph\n    '''\n    layers = []\n    for layer in graph_json['layers']:\n        layer_info = Layer(layer['graph_type'], layer['input'], layer['output'], layer['size'], layer['hash_id'])\n        layer_info.is_delete = layer['is_delete']\n        _logger.debug('append layer {}'.format(layer_info))\n        layers.append(layer_info)\n    graph = Graph(graph_json['max_layer_num'], graph_json['min_layer_num'], [], [], [])\n    graph.layers = layers\n    _logger.debug('graph {} loaded'.format(graph))\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_hash(self, layers: Iterable):\n        if self.graph_type == LayerType.input.value:\n            return\n        hasher = hashlib.md5()\n        hasher.update(LayerType(self.graph_type).name.encode('ascii'))\n        hasher.update(str(self.size).encode('ascii'))\n        for i in self.input:\n            if layers[i].hash_id is None:\n                raise ValueError('Hash id of layer {}: {} not generated!'.format(i, layers[i]))\n            hasher.update(layers[i].hash_id.encode('ascii'))\n        self.hash_id = hasher.hexdigest()", "response": "Calculates the hash_id of the layer and updates the hash_id of the input layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_hash(self):\n        _logger.debug('update hash')\n        layer_in_cnt = [len(layer.input) for layer in self.layers]\n        topo_queue = deque([i for i, layer in enumerate(self.layers) if not layer.is_delete and layer.graph_type == LayerType.input.value])\n        while topo_queue:\n            layer_i = topo_queue.pop()\n            self.layers[layer_i].update_hash(self.layers)\n            for layer_j in self.layers[layer_i].output:\n                layer_in_cnt[layer_j] -= 1\n                if layer_in_cnt[layer_j] == 0:\n                    topo_queue.appendleft(layer_j)", "response": "update hash id of each layer in topological order"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize root logger. This will redirect anything from logging.getLogger() as well as stdout to specified file. logger_file_path: path of logger file (path-like object).", "response": "def init_logger(logger_file_path, log_level_name='info'):\n    \"\"\"Initialize root logger.\n    This will redirect anything from logging.getLogger() as well as stdout to specified file.\n    logger_file_path: path of logger file (path-like object).\n    \"\"\"\n    log_level = log_level_map.get(log_level_name, logging.INFO)\n    logger_file = open(logger_file_path, 'w')\n    fmt = '[%(asctime)s] %(levelname)s (%(name)s/%(threadName)s) %(message)s'\n    logging.Formatter.converter = time.localtime\n    formatter = logging.Formatter(fmt, _time_format)\n    handler = logging.StreamHandler(logger_file)\n    handler.setFormatter(formatter)\n\n    root_logger = logging.getLogger()\n    root_logger.addHandler(handler)\n    root_logger.setLevel(log_level)\n\n    # these modules are too verbose\n    logging.getLogger('matplotlib').setLevel(log_level)\n\n    sys.stdout = _LoggerFileWrapper(logger_file)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a MNIST model.", "response": "def create_mnist_model(hyper_params, input_shape=(H, W, 1), num_classes=NUM_CLASSES):\n    '''\n    Create simple convolutional model\n    '''\n    layers = [\n        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(100, activation='relu'),\n        Dense(num_classes, activation='softmax')\n    ]\n\n    model = Sequential(layers)\n\n    if hyper_params['optimizer'] == 'Adam':\n        optimizer = keras.optimizers.Adam(lr=hyper_params['learning_rate'])\n    else:\n        optimizer = keras.optimizers.SGD(lr=hyper_params['learning_rate'], momentum=0.9)\n    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns on end of each epoch", "response": "def on_epoch_end(self, epoch, logs={}):\n        '''\n        Run on end of each epoch\n        '''\n        LOG.debug(logs)\n        nni.report_intermediate_result(logs[\"val_acc\"])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all of config values", "response": "def get_all_config(self):\n        '''get all of config values'''\n        return json.dumps(self.config, indent=4, sort_keys=True, separators=(',', ':'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_config(self, key, value):\n        '''set {key:value} paris to self.config'''\n        self.config = self.read_file()\n        self.config[key] = value\n        self.write_file()", "response": "set value paris to self. config"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_file(self):\n        '''save config to local file'''\n        if self.config:\n            try:\n                with open(self.config_file, 'w') as file:\n                    json.dump(self.config, file)\n            except IOError as error:\n                print('Error:', error)\n                return", "response": "save config to local file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an experiment to the experiment list", "response": "def add_experiment(self, id, port, time, file_name, platform):\n        '''set {key:value} paris to self.experiment'''\n        self.experiments[id] = {}\n        self.experiments[id]['port'] = port\n        self.experiments[id]['startTime'] = time\n        self.experiments[id]['endTime'] = 'N/A'\n        self.experiments[id]['status'] = 'INITIALIZED'\n        self.experiments[id]['fileName'] = file_name\n        self.experiments[id]['platform'] = platform\n        self.write_file()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_experiment(self, id, key, value):\n        '''Update experiment'''\n        if id not in self.experiments:\n            return False\n        self.experiments[id][key] = value\n        self.write_file()\n        return True", "response": "Update the value of an experiment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving an experiment by id", "response": "def remove_experiment(self, id):\n        '''remove an experiment by id'''\n        if id in self.experiments:\n            self.experiments.pop(id)\n        self.write_file()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving config to local file", "response": "def write_file(self):\n        '''save config to local file'''\n        try:\n            with open(self.experiment_file, 'w') as file:\n                json.dump(self.experiments, file)\n        except IOError as error:\n            print('Error:', error)\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_file(self):\n        '''load config from local file'''\n        if os.path.exists(self.experiment_file):\n            try:\n                with open(self.experiment_file, 'r') as file:\n                    return json.load(file)\n            except ValueError:\n                return {}\n        return {}", "response": "load config from local file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading data from file", "response": "def load_from_file(path, fmt=None, is_training=True):\n    '''\n    load data from file\n    '''\n    if fmt is None:\n        fmt = 'squad'\n    assert fmt in ['squad', 'csv'], 'input format must be squad or csv'\n    qp_pairs = []\n    if fmt == 'squad':\n        with open(path) as data_file:\n            data = json.load(data_file)['data']\n            for doc in data:\n                for paragraph in doc['paragraphs']:\n                    passage = paragraph['context']\n                    for qa_pair in paragraph['qas']:\n                        question = qa_pair['question']\n                        qa_id = qa_pair['id']\n                        if not is_training:\n                            qp_pairs.append(\n                                {'passage': passage, 'question': question, 'id': qa_id})\n                        else:\n                            for answer in qa_pair['answers']:\n                                answer_begin = int(answer['answer_start'])\n                                answer_end = answer_begin + len(answer['text'])\n                                qp_pairs.append({'passage': passage,\n                                                 'question': question,\n                                                 'id': qa_id,\n                                                 'answer_begin': answer_begin,\n                                                 'answer_end': answer_end})\n    else:\n        with open(path, newline='') as csvfile:\n            reader = csv.reader(csvfile, delimiter='\\t')\n            line_num = 0\n            for row in reader:\n                qp_pairs.append(\n                    {'passage': row[1], 'question': row[0], 'id': line_num})\n                line_num += 1\n    return qp_pairs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild the vocab from corpus.", "response": "def collect_vocab(qp_pairs):\n    '''\n    Build the vocab from corpus.\n    '''\n    vocab = set()\n    for qp_pair in qp_pairs:\n        for word in qp_pair['question_tokens']:\n            vocab.add(word['word'])\n        for word in qp_pair['passage_tokens']:\n            vocab.add(word['word'])\n    return vocab"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shuffle_step(entries, step):\n    '''\n    Shuffle the step\n    '''\n    answer = []\n    for i in range(0, len(entries), step):\n        sub = entries[i:i+step]\n        shuffle(sub)\n        answer += sub\n    return answer", "response": "Shuffle the step of the sequence"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_batches(qp_pairs, batch_size, need_sort=True):\n    '''\n    Get batches data and shuffle.\n    '''\n    if need_sort:\n        qp_pairs = sorted(qp_pairs, key=lambda qp: (\n            len(qp['passage_tokens']), qp['id']), reverse=True)\n    batches = [{'qp_pairs': qp_pairs[i:(i + batch_size)]}\n               for i in range(0, len(qp_pairs), batch_size)]\n    shuffle(batches)\n    return batches", "response": "Get batches data and shuffle."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a list of tokens and a character index return the index of the word that is in the list of tokens.", "response": "def get_word_index(tokens, char_index):\n    '''\n    Given word return word index.\n    '''\n    for (i, token) in enumerate(tokens):\n        if token['char_end'] == 0:\n            continue\n        if token['char_begin'] <= char_index and char_index <= token['char_end']:\n            return i\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets answer s index of begin and end.", "response": "def get_answer_begin_end(data):\n    '''\n    Get answer's index of begin and end.\n    '''\n    begin = []\n    end = []\n    for qa_pair in data:\n        tokens = qa_pair['passage_tokens']\n        char_begin = qa_pair['answer_begin']\n        char_end = qa_pair['answer_end']\n        word_begin = get_word_index(tokens, char_begin)\n        word_end = get_word_index(tokens, char_end)\n        begin.append(word_begin)\n        end.append(word_end)\n    return np.asarray(begin), np.asarray(end)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_buckets(min_length, max_length, bucket_count):\n    '''\n    Get bucket by length.\n    '''\n    if bucket_count <= 0:\n        return [max_length]\n    unit_length = int((max_length - min_length) // (bucket_count))\n    buckets = [min_length + unit_length *\n               (i + 1) for i in range(0, bucket_count)]\n    buckets[-1] = max_length\n    return buckets", "response": "Get a list of buckets by length."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tokenize(self, text):\n        '''\n        tokenize function in Tokenizer.\n        '''\n        start = -1\n        tokens = []\n        for i, character in enumerate(text):\n            if character == ' ' or character == '\\t':\n                if start >= 0:\n                    word = text[start:i]\n                    tokens.append({\n                        'word': word,\n                        'original_text': word,\n                        'char_begin': start,\n                        'char_end': i})\n                    start = -1\n            else:\n                if start < 0:\n                    start = i\n        if start >= 0:\n            tokens.append({\n                'word': text[start:len(text)],\n                'original_text': text[start:len(text)],\n                'char_begin': start,\n                'char_end': len(text)\n            })\n        return tokens", "response": "tokenize function in Tokenizer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating new id and event hook for new Individual", "response": "def generate_new_id(self):\n        \"\"\"\n        generate new id and event hook for new Individual\n        \"\"\"\n        self.events.append(Event())\n        indiv_id = self.indiv_counter\n        self.indiv_counter += 1\n        return indiv_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize populations for evolution tuner", "response": "def init_population(self, population_size, graph_max_layer, graph_min_layer):\n        \"\"\"\n        initialize populations for evolution tuner\n        \"\"\"\n        population = []\n        graph = Graph(max_layer_num=graph_max_layer, min_layer_num=graph_min_layer,\n                      inputs=[Layer(LayerType.input.value, output=[4, 5], size='x'), Layer(LayerType.input.value, output=[4, 5], size='y')],\n                      output=[Layer(LayerType.output.value, inputs=[4], size='x'), Layer(LayerType.output.value, inputs=[5], size='y')],\n                      hide=[Layer(LayerType.attention.value, inputs=[0, 1], output=[2]),\n                            Layer(LayerType.attention.value, inputs=[1, 0], output=[3])])\n        for _ in range(population_size):\n            graph_tmp = copy.deepcopy(graph)\n            graph_tmp.mutation()\n            population.append(Individual(indiv_id=self.generate_new_id(), graph_cfg=graph_tmp, result=None))\n        return population"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_parameters(self, parameter_id):\n        logger.debug('acquiring lock for param {}'.format(parameter_id))\n        self.thread_lock.acquire()\n        logger.debug('lock for current thread acquired')\n        if not self.population:\n            logger.debug(\"the len of poplution lower than zero.\")\n            raise Exception('The population is empty')\n        pos = -1\n        for i in range(len(self.population)):\n            if self.population[i].result is None:\n                pos = i\n                break\n        if pos != -1:\n            indiv = copy.deepcopy(self.population[pos])\n            self.population.pop(pos)\n            graph_param = json.loads(graph_dumps(indiv.config))\n        else:\n            random.shuffle(self.population)\n            if self.population[0].result < self.population[1].result:\n                self.population[0] = self.population[1]\n            indiv = copy.deepcopy(self.population[0])\n            self.population.pop(1)\n            indiv.mutation(indiv_id = self.generate_new_id())\n            graph_param = json.loads(graph_dumps(indiv.config))\n        param_json = {\n            'graph': graph_param,\n            'restore_dir': self.save_dir(indiv.parent_id),\n            'save_dir': self.save_dir(indiv.indiv_id),\n            'shared_id': list(indiv.shared_ids) if indiv.parent_id is not None else None,\n        }\n        logger.debug('generate_parameter return value is:')\n        logger.debug(param_json)\n        logger.debug('releasing lock')\n        self.thread_lock.release()\n        if indiv.parent_id is not None:\n            logger.debug(\"new trial {} pending on parent experiment {}\".format(indiv.indiv_id, indiv.parent_id))\n            self.events[indiv.parent_id].wait()\n        logger.debug(\"trial {} ready\".format(indiv.indiv_id))\n        return param_json", "response": "Generates a set of parameters for the current trial."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrecords an observation of the objective function parameter_id : int parameters : dict of parameters value: final metrics of the trial, including reward", "response": "def receive_trial_result(self, parameter_id, parameters, value):\n        '''\n        Record an observation of the objective function\n        parameter_id : int\n        parameters : dict of parameters\n        value: final metrics of the trial, including reward\n        '''\n        logger.debug('acquiring lock for param {}'.format(parameter_id))\n        self.thread_lock.acquire()\n        logger.debug('lock for current acquired')\n        reward = extract_scalar_reward(value)\n        if self.optimize_mode is OptimizeMode.Minimize:\n            reward = -reward\n\n        logger.debug('receive trial result is:\\n')\n        logger.debug(str(parameters))\n        logger.debug(str(reward))\n\n        indiv = Individual(indiv_id=int(os.path.split(parameters['save_dir'])[1]),\n                           graph_cfg=graph_loads(parameters['graph']), result=reward)\n        self.population.append(indiv)\n        logger.debug('releasing lock')\n        self.thread_lock.release()\n        self.events[indiv.indiv_id].set()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_data(self, trial_job_id, trial_history):\n        if trial_job_id not in self.running_history:\n            self.running_history[trial_job_id] = []\n        self.running_history[trial_job_id].extend(trial_history[len(self.running_history[trial_job_id]):])", "response": "update data with the current trial job id and the history matrix"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assess_trial(self, trial_job_id, trial_history):\n        curr_step = len(trial_history)\n        if curr_step < self.start_step:\n            return AssessResult.Good\n\n        try:\n            num_trial_history = [float(ele) for ele in trial_history]\n        except (TypeError, ValueError) as error:\n            logger.warning('incorrect data type or value:')\n            logger.exception(error)\n        except Exception as error:\n            logger.warning('unrecognized exception in medianstop_assessor:')\n            logger.excpetion(error)\n\n        self._update_data(trial_job_id, num_trial_history)\n        if self.high_better:\n            best_history = max(trial_history)\n        else:\n            best_history = min(trial_history)\n\n        avg_array = []\n        for id in self.completed_avg_history:\n            if len(self.completed_avg_history[id]) >= curr_step:\n                avg_array.append(self.completed_avg_history[id][curr_step - 1])\n        if len(avg_array) > 0:\n            avg_array.sort()\n            if self.high_better:\n                median = avg_array[(len(avg_array)-1) // 2]\n                return AssessResult.Bad if best_history < median else AssessResult.Good\n            else:\n                median = avg_array[len(avg_array) // 2]\n                return AssessResult.Bad if best_history > median else AssessResult.Good\n        else:\n            return AssessResult.Good", "response": "This function is used to assess a trial."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copyHdfsDirectoryToLocal(hdfsDirectory, localDirectory, hdfsClient):\n    '''Copy directory from HDFS to local'''\n    if not os.path.exists(localDirectory):\n        os.makedirs(localDirectory)\n    try:\n        listing = hdfsClient.list_status(hdfsDirectory)\n    except Exception as exception:\n        nni_log(LogType.Error, 'List hdfs directory {0} error: {1}'.format(hdfsDirectory, str(exception)))\n        raise exception\n\n    for f in listing:\n        if f.type == 'DIRECTORY':\n            subHdfsDirectory = posixpath.join(hdfsDirectory, f.pathSuffix)\n            subLocalDirectory = os.path.join(localDirectory, f.pathSuffix)\n            copyHdfsDirectoryToLocal(subHdfsDirectory, subLocalDirectory, hdfsClient)\n        elif f.type == 'FILE':\n            hdfsFilePath = posixpath.join(hdfsDirectory, f.pathSuffix)\n            localFilePath = os.path.join(localDirectory, f.pathSuffix)            \n            copyHdfsFileToLocal(hdfsFilePath, localFilePath, hdfsClient)\n        else: \n            raise AssertionError('unexpected type {}'.format(f.type))", "response": "Copy directory from HDFS to local"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy file from HDFS to local file", "response": "def copyHdfsFileToLocal(hdfsFilePath, localFilePath, hdfsClient, override=True):\n    '''Copy file from HDFS to local'''\n    if not hdfsClient.exists(hdfsFilePath):\n        raise Exception('HDFS file {} does not exist!'.format(hdfsFilePath))\n    try: \n        file_status = hdfsClient.get_file_status(hdfsFilePath)\n        if file_status.type != 'FILE':\n            raise Exception('HDFS file path {} is not a file'.format(hdfsFilePath))\n    except Exception as exception:\n        nni_log(LogType.Error, 'Get hdfs file {0} status error: {1}'.format(hdfsFilePath, str(exception)))\n        raise exception\n\n    if os.path.exists(localFilePath) and override:\n        os.remove(localFilePath)\n    try:\n        hdfsClient.copy_to_local(hdfsFilePath, localFilePath)\n    except Exception as exception:\n        nni_log(LogType.Error, 'Copy hdfs file {0} to {1} error: {2}'.format(hdfsFilePath, localFilePath, str(exception)))\n        raise exception\n    nni_log(LogType.Info, 'Successfully copied hdfs file {0} to {1}, {2} bytes'.format(hdfsFilePath, localFilePath, file_status.length))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copyDirectoryToHdfs(localDirectory, hdfsDirectory, hdfsClient):\n    '''Copy directory from local to HDFS'''\n    if not os.path.exists(localDirectory):\n        raise Exception('Local Directory does not exist!')\n    hdfsClient.mkdirs(hdfsDirectory)\n    result = True\n    for file in os.listdir(localDirectory):\n        file_path = os.path.join(localDirectory, file)\n        if os.path.isdir(file_path):\n            hdfs_directory = os.path.join(hdfsDirectory, file)\n            try:\n                result = result and copyDirectoryToHdfs(file_path, hdfs_directory, hdfsClient)\n            except Exception as exception:\n                nni_log(LogType.Error, 'Copy local directory {0} to hdfs directory {1} error: {2}'.format(file_path, hdfs_directory, str(exception)))\n                result = False\n        else:\n            hdfs_file_path = os.path.join(hdfsDirectory, file)\n            try:\n                result = result and copyFileToHdfs(file_path, hdfs_file_path, hdfsClient)\n            except Exception as exception:\n                nni_log(LogType.Error, 'Copy local file {0} to hdfs {1} error: {2}'.format(file_path, hdfs_file_path, str(exception)))\n                result = False\n    return result", "response": "Copy directory from local to HDFS"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copyFileToHdfs(localFilePath, hdfsFilePath, hdfsClient, override=True):\n    '''Copy a local file to HDFS directory'''\n    if not os.path.exists(localFilePath):\n        raise Exception('Local file Path does not exist!')\n    if os.path.isdir(localFilePath):\n        raise Exception('localFile should not a directory!')\n    if hdfsClient.exists(hdfsFilePath):\n        if override:\n            hdfsClient.delete(hdfsFilePath)\n        else:\n            return False\n    try:\n        hdfsClient.copy_from_local(localFilePath, hdfsFilePath)\n        return True\n    except Exception as exception:\n        nni_log(LogType.Error, 'Copy local file {0} to hdfs file {1} error: {2}'.format(localFilePath, hdfsFilePath, str(exception)))\n        return False", "response": "Copy a local file to HDFS directory"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading dataset use boston dataset", "response": "def load_data():\n    '''Load dataset, use boston dataset'''\n    boston = load_boston()\n    X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=99, test_size=0.25)\n    #normalize data\n    ss_X = StandardScaler()\n    ss_y = StandardScaler()\n\n    X_train = ss_X.fit_transform(X_train)\n    X_test = ss_X.transform(X_test)\n    y_train = ss_y.fit_transform(y_train[:, None])[:,0]\n    y_test = ss_y.transform(y_test[:, None])[:,0]\n\n    return X_train, X_test, y_train, y_test"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget model according to parameters", "response": "def get_model(PARAMS):\n    '''Get model according to parameters'''\n    model_dict = {\n        'LinearRegression': LinearRegression(),\n        'SVR': SVR(),\n        'KNeighborsRegressor': KNeighborsRegressor(),\n        'DecisionTreeRegressor': DecisionTreeRegressor()\n    }\n    if not model_dict.get(PARAMS['model_name']):\n        LOG.exception('Not supported model!')\n        exit(1)\n    \n    model = model_dict[PARAMS['model_name']]\n    \n    try:\n        if PARAMS['model_name'] == 'SVR':\n            model.kernel = PARAMS['svr_kernel']\n        elif PARAMS['model_name'] == 'KNeighborsRegressor':\n            model.weights = PARAMS['knr_weights']\n    except Exception as exception:\n        LOG.exception(exception)\n        raise\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(X_train, X_test, y_train, y_test, PARAMS):\n    '''Train model and predict result'''\n    model.fit(X_train, y_train)\n    predict_y = model.predict(X_test)\n    score = r2_score(y_test, predict_y)\n    LOG.debug('r2 score: %s' % score)\n    nni.report_final_result(score)", "response": "Train model and predict result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_skip_connection(self, u, v, connection_type):\n        if connection_type not in [self.CONCAT_CONNECT, self.ADD_CONNECT]:\n            raise ValueError(\n                \"connection_type should be NetworkDescriptor.CONCAT_CONNECT \"\n                \"or NetworkDescriptor.ADD_CONNECT.\"\n            )\n        self.skip_connections.append((u, v, connection_type))", "response": "Adds a skip - connection to the descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a layer to the Graph.", "response": "def add_layer(self, layer, input_node_id):\n        \"\"\"Add a layer to the Graph.\n        Args:\n            layer: An instance of the subclasses of StubLayer in layers.py.\n            input_node_id: An integer. The ID of the input node of the layer.\n        Returns:\n            output_node_id: An integer. The ID of the output node of the layer.\n        \"\"\"\n        if isinstance(input_node_id, Iterable):\n            layer.input = list(map(lambda x: self.node_list[x], input_node_id))\n            output_node_id = self._add_node(Node(layer.output_shape))\n            for node_id in input_node_id:\n                self._add_edge(layer, node_id, output_node_id)\n\n        else:\n            layer.input = self.node_list[input_node_id]\n            output_node_id = self._add_node(Node(layer.output_shape))\n            self._add_edge(layer, input_node_id, output_node_id)\n\n        layer.output = self.node_list[output_node_id]\n        return output_node_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new node to the node list and give the node an ID.", "response": "def _add_node(self, node):\n        \"\"\"Add a new node to node_list and give the node an ID.\n        Args:\n            node: An instance of Node.\n        Returns:\n            node_id: An integer.\n        \"\"\"\n        node_id = len(self.node_list)\n        self.node_to_id[node] = node_id\n        self.node_list.append(node)\n        self.adj_list[node_id] = []\n        self.reverse_adj_list[node_id] = []\n        return node_id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_edge(self, layer, input_id, output_id):\n\n        if layer in self.layer_to_id:\n            layer_id = self.layer_to_id[layer]\n            if input_id not in self.layer_id_to_input_node_ids[layer_id]:\n                self.layer_id_to_input_node_ids[layer_id].append(input_id)\n            if output_id not in self.layer_id_to_output_node_ids[layer_id]:\n                self.layer_id_to_output_node_ids[layer_id].append(output_id)\n        else:\n            layer_id = len(self.layer_list)\n            self.layer_list.append(layer)\n            self.layer_to_id[layer] = layer_id\n            self.layer_id_to_input_node_ids[layer_id] = [input_id]\n            self.layer_id_to_output_node_ids[layer_id] = [output_id]\n\n        self.adj_list[input_id].append((output_id, layer_id))\n        self.reverse_adj_list[output_id].append((input_id, layer_id))", "response": "Add a new edge between two nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _redirect_edge(self, u_id, v_id, new_v_id):\n        layer_id = None\n        for index, edge_tuple in enumerate(self.adj_list[u_id]):\n            if edge_tuple[0] == v_id:\n                layer_id = edge_tuple[1]\n                self.adj_list[u_id][index] = (new_v_id, layer_id)\n                self.layer_list[layer_id].output = self.node_list[new_v_id]\n                break\n\n        for index, edge_tuple in enumerate(self.reverse_adj_list[v_id]):\n            if edge_tuple[0] == u_id:\n                layer_id = edge_tuple[1]\n                self.reverse_adj_list[v_id].remove(edge_tuple)\n                break\n        self.reverse_adj_list[new_v_id].append((u_id, layer_id))\n        for index, value in enumerate(self.layer_id_to_output_node_ids[layer_id]):\n            if value == v_id:\n                self.layer_id_to_output_node_ids[layer_id][index] = new_v_id\n                break", "response": "Redirect the edge from u_id to v_id into a new node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces the layer with a new layer.", "response": "def _replace_layer(self, layer_id, new_layer):\n        \"\"\"Replace the layer with a new layer.\"\"\"\n        old_layer = self.layer_list[layer_id]\n        new_layer.input = old_layer.input\n        new_layer.output = old_layer.output\n        new_layer.output.shape = new_layer.output_shape\n        self.layer_list[layer_id] = new_layer\n        self.layer_to_id[new_layer] = layer_id\n        self.layer_to_id.pop(old_layer)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef topological_order(self):\n        q = Queue()\n        in_degree = {}\n        for i in range(self.n_nodes):\n            in_degree[i] = 0\n        for u in range(self.n_nodes):\n            for v, _ in self.adj_list[u]:\n                in_degree[v] += 1\n        for i in range(self.n_nodes):\n            if in_degree[i] == 0:\n                q.put(i)\n\n        order_list = []\n        while not q.empty():\n            u = q.get()\n            order_list.append(u)\n            for v, _ in self.adj_list[u]:\n                in_degree[v] -= 1\n                if in_degree[v] == 0:\n                    q.put(v)\n        return order_list", "response": "Return the topological order of the node IDs from the input node to the output node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_pooling_layers(self, start_node_id, end_node_id):\n        layer_list = []\n        node_list = [start_node_id]\n        assert self._depth_first_search(end_node_id, layer_list, node_list)\n        ret = []\n        for layer_id in layer_list:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"Pooling\"):\n                ret.append(layer)\n            elif is_layer(layer, \"Conv\") and layer.stride != 1:\n                ret.append(layer)\n        return ret", "response": "Given two node IDs return all the pooling layers between them."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _depth_first_search(self, target_id, layer_id_list, node_list):\n        assert len(node_list) <= self.n_nodes\n        u = node_list[-1]\n        if u == target_id:\n            return True\n\n        for v, layer_id in self.adj_list[u]:\n            layer_id_list.append(layer_id)\n            node_list.append(v)\n            if self._depth_first_search(target_id, layer_id_list, node_list):\n                return True\n            layer_id_list.pop()\n            node_list.pop()\n\n        return False", "response": "Recursive function to search for all the layers and nodes down the path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _search(self, u, start_dim, total_dim, n_add):\n        if (u, start_dim, total_dim, n_add) in self.vis:\n            return\n        self.vis[(u, start_dim, total_dim, n_add)] = True\n        for v, layer_id in self.adj_list[u]:\n            layer = self.layer_list[layer_id]\n\n            if is_layer(layer, \"Conv\"):\n                new_layer = wider_next_conv(\n                    layer, start_dim, total_dim, n_add, self.weighted\n                )\n                self._replace_layer(layer_id, new_layer)\n\n            elif is_layer(layer, \"Dense\"):\n                new_layer = wider_next_dense(\n                    layer, start_dim, total_dim, n_add, self.weighted\n                )\n                self._replace_layer(layer_id, new_layer)\n\n            elif is_layer(layer, \"BatchNormalization\"):\n                new_layer = wider_bn(layer, start_dim, total_dim, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n                self._search(v, start_dim, total_dim, n_add)\n\n            elif is_layer(layer, \"Concatenate\"):\n                if self.layer_id_to_input_node_ids[layer_id][1] == u:\n                    # u is on the right of the concat\n                    # next_start_dim += next_total_dim - total_dim\n                    left_dim = self._upper_layer_width(\n                        self.layer_id_to_input_node_ids[layer_id][0]\n                    )\n                    next_start_dim = start_dim + left_dim\n                    next_total_dim = total_dim + left_dim\n                else:\n                    next_start_dim = start_dim\n                    next_total_dim = total_dim + self._upper_layer_width(\n                        self.layer_id_to_input_node_ids[layer_id][1]\n                    )\n                self._search(v, next_start_dim, next_total_dim, n_add)\n\n            else:\n                self._search(v, start_dim, total_dim, n_add)\n\n        for v, layer_id in self.reverse_adj_list[u]:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"Conv\"):\n                new_layer = wider_pre_conv(layer, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n            elif is_layer(layer, \"Dense\"):\n                new_layer = wider_pre_dense(layer, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n            elif is_layer(layer, \"Concatenate\"):\n                continue\n            else:\n                self._search(v, start_dim, total_dim, n_add)", "response": "This function searches the graph for all the layers to be widened caused by an operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_deeper_model(self, target_id, new_layer):\n        self.operation_history.append((\"to_deeper_model\", target_id, new_layer))\n        input_id = self.layer_id_to_input_node_ids[target_id][0]\n        output_id = self.layer_id_to_output_node_ids[target_id][0]\n        if self.weighted:\n            if is_layer(new_layer, \"Dense\"):\n                init_dense_weight(new_layer)\n            elif is_layer(new_layer, \"Conv\"):\n                init_conv_weight(new_layer)\n            elif is_layer(new_layer, \"BatchNormalization\"):\n                init_bn_weight(new_layer)\n\n        self._insert_new_layers([new_layer], input_id, output_id)", "response": "Inserts a relu - conv - bn block after the target block."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_wider_model(self, pre_layer_id, n_add):\n        self.operation_history.append((\"to_wider_model\", pre_layer_id, n_add))\n        pre_layer = self.layer_list[pre_layer_id]\n        output_id = self.layer_id_to_output_node_ids[pre_layer_id][0]\n        dim = layer_width(pre_layer)\n        self.vis = {}\n        self._search(output_id, dim, dim, n_add)\n        # Update the tensor shapes.\n        for u in self.topological_order:\n            for v, layer_id in self.adj_list[u]:\n                self.node_list[v].shape = self.layer_list[layer_id].output_shape", "response": "Widen the last dimension of the output of the pre_layer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _insert_new_layers(self, new_layers, start_node_id, end_node_id):\n        new_node_id = self._add_node(deepcopy(self.node_list[end_node_id]))\n        temp_output_id = new_node_id\n        for layer in new_layers[:-1]:\n            temp_output_id = self.add_layer(layer, temp_output_id)\n\n        self._add_edge(new_layers[-1], temp_output_id, end_node_id)\n        new_layers[-1].input = self.node_list[temp_output_id]\n        new_layers[-1].output = self.node_list[end_node_id]\n        self._redirect_edge(start_node_id, end_node_id, new_node_id)", "response": "Insert the new_layers after the node with start_node_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_add_skip_model(self, start_id, end_id):\n        self.operation_history.append((\"to_add_skip_model\", start_id, end_id))\n        filters_end = self.layer_list[end_id].output.shape[-1]\n        filters_start = self.layer_list[start_id].output.shape[-1]\n        start_node_id = self.layer_id_to_output_node_ids[start_id][0]\n\n        pre_end_node_id = self.layer_id_to_input_node_ids[end_id][0]\n        end_node_id = self.layer_id_to_output_node_ids[end_id][0]\n\n        skip_output_id = self._insert_pooling_layer_chain(start_node_id, end_node_id)\n\n        # Add the conv layer\n        new_conv_layer = get_conv_class(self.n_dim)(filters_start, filters_end, 1)\n        skip_output_id = self.add_layer(new_conv_layer, skip_output_id)\n\n        # Add the add layer.\n        add_input_node_id = self._add_node(deepcopy(self.node_list[end_node_id]))\n        add_layer = StubAdd()\n\n        self._redirect_edge(pre_end_node_id, end_node_id, add_input_node_id)\n        self._add_edge(add_layer, add_input_node_id, end_node_id)\n        self._add_edge(add_layer, skip_output_id, end_node_id)\n        add_layer.input = [\n            self.node_list[add_input_node_id],\n            self.node_list[skip_output_id],\n        ]\n        add_layer.output = self.node_list[end_node_id]\n        self.node_list[end_node_id].shape = add_layer.output_shape\n\n        # Set weights to the additional conv layer.\n        if self.weighted:\n            filter_shape = (1,) * self.n_dim\n            weights = np.zeros((filters_end, filters_start) + filter_shape)\n            bias = np.zeros(filters_end)\n            new_conv_layer.set_weights(\n                (add_noise(weights, np.array([0, 1])), add_noise(bias, np.array([0, 1])))\n            )", "response": "Add a weighted add skip - connection from after start node to end node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_concat_skip_model(self, start_id, end_id):\n        self.operation_history.append((\"to_concat_skip_model\", start_id, end_id))\n        filters_end = self.layer_list[end_id].output.shape[-1]\n        filters_start = self.layer_list[start_id].output.shape[-1]\n        start_node_id = self.layer_id_to_output_node_ids[start_id][0]\n\n        pre_end_node_id = self.layer_id_to_input_node_ids[end_id][0]\n        end_node_id = self.layer_id_to_output_node_ids[end_id][0]\n\n        skip_output_id = self._insert_pooling_layer_chain(start_node_id, end_node_id)\n\n        concat_input_node_id = self._add_node(deepcopy(self.node_list[end_node_id]))\n        self._redirect_edge(pre_end_node_id, end_node_id, concat_input_node_id)\n\n        concat_layer = StubConcatenate()\n        concat_layer.input = [\n            self.node_list[concat_input_node_id],\n            self.node_list[skip_output_id],\n        ]\n        concat_output_node_id = self._add_node(Node(concat_layer.output_shape))\n        self._add_edge(concat_layer, concat_input_node_id, concat_output_node_id)\n        self._add_edge(concat_layer, skip_output_id, concat_output_node_id)\n        concat_layer.output = self.node_list[concat_output_node_id]\n        self.node_list[concat_output_node_id].shape = concat_layer.output_shape\n\n        # Add the concatenate layer.\n        new_conv_layer = get_conv_class(self.n_dim)(\n            filters_start + filters_end, filters_end, 1\n        )\n        self._add_edge(new_conv_layer, concat_output_node_id, end_node_id)\n        new_conv_layer.input = self.node_list[concat_output_node_id]\n        new_conv_layer.output = self.node_list[end_node_id]\n        self.node_list[end_node_id].shape = new_conv_layer.output_shape\n\n        if self.weighted:\n            filter_shape = (1,) * self.n_dim\n            weights = np.zeros((filters_end, filters_end) + filter_shape)\n            for i in range(filters_end):\n                filter_weight = np.zeros((filters_end,) + filter_shape)\n                center_index = (i,) + (0,) * self.n_dim\n                filter_weight[center_index] = 1\n                weights[i, ...] = filter_weight\n            weights = np.concatenate(\n                (weights, np.zeros((filters_end, filters_start) + filter_shape)), axis=1\n            )\n            bias = np.zeros(filters_end)\n            new_conv_layer.set_weights(\n                (add_noise(weights, np.array([0, 1])), add_noise(bias, np.array([0, 1])))\n            )", "response": "Add a weighted add concatenate connection from after start node to after end node."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the description of the Graph as an instance of NetworkDescriptor.", "response": "def extract_descriptor(self):\n        \"\"\"Extract the the description of the Graph as an instance of NetworkDescriptor.\"\"\"\n        main_chain = self.get_main_chain()\n        index_in_main_chain = {}\n        for index, u in enumerate(main_chain):\n            index_in_main_chain[u] = index\n\n        ret = NetworkDescriptor()\n        for u in main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v not in index_in_main_chain:\n                    continue\n                layer = self.layer_list[layer_id]\n                copied_layer = copy(layer)\n                copied_layer.weights = None\n                ret.add_layer(deepcopy(copied_layer))\n\n        for u in index_in_main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v not in index_in_main_chain:\n                    temp_u = u\n                    temp_v = v\n                    temp_layer_id = layer_id\n                    skip_type = None\n                    while not (temp_v in index_in_main_chain and temp_u in index_in_main_chain):\n                        if is_layer(self.layer_list[temp_layer_id], \"Concatenate\"):\n                            skip_type = NetworkDescriptor.CONCAT_CONNECT\n                        if is_layer(self.layer_list[temp_layer_id], \"Add\"):\n                            skip_type = NetworkDescriptor.ADD_CONNECT\n                        temp_u = temp_v\n                        temp_v, temp_layer_id = self.adj_list[temp_v][0]\n                    ret.add_skip_connection(\n                        index_in_main_chain[u], index_in_main_chain[temp_u], skip_type\n                    )\n\n                elif index_in_main_chain[v] - index_in_main_chain[u] != 1:\n                    skip_type = None\n                    if is_layer(self.layer_list[layer_id], \"Concatenate\"):\n                        skip_type = NetworkDescriptor.CONCAT_CONNECT\n                    if is_layer(self.layer_list[layer_id], \"Add\"):\n                        skip_type = NetworkDescriptor.ADD_CONNECT\n                    ret.add_skip_connection(\n                        index_in_main_chain[u], index_in_main_chain[v], skip_type\n                    )\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear weights of the graph", "response": "def clear_weights(self):\n        ''' clear weights of the graph\n        '''\n        self.weighted = False\n        for layer in self.layer_list:\n            layer.weights = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of layer IDs in the main chain.", "response": "def get_main_chain_layers(self):\n        \"\"\"Return a list of layer IDs in the main chain.\"\"\"\n        main_chain = self.get_main_chain()\n        ret = []\n        for u in main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v in main_chain and u in main_chain:\n                    ret.append(layer_id)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the main chain node ID list.", "response": "def get_main_chain(self):\n        \"\"\"Returns the main chain node ID list.\"\"\"\n        pre_node = {}\n        distance = {}\n        for i in range(self.n_nodes):\n            distance[i] = 0\n            pre_node[i] = i\n        for i in range(self.n_nodes - 1):\n            for u in range(self.n_nodes):\n                for v, _ in self.adj_list[u]:\n                    if distance[u] + 1 > distance[v]:\n                        distance[v] = distance[u] + 1\n                        pre_node[v] = u\n        temp_id = 0\n        for i in range(self.n_nodes):\n            if distance[i] > distance[temp_id]:\n                temp_id = i\n        ret = []\n        for i in range(self.n_nodes + 5):\n            ret.append(temp_id)\n            if pre_node[temp_id] == temp_id:\n                break\n            temp_id = pre_node[temp_id]\n        assert temp_id == pre_node[temp_id]\n        ret.reverse()\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        _logger.info('Start dispatcher')\n        if dispatcher_env_vars.NNI_MODE == 'resume':\n            self.load_checkpoint()\n\n        while True:\n            command, data = receive()\n            if data:\n                data = json_tricks.loads(data)\n\n            if command is None or command is CommandType.Terminate:\n                break\n            if multi_thread_enabled():\n                result = self.pool.map_async(self.process_command_thread, [(command, data)])\n                self.thread_results.append(result)\n                if any([thread_result.ready() and not thread_result.successful() for thread_result in self.thread_results]):\n                    _logger.debug('Caught thread exception')\n                    break\n            else:\n                self.enqueue_command(command, data)\n                if self.worker_exceptions:\n                    break\n\n        _logger.info('Dispatcher exiting...')\n        self.stopping = True\n        if multi_thread_enabled():\n            self.pool.close()\n            self.pool.join()\n        else:\n            self.default_worker.join()\n            self.assessor_worker.join()\n\n        _logger.info('Terminated by NNI manager')", "response": "This function will never return unless raise. This function will never return unless raise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef command_queue_worker(self, command_queue):\n        while True:\n            try:\n                # set timeout to ensure self.stopping is checked periodically\n                command, data = command_queue.get(timeout=3)\n                try:\n                    self.process_command(command, data)\n                except Exception as e:\n                    _logger.exception(e)\n                    self.worker_exceptions.append(e)\n                    break\n            except Empty:\n                pass\n            if self.stopping and (_worker_fast_exit_on_terminate or command_queue.empty()):\n                break", "response": "Process commands in command queues."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enqueue_command(self, command, data):\n        if command == CommandType.TrialEnd or (command == CommandType.ReportMetricData and data['type'] == 'PERIODICAL'):\n            self.assessor_command_queue.put((command, data))\n        else:\n            self.default_command_queue.put((command, data))\n\n        qsize = self.default_command_queue.qsize()\n        if qsize >= QUEUE_LEN_WARNING_MARK:\n            _logger.warning('default queue length: %d', qsize)\n\n        qsize = self.assessor_command_queue.qsize()\n        if qsize >= QUEUE_LEN_WARNING_MARK:\n            _logger.warning('assessor queue length: %d', qsize)", "response": "Enqueue command into command queues\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess a command from the command thread.", "response": "def process_command_thread(self, request):\n        \"\"\"Worker thread to process a command.\n        \"\"\"\n        command, data = request\n        if multi_thread_enabled():\n            try:\n                self.process_command(command, data)\n            except Exception as e:\n                _logger.exception(str(e))\n                raise\n        else:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate values in the array to match their corresponding type", "response": "def match_val_type(vals, vals_bounds, vals_types):\n    '''\n    Update values in the array, to match their corresponding type\n    '''\n    vals_new = []\n\n    for i, _ in enumerate(vals_types):\n        if vals_types[i] == \"discrete_int\":\n            # Find the closest integer in the array, vals_bounds\n            vals_new.append(min(vals_bounds[i], key=lambda x: abs(x - vals[i])))\n        elif vals_types[i] == \"range_int\":\n            # Round down to the nearest integer\n            vals_new.append(math.floor(vals[i]))\n        elif vals_types[i] == \"range_continuous\":\n            # Don't do any processing for continous numbers\n            vals_new.append(vals[i])\n        else:\n            return None\n\n    return vals_new"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_wider_graph(graph):\n    ''' wider graph\n    '''\n    weighted_layer_ids = graph.wide_layer_ids()\n    weighted_layer_ids = list(\n        filter(lambda x: graph.layer_list[x].output.shape[-1], weighted_layer_ids)\n    )\n    wider_layers = sample(weighted_layer_ids, 1)\n\n    for layer_id in wider_layers:\n        layer = graph.layer_list[layer_id]\n        if is_layer(layer, \"Conv\"):\n            n_add = layer.filters\n        else:\n            n_add = layer.units\n\n        graph.to_wider_model(layer_id, n_add)\n    return graph", "response": "converts a graph into a wider graph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate ethernet graph of the graph", "response": "def create_new_layer(layer, n_dim):\n    ''' create  new layer for the graph\n    '''\n\n    input_shape = layer.output.shape\n    dense_deeper_classes = [StubDense, get_dropout_class(n_dim), StubReLU]\n    conv_deeper_classes = [get_conv_class(n_dim), get_batch_norm_class(n_dim), StubReLU]\n    if is_layer(layer, \"ReLU\"):\n        conv_deeper_classes = [get_conv_class(n_dim), get_batch_norm_class(n_dim)]\n        dense_deeper_classes = [StubDense, get_dropout_class(n_dim)]\n    elif is_layer(layer, \"Dropout\"):\n        dense_deeper_classes = [StubDense, StubReLU]\n    elif is_layer(layer, \"BatchNormalization\"):\n        conv_deeper_classes = [get_conv_class(n_dim), StubReLU]\n\n    layer_class = None\n    if len(input_shape) == 1:\n        # It is in the dense layer part.\n        layer_class = sample(dense_deeper_classes, 1)[0]\n    else:\n        # It is in the conv layer part.\n        layer_class = sample(conv_deeper_classes, 1)[0]\n\n    if layer_class == StubDense:\n        new_layer = StubDense(input_shape[0], input_shape[0])\n\n    elif layer_class == get_dropout_class(n_dim):\n        new_layer = layer_class(Constant.DENSE_DROPOUT_RATE)\n\n    elif layer_class == get_conv_class(n_dim):\n        new_layer = layer_class(\n            input_shape[-1], input_shape[-1], sample((1, 3, 5), 1)[0], stride=1\n        )\n\n    elif layer_class == get_batch_norm_class(n_dim):\n        new_layer = layer_class(input_shape[-1])\n\n    elif layer_class == get_pooling_class(n_dim):\n        new_layer = layer_class(sample((1, 3, 5), 1)[0])\n\n    else:\n        new_layer = layer_class()\n\n    return new_layer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_deeper_graph(graph):\n    ''' deeper graph\n    '''\n\n    weighted_layer_ids = graph.deep_layer_ids()\n    if len(weighted_layer_ids) >= Constant.MAX_LAYERS:\n        return None\n\n    deeper_layer_ids = sample(weighted_layer_ids, 1)\n\n    for layer_id in deeper_layer_ids:\n        layer = graph.layer_list[layer_id]\n        new_layer = create_new_layer(layer, graph.n_dim)\n        graph.to_deeper_model(layer_id, new_layer)\n    return graph", "response": "Convert a graph into a deep graph."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef legal_graph(graph):\n    '''judge if a graph is legal or not.\n    '''\n\n    descriptor = graph.extract_descriptor()\n    skips = descriptor.skip_connections\n    if len(skips) != len(set(skips)):\n        return False\n    return True", "response": "judge if a graph is legal or not."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transform(graph):\n    '''core transform function for graph.\n    '''\n\n    graphs = []\n    for _ in range(Constant.N_NEIGHBOURS * 2):\n        random_num = randrange(3)\n        temp_graph = None\n        if random_num == 0:\n            temp_graph = to_deeper_graph(deepcopy(graph))\n        elif random_num == 1:\n            temp_graph = to_wider_graph(deepcopy(graph))\n        elif random_num == 2:\n            temp_graph = to_skip_connection_graph(deepcopy(graph))\n\n        if temp_graph is not None and temp_graph.size() <= Constant.MAX_MODEL_SIZE:\n            graphs.append(temp_graph)\n\n        if len(graphs) >= Constant.N_NEIGHBOURS:\n            break\n\n    return graphs", "response": "core transform function for graph.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef uniform(low, high, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    random_state: an object of numpy.random.RandomState\n    '''\n    assert high > low, 'Upper bound must be larger than lower bound'\n    return random_state.uniform(low, high)", "response": "Returns a random number from the given lower and upper bounds."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a random sample of the n - dimensional sequence from low to high.", "response": "def quniform(low, high, q, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.round(uniform(low, high, random_state) / q) * q"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef loguniform(low, high, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    random_state: an object of numpy.random.RandomState\n    '''\n    assert low > 0, 'Lower bound must be positive'\n    return np.exp(uniform(np.log(low), np.log(high), random_state))", "response": "Log - uniform variates on a random variates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef qloguniform(low, high, q, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.round(loguniform(low, high, random_state) / q) * q", "response": "Return a random sample of the log - uniform variates of the given range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef qnormal(mu, sigma, q, random_state):\n    '''\n    mu: float or array_like of floats\n    sigma: float or array_like of floats\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.round(normal(mu, sigma, random_state) / q) * q", "response": "Random variates from mu sigma q"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lognormal(mu, sigma, random_state):\n    '''\n    mu: float or array_like of floats\n    sigma: float or array_like of floats\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.exp(normal(mu, sigma, random_state))", "response": "mu: float or array_like of floats\n    sigma: float or array_like of floats\n    random_state: an object of numpy.random.RandomState"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict(parameters_value, regressor_gp):\n    '''\n    Predict by Gaussian Process Model\n    '''\n    parameters_value = numpy.array(parameters_value).reshape(-1, len(parameters_value))\n    mu, sigma = regressor_gp.predict(parameters_value, return_std=True)\n\n    return mu[0], sigma[0]", "response": "Predict by Gaussian Process Model\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rest_get(url, timeout):\n    '''Call rest get method'''\n    try:\n        response = requests.get(url, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http get to url {1}'.format(str(e), url))\n        return None", "response": "Call rest get method"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall rest post method", "response": "def rest_post(url, data, timeout, rethrow_exception=False):\n    '''Call rest post method'''\n    try:\n        response = requests.post(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                 data=data, timeout=timeout)\n        return response\n    except Exception as e:\n        if rethrow_exception is True:\n            raise\n        print('Get exception {0} when sending http post to url {1}'.format(str(e), url))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rest_put(url, data, timeout):\n    '''Call rest put method'''\n    try:\n        response = requests.put(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                data=data, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http put to url {1}'.format(str(e), url))\n        return None", "response": "Call REST put method"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall rest delete method", "response": "def rest_delete(url, timeout):\n    '''Call rest delete method'''\n    try:\n        response = requests.delete(url, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http delete to url {1}'.format(str(e), url))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef trial_end(self, trial_job_id, success):\n        if success:\n            if self.set_best_performance:\n                self.completed_best_performance = max(self.completed_best_performance, self.trial_history[-1])\n            else:\n                self.set_best_performance = True\n                self.completed_best_performance = self.trial_history[-1]\n            logger.info('Updated complted best performance, trial job id:', trial_job_id)\n        else:\n            logger.info('No need to update, trial job id: ', trial_job_id)", "response": "update the best performance of completed trial job"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassess whether a trial should be early stop by curve fitting algorithm.", "response": "def assess_trial(self, trial_job_id, trial_history):\n        \"\"\"assess whether a trial should be early stop by curve fitting algorithm\n\n        Parameters\n        ----------\n        trial_job_id: int\n            trial job id\n        trial_history: list\n            The history performance matrix of each trial\n\n        Returns\n        -------\n        bool\n            AssessResult.Good or AssessResult.Bad\n\n        Raises\n        ------\n        Exception\n            unrecognize exception in curvefitting_assessor\n        \"\"\"\n        self.trial_job_id = trial_job_id\n        self.trial_history = trial_history\n        if not self.set_best_performance:\n            return AssessResult.Good\n        curr_step = len(trial_history)\n        if curr_step < self.start_step:\n            return AssessResult.Good\n        \n        if trial_job_id in self.last_judgment_num.keys() and curr_step - self.last_judgment_num[trial_job_id] < self.gap:\n            return AssessResult.Good\n        self.last_judgment_num[trial_job_id] = curr_step\n\n        try:\n            start_time = datetime.datetime.now()\n            # Predict the final result\n            curvemodel = CurveModel(self.target_pos)\n            predict_y = curvemodel.predict(trial_history)\n            logger.info('Prediction done. Trial job id = ', trial_job_id, '. Predict value = ', predict_y)\n            if predict_y is None:\n                logger.info('wait for more information to predict precisely')\n                return AssessResult.Good\n            standard_performance = self.completed_best_performance * self.threshold\n\n            end_time = datetime.datetime.now()\n            if (end_time - start_time).seconds > 60:\n                logger.warning('Curve Fitting Assessor Runtime Exceeds 60s, Trial Id = ', self.trial_job_id, 'Trial History = ', self.trial_history)\n\n            if self.higher_better:\n                if predict_y > standard_performance:\n                    return AssessResult.Good\n                return AssessResult.Bad\n            else:\n                if predict_y < standard_performance:\n                    return AssessResult.Good\n                return AssessResult.Bad\n\n        except Exception as exception:\n            logger.exception('unrecognize exception in curvefitting_assessor', exception)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles an initialize command.", "response": "def handle_initialize(self, data):\n        '''\n        data is search space\n        '''\n        self.tuner.update_search_space(data)\n        send(CommandType.Initialized, '')\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a set of trial neural architecture parameters for a given parameter id.", "response": "def generate_parameters(self, parameter_id):\n        \"\"\"\n        Returns a set of trial neural architecture, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n        \"\"\"\n        if not self.history:\n            self.init_search()\n\n        new_father_id = None\n        generated_graph = None\n        if not self.training_queue:\n            new_father_id, generated_graph = self.generate()\n            new_model_id = self.model_count\n            self.model_count += 1\n            self.training_queue.append((generated_graph, new_father_id, new_model_id))\n            self.descriptors.append(generated_graph.extract_descriptor())\n\n        graph, father_id, model_id = self.training_queue.pop(0)\n\n        # from graph to json\n        json_model_path = os.path.join(self.path, str(model_id) + \".json\")\n        json_out = graph_to_json(graph, json_model_path)\n        self.total_data[parameter_id] = (json_out, father_id, model_id)\n\n        return json_out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrecord an observation of the objective function.", "response": "def receive_trial_result(self, parameter_id, parameters, value):\n        \"\"\" Record an observation of the objective function.\n    \n        Parameters\n        ----------\n        parameter_id : int\n        parameters : dict\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n        \"\"\"\n        reward = extract_scalar_reward(value)\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError(\"Received parameter_id not in total_data.\")\n\n        (_, father_id, model_id) = self.total_data[parameter_id]\n\n        graph = self.bo.searcher.load_model_by_id(model_id)\n\n        # to use the value and graph\n        self.add_model(reward, model_id)\n        self.update(father_id, graph, reward, model_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_search(self):\n        if self.verbose:\n            logger.info(\"Initializing search.\")\n        for generator in self.generators:\n            graph = generator(self.n_classes, self.input_shape).generate(\n                self.default_model_len, self.default_model_width\n            )\n            model_id = self.model_count\n            self.model_count += 1\n            self.training_queue.append((graph, -1, model_id))\n            self.descriptors.append(graph.extract_descriptor())\n\n        if self.verbose:\n            logger.info(\"Initialization finished.\")", "response": "Call the generators to generate the initial architectures for the search."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate(self):\n        generated_graph, new_father_id = self.bo.generate(self.descriptors)\n        if new_father_id is None:\n            new_father_id = 0\n            generated_graph = self.generators[0](\n                self.n_classes, self.input_shape\n            ).generate(self.default_model_len, self.default_model_width)\n\n        return new_father_id, generated_graph", "response": "Generate the next neural architecture."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, other_info, graph, metric_value, model_id):\n        father_id = other_info\n        self.bo.fit([graph.extract_descriptor()], [metric_value])\n        self.bo.add_child(father_id, model_id)", "response": "Update the controller with the evaluation result of a neural architecture."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a new model to the history and update the best_model. txt file.", "response": "def add_model(self, metric_value, model_id):\n        \"\"\" Add model to the history, x_queue and y_queue\n\n        Parameters\n        ----------\n        metric_value : float\n        graph : dict\n        model_id : int\n\n        Returns\n        -------\n        model : dict\n        \"\"\"\n        if self.verbose:\n            logger.info(\"Saving model.\")\n\n        # Update best_model text file\n        ret = {\"model_id\": model_id, \"metric_value\": metric_value}\n        self.history.append(ret)\n        if model_id == self.get_best_model_id():\n            file = open(os.path.join(self.path, \"best_model.txt\"), \"w\")\n            file.write(\"best model: \" + str(model_id))\n            file.close()\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_best_model_id(self):\n\n        if self.optimize_mode is OptimizeMode.Maximize:\n            return max(self.history, key=lambda x: x[\"metric_value\"])[\"model_id\"]\n        return min(self.history, key=lambda x: x[\"metric_value\"])[\"model_id\"]", "response": "Get the best model_id from history using the metric value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_model_by_id(self, model_id):\n\n        with open(os.path.join(self.path, str(model_id) + \".json\")) as fin:\n            json_str = fin.read().replace(\"\\n\", \"\")\n\n        load_model = json_to_graph(json_str)\n        return load_model", "response": "Load the model by model_id"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_median(temp_list):\n    num = len(temp_list)\n    temp_list.sort()\n    print(temp_list)\n    if num % 2 == 0:\n        median = (temp_list[int(num/2)] + temp_list[int(num/2) - 1]) / 2\n    else:\n        median = temp_list[int(num/2)]\n    return median", "response": "Return the median of a list of resources"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the self. x_bounds and self. x_types by the search_space. json.", "response": "def update_search_space(self, search_space):\n        \"\"\"Update the self.x_bounds and self.x_types by the search_space.json\n\n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        self.x_bounds = [[] for i in range(len(search_space))]\n        self.x_types = [NONE_TYPE for i in range(len(search_space))]\n\n        for key in search_space:\n            self.key_order.append(key)\n\n        key_type = {}\n        if isinstance(search_space, dict):\n            for key in search_space:\n                key_type = search_space[key]['_type']\n                key_range = search_space[key]['_value']\n                idx = self.key_order.index(key)\n                if key_type == 'quniform':\n                    if key_range[2] == 1:\n                        self.x_bounds[idx] = [key_range[0], key_range[1]]\n                        self.x_types[idx] = 'range_int'\n                    else:\n                        bounds = []\n                        for value in np.arange(key_range[0], key_range[1], key_range[2]):\n                            bounds.append(value)\n                        self.x_bounds[idx] = bounds\n                        self.x_types[idx] = 'discrete_int'\n                elif key_type == 'randint':\n                    self.x_bounds[idx] = [0, key_range[0]]\n                    self.x_types[idx] = 'range_int'\n                elif key_type == 'uniform':\n                    self.x_bounds[idx] = [key_range[0], key_range[1]]\n                    self.x_types[idx] = 'range_continuous'\n                elif key_type == 'choice':\n                    self.x_bounds[idx] = key_range\n\n                    for key_value in key_range:\n                        if not isinstance(key_value, (int, float)):\n                            raise RuntimeError(\"Metis Tuner only support numerical choice.\")\n\n                    self.x_types[idx] = 'discrete_int'\n                else:\n                    logger.info(\"Metis Tuner doesn't support this kind of variable: \" + str(key_type))\n                    raise RuntimeError(\"Metis Tuner doesn't support this kind of variable: \" + str(key_type))\n        else:\n            logger.info(\"The format of search space is not a dict.\")\n            raise RuntimeError(\"The format of search space is not a dict.\")\n\n        self.minimize_starting_points = _rand_init(self.x_bounds, self.x_types, \\\n                                                        self.selection_num_starting_points)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _pack_output(self, init_parameter):\n        output = {}\n        for i, param in enumerate(init_parameter):\n            output[self.key_order[i]] = param\n        return output", "response": "Pack the output dictionary into a single dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_parameters(self, parameter_id):\n        if len(self.samples_x) < self.cold_start_num:\n            init_parameter = _rand_init(self.x_bounds, self.x_types, 1)[0]\n            results = self._pack_output(init_parameter)\n        else:\n            self.minimize_starting_points = _rand_init(self.x_bounds, self.x_types, \\\n                                                       self.selection_num_starting_points)\n            results = self._selection(self.samples_x, self.samples_y_aggregation, self.samples_y,\n                                      self.x_bounds, self.x_types,\n                                      threshold_samplessize_resampling=(None if self.no_resampling is True else 50),\n                                      no_candidates=self.no_candidates,\n                                      minimize_starting_points=self.minimize_starting_points,\n                                      minimize_constraints_fun=self.minimize_constraints_fun)\n\n        logger.info(\"Generate paramageters:\\n\" + str(results))\n        return results", "response": "Generate next parameter for a given trial."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef receive_trial_result(self, parameter_id, parameters, value):\n        value = extract_scalar_reward(value)\n        if self.optimize_mode == OptimizeMode.Maximize:\n            value = -value\n\n        logger.info(\"Received trial result.\")\n        logger.info(\"value is :\" + str(value))\n        logger.info(\"parameter is : \" + str(parameters))\n\n        # parse parameter to sample_x\n        sample_x = [0 for i in range(len(self.key_order))]\n        for key in parameters:\n            idx = self.key_order.index(key)\n            sample_x[idx] = parameters[key]\n\n        # parse value to sample_y\n        temp_y = []\n        if sample_x in self.samples_x:\n            idx = self.samples_x.index(sample_x)\n            temp_y = self.samples_y[idx]\n            temp_y.append(value)\n            self.samples_y[idx] = temp_y\n\n            # calculate y aggregation\n            median = get_median(temp_y)\n            self.samples_y_aggregation[idx] = [median]\n        else:\n            self.samples_x.append(sample_x)\n            self.samples_y.append([value])\n\n            # calculate y aggregation\n            self.samples_y_aggregation.append([value])", "response": "Tuner receive result from trial."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_data(self, data):\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\" %(_completed_num, len(data)))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\" %_value)\n                continue\n            self.supplement_data_num += 1\n            _parameter_id = '_'.join([\"ImportData\", str(self.supplement_data_num)])\n            self.total_data.append(_params)\n            self.receive_trial_result(parameter_id=_parameter_id, parameters=_params, value=_value)\n        logger.info(\"Successfully import data to metis tuner.\")", "response": "Import additional data for tuning\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a GP regression model for the given samples.", "response": "def create_model(samples_x, samples_y_aggregation,\n                 n_restarts_optimizer=250, is_white_kernel=False):\n    '''\n    Trains GP regression model\n    '''\n    kernel = gp.kernels.ConstantKernel(constant_value=1,\n                                       constant_value_bounds=(1e-12, 1e12)) * \\\n                                                gp.kernels.Matern(nu=1.5)\n    if is_white_kernel is True:\n        kernel += gp.kernels.WhiteKernel(noise_level=1, noise_level_bounds=(1e-12, 1e12))\n    regressor = gp.GaussianProcessRegressor(kernel=kernel,\n                                            n_restarts_optimizer=n_restarts_optimizer,\n                                            normalize_y=True,\n                                            alpha=1e-10)\n    regressor.fit(numpy.array(samples_x), numpy.array(samples_y_aggregation))\n\n    model = {}\n    model['model'] = regressor\n    model['kernel_prior'] = str(kernel)\n    model['kernel_posterior'] = str(regressor.kernel_)\n    model['model_loglikelihood'] = regressor.log_marginal_likelihood(regressor.kernel_.theta)\n\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef json2paramater(self, ss_spec):\n        '''\n        generate all possible configs for hyperparameters from hyperparameter space.\n        ss_spec: hyperparameter space\n        '''\n        if isinstance(ss_spec, dict):\n            if '_type' in ss_spec.keys():\n                _type = ss_spec['_type']\n                _value = ss_spec['_value']\n                chosen_params = list()\n                if _type == 'choice':\n                    for value in _value:\n                        choice = self.json2paramater(value)\n                        if isinstance(choice, list):\n                            chosen_params.extend(choice)\n                        else:\n                            chosen_params.append(choice)\n                else:\n                    chosen_params = self.parse_qtype(_type, _value)\n            else:\n                chosen_params = dict()\n                for key in ss_spec.keys():\n                    chosen_params[key] = self.json2paramater(ss_spec[key])\n                return self.expand_parameters(chosen_params)\n        elif isinstance(ss_spec, list):\n            chosen_params = list()\n            for subspec in ss_spec[1:]:\n                choice = self.json2paramater(subspec)\n                if isinstance(choice, list):\n                    chosen_params.extend(choice)\n                else:\n                    chosen_params.append(choice)\n            chosen_params = list(map(lambda v: {ss_spec[0]: v}, chosen_params))\n        else:\n            chosen_params = copy.deepcopy(ss_spec)\n        return chosen_params", "response": "generate all possible parameters from hyperparameter space."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing type of quniform parameter and return a list", "response": "def _parse_quniform(self, param_value):\n        '''parse type of quniform parameter and return a list'''\n        if param_value[2] < 2:\n            raise RuntimeError(\"The number of values sampled (q) should be at least 2\")\n        low, high, count = param_value[0], param_value[1], param_value[2]\n        interval = (high - low) / (count - 1)\n        return [float(low + interval * i) for i in range(count)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_qtype(self, param_type, param_value):\n        '''parse type of quniform or qloguniform'''\n        if param_type == 'quniform':\n            return self._parse_quniform(param_value)\n        if param_type == 'qloguniform':\n            param_value[:2] = np.log(param_value[:2])\n            return list(np.exp(self._parse_quniform(param_value)))\n\n        raise RuntimeError(\"Not supported type: %s\" % param_type)", "response": "parse type of quniform or qloguniform"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expand_parameters(self, para):\n        '''\n        Enumerate all possible combinations of all parameters\n        para: {key1: [v11, v12, ...], key2: [v21, v22, ...], ...}\n        return: {{key1: v11, key2: v21, ...}, {key1: v11, key2: v22, ...}, ...}\n        '''\n        if len(para) == 1:\n            for key, values in para.items():\n                return list(map(lambda v: {key: v}, values))\n\n        key = list(para)[0]\n        values = para.pop(key)\n        rest_para = self.expand_parameters(para)\n        ret_para = list()\n        for val in values:\n            for config in rest_para:\n                config[key] = val\n                ret_para.append(copy.deepcopy(config))\n        return ret_para", "response": "Expand all possible combinations of all parameters in para and return a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimport additional data for tuning", "response": "def import_data(self, data):\n        \"\"\"Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\" %(_completed_num, len(data)))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\" %_value)\n                continue\n            _params_tuple = convert_dict2tuple(_params)\n            self.supplement_data[_params_tuple] = True\n        logger.info(\"Successfully import data to grid search tuner.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nni_log(log_type, log_message):\n    '''Log message into stdout'''\n    dt = datetime.now()\n    print('[{0}] {1} {2}'.format(dt, log_type.value, log_message))", "response": "Log message into stdout"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, buf):\n        '''\n        Write buffer data into logger/stdout\n        '''\n        for line in buf.rstrip().splitlines():\n            self.orig_stdout.write(line.rstrip() + '\\n')\n            self.orig_stdout.flush()\n            try:\n                self.logger.log(self.log_level, line.rstrip())\n            except Exception as e:\n                pass", "response": "Write buffer data into logger"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self):\n        for line in iter(self.pipeReader.readline, ''):\n            self.orig_stdout.write(line.rstrip() + '\\n')\n            self.orig_stdout.flush()\n            if self.log_collection == 'none':\n                # If not match metrics, do not put the line into queue\n                if not self.log_pattern.match(line):\n                    continue\n            self.queue.put(line)\n            \n        self.pipeReader.close()", "response": "Run the thread logging everything."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract scalar reward from trial result.", "response": "def extract_scalar_reward(value, scalar_key='default'):\n    \"\"\"\n    Extract scalar reward from trial result.\n\n    Raises\n    ------\n    RuntimeError\n        Incorrect final result: the final result should be float/int,\n        or a dict which has a key named \"default\" whose value is float/int.\n    \"\"\"\n    if isinstance(value, float) or isinstance(value, int):\n        reward = value\n    elif isinstance(value, dict) and scalar_key in value and isinstance(value[scalar_key], (float, int)):\n        reward = value[scalar_key]\n    else:\n        raise RuntimeError('Incorrect final result: the final result should be float/int, or a dict which has a key named \"default\" whose value is float/int.')\n    return reward"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_dict2tuple(value):\n    if isinstance(value, dict):\n        for _keys in value:\n            value[_keys] = convert_dict2tuple(value[_keys])\n        return tuple(sorted(value.items()))\n    else:\n        return value", "response": "convert dict type to tuple"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize dispatcher logging configuration", "response": "def init_dispatcher_logger():\n    \"\"\" Initialize dispatcher logging configuration\"\"\"\n    logger_file_path = 'dispatcher.log'\n    if dispatcher_env_vars.NNI_LOG_DIRECTORY is not None:\n        logger_file_path = os.path.join(dispatcher_env_vars.NNI_LOG_DIRECTORY, logger_file_path)\n    init_logger(logger_file_path, dispatcher_env_vars.NNI_LOG_LEVEL)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction to sample a new configuration from the configspace", "response": "def get_config(self, budget):\n        \"\"\"Function to sample a new configuration\n        This function is called inside BOHB to query a new configuration\n\n        Parameters:\n        -----------\n        budget: float\n            the budget for which this configuration is scheduled\n\n        Returns\n        -------\n        config\n            return a valid configuration with parameters and budget\n        \"\"\"\n        logger.debug('start sampling a new configuration.')\n        sample = None\n        info_dict = {}\n\n        # If no model is available, sample from prior\n        # also mix in a fraction of random configs\n        if len(self.kde_models.keys()) == 0 or np.random.rand() < self.random_fraction:\n            sample = self.configspace.sample_configuration()\n            info_dict['model_based_pick'] = False\n\n        if sample is None:\n            sample, info_dict= self.sample_from_largest_budget(info_dict)\n\n        sample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n            configuration_space=self.configspace,\n            configuration=sample.get_dictionary()\n        ).get_dictionary()\n\n        logger.debug('done sampling a new configuration.')\n        sample['TRIAL_BUDGET'] = budget\n        return sample"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new_result(self, loss, budget, parameters, update_model=True):\n        if loss is None:\n            # One could skip crashed results, but we decided\n            # assign a +inf loss and count them as bad configurations\n            loss = np.inf\n\n        if budget not in self.configs.keys():\n            self.configs[budget] = []\n            self.losses[budget] = []\n\n        # skip model building if we already have a bigger model\n        if max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n            return\n\n        # We want to get a numerical representation of the configuration in the original space\n        conf = ConfigSpace.Configuration(self.configspace, parameters)\n        self.configs[budget].append(conf.get_array())\n        self.losses[budget].append(loss)\n\n        # skip model building:\n        # a) if not enough points are available\n        if len(self.configs[budget]) <= self.min_points_in_model - 1:\n            logger.debug(\"Only %i run(s) for budget %f available, need more than %s \\\n            -> can't build model!\"%(len(self.configs[budget]), budget, self.min_points_in_model+1))\n            return\n        # b) during warnm starting when we feed previous results in and only update once\n        if not update_model:\n            return\n\n        train_configs = np.array(self.configs[budget])\n        train_losses = np.array(self.losses[budget])\n\n        n_good = max(self.min_points_in_model, (self.top_n_percent * train_configs.shape[0])//100)\n        n_bad = max(self.min_points_in_model, ((100-self.top_n_percent)*train_configs.shape[0])//100)\n\n        # Refit KDE for the current budget\n        idx = np.argsort(train_losses)\n\n        train_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n        train_data_bad = self.impute_conditional_data(train_configs[idx[n_good:n_good+n_bad]])\n\n        if train_data_good.shape[0] <= train_data_good.shape[1]:\n            return\n        if train_data_bad.shape[0] <= train_data_bad.shape[1]:\n            return\n\n        #more expensive crossvalidation method\n        #bw_estimation = 'cv_ls'\n        # quick rule of thumb\n        bw_estimation = 'normal_reference'\n\n        bad_kde = sm.nonparametric.KDEMultivariate(data=train_data_bad, var_type=self.kde_vartypes, bw=bw_estimation)\n        good_kde = sm.nonparametric.KDEMultivariate(data=train_data_good, var_type=self.kde_vartypes, bw=bw_estimation)\n\n        bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n        good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n\n        self.kde_models[budget] = {\n            'good': good_kde,\n            'bad' : bad_kde\n        }\n\n        # update probs for the categorical parameters for later sampling\n        logger.debug('done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n'\n                     %(budget, n_good, n_bad, np.min(train_losses)))", "response": "Function to register finished runs and update the model with the new result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the search space is valid.", "response": "def is_valid(self, search_space):\n        \"\"\"\n        Check the search space is valid: only contains 'choice' type\n        \n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        if not len(search_space) == 1:\n            raise RuntimeError('BatchTuner only supprt one combined-paramreters key.')\n        \n        for param in search_space:\n            param_type = search_space[param][TYPE]\n            if not param_type == CHOICE:\n                raise RuntimeError('BatchTuner only supprt one combined-paramreters type is choice.')\n            else:\n                if isinstance(search_space[param][VALUE], list):\n                    return search_space[param][VALUE]\n                raise RuntimeError('The combined-paramreters value in BatchTuner is not a list.')\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_parameters(self, parameter_id):\n        self.count +=1\n        if self.count>len(self.values)-1:\n            raise nni.NoMoreTrialError('no more parameters now.')\n        return self.values[self.count]", "response": "Returns a dict of trial ( hyper - ) parameters as a serializable object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply layer normalization. Args: inputs: A tensor with 2 or more dimensions, where the first dimension has `batch_size`. epsilon: A floating number. A very small number for preventing ZeroDivision Error. scope: Optional scope for `variable_scope`. reuse: Boolean, whether to reuse the weights of a previous layer by the same name. Returns: A tensor with the same shape and data dtype as `inputs`.", "response": "def normalize(inputs,\n              epsilon=1e-8,\n              scope=\"ln\"):\n    '''Applies layer normalization.\n\n    Args:\n      inputs: A tensor with 2 or more dimensions, where the first dimension has\n        `batch_size`.\n      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A tensor with the same shape and data dtype as `inputs`.\n    '''\n    with tf.variable_scope(scope):\n        inputs_shape = inputs.get_shape()\n        params_shape = inputs_shape[-1:]\n\n        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n        beta = tf.Variable(tf.zeros(params_shape))\n        gamma = tf.Variable(tf.ones(params_shape))\n        normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n        outputs = gamma * normalized + beta\n\n    return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multihead_attention(queries,\n                        keys,\n                        scope=\"multihead_attention\",\n                        num_units=None,\n                        num_heads=4,\n                        dropout_rate=0,\n                        is_training=True,\n                        causality=False):\n    '''Applies multihead attention.\n\n    Args:\n      queries: A 3d tensor with shape of [N, T_q, C_q].\n      keys: A 3d tensor with shape of [N, T_k, C_k].\n      num_units: A cdscalar. Attention size.\n      dropout_rate: A floating point number.\n      is_training: Boolean. Controller of mechanism for dropout.\n      causality: Boolean. If true, units that reference the future are masked.\n      num_heads: An int. Number of heads.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns\n      A 3d tensor with shape of (N, T_q, C)\n    '''\n    global look5\n    with tf.variable_scope(scope):\n        # Set the fall back option for num_units\n        if num_units is None:\n            num_units = queries.get_shape().as_list()[-1]\n\n        Q_ = []\n        K_ = []\n        V_ = []\n        for head_i in range(num_heads):\n            Q = tf.layers.dense(queries, num_units / num_heads,\n                                activation=tf.nn.relu, name='Query' + str(head_i))  # (N, T_q, C)\n            K = tf.layers.dense(keys, num_units / num_heads,\n                                activation=tf.nn.relu, name='Key' + str(head_i))  # (N, T_k, C)\n            V = tf.layers.dense(keys, num_units / num_heads,\n                                activation=tf.nn.relu, name='Value' + str(head_i))  # (N, T_k, C)\n            Q_.append(Q)\n            K_.append(K)\n            V_.append(V)\n\n        # Split and concat\n        Q_ = tf.concat(Q_, axis=0)  # (h*N, T_q, C/h)\n        K_ = tf.concat(K_, axis=0)  # (h*N, T_k, C/h)\n        V_ = tf.concat(V_, axis=0)  # (h*N, T_k, C/h)\n\n        # Multiplication\n        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (h*N, T_q, T_k)\n\n        # Scale\n        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n\n        # Key Masking\n        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)\n        key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)\n        key_masks = tf.tile(tf.expand_dims(key_masks, 1),\n                            [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)\n\n        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n        outputs = tf.where(tf.equal(key_masks, 0), paddings,\n                           outputs)  # (h*N, T_q, T_k)\n\n        # Causality = Future blinding\n        if causality:\n            diag_vals = tf.ones_like(outputs[0, :, :])  # (T_q, T_k)\n            tril = tf.contrib.linalg.LinearOperatorTriL(\n                diag_vals).to_dense()  # (T_q, T_k)\n            masks = tf.tile(tf.expand_dims(tril, 0),\n                            [tf.shape(outputs)[0], 1, 1])  # (h*N, T_q, T_k)\n\n            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n            outputs = tf.where(tf.equal(masks, 0), paddings,\n                               outputs)  # (h*N, T_q, T_k)\n\n        # Activation\n        look5 = outputs\n        outputs = tf.nn.softmax(outputs)  # (h*N, T_q, T_k)\n\n        # Query Masking\n        query_masks = tf.sign(\n            tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)\n        query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)\n        query_masks = tf.tile(tf.expand_dims(\n            query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)\n        outputs *= query_masks  # broadcasting. (N, T_q, C)\n\n        # Dropouts\n        outputs = dropout(outputs, dropout_rate, is_training)\n\n        # Weighted sum\n        outputs = tf.matmul(outputs, V_)  # ( h*N, T_q, C/h)\n\n        # Restore shape\n        outputs = tf.concat(tf.split(outputs, num_heads,\n                                     axis=0), axis=2)  # (N, T_q, C)\n\n        # Residual connection\n        if queries.get_shape().as_list()[-1] == num_units:\n            outputs += queries\n\n        # Normalize\n        outputs = normalize(outputs, scope=scope)  # (N, T_q, C)\n\n    return outputs", "response": "Applies multihead attention.\n\n    Args:\n      queries: A 3d tensor with shape of [N, T_q, C_q].\n      keys: A 3d tensor with shape of [N, T_k, C_k].\n      num_units: A cdscalar. Attention size.\n      dropout_rate: A floating point number.\n      is_training: Boolean. Controller of mechanism for dropout.\n      causality: Boolean. If true, units that reference the future are masked.\n      num_heads: An int. Number of heads.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns\n      A 3d tensor with shape of (N, T_q, C)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef positional_encoding(inputs,\n                        num_units=None,\n                        zero_pad=True,\n                        scale=True,\n                        scope=\"positional_encoding\",\n                        reuse=None):\n    '''\n    Return positinal embedding.\n    '''\n    Shape = tf.shape(inputs)\n    N = Shape[0]\n    T = Shape[1]\n    num_units = Shape[2]\n    with tf.variable_scope(scope, reuse=reuse):\n        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n\n        # First part of the PE function: sin and cos argument\n        #  Second part, apply the cosine to even columns and sin to odds.\n        X = tf.expand_dims(tf.cast(tf.range(T), tf.float32), axis=1)\n        Y = tf.expand_dims(\n            tf.cast(10000 ** -(2 * tf.range(num_units) / num_units), tf.float32), axis=0)\n        h1 = tf.cast((tf.range(num_units) + 1) % 2, tf.float32)\n        h2 = tf.cast((tf.range(num_units) % 2), tf.float32)\n        position_enc = tf.multiply(X, Y)\n        position_enc = tf.sin(position_enc) * tf.multiply(tf.ones_like(X), h1) + \\\n            tf.cos(position_enc) * tf.multiply(tf.ones_like(X), h2)\n\n        # Convert to a tensor\n        lookup_table = position_enc\n\n        if zero_pad:\n            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n                                      lookup_table[1:, :]), 0)\n        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n\n        if scale:\n            outputs = outputs * tf.sqrt(tf.cast(num_units, tf.float32))\n\n        return outputs", "response": "Returns a tensor of the positinal embedding of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npoint - wise feed forward net.", "response": "def feedforward(inputs,\n                num_units,\n                scope=\"multihead_attention\"):\n    '''Point-wise feed forward net.\n\n    Args:\n      inputs: A 3d tensor with shape of [N, T, C].\n      num_units: A list of two integers.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A 3d tensor with the same shape and dtype as inputs\n    '''\n    with tf.variable_scope(scope):\n        # Inner layer\n        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n                  \"activation\": tf.nn.relu, \"use_bias\": True}\n        outputs = tf.layers.conv1d(**params)\n\n        # Readout layer\n        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n                  \"activation\": None, \"use_bias\": True}\n        outputs = tf.layers.conv1d(**params)\n\n        # Residual connection\n        outputs += inputs\n\n        # Normalize\n        outputs = normalize(outputs)\n\n    return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate search space object.", "response": "def generate(module_name, code):\n    \"\"\"Generate search space.\n    Return a serializable search space object.\n    module_name: name of the module (str)\n    code: user code (str)\n    \"\"\"\n    try:\n        ast_tree = ast.parse(code)\n    except Exception:\n        raise RuntimeError('Bad Python code')\n\n    visitor = SearchSpaceGenerator(module_name)\n    try:\n        visitor.visit(ast_tree)\n    except AssertionError as exc:\n        raise RuntimeError('%d: %s' % (visitor.last_line, exc.args[0]))\n    return visitor.search_space, astor.to_source(ast_tree)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rest_put(url, data, timeout, show_error=False):\n    '''Call rest put method'''\n    try:\n        response = requests.put(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                data=data, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None", "response": "Call rest put method"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling rest post method", "response": "def rest_post(url, data, timeout, show_error=False):\n    '''Call rest post method'''\n    try:\n        response = requests.post(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                 data=data, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls rest get method", "response": "def rest_get(url, timeout, show_error=False):\n    '''Call rest get method'''\n    try:\n        response = requests.get(url, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rest_delete(url, timeout, show_error=False):\n    '''Call rest delete method'''\n    try:\n        response = requests.delete(url, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None", "response": "Call rest delete method"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_rest_server(rest_port):\n    '''Check if restful server is ready'''\n    retry_count = 5\n    for _ in range(retry_count):\n        response = rest_get(check_status_url(rest_port), REST_TIME_OUT)\n        if response:\n            if response.status_code == 200:\n                return True, response\n            else:\n                return False, response\n        else:\n            time.sleep(3)\n    return  False, response", "response": "Check if restful server is ready"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_rest_server_quick(rest_port):\n    '''Check if restful server is ready, only check once'''\n    response = rest_get(check_status_url(rest_port), 5)\n    if response and response.status_code == 200:\n        return True, response\n    return False, None", "response": "Check if restful server is ready only check once"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef vap(x, a, b, c):\n    return np.exp(a+b/x+c*np.log(x))", "response": "Vapor pressure model\n    \n    Parameters\n    ----------\n    x: int\n    a: float\n    b: float\n    c: float\n\n    Returns\n    -------\n    float\n        np.exp(a+b/x+c*np.log(x))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef logx_linear(x, a, b):\n    x = np.log(x)\n    return a*x + b", "response": "logx linear implementation of log"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dr_hill_zero_background(x, theta, eta, kappa):\n    return (theta* x**eta) / (kappa**eta + x**eta)", "response": "dr hill zero background"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the mmf of a single node.", "response": "def mmf(x, alpha, beta, kappa, delta):\n    \"\"\"Morgan-Mercer-Flodin\n    http://www.pisces-conservation.com/growthhelp/index.html?morgan_mercer_floden.htm\n\n    Parameters\n    ----------\n    x: int\n    alpha: float\n    beta: float\n    kappa: float\n    delta: float\n\n    Returns\n    -------\n    float\n        alpha - (alpha - beta) / (1. + (kappa * x)**delta)\n    \"\"\"\n    return alpha - (alpha - beta) / (1. + (kappa * x)**delta)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef janoschek(x, a, beta, k, delta):\n    return a - (a - beta) * np.exp(-k*x**delta)", "response": "A function that computes the Janoschek distribution."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndefining the arguments users need to follow and input", "response": "def parse_args():\n    '''Definite the arguments users need to follow and input'''\n    parser = argparse.ArgumentParser(prog='nnictl', description='use nnictl command to control nni experiments')\n    parser.add_argument('--version', '-v', action='store_true')\n    parser.set_defaults(func=nni_info)\n\n    # create subparsers for args with sub values\n    subparsers = parser.add_subparsers()\n\n    # parse start command\n    parser_start = subparsers.add_parser('create', help='create a new experiment')\n    parser_start.add_argument('--config', '-c', required=True, dest='config', help='the path of yaml config file')\n    parser_start.add_argument('--port', '-p', default=DEFAULT_REST_PORT, dest='port', help='the port of restful server')\n    parser_start.add_argument('--debug', '-d', action='store_true', help=' set debug mode')\n    parser_start.set_defaults(func=create_experiment)\n\n    # parse resume command\n    parser_resume = subparsers.add_parser('resume', help='resume a new experiment')\n    parser_resume.add_argument('id', nargs='?', help='The id of the experiment you want to resume')\n    parser_resume.add_argument('--port', '-p', default=DEFAULT_REST_PORT, dest='port', help='the port of restful server')\n    parser_resume.add_argument('--debug', '-d', action='store_true', help=' set debug mode')\n    parser_resume.set_defaults(func=resume_experiment)\n\n    # parse update command\n    parser_updater = subparsers.add_parser('update', help='update the experiment')\n    #add subparsers for parser_updater\n    parser_updater_subparsers = parser_updater.add_subparsers()\n    parser_updater_searchspace = parser_updater_subparsers.add_parser('searchspace', help='update searchspace')\n    parser_updater_searchspace.add_argument('id', nargs='?', help='the id of experiment')\n    parser_updater_searchspace.add_argument('--filename', '-f', required=True)\n    parser_updater_searchspace.set_defaults(func=update_searchspace)\n    parser_updater_concurrency = parser_updater_subparsers.add_parser('concurrency', help='update concurrency')\n    parser_updater_concurrency.add_argument('id', nargs='?', help='the id of experiment')\n    parser_updater_concurrency.add_argument('--value', '-v', required=True)\n    parser_updater_concurrency.set_defaults(func=update_concurrency)\n    parser_updater_duration = parser_updater_subparsers.add_parser('duration', help='update duration')\n    parser_updater_duration.add_argument('id', nargs='?', help='the id of experiment')\n    parser_updater_duration.add_argument('--value', '-v', required=True, help='the unit of time should in {\\'s\\', \\'m\\', \\'h\\', \\'d\\'}')\n    parser_updater_duration.set_defaults(func=update_duration)\n    parser_updater_trialnum = parser_updater_subparsers.add_parser('trialnum', help='update maxtrialnum')\n    parser_updater_trialnum.add_argument('--id', '-i', dest='id', help='the id of experiment')\n    parser_updater_trialnum.add_argument('--value', '-v', required=True)\n    parser_updater_trialnum.set_defaults(func=update_trialnum)\n\n    #parse stop command\n    parser_stop = subparsers.add_parser('stop', help='stop the experiment')\n    parser_stop.add_argument('id', nargs='?', help='the id of experiment, use \\'all\\' to stop all running experiments')\n    parser_stop.set_defaults(func=stop_experiment)\n\n    #parse trial command\n    parser_trial = subparsers.add_parser('trial', help='get trial information')\n    #add subparsers for parser_trial\n    parser_trial_subparsers = parser_trial.add_subparsers()\n    parser_trial_ls = parser_trial_subparsers.add_parser('ls', help='list trial jobs')\n    parser_trial_ls.add_argument('id', nargs='?', help='the id of experiment')\n    parser_trial_ls.set_defaults(func=trial_ls)\n    parser_trial_kill = parser_trial_subparsers.add_parser('kill', help='kill trial jobs')\n    parser_trial_kill.add_argument('id', nargs='?', help='the id of experiment')\n    parser_trial_kill.add_argument('--trial_id', '-T', required=True, dest='trial_id', help='the id of trial to be killed')\n    parser_trial_kill.set_defaults(func=trial_kill)\n\n    #parse experiment command\n    parser_experiment = subparsers.add_parser('experiment', help='get experiment information')\n    #add subparsers for parser_experiment\n    parser_experiment_subparsers = parser_experiment.add_subparsers()\n    parser_experiment_show = parser_experiment_subparsers.add_parser('show', help='show the information of experiment')\n    parser_experiment_show.add_argument('id', nargs='?', help='the id of experiment')\n    parser_experiment_show.set_defaults(func=list_experiment)\n    parser_experiment_status = parser_experiment_subparsers.add_parser('status', help='show the status of experiment')\n    parser_experiment_status.add_argument('id', nargs='?', help='the id of experiment')\n    parser_experiment_status.set_defaults(func=experiment_status)\n    parser_experiment_list = parser_experiment_subparsers.add_parser('list', help='list all of running experiment ids')\n    parser_experiment_list.add_argument('all', nargs='?', help='list all of experiments')\n    parser_experiment_list.set_defaults(func=experiment_list)\n    #import tuning data\n    parser_import_data = parser_experiment_subparsers.add_parser('import', help='import additional data')\n    parser_import_data.add_argument('id', nargs='?', help='the id of experiment')\n    parser_import_data.add_argument('--filename', '-f', required=True)\n    parser_import_data.set_defaults(func=import_data)\n    #export trial data\n    parser_trial_export = parser_experiment_subparsers.add_parser('export', help='export trial job results to csv or json')\n    parser_trial_export.add_argument('id', nargs='?', help='the id of experiment')\n    parser_trial_export.add_argument('--type', '-t', choices=['json', 'csv'], required=True, dest='type', help='target file type')\n    parser_trial_export.add_argument('--filename', '-f', required=True, dest='path', help='target file path')\n    parser_trial_export.set_defaults(func=export_trials_data)\n\n    #TODO:finish webui function\n    #parse board command\n    parser_webui = subparsers.add_parser('webui', help='get web ui information')\n    #add subparsers for parser_board\n    parser_webui_subparsers = parser_webui.add_subparsers()\n    parser_webui_url = parser_webui_subparsers.add_parser('url', help='show the url of web ui')\n    parser_webui_url.add_argument('id', nargs='?', help='the id of experiment')\n    parser_webui_url.set_defaults(func=webui_url)\n\n    #parse config command\n    parser_config = subparsers.add_parser('config', help='get config information')\n    parser_config_subparsers = parser_config.add_subparsers()\n    parser_config_show = parser_config_subparsers.add_parser('show', help='show the information of config')\n    parser_config_show.add_argument('id', nargs='?', help='the id of experiment')\n    parser_config_show.set_defaults(func=get_config)\n\n    #parse log command\n    parser_log = subparsers.add_parser('log', help='get log information')\n    # add subparsers for parser_log\n    parser_log_subparsers = parser_log.add_subparsers()\n    parser_log_stdout = parser_log_subparsers.add_parser('stdout', help='get stdout information')\n    parser_log_stdout.add_argument('id', nargs='?', help='the id of experiment')\n    parser_log_stdout.add_argument('--tail', '-T', dest='tail', type=int, help='get tail -100 content of stdout')\n    parser_log_stdout.add_argument('--head', '-H', dest='head', type=int, help='get head -100 content of stdout')\n    parser_log_stdout.add_argument('--path', action='store_true', default=False, help='get the path of stdout file')\n    parser_log_stdout.set_defaults(func=log_stdout)\n    parser_log_stderr = parser_log_subparsers.add_parser('stderr', help='get stderr information')\n    parser_log_stderr.add_argument('id', nargs='?', help='the id of experiment')\n    parser_log_stderr.add_argument('--tail', '-T', dest='tail', type=int, help='get tail -100 content of stderr')\n    parser_log_stderr.add_argument('--head', '-H', dest='head', type=int, help='get head -100 content of stderr')\n    parser_log_stderr.add_argument('--path', action='store_true', default=False, help='get the path of stderr file')\n    parser_log_stderr.set_defaults(func=log_stderr)\n    parser_log_trial = parser_log_subparsers.add_parser('trial', help='get trial log path')\n    parser_log_trial.add_argument('id', nargs='?', help='the id of experiment')\n    parser_log_trial.add_argument('--trial_id', '-T', dest='trial_id', help='find trial log path by id')\n    parser_log_trial.set_defaults(func=log_trial)\n\n    #parse package command\n    parser_package = subparsers.add_parser('package', help='control nni tuner and assessor packages')\n    # add subparsers for parser_package\n    parser_package_subparsers = parser_package.add_subparsers()\n    parser_package_install = parser_package_subparsers.add_parser('install', help='install packages')\n    parser_package_install.add_argument('--name', '-n', dest='name', help='package name to be installed')\n    parser_package_install.set_defaults(func=package_install) \n    parser_package_show = parser_package_subparsers.add_parser('show', help='show the information of packages')\n    parser_package_show.set_defaults(func=package_show)\n\n    #parse tensorboard command\n    parser_tensorboard = subparsers.add_parser('tensorboard', help='manage tensorboard')\n    parser_tensorboard_subparsers = parser_tensorboard.add_subparsers()\n    parser_tensorboard_start = parser_tensorboard_subparsers.add_parser('start', help='start tensorboard')\n    parser_tensorboard_start.add_argument('id', nargs='?', help='the id of experiment')\n    parser_tensorboard_start.add_argument('--trial_id', '-T', dest='trial_id', help='the id of trial')\n    parser_tensorboard_start.add_argument('--port', dest='port', default=6006, help='the port to start tensorboard')\n    parser_tensorboard_start.set_defaults(func=start_tensorboard)\n    parser_tensorboard_start = parser_tensorboard_subparsers.add_parser('stop', help='stop tensorboard')\n    parser_tensorboard_start.add_argument('id', nargs='?', help='the id of experiment')\n    parser_tensorboard_start.set_defaults(func=stop_tensorboard)\n\n    #parse top command\n    parser_top = subparsers.add_parser('top', help='monitor the experiment')\n    parser_top.add_argument('--time', '-t', dest='time', type=int, default=3, help='the time interval to update the experiment status, ' \\\n    'the unit is second')\n    parser_top.set_defaults(func=monitor_experiment)\n\n    args = parser.parse_args()\n    args.func(args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_log_path(config_file_name):\n    '''generate stdout and stderr log path'''\n    stdout_full_path = os.path.join(NNICTL_HOME_DIR, config_file_name, 'stdout')\n    stderr_full_path = os.path.join(NNICTL_HOME_DIR, config_file_name, 'stderr')\n    return stdout_full_path, stderr_full_path", "response": "generate stdout and stderr log path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun nni manager process", "response": "def start_rest_server(port, platform, mode, config_file_name, experiment_id=None, log_dir=None, log_level=None):\n    '''Run nni manager process'''\n    nni_config = Config(config_file_name)\n    if detect_port(port):\n        print_error('Port %s is used by another process, please reset the port!\\n' \\\n        'You could use \\'nnictl create --help\\' to get help information' % port)\n        exit(1)\n    \n    if (platform != 'local') and detect_port(int(port) + 1):\n        print_error('PAI mode need an additional adjacent port %d, and the port %d is used by another process!\\n' \\\n        'You could set another port to start experiment!\\n' \\\n        'You could use \\'nnictl create --help\\' to get help information' % ((int(port) + 1), (int(port) + 1)))\n        exit(1)\n\n    print_normal('Starting restful server...')\n    \n    entry_dir = get_nni_installation_path()\n    entry_file = os.path.join(entry_dir, 'main.js')\n    \n    node_command = 'node'\n    if sys.platform == 'win32':\n        node_command = os.path.join(entry_dir[:-3], 'Scripts', 'node.exe')\n    cmds = [node_command, entry_file, '--port', str(port), '--mode', platform, '--start_mode', mode]\n    if log_dir is not None:\n        cmds += ['--log_dir', log_dir]\n    if log_level is not None:\n        cmds += ['--log_level', log_level]\n    if mode == 'resume':\n        cmds += ['--experiment_id', experiment_id]\n    stdout_full_path, stderr_full_path = get_log_path(config_file_name)\n    stdout_file = open(stdout_full_path, 'a+')\n    stderr_file = open(stderr_full_path, 'a+')\n    time_now = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))\n    #add time information in the header of log files\n    log_header = LOG_HEADER % str(time_now)\n    stdout_file.write(log_header)\n    stderr_file.write(log_header)\n    if sys.platform == 'win32':\n        from subprocess import CREATE_NEW_PROCESS_GROUP\n        process = Popen(cmds, cwd=entry_dir, stdout=stdout_file, stderr=stderr_file, creationflags=CREATE_NEW_PROCESS_GROUP)\n    else:\n        process = Popen(cmds, cwd=entry_dir, stdout=stdout_file, stderr=stderr_file)\n    return process, str(time_now)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling setClusterMetadata to pass trial", "response": "def set_remote_config(experiment_config, port, config_file_name):\n    '''Call setClusterMetadata to pass trial'''\n    #set machine_list\n    request_data = dict()\n    request_data['machine_list'] = experiment_config['machineList']\n    if request_data['machine_list']:\n        for i in range(len(request_data['machine_list'])):\n            if isinstance(request_data['machine_list'][i].get('gpuIndices'), int):\n                request_data['machine_list'][i]['gpuIndices'] = str(request_data['machine_list'][i].get('gpuIndices'))\n    response = rest_put(cluster_metadata_url(port), json.dumps(request_data), REST_TIME_OUT)\n    err_message = ''\n    if not response or not check_response(response):\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_experiment(experiment_config, mode, port, config_file_name):\n    '''Call startExperiment (rest POST /experiment) with yaml file content'''\n    request_data = dict()\n    request_data['authorName'] = experiment_config['authorName']\n    request_data['experimentName'] = experiment_config['experimentName']\n    request_data['trialConcurrency'] = experiment_config['trialConcurrency']\n    request_data['maxExecDuration'] = experiment_config['maxExecDuration']\n    request_data['maxTrialNum'] = experiment_config['maxTrialNum']\n    request_data['searchSpace'] = experiment_config.get('searchSpace')\n    request_data['trainingServicePlatform'] = experiment_config.get('trainingServicePlatform')\n\n    if experiment_config.get('description'):\n        request_data['description'] = experiment_config['description']\n    if experiment_config.get('multiPhase'):\n        request_data['multiPhase'] = experiment_config.get('multiPhase')\n    if experiment_config.get('multiThread'):\n        request_data['multiThread'] = experiment_config.get('multiThread')\n    if experiment_config.get('advisor'):\n        request_data['advisor'] = experiment_config['advisor']\n    else:\n        request_data['tuner'] = experiment_config['tuner']\n        if 'assessor' in experiment_config:\n            request_data['assessor'] = experiment_config['assessor']\n    #debug mode should disable version check\n    if experiment_config.get('debug') is not None:\n        request_data['versionCheck'] = not experiment_config.get('debug')\n    if experiment_config.get('logCollection'):\n        request_data['logCollection'] = experiment_config.get('logCollection')\n\n    request_data['clusterMetaData'] = []\n    if experiment_config['trainingServicePlatform'] == 'local':\n        request_data['clusterMetaData'].append(\n            {'key':'codeDir', 'value':experiment_config['trial']['codeDir']})\n        request_data['clusterMetaData'].append(\n            {'key': 'command', 'value': experiment_config['trial']['command']})\n    elif experiment_config['trainingServicePlatform'] == 'remote':\n        request_data['clusterMetaData'].append(\n            {'key': 'machine_list', 'value': experiment_config['machineList']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'pai':\n        request_data['clusterMetaData'].append(\n            {'key': 'pai_config', 'value': experiment_config['paiConfig']})        \n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'kubeflow':\n        request_data['clusterMetaData'].append(\n            {'key': 'kubeflow_config', 'value': experiment_config['kubeflowConfig']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'frameworkcontroller':\n        request_data['clusterMetaData'].append(\n            {'key': 'frameworkcontroller_config', 'value': experiment_config['frameworkcontrollerConfig']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n\n    response = rest_post(experiment_url(port), json.dumps(request_data), REST_TIME_OUT, show_error=True)\n    if check_response(response):\n        return response\n    else:\n        _, stderr_full_path = get_log_path(config_file_name)\n        if response is not None:\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(response.text), indent=4, sort_keys=True, separators=(',', ':')))\n            print_error('Setting experiment error, error message is {}'.format(response.text))\n        return None", "response": "Set the experiment to the given configuration."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef launch_experiment(args, experiment_config, mode, config_file_name, experiment_id=None):\n    '''follow steps to start rest server and start experiment'''\n    nni_config = Config(config_file_name)\n    # check packages for tuner\n    if experiment_config.get('tuner') and experiment_config['tuner'].get('builtinTunerName'):\n        tuner_name = experiment_config['tuner']['builtinTunerName']\n        module_name = ModuleName[tuner_name]\n        try:\n            check_call([sys.executable, '-c', 'import %s'%(module_name)])\n        except ModuleNotFoundError as e:\n            print_error('The tuner %s should be installed through nnictl'%(tuner_name))\n            exit(1)\n    log_dir = experiment_config['logDir'] if experiment_config.get('logDir') else None\n    log_level = experiment_config['logLevel'] if experiment_config.get('logLevel') else None\n    if log_level not in ['trace', 'debug'] and args.debug:\n        log_level = 'debug'\n    # start rest server\n    rest_process, start_time = start_rest_server(args.port, experiment_config['trainingServicePlatform'], mode, config_file_name, experiment_id, log_dir, log_level)\n    nni_config.set_config('restServerPid', rest_process.pid)\n    # Deal with annotation\n    if experiment_config.get('useAnnotation'):\n        path = os.path.join(tempfile.gettempdir(), get_user(), 'nni', 'annotation')\n        if not os.path.isdir(path):\n            os.makedirs(path)\n        path = tempfile.mkdtemp(dir=path)\n        code_dir = expand_annotations(experiment_config['trial']['codeDir'], path)\n        experiment_config['trial']['codeDir'] = code_dir\n        search_space = generate_search_space(code_dir)\n        experiment_config['searchSpace'] = json.dumps(search_space)\n        assert search_space, ERROR_INFO % 'Generated search space is empty'\n    elif experiment_config.get('searchSpacePath'):\n        search_space = get_json_content(experiment_config.get('searchSpacePath'))\n        experiment_config['searchSpace'] = json.dumps(search_space)\n    else:\n        experiment_config['searchSpace'] = json.dumps('')\n\n    # check rest server\n    running, _ = check_rest_server(args.port)\n    if running:\n        print_normal('Successfully started Restful server!')\n    else:\n        print_error('Restful server start failed!')\n        print_log_content(config_file_name)\n        try:\n            kill_command(rest_process.pid)\n        except Exception:\n            raise Exception(ERROR_INFO % 'Rest server stopped!')\n        exit(1)\n\n    # set remote config\n    if experiment_config['trainingServicePlatform'] == 'remote':\n        print_normal('Setting remote config...')\n        config_result, err_msg = set_remote_config(experiment_config, args.port, config_file_name)\n        if config_result:\n            print_normal('Successfully set remote config!')\n        else:\n            print_error('Failed! Error is: {}'.format(err_msg))\n            try:\n                kill_command(rest_process.pid)\n            except Exception:\n                raise Exception(ERROR_INFO % 'Rest server stopped!')\n            exit(1)\n\n    # set local config\n    if experiment_config['trainingServicePlatform'] == 'local':\n        print_normal('Setting local config...')\n        if set_local_config(experiment_config, args.port, config_file_name):\n            print_normal('Successfully set local config!')\n        else:\n            print_error('Set local config failed!')\n            try:\n                kill_command(rest_process.pid)\n            except Exception:\n                raise Exception(ERROR_INFO % 'Rest server stopped!')\n            exit(1)\n    \n    #set pai config\n    if experiment_config['trainingServicePlatform'] == 'pai':\n        print_normal('Setting pai config...')\n        config_result, err_msg = set_pai_config(experiment_config, args.port, config_file_name)\n        if config_result:\n            print_normal('Successfully set pai config!')\n        else:\n            if err_msg:\n                print_error('Failed! Error is: {}'.format(err_msg))\n            try:\n                kill_command(rest_process.pid)\n            except Exception:\n                raise Exception(ERROR_INFO % 'Restful server stopped!')\n            exit(1)\n    \n    #set kubeflow config\n    if experiment_config['trainingServicePlatform'] == 'kubeflow':\n        print_normal('Setting kubeflow config...')\n        config_result, err_msg = set_kubeflow_config(experiment_config, args.port, config_file_name)\n        if config_result:\n            print_normal('Successfully set kubeflow config!')\n        else:\n            if err_msg:\n                print_error('Failed! Error is: {}'.format(err_msg))\n            try:\n                kill_command(rest_process.pid)\n            except Exception:\n                raise Exception(ERROR_INFO % 'Restful server stopped!')\n            exit(1)\n    \n        #set kubeflow config\n    if experiment_config['trainingServicePlatform'] == 'frameworkcontroller':\n        print_normal('Setting frameworkcontroller config...')\n        config_result, err_msg = set_frameworkcontroller_config(experiment_config, args.port, config_file_name)\n        if config_result:\n            print_normal('Successfully set frameworkcontroller config!')\n        else:\n            if err_msg:\n                print_error('Failed! Error is: {}'.format(err_msg))\n            try:\n                kill_command(rest_process.pid)\n            except Exception:\n                raise Exception(ERROR_INFO % 'Restful server stopped!')\n            exit(1)\n\n    # start a new experiment\n    print_normal('Starting experiment...')\n    # set debug configuration\n    if experiment_config.get('debug') is None:\n        experiment_config['debug'] = args.debug\n    response = set_experiment(experiment_config, mode, args.port, config_file_name)\n    if response:\n        if experiment_id is None:\n            experiment_id = json.loads(response.text).get('experiment_id')\n        nni_config.set_config('experimentId', experiment_id)\n    else:\n        print_error('Start experiment failed!')\n        print_log_content(config_file_name)\n        try:\n            kill_command(rest_process.pid)\n        except Exception:\n            raise Exception(ERROR_INFO % 'Restful server stopped!')\n        exit(1)\n    if experiment_config.get('nniManagerIp'):\n        web_ui_url_list = ['{0}:{1}'.format(experiment_config['nniManagerIp'], str(args.port))]\n    else:\n        web_ui_url_list = get_local_urls(args.port)\n    nni_config.set_config('webuiUrl', web_ui_url_list)\n    \n    #save experiment information\n    nnictl_experiment_config = Experiments()\n    nnictl_experiment_config.add_experiment(experiment_id, args.port, start_time, config_file_name, experiment_config['trainingServicePlatform'])\n\n    print_normal(EXPERIMENT_SUCCESS_INFO % (experiment_id, '   '.join(web_ui_url_list)))", "response": "start experiment and start rest server"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting a new experiment", "response": "def create_experiment(args):\n    '''start a new experiment'''\n    config_file_name = ''.join(random.sample(string.ascii_letters + string.digits, 8))\n    nni_config = Config(config_file_name)\n    config_path = os.path.abspath(args.config)\n    if not os.path.exists(config_path):\n        print_error('Please set correct config path!')\n        exit(1)\n    experiment_config = get_yml_content(config_path)\n    validate_all_content(experiment_config, config_path)\n\n    nni_config.set_config('experimentConfig', experiment_config)\n    launch_experiment(args, experiment_config, 'new', config_file_name)\n    nni_config.set_config('restServerPort', args.port)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit_theta(self):\n        x = range(1, self.point_num + 1)\n        y = self.trial_history\n        for i in range(NUM_OF_FUNCTIONS):\n            model = curve_combination_models[i]\n            try:\n                # The maximum number of iterations to fit is 100*(N+1), where N is the number of elements in `x0`.\n                if model_para_num[model] == 2:\n                    a, b = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                elif model_para_num[model] == 3:\n                    a, b, c = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                    model_para[model][2] = c\n                elif model_para_num[model] == 4:\n                    a, b, c, d = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                    model_para[model][2] = c\n                    model_para[model][3] = d\n            except (RuntimeError, FloatingPointError, OverflowError, ZeroDivisionError):\n                # Ignore exceptions caused by numerical calculations\n                pass\n            except Exception as exception:\n                logger.critical(\"Exceptions in fit_theta:\", exception)", "response": "use least squares to fit all default curves parameter seperately\n        \n        Returns None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters the poor performing curve Returns ------- None", "response": "def filter_curve(self):\n        \"\"\"filter the poor performing curve\n        \n        Returns\n        -------\n        None\n        \"\"\"\n        avg = np.sum(self.trial_history) / self.point_num\n        standard = avg * avg * self.point_num\n        predict_data = []\n        tmp_model = []\n        for i in range(NUM_OF_FUNCTIONS):\n            var = 0\n            model = curve_combination_models[i]\n            for j in range(1, self.point_num + 1):\n                y = self.predict_y(model, j)\n                var += (y - self.trial_history[j - 1]) * (y - self.trial_history[j - 1])\n            if var < standard:\n                predict_data.append(y)\n                tmp_model.append(curve_combination_models[i])\n        median = np.median(predict_data)\n        std = np.std(predict_data)\n        for model in tmp_model:\n            y = self.predict_y(model, self.target_pos)\n            epsilon = self.point_num / 10 * std\n            if y < median + epsilon and y > median - epsilon:\n                self.effective_model.append(model)\n        self.effective_model_num = len(self.effective_model)\n        logger.info('List of effective model: ', self.effective_model)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict_y(self, model, pos):\n        if model_para_num[model] == 2:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1])\n        elif model_para_num[model] == 3:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1], model_para[model][2])\n        elif model_para_num[model] == 4:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1], model_para[model][2], model_para[model][3])\n        return y", "response": "return the predict y of model when epoch = pos\n        "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f_comb(self, pos, sample):\n        ret = 0\n        for i in range(self.effective_model_num):\n            model = self.effective_model[i]\n            y = self.predict_y(model, pos)\n            ret += sample[i] * y\n        return ret", "response": "return the value of the f_comb when epoch = pos"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nnormalizing weight of a collection of samples", "response": "def normalize_weight(self, samples):\n        \"\"\"normalize weight\n        \n        Parameters\n        ----------\n        samples: list\n            a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n\n        Returns\n        -------\n        list\n            samples after normalize weight\n        \"\"\"\n        for i in range(NUM_OF_INSTANCE):\n            total = 0\n            for j in range(self.effective_model_num):\n                total += samples[i][j]\n            for j in range(self.effective_model_num):\n                samples[i][j] /= total\n        return samples"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sigma_sq(self, sample):\n        ret = 0\n        for i in range(1, self.point_num + 1):\n            temp = self.trial_history[i - 1] - self.f_comb(i, sample)\n            ret += temp * temp\n        return 1.0 * ret / self.point_num", "response": "returns the value of sigma square given the sample"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normal_distribution(self, pos, sample):\n        curr_sigma_sq = self.sigma_sq(sample)\n        delta = self.trial_history[pos - 1] - self.f_comb(pos, sample)\n        return np.exp(np.square(delta) / (-2.0 * curr_sigma_sq)) / np.sqrt(2 * np.pi * np.sqrt(curr_sigma_sq))", "response": "returns the value of normal distribution given the weight s sample and target position\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the likelihood of a list of items in the log - likelihood matrix.", "response": "def likelihood(self, samples):\n        \"\"\"likelihood\n\n        Parameters\n        ----------\n        sample: list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n        \n        Returns\n        -------\n        float\n            likelihood\n        \"\"\"\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            for j in range(1, self.point_num + 1):\n                ret[i] *= self.normal_distribution(j, samples[i])\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prior(self, samples):\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            for j in range(self.effective_model_num):\n                if not samples[i][j] > 0:\n                    ret[i] = 0\n            if self.f_comb(1, samples[i]) >= self.f_comb(self.target_pos, samples[i]):\n                ret[i] = 0\n        return ret", "response": "Returns a priori distribution\n \n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the target distribution of the posterior probability of the log - likelihood of the set of samples.", "response": "def target_distribution(self, samples):\n        \"\"\"posterior probability\n        \n        Parameters\n        ----------\n        samples: list\n            a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n        \n        Returns\n        -------\n        float\n            posterior probability\n        \"\"\"\n        curr_likelihood = self.likelihood(samples)\n        curr_prior = self.prior(samples)\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            ret[i] = curr_likelihood[i] * curr_prior[i]\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadjust the weight of each function using mcmc sampling.", "response": "def mcmc_sampling(self):\n        \"\"\"Adjust the weight of each function using mcmc sampling.\n        The initial value of each weight is evenly distribute.\n        Brief introduction:\n        (1)Definition of sample:\n            Sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n        (2)Definition of samples:\n            Samples is a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n        (3)Definition of model:\n            Model is the function we chose right now. Such as: 'wap', 'weibull'.\n        (4)Definition of pos:\n            Pos is the position we want to predict, corresponds to the value of epoch.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        init_weight = np.ones((self.effective_model_num), dtype=np.float) / self.effective_model_num\n        self.weight_samples = np.broadcast_to(init_weight, (NUM_OF_INSTANCE, self.effective_model_num))\n        for i in range(NUM_OF_SIMULATION_TIME):\n            # sample new value from Q(i, j)\n            new_values = np.random.randn(NUM_OF_INSTANCE, self.effective_model_num) * STEP_SIZE + self.weight_samples\n            new_values = self.normalize_weight(new_values)\n            # compute alpha(i, j) = min{1, P(j)Q(j, i)/P(i)Q(i, j)}\n            alpha = np.minimum(1, self.target_distribution(new_values) / self.target_distribution(self.weight_samples))\n            # sample u\n            u = np.random.rand(NUM_OF_INSTANCE)\n            # new value\n            change_value_flag = (u < alpha).astype(np.int)\n            for j in range(NUM_OF_INSTANCE):\n                new_values[j] = self.weight_samples[j] * (1 - change_value_flag[j]) + new_values[j] * change_value_flag[j]\n            self.weight_samples = new_values"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef predict(self, trial_history):\n        self.trial_history = trial_history\n        self.point_num = len(trial_history)\n        self.fit_theta()\n        self.filter_curve()\n        if self.effective_model_num < LEAST_FITTED_FUNCTION:\n            # different curve's predictions are too scattered, requires more information\n            return None\n        self.mcmc_sampling()\n        ret = 0\n        for i in range(NUM_OF_INSTANCE):\n            ret += self.f_comb(self.target_pos, self.weight_samples[i])\n        return ret / NUM_OF_INSTANCE", "response": "predict the value of target position\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _outlierDetection_threaded(inputs):\n    '''\n    Detect the outlier\n    '''\n    [samples_idx, samples_x, samples_y_aggregation] = inputs\n    sys.stderr.write(\"[%s] DEBUG: Evaluating %dth of %d samples\\n\"\\\n                        % (os.path.basename(__file__), samples_idx + 1, len(samples_x)))\n    outlier = None\n\n    # Create a diagnostic regression model which removes the sample that we want to evaluate\n    diagnostic_regressor_gp = gp_create_model.create_model(\\\n                                    samples_x[0:samples_idx] + samples_x[samples_idx + 1:],\\\n                                    samples_y_aggregation[0:samples_idx] + samples_y_aggregation[samples_idx + 1:])\n    mu, sigma = gp_prediction.predict(samples_x[samples_idx], diagnostic_regressor_gp['model'])\n\n    # 2.33 is the z-score for 98% confidence level\n    if abs(samples_y_aggregation[samples_idx] - mu) > (2.33 * sigma):\n        outlier = {\"samples_idx\": samples_idx,\n                   \"expected_mu\": mu,\n                   \"expected_sigma\": sigma,\n                   \"difference\": abs(samples_y_aggregation[samples_idx] - mu) - (2.33 * sigma)}\n    return outlier", "response": "This function is used in the main thread of the detection of the outlier."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef outlierDetection_threaded(samples_x, samples_y_aggregation):\n    '''\n    Use Multi-thread to detect the outlier\n    '''\n    outliers = []\n\n    threads_inputs = [[samples_idx, samples_x, samples_y_aggregation]\\\n                            for samples_idx in range(0, len(samples_x))]\n    threads_pool = ThreadPool(min(4, len(threads_inputs)))\n    threads_results = threads_pool.map(_outlierDetection_threaded, threads_inputs)\n    threads_pool.close()\n    threads_pool.join()\n\n    for threads_result in threads_results:\n        if threads_result is not None:\n            outliers.append(threads_result)\n        else:\n            print(\"error here.\")\n\n    outliers = None if len(outliers) == 0 else outliers\n    return outliers", "response": "Use Multi - thread to detect the outlier of a single node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wider_pre_conv(layer, n_add_filters, weighted=True):\n    '''wider previous conv layer.\n    '''\n    n_dim = get_n_dim(layer)\n    if not weighted:\n        return get_conv_class(n_dim)(\n            layer.input_channel,\n            layer.filters + n_add_filters,\n            kernel_size=layer.kernel_size,\n        )\n\n    n_pre_filters = layer.filters\n    rand = np.random.randint(n_pre_filters, size=n_add_filters)\n    teacher_w, teacher_b = layer.get_weights()\n\n    student_w = teacher_w.copy()\n    student_b = teacher_b.copy()\n    # target layer update (i)\n    for i in range(len(rand)):\n        teacher_index = rand[i]\n        new_weight = teacher_w[teacher_index, ...]\n        new_weight = new_weight[np.newaxis, ...]\n        student_w = np.concatenate((student_w, new_weight), axis=0)\n        student_b = np.append(student_b, teacher_b[teacher_index])\n    new_pre_layer = get_conv_class(n_dim)(\n        layer.input_channel, n_pre_filters + n_add_filters, layer.kernel_size\n    )\n    new_pre_layer.set_weights(\n        (add_noise(student_w, teacher_w), add_noise(student_b, teacher_b))\n    )\n    return new_pre_layer", "response": "wider previous conv layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wider_next_conv(layer, start_dim, total_dim, n_add, weighted=True):\n    '''wider next conv layer.\n    '''\n    n_dim = get_n_dim(layer)\n    if not weighted:\n        return get_conv_class(n_dim)(layer.input_channel + n_add,\n                                     layer.filters,\n                                     kernel_size=layer.kernel_size,\n                                     stride=layer.stride)\n    n_filters = layer.filters\n    teacher_w, teacher_b = layer.get_weights()\n\n    new_weight_shape = list(teacher_w.shape)\n    new_weight_shape[1] = n_add\n    new_weight = np.zeros(tuple(new_weight_shape))\n\n    student_w = np.concatenate((teacher_w[:, :start_dim, ...].copy(),\n                                add_noise(new_weight, teacher_w),\n                                teacher_w[:, start_dim:total_dim, ...].copy()), axis=1)\n    new_layer = get_conv_class(n_dim)(layer.input_channel + n_add,\n                                      n_filters,\n                                      kernel_size=layer.kernel_size,\n                                      stride=layer.stride)\n    new_layer.set_weights((student_w, teacher_b))\n    return new_layer", "response": "wider next conv layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wider_next_dense(layer, start_dim, total_dim, n_add, weighted=True):\n    '''wider next dense layer.\n    '''\n    if not weighted:\n        return StubDense(layer.input_units + n_add, layer.units)\n    teacher_w, teacher_b = layer.get_weights()\n    student_w = teacher_w.copy()\n    n_units_each_channel = int(teacher_w.shape[1] / total_dim)\n\n    new_weight = np.zeros((teacher_w.shape[0], n_add * n_units_each_channel))\n    student_w = np.concatenate(\n        (\n            student_w[:, : start_dim * n_units_each_channel],\n            add_noise(new_weight, student_w),\n            student_w[\n                :, start_dim * n_units_each_channel : total_dim * n_units_each_channel\n            ],\n        ),\n        axis=1,\n    )\n\n    new_layer = StubDense(layer.input_units + n_add, layer.units)\n    new_layer.set_weights((student_w, teacher_b))\n    return new_layer", "response": "wider next dense layer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_noise(weights, other_weights):\n    '''add noise to the layer.\n    '''\n    w_range = np.ptp(other_weights.flatten())\n    noise_range = NOISE_RATIO * w_range\n    noise = np.random.uniform(-noise_range / 2.0, noise_range / 2.0, weights.shape)\n    return np.add(noise, weights)", "response": "add noise to the layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_dense_weight(layer):\n    '''initilize dense layer weight.\n    '''\n    units = layer.units\n    weight = np.eye(units)\n    bias = np.zeros(units)\n    layer.set_weights(\n        (add_noise(weight, np.array([0, 1])), add_noise(bias, np.array([0, 1])))\n    )", "response": "initilize dense layer weight.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_bn_weight(layer):\n    '''initilize batch norm layer weight.\n    '''\n    n_filters = layer.num_features\n    new_weights = [\n        add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n    ]\n    layer.set_weights(new_weights)", "response": "initilize batch norm layer weight.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_data_from_remote(args, nni_config, trial_content, path_list, host_list, temp_nni_path):\n    '''use ssh client to copy data from remote machine to local machien'''\n    machine_list = nni_config.get_config('experimentConfig').get('machineList')\n    machine_dict = {}\n    local_path_list = []\n    for machine in machine_list:\n        machine_dict[machine['ip']] = {'port': machine['port'], 'passwd': machine['passwd'], 'username': machine['username']}\n    for index, host in enumerate(host_list):\n        local_path = os.path.join(temp_nni_path, trial_content[index].get('id'))\n        local_path_list.append(local_path)\n        print_normal('Copying log data from %s to %s' % (host + ':' + path_list[index], local_path))\n        sftp = create_ssh_sftp_client(host, machine_dict[host]['port'], machine_dict[host]['username'], machine_dict[host]['passwd'])\n        copy_remote_directory_to_local(sftp, path_list[index], local_path)\n    print_normal('Copy done!')\n    return local_path_list", "response": "use ssh client to copy data from remote machine to local machien"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget path list according to different platform", "response": "def get_path_list(args, nni_config, trial_content, temp_nni_path):\n    '''get path list according to different platform'''\n    path_list, host_list = parse_log_path(args, trial_content)\n    platform = nni_config.get_config('experimentConfig').get('trainingServicePlatform')\n    if platform == 'local':\n        print_normal('Log path: %s' % ' '.join(path_list))\n        return path_list\n    elif platform == 'remote':\n        path_list = copy_data_from_remote(args, nni_config, trial_content, path_list, host_list, temp_nni_path)\n        print_normal('Log path: %s' % ' '.join(path_list))\n        return path_list\n    else:\n        print_error('Not supported platform!')\n        exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting a tensorboard process in the local machine", "response": "def start_tensorboard_process(args, nni_config, path_list, temp_nni_path):\n    '''call cmds to start tensorboard process in local machine'''\n    if detect_port(args.port):\n        print_error('Port %s is used by another process, please reset port!' % str(args.port))\n        exit(1)\n    \n    stdout_file = open(os.path.join(temp_nni_path, 'tensorboard_stdout'), 'a+')\n    stderr_file = open(os.path.join(temp_nni_path, 'tensorboard_stderr'), 'a+')\n    cmds = ['tensorboard', '--logdir', format_tensorboard_log_path(path_list), '--port', str(args.port)]\n    tensorboard_process = Popen(cmds, stdout=stdout_file, stderr=stderr_file)\n    url_list = get_local_urls(args.port)\n    print_normal(COLOR_GREEN_FORMAT % 'Start tensorboard success!\\n' + 'Tensorboard urls: ' + '     '.join(url_list))\n    tensorboard_process_pid_list = nni_config.get_config('tensorboardPidList')\n    if tensorboard_process_pid_list is None:\n        tensorboard_process_pid_list = [tensorboard_process.pid]\n    else:\n        tensorboard_process_pid_list.append(tensorboard_process.pid)\n    nni_config.set_config('tensorboardPidList', tensorboard_process_pid_list)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the ratio and sigma of the clustering model.", "response": "def _ratio_scores(parameters_value, clusteringmodel_gmm_good, clusteringmodel_gmm_bad):\n    '''\n    The ratio is smaller the better\n    '''\n    ratio = clusteringmodel_gmm_good.score([parameters_value]) / clusteringmodel_gmm_bad.score([parameters_value])\n    sigma = 0\n    return ratio, sigma"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls selection with random starting points.", "response": "def selection_r(x_bounds,\n                x_types,\n                clusteringmodel_gmm_good,\n                clusteringmodel_gmm_bad,\n                num_starting_points=100,\n                minimize_constraints_fun=None):\n    '''\n    Call selection\n    '''\n    minimize_starting_points = [lib_data.rand(x_bounds, x_types)\\\n                                    for i in range(0, num_starting_points)]\n    outputs = selection(x_bounds, x_types,\n                        clusteringmodel_gmm_good,\n                        clusteringmodel_gmm_bad,\n                        minimize_starting_points,\n                        minimize_constraints_fun)\n    return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting the lowest mu value from the given bounds.", "response": "def selection(x_bounds,\n              x_types,\n              clusteringmodel_gmm_good,\n              clusteringmodel_gmm_bad,\n              minimize_starting_points,\n              minimize_constraints_fun=None):\n    '''\n    Select the lowest mu value\n    '''\n    results = lib_acquisition_function.next_hyperparameter_lowest_mu(\\\n                    _ratio_scores, [clusteringmodel_gmm_good, clusteringmodel_gmm_bad],\\\n                    x_bounds, x_types, minimize_starting_points, \\\n                    minimize_constraints_fun=minimize_constraints_fun)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _minimize_constraints_fun_summation(x):\n    '''\n    Minimize constraints fun summation\n    '''\n    summation = sum([x[i] for i in CONSTRAINT_PARAMS_IDX])\n    return CONSTRAINT_UPPERBOUND >= summation >= CONSTRAINT_LOWERBOUND", "response": "Minimize constraints fun summation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload dataset use 20newsgroups dataset", "response": "def load_data():\n    '''Load dataset, use 20newsgroups dataset'''\n    digits = load_digits()\n    X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=99, test_size=0.25)\n\n    ss = StandardScaler()\n    X_train = ss.fit_transform(X_train)\n    X_test = ss.transform(X_test)\n\n    return X_train, X_test, y_train, y_test"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets model according to parameters", "response": "def get_model(PARAMS):\n    '''Get model according to parameters'''\n    model = SVC()\n    model.C = PARAMS.get('C')\n    model.keral = PARAMS.get('keral')\n    model.degree = PARAMS.get('degree')\n    model.gamma = PARAMS.get('gamma')\n    model.coef0 = PARAMS.get('coef0')\n    \n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate num hyperparameter configurations from search space using Bayesian optimization", "response": "def get_hyperparameter_configurations(self, num, r, config_generator):\n        \"\"\"generate num hyperparameter configurations from search space using Bayesian optimization\n\n        Parameters\n        ----------\n        num: int\n            the number of hyperparameter configurations\n\n        Returns\n        -------\n        list\n            a list of hyperparameter configurations. Format: [[key1, value1], [key2, value2], ...]\n        \"\"\"\n        global _KEY\n        assert self.i == 0\n        hyperparameter_configs = dict()\n        for _ in range(num):\n            params_id = create_bracket_parameter_id(self.s, self.i)\n            params = config_generator.get_config(r)\n            params[_KEY] = r\n            hyperparameter_configs[params_id] = params\n        self._record_hyper_configs(hyperparameter_configs)\n        return [[key, value] for key, value in hyperparameter_configs.items()]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_initialize(self, data):\n        logger.info('start to handle_initialize')\n        # convert search space jason to ConfigSpace\n        self.handle_update_search_space(data)\n\n        # generate BOHB config_generator using Bayesian optimization\n        if self.search_space:\n            self.cg = CG_BOHB(configspace=self.search_space,\n                              min_points_in_model=self.min_points_in_model,\n                              top_n_percent=self.top_n_percent,\n                              num_samples=self.num_samples,\n                              random_fraction=self.random_fraction,\n                              bandwidth_factor=self.bandwidth_factor,\n                              min_bandwidth=self.min_bandwidth)\n        else:\n            raise ValueError('Error: Search space is None')\n        # generate first brackets\n        self.generate_new_bracket()\n        send(CommandType.Initialized, '')", "response": "Handles the initialization of a Tuner with a search space."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_new_bracket(self):\n        logger.debug(\n            'start to create a new SuccessiveHalving iteration, self.curr_s=%d', self.curr_s)\n        if self.curr_s < 0:\n            logger.info(\"s < 0, Finish this round of Hyperband in BOHB. Generate new round\")\n            self.curr_s = self.s_max\n        self.brackets[self.curr_s] = Bracket(s=self.curr_s, s_max=self.s_max, eta=self.eta,\n                                    max_budget=self.max_budget, optimize_mode=self.optimize_mode)\n        next_n, next_r = self.brackets[self.curr_s].get_n_r()\n        logger.debug(\n            'new SuccessiveHalving iteration, next_n=%d, next_r=%d', next_n, next_r)\n        # rewrite with TPE\n        generated_hyper_configs = self.brackets[self.curr_s].get_hyperparameter_configurations(\n            next_n, next_r, self.cg)\n        self.generated_hyper_configs = generated_hyper_configs.copy()", "response": "generate a new bracket"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_request_trial_jobs(self, data):\n        # Receive new request\n        self.credit += data\n\n        for _ in range(self.credit):\n            self._request_one_trial_job()", "response": "recerive the number of request and generate trials\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _request_one_trial_job(self):\n        if not self.generated_hyper_configs:\n            ret = {\n                'parameter_id': '-1_0_0',\n                'parameter_source': 'algorithm',\n                'parameters': ''\n            }\n            send(CommandType.NoMoreTrialJobs, json_tricks.dumps(ret))\n            return\n        assert self.generated_hyper_configs\n        params = self.generated_hyper_configs.pop()\n        ret = {\n            'parameter_id': params[0],\n            'parameter_source': 'algorithm',\n            'parameters': params[1]\n        }\n        self.parameters[params[0]] = params[1]\n        send(CommandType.NewTrialJob, json_tricks.dumps(ret))\n        self.credit -= 1", "response": "request one trial job i. e. one hyperparameter configuration"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_update_search_space(self, data):\n        search_space = data\n        cs = CS.ConfigurationSpace()\n        for var in search_space:\n            _type = str(search_space[var][\"_type\"])\n            if _type == 'choice':\n                cs.add_hyperparameter(CSH.CategoricalHyperparameter(\n                    var, choices=search_space[var][\"_value\"]))\n            elif _type == 'randint':\n                cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\n                    var, lower=0, upper=search_space[var][\"_value\"][0]))\n            elif _type == 'uniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1]))\n            elif _type == 'quniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1], \n                    q=search_space[var][\"_value\"][2]))\n            elif _type == 'loguniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1], \n                    log=True))\n            elif _type == 'qloguniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1], \n                    q=search_space[var][\"_value\"][2], log=True))\n            elif _type == 'normal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2]))\n            elif _type == 'qnormal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2], \n                    q=search_space[var][\"_value\"][3]))\n            elif _type == 'lognormal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2], \n                    log=True))\n            elif _type == 'qlognormal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2], \n                    q=search_space[var][\"_value\"][3], log=True))\n            else:\n                raise ValueError(\n                    'unrecognized type in search_space, type is {}'.format(_type))\n\n        self.search_space = cs", "response": "handle update of the search space of a specific experiment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_trial_end(self, data):\n        logger.debug('Tuner handle trial end, result is %s', data)\n\n        hyper_params = json_tricks.loads(data['hyper_params'])\n        s, i, _ = hyper_params['parameter_id'].split('_')\n        hyper_configs = self.brackets[int(s)].inform_trial_end(int(i))\n\n        if hyper_configs is not None:\n            logger.debug(\n                'bracket %s next round %s, hyper_configs: %s', s, i, hyper_configs)\n            self.generated_hyper_configs = self.generated_hyper_configs + hyper_configs\n            for _ in range(self.credit):\n                self._request_one_trial_job()\n        # Finish this bracket and generate a new bracket\n        elif self.brackets[int(s)].no_more_trial:\n            self.curr_s -= 1\n            self.generate_new_bracket()\n            for _ in range(self.credit):\n                self._request_one_trial_job()", "response": "handle trial end and generate next configuaration"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_report_metric_data(self, data):\n        logger.debug('handle report metric data = %s', data)\n\n        assert 'value' in data\n        value = extract_scalar_reward(data['value'])\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -value\n        else:\n            reward = value\n        assert 'parameter_id' in data\n        s, i, _ = data['parameter_id'].split('_')\n\n        logger.debug('bracket id = %s, metrics value = %s, type = %s', s, value, data['type'])\n        s = int(s)\n\n        assert 'type' in data\n        if data['type'] == 'FINAL':\n            # and PERIODICAL metric are independent, thus, not comparable.\n            assert 'sequence' in data\n            self.brackets[s].set_config_perf(\n                int(i), data['parameter_id'], sys.maxsize, value)\n            self.completed_hyper_configs.append(data)\n       \n            _parameters = self.parameters[data['parameter_id']]\n            _parameters.pop(_KEY)\n            # update BO with loss, max_s budget, hyperparameters\n            self.cg.new_result(loss=reward, budget=data['sequence'], parameters=_parameters, update_model=True)\n        elif data['type'] == 'PERIODICAL':\n            self.brackets[s].set_config_perf(\n                int(i), data['parameter_id'], data['sequence'], value)\n        else:\n            raise ValueError(\n                'Data type not supported: {}'.format(data['type']))", "response": "reveice the metric data and update the optimization with final result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_import_data(self, data):\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\" %(_completed_num, len(data)))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\" %_value)\n                continue\n            budget_exist_flag = False\n            barely_params = dict()\n            for keys in _params:\n                if keys == _KEY:\n                    _budget = _params[keys]\n                    budget_exist_flag = True\n                else:\n                    barely_params[keys] = _params[keys]\n            if not budget_exist_flag:\n                _budget = self.max_budget\n                logger.info(\"Set \\\"TRIAL_BUDGET\\\" value to %s (max budget)\" %self.max_budget)\n            if self.optimize_mode is OptimizeMode.Maximize:\n                reward = -_value\n            else:\n                reward = _value\n            self.cg.new_result(loss=reward, budget=_budget, parameters=barely_params, update_model=True)\n        logger.info(\"Successfully import tuning data to BOHB advisor.\")", "response": "Handle the import data for the tuning system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the mean and std value of dataset.", "response": "def get_mean_and_std(dataset):\n    \"\"\"Compute the mean and std value of dataset.\"\"\"\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=1, shuffle=True, num_workers=2\n    )\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(\"==> Computing mean and std..\")\n    for inputs, _ in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()\n            std[i] += inputs[:, i, :, :].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_feasibility(x_bounds, lowerbound, upperbound):\n    '''\n    This can have false positives.\n    For examples, parameters can only be 0 or 5, and the summation constraint is between 6 and 7.\n    '''\n    # x_bounds should be sorted, so even for \"discrete_int\" type,\n    # the smallest and the largest number should the first and the last element\n    x_bounds_lowerbound = sum([x_bound[0] for x_bound in x_bounds])\n    x_bounds_upperbound = sum([x_bound[-1] for x_bound in x_bounds])\n\n    # return ((x_bounds_lowerbound <= lowerbound) and (x_bounds_upperbound >= lowerbound)) or \\\n    #        ((x_bounds_lowerbound <= upperbound) and (x_bounds_upperbound >= upperbound))\n    return (x_bounds_lowerbound <= lowerbound <= x_bounds_upperbound) or \\\n           (x_bounds_lowerbound <= upperbound <= x_bounds_upperbound)", "response": "Check feasibility of a given parameter range."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nkey idea is that we try to move towards upperbound, by randomly choose one value for each parameter. However, for the last parameter, we need to make sure that its value can help us get above lowerbound", "response": "def rand(x_bounds, x_types, lowerbound, upperbound, max_retries=100):\n    '''\n    Key idea is that we try to move towards upperbound, by randomly choose one\n    value for each parameter. However, for the last parameter,\n    we need to make sure that its value can help us get above lowerbound\n    '''\n    outputs = None\n\n    if check_feasibility(x_bounds, lowerbound, upperbound) is True:\n        # Order parameters by their range size. We want the smallest range first,\n        # because the corresponding parameter has less numbers to choose from\n        x_idx_sorted = []\n        for i, _ in enumerate(x_bounds):\n            if x_types[i] == \"discrete_int\":\n                x_idx_sorted.append([i, len(x_bounds[i])])\n            elif (x_types[i] == \"range_int\") or (x_types[i] == \"range_continuous\"):\n                x_idx_sorted.append([i, math.floor(x_bounds[i][1] - x_bounds[i][0])])\n        x_idx_sorted = sorted(x_idx_sorted, key=itemgetter(1))\n\n        for _ in range(max_retries):\n            budget_allocated = 0\n            outputs = [None] * len(x_bounds)\n\n            for i, _ in enumerate(x_idx_sorted):\n                x_idx = x_idx_sorted[i][0]\n                # The amount of unallocated space that we have\n                budget_max = upperbound - budget_allocated\n                # NOT the Last x that we need to assign a random number\n                if i < (len(x_idx_sorted) - 1):\n                    if x_bounds[x_idx][0] <= budget_max:\n                        if x_types[x_idx] == \"discrete_int\":\n                            # Note the valid integer\n                            temp = []\n                            for j in x_bounds[x_idx]:\n                                if j <= budget_max:\n                                    temp.append(j)\n                            # Randomly pick a number from the integer array\n                            if temp:\n                                outputs[x_idx] = temp[random.randint(0, len(temp) - 1)]\n\n                        elif (x_types[x_idx] == \"range_int\") or \\\n                                    (x_types[x_idx] == \"range_continuous\"):\n                            outputs[x_idx] = random.randint(x_bounds[x_idx][0],\n                                                            min(x_bounds[x_idx][-1], budget_max))\n\n                else:\n                    # The last x that we need to assign a random number\n                    randint_lowerbound = lowerbound - budget_allocated\n                    randint_lowerbound = 0 if randint_lowerbound < 0 else randint_lowerbound\n\n                    # This check:\n                    # is our smallest possible value going to overflow the available budget space,\n                    # and is our largest possible value going to underflow the lower bound\n                    if (x_bounds[x_idx][0] <= budget_max) and \\\n                            (x_bounds[x_idx][-1] >= randint_lowerbound):\n                        if x_types[x_idx] == \"discrete_int\":\n                            temp = []\n                            for j in x_bounds[x_idx]:\n                                # if (j <= budget_max) and (j >= randint_lowerbound):\n                                if randint_lowerbound <= j <= budget_max:\n                                    temp.append(j)\n                            if temp:\n                                outputs[x_idx] = temp[random.randint(0, len(temp) - 1)]\n                        elif (x_types[x_idx] == \"range_int\") or \\\n                                (x_types[x_idx] == \"range_continuous\"):\n                            outputs[x_idx] = random.randint(randint_lowerbound,\n                                                            min(x_bounds[x_idx][1], budget_max))\n                if outputs[x_idx] is None:\n                    break\n                else:\n                    budget_allocated += outputs[x_idx]\n            if None not in outputs:\n                break\n    return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expand_path(experiment_config, key):\n    '''Change '~' to user home directory'''\n    if experiment_config.get(key):\n        experiment_config[key] = os.path.expanduser(experiment_config[key])", "response": "Change ~ to user home directory"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange relative path to absolute path", "response": "def parse_relative_path(root_path, experiment_config, key):\n    '''Change relative path to absolute path'''\n    if experiment_config.get(key) and not os.path.isabs(experiment_config.get(key)):\n        absolute_path = os.path.join(root_path, experiment_config.get(key))\n        print_normal('expand %s: %s to %s ' % (key, experiment_config[key], absolute_path))\n        experiment_config[key] = absolute_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_time(time):\n    '''Change the time to seconds'''\n    unit = time[-1]\n    if unit not in ['s', 'm', 'h', 'd']:\n        print_error('the unit of time could only from {s, m, h, d}')\n        exit(1)\n    time = time[:-1]\n    if not time.isdigit():\n        print_error('time format error!')\n        exit(1)\n    parse_dict = {'s':1, 'm':60, 'h':3600, 'd':86400}\n    return int(time) * parse_dict[unit]", "response": "Change the time to seconds"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse path in config file", "response": "def parse_path(experiment_config, config_path):\n    '''Parse path in config file'''\n    expand_path(experiment_config, 'searchSpacePath')\n    if experiment_config.get('trial'):\n        expand_path(experiment_config['trial'], 'codeDir')\n    if experiment_config.get('tuner'):\n        expand_path(experiment_config['tuner'], 'codeDir')\n    if experiment_config.get('assessor'):\n        expand_path(experiment_config['assessor'], 'codeDir')\n    if experiment_config.get('advisor'):\n        expand_path(experiment_config['advisor'], 'codeDir')\n    \n    #if users use relative path, convert it to absolute path\n    root_path = os.path.dirname(config_path)\n    if experiment_config.get('searchSpacePath'):\n        parse_relative_path(root_path, experiment_config, 'searchSpacePath')\n    if experiment_config.get('trial'):\n        parse_relative_path(root_path, experiment_config['trial'], 'codeDir')\n    if experiment_config.get('tuner'):\n        parse_relative_path(root_path, experiment_config['tuner'], 'codeDir')\n    if experiment_config.get('assessor'):\n        parse_relative_path(root_path, experiment_config['assessor'], 'codeDir')\n    if experiment_config.get('advisor'):\n        parse_relative_path(root_path, experiment_config['advisor'], 'codeDir')\n    if experiment_config.get('machineList'):\n        for index in range(len(experiment_config['machineList'])):\n            parse_relative_path(root_path, experiment_config['machineList'][index], 'sshKeyPath')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates searchspace content, if the searchspace file is not json format or its values does not contain _type and _value which must be specified, it will not be a valid searchspace file", "response": "def validate_search_space_content(experiment_config):\n    '''Validate searchspace content, \n       if the searchspace file is not json format or its values does not contain _type and _value which must be specified, \n       it will not be a valid searchspace file'''\n    try:\n        search_space_content = json.load(open(experiment_config.get('searchSpacePath'), 'r'))\n        for value in search_space_content.values():\n            if not value.get('_type') or not value.get('_value'):\n                print_error('please use _type and _value to specify searchspace!')\n                exit(1)\n    except:\n        print_error('searchspace file is not a valid json format!')\n        exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_kubeflow_operators(experiment_config):\n    '''Validate whether the kubeflow operators are valid'''\n    if experiment_config.get('kubeflowConfig'):\n        if experiment_config.get('kubeflowConfig').get('operator') == 'tf-operator':\n            if experiment_config.get('trial').get('master') is not None:\n                print_error('kubeflow with tf-operator can not set master')\n                exit(1)\n            if experiment_config.get('trial').get('worker') is None:\n                print_error('kubeflow with tf-operator must set worker')\n                exit(1)\n        elif experiment_config.get('kubeflowConfig').get('operator') == 'pytorch-operator':\n            if experiment_config.get('trial').get('ps') is not None:\n                print_error('kubeflow with pytorch-operator can not set ps')\n                exit(1)\n            if experiment_config.get('trial').get('master') is None:\n                print_error('kubeflow with pytorch-operator must set master')\n                exit(1)\n        \n        if experiment_config.get('kubeflowConfig').get('storage') == 'nfs':\n            if experiment_config.get('kubeflowConfig').get('nfs') is None:\n                print_error('please set nfs configuration!')\n                exit(1)\n        elif experiment_config.get('kubeflowConfig').get('storage') == 'azureStorage':\n            if experiment_config.get('kubeflowConfig').get('azureStorage') is None:\n                print_error('please set azureStorage configuration!')\n                exit(1)\n        elif experiment_config.get('kubeflowConfig').get('storage') is None:\n            if experiment_config.get('kubeflowConfig').get('azureStorage'):\n                print_error('please set storage type!')\n                exit(1)", "response": "Validate whether the kubeflow operators are valid"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_common_content(experiment_config):\n    '''Validate whether the common values in experiment_config is valid'''\n    if not experiment_config.get('trainingServicePlatform') or \\\n        experiment_config.get('trainingServicePlatform') not in ['local', 'remote', 'pai', 'kubeflow', 'frameworkcontroller']:\n        print_error('Please set correct trainingServicePlatform!')\n        exit(1)\n    schema_dict = {\n            'local': LOCAL_CONFIG_SCHEMA,\n            'remote': REMOTE_CONFIG_SCHEMA,\n            'pai': PAI_CONFIG_SCHEMA,\n            'kubeflow': KUBEFLOW_CONFIG_SCHEMA,\n            'frameworkcontroller': FRAMEWORKCONTROLLER_CONFIG_SCHEMA\n        }\n    separate_schema_dict = {\n        'tuner': tuner_schema_dict,\n        'advisor': advisor_schema_dict,\n        'assessor': assessor_schema_dict\n    }\n    separate_builtInName_dict = {\n        'tuner': 'builtinTunerName',\n        'advisor': 'builtinAdvisorName',\n        'assessor': 'builtinAssessorName'\n    }\n    try:\n        schema_dict.get(experiment_config['trainingServicePlatform']).validate(experiment_config)\n        for separate_key in separate_schema_dict.keys():\n            if experiment_config.get(separate_key):\n                if experiment_config[separate_key].get(separate_builtInName_dict[separate_key]):\n                    validate = False\n                    for key in separate_schema_dict[separate_key].keys():\n                        if key.__contains__(experiment_config[separate_key][separate_builtInName_dict[separate_key]]):\n                            Schema({**separate_schema_dict[separate_key][key]}).validate(experiment_config[separate_key])\n                            validate = True\n                            break\n                    if not validate:\n                        print_error('%s %s error!' % (separate_key, separate_builtInName_dict[separate_key]))\n                        exit(1)\n                else:\n                    Schema({**separate_schema_dict[separate_key]['customized']}).validate(experiment_config[separate_key])\n    except SchemaError as error:\n        print_error('Your config file is not correct, please check your config file content!')\n        if error.__str__().__contains__('Wrong key'):\n            print_error(' '.join(error.__str__().split()[:3]))\n        else:\n            print_error(error)\n        exit(1)\n    \n    #set default value\n    if experiment_config.get('maxExecDuration') is None:\n        experiment_config['maxExecDuration'] = '999d'\n    if experiment_config.get('maxTrialNum') is None:\n        experiment_config['maxTrialNum'] = 99999\n    if experiment_config['trainingServicePlatform'] == 'remote':\n        for index in range(len(experiment_config['machineList'])):\n            if experiment_config['machineList'][index].get('port') is None:\n                experiment_config['machineList'][index]['port'] = 22", "response": "Validate whether the common content in experiment_config is valid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether the file of customized tuner assessor or advisor exists ArcGIS", "response": "def validate_customized_file(experiment_config, spec_key):\n    '''\n    check whether the file of customized tuner/assessor/advisor exists\n    spec_key: 'tuner', 'assessor', 'advisor'\n    '''\n    if experiment_config[spec_key].get('codeDir') and \\\n        experiment_config[spec_key].get('classFileName') and \\\n        experiment_config[spec_key].get('className'):\n        if not os.path.exists(os.path.join(\n                experiment_config[spec_key]['codeDir'],\n                experiment_config[spec_key]['classFileName'])):\n            print_error('%s file directory is not valid!'%(spec_key))\n            exit(1)\n    else:\n        print_error('%s file directory is not valid!'%(spec_key))\n        exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates whether assessor in experiment_config is valid", "response": "def parse_assessor_content(experiment_config):\n    '''Validate whether assessor in experiment_config is valid'''\n    if experiment_config.get('assessor'):\n        if experiment_config['assessor'].get('builtinAssessorName'):\n            experiment_config['assessor']['className'] = experiment_config['assessor']['builtinAssessorName']\n        else:\n            validate_customized_file(experiment_config, 'assessor')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_pai_trial_conifg(experiment_config):\n    '''validate the trial config in pai platform'''\n    if experiment_config.get('trainingServicePlatform') == 'pai':\n        if experiment_config.get('trial').get('shmMB') and \\\n        experiment_config['trial']['shmMB'] > experiment_config['trial']['memoryMB']:\n            print_error('shmMB should be no more than memoryMB!')\n            exit(1)", "response": "validate the trial config in pai platform"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates whether all content in the experiment_config is valid.", "response": "def validate_all_content(experiment_config, config_path):\n    '''Validate whether experiment_config is valid'''\n    parse_path(experiment_config, config_path)\n    validate_common_content(experiment_config)\n    validate_pai_trial_conifg(experiment_config)\n    experiment_config['maxExecDuration'] = parse_time(experiment_config['maxExecDuration'])\n    if experiment_config.get('advisor'):\n        if experiment_config.get('assessor') or experiment_config.get('tuner'):\n            print_error('advisor could not be set with assessor or tuner simultaneously!')\n            exit(1)\n        parse_advisor_content(experiment_config)\n        validate_annotation_content(experiment_config, 'advisor', 'builtinAdvisorName')\n    else:\n        if not experiment_config.get('tuner'):\n            raise Exception('Please provide tuner spec!')\n        parse_tuner_content(experiment_config)\n        parse_assessor_content(experiment_config)\n        validate_annotation_content(experiment_config, 'tuner', 'builtinTunerName')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets urls of local machine", "response": "def get_local_urls(port):\n    '''get urls of local machine'''\n    url_list = []\n    for name, info in psutil.net_if_addrs().items():\n        for addr in info:\n            if AddressFamily.AF_INET == addr.family:\n                url_list.append('http://{}:{}'.format(addr.address, port))\n    return url_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_annotation(code):\n    module = ast.parse(code)\n    assert type(module) is ast.Module, 'internal error #1'\n    assert len(module.body) == 1, 'Annotation contains more than one expression'\n    assert type(module.body[0]) is ast.Expr, 'Annotation is not expression'\n    return module.body[0]", "response": "Parse an annotation string. Returns an AST Expr node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing an annotation function.", "response": "def parse_annotation_function(code, func_name):\n    \"\"\"Parse an annotation function.\n    Return the value of `name` keyword argument and the AST Call node.\n    func_name: expected function name\n    \"\"\"\n    expr = parse_annotation(code)\n    call = expr.value\n    assert type(call) is ast.Call, 'Annotation is not a function call'\n\n    assert type(call.func) is ast.Attribute, 'Unexpected annotation function'\n    assert type(call.func.value) is ast.Name, 'Invalid annotation function name'\n    assert call.func.value.id == 'nni', 'Annotation is not a NNI function'\n    assert call.func.attr == func_name, 'internal error #2'\n\n    assert len(call.keywords) == 1, 'Annotation function contains more than one keyword argument'\n    assert call.keywords[0].arg == 'name', 'Annotation keyword argument is not \"name\"'\n    name = call.keywords[0].value\n\n    return name, call"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a NNI variable expression.", "response": "def parse_nni_variable(code):\n    \"\"\"Parse `nni.variable` expression.\n    Return the name argument and AST node of annotated expression.\n    code: annotation string\n    \"\"\"\n    name, call = parse_annotation_function(code, 'variable')\n\n    assert len(call.args) == 1, 'nni.variable contains more than one arguments'\n    arg = call.args[0]\n    assert type(arg) is ast.Call, 'Value of nni.variable is not a function call'\n    assert type(arg.func) is ast.Attribute, 'nni.variable value is not a NNI function'\n    assert type(arg.func.value) is ast.Name, 'nni.variable value is not a NNI function'\n    assert arg.func.value.id == 'nni', 'nni.variable value is not a NNI function'\n\n    name_str = astor.to_source(name).strip()\n    keyword_arg = ast.keyword(arg='name', value=ast.Str(s=name_str))\n    arg.keywords.append(keyword_arg)\n    if arg.func.attr == 'choice':\n        convert_args_to_dict(arg)\n\n    return name, arg"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse nni. function_choice expression. Return the AST node of annotated expression and a list of dumped function call expressions.", "response": "def parse_nni_function(code):\n    \"\"\"Parse `nni.function_choice` expression.\n    Return the AST node of annotated expression and a list of dumped function call expressions.\n    code: annotation string\n    \"\"\"\n    name, call = parse_annotation_function(code, 'function_choice')\n    funcs = [ast.dump(func, False) for func in call.args]\n    convert_args_to_dict(call, with_lambda=True)\n\n    name_str = astor.to_source(name).strip()\n    call.keywords[0].value = ast.Str(s=name_str)\n\n    return call, funcs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_args_to_dict(call, with_lambda=False):\n    keys, values = list(), list()\n    for arg in call.args:\n        if type(arg) in [ast.Str, ast.Num]:\n            arg_value = arg\n        else:\n        # if arg is not a string or a number, we use its source code as the key\n            arg_value = astor.to_source(arg).strip('\\n\"')\n            arg_value = ast.Str(str(arg_value))\n        arg = make_lambda(arg) if with_lambda else arg\n        keys.append(arg_value)\n        values.append(arg)\n    del call.args[:]\n    call.args.append(ast.Dict(keys=keys, values=values))\n\n    return call", "response": "Convert all args to a dict such that every key and value in the dict is the same as the value of the arg."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps an AST Call node to lambda expression node.", "response": "def make_lambda(call):\n    \"\"\"Wrap an AST Call node to lambda expression node.\n    call: ast.Call node\n    \"\"\"\n    empty_args = ast.arguments(args=[], vararg=None, kwarg=None, defaults=[])\n    return ast.Lambda(args=empty_args, body=call)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreplacing a node annotated by nni. variable.", "response": "def replace_variable_node(node, annotation):\n    \"\"\"Replace a node annotated by `nni.variable`.\n    node: the AST node to replace\n    annotation: annotation string\n    \"\"\"\n    assert type(node) is ast.Assign, 'nni.variable is not annotating assignment expression'\n    assert len(node.targets) == 1, 'Annotated assignment has more than one left-hand value'\n    name, expr = parse_nni_variable(annotation)\n    assert test_variable_equal(node.targets[0], name), 'Annotated variable has wrong name'\n    node.value = expr\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef replace_function_node(node, annotation):\n    target, funcs = parse_nni_function(annotation)\n    FuncReplacer(funcs, target).visit(node)\n    return node", "response": "Replace a node annotated by nni. function_choice."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nannotating user code. Return annotated code if not.", "response": "def parse(code):\n    \"\"\"Annotate user code.\n    Return annotated code (str) if annotation detected; return None if not.\n    code: original user code (str)\n    \"\"\"\n    try:\n        ast_tree = ast.parse(code)\n    except Exception:\n        raise RuntimeError('Bad Python code')\n\n    transformer = Transformer()\n    try:\n        transformer.visit(ast_tree)\n    except AssertionError as exc:\n        raise RuntimeError('%d: %s' % (ast_tree.last_line, exc.args[0]))\n\n    if not transformer.annotated:\n        return None\n\n    last_future_import = -1\n    import_nni = ast.Import(names=[ast.alias(name='nni', asname=None)])\n    nodes = ast_tree.body\n    for i, _ in enumerate(nodes):\n        if type(nodes[i]) is ast.ImportFrom and nodes[i].module == '__future__':\n            last_future_import = i\n    nodes.insert(last_future_import + 1, import_nni)\n\n    return astor.to_source(ast_tree)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload yaml file content", "response": "def get_yml_content(file_path):\n    '''Load yaml file content'''\n    try:\n        with open(file_path, 'r') as file:\n            return yaml.load(file, Loader=yaml.Loader)\n    except yaml.scanner.ScannerError as err:\n        print_error('yaml file format error!')\n        exit(1)\n    except Exception as exception:\n        print_error(exception)\n        exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef detect_port(port):\n    '''Detect if the port is used'''\n    socket_test = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n    try:\n        socket_test.connect(('127.0.0.1', int(port)))\n        socket_test.close()\n        return True\n    except:\n        return False", "response": "Detect if the port is used"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_model(samples_x, samples_y_aggregation, percentage_goodbatch=0.34):\n    '''\n    Create the Gaussian Mixture Model\n    '''\n    samples = [samples_x[i] + [samples_y_aggregation[i]] for i in range(0, len(samples_x))]\n\n    # Sorts so that we can get the top samples\n    samples = sorted(samples, key=itemgetter(-1))\n    samples_goodbatch_size = int(len(samples) * percentage_goodbatch)\n    samples_goodbatch = samples[0:samples_goodbatch_size]\n    samples_badbatch = samples[samples_goodbatch_size:]\n\n    samples_x_goodbatch = [sample_goodbatch[0:-1] for sample_goodbatch in samples_goodbatch]\n    #samples_y_goodbatch = [sample_goodbatch[-1] for sample_goodbatch in samples_goodbatch]\n    samples_x_badbatch = [sample_badbatch[0:-1] for sample_badbatch in samples_badbatch]\n\n    # === Trains GMM clustering models === #\n    #sys.stderr.write(\"[%s] Train GMM's GMM model\\n\" % (os.path.basename(__file__)))\n    bgmm_goodbatch = mm.BayesianGaussianMixture(n_components=max(1, samples_goodbatch_size - 1))\n    bad_n_components = max(1, len(samples_x) - samples_goodbatch_size - 1)\n    bgmm_badbatch = mm.BayesianGaussianMixture(n_components=bad_n_components)\n    bgmm_goodbatch.fit(samples_x_goodbatch)\n    bgmm_badbatch.fit(samples_x_badbatch)\n\n    model = {}\n    model['clusteringmodel_good'] = bgmm_goodbatch\n    model['clusteringmodel_bad'] = bgmm_badbatch\n    return model", "response": "Create the Gaussian Mixture Model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef report_intermediate_result(metric):\n    global _intermediate_seq\n    assert _params is not None, 'nni.get_next_parameter() needs to be called before report_intermediate_result'\n    metric = json_tricks.dumps({\n        'parameter_id': _params['parameter_id'],\n        'trial_job_id': trial_env_vars.NNI_TRIAL_JOB_ID,\n        'type': 'PERIODICAL',\n        'sequence': _intermediate_seq,\n        'value': metric\n    })\n    _intermediate_seq += 1\n    platform.send_metric(metric)", "response": "Reports intermediate result to Assessor.\n    metric"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreport final result to tuner. metric", "response": "def report_final_result(metric):\n    \"\"\"Reports final result to tuner.\n    metric: serializable object.\n    \"\"\"\n    assert _params is not None, 'nni.get_next_parameter() needs to be called before report_final_result'\n    metric = json_tricks.dumps({\n        'parameter_id': _params['parameter_id'],\n        'trial_job_id': trial_env_vars.NNI_TRIAL_JOB_ID,\n        'type': 'FINAL',\n        'sequence': 0,  # TODO: may be unnecessary\n        'value': metric\n    })\n    platform.send_metric(metric)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_args():\n    parser = argparse.ArgumentParser(\"FashionMNIST\")\n    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size\")\n    parser.add_argument(\"--optimizer\", type=str, default=\"SGD\", help=\"optimizer\")\n    parser.add_argument(\"--epochs\", type=int, default=200, help=\"epoch limit\")\n    parser.add_argument(\n        \"--learning_rate\", type=float, default=0.001, help=\"learning rate\"\n    )\n    parser.add_argument(\"--cutout\", action=\"store_true\", default=False, help=\"use cutout\")\n    parser.add_argument(\"--cutout_length\", type=int, default=8, help=\"cutout length\")\n    parser.add_argument(\n        \"--model_path\", type=str, default=\"./\", help=\"Path to save the destination model\"\n    )\n    return parser.parse_args()", "response": "get args from command line\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a model from json representation", "response": "def build_graph_from_json(ir_model_json):\n    \"\"\"build model from json representation\n    \"\"\"\n    graph = json_to_graph(ir_model_json)\n    logging.debug(graph.operation_history)\n    model = graph.produce_torch_model()\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_rev_args(receive_msg):\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    # Loading Data\n    logger.debug(\"Preparing data..\")\n\n    raw_train_data = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=True, download=True\n    )\n\n    dataset_mean, dataset_std = (\n        [raw_train_data.train_data.float().mean() / 255],\n        [raw_train_data.train_data.float().std() / 255],\n    )\n\n    transform_train, transform_test = utils.data_transforms_mnist(\n        args, dataset_mean, dataset_std\n    )\n\n    trainset = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=True, download=True, transform=transform_train\n    )\n    trainloader = torch.utils.data.DataLoader(\n        trainset, batch_size=args.batch_size, shuffle=True, num_workers=2\n    )\n\n    testset = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=False, download=True, transform=transform_test\n    )\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=args.batch_size, shuffle=False, num_workers=2\n    )\n\n    # Model\n    logger.debug(\"Building model..\")\n    net = build_graph_from_json(receive_msg)\n\n    net = net.to(device)\n    criterion = nn.CrossEntropyLoss()\n\n    if args.optimizer == \"SGD\":\n        optimizer = optim.SGD(\n            net.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=5e-4\n        )\n    if args.optimizer == \"Adadelta\":\n        optimizer = optim.Adadelta(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == \"Adagrad\":\n        optimizer = optim.Adagrad(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == \"Adam\":\n        optimizer = optim.Adam(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == \"Adamax\":\n        optimizer = optim.Adamax(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == \"RMSprop\":\n        optimizer = optim.RMSprop(net.parameters(), lr=args.learning_rate)\n\n    return 0", "response": "parse the reveive msgs to global variable\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntraining model on each epoch in trainset", "response": "def train(epoch):\n    \"\"\" train model on each epoch in trainset\n    \"\"\"\n\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    logger.debug(\"Epoch: %d\", epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        acc = 100.0 * correct / total\n\n        logger.debug(\n            \"Loss: %.3f | Acc: %.3f%% (%d/%d)\",\n            train_loss / (batch_idx + 1),\n            100.0 * correct / total,\n            correct,\n            total,\n        )\n\n    return acc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the reveive msgs to global variable", "response": "def parse_rev_args(receive_msg):\n    \"\"\" parse reveive msgs to global variable\n    \"\"\"\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    # Loading Data\n    logger.debug(\"Preparing data..\")\n\n    transform_train, transform_test = utils.data_transforms_cifar10(args)\n\n    trainset = torchvision.datasets.CIFAR10(\n        root=\"./data\", train=True, download=True, transform=transform_train\n    )\n    trainloader = torch.utils.data.DataLoader(\n        trainset, batch_size=args.batch_size, shuffle=True, num_workers=2\n    )\n\n    testset = torchvision.datasets.CIFAR10(\n        root=\"./data\", train=False, download=True, transform=transform_test\n    )\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=args.batch_size, shuffle=False, num_workers=2\n    )\n\n    # Model\n    logger.debug(\"Building model..\")\n    net = build_graph_from_json(receive_msg)\n\n    net = net.to(device)\n    criterion = nn.CrossEntropyLoss()\n    if device == \"cuda\" and torch.cuda.device_count() > 1:\n        net = torch.nn.DataParallel(net)\n\n    if args.optimizer == \"SGD\":\n        optimizer = optim.SGD(\n            net.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=5e-4\n        )\n    if args.optimizer == \"Adadelta\":\n        optimizer = optim.Adadelta(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == \"Adagrad\":\n        optimizer = optim.Adagrad(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == \"Adam\":\n        optimizer = optim.Adam(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == \"Adamax\":\n        optimizer = optim.Adamax(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == \"RMSprop\":\n        optimizer = optim.RMSprop(net.parameters(), lr=args.learning_rate)\n\n\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare(self, config_file=None, user=None, password=None, **kwargs):\n        if config_file is not None:\n            self.read_config(config_file)\n        else:\n            self._prepare_account(user, password, **kwargs)\n        self.autologin()", "response": "prepare a new instance of the class"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef autologin(self, limit=10):\n        for _ in range(limit):\n            if self.login():\n                break\n        else:\n            raise exceptions.NotLoginError(\n                \"\u767b\u5f55\u5931\u8d25\u6b21\u6570\u8fc7\u591a, \u8bf7\u68c0\u67e5\u5bc6\u7801\u662f\u5426\u6b63\u786e / \u5238\u5546\u670d\u52a1\u5668\u662f\u5426\u5904\u4e8e\u7ef4\u62a4\u4e2d / \u7f51\u7edc\u8fde\u63a5\u662f\u5426\u6b63\u5e38\"\n            )\n        self.keepalive()", "response": "autologin - Automatically logs in."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __read_config(self):\n        self.config = helpers.file2dict(self.config_path)\n        self.global_config = helpers.file2dict(self.global_config_path)\n        self.config.update(self.global_config)", "response": "read config from file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do(self, params):\n        request_params = self.create_basic_params()\n        request_params.update(params)\n        response_data = self.request(request_params)\n        try:\n            format_json_data = self.format_response_data(response_data)\n        # pylint: disable=broad-except\n        except Exception:\n            # Caused by server force logged out\n            return None\n        return_data = self.fix_error_data(format_json_data)\n        try:\n            self.check_login_status(return_data)\n        except exceptions.NotLoginError:\n            self.autologin()\n        return return_data", "response": "method performs the API call"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat the response data type of the resource.", "response": "def format_response_data_type(self, response_data):\n        \"\"\"\u683c\u5f0f\u5316\u8fd4\u56de\u7684\u503c\u4e3a\u6b63\u786e\u7684\u7c7b\u578b\n        :param response_data: \u8fd4\u56de\u7684\u6570\u636e\n        \"\"\"\n        if isinstance(response_data, list) and not isinstance(\n            response_data, str\n        ):\n            return response_data\n\n        int_match_str = \"|\".join(self.config[\"response_format\"][\"int\"])\n        float_match_str = \"|\".join(self.config[\"response_format\"][\"float\"])\n        for item in response_data:\n            for key in item:\n                try:\n                    if re.search(int_match_str, key) is not None:\n                        item[key] = helpers.str2num(item[key], \"int\")\n                    elif re.search(float_match_str, key) is not None:\n                        item[key] = helpers.str2num(item[key], \"float\")\n                except ValueError:\n                    continue\n        return response_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare a new entry in the cache.", "response": "def prepare(\n        self,\n        config_path=None,\n        user=None,\n        password=None,\n        exe_path=None,\n        comm_password=None,\n        **kwargs\n    ):\n        \"\"\"\n        \u767b\u9646\u5ba2\u6237\u7aef\n        :param config_path: \u767b\u9646\u914d\u7f6e\u6587\u4ef6\uff0c\u8ddf\u53c2\u6570\u767b\u9646\u65b9\u5f0f\u4e8c\u9009\u4e00\n        :param user: \u8d26\u53f7\n        :param password: \u660e\u6587\u5bc6\u7801\n        :param exe_path: \u5ba2\u6237\u7aef\u8def\u5f84\u7c7b\u4f3c r'C:\\\\htzqzyb2\\\\xiadan.exe', \u9ed8\u8ba4 r'C:\\\\htzqzyb2\\\\xiadan.exe'\n        :param comm_password: \u901a\u8baf\u5bc6\u7801\n        :return:\n        \"\"\"\n        params = locals().copy()\n        params.pop(\"self\")\n\n        if config_path is not None:\n            account = helpers.file2dict(config_path)\n            params[\"user\"] = account[\"user\"]\n            params[\"password\"] = account[\"password\"]\n\n        params[\"broker\"] = self._broker\n\n        response = self._s.post(self._api + \"/prepare\", json=params)\n        if response.status_code >= 300:\n            raise Exception(response.json()[\"error\"])\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfollow - \u8ddf\u8e2aricequant \u5bf9\u5e94\u7684\u6a21\u62df\u4ea4\u6613\uff0c \u652f\u6301easytrader \u7684 \u6a21\u62df\u4ea4\u6613ID", "response": "def follow(\n        self,\n        users,\n        run_id,\n        track_interval=1,\n        trade_cmd_expire_seconds=120,\n        cmd_cache=True,\n        entrust_prop=\"limit\",\n        send_interval=0,\n    ):\n        \"\"\"\u8ddf\u8e2aricequant\u5bf9\u5e94\u7684\u6a21\u62df\u4ea4\u6613\uff0c\u652f\u6301\u591a\u7528\u6237\u591a\u7b56\u7565\n        :param users: \u652f\u6301easytrader\u7684\u7528\u6237\u5bf9\u8c61\uff0c\u652f\u6301\u4f7f\u7528 [] \u6307\u5b9a\u591a\u4e2a\u7528\u6237\n        :param run_id: ricequant \u7684\u6a21\u62df\u4ea4\u6613ID\uff0c\u652f\u6301\u4f7f\u7528 [] \u6307\u5b9a\u591a\u4e2a\u6a21\u62df\u4ea4\u6613\n        :param track_interval: \u8f6e\u8bad\u6a21\u62df\u4ea4\u6613\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2\n        :param trade_cmd_expire_seconds: \u4ea4\u6613\u6307\u4ee4\u8fc7\u671f\u65f6\u95f4, \u5355\u4f4d\u4e3a\u79d2\n        :param cmd_cache: \u662f\u5426\u8bfb\u53d6\u5b58\u50a8\u5386\u53f2\u6267\u884c\u8fc7\u7684\u6307\u4ee4\uff0c\u9632\u6b62\u91cd\u542f\u65f6\u91cd\u590d\u6267\u884c\u5df2\u7ecf\u4ea4\u6613\u8fc7\u7684\u6307\u4ee4\n        :param entrust_prop: \u59d4\u6258\u65b9\u5f0f, 'limit' \u4e3a\u9650\u4ef7\uff0c'market' \u4e3a\u5e02\u4ef7, \u4ec5\u5728\u94f6\u6cb3\u5b9e\u73b0\n        :param send_interval: \u4ea4\u6613\u53d1\u9001\u95f4\u9694\uff0c \u9ed8\u8ba4\u4e3a0s\u3002\u8c03\u5927\u53ef\u9632\u6b62\u5356\u51fa\u4e70\u5165\u65f6\u5356\u51fa\u5355\u6ca1\u6709\u53ca\u65f6\u6210\u4ea4\u5bfc\u81f4\u7684\u4e70\u5165\u91d1\u989d\u4e0d\u8db3\n        \"\"\"\n        users = self.warp_list(users)\n        run_ids = self.warp_list(run_id)\n\n        if cmd_cache:\n            self.load_expired_cmd_cache()\n\n        self.start_trader_thread(\n            users, trade_cmd_expire_seconds, entrust_prop, send_interval\n        )\n\n        workers = []\n        for id_ in run_ids:\n            strategy_name = self.extract_strategy_name(id_)\n            strategy_worker = Thread(\n                target=self.track_strategy_worker,\n                args=[id_, strategy_name],\n                kwargs={\"interval\": track_interval},\n            )\n            strategy_worker.start()\n            workers.append(strategy_worker)\n            log.info(\"\u5f00\u59cb\u8ddf\u8e2a\u7b56\u7565: %s\", strategy_name)\n        for worker in workers:\n            worker.join()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlogin to the AIKIF server.", "response": "def login(self, user, password, exe_path, comm_password=None, **kwargs):\n        \"\"\"\n        \u767b\u9646\u5ba2\u6237\u7aef\n\n        :param user: \u8d26\u53f7\n        :param password: \u660e\u6587\u5bc6\u7801\n        :param exe_path: \u5ba2\u6237\u7aef\u8def\u5f84\u7c7b\u4f3c 'C:\\\\\u4e2d\u56fd\u94f6\u6cb3\u8bc1\u5238\u53cc\u5b50\u661f3.2\\\\Binarystar.exe',\n            \u9ed8\u8ba4 'C:\\\\\u4e2d\u56fd\u94f6\u6cb3\u8bc1\u5238\u53cc\u5b50\u661f3.2\\\\Binarystar.exe'\n        :param comm_password: \u901a\u8baf\u5bc6\u7801, \u534e\u6cf0\u9700\u8981\uff0c\u53ef\u4e0d\u8bbe\n        :return:\n        \"\"\"\n        try:\n            self._app = pywinauto.Application().connect(\n                path=self._run_exe_path(exe_path), timeout=1\n            )\n        # pylint: disable=broad-except\n        except Exception:\n            self._app = pywinauto.Application().start(exe_path)\n\n            # wait login window ready\n            while True:\n                try:\n                    self._app.top_window().Edit1.wait(\"ready\")\n                    break\n                except RuntimeError:\n                    pass\n\n            self._app.top_window().Edit1.type_keys(user)\n            self._app.top_window().Edit2.type_keys(password)\n            edit3 = self._app.top_window().window(control_id=0x3eb)\n            while True:\n                try:\n                    code = self._handle_verify_code()\n                    edit3.type_keys(code)\n                    time.sleep(1)\n                    self._app.top_window()[\"\u786e\u5b9a(Y)\"].click()\n                    # detect login is success or not\n                    try:\n                        self._app.top_window().wait_not(\"exists\", 5)\n                        break\n\n                    # pylint: disable=broad-except\n                    except Exception:\n                        self._app.top_window()[\"\u786e\u5b9a\"].click()\n\n                # pylint: disable=broad-except\n                except Exception:\n                    pass\n\n            self._app = pywinauto.Application().connect(\n                path=self._run_exe_path(exe_path), timeout=10\n            )\n        self._main = self._app.window(title=\"\u7f51\u4e0a\u80a1\u7968\u4ea4\u6613\u7cfb\u7edf5.0\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef login(self, user, password, exe_path, comm_password=None, **kwargs):\r\n        if comm_password is None:\r\n            raise ValueError(\"\u534e\u6cf0\u5fc5\u987b\u8bbe\u7f6e\u901a\u8baf\u5bc6\u7801\")\r\n\r\n        try:\r\n            self._app = pywinauto.Application().connect(\r\n                path=self._run_exe_path(exe_path), timeout=1\r\n            )\r\n        # pylint: disable=broad-except\r\n        except Exception:\r\n            self._app = pywinauto.Application().start(exe_path)\r\n\r\n            # wait login window ready\r\n            while True:\r\n                try:\r\n                    self._app.top_window().Edit1.wait(\"ready\")\r\n                    break\r\n                except RuntimeError:\r\n                    pass\r\n\r\n            self._app.top_window().Edit1.type_keys(user)\r\n            self._app.top_window().Edit2.type_keys(password)\r\n\r\n            self._app.top_window().Edit3.type_keys(comm_password)\r\n\r\n            self._app.top_window().button0.click()\r\n\r\n            # detect login is success or not\r\n            self._app.top_window().wait_not(\"exists\", 10)\r\n\r\n            self._app = pywinauto.Application().connect(\r\n                path=self._run_exe_path(exe_path), timeout=10\r\n            )\r\n        self._close_prompt_windows()\r\n        self._main = self._app.window(title=\"\u7f51\u4e0a\u80a1\u7968\u4ea4\u6613\u7cfb\u7edf5.0\")", "response": "Login to the specified user and password."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef login(self, user=None, password=None, **kwargs):\n        cookies = kwargs.get('cookies')\n        if cookies is None:\n            raise TypeError('\u96ea\u7403\u767b\u9646\u9700\u8981\u8bbe\u7f6e cookies\uff0c \u5177\u4f53\u89c1'\n                            'https://smalltool.github.io/2016/08/02/cookie/')\n        headers = self._generate_headers()\n        self.s.headers.update(headers)\n\n        self.s.get(self.LOGIN_PAGE)\n\n        cookie_dict = helpers.parse_cookies_str(cookies)\n        self.s.cookies.update(cookie_dict)\n\n        log.info('\u767b\u5f55\u6210\u529f')", "response": "Login to the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfollow a set of users and strategies.", "response": "def follow(  # type: ignore\n            self,\n            users,\n            strategies,\n            total_assets=10000,\n            initial_assets=None,\n            adjust_sell=False,\n            track_interval=10,\n            trade_cmd_expire_seconds=120,\n            cmd_cache=True,\n            slippage: float = 0.0):\n        \"\"\"\u8ddf\u8e2a joinquant \u5bf9\u5e94\u7684\u6a21\u62df\u4ea4\u6613\uff0c\u652f\u6301\u591a\u7528\u6237\u591a\u7b56\u7565\n        :param users: \u652f\u6301 easytrader \u7684\u7528\u6237\u5bf9\u8c61\uff0c\u652f\u6301\u4f7f\u7528 [] \u6307\u5b9a\u591a\u4e2a\u7528\u6237\n        :param strategies: \u96ea\u7403\u7ec4\u5408\u540d, \u7c7b\u4f3c ZH123450\n        :param total_assets: \u96ea\u7403\u7ec4\u5408\u5bf9\u5e94\u7684\u603b\u8d44\u4ea7\uff0c \u683c\u5f0f [\u7ec4\u54081\u5bf9\u5e94\u8d44\u91d1, \u7ec4\u54082\u5bf9\u5e94\u8d44\u91d1]\n            \u82e5 strategies=['ZH000001', 'ZH000002'],\n                \u8bbe\u7f6e total_assets=[10000, 10000], \u5219\u8868\u660e\u6bcf\u4e2a\u7ec4\u5408\u5bf9\u5e94\u7684\u8d44\u4ea7\u4e3a 1w \u5143\n            \u5047\u8bbe\u7ec4\u5408 ZH000001 \u52a0\u4ed3 \u4ef7\u683c\u4e3a p \u80a1\u7968 A 10%,\n                \u5219\u5bf9\u5e94\u7684\u4ea4\u6613\u6307\u4ee4\u4e3a \u4e70\u5165 \u80a1\u7968 A \u4ef7\u683c P \u80a1\u6570 1w * 10% / p \u5e76\u6309 100 \u53d6\u6574\n        :param adjust_sell: \u662f\u5426\u6839\u636e\u7528\u6237\u7684\u5b9e\u9645\u6301\u4ed3\u6570\u8c03\u6574\u5356\u51fa\u80a1\u7968\u6570\u91cf\uff0c\n            \u5f53\u5356\u51fa\u80a1\u7968\u6570\u5927\u4e8e\u5b9e\u9645\u6301\u4ed3\u6570\u65f6\uff0c\u8c03\u6574\u4e3a\u5b9e\u9645\u6301\u4ed3\u6570\u3002\u76ee\u524d\u4ec5\u5728\u94f6\u6cb3\u5ba2\u6237\u7aef\u6d4b\u8bd5\u901a\u8fc7\u3002\n            \u5f53 users \u4e3a\u591a\u4e2a\u65f6\uff0c\u6839\u636e\u7b2c\u4e00\u4e2a user \u7684\u6301\u4ed3\u6570\u51b3\u5b9a\n        :type adjust_sell: bool\n        :param initial_assets: \u96ea\u7403\u7ec4\u5408\u5bf9\u5e94\u7684\u521d\u59cb\u8d44\u4ea7,\n            \u683c\u5f0f [ \u7ec4\u54081\u5bf9\u5e94\u8d44\u91d1, \u7ec4\u54082\u5bf9\u5e94\u8d44\u91d1 ]\n            \u603b\u8d44\u4ea7\u7531 \u521d\u59cb\u8d44\u4ea7 \u00d7 \u7ec4\u5408\u51c0\u503c \u7b97\u5f97\uff0c total_assets \u4f1a\u8986\u76d6\u6b64\u53c2\u6570\n        :param track_interval: \u8f6e\u8bad\u6a21\u62df\u4ea4\u6613\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2\n        :param trade_cmd_expire_seconds: \u4ea4\u6613\u6307\u4ee4\u8fc7\u671f\u65f6\u95f4, \u5355\u4f4d\u4e3a\u79d2\n        :param cmd_cache: \u662f\u5426\u8bfb\u53d6\u5b58\u50a8\u5386\u53f2\u6267\u884c\u8fc7\u7684\u6307\u4ee4\uff0c\u9632\u6b62\u91cd\u542f\u65f6\u91cd\u590d\u6267\u884c\u5df2\u7ecf\u4ea4\u6613\u8fc7\u7684\u6307\u4ee4\n        :param slippage: \u6ed1\u70b9\uff0c0.0 \u8868\u793a\u65e0\u6ed1\u70b9, 0.05 \u8868\u793a\u6ed1\u70b9\u4e3a 5%\n        \"\"\"\n        super().follow(users=users,\n                       strategies=strategies,\n                       track_interval=track_interval,\n                       trade_cmd_expire_seconds=trade_cmd_expire_seconds,\n                       cmd_cache=cmd_cache,\n                       slippage=slippage)\n\n        self._adjust_sell = adjust_sell\n\n        self._users = self.warp_list(users)\n\n        strategies = self.warp_list(strategies)\n        total_assets = self.warp_list(total_assets)\n        initial_assets = self.warp_list(initial_assets)\n\n        if cmd_cache:\n            self.load_expired_cmd_cache()\n\n        self.start_trader_thread(self._users, trade_cmd_expire_seconds)\n\n        for strategy_url, strategy_total_assets, strategy_initial_assets in zip(\n                strategies, total_assets, initial_assets):\n            assets = self.calculate_assets(strategy_url, strategy_total_assets,\n                                           strategy_initial_assets)\n            try:\n                strategy_id = self.extract_strategy_id(strategy_url)\n                strategy_name = self.extract_strategy_name(strategy_url)\n            except:\n                log.error('\u62bd\u53d6\u4ea4\u6613id\u548c\u7b56\u7565\u540d\u5931\u8d25, \u65e0\u6548\u6a21\u62df\u4ea4\u6613url: %s', strategy_url)\n                raise\n            strategy_worker = Thread(\n                target=self.track_strategy_worker,\n                args=[strategy_id, strategy_name],\n                kwargs={\n                    'interval': track_interval,\n                    'assets': assets\n                })\n            strategy_worker.start()\n            log.info('\u5f00\u59cb\u8ddf\u8e2a\u7b56\u7565: %s', strategy_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadjust the amount of a given stock code.", "response": "def _adjust_sell_amount(self, stock_code, amount):\n        \"\"\"\n        \u6839\u636e\u5b9e\u9645\u6301\u4ed3\u503c\u8ba1\u7b97\u96ea\u7403\u5356\u51fa\u80a1\u6570\n          \u56e0\u4e3a\u96ea\u7403\u7684\u4ea4\u6613\u6307\u4ee4\u662f\u57fa\u4e8e\u6301\u4ed3\u767e\u5206\u6bd4\uff0c\u5728\u53d6\u8fd1\u4f3c\u503c\u7684\u60c5\u51b5\u4e0b\u53ef\u80fd\u51fa\u73b0\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002\n        \u5bfc\u81f4\u5982\u4e0b\u60c5\u51b5\u7684\u4ea7\u751f\uff0c\u8ba1\u7b97\u51fa\u7684\u6307\u4ee4\u4e3a\u4e70\u5165 1049 \u80a1\uff0c\u53d6\u8fd1\u4f3c\u503c\u4e70\u5165 1000 \u80a1\u3002\n        \u800c\u5356\u51fa\u7684\u6307\u4ee4\u8ba1\u7b97\u51fa\u4e3a\u5356\u51fa 1051 \u80a1\uff0c\u53d6\u8fd1\u4f3c\u503c\u5356\u51fa 1100 \u80a1\uff0c\u8d85\u8fc7 1000 \u80a1\u7684\u4e70\u5165\u91cf\uff0c\n        \u5bfc\u81f4\u5356\u51fa\u5931\u8d25\n        :param stock_code: \u8bc1\u5238\u4ee3\u7801\n        :type stock_code: str\n        :param amount: \u5356\u51fa\u80a1\u4efd\u6570\n        :type amount: int\n        :return: \u8003\u8651\u5b9e\u9645\u6301\u4ed3\u4e4b\u540e\u7684\u5356\u51fa\u80a1\u4efd\u6570\n        :rtype: int\n        \"\"\"\n        stock_code = stock_code[-6:]\n        user = self._users[0]\n        position = user.position\n        try:\n            stock = next(s for s in position if s['\u8bc1\u5238\u4ee3\u7801'] == stock_code)\n        except StopIteration:\n            log.info('\u6839\u636e\u6301\u4ed3\u8c03\u6574 %s \u5356\u51fa\u989d\uff0c\u53d1\u73b0\u672a\u6301\u6709\u80a1\u7968 %s, \u4e0d\u505a\u4efb\u4f55\u8c03\u6574',\n                     stock_code, stock_code)\n            return amount\n\n        available_amount = stock['\u53ef\u7528\u4f59\u989d']\n        if available_amount >= amount:\n            return amount\n\n        adjust_amount = available_amount // 100 * 100\n        log.info('\u80a1\u7968 %s \u5b9e\u9645\u53ef\u7528\u4f59\u989d %s, \u6307\u4ee4\u5356\u51fa\u80a1\u6570\u4e3a %s, \u8c03\u6574\u4e3a %s',\n                 stock_code, available_amount, amount, adjust_amount)\n        return adjust_amount"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing cookies str to dict", "response": "def parse_cookies_str(cookies):\n    \"\"\"\n    parse cookies str to dict\n    :param cookies: cookies str\n    :type cookies: str\n    :return: cookie dict\n    :rtype: dict\n    \"\"\"\n    cookie_dict = {}\n    for record in cookies.split(\";\"):\n        key, value = record.strip().split(\"=\", 1)\n        cookie_dict[key] = value\n    return cookie_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_stock_type(stock_code):\n    stock_code = str(stock_code)\n    if stock_code.startswith((\"sh\", \"sz\")):\n        return stock_code[:2]\n    if stock_code.startswith(\n        (\"50\", \"51\", \"60\", \"73\", \"90\", \"110\", \"113\", \"132\", \"204\", \"78\")\n    ):\n        return \"sh\"\n    if stock_code.startswith(\n        (\"00\", \"13\", \"18\", \"15\", \"16\", \"18\", \"20\", \"30\", \"39\", \"115\", \"1318\")\n    ):\n        return \"sz\"\n    if stock_code.startswith((\"5\", \"6\", \"9\")):\n        return \"sh\"\n    return \"sz\"", "response": "\u5224\u65ad\u80a1\u7968ID\u5bf9\u5e94\u7684\u8bc1\u5238\u5e02\u573a\n    \u5339\u914d\u89c4\u5219\n    ['50', '51', '60', '90', '110'] \u4e3a sh\n    ['00', '13', '18', '15', '16', '18', '20', '30', '39', '115'] \u4e3a sz\n    ['5', '6', '9'] \u5f00\u5934\u7684\u4e3a sh\uff0c \u5176\u4f59\u4e3a sz\n    :param stock_code:\u80a1\u7968ID, \u82e5\u4ee5 'sz', 'sh' \u5f00\u5934\u76f4\u63a5\u8fd4\u56de\u5bf9\u5e94\u7c7b\u578b\uff0c\u5426\u5219\u4f7f\u7528\u5185\u7f6e\u89c4\u5219\u5224\u65ad\n    :return 'sh' or 'sz"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef recognize_verify_code(image_path, broker=\"ht\"):\n\n    if broker == \"gf\":\n        return detect_gf_result(image_path)\n    if broker in [\"yh_client\", \"gj_client\"]:\n        return detect_yh_client_result(image_path)\n    # \u8c03\u7528 tesseract \u8bc6\u522b\n    return default_verify_code_detect(image_path)", "response": "\u8bc6\u522b\u9a8c\u8bc1\u7801\uff0c\u8fd4\u56de\u8bc6\u522b\u540e\u7684\u5b57\u7b26\u4e32\uff0c\u4f7f\u7528 tesseract \u5b9e\u73b0\n    :param image_path: \u56fe\u7247\u8def\u5f84\n    :param broker: \u5238\u5546 ['ht', 'yjb', 'gf', 'yh']\n    :return recognized: verify code string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetecting YH client result", "response": "def detect_yh_client_result(image_path):\n    \"\"\"\u5c01\u88c5\u4e86tesseract\u7684\u8bc6\u522b\uff0c\u90e8\u7f72\u5728\u963f\u91cc\u4e91\u4e0a\uff0c\u670d\u52a1\u7aef\u6e90\u7801\u5730\u5740\u4e3a\uff1a https://github.com/shidenggui/yh_verify_code_docker\"\"\"\n    api = \"http://yh.ez.shidenggui.com:5000/yh_client\"\n    with open(image_path, \"rb\") as f:\n        rep = requests.post(api, files={\"image\": f})\n    if rep.status_code != 201:\n        error = rep.json()[\"message\"]\n        raise exceptions.TradeError(\"request {} error: {}\".format(api, error))\n    return rep.json()[\"result\"]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting today IPO data", "response": "def get_today_ipo_data():\n    \"\"\"\n    \u67e5\u8be2\u4eca\u5929\u53ef\u4ee5\u7533\u8d2d\u7684\u65b0\u80a1\u4fe1\u606f\n    :return: \u4eca\u65e5\u53ef\u7533\u8d2d\u65b0\u80a1\u5217\u8868 apply_code\u7533\u8d2d\u4ee3\u7801 price\u53d1\u884c\u4ef7\u683c\n    \"\"\"\n\n    agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:43.0) Gecko/20100101 Firefox/43.0\"\n    send_headers = {\n        \"Host\": \"xueqiu.com\",\n        \"User-Agent\": agent,\n        \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n        \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3\",\n        \"Accept-Encoding\": \"deflate\",\n        \"Cache-Control\": \"no-cache\",\n        \"X-Requested-With\": \"XMLHttpRequest\",\n        \"Referer\": \"https://xueqiu.com/hq\",\n        \"Connection\": \"keep-alive\",\n    }\n\n    timestamp = random.randint(1000000000000, 9999999999999)\n    home_page_url = \"https://xueqiu.com\"\n    ipo_data_url = (\n        \"https://xueqiu.com/proipo/query.json?column=symbol,name,onl_subcode,onl_subbegdate,actissqty,onl\"\n        \"_actissqty,onl_submaxqty,iss_price,onl_lotwiner_stpub_date,onl_lotwinrt,onl_lotwin_amount,stock_\"\n        \"income&orderBy=onl_subbegdate&order=desc&stockType=&page=1&size=30&_=%s\"\n        % (str(timestamp))\n    )\n\n    session = requests.session()\n    session.get(home_page_url, headers=send_headers)  # \u4ea7\u751fcookies\n    ipo_response = session.post(ipo_data_url, headers=send_headers)\n\n    json_data = json.loads(ipo_response.text)\n    today_ipo = []\n\n    for line in json_data[\"data\"]:\n        if datetime.datetime.now().strftime(\"%a %b %d\") == line[3][:10]:\n            today_ipo.append(\n                {\n                    \"stock_code\": line[0],\n                    \"stock_name\": line[1],\n                    \"apply_code\": line[2],\n                    \"price\": line[7],\n                }\n            )\n\n    return today_ipo"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef login(self, user=None, password=None, **kwargs):\n        headers = self._generate_headers()\n        self.s.headers.update(headers)\n\n        # init cookie\n        self.s.get(self.LOGIN_PAGE)\n\n        # post for login\n        params = self.create_login_params(user, password, **kwargs)\n        rep = self.s.post(self.LOGIN_API, data=params)\n\n        self.check_login_success(rep)\n        log.info(\"\u767b\u5f55\u6210\u529f\")", "response": "Login to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef follow(\n        self,\n        users,\n        strategies,\n        track_interval=1,\n        trade_cmd_expire_seconds=120,\n        cmd_cache=True,\n        slippage: float = 0.0,\n        **kwargs\n    ):\n        \"\"\"\u8ddf\u8e2a\u5e73\u53f0\u5bf9\u5e94\u7684\u6a21\u62df\u4ea4\u6613\uff0c\u652f\u6301\u591a\u7528\u6237\u591a\u7b56\u7565\n\n        :param users: \u652f\u6301easytrader\u7684\u7528\u6237\u5bf9\u8c61\uff0c\u652f\u6301\u4f7f\u7528 [] \u6307\u5b9a\u591a\u4e2a\u7528\u6237\n        :param strategies: \u96ea\u7403\u7ec4\u5408\u540d, \u7c7b\u4f3c ZH123450\n        :param total_assets: \u96ea\u7403\u7ec4\u5408\u5bf9\u5e94\u7684\u603b\u8d44\u4ea7\uff0c \u683c\u5f0f [ \u7ec4\u54081\u5bf9\u5e94\u8d44\u91d1, \u7ec4\u54082\u5bf9\u5e94\u8d44\u91d1 ]\n            \u82e5 strategies=['ZH000001', 'ZH000002'] \u8bbe\u7f6e total_assets=[10000, 10000], \u5219\u8868\u660e\u6bcf\u4e2a\u7ec4\u5408\u5bf9\u5e94\u7684\u8d44\u4ea7\u4e3a 1w \u5143\uff0c\n            \u5047\u8bbe\u7ec4\u5408 ZH000001 \u52a0\u4ed3 \u4ef7\u683c\u4e3a p \u80a1\u7968 A 10%, \u5219\u5bf9\u5e94\u7684\u4ea4\u6613\u6307\u4ee4\u4e3a \u4e70\u5165 \u80a1\u7968 A \u4ef7\u683c P \u80a1\u6570 1w * 10% / p \u5e76\u6309 100 \u53d6\u6574\n        :param initial_assets:\u96ea\u7403\u7ec4\u5408\u5bf9\u5e94\u7684\u521d\u59cb\u8d44\u4ea7, \u683c\u5f0f [ \u7ec4\u54081\u5bf9\u5e94\u8d44\u91d1, \u7ec4\u54082\u5bf9\u5e94\u8d44\u91d1 ]\n            \u603b\u8d44\u4ea7\u7531 \u521d\u59cb\u8d44\u4ea7 \u00d7 \u7ec4\u5408\u51c0\u503c \u7b97\u5f97\uff0c total_assets \u4f1a\u8986\u76d6\u6b64\u53c2\u6570\n        :param track_interval: \u8f6e\u8be2\u6a21\u62df\u4ea4\u6613\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2\n        :param trade_cmd_expire_seconds: \u4ea4\u6613\u6307\u4ee4\u8fc7\u671f\u65f6\u95f4, \u5355\u4f4d\u4e3a\u79d2\n        :param cmd_cache: \u662f\u5426\u8bfb\u53d6\u5b58\u50a8\u5386\u53f2\u6267\u884c\u8fc7\u7684\u6307\u4ee4\uff0c\u9632\u6b62\u91cd\u542f\u65f6\u91cd\u590d\u6267\u884c\u5df2\u7ecf\u4ea4\u6613\u8fc7\u7684\u6307\u4ee4\n        :param slippage: \u6ed1\u70b9\uff0c0.0 \u8868\u793a\u65e0\u6ed1\u70b9, 0.05 \u8868\u793a\u6ed1\u70b9\u4e3a 5%\n        \"\"\"\n        self.slippage = slippage", "response": "Follow a set of users and strategies."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the price by the slippage", "response": "def _calculate_price_by_slippage(self, action: str, price: float) -> float:\n        \"\"\"\n        \u8ba1\u7b97\u8003\u8651\u6ed1\u70b9\u4e4b\u540e\u7684\u4ef7\u683c\n        :param action: \u4ea4\u6613\u52a8\u4f5c\uff0c \u652f\u6301 ['buy', 'sell']\n        :param price: \u539f\u59cb\u4ea4\u6613\u4ef7\u683c\n        :return: \u8003\u8651\u6ed1\u70b9\u540e\u7684\u4ea4\u6613\u4ef7\u683c\n        \"\"\"\n        if action == \"buy\":\n            return price * (1 + self.slippage)\n        if action == \"sell\":\n            return price * (1 - self.slippage)\n        return price"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef track_strategy_worker(self, strategy, name, interval=10, **kwargs):\n        while True:\n            try:\n                transactions = self.query_strategy_transaction(\n                    strategy, **kwargs\n                )\n            # pylint: disable=broad-except\n            except Exception as e:\n                log.exception(\"\u65e0\u6cd5\u83b7\u53d6\u7b56\u7565 %s \u8c03\u4ed3\u4fe1\u606f, \u9519\u8bef: %s, \u8df3\u8fc7\u6b64\u6b21\u8c03\u4ed3\u67e5\u8be2\", name, e)\n                time.sleep(3)\n                continue\n            for transaction in transactions:\n                trade_cmd = {\n                    \"strategy\": strategy,\n                    \"strategy_name\": name,\n                    \"action\": transaction[\"action\"],\n                    \"stock_code\": transaction[\"stock_code\"],\n                    \"amount\": transaction[\"amount\"],\n                    \"price\": transaction[\"price\"],\n                    \"datetime\": transaction[\"datetime\"],\n                }\n                if self.is_cmd_expired(trade_cmd):\n                    continue\n                log.info(\n                    \"\u7b56\u7565 [%s] \u53d1\u9001\u6307\u4ee4\u5230\u4ea4\u6613\u961f\u5217, \u80a1\u7968: %s \u52a8\u4f5c: %s \u6570\u91cf: %s \u4ef7\u683c: %s \u4fe1\u53f7\u4ea7\u751f\u65f6\u95f4: %s\",\n                    name,\n                    trade_cmd[\"stock_code\"],\n                    trade_cmd[\"action\"],\n                    trade_cmd[\"amount\"],\n                    trade_cmd[\"price\"],\n                    trade_cmd[\"datetime\"],\n                )\n                self.trade_queue.put(trade_cmd)\n                self.add_cmd_to_expired_cmds(trade_cmd)\n            try:\n                for _ in range(interval):\n                    time.sleep(1)\n            except KeyboardInterrupt:\n                log.info(\"\u7a0b\u5e8f\u9000\u51fa\")\n                break", "response": "This method is used to track the strategy."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting the trade command.", "response": "def _execute_trade_cmd(\n        self, trade_cmd, users, expire_seconds, entrust_prop, send_interval\n    ):\n        \"\"\"\u5206\u53d1\u4ea4\u6613\u6307\u4ee4\u5230\u5bf9\u5e94\u7684 user \u5e76\u6267\u884c\n        :param trade_cmd:\n        :param users:\n        :param expire_seconds:\n        :param entrust_prop:\n        :param send_interval:\n        :return:\n        \"\"\"\n        for user in users:\n            # check expire\n            now = datetime.datetime.now()\n            expire = (now - trade_cmd[\"datetime\"]).total_seconds()\n            if expire > expire_seconds:\n                log.warning(\n                    \"\u7b56\u7565 [%s] \u6307\u4ee4(\u80a1\u7968: %s \u52a8\u4f5c: %s \u6570\u91cf: %s \u4ef7\u683c: %s)\u8d85\u65f6\uff0c\u6307\u4ee4\u4ea7\u751f\u65f6\u95f4: %s \u5f53\u524d\u65f6\u95f4: %s, \u8d85\u8fc7\u8bbe\u7f6e\u7684\u6700\u5927\u8fc7\u671f\u65f6\u95f4 %s \u79d2, \u88ab\u4e22\u5f03\",\n                    trade_cmd[\"strategy_name\"],\n                    trade_cmd[\"stock_code\"],\n                    trade_cmd[\"action\"],\n                    trade_cmd[\"amount\"],\n                    trade_cmd[\"price\"],\n                    trade_cmd[\"datetime\"],\n                    now,\n                    expire_seconds,\n                )\n                break\n\n            # check price\n            price = trade_cmd[\"price\"]\n            if not self._is_number(price) or price <= 0:\n                log.warning(\n                    \"\u7b56\u7565 [%s] \u6307\u4ee4(\u80a1\u7968: %s \u52a8\u4f5c: %s \u6570\u91cf: %s \u4ef7\u683c: %s)\u8d85\u65f6\uff0c\u6307\u4ee4\u4ea7\u751f\u65f6\u95f4: %s \u5f53\u524d\u65f6\u95f4: %s, \u4ef7\u683c\u65e0\u6548 , \u88ab\u4e22\u5f03\",\n                    trade_cmd[\"strategy_name\"],\n                    trade_cmd[\"stock_code\"],\n                    trade_cmd[\"action\"],\n                    trade_cmd[\"amount\"],\n                    trade_cmd[\"price\"],\n                    trade_cmd[\"datetime\"],\n                    now,\n                )\n                break\n\n            # check amount\n            if trade_cmd[\"amount\"] <= 0:\n                log.warning(\n                    \"\u7b56\u7565 [%s] \u6307\u4ee4(\u80a1\u7968: %s \u52a8\u4f5c: %s \u6570\u91cf: %s \u4ef7\u683c: %s)\u8d85\u65f6\uff0c\u6307\u4ee4\u4ea7\u751f\u65f6\u95f4: %s \u5f53\u524d\u65f6\u95f4: %s, \u4e70\u5165\u80a1\u6570\u65e0\u6548 , \u88ab\u4e22\u5f03\",\n                    trade_cmd[\"strategy_name\"],\n                    trade_cmd[\"stock_code\"],\n                    trade_cmd[\"action\"],\n                    trade_cmd[\"amount\"],\n                    trade_cmd[\"price\"],\n                    trade_cmd[\"datetime\"],\n                    now,\n                )\n                break\n\n            actual_price = self._calculate_price_by_slippage(\n                trade_cmd[\"action\"], trade_cmd[\"price\"]\n            )\n            args = {\n                \"security\": trade_cmd[\"stock_code\"],\n                \"price\": actual_price,\n                \"amount\": trade_cmd[\"amount\"],\n                \"entrust_prop\": entrust_prop,\n            }\n            try:\n                response = getattr(user, trade_cmd[\"action\"])(**args)\n            except exceptions.TradeError as e:\n                trader_name = type(user).__name__\n                err_msg = \"{}: {}\".format(type(e).__name__, e.args)\n                log.error(\n                    \"%s \u6267\u884c \u7b56\u7565 [%s] \u6307\u4ee4(\u80a1\u7968: %s \u52a8\u4f5c: %s \u6570\u91cf: %s \u4ef7\u683c(\u8003\u8651\u6ed1\u70b9): %s \u6307\u4ee4\u4ea7\u751f\u65f6\u95f4: %s) \u5931\u8d25, \u9519\u8bef\u4fe1\u606f: %s\",\n                    trader_name,\n                    trade_cmd[\"strategy_name\"],\n                    trade_cmd[\"stock_code\"],\n                    trade_cmd[\"action\"],\n                    trade_cmd[\"amount\"],\n                    actual_price,\n                    trade_cmd[\"datetime\"],\n                    err_msg,\n                )\n            else:\n                log.info(\n                    \"\u7b56\u7565 [%s] \u6307\u4ee4(\u80a1\u7968: %s \u52a8\u4f5c: %s \u6570\u91cf: %s \u4ef7\u683c(\u8003\u8651\u6ed1\u70b9): %s \u6307\u4ee4\u4ea7\u751f\u65f6\u95f4: %s) \u6267\u884c\u6210\u529f, \u8fd4\u56de: %s\",\n                    trade_cmd[\"strategy_name\"],\n                    trade_cmd[\"stock_code\"],\n                    trade_cmd[\"action\"],\n                    trade_cmd[\"amount\"],\n                    actual_price,\n                    trade_cmd[\"datetime\"],\n                    response,\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef trade_worker(\n        self, users, expire_seconds=120, entrust_prop=\"limit\", send_interval=0\n    ):\n        \"\"\"\n        :param send_interval: \u4ea4\u6613\u53d1\u9001\u95f4\u9694\uff0c \u9ed8\u8ba4\u4e3a0s\u3002\u8c03\u5927\u53ef\u9632\u6b62\u5356\u51fa\u4e70\u5165\u65f6\u4e70\u51fa\u5355\u6ca1\u6709\u53ca\u65f6\u6210\u4ea4\u5bfc\u81f4\u7684\u4e70\u5165\u91d1\u989d\u4e0d\u8db3\n        \"\"\"\n        while True:\n            trade_cmd = self.trade_queue.get()\n            self._execute_trade_cmd(\n                trade_cmd, users, expire_seconds, entrust_prop, send_interval\n            )\n            time.sleep(send_interval)", "response": "This function is the main worker function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_cookies(self, cookies):\n        cookie_dict = helpers.parse_cookies_str(cookies)\n        self.s.cookies.update(cookie_dict)", "response": "Set the cookies of the current session."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prepare_account(self, user=\"\", password=\"\", **kwargs):\n        if \"portfolio_code\" not in kwargs:\n            raise TypeError(\"\u96ea\u7403\u767b\u5f55\u9700\u8981\u8bbe\u7f6e portfolio_code(\u7ec4\u5408\u4ee3\u7801) \u53c2\u6570\")\n        if \"portfolio_market\" not in kwargs:\n            kwargs[\"portfolio_market\"] = \"cn\"\n        if \"cookies\" not in kwargs:\n            raise TypeError(\n                \"\u96ea\u7403\u767b\u9646\u9700\u8981\u8bbe\u7f6e cookies\uff0c \u5177\u4f53\u89c1\"\n                \"https://smalltool.github.io/2016/08/02/cookie/\"\n            )\n        self.account_config = {\n            \"cookies\": kwargs[\"cookies\"],\n            \"portfolio_code\": kwargs[\"portfolio_code\"],\n            \"portfolio_market\": kwargs[\"portfolio_market\"],\n        }", "response": "\u8f6c\u6362\u53c2\u6570\u5230\u767b\u5f55\u6240\u9700\u7684\u5b57\u5178\u683c\u5f0f\n        :param cookies: \u96ea\u7403\u767b\u9646\u9700\u8981\u8bbe\u7f6e cookies\uff0c \u5177\u4f53\u89c1\n            https://smalltool.github.io/2016/08/02/cookie/\n        :param portfolio_code: \u7ec4\u5408\u4ee3\u7801\n        :param portfolio_market: \u4ea4\u6613\u5e02\u573a\uff0c \u53ef\u9009['cn', 'us', 'hk'] \u9ed8\u8ba4 'cn'\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _search_stock_info(self, code):\n        data = {\n            \"code\": str(code),\n            \"size\": \"300\",\n            \"key\": \"47bce5c74f\",\n            \"market\": self.account_config[\"portfolio_market\"],\n        }\n        r = self.s.get(self.config[\"search_stock_url\"], params=data)\n        stocks = json.loads(r.text)\n        stocks = stocks[\"stocks\"]\n        stock = None\n        if len(stocks) > 0:\n            stock = stocks[0]\n        return stock", "response": "Search for stock info by code."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_balance(self):\n        portfolio_code = self.account_config.get(\"portfolio_code\", \"ch\")\n        portfolio_info = self._get_portfolio_info(portfolio_code)\n        asset_balance = self._virtual_to_balance(\n            float(portfolio_info[\"net_value\"])\n        )  # \u603b\u8d44\u4ea7\n        position = portfolio_info[\"view_rebalancing\"]  # \u4ed3\u4f4d\u7ed3\u6784\n        cash = asset_balance * float(position[\"cash\"]) / 100\n        market = asset_balance - cash\n        return [\n            {\n                \"asset_balance\": asset_balance,\n                \"current_balance\": cash,\n                \"enable_balance\": cash,\n                \"market_value\": market,\n                \"money_type\": u\"\u4eba\u6c11\u5e01\",\n                \"pre_interest\": 0.25,\n            }\n        ]", "response": "Get the balance of the asset and the cash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the position of the rebalancing", "response": "def _get_position(self):\n        \"\"\"\n        \u83b7\u53d6\u96ea\u7403\u6301\u4ed3\n        :return:\n        \"\"\"\n        portfolio_code = self.account_config[\"portfolio_code\"]\n        portfolio_info = self._get_portfolio_info(portfolio_code)\n        position = portfolio_info[\"view_rebalancing\"]  # \u4ed3\u4f4d\u7ed3\u6784\n        stocks = position[\"holdings\"]  # \u6301\u4ed3\u80a1\u7968\n        return stocks"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the list of all the xq related items", "response": "def _get_xq_history(self):\n        \"\"\"\n        \u83b7\u53d6\u96ea\u7403\u8c03\u4ed3\u5386\u53f2\n        :param instance:\n        :param owner:\n        :return:\n        \"\"\"\n        data = {\n            \"cube_symbol\": str(self.account_config[\"portfolio_code\"]),\n            \"count\": 20,\n            \"page\": 1,\n        }\n        resp = self.s.get(self.config[\"history_url\"], params=data)\n        res = json.loads(resp.text)\n        return res[\"list\"]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_entrust(self):\n        xq_entrust_list = self._get_xq_history()\n        entrust_list = []\n        replace_none = lambda s: s or 0\n        for xq_entrusts in xq_entrust_list:\n            status = xq_entrusts[\"status\"]  # \u8c03\u4ed3\u72b6\u6001\n            if status == \"pending\":\n                status = \"\u5df2\u62a5\"\n            elif status in [\"canceled\", \"failed\"]:\n                status = \"\u5e9f\u5355\"\n            else:\n                status = \"\u5df2\u6210\"\n            for entrust in xq_entrusts[\"rebalancing_histories\"]:\n                price = entrust[\"price\"]\n                entrust_list.append(\n                    {\n                        \"entrust_no\": entrust[\"id\"],\n                        \"entrust_bs\": u\"\u4e70\u5165\"\n                        if entrust[\"target_weight\"]\n                        > replace_none(entrust[\"prev_weight\"])\n                        else u\"\u5356\u51fa\",\n                        \"report_time\": self._time_strftime(\n                            entrust[\"updated_at\"]\n                        ),\n                        \"entrust_status\": status,\n                        \"stock_code\": entrust[\"stock_symbol\"],\n                        \"stock_name\": entrust[\"stock_name\"],\n                        \"business_amount\": 100,\n                        \"business_price\": price,\n                        \"entrust_amount\": 100,\n                        \"entrust_price\": price,\n                    }\n                )\n        return entrust_list", "response": "Get a list of all the entrust tables."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cancel_entrust(self, entrust_no):\n        xq_entrust_list = self._get_xq_history()\n        is_have = False\n        for xq_entrusts in xq_entrust_list:\n            status = xq_entrusts[\"status\"]  # \u8c03\u4ed3\u72b6\u6001\n            for entrust in xq_entrusts[\"rebalancing_histories\"]:\n                if entrust[\"id\"] == entrust_no and status == \"pending\":\n                    is_have = True\n                    buy_or_sell = (\n                        \"buy\"\n                        if entrust[\"target_weight\"] < entrust[\"weight\"]\n                        else \"sell\"\n                    )\n                    if (\n                        entrust[\"target_weight\"] == 0\n                        and entrust[\"weight\"] == 0\n                    ):\n                        raise exceptions.TradeError(u\"\u79fb\u9664\u7684\u80a1\u7968\u64cd\u4f5c\u65e0\u6cd5\u64a4\u9500,\u5efa\u8bae\u91cd\u65b0\u4e70\u5165\")\n                    balance = self.get_balance()[0]\n                    volume = (\n                        abs(entrust[\"target_weight\"] - entrust[\"weight\"])\n                        * balance[\"asset_balance\"]\n                        / 100\n                    )\n                    r = self._trade(\n                        security=entrust[\"stock_symbol\"],\n                        volume=volume,\n                        entrust_bs=buy_or_sell,\n                    )\n                    if len(r) > 0 and \"error_info\" in r[0]:\n                        raise exceptions.TradeError(\n                            u\"\u64a4\u9500\u5931\u8d25!%s\" % (\"error_info\" in r[0])\n                        )\n        if not is_have:\n            raise exceptions.TradeError(u\"\u64a4\u9500\u5bf9\u8c61\u5df2\u5931\u6548\")\n        return True", "response": "\u5bf9\u672a\u6210\u4ea4\u7684\u8c03\u4ed3\u8fdb\u884c\u4f2a\u64a4\u5355\n        :param entrust_no:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadjusts the weight of a stock.", "response": "def adjust_weight(self, stock_code, weight):\n        \"\"\"\n        \u96ea\u7403\u7ec4\u5408\u8c03\u4ed3, weight \u4e3a\u8c03\u6574\u540e\u7684\u4ed3\u4f4d\u6bd4\u4f8b\n        :param stock_code: str \u80a1\u7968\u4ee3\u7801\n        :param weight: float \u8c03\u6574\u4e4b\u540e\u7684\u6301\u4ed3\u767e\u5206\u6bd4\uff0c 0 - 100 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\n        \"\"\"\n\n        stock = self._search_stock_info(stock_code)\n        if stock is None:\n            raise exceptions.TradeError(u\"\u6ca1\u6709\u67e5\u8be2\u8981\u64cd\u4f5c\u7684\u80a1\u7968\u4fe1\u606f\")\n        if stock[\"flag\"] != 1:\n            raise exceptions.TradeError(u\"\u672a\u4e0a\u5e02\u3001\u505c\u724c\u3001\u6da8\u8dcc\u505c\u3001\u9000\u5e02\u7684\u80a1\u7968\u65e0\u6cd5\u64cd\u4f5c\u3002\")\n\n        # \u4ed3\u4f4d\u6bd4\u4f8b\u5411\u4e0b\u53d6\u4e24\u4f4d\u6570\n        weight = round(weight, 2)\n        # \u83b7\u53d6\u539f\u6709\u4ed3\u4f4d\u4fe1\u606f\n        position_list = self._get_position()\n\n        # \u8c03\u6574\u540e\u7684\u6301\u4ed3\n        for position in position_list:\n            if position[\"stock_id\"] == stock[\"stock_id\"]:\n                position[\"proactive\"] = True\n                position[\"weight\"] = weight\n\n        if weight != 0 and stock[\"stock_id\"] not in [\n            k[\"stock_id\"] for k in position_list\n        ]:\n            position_list.append(\n                {\n                    \"code\": stock[\"code\"],\n                    \"name\": stock[\"name\"],\n                    \"enName\": stock[\"enName\"],\n                    \"hasexist\": stock[\"hasexist\"],\n                    \"flag\": stock[\"flag\"],\n                    \"type\": stock[\"type\"],\n                    \"current\": stock[\"current\"],\n                    \"chg\": stock[\"chg\"],\n                    \"percent\": str(stock[\"percent\"]),\n                    \"stock_id\": stock[\"stock_id\"],\n                    \"ind_id\": stock[\"ind_id\"],\n                    \"ind_name\": stock[\"ind_name\"],\n                    \"ind_color\": stock[\"ind_color\"],\n                    \"textname\": stock[\"name\"],\n                    \"segment_name\": stock[\"ind_name\"],\n                    \"weight\": weight,\n                    \"url\": \"/S/\" + stock[\"code\"],\n                    \"proactive\": True,\n                    \"price\": str(stock[\"current\"]),\n                }\n            )\n\n        remain_weight = 100 - sum(i.get(\"weight\") for i in position_list)\n        cash = round(remain_weight, 2)\n        log.debug(\"\u8c03\u4ed3\u6bd4\u4f8b:%f, \u5269\u4f59\u6301\u4ed3 :%f\", weight, remain_weight)\n        data = {\n            \"cash\": cash,\n            \"holdings\": str(json.dumps(position_list)),\n            \"cube_symbol\": str(self.account_config[\"portfolio_code\"]),\n            \"segment\": \"true\",\n            \"comment\": \"\",\n        }\n\n        try:\n            resp = self.s.post(self.config[\"rebalance_url\"], data=data)\n        # pylint: disable=broad-except\n        except Exception as e:\n            log.warning(\"\u8c03\u4ed3\u5931\u8d25: %s \", e)\n            return None\n        log.debug(\"\u8c03\u4ed3 %s: \u6301\u4ed3\u6bd4\u4f8b%d\", stock[\"name\"], weight)\n        resp_json = json.loads(resp.text)\n        if \"error_description\" in resp_json and resp.status_code != 200:\n            log.error(\"\u8c03\u4ed3\u9519\u8bef: %s\", resp_json[\"error_description\"])\n            return [\n                {\n                    \"error_no\": resp_json[\"error_code\"],\n                    \"error_info\": resp_json[\"error_description\"],\n                }\n            ]\n        log.debug(\"\u8c03\u4ed3\u6210\u529f %s: \u6301\u4ed3\u6bd4\u4f8b%d\", stock[\"name\"], weight)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef buy(self, security, price=0, amount=0, volume=0, entrust_prop=0):\n        return self._trade(security, price, amount, volume, \"buy\")", "response": "trade to \u4e70\u5165 \u5356\u51fa \u80a1\u7968"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfollow a set of users and strategies.", "response": "def follow(\n        self,\n        users,\n        strategies,\n        track_interval=1,\n        trade_cmd_expire_seconds=120,\n        cmd_cache=True,\n        entrust_prop=\"limit\",\n        send_interval=0,\n    ):\n        \"\"\"\u8ddf\u8e2ajoinquant\u5bf9\u5e94\u7684\u6a21\u62df\u4ea4\u6613\uff0c\u652f\u6301\u591a\u7528\u6237\u591a\u7b56\u7565\n        :param users: \u652f\u6301easytrader\u7684\u7528\u6237\u5bf9\u8c61\uff0c\u652f\u6301\u4f7f\u7528 [] \u6307\u5b9a\u591a\u4e2a\u7528\u6237\n        :param strategies: joinquant \u7684\u6a21\u62df\u4ea4\u6613\u5730\u5740\uff0c\u652f\u6301\u4f7f\u7528 [] \u6307\u5b9a\u591a\u4e2a\u6a21\u62df\u4ea4\u6613,\n            \u5730\u5740\u7c7b\u4f3c https://www.joinquant.com/algorithm/live/index?backtestId=xxx\n        :param track_interval: \u8f6e\u8bad\u6a21\u62df\u4ea4\u6613\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2\n        :param trade_cmd_expire_seconds: \u4ea4\u6613\u6307\u4ee4\u8fc7\u671f\u65f6\u95f4, \u5355\u4f4d\u4e3a\u79d2\n        :param cmd_cache: \u662f\u5426\u8bfb\u53d6\u5b58\u50a8\u5386\u53f2\u6267\u884c\u8fc7\u7684\u6307\u4ee4\uff0c\u9632\u6b62\u91cd\u542f\u65f6\u91cd\u590d\u6267\u884c\u5df2\u7ecf\u4ea4\u6613\u8fc7\u7684\u6307\u4ee4\n        :param entrust_prop: \u59d4\u6258\u65b9\u5f0f, 'limit' \u4e3a\u9650\u4ef7\uff0c'market' \u4e3a\u5e02\u4ef7, \u4ec5\u5728\u94f6\u6cb3\u5b9e\u73b0\n        :param send_interval: \u4ea4\u6613\u53d1\u9001\u95f4\u9694\uff0c \u9ed8\u8ba4\u4e3a0s\u3002\u8c03\u5927\u53ef\u9632\u6b62\u5356\u51fa\u4e70\u5165\u65f6\u5356\u51fa\u5355\u6ca1\u6709\u53ca\u65f6\u6210\u4ea4\u5bfc\u81f4\u7684\u4e70\u5165\u91d1\u989d\u4e0d\u8db3\n        \"\"\"\n        users = self.warp_list(users)\n        strategies = self.warp_list(strategies)\n\n        if cmd_cache:\n            self.load_expired_cmd_cache()\n\n        self.start_trader_thread(\n            users, trade_cmd_expire_seconds, entrust_prop, send_interval\n        )\n\n        workers = []\n        for strategy_url in strategies:\n            try:\n                strategy_id = self.extract_strategy_id(strategy_url)\n                strategy_name = self.extract_strategy_name(strategy_url)\n            except:\n                log.error(\"\u62bd\u53d6\u4ea4\u6613id\u548c\u7b56\u7565\u540d\u5931\u8d25, \u65e0\u6548\u7684\u6a21\u62df\u4ea4\u6613url: %s\", strategy_url)\n                raise\n            strategy_worker = Thread(\n                target=self.track_strategy_worker,\n                args=[strategy_id, strategy_name],\n                kwargs={\"interval\": track_interval},\n            )\n            strategy_worker.start()\n            workers.append(strategy_worker)\n            log.info(\"\u5f00\u59cb\u8ddf\u8e2a\u7b56\u7565: %s\", strategy_name)\n        for worker in workers:\n            worker.join()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect(self, exe_path=None, **kwargs):\n        connect_path = exe_path or self._config.DEFAULT_EXE_PATH\n        if connect_path is None:\n            raise ValueError(\n                \"\u53c2\u6570 exe_path \u672a\u8bbe\u7f6e\uff0c\u8bf7\u8bbe\u7f6e\u5ba2\u6237\u7aef\u5bf9\u5e94\u7684 exe \u5730\u5740,\u7c7b\u4f3c C:\\\\\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\\\\xiadan.exe\"\n            )\n\n        self._app = pywinauto.Application().connect(\n            path=connect_path, timeout=10\n        )\n        self._close_prompt_windows()\n        self._main = self._app.top_window()", "response": "Connect to the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_market_trade_type(self, ttype):\n        selects = self._main.child_window(\n            control_id=self._config.TRADE_MARKET_TYPE_CONTROL_ID,\n            class_name=\"ComboBox\",\n        )\n        for i, text in selects.texts():\n            # skip 0 index, because 0 index is current select index\n            if i == 0:\n                continue\n            if ttype in text:\n                selects.select(i - 1)\n                break\n        else:\n            raise TypeError(\"\u4e0d\u652f\u6301\u5bf9\u5e94\u7684\u5e02\u4ef7\u7c7b\u578b: {}\".format(ttype))", "response": "set market trade type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare(\n        self,\n        config_path=None,\n        user=None,\n        password=None,\n        exe_path=None,\n        comm_password=None,\n        **kwargs\n    ):\n        \"\"\"\n        \u767b\u9646\u5ba2\u6237\u7aef\n        :param config_path: \u767b\u9646\u914d\u7f6e\u6587\u4ef6\uff0c\u8ddf\u53c2\u6570\u767b\u9646\u65b9\u5f0f\u4e8c\u9009\u4e00\n        :param user: \u8d26\u53f7\n        :param password: \u660e\u6587\u5bc6\u7801\n        :param exe_path: \u5ba2\u6237\u7aef\u8def\u5f84\u7c7b\u4f3c r'C:\\\\htzqzyb2\\\\xiadan.exe', \u9ed8\u8ba4 r'C:\\\\htzqzyb2\\\\xiadan.exe'\n        :param comm_password: \u901a\u8baf\u5bc6\u7801\n        :return:\n        \"\"\"\n        if config_path is not None:\n            account = helpers.file2dict(config_path)\n            user = account[\"user\"]\n            password = account[\"password\"]\n            comm_password = account.get(\"comm_password\")\n            exe_path = account.get(\"exe_path\")\n        self.login(\n            user,\n            password,\n            exe_path or self._config.DEFAULT_EXE_PATH,\n            comm_password,\n            **kwargs\n        )", "response": "Prepare the object for use with the current user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef login(self, user, password, exe_path, comm_password=None, **kwargs):\n        try:\n            self._app = pywinauto.Application().connect(\n                path=self._run_exe_path(exe_path), timeout=1\n            )\n        # pylint: disable=broad-except\n        except Exception:\n            self._app = pywinauto.Application().start(exe_path)\n            is_xiadan = True if \"xiadan.exe\" in exe_path else False\n            # wait login window ready\n            while True:\n                try:\n                    self._app.top_window().Edit1.wait(\"ready\")\n                    break\n                except RuntimeError:\n                    pass\n\n            self._app.top_window().Edit1.type_keys(user)\n            self._app.top_window().Edit2.type_keys(password)\n            while True:\n                self._app.top_window().Edit3.type_keys(\n                    self._handle_verify_code(is_xiadan)\n                )\n                self._app.top_window()[\"\u786e\u5b9a\" if is_xiadan else \"\u767b\u5f55\"].click()\n\n                # detect login is success or not\n                try:\n                    self._app.top_window().wait_not(\"exists visible\", 10)\n                    break\n                # pylint: disable=broad-except\n                except Exception:\n                    if is_xiadan:\n                        self._app.top_window()[\"\u786e\u5b9a\"].click()\n\n            self._app = pywinauto.Application().connect(\n                path=self._run_exe_path(exe_path), timeout=10\n            )\n        self._close_prompt_windows()\n        self._main = self._app.window(title=\"\u7f51\u4e0a\u80a1\u7968\u4ea4\u6613\u7cfb\u7edf5.0\")\n        try:\n            self._main.child_window(\n                control_id=129, class_name=\"SysTreeView32\"\n            ).wait(\"ready\", 2)\n        # pylint: disable=broad-except\n        except Exception:\n            self.wait(2)\n            self._switch_window_to_normal_mode()", "response": "Login to the user and password."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef use(broker, debug=True, **kwargs):\n    if not debug:\n        log.setLevel(logging.INFO)\n    if broker.lower() in [\"xq\", \"\u96ea\u7403\"]:\n        return XueQiuTrader(**kwargs)\n    if broker.lower() in [\"yh_client\", \"\u94f6\u6cb3\u5ba2\u6237\u7aef\"]:\n        from .yh_clienttrader import YHClientTrader\n\n        return YHClientTrader()\n    if broker.lower() in [\"ht_client\", \"\u534e\u6cf0\u5ba2\u6237\u7aef\"]:\n        from .ht_clienttrader import HTClientTrader\n\n        return HTClientTrader()\n    if broker.lower() in [\"gj_client\", \"\u56fd\u91d1\u5ba2\u6237\u7aef\"]:\n        from .gj_clienttrader import GJClientTrader\n\n        return GJClientTrader()\n    if broker.lower() in [\"ths\", \"\u540c\u82b1\u987a\u5ba2\u6237\u7aef\"]:\n        from .clienttrader import ClientTrader\n\n        return ClientTrader()\n\n    raise NotImplementedError", "response": "\u7528\u4e8e\u751f\u6210\u7279\u5b9a\u7684\u5238\u5546\u5bf9\u8c61\n    :param broker:\u5238\u5546\u540d\u652f\u6301 ['yh_client', '\u94f6\u6cb3\u5ba2\u6237\u7aef'] ['ht_client', '\u534e\u6cf0\u5ba2\u6237\u7aef']\n    :param debug: \u63a7\u5236 debug \u65e5\u5fd7\u7684\u663e\u793a, \u9ed8\u8ba4\u4e3a True\n    :param initial_assets: [\u96ea\u7403\u53c2\u6570] \u63a7\u5236\u96ea\u7403\u521d\u59cb\u8d44\u91d1\uff0c\u9ed8\u8ba4\u4e3a\u4e00\u767e\u4e07\n    :return the class of trader\n\n    Usage::\n\n        >>> import easytrader\n        >>> user = easytrader.use('xq')\n        >>> user.prepare('xq.json')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef follower(platform, **kwargs):\n    if platform.lower() in [\"rq\", \"ricequant\", \"\u7c73\u7b50\"]:\n        return RiceQuantFollower()\n    if platform.lower() in [\"jq\", \"joinquant\", \"\u805a\u5bbd\"]:\n        return JoinQuantFollower()\n    if platform.lower() in [\"xq\", \"xueqiu\", \"\u96ea\u7403\"]:\n        return XueQiuFollower(**kwargs)\n    raise NotImplementedError", "response": "Returns a class of follower that follows the given platform."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CaffeLMDB(lmdb_path, shuffle=True, keys=None):\n\n    cpb = get_caffe_pb()\n    lmdb_data = LMDBData(lmdb_path, shuffle, keys)\n\n    def decoder(k, v):\n        try:\n            datum = cpb.Datum()\n            datum.ParseFromString(v)\n            img = np.fromstring(datum.data, dtype=np.uint8)\n            img = img.reshape(datum.channels, datum.height, datum.width)\n        except Exception:\n            log_once(\"Cannot read key {}\".format(k), 'warn')\n            return None\n        return [img.transpose(1, 2, 0), datum.label]\n    logger.warn(\"Caffe LMDB format doesn't store jpeg-compressed images, \\\n        it's not recommended due to its inferior performance.\")\n    return LMDBDataDecoder(lmdb_data, decoder)", "response": "Read a Caffe LMDB file and return a LMDBDataDecoder instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef memory(self):\n        class GpuMemoryInfo(Structure):\n            _fields_ = [\n                ('total', c_ulonglong),\n                ('free', c_ulonglong),\n                ('used', c_ulonglong),\n            ]\n\n        c_memory = GpuMemoryInfo()\n        _check_return(_NVML.get_function(\n            \"nvmlDeviceGetMemoryInfo\")(self.hnd, byref(c_memory)))\n        return {'total': c_memory.total, 'free': c_memory.free, 'used': c_memory.used}", "response": "Return total free and free memory in bytes\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the utilization of the current system.", "response": "def utilization(self):\n        \"\"\"Percent of time over the past second was utilized.\n\n        Details:\n           Percent of time over the past second during which one or more kernels was executing on the GPU.\n           Percent of time over the past second during which global (device) memory was being read or written\n\n        Example:\n\n            >>> print(ctx.device(0).utilization())\n            {'gpu': 4L, 'memory': 6L}\n\n        \"\"\"\n        class GpuUtilizationInfo(Structure):\n\n            _fields_ = [\n                ('gpu', c_uint),\n                ('memory', c_uint),\n            ]\n\n        c_util = GpuUtilizationInfo()\n        _check_return(_NVML.get_function(\n            \"nvmlDeviceGetUtilizationRates\")(self.hnd, byref(c_util)))\n        return {'gpu': c_util.gpu, 'memory': c_util.memory}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets number of devices in the system", "response": "def num_devices(self):\n        \"\"\"Get number of devices \"\"\"\n        c_count = c_uint()\n        _check_return(_NVML.get_function(\n            \"nvmlDeviceGetCount_v2\")(byref(c_count)))\n        return c_count.value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a specific GPU device", "response": "def device(self, idx):\n        \"\"\"Get a specific GPU device\n\n        Args:\n            idx: index of device\n\n        Returns:\n            NvidiaDevice: single GPU device\n        \"\"\"\n\n        class GpuDevice(Structure):\n            pass\n\n        c_nvmlDevice_t = POINTER(GpuDevice)\n\n        c_index = c_uint(idx)\n        device = c_nvmlDevice_t()\n        _check_return(_NVML.get_function(\n            \"nvmlDeviceGetHandleByIndex_v2\")(c_index, byref(device)))\n        return NvidiaDevice(device)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading and extract the tarball from Alex s website. Copied from tensorflow example", "response": "def maybe_download_and_extract(dest_directory, cifar_classnum):\n    \"\"\"Download and extract the tarball from Alex's website. Copied from tensorflow example \"\"\"\n    assert cifar_classnum == 10 or cifar_classnum == 100\n    if cifar_classnum == 10:\n        cifar_foldername = 'cifar-10-batches-py'\n    else:\n        cifar_foldername = 'cifar-100-python'\n    if os.path.isdir(os.path.join(dest_directory, cifar_foldername)):\n        logger.info(\"Found cifar{} data in {}.\".format(cifar_classnum, dest_directory))\n        return\n    else:\n        DATA_URL = DATA_URL_CIFAR_10 if cifar_classnum == 10 else DATA_URL_CIFAR_100\n        filename = DATA_URL[0].split('/')[-1]\n        filepath = os.path.join(dest_directory, filename)\n        download(DATA_URL[0], dest_directory, expect_size=DATA_URL[1])\n        tarfile.open(filepath, 'r:gz').extractall(dest_directory)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_per_pixel_mean(self, names=('train', 'test')):\n        for name in names:\n            assert name in ['train', 'test'], name\n        train_files, test_files, _ = get_filenames(self.dir, self.cifar_classnum)\n        all_files = []\n        if 'train' in names:\n            all_files.extend(train_files)\n        if 'test' in names:\n            all_files.extend(test_files)\n        all_imgs = [x[0] for x in read_cifar(all_files, self.cifar_classnum)]\n        arr = np.array(all_imgs, dtype='float32')\n        mean = np.mean(arr, axis=0)\n        return mean", "response": "Returns the mean image of all images in the given datasets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_per_channel_mean(self, names=('train', 'test')):\n        mean = self.get_per_pixel_mean(names)\n        return np.mean(mean, axis=(0, 1))", "response": "Returns the mean of each image in the given datasets."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maskrcnn_loss(mask_logits, fg_labels, fg_target_masks):\n    num_fg = tf.size(fg_labels, out_type=tf.int64)\n    indices = tf.stack([tf.range(num_fg), fg_labels - 1], axis=1)  # #fgx2\n    mask_logits = tf.gather_nd(mask_logits, indices)  # #fgxhxw\n    mask_probs = tf.sigmoid(mask_logits)\n\n    # add some training visualizations to tensorboard\n    with tf.name_scope('mask_viz'):\n        viz = tf.concat([fg_target_masks, mask_probs], axis=1)\n        viz = tf.expand_dims(viz, 3)\n        viz = tf.cast(viz * 255, tf.uint8, name='viz')\n        tf.summary.image('mask_truth|pred', viz, max_outputs=10)\n\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=fg_target_masks, logits=mask_logits)\n    loss = tf.reduce_mean(loss, name='maskrcnn_loss')\n\n    pred_label = mask_probs > 0.5\n    truth_label = fg_target_masks > 0.5\n    accuracy = tf.reduce_mean(\n        tf.cast(tf.equal(pred_label, truth_label), tf.float32),\n        name='accuracy')\n    pos_accuracy = tf.logical_and(\n        tf.equal(pred_label, truth_label),\n        tf.equal(truth_label, True))\n    pos_accuracy = tf.reduce_mean(tf.cast(pos_accuracy, tf.float32), name='pos_accuracy')\n    fg_pixel_ratio = tf.reduce_mean(tf.cast(truth_label, tf.float32), name='fg_pixel_ratio')\n\n    add_moving_summary(loss, accuracy, fg_pixel_ratio, pos_accuracy)\n    return loss", "response": "Returns a tensorboard loss for masking the given logits."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmask the head layer of a feature with a 2D convolution.", "response": "def maskrcnn_upXconv_head(feature, num_category, num_convs, norm=None):\n    \"\"\"\n    Args:\n        feature (NxCx s x s): size is 7 in C4 models and 14 in FPN models.\n        num_category(int):\n        num_convs (int): number of convolution layers\n        norm (str or None): either None or 'GN'\n\n    Returns:\n        mask_logits (N x num_category x 2s x 2s):\n    \"\"\"\n    assert norm in [None, 'GN'], norm\n    l = feature\n    with argscope([Conv2D, Conv2DTranspose], data_format='channels_first',\n                  kernel_initializer=tf.variance_scaling_initializer(\n                      scale=2.0, mode='fan_out',\n                      distribution='untruncated_normal' if get_tf_version_tuple() >= (1, 12) else 'normal')):\n        # c2's MSRAFill is fan_out\n        for k in range(num_convs):\n            l = Conv2D('fcn{}'.format(k), l, cfg.MRCNN.HEAD_DIM, 3, activation=tf.nn.relu)\n            if norm is not None:\n                l = GroupNorm('gn{}'.format(k), l)\n        l = Conv2DTranspose('deconv', l, cfg.MRCNN.HEAD_DIM, 2, strides=2, activation=tf.nn.relu)\n        l = Conv2D('conv', l, num_category, 1)\n    return l"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the mean of all images in the given datasets.", "response": "def get_per_pixel_mean(names=('train', 'test', 'extra')):\n        \"\"\"\n        Args:\n            names (tuple[str]): names of the dataset split\n\n        Returns:\n            a 32x32x3 image, the mean of all images in the given datasets\n        \"\"\"\n        for name in names:\n            assert name in ['train', 'test', 'extra'], name\n        images = [SVHNDigit(x).X for x in names]\n        return np.concatenate(tuple(images)).mean(axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_or_reuse_placeholder(tensor_spec):\n    g = tfv1.get_default_graph()\n    name = tensor_spec.name\n    try:\n        tensor = g.get_tensor_by_name(name + ':0')\n        assert \"Placeholder\" in tensor.op.type, \"Tensor {} exists but is not a placeholder!\".format(name)\n        assert tensor_spec.is_compatible_with(tensor), \\\n            \"Tensor {} exists but is not compatible with the signature!\".format(tensor)\n        return tensor\n    except KeyError:\n        with tfv1.name_scope(None):   # clear any name scope it might get called in\n            ret = tfv1.placeholder(\n                tensor_spec.dtype, shape=tensor_spec.shape, name=tensor_spec.name)\n        return ret", "response": "Build a tf. placeholder from the given tensor spec or return an existing one."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_input_signature(self):\n        with tf.Graph().as_default() as G:   # create these placeholder in a temporary graph\n            inputs = self.inputs()\n            if isinstance(inputs[0], tf.Tensor):\n                for p in inputs:\n                    assert \"Placeholder\" in p.op.type, \\\n                        \"inputs() have to return TensorSpec or placeholders! Found {} instead.\".format(p)\n                    assert p.graph == G, \"Placeholders returned by inputs() should be created inside inputs()!\"\n            return [TensorSpec(shape=p.shape, dtype=p.dtype, name=get_op_tensor_name(p.name)[0]) for p in inputs]", "response": "Returns a list of TensorSpec which describes the inputs of this model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that op is in the subgraph induced by the dependencies of targets.", "response": "def dependency_of_targets(targets, op):\n    \"\"\"\n    Check that op is in the subgraph induced by the dependencies of targets.\n    The result is memoized.\n\n    This is useful if some SessionRunHooks should be run only together with certain ops.\n\n    Args:\n        targets: a tuple of ops or tensors. The targets to find dependencies of.\n        op (tf.Operation or tf.Tensor):\n\n    Returns:\n        bool: True if any one of `targets` depend on `op`.\n    \"\"\"\n    # TODO tensorarray? sparsetensor?\n    if isinstance(op, tf.Tensor):\n        op = op.op\n    assert isinstance(op, tf.Operation), op\n\n    from tensorflow.contrib.graph_editor import get_backward_walk_ops\n    # alternative implementation can use graph_util.extract_sub_graph\n    dependent_ops = get_backward_walk_ops(targets, control_inputs=True)\n    return op in dependent_ops"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dependency_of_fetches(fetches, op):\n    try:\n        from tensorflow.python.client.session import _FetchHandler as FetchHandler\n        # use the graph of the op, so that this function can be called without being under a default graph\n        handler = FetchHandler(op.graph, fetches, {})\n        targets = tuple(handler.fetches() + handler.targets())\n    except ImportError:\n        if isinstance(fetches, list):\n            targets = tuple(fetches)\n        elif isinstance(fetches, dict):\n            raise ValueError(\"Don't know how to parse dictionary to fetch list! \"\n                             \"This is a bug of tensorpack.\")\n        else:\n            targets = (fetches, )\n    return dependency_of_targets(targets, op)", "response": "Check that op is in the subgraph induced by the dependencies of fetches."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_scalar_summary(name, v):\n    assert isinstance(name, six.string_types), type(name)\n    v = float(v)\n    s = tf.Summary()\n    s.value.add(tag=name, simple_value=v)\n    return s", "response": "Create a summary object with a name and a scalar value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_image_summary(name, val):\n    assert isinstance(name, six.string_types), type(name)\n    n, h, w, c = val.shape\n    val = val.astype('uint8')\n    s = tf.Summary()\n    imparams = [cv2.IMWRITE_PNG_COMPRESSION, 9]\n    for k in range(n):\n        arr = val[k]\n        # CV2 will only write correctly in BGR chanel order\n        if c == 3:\n            arr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n        elif c == 4:\n            arr = cv2.cvtColor(arr, cv2.COLOR_RGBA2BGRA)\n        tag = name if n == 1 else '{}/{}'.format(name, k)\n        retval, img_str = cv2.imencode('.png', arr, imparams)\n        if not retval:\n            # Encoding has failed.\n            continue\n        img_str = img_str.tostring()\n\n        img = tf.Summary.Image()\n        img.height = h\n        img.width = w\n        # 1 - grayscale 3 - RGB 4 - RGBA\n        img.colorspace = c\n        img.encoded_image_string = img_str\n        s.value.add(tag=tag, image=img)\n    return s", "response": "Create summary of NHWC images."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a tensor to the tensorboard that has summary information for the specified types.", "response": "def add_tensor_summary(x, types, name=None, collections=None,\n                       main_tower_only=True):\n    \"\"\"\n    Summarize a tensor by different methods.\n\n    Args:\n        x (tf.Tensor): a tensor to summarize\n        types (list[str]): summary types, can be scalar/histogram/sparsity/mean/rms\n        name (str): summary name. Defaults to be the op name.\n        collections (list[str]): collections of the summary ops.\n        main_tower_only (bool): Only run under main training tower. If\n            set to True, calling this function under other TowerContext\n            has no effect.\n\n    Example:\n\n    .. code-block:: python\n\n        with tf.name_scope('mysummaries'):  # to not mess up tensorboard\n            add_tensor_summary(\n                tensor, ['histogram', 'rms', 'sparsity'], name='mytensor')\n    \"\"\"\n    types = set(types)\n    if name is None:\n        name = x.op.name\n    ctx = get_current_tower_context()\n    if main_tower_only and ctx is not None and not ctx.is_main_training_tower:\n        return\n\n    SUMMARY_TYPES_DIC = {\n        'scalar': lambda: tf.summary.scalar(name + '-summary', x, collections=collections),\n        'histogram': lambda: tf.summary.histogram(name + '-histogram', x, collections=collections),\n        'sparsity': lambda: tf.summary.scalar(\n            name + '-sparsity', tf.nn.zero_fraction(x),\n            collections=collections),\n        'mean': lambda: tf.summary.scalar(\n            name + '-mean', tf.reduce_mean(x),\n            collections=collections),\n        'rms': lambda: tf.summary.scalar(\n            name + '-rms', rms(x), collections=collections)\n    }\n    for typ in types:\n        SUMMARY_TYPES_DIC[typ]()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_activation_summary(x, types=None, name=None, collections=None):\n    ndim = x.get_shape().ndims\n    if ndim < 2:\n        logger.warn(\"Cannot summarize scalar activation {}\".format(x.name))\n        return\n    if types is None:\n        types = ['sparsity', 'rms', 'histogram']\n    with cached_name_scope('activation-summary'):\n        add_tensor_summary(x, types, name=name, collections=collections)", "response": "Add a summary of the activation of a tensor x to the current name scope."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd summary ops for all trainable variables matching the regex.", "response": "def add_param_summary(*summary_lists, **kwargs):\n    \"\"\"\n    Add summary ops for all trainable variables matching the regex, under a\n    reused 'param-summary' name scope.\n    This function is a no-op if not calling from main training tower.\n\n    Args:\n        summary_lists (list): each is (regex, [list of summary type]).\n            Summary type is defined in :func:`add_tensor_summary`.\n        collections (list[str]): collections of the summary ops.\n\n    Example:\n\n    .. code-block:: python\n\n        add_param_summary(\n            ('.*/W', ['histogram', 'rms']),\n            ('.*/gamma', ['scalar']),\n        )\n    \"\"\"\n    collections = kwargs.pop('collections', None)\n    assert len(kwargs) == 0, \"Unknown kwargs: \" + str(kwargs)\n    ctx = get_current_tower_context()\n    if ctx is not None and not ctx.is_main_training_tower:\n        return\n\n    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    with cached_name_scope('param-summary'):\n        for p in params:\n            name = p.op.name\n            for rgx, actions in summary_lists:\n                if not rgx.endswith('$'):\n                    rgx = rgx + '$'\n                if re.match(rgx, name):\n                    add_tensor_summary(p, actions, name=name, collections=collections)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a moving average summary op to the list of tensors that can be used to maintain the EMA.", "response": "def add_moving_summary(*args, **kwargs):\n    \"\"\"\n    Summarize the moving average for scalar tensors.\n    This function is a no-op if not calling from main training tower.\n\n    Args:\n        args: scalar tensors to summarize\n        decay (float): the decay rate. Defaults to 0.95.\n        collection (str or None): the name of the collection to add EMA-maintaining ops.\n            The default will work together with the default\n            :class:`MovingAverageSummary` callback.\n        summary_collections ([str]): the names of collections to add the\n            summary op. Default is TF's default (`tf.GraphKeys.SUMMARIES`).\n\n    Returns:\n        [tf.Tensor]: list of tensors returned by assign_moving_average,\n            which can be used to maintain the EMA.\n    \"\"\"\n    decay = kwargs.pop('decay', 0.95)\n    coll = kwargs.pop('collection', MOVING_SUMMARY_OPS_KEY)\n    summ_coll = kwargs.pop('summary_collections', None)\n    assert len(kwargs) == 0, \"Unknown arguments: \" + str(kwargs)\n\n    ctx = get_current_tower_context()\n    # allow ctx to be none\n    if ctx is not None and not ctx.is_main_training_tower:\n        return []\n\n    graph = tf.get_default_graph()\n    try:\n        control_flow_ctx = graph._get_control_flow_context()\n        # XLA does not support summaries anyway\n        # However, this function will generate unnecessary dependency edges,\n        # which makes the tower function harder to compile under XLA, so we skip it\n        if control_flow_ctx is not None and control_flow_ctx.IsXLAContext():\n            return\n    except Exception:\n        pass\n\n    if tf.get_variable_scope().reuse is True:\n        logger.warn(\"add_moving_summary() called under reuse=True scope, ignored.\")\n        return []\n\n    for x in args:\n        assert isinstance(x, (tf.Tensor, tf.Variable)), x\n        assert x.get_shape().ndims == 0, \\\n            \"add_moving_summary() only accepts scalar tensor! Got one with {}\".format(x.get_shape())\n\n    ema_ops = []\n    for c in args:\n        name = re.sub('tower[0-9]+/', '', c.op.name)\n        with tf.name_scope(None):\n            if not c.dtype.is_floating:\n                c = tf.cast(c, tf.float32)\n            # assign_moving_average creates variables with op names, therefore clear ns first.\n            with _enter_vs_reuse_ns('EMA') as vs:\n                ema_var = tf.get_variable(name, shape=c.shape, dtype=c.dtype,\n                                          initializer=tf.constant_initializer(),\n                                          trainable=False)\n                ns = vs.original_name_scope\n            with tf.name_scope(ns):     # reuse VS&NS so that EMA_1 won't appear\n                ema_op = moving_averages.assign_moving_average(\n                    ema_var, c, decay,\n                    zero_debias=True, name=name + '_EMA_apply')\n            ema_ops.append(ema_op)\n        with tf.name_scope(None):\n            tf.summary.scalar(\n                name + '-summary', ema_op,\n                collections=summ_coll)    # write the EMA value as a summary\n    if coll is not None:\n        for op in ema_ops:\n            tf.add_to_collection(coll, op)\n    return ema_ops"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_head(self, proposals, stage):\n        reg_weights = tf.constant(cfg.CASCADE.BBOX_REG_WEIGHTS[stage], dtype=tf.float32)\n        pooled_feature = self.roi_func(proposals.boxes)  # N,C,S,S\n        pooled_feature = self.scale_gradient(pooled_feature)\n        head_feature = self.fastrcnn_head_func('head', pooled_feature)\n        label_logits, box_logits = fastrcnn_outputs(\n            'outputs', head_feature, self.num_classes, class_agnostic_regression=True)\n        head = FastRCNNHead(proposals, box_logits, label_logits, self.gt_boxes, reg_weights)\n\n        refined_boxes = head.decoded_output_boxes_class_agnostic()\n        refined_boxes = clip_boxes(refined_boxes, self.image_shape2d)\n        return head, tf.stop_gradient(refined_boxes, name='output_boxes')", "response": "Runs FastRCNNHead on the given set of BoxProposals."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef match_box_with_gt(self, boxes, iou_threshold):\n        if self.is_training:\n            with tf.name_scope('match_box_with_gt_{}'.format(iou_threshold)):\n                iou = pairwise_iou(boxes, self.gt_boxes)  # NxM\n                max_iou_per_box = tf.reduce_max(iou, axis=1)  # N\n                best_iou_ind = tf.argmax(iou, axis=1)  # N\n                labels_per_box = tf.gather(self.gt_labels, best_iou_ind)\n                fg_mask = max_iou_per_box >= iou_threshold\n                fg_inds_wrt_gt = tf.boolean_mask(best_iou_ind, fg_mask)\n                labels_per_box = tf.stop_gradient(labels_per_box * tf.cast(fg_mask, tf.int64))\n                return BoxProposals(boxes, labels_per_box, fg_inds_wrt_gt)\n        else:\n            return BoxProposals(boxes)", "response": "Returns a list of boxes that are greater than or equal to the given threshold."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decoded_output_boxes(self):\n        ret = self._cascade_boxes[-1]\n        ret = tf.expand_dims(ret, 1)     # class-agnostic\n        return tf.tile(ret, [1, self.num_classes, 1])", "response": "Returns the decoded output boxes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the output scores for the cascade_scores_stage.", "response": "def output_scores(self, name=None):\n        \"\"\"\n        Returns:\n            Nx#class\n        \"\"\"\n        scores = [head.output_scores('cascade_scores_stage{}'.format(idx + 1))\n                  for idx, head in enumerate(self._heads)]\n        return tf.multiply(tf.add_n(scores), (1.0 / self.num_cascade_stages), name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvisualizing some intermediate results inside the pipeline.", "response": "def do_visualize(model, model_path, nr_visualize=100, output_dir='output'):\n    \"\"\"\n    Visualize some intermediate results (proposals, raw predictions) inside the pipeline.\n    \"\"\"\n    df = get_train_dataflow()   # we don't visualize mask stuff\n    df.reset_state()\n\n    pred = OfflinePredictor(PredictConfig(\n        model=model,\n        session_init=get_model_loader(model_path),\n        input_names=['image', 'gt_boxes', 'gt_labels'],\n        output_names=[\n            'generate_{}_proposals/boxes'.format('fpn' if cfg.MODE_FPN else 'rpn'),\n            'generate_{}_proposals/scores'.format('fpn' if cfg.MODE_FPN else 'rpn'),\n            'fastrcnn_all_scores',\n            'output/boxes',\n            'output/scores',\n            'output/labels',\n        ]))\n\n    if os.path.isdir(output_dir):\n        shutil.rmtree(output_dir)\n    utils.fs.mkdir_p(output_dir)\n    with tqdm.tqdm(total=nr_visualize) as pbar:\n        for idx, dp in itertools.islice(enumerate(df), nr_visualize):\n            img, gt_boxes, gt_labels = dp['image'], dp['gt_boxes'], dp['gt_labels']\n\n            rpn_boxes, rpn_scores, all_scores, \\\n                final_boxes, final_scores, final_labels = pred(img, gt_boxes, gt_labels)\n\n            # draw groundtruth boxes\n            gt_viz = draw_annotation(img, gt_boxes, gt_labels)\n            # draw best proposals for each groundtruth, to show recall\n            proposal_viz, good_proposals_ind = draw_proposal_recall(img, rpn_boxes, rpn_scores, gt_boxes)\n            # draw the scores for the above proposals\n            score_viz = draw_predictions(img, rpn_boxes[good_proposals_ind], all_scores[good_proposals_ind])\n\n            results = [DetectionResult(*args) for args in\n                       zip(final_boxes, final_scores, final_labels,\n                           [None] * len(final_labels))]\n            final_viz = draw_final_outputs(img, results)\n\n            viz = tpviz.stack_patches([\n                gt_viz, proposal_viz,\n                score_viz, final_viz], 2, 2)\n\n            if os.environ.get('DISPLAY', None):\n                tpviz.interactive_imshow(viz)\n            cv2.imwrite(\"{}/{:03d}.png\".format(output_dir, idx), viz)\n            pbar.update()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the wrapped layer function that is registered with the given name.", "response": "def get_registered_layer(name):\n    \"\"\"\n    Args:\n        name (str): the name of the layer, e.g. 'Conv2D'\n    Returns:\n        the wrapped layer function, or None if not registered.\n    \"\"\"\n    ret = _LAYER_REGISTRY.get(name, None)\n    if ret == _NameConflict:\n        raise KeyError(\"Layer named '{}' is registered with `@layer_register` more than once!\".format(name))\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_serving(model_path):\n    pred_config = PredictConfig(\n        session_init=get_model_loader(model_path),\n        model=InferenceOnlyModel(),\n        input_names=['input_img_bytes'],\n        output_names=['prediction_img_bytes'])\n    ModelExporter(pred_config).export_serving('/tmp/exported')", "response": "Export trained model to use it in TensorFlow Serving or cloudML."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_compact(model_path):\n    pred_config = PredictConfig(\n        session_init=get_model_loader(model_path),\n        model=Model(),\n        input_names=['input_img'],\n        output_names=['prediction_img'])\n    ModelExporter(pred_config).export_compact('/tmp/compact_graph.pb')", "response": "Export trained model to use it as a frozen and pruned inference graph in\n       mobile applications."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning inference from a training model checkpoint.", "response": "def apply(model_path):\n    \"\"\"Run inference from a training model checkpoint. \"\"\"\n    pred_config = PredictConfig(\n        session_init=get_model_loader(model_path),\n        model=Model(),\n        input_names=['input_img'],\n        output_names=['prediction_img'])\n\n    pred = OfflinePredictor(pred_config)\n    img = cv2.imread('lena.png')\n    prediction = pred([img])[0]\n    cv2.imwrite('applied_default.jpg', prediction[0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning inference from a different graph which receives encoded images buffers.", "response": "def apply_inference_graph(model_path):\n    \"\"\"Run inference from a different graph, which receives encoded images buffers. \"\"\"\n    pred_config = PredictConfig(\n        session_init=get_model_loader(model_path),\n        model=InferenceOnlyModel(),\n        input_names=['input_img_bytes'],\n        output_names=['prediction_img_bytes'])\n\n    pred = OfflinePredictor(pred_config)\n    buf = open('lena.png', 'rb').read()\n    prediction = pred([buf])[0]\n    with open('applied_inference_graph.png', 'wb') as f:\n        f.write(prediction[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_compact(graph_path):\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n        # Note, we just load the graph and do *not* need to initialize anything.\n        with tf.gfile.GFile(graph_path, \"rb\") as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def)\n\n        input_img = sess.graph.get_tensor_by_name('import/input_img:0')\n        prediction_img = sess.graph.get_tensor_by_name('import/prediction_img:0')\n\n        prediction = sess.run(prediction_img, {input_img: cv2.imread('lena.png')[None, ...]})\n        cv2.imwrite('applied_compact.png', prediction[0])", "response": "Run the pruned and frozen inference graph."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SelectComponent(ds, idxs):\n    return MapData(ds, lambda dp: [dp[i] for i in idxs])", "response": "Select components from datapoints."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngather useful debug information from a datapoint.", "response": "def _analyze_input_data(self, entry, k, depth=1, max_depth=3, max_list=3):\n        \"\"\"\n        Gather useful debug information from a datapoint.\n\n        Args:\n            entry: the datapoint component\n            k (int): index of this component in current datapoint\n            depth (int, optional): recursion depth\n            max_depth, max_list: same as in :meth:`__init__`.\n\n        Returns:\n            string: debug message\n        \"\"\"\n\n        class _elementInfo(object):\n            def __init__(self, el, pos, depth=0, max_list=3):\n                self.shape = \"\"\n                self.type = type(el).__name__\n                self.dtype = \"\"\n                self.range = \"\"\n\n                self.sub_elements = []\n\n                self.ident = \" \" * (depth * 2)\n                self.pos = pos\n\n                numpy_scalar_types = list(itertools.chain(*np.sctypes.values()))\n\n                if isinstance(el, (int, float, bool)):\n                    self.range = \" with value {}\".format(el)\n                elif type(el) is np.ndarray:\n                    self.shape = \" of shape {}\".format(el.shape)\n                    self.dtype = \":{}\".format(str(el.dtype))\n                    self.range = \" in range [{}, {}]\".format(el.min(), el.max())\n                elif type(el) in numpy_scalar_types:\n                    self.range = \" with value {}\".format(el)\n                elif isinstance(el, (list)):\n                    self.shape = \" of len {}\".format(len(el))\n\n                    if depth < max_depth:\n                        for k, subel in enumerate(el):\n                            if k < max_list:\n                                self.sub_elements.append(_elementInfo(subel, k, depth + 1, max_list))\n                            else:\n                                self.sub_elements.append(\" \" * ((depth + 1) * 2) + '...')\n                                break\n                    else:\n                        if len(el) > 0:\n                            self.sub_elements.append(\" \" * ((depth + 1) * 2) + ' ...')\n\n            def __str__(self):\n                strings = []\n                vals = (self.ident, self.pos, self.type, self.dtype, self.shape, self.range)\n                strings.append(\"{}{}: {}{}{}{}\".format(*vals))\n\n                for k, el in enumerate(self.sub_elements):\n                    strings.append(str(el))\n                return \"\\n\".join(strings)\n\n        return str(_elementInfo(entry, k, depth, max_list))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef feed(self, count, total=1):\n        self._tot += total\n        self._cnt += count", "response": "Feeds the specified number of events into the internal cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfeeding the set of entries in the logarithmic log.", "response": "def feed(self, pred, label):\n        \"\"\"\n        Args:\n            pred (np.ndarray): binary array.\n            label (np.ndarray): binary array of the same size.\n        \"\"\"\n        assert pred.shape == label.shape, \"{} != {}\".format(pred.shape, label.shape)\n        self.nr_pos += (label == 1).sum()\n        self.nr_neg += (label == 0).sum()\n        self.nr_pred_pos += (pred == 1).sum()\n        self.nr_pred_neg += (pred == 0).sum()\n        self.corr_pos += ((pred == 1) & (pred == label)).sum()\n        self.corr_neg += ((pred == 0) & (pred == label)).sum()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap around optimizers to apply gradient processors.", "response": "def apply_grad_processors(opt, gradprocs):\n    \"\"\"\n    Wrapper around optimizers to apply gradient processors.\n\n    Args:\n        opt (tf.train.Optimizer):\n        gradprocs (list[GradientProcessor]): gradient processors to add to the\n            optimizer.\n\n    Returns:\n        a :class:`tf.train.Optimizer` instance which runs the gradient\n        processors before updating the variables.\n    \"\"\"\n    assert isinstance(gradprocs, (list, tuple)), gradprocs\n    for gp in gradprocs:\n        assert isinstance(gp, GradientProcessor), gp\n\n    class _ApplyGradientProcessor(ProxyOptimizer):\n        def __init__(self, opt, gradprocs):\n            self._gradprocs = gradprocs[:]\n            super(_ApplyGradientProcessor, self).__init__(opt)\n\n        def apply_gradients(self, grads_and_vars,\n                            global_step=None, name=None):\n            g = self._apply(grads_and_vars)\n            return self._opt.apply_gradients(g, global_step, name)\n\n        def _apply(self, g):\n            for proc in self._gradprocs:\n                g = proc.process(g)\n            return g\n\n    return _ApplyGradientProcessor(opt, gradprocs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npastes a mask into a new image of size h.", "response": "def _paste_mask(box, mask, shape):\n    \"\"\"\n    Args:\n        box: 4 float\n        mask: MxM floats\n        shape: h,w\n    Returns:\n        A uint8 binary image of hxw.\n    \"\"\"\n    # int() is floor\n    # box fpcoor=0.0 -> intcoor=0.0\n    x0, y0 = list(map(int, box[:2] + 0.5))\n    # box fpcoor=h -> intcoor=h-1, inclusive\n    x1, y1 = list(map(int, box[2:] - 0.5))    # inclusive\n    x1 = max(x0, x1)    # require at least 1x1\n    y1 = max(y0, y1)\n\n    w = x1 + 1 - x0\n    h = y1 + 1 - y0\n\n    # rounding errors could happen here, because masks were not originally computed for this shape.\n    # but it's hard to do better, because the network does not know the \"original\" scale\n    mask = (cv2.resize(mask, (w, h)) > 0.5).astype('uint8')\n    ret = np.zeros(shape, dtype='uint8')\n    ret[y0:y1 + 1, x0:x1 + 1] = mask\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict_image(img, model_func):\n\n    orig_shape = img.shape[:2]\n    resizer = CustomResize(cfg.PREPROC.TEST_SHORT_EDGE_SIZE, cfg.PREPROC.MAX_SIZE)\n    resized_img = resizer.augment(img)\n    scale = np.sqrt(resized_img.shape[0] * 1.0 / img.shape[0] * resized_img.shape[1] / img.shape[1])\n    boxes, probs, labels, *masks = model_func(resized_img)\n    boxes = boxes / scale\n    # boxes are already clipped inside the graph, but after the floating point scaling, this may not be true any more.\n    boxes = clip_boxes(boxes, orig_shape)\n\n    if masks:\n        # has mask\n        full_masks = [_paste_mask(box, mask, orig_shape)\n                      for box, mask in zip(boxes, masks[0])]\n        masks = full_masks\n    else:\n        # fill with none\n        masks = [None] * len(boxes)\n\n    results = [DetectionResult(*args) for args in zip(boxes, probs, labels.tolist(), masks)]\n    return results", "response": "Run detection on one image using the TF callable."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef predict_dataflow(df, model_func, tqdm_bar=None):\n    df.reset_state()\n    all_results = []\n    with ExitStack() as stack:\n        # tqdm is not quite thread-safe: https://github.com/tqdm/tqdm/issues/323\n        if tqdm_bar is None:\n            tqdm_bar = stack.enter_context(get_tqdm(total=df.size()))\n        for img, img_id in df:\n            results = predict_image(img, model_func)\n            for r in results:\n                # int()/float() to make it json-serializable\n                res = {\n                    'image_id': img_id,\n                    'category_id': int(r.class_id),\n                    'bbox': [round(float(x), 4) for x in r.box],\n                    'score': round(float(r.score), 4),\n                }\n\n                # also append segmentation to results\n                if r.mask is not None:\n                    rle = cocomask.encode(\n                        np.array(r.mask[:, :, None], order='F'))[0]\n                    rle['counts'] = rle['counts'].decode('ascii')\n                    res['segmentation'] = rle\n                all_results.append(res)\n            tqdm_bar.update(1)\n    return all_results", "response": "Predicts the dataflow and returns a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef multithread_predict_dataflow(dataflows, model_funcs):\n    num_worker = len(model_funcs)\n    assert len(dataflows) == num_worker\n    if num_worker == 1:\n        return predict_dataflow(dataflows[0], model_funcs[0])\n    kwargs = {'thread_name_prefix': 'EvalWorker'} if sys.version_info.minor >= 6 else {}\n    with ThreadPoolExecutor(max_workers=num_worker, **kwargs) as executor, \\\n            tqdm.tqdm(total=sum([df.size() for df in dataflows])) as pbar:\n        futures = []\n        for dataflow, pred in zip(dataflows, model_funcs):\n            futures.append(executor.submit(predict_dataflow, dataflow, pred, pbar))\n        all_results = list(itertools.chain(*[fut.result() for fut in futures]))\n        return all_results", "response": "Running multiple predict_dataflow in multiple threads and aggregate the results."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef batch_flatten(x):\n    shape = x.get_shape().as_list()[1:]\n    if None not in shape:\n        return tf.reshape(x, [-1, int(np.prod(shape))])\n    return tf.reshape(x, tf.stack([tf.shape(x)[0], -1]))", "response": "Flatten the tensor except the first dimension."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef FullyConnected(\n        inputs,\n        units,\n        activation=None,\n        use_bias=True,\n        kernel_initializer=None,\n        bias_initializer=tf.zeros_initializer(),\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None):\n    \"\"\"\n    A wrapper around `tf.layers.Dense`.\n    One difference to maintain backward-compatibility:\n    Default weight initializer is variance_scaling_initializer(2.0).\n\n    Variable Names:\n\n    * ``W``: weights of shape [in_dim, out_dim]\n    * ``b``: bias\n    \"\"\"\n    if kernel_initializer is None:\n        if get_tf_version_tuple() <= (1, 12):\n            kernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0)\n        else:\n            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')\n\n    inputs = batch_flatten(inputs)\n    with rename_get_variable({'kernel': 'W', 'bias': 'b'}):\n        layer = tf.layers.Dense(\n            units=units,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            _reuse=tf.get_variable_scope().reuse)\n        ret = layer.apply(inputs, scope=tf.get_variable_scope())\n        ret = tf.identity(ret, name='output')\n\n    ret.variables = VariableHolder(W=layer.kernel)\n    if use_bias:\n        ret.variables.b = layer.bias\n    return ret", "response": "A fully connected version of the ARIMA."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the CUDA_VISIBLE_DEVICES environment variable.", "response": "def _init_runtime(self):\n        \"\"\" Call _init_runtime under different CUDA_VISIBLE_DEVICES, you'll\n            have workers that run on multiGPUs\n        \"\"\"\n        if self.idx != 0:\n            from tensorpack.models.registry import disable_layer_logging\n            disable_layer_logging()\n        self.predictor = OfflinePredictor(self.config)\n        if self.idx == 0:\n            with self.predictor.graph.as_default():\n                describe_trainable_vars()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fetch_batch(self):\n        inp, f = self.queue.get()\n        nr_input_var = len(inp)\n        batched, futures = [[] for _ in range(nr_input_var)], []\n        for k in range(nr_input_var):\n            batched[k].append(inp[k])\n        futures.append(f)\n        while len(futures) < self.batch_size:\n            try:\n                inp, f = self.queue.get_nowait()\n                for k in range(nr_input_var):\n                    batched[k].append(inp[k])\n                futures.append(f)\n            except queue.Empty:\n                break   # do not wait\n\n        for k in range(nr_input_var):\n            batched[k] = np.asarray(batched[k])\n        return batched, futures", "response": "Fetch a batch of data without waiting"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nput a task to the input queue.", "response": "def put_task(self, dp, callback=None):\n        \"\"\"\n        Same as in :meth:`AsyncPredictorBase.put_task`.\n        \"\"\"\n        f = Future()\n        if callback is not None:\n            f.add_done_callback(callback)\n        self.input_queue.put((dp, f))\n        return f"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loads_msgpack(buf):\n    # Since 0.6, the default max size was set to 1MB.\n    # We change it to approximately 1G.\n    return msgpack.loads(buf, raw=False,\n                         max_bin_len=MAX_MSGPACK_LEN,\n                         max_array_len=MAX_MSGPACK_LEN,\n                         max_map_len=MAX_MSGPACK_LEN,\n                         max_str_len=MAX_MSGPACK_LEN)", "response": "loads a messagepack object into a new object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbatch Renormalization layer, as described in the paper: `Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models <https://arxiv.org/abs/1702.03275>`_. This implementation is a wrapper around `tf.layers.batch_normalization`. Args: x (tf.Tensor): a NHWC or NC tensor. rmax, dmax (tf.Tensor): a scalar tensor, the maximum allowed corrections. decay (float): decay rate of moving average. epsilon (float): epsilon to avoid divide-by-zero. use_scale, use_bias (bool): whether to use the extra affine transformation or not. Returns: tf.Tensor: a tensor named ``output`` with the same shape of x. Variable Names: * ``beta``: the bias term. * ``gamma``: the scale term. Input will be transformed by ``x * gamma + beta``. * ``moving_mean, renorm_mean, renorm_mean_weight``: See TF documentation. * ``moving_variance, renorm_stddev, renorm_stddev_weight``: See TF documentation.", "response": "def BatchRenorm(x, rmax, dmax, momentum=0.9, epsilon=1e-5,\n                center=True, scale=True, gamma_initializer=None,\n                data_format='channels_last'):\n    \"\"\"\n    Batch Renormalization layer, as described in the paper:\n    `Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models\n    <https://arxiv.org/abs/1702.03275>`_.\n    This implementation is a wrapper around `tf.layers.batch_normalization`.\n\n    Args:\n        x (tf.Tensor): a NHWC or NC tensor.\n        rmax, dmax (tf.Tensor): a scalar tensor, the maximum allowed corrections.\n        decay (float): decay rate of moving average.\n        epsilon (float): epsilon to avoid divide-by-zero.\n        use_scale, use_bias (bool): whether to use the extra affine transformation or not.\n\n    Returns:\n        tf.Tensor: a tensor named ``output`` with the same shape of x.\n\n    Variable Names:\n\n    * ``beta``: the bias term.\n    * ``gamma``: the scale term. Input will be transformed by ``x * gamma + beta``.\n    * ``moving_mean, renorm_mean, renorm_mean_weight``: See TF documentation.\n    * ``moving_variance, renorm_stddev, renorm_stddev_weight``: See TF documentation.\n    \"\"\"\n\n    shape = x.get_shape().as_list()\n    ndims = len(shape)\n    assert ndims in [2, 4]\n    if ndims == 2:\n        data_format = 'channels_first'\n\n    ctx = get_current_tower_context()\n    coll_bk = backup_collection([tf.GraphKeys.UPDATE_OPS])\n    layer = tf.layers.BatchNormalization(\n        axis=1 if data_format == 'channels_first' else 3,\n        momentum=momentum, epsilon=epsilon,\n        center=center, scale=scale,\n        renorm=True,\n        renorm_clipping={\n            'rmin': 1.0 / rmax,\n            'rmax': rmax,\n            'dmax': dmax},\n        renorm_momentum=0.99,\n        gamma_initializer=gamma_initializer,\n        fused=False,\n        _reuse=tf.get_variable_scope().reuse)\n    xn = layer.apply(x, training=ctx.is_training, scope=tf.get_variable_scope())\n\n    if ctx.is_main_training_tower:\n        for v in layer.non_trainable_variables:\n            if isinstance(v, tf.Variable):\n                tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, v)\n    else:\n        # only run UPDATE_OPS in the first tower\n        restore_collection(coll_bk)\n\n    if ndims == 2:\n        xn = tf.squeeze(xn, [1, 2])\n    ret = tf.identity(xn, name='output')\n\n    # TODO not sure whether to add moving_mean/moving_var to VH now\n    vh = ret.variables = VariableHolder()\n    if scale:\n        vh.gamma = layer.gamma\n    if center:\n        vh.beta = layer.beta\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an image generated from z", "response": "def generator(self, z):\n        \"\"\" return an image generated from z\"\"\"\n        nf = 64\n        l = FullyConnected('fc0', z, nf * 8 * 4 * 4, activation=tf.identity)\n        l = tf.reshape(l, [-1, 4, 4, nf * 8])\n        l = BNReLU(l)\n        with argscope(Conv2DTranspose, activation=BNReLU, kernel_size=4, strides=2):\n            l = Conv2DTranspose('deconv1', l, nf * 4)\n            l = Conv2DTranspose('deconv2', l, nf * 2)\n            l = Conv2DTranspose('deconv3', l, nf)\n            l = Conv2DTranspose('deconv4', l, 3, activation=tf.identity)\n            l = tf.tanh(l, name='gen')\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef discriminator(self, imgs):\n        nf = 64\n        with argscope(Conv2D, kernel_size=4, strides=2):\n            l = (LinearWrap(imgs)\n                 .Conv2D('conv0', nf, activation=tf.nn.leaky_relu)\n                 .Conv2D('conv1', nf * 2)\n                 .BatchNorm('bn1')\n                 .tf.nn.leaky_relu()\n                 .Conv2D('conv2', nf * 4)\n                 .BatchNorm('bn2')\n                 .tf.nn.leaky_relu()\n                 .Conv2D('conv3', nf * 8)\n                 .BatchNorm('bn3')\n                 .tf.nn.leaky_relu()\n                 .FullyConnected('fct', 1)())\n        return l", "response": "a ( b 1 ) logits"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing pairwise intersection areas between boxes.", "response": "def pairwise_intersection(boxlist1, boxlist2):\n    \"\"\"Compute pairwise intersection areas between boxes.\n\n    Args:\n      boxlist1: Nx4 floatbox\n      boxlist2: Mx4\n\n    Returns:\n      a tensor with shape [N, M] representing pairwise intersections\n    \"\"\"\n    x_min1, y_min1, x_max1, y_max1 = tf.split(boxlist1, 4, axis=1)\n    x_min2, y_min2, x_max2, y_max2 = tf.split(boxlist2, 4, axis=1)\n    all_pairs_min_ymax = tf.minimum(y_max1, tf.transpose(y_max2))\n    all_pairs_max_ymin = tf.maximum(y_min1, tf.transpose(y_min2))\n    intersect_heights = tf.maximum(0.0, all_pairs_min_ymax - all_pairs_max_ymin)\n    all_pairs_min_xmax = tf.minimum(x_max1, tf.transpose(x_max2))\n    all_pairs_max_xmin = tf.maximum(x_min1, tf.transpose(x_min2))\n    intersect_widths = tf.maximum(0.0, all_pairs_min_xmax - all_pairs_max_xmin)\n    return intersect_heights * intersect_widths"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing pairwise intersection - over - union between two box collections.", "response": "def pairwise_iou(boxlist1, boxlist2):\n    \"\"\"Computes pairwise intersection-over-union between box collections.\n\n    Args:\n      boxlist1: Nx4 floatbox\n      boxlist2: Mx4\n\n    Returns:\n      a tensor with shape [N, M] representing pairwise iou scores.\n    \"\"\"\n    intersections = pairwise_intersection(boxlist1, boxlist2)\n    areas1 = area(boxlist1)\n    areas2 = area(boxlist2)\n    unions = (\n        tf.expand_dims(areas1, 1) + tf.expand_dims(areas2, 0) - intersections)\n    return tf.where(\n        tf.equal(intersections, 0.0),\n        tf.zeros_like(intersections), tf.truediv(intersections, unions))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample(path, start, length):\n    # initialize vocabulary and sequence length\n    param.seq_len = 1\n    ds = CharRNNData(param.corpus, 100000)\n\n    pred = OfflinePredictor(PredictConfig(\n        model=Model(),\n        session_init=SaverRestore(path),\n        input_names=['input', 'c0', 'h0', 'c1', 'h1'],\n        output_names=['prob', 'last_state']))\n\n    # feed the starting sentence\n    initial = np.zeros((1, param.rnn_size))\n    for c in start[:-1]:\n        x = np.array([[ds.char2idx[c]]], dtype='int32')\n        _, state = pred(x, initial, initial, initial, initial)\n\n    def pick(prob):\n        t = np.cumsum(prob)\n        s = np.sum(prob)\n        return(int(np.searchsorted(t, np.random.rand(1) * s)))\n\n    # generate more\n    ret = start\n    c = start[-1]\n    for k in range(length):\n        x = np.array([[ds.char2idx[c]]], dtype='int32')\n        prob, state = pred(x, state[0, 0], state[0, 1], state[1, 0], state[1, 1])\n        c = ds.chars[pick(prob[0])]\n        ret += c\n    print(ret)", "response": "Generate a random set of random characters from the model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef backbone_scope(freeze):\n    def nonlin(x):\n        x = get_norm()(x)\n        return tf.nn.relu(x)\n\n    with argscope([Conv2D, MaxPooling, BatchNorm], data_format='channels_first'), \\\n            argscope(Conv2D, use_bias=False, activation=nonlin,\n                     kernel_initializer=tf.variance_scaling_initializer(\n                         scale=2.0, mode='fan_out')), \\\n            ExitStack() as stack:\n        if cfg.BACKBONE.NORM in ['FreezeBN', 'SyncBN']:\n            if freeze or cfg.BACKBONE.NORM == 'FreezeBN':\n                stack.enter_context(argscope(BatchNorm, training=False))\n            else:\n                stack.enter_context(argscope(\n                    BatchNorm, sync_statistics='nccl' if cfg.TRAINER == 'replicated' else 'horovod'))\n\n        if freeze:\n            stack.enter_context(freeze_variables(stop_gradient=False, skip_collection=True))\n        else:\n            # the layers are not completely freezed, but we may want to only freeze the affine\n            if cfg.BACKBONE.FREEZE_AFFINE:\n                stack.enter_context(custom_getter_scope(freeze_affine_getter))\n        yield", "response": "A context manager that creates a scope for the backbone of the given language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_images(filename):\n    with gzip.open(filename) as bytestream:\n        magic = _read32(bytestream)\n        if magic != 2051:\n            raise ValueError(\n                'Invalid magic number %d in MNIST image file: %s' %\n                (magic, filename))\n        num_images = _read32(bytestream)\n        rows = _read32(bytestream)\n        cols = _read32(bytestream)\n        buf = bytestream.read(rows * cols * num_images)\n        data = numpy.frombuffer(buf, dtype=numpy.uint8)\n        data = data.reshape(num_images, rows, cols, 1)\n        data = data.astype('float32') / 255.0\n        return data", "response": "Extract the images into a 4D uint8 numpy array [ index y x depth ]."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_labels(filename):\n    with gzip.open(filename) as bytestream:\n        magic = _read32(bytestream)\n        if magic != 2049:\n            raise ValueError(\n                'Invalid magic number %d in MNIST label file: %s' %\n                (magic, filename))\n        num_items = _read32(bytestream)\n        buf = bytestream.read(num_items)\n        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n        return labels", "response": "Extract the labels into a 1D uint8 numpy array [ index ]."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_dummy_class(klass, dependency):\n    assert not building_rtfd()\n\n    class _DummyMetaClass(type):\n        # throw error on class attribute access\n        def __getattr__(_, __):\n            raise AttributeError(\"Cannot import '{}', therefore '{}' is not available\".format(dependency, klass))\n\n    @six.add_metaclass(_DummyMetaClass)\n    class _Dummy(object):\n        # throw error on constructor\n        def __init__(self, *args, **kwargs):\n            raise ImportError(\"Cannot import '{}', therefore '{}' is not available\".format(dependency, klass))\n\n    return _Dummy", "response": "Create a dummy class which throws ImportError when used."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_dummy_func(func, dependency):\n    assert not building_rtfd()\n\n    if isinstance(dependency, (list, tuple)):\n        dependency = ','.join(dependency)\n\n    def _dummy(*args, **kwargs):\n        raise ImportError(\"Cannot import '{}', therefore '{}' is not available\".format(dependency, func))\n    return _dummy", "response": "Create a dummy function which throws ImportError when used."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs a deprecation warning.", "response": "def log_deprecated(name=\"\", text=\"\", eos=\"\"):\n    \"\"\"\n    Log deprecation warning.\n\n    Args:\n        name (str): name of the deprecated item.\n        text (str, optional): information about the deprecation.\n        eos (str, optional): end of service date such as \"YYYY-MM-DD\".\n    \"\"\"\n    assert name or text\n    if eos:\n        eos = \"after \" + datetime(*map(int, eos.split(\"-\"))).strftime(\"%d %b\")\n    if name:\n        if eos:\n            warn_msg = \"%s will be deprecated %s. %s\" % (name, eos, text)\n        else:\n            warn_msg = \"%s was deprecated. %s\" % (name, text)\n    else:\n        warn_msg = text\n        if eos:\n            warn_msg += \" Legacy period ends %s\" % eos\n    logger.warn(\"[Deprecated] \" + warn_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear the queue then call dataflow. __iter__ again and fill into the queue.", "response": "def refill_queue(self):\n        \"\"\"\n        Clear the queue, then call dataflow.__iter__() again and fill into the queue.\n        \"\"\"\n        self.thread.pause()     # pause enqueue\n\n        opt = tfv1.RunOptions()\n        opt.timeout_in_ms = 2000   # 2s\n        sess = tfv1.get_default_session()\n        # dequeue until empty\n        try:\n            while True:\n                sess.run(self._dequeue_op, options=opt)\n        except tf.errors.DeadlineExceededError:\n            pass\n\n        # reset dataflow, start thread\n        self.thread.reinitialize_dataflow()\n        self.thread.resume()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_ema_callback(self):\n        with self.cached_name_scope():\n            # in TF there is no API to get queue capacity, so we can only summary the size\n            size = tf.cast(self.queue.size(), tf.float32, name='queue_size')\n        size_ema_op = add_moving_summary(size, collection=None, decay=0.5)[0].op\n        ret = RunOp(\n            lambda: size_ema_op,\n            run_before=False,\n            run_as_trigger=False,\n            run_step=True)\n        ret.name_scope = \"InputSource/EMA\"\n        return ret", "response": "Create a hook - only callback which maintain the EMA of the queue size."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting up the queue for CPU prefetching.", "response": "def _setup(self, inputs):\n        logger.info(\"Setting up the queue for CPU prefetching ...\")\n        self.input_placehdrs = [build_or_reuse_placeholder(v) for v in inputs]\n        assert len(self.input_placehdrs) > 0, \\\n            \"BatchQueueInput has to be used with some input signature!\"\n\n        # prepare placeholders without the first dimension\n        placehdrs_nobatch = []\n        for p in self.input_placehdrs:\n            placehdrs_nobatch.append(tfv1.placeholder(\n                dtype=p.dtype, shape=p.get_shape().as_list()[1:],\n                name=get_op_tensor_name(p.name)[0] + '-nobatch'))\n\n        # dequeue_many requires fully-defined shapes\n        shape_err = \"Use of BatchQueueInput requires inputs to have fully-defined \"\n        \"shapes except for the batch dimension\"\n        shapes = []\n        for p in placehdrs_nobatch:\n            assert p.get_shape().is_fully_defined(), shape_err\n            shapes.append(p.get_shape())\n\n        with self.cached_name_scope():\n            if self.queue is None:\n                self.queue = tf.FIFOQueue(\n                    3000, [x.dtype for x in self.input_placehdrs],\n                    shapes=shapes,\n                    name='input_queue')\n            for shp in self.queue.shapes:\n                assert shp.is_fully_defined(), shape_err\n\n            self.thread = EnqueueThread(self.queue, self._inf_ds, placehdrs_nobatch)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps a dataflow to tf. data. Dataset.", "response": "def dataflow_to_dataset(df, types):\n        \"\"\"\n        Wrap a dataflow to tf.data.Dataset.\n        This function will also reset the dataflow.\n\n        If the dataflow itself is finite, the returned dataset is also finite.\n        Therefore, if used for training, you'll need to add `.repeat()` on the returned\n        dataset.\n\n        Args:\n            df (DataFlow): a dataflow which produces lists\n            types([tf.DType]): list of types\n\n        Returns:\n            (tf.data.Dataset)\n        \"\"\"\n        # TODO theoretically it can support dict\n        assert isinstance(df, DataFlow), df\n        assert isinstance(types, (list, tuple)), types\n        df = MapData(df, lambda dp: tuple(dp))\n        df.reset_state()\n        ds = tf.data.Dataset.from_generator(\n            df.get_data, tuple(types))\n        return ds"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the nth predictor on the nth tower.", "response": "def get_predictor(self, n):\n        \"\"\"\n        Returns:\n            OnlinePredictor: the nth predictor on the nth tower.\n        \"\"\"\n        l = len(self.predictors)\n        if n >= l:\n            logger.warn(\"n > #towers, will assign predictor to GPU by round-robin\")\n        return [self.predictors[k % l] for k in range(n)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes pairwise intersection areas between boxes.", "response": "def intersection(boxes1, boxes2):\n  \"\"\"Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes\n    boxes2: a numpy array with shape [M, 4] holding M boxes\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  \"\"\"\n  [y_min1, x_min1, y_max1, x_max1] = np.split(boxes1, 4, axis=1)\n  [y_min2, x_min2, y_max2, x_max2] = np.split(boxes2, 4, axis=1)\n\n  all_pairs_min_ymax = np.minimum(y_max1, np.transpose(y_max2))\n  all_pairs_max_ymin = np.maximum(y_min1, np.transpose(y_min2))\n  intersect_heights = np.maximum(\n      np.zeros(all_pairs_max_ymin.shape, dtype='f4'),\n      all_pairs_min_ymax - all_pairs_max_ymin)\n  all_pairs_min_xmax = np.minimum(x_max1, np.transpose(x_max2))\n  all_pairs_max_xmin = np.maximum(x_min1, np.transpose(x_min2))\n  intersect_widths = np.maximum(\n      np.zeros(all_pairs_max_xmin.shape, dtype='f4'),\n      all_pairs_min_xmax - all_pairs_max_xmin)\n  return intersect_heights * intersect_widths"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes pairwise intersection - over - union between box collections.", "response": "def iou(boxes1, boxes2):\n  \"\"\"Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding M boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  \"\"\"\n  intersect = intersection(boxes1, boxes2)\n  area1 = area(boxes1)\n  area2 = area(boxes2)\n  union = np.expand_dims(area1, axis=1) + np.expand_dims(\n      area2, axis=0) - intersect\n  return intersect / union"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing pairwise intersection - over - area between two boxes.", "response": "def ioa(boxes1, boxes2):\n  \"\"\"Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2's area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  \"\"\"\n  intersect = intersection(boxes1, boxes2)\n  inv_areas = np.expand_dims(1.0 / area(boxes2), axis=0)\n  return intersect * inv_areas"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads the data from Marlin s website unless it s already here.", "response": "def maybe_download(url, work_directory):\n    \"\"\"Download the data from Marlin's website, unless it's already here.\"\"\"\n    filename = url.split(\"/\")[-1]\n    filepath = os.path.join(work_directory, filename)\n    if not os.path.exists(filepath):\n        logger.info(\"Downloading to {}...\".format(filepath))\n        download(url, work_directory)\n    return filepath"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dict of all the synset IDs in the current directory.", "response": "def get_synset_1000(self):\n        \"\"\"\n        Returns:\n            dict: {cls_number: synset_id}\n        \"\"\"\n        fname = os.path.join(self.dir, 'synsets.txt')\n        assert os.path.isfile(fname)\n        lines = [x.strip() for x in open(fname).readlines()]\n        return dict(enumerate(lines))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of images for the given name and dir_structure.", "response": "def get_image_list(self, name, dir_structure='original'):\n        \"\"\"\n        Args:\n            name (str): 'train' or 'val' or 'test'\n            dir_structure (str): same as in :meth:`ILSVRC12.__init__()`.\n        Returns:\n            list: list of (image filename, label)\n        \"\"\"\n        assert name in ['train', 'val', 'test']\n        assert dir_structure in ['original', 'train']\n        add_label_to_fname = (name != 'train' and dir_structure != 'original')\n        if add_label_to_fname:\n            synset = self.get_synset_1000()\n\n        fname = os.path.join(self.dir, name + '.txt')\n        assert os.path.isfile(fname), fname\n        with open(fname) as f:\n            ret = []\n            for line in f.readlines():\n                name, cls = line.strip().split()\n                cls = int(cls)\n\n                if add_label_to_fname:\n                    name = os.path.join(synset[cls], name)\n\n                ret.append((name.strip(), cls))\n        assert len(ret), fname\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_per_pixel_mean(self, size=None):\n        if self.caffepb is None:\n            self.caffepb = get_caffe_pb()\n        obj = self.caffepb.BlobProto()\n\n        mean_file = os.path.join(self.dir, 'imagenet_mean.binaryproto')\n        with open(mean_file, 'rb') as f:\n            obj.ParseFromString(f.read())\n        arr = np.array(obj.data).reshape((3, 256, 256)).astype('float32')\n        arr = np.transpose(arr, [1, 2, 0])\n        if size is not None:\n            arr = cv2.resize(arr, size[::-1])\n        return arr", "response": "Returns the per - pixel mean of the caffe."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef guess_dir_structure(dir):\n        subdir = os.listdir(dir)[0]\n        # find a subdir starting with 'n'\n        if subdir.startswith('n') and \\\n                os.path.isdir(os.path.join(dir, subdir)):\n            dir_structure = 'train'\n        else:\n            dir_structure = 'original'\n        logger.info(\n            \"[ILSVRC12] Assuming directory {} has '{}' structure.\".format(\n                dir, dir_structure))\n        return dir_structure", "response": "Guesses the directory structure of dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_coco_metrics(self, json_file):\n        from pycocotools.cocoeval import COCOeval\n        ret = {}\n        cocoDt = self.coco.loadRes(json_file)\n        cocoEval = COCOeval(self.coco, cocoDt, 'bbox')\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()\n        fields = ['IoU=0.5:0.95', 'IoU=0.5', 'IoU=0.75', 'small', 'medium', 'large']\n        for k in range(6):\n            ret['mAP(bbox)/' + fields[k]] = cocoEval.stats[k]\n\n        json_obj = json.load(open(json_file))\n        if len(json_obj) > 0 and 'segmentation' in json_obj[0]:\n            cocoEval = COCOeval(self.coco, cocoDt, 'segm')\n            cocoEval.evaluate()\n            cocoEval.accumulate()\n            cocoEval.summarize()\n            for k in range(6):\n                ret['mAP(segm)/' + fields[k]] = cocoEval.stats[k]\n        return ret", "response": "This function returns the evaluation metrics for the specified coco format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self, add_gt=True, add_mask=False):\n        if add_mask:\n            assert add_gt\n        with timed_operation('Load Groundtruth Boxes for {}'.format(self.name)):\n            img_ids = self.coco.getImgIds()\n            img_ids.sort()\n            # list of dict, each has keys: height,width,id,file_name\n            imgs = self.coco.loadImgs(img_ids)\n\n            for img in tqdm.tqdm(imgs):\n                img['image_id'] = img.pop('id')\n                self._use_absolute_file_name(img)\n                if add_gt:\n                    self._add_detection_gt(img, add_mask)\n            return imgs", "response": "Loads the images from the Crowd and Crowd archive into a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _use_absolute_file_name(self, img):\n        img['file_name'] = os.path.join(\n            self._imgdir, img['file_name'])\n        assert os.path.isfile(img['file_name']), img['file_name']", "response": "Change absolute filename to abosolute file name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds detection boxes class is_crowd and segmentation in coco poly format.", "response": "def _add_detection_gt(self, img, add_mask):\n        \"\"\"\n        Add 'boxes', 'class', 'is_crowd' of this image to the dict, used by detection.\n        If add_mask is True, also add 'segmentation' in coco poly format.\n        \"\"\"\n        # ann_ids = self.coco.getAnnIds(imgIds=img['image_id'])\n        # objs = self.coco.loadAnns(ann_ids)\n        objs = self.coco.imgToAnns[img['image_id']]  # equivalent but faster than the above two lines\n\n        # clean-up boxes\n        valid_objs = []\n        width = img.pop('width')\n        height = img.pop('height')\n        for objid, obj in enumerate(objs):\n            if obj.get('ignore', 0) == 1:\n                continue\n            x1, y1, w, h = obj['bbox']\n            # bbox is originally in float\n            # x1/y1 means upper-left corner and w/h means true w/h. This can be verified by segmentation pixels.\n            # But we do make an assumption here that (0.0, 0.0) is upper-left corner of the first pixel\n\n            x1 = np.clip(float(x1), 0, width)\n            y1 = np.clip(float(y1), 0, height)\n            w = np.clip(float(x1 + w), 0, width) - x1\n            h = np.clip(float(y1 + h), 0, height) - y1\n            # Require non-zero seg area and more than 1x1 box size\n            if obj['area'] > 1 and w > 0 and h > 0 and w * h >= 4:\n                obj['bbox'] = [x1, y1, x1 + w, y1 + h]\n                valid_objs.append(obj)\n\n                if add_mask:\n                    segs = obj['segmentation']\n                    if not isinstance(segs, list):\n                        assert obj['iscrowd'] == 1\n                        obj['segmentation'] = None\n                    else:\n                        valid_segs = [np.asarray(p).reshape(-1, 2).astype('float32') for p in segs if len(p) >= 6]\n                        if len(valid_segs) == 0:\n                            logger.error(\"Object {} in image {} has no valid polygons!\".format(objid, img['file_name']))\n                        elif len(valid_segs) < len(segs):\n                            logger.warn(\"Object {} in image {} has invalid polygons!\".format(objid, img['file_name']))\n\n                        obj['segmentation'] = valid_segs\n\n        # all geometrically-valid boxes are returned\n        boxes = np.asarray([obj['bbox'] for obj in valid_objs], dtype='float32')  # (n, 4)\n        cls = np.asarray([\n            self.COCO_id_to_category_id[obj['category_id']]\n            for obj in valid_objs], dtype='int32')  # (n,)\n        is_crowd = np.asarray([obj['iscrowd'] for obj in valid_objs], dtype='int8')\n\n        # add the keys\n        img['boxes'] = boxes        # nx4\n        img['class'] = cls          # n, always >0\n        img['is_crowd'] = is_crowd  # n,\n        if add_mask:\n            # also required to be float32\n            img['segmentation'] = [\n                obj['segmentation'] for obj in valid_objs]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_many(basedir, names, add_gt=True, add_mask=False):\n        if not isinstance(names, (list, tuple)):\n            names = [names]\n        ret = []\n        for n in names:\n            coco = COCODetection(basedir, n)\n            ret.extend(coco.load(add_gt, add_mask=add_mask))\n        return ret", "response": "Load and merges several instance files together."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_training_roidbs(self, names):\n        return COCODetection.load_many(\n            cfg.DATA.BASEDIR, names, add_gt=True, add_mask=cfg.MODE_MASK)", "response": "Load training roidbs for a list of training datasets."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_inference_roidbs(self, name):\n        return COCODetection.load_many(cfg.DATA.BASEDIR, name, add_gt=False)", "response": "Loads all roidbs for a single inference dataset"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef eval_or_save_inference_results(self, results, dataset, output=None):\n        continuous_id_to_COCO_id = {v: k for k, v in COCODetection.COCO_id_to_category_id.items()}\n        for res in results:\n            # convert to COCO's incontinuous category id\n            res['category_id'] = continuous_id_to_COCO_id[res['category_id']]\n            # COCO expects results in xywh format\n            box = res['bbox']\n            box[2] -= box[0]\n            box[3] -= box[1]\n            res['bbox'] = [round(float(x), 3) for x in box]\n\n        assert output is not None, \"COCO evaluation requires an output file!\"\n        with open(output, 'w') as f:\n            json.dump(results, f)\n        if len(results):\n            # sometimes may crash if the results are empty?\n            return COCODetection(cfg.DATA.BASEDIR, dataset).print_coco_metrics(output)\n        else:\n            return {}", "response": "Evaluate the results of the load_inference_roidbs function and save the results to a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsurround a context with a timer. Args: msg(str): the log to print. log_start(bool): whether to print also at the beginning. Example: .. code-block:: python with timed_operation('Good Stuff'): time.sleep(1) Will print: .. code-block:: python Good stuff finished, time:1sec.", "response": "def timed_operation(msg, log_start=False):\n    \"\"\"\n    Surround a context with a timer.\n\n    Args:\n        msg(str): the log to print.\n        log_start(bool): whether to print also at the beginning.\n\n    Example:\n        .. code-block:: python\n\n            with timed_operation('Good Stuff'):\n                time.sleep(1)\n\n        Will print:\n\n        .. code-block:: python\n\n            Good stuff finished, time:1sec.\n    \"\"\"\n    assert len(msg)\n    if log_start:\n        logger.info('Start {} ...'.format(msg))\n    start = timer()\n    yield\n    msg = msg[0].upper() + msg[1:]\n    logger.info('{} finished, time:{:.4f} sec.'.format(\n        msg, timer() - start))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints the content of the TotalTimer.", "response": "def print_total_timer():\n    \"\"\"\n    Print the content of the TotalTimer, if it's not empty. This function will automatically get\n    called when program exits.\n    \"\"\"\n    if len(_TOTAL_TIMER_DATA) == 0:\n        return\n    for k, v in six.iteritems(_TOTAL_TIMER_DATA):\n        logger.info(\"Total Time: {} -> {:.2f} sec, {} times, {:.3g} sec/time\".format(\n            k, v.sum, v.count, v.average))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake sure processes terminate when main process exit.", "response": "def ensure_proc_terminate(proc):\n    \"\"\"\n    Make sure processes terminate when main process exit.\n\n    Args:\n        proc (multiprocessing.Process or list)\n    \"\"\"\n    if isinstance(proc, list):\n        for p in proc:\n            ensure_proc_terminate(p)\n        return\n\n    def stop_proc_by_weak_ref(ref):\n        proc = ref()\n        if proc is None:\n            return\n        if not proc.is_alive():\n            return\n        proc.terminate()\n        proc.join()\n\n    assert isinstance(proc, mp.Process)\n    atexit.register(stop_proc_by_weak_ref, weakref.ref(proc))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enable_death_signal(_warn=True):\n    if platform.system() != 'Linux':\n        return\n    try:\n        import prctl    # pip install python-prctl\n    except ImportError:\n        if _warn:\n            log_once('\"import prctl\" failed! Install python-prctl so that processes can be cleaned with guarantee.',\n                     'warn')\n        return\n    else:\n        assert hasattr(prctl, 'set_pdeathsig'), \\\n            \"prctl.set_pdeathsig does not exist! Note that you need to install 'python-prctl' instead of 'prctl'.\"\n        # is SIGHUP a good choice?\n        prctl.set_pdeathsig(signal.SIGHUP)", "response": "Enable death signal of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmask the current context where SIGINT is ignored.", "response": "def mask_sigint():\n    \"\"\"\n    Returns:\n        If called in main thread, returns a context where ``SIGINT`` is ignored, and yield True.\n        Otherwise yield False.\n    \"\"\"\n    if is_main_thread():\n        sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n        yield True\n        signal.signal(signal.SIGINT, sigint_handler)\n    else:\n        yield False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts processes with SIGINT ignored.", "response": "def start_proc_mask_signal(proc):\n    \"\"\"\n    Start process(es) with SIGINT ignored.\n\n    Args:\n        proc: (mp.Process or list)\n\n    Note:\n        The signal mask is only applied when called from main thread.\n    \"\"\"\n    if not isinstance(proc, list):\n        proc = [proc]\n\n    with mask_sigint():\n        for p in proc:\n            if isinstance(p, mp.Process):\n                if sys.version_info < (3, 4) or mp.get_start_method() == 'fork':\n                    log_once(\n\"Starting a process with 'fork' method is not safe and may consume unnecessary extra memory.\"\n\" Use 'forkserver' method (available after Py3.4) instead if you run into any issues. \"\n\"See https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\",\n'warn')  # noqa\n            p.start()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting a command with timeout and return STDOUT and STDERR.", "response": "def subproc_call(cmd, timeout=None):\n    \"\"\"\n    Execute a command with timeout, and return STDOUT and STDERR\n\n    Args:\n        cmd(str): the command to execute.\n        timeout(float): timeout in seconds.\n\n    Returns:\n        output(bytes), retcode(int). If timeout, retcode is -1.\n    \"\"\"\n    try:\n        output = subprocess.check_output(\n            cmd, stderr=subprocess.STDOUT,\n            shell=True, timeout=timeout)\n        return output, 0\n    except subprocess.TimeoutExpired as e:\n        logger.warn(\"Command '{}' timeout!\".format(cmd))\n        logger.warn(e.output.decode('utf-8'))\n        return e.output, -1\n    except subprocess.CalledProcessError as e:\n        logger.warn(\"Command '{}' failed, return code={}\".format(cmd, e.returncode))\n        logger.warn(e.output.decode('utf-8'))\n        return e.output, e.returncode\n    except Exception:\n        logger.warn(\"Command '{}' failed to run.\".format(cmd))\n        return \"\", -2"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nput obj to queue but will give up when thread is stopped", "response": "def queue_put_stoppable(self, q, obj):\n        \"\"\" Put obj to queue, but will give up when the thread is stopped\"\"\"\n        while not self.stopped():\n            try:\n                q.put(obj, timeout=5)\n                break\n            except queue.Full:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes obj from queue but will give up when thread is stopped", "response": "def queue_get_stoppable(self, q):\n        \"\"\" Take obj from queue, but will give up when the thread is stopped\"\"\"\n        while not self.stopped():\n            try:\n                return q.get(timeout=5)\n            except queue.Empty:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an entry to the list of ranks and data.", "response": "def put(self, rank, val):\n        \"\"\"\n        Args:\n            rank(int): rank of th element. All elements must have different ranks.\n            val: an object\n        \"\"\"\n        idx = bisect.bisect(self.ranks, rank)\n        self.ranks.insert(idx, rank)\n        self.data.insert(idx, val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visualize_conv_weights(filters, name):\n    with tf.name_scope('visualize_w_' + name):\n        filters = tf.transpose(filters, (3, 2, 0, 1))   # [h, w, cin, cout] -> [cout, cin, h, w]\n        filters = tf.unstack(filters)                   # --> cout * [cin, h, w]\n        filters = tf.concat(filters, 1)                 # --> [cin, cout * h, w]\n        filters = tf.unstack(filters)                   # --> cin * [cout * h, w]\n        filters = tf.concat(filters, 1)                 # --> [cout * h, cin * w]\n        filters = tf.expand_dims(filters, 0)\n        filters = tf.expand_dims(filters, -1)\n\n    tf.summary.image('visualize_w_' + name, filters)", "response": "Visualize use weights in convolution filters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef visualize_conv_activations(activation, name):\n    import math\n    with tf.name_scope('visualize_act_' + name):\n        _, h, w, c = activation.get_shape().as_list()\n        rows = []\n        c_per_row = int(math.sqrt(c))\n        for y in range(0, c - c_per_row, c_per_row):\n            row = activation[:, :, :, y:y + c_per_row]  # [?, H, W, 32] --> [?, H, W, 5]\n            cols = tf.unstack(row, axis=3)              # [?, H, W, 5] --> 5 * [?, H, W]\n            row = tf.concat(cols, 1)\n            rows.append(row)\n\n        viz = tf.concat(rows, 2)\n    tf.summary.image('visualize_act_' + name, tf.expand_dims(viz, -1))", "response": "Visualize activations for convolution layers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a tensor with a less specific shape.", "response": "def shapeless_placeholder(x, axis, name):\n    \"\"\"\n    Make the static shape of a tensor less specific.\n\n    If you want to feed to a tensor, the shape of the feed value must match\n    the tensor's static shape. This function creates a placeholder which\n    defaults to x if not fed, but has a less specific static shape than x.\n    See also `tensorflow#5680 <https://github.com/tensorflow/tensorflow/issues/5680>`_.\n\n    Args:\n        x: a tensor\n        axis(int or list of ints): these axes of ``x.get_shape()`` will become\n            None in the output.\n        name(str): name of the output tensor\n\n    Returns:\n        a tensor equal to x, but shape information is partially cleared.\n    \"\"\"\n    shp = x.get_shape().as_list()\n    if not isinstance(axis, list):\n        axis = [axis]\n    for a in axis:\n        if shp[a] is None:\n            raise ValueError(\"Axis {} of shape {} is already unknown!\".format(a, shp))\n        shp[a] = None\n    x = tf.placeholder_with_default(x, shape=shp, name=name)\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef entropy_from_samples(samples, vec):\n    samples_cat = tf.argmax(samples[:, :NUM_CLASS], axis=1, output_type=tf.int32)\n    samples_uniform = samples[:, NUM_CLASS:]\n    cat, uniform = get_distributions(vec[:, :NUM_CLASS], vec[:, NUM_CLASS:])\n\n    def neg_logprob(dist, sample, name):\n        nll = -dist.log_prob(sample)\n        # average over batch\n        return tf.reduce_sum(tf.reduce_mean(nll, axis=0), name=name)\n\n    entropies = [neg_logprob(cat, samples_cat, 'nll_cat'),\n                 neg_logprob(uniform, samples_uniform, 'nll_uniform')]\n    return entropies", "response": "Estimate H - log - probability of samples."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample_prior(batch_size):\n    cat, _ = get_distributions(DIST_PRIOR_PARAM[:NUM_CLASS], DIST_PRIOR_PARAM[NUM_CLASS:])\n    sample_cat = tf.one_hot(cat.sample(batch_size), NUM_CLASS)\n\n    \"\"\"\n    OpenAI official code actually models the \"uniform\" latent code as\n    a Gaussian distribution, but obtain the samples from a uniform distribution.\n    \"\"\"\n    sample_uni = tf.random_uniform([batch_size, NUM_UNIFORM], -1, 1)\n    samples = tf.concat([sample_cat, sample_uni], axis=1)\n    return samples", "response": "Sample from the prior."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the graph for the next sample of the class.", "response": "def build_graph(self, real_sample):\n        real_sample = tf.expand_dims(real_sample, -1)\n\n        # sample the latent code:\n        zc = shapeless_placeholder(sample_prior(BATCH), 0, name='z_code')\n        z_noise = shapeless_placeholder(\n            tf.random_uniform([BATCH, NOISE_DIM], -1, 1), 0, name='z_noise')\n        z = tf.concat([zc, z_noise], 1, name='z')\n\n        with argscope([Conv2D, Conv2DTranspose, FullyConnected],\n                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n            with tf.variable_scope('gen'):\n                fake_sample = self.generator(z)\n                fake_sample_viz = tf.cast((fake_sample) * 255.0, tf.uint8, name='viz')\n                tf.summary.image('gen', fake_sample_viz, max_outputs=30)\n\n            # may need to investigate how bn stats should be updated across two discrim\n            with tf.variable_scope('discrim'):\n                real_pred, _ = self.discriminator(real_sample)\n                fake_pred, dist_param = self.discriminator(fake_sample)\n\n        \"\"\"\n        Mutual information between x (i.e. zc in this case) and some\n        information s (the generated samples in this case):\n\n                    I(x;s) = H(x) - H(x|s)\n                           = H(x) + E[\\log P(x|s)]\n\n        The distribution from which zc is sampled, in this case, is set to a fixed prior already.\n        So the first term is a constant.\n        For the second term, we can maximize its variational lower bound:\n                    E_{x \\sim P(x|s)}[\\log Q(x|s)]\n        where Q(x|s) is a proposal distribution to approximate P(x|s).\n\n        Here, Q(x|s) is assumed to be a distribution which shares the form\n        of P, and whose parameters are predicted by the discriminator network.\n        \"\"\"\n        with tf.name_scope(\"mutual_information\"):\n            with tf.name_scope('prior_entropy'):\n                cat, uni = get_distributions(DIST_PRIOR_PARAM[:NUM_CLASS], DIST_PRIOR_PARAM[NUM_CLASS:])\n                ents = [cat.entropy(name='cat_entropy'), tf.reduce_sum(uni.entropy(), name='uni_entropy')]\n                entropy = tf.add_n(ents, name='total_entropy')\n                # Note that the entropy of prior is a constant. The paper mentioned it but didn't use it.\n\n            with tf.name_scope('conditional_entropy'):\n                cond_ents = entropy_from_samples(zc, dist_param)\n                cond_entropy = tf.add_n(cond_ents, name=\"total_entropy\")\n\n            MI = tf.subtract(entropy, cond_entropy, name='mutual_information')\n            summary.add_moving_summary(entropy, cond_entropy, MI, *cond_ents)\n\n        # default GAN objective\n        self.build_losses(real_pred, fake_pred)\n\n        # subtract mutual information for latent factors (we want to maximize them)\n        self.g_loss = tf.subtract(self.g_loss, MI, name='total_g_loss')\n        self.d_loss = tf.subtract(self.d_loss, MI, name='total_d_loss')\n\n        summary.add_moving_summary(self.g_loss, self.d_loss)\n\n        # distinguish between variables of generator and discriminator updates\n        self.collect_variables()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsees \"Dynamic Filter Networks\" (NIPS 2016) by Bert De Brabandere*, Xu Jia*, Tinne Tuytelaars and Luc Van Gool Remarks: This is the convolution version of a dynamic filter. Args: inputs : unfiltered input [b, h, w, 1] only grayscale images. filters : learned filters of [b, k, k, 1] (dynamically generated by the network). out_channel (int): number of output channel. kernel_shape: (h, w) tuple or a int. stride: (h, w) tuple or a int. padding (str): 'valid' or 'same'. Case insensitive. Returns tf.Tensor named ``output``.", "response": "def DynamicConvFilter(inputs, filters, out_channel,\n                      kernel_shape,\n                      stride=1,\n                      padding='SAME'):\n    \"\"\" see \"Dynamic Filter Networks\" (NIPS 2016)\n        by Bert De Brabandere*, Xu Jia*, Tinne Tuytelaars and Luc Van Gool\n\n    Remarks:\n        This is the convolution version of a dynamic filter.\n\n    Args:\n        inputs : unfiltered input [b, h, w, 1] only grayscale images.\n        filters : learned filters of [b, k, k, 1] (dynamically generated by the network).\n        out_channel (int): number of output channel.\n        kernel_shape: (h, w) tuple or a int.\n        stride: (h, w) tuple or a int.\n        padding (str): 'valid' or 'same'. Case insensitive.\n\n    Returns\n        tf.Tensor named ``output``.\n    \"\"\"\n\n    # tf.unstack only works with known batch_size :-(\n    batch_size, h, w, in_channel = inputs.get_shape().as_list()\n    stride = shape4d(stride)\n\n    inputs = tf.unstack(inputs)\n    filters = tf.reshape(filters, [batch_size] + shape2d(kernel_shape) + [in_channel, out_channel])\n    filters = tf.unstack(filters)\n\n    # this is ok as TF uses the cuda stream context\n    rsl = [tf.nn.conv2d(tf.reshape(d, [1, h, w, in_channel]),\n                        tf.reshape(k, [kernel_shape, kernel_shape, in_channel, out_channel]),\n                        stride, padding=\"SAME\") for d, k in zip(inputs, filters)]\n    rsl = tf.concat(rsl, axis=0, name='output')\n    return rsl"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parameter_net(self, theta, kernel_shape=9):\n        with argscope(FullyConnected, nl=tf.nn.leaky_relu):\n            net = FullyConnected('fc1', theta, 64)\n            net = FullyConnected('fc2', net, 128)\n\n        pred_filter = FullyConnected('fc3', net, kernel_shape ** 2, nl=tf.identity)\n        pred_filter = tf.reshape(pred_filter, [BATCH, kernel_shape, kernel_shape, 1], name=\"pred_filter\")\n        logger.info('Parameter net output: {}'.format(pred_filter.get_shape().as_list()))\n        return pred_filter", "response": "Estimate filters for convolution layers\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimplements a steerable Gaussian filter.", "response": "def filter_with_theta(image, theta, sigma=1., filter_size=9):\n        \"\"\"Implements a steerable Gaussian filter.\n\n        This function can be used to evaluate the first\n        directional derivative of an image, using the\n        method outlined in\n\n            W. T. Freeman and E. H. Adelson, \"The Design\n            and Use of Steerable Filters\", IEEE PAMI, 1991.\n\n        It evaluates the directional derivative of the input\n        image I, oriented at THETA degrees with respect to the\n        image rows. The standard deviation of the Gaussian kernel\n        is given by SIGMA (assumed to be equal to unity by default).\n\n        Args:\n            image: any input image (only one channel)\n            theta: orientation of filter [0, 2 * pi]\n            sigma (float, optional): standard derivation of Gaussian\n            filter_size (int, optional): filter support\n\n        Returns:\n            filtered image and the filter\n        \"\"\"\n        x = np.arange(-filter_size // 2 + 1, filter_size // 2 + 1)\n        # 1D Gaussian\n        g = np.array([np.exp(-(x**2) / (2 * sigma**2))])\n        # first-derivative of 1D Gaussian\n        gp = np.array([-(x / sigma) * np.exp(-(x**2) / (2 * sigma**2))])\n\n        ix = convolve2d(image, -gp, mode='same', boundary='fill', fillvalue=0)\n        ix = convolve2d(ix, g.T, mode='same', boundary='fill', fillvalue=0)\n\n        iy = convolve2d(image, g, mode='same', boundary='fill', fillvalue=0)\n        iy = convolve2d(iy, -gp.T, mode='same', boundary='fill', fillvalue=0)\n\n        output = np.cos(theta) * ix + np.sin(theta) * iy\n\n        # np.cos(theta) * np.matmul(g.T, gp) + np.sin(theta) * np.matmul(gp.T, g)\n        gt_filter = np.matmul(g.T, gp)\n        gt_filter = np.cos(theta) * gt_filter + np.sin(theta) * gt_filter.T\n\n        return output, gt_filter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncollecting all variables that are defined in the given scope.", "response": "def collect_variables(self, g_scope='gen', d_scope='discrim'):\n        \"\"\"\n        Assign `self.g_vars` to the parameters under scope `g_scope`,\n        and same with `self.d_vars`.\n        \"\"\"\n        self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, g_scope)\n        assert self.g_vars\n        self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, d_scope)\n        assert self.d_vars"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_losses(self, logits_real, logits_fake):\n        with tf.name_scope(\"GAN_loss\"):\n            score_real = tf.sigmoid(logits_real)\n            score_fake = tf.sigmoid(logits_fake)\n            tf.summary.histogram('score-real', score_real)\n            tf.summary.histogram('score-fake', score_fake)\n\n            with tf.name_scope(\"discrim\"):\n                d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                    logits=logits_real, labels=tf.ones_like(logits_real)), name='loss_real')\n                d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                    logits=logits_fake, labels=tf.zeros_like(logits_fake)), name='loss_fake')\n\n                d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name='accuracy_real')\n                d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name='accuracy_fake')\n\n                d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')\n                self.d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name='loss')\n\n            with tf.name_scope(\"gen\"):\n                self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                    logits=logits_fake, labels=tf.ones_like(logits_fake)), name='loss')\n                g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')\n\n            add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)", "response": "Builds standard GAN loss and set self. g_loss and self. d_loss."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_gan_trainer(self, input, model):\n        # Build the graph\n        self.tower_func = TowerFuncWrapper(model.build_graph, model.get_input_signature())\n        with TowerContext('', is_training=True):\n            self.tower_func(*input.get_input_tensors())\n        opt = model.get_optimizer()\n\n        # Define the training iteration\n        # by default, run one d_min after one g_min\n        with tf.name_scope('optimize'):\n            g_min = opt.minimize(model.g_loss, var_list=model.g_vars, name='g_op')\n            with tf.control_dependencies([g_min]):\n                d_min = opt.minimize(model.d_loss, var_list=model.d_vars, name='d_op')\n        self.train_op = d_min", "response": "Build the graph for the GanTrainer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rename_get_variable(mapping):\n    def custom_getter(getter, name, *args, **kwargs):\n        splits = name.split('/')\n        basename = splits[-1]\n        if basename in mapping:\n            basename = mapping[basename]\n            splits[-1] = basename\n            name = '/'.join(splits)\n        return getter(name, *args, **kwargs)\n    return custom_getter_scope(custom_getter)", "response": "A decorator that renames the variable basename in the current context."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregularizes the current tower with a regularization function.", "response": "def regularize_cost(regex, func, name='regularize_cost'):\n    \"\"\"\n    Apply a regularizer on trainable variables matching the regex, and print\n    the matched variables (only print once in multi-tower training).\n    In replicated mode, it will only regularize variables within the current tower.\n\n    If called under a TowerContext with `is_training==False`, this function returns a zero constant tensor.\n\n    Args:\n        regex (str): a regex to match variable names, e.g. \"conv.*/W\"\n        func: the regularization function, which takes a tensor and returns a scalar tensor.\n            E.g., ``tf.nn.l2_loss, tf.contrib.layers.l1_regularizer(0.001)``.\n\n    Returns:\n        tf.Tensor: a scalar, the total regularization cost.\n\n    Example:\n        .. code-block:: python\n\n            cost = cost + regularize_cost(\"fc.*/W\", l2_regularizer(1e-5))\n    \"\"\"\n    assert len(regex)\n    ctx = get_current_tower_context()\n    if not ctx.is_training:\n        # Currently cannot build the wd_cost correctly at inference,\n        # because ths vs_name used in inference can be '', therefore the\n        # variable filter will fail\n        return tf.constant(0, dtype=tf.float32, name='empty_' + name)\n\n    # If vars are shared, regularize all of them\n    # If vars are replicated, only regularize those in the current tower\n    if ctx.has_own_variables:\n        params = ctx.get_collection_in_tower(tfv1.GraphKeys.TRAINABLE_VARIABLES)\n    else:\n        params = tfv1.trainable_variables()\n\n    names = []\n\n    with tfv1.name_scope(name + '_internals'):\n        costs = []\n        for p in params:\n            para_name = p.op.name\n            if re.search(regex, para_name):\n                regloss = func(p)\n                assert regloss.dtype.is_floating, regloss\n                # Some variables may not be fp32, but it should\n                # be fine to assume regularization in fp32\n                if regloss.dtype != tf.float32:\n                    regloss = tf.cast(regloss, tf.float32)\n                costs.append(regloss)\n                names.append(p.name)\n        if not costs:\n            return tf.constant(0, dtype=tf.float32, name='empty_' + name)\n\n    # remove tower prefix from names, and print\n    if len(ctx.vs_name):\n        prefix = ctx.vs_name + '/'\n        prefixlen = len(prefix)\n\n        def f(name):\n            if name.startswith(prefix):\n                return name[prefixlen:]\n            return name\n        names = list(map(f, names))\n    logger.info(\"regularize_cost() found {} variables to regularize.\".format(len(names)))\n    _log_once(\"The following tensors will be regularized: {}\".format(', '.join(names)))\n\n    return tf.add_n(costs, name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the cost of regularizing variables created within the current tower.", "response": "def regularize_cost_from_collection(name='regularize_cost'):\n    \"\"\"\n    Get the cost from the regularizers in ``tf.GraphKeys.REGULARIZATION_LOSSES``.\n    If in replicated mode, will only regularize variables created within the current tower.\n\n    Args:\n        name (str): the name of the returned tensor\n\n    Returns:\n        tf.Tensor: a scalar, the total regularization cost.\n    \"\"\"\n    ctx = get_current_tower_context()\n    if not ctx.is_training:\n        # TODO Currently cannot build the wd_cost correctly at inference,\n        # because ths vs_name used in inference can be '', therefore the\n        # variable filter will fail\n        return tf.constant(0, dtype=tf.float32, name='empty_' + name)\n\n    # NOTE: this collection doesn't always grow with towers.\n    # It only grows with actual variable creation, but not get_variable call.\n    if ctx.has_own_variables:   # be careful of the first tower (name='')\n        losses = ctx.get_collection_in_tower(tfv1.GraphKeys.REGULARIZATION_LOSSES)\n    else:\n        losses = tfv1.get_collection(tfv1.GraphKeys.REGULARIZATION_LOSSES)\n    if len(losses) > 0:\n        logger.info(\"regularize_cost_from_collection() found {} regularizers \"\n                    \"in REGULARIZATION_LOSSES collection.\".format(len(losses)))\n\n        def maploss(l):\n            assert l.dtype.is_floating, l\n            if l.dtype != tf.float32:\n                l = tf.cast(l, tf.float32)\n            return l\n\n        losses = [maploss(l) for l in losses]\n        reg_loss = tf.add_n(losses, name=name)\n        return reg_loss\n    else:\n        return tf.constant(0, dtype=tf.float32, name='empty_' + name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Dropout(x, *args, **kwargs):\n    if 'is_training' in kwargs:\n        kwargs['training'] = kwargs.pop('is_training')\n    if len(args) > 0:\n        if args[0] != 0.5:\n            logger.warn(\n                \"The first positional argument to tensorpack.Dropout is the probability to keep, rather than to drop. \"\n                \"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \"\n                \"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")\n        rate = 1 - args[0]\n    elif 'keep_prob' in kwargs:\n        assert 'rate' not in kwargs, \"Cannot set both keep_prob and rate!\"\n        rate = 1 - kwargs.pop('keep_prob')\n    elif 'rate' in kwargs:\n        rate = kwargs.pop('rate')\n    else:\n        rate = 0.5\n\n    if kwargs.get('training', None) is None:\n        kwargs['training'] = get_current_tower_context().is_training\n\n    if get_tf_version_tuple() <= (1, 12):\n        return tf.layers.dropout(x, rate=rate, **kwargs)\n    else:\n        return tf.nn.dropout(x, rate=rate if kwargs['training'] else 0.)", "response": "Same as tf. layers. dropout."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fill(self, background_shape, img):\n        background_shape = tuple(background_shape)\n        return self._fill(background_shape, img)", "response": "Fill a proper background image of background_shape given img."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying a function on the wrapped tensor.", "response": "def apply(self, func, *args, **kwargs):\n        \"\"\"\n        Apply a function on the wrapped tensor.\n\n        Returns:\n            LinearWrap: ``LinearWrap(func(self.tensor(), *args, **kwargs))``.\n        \"\"\"\n        ret = func(self._t, *args, **kwargs)\n        return LinearWrap(ret)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply a function on the wrapped tensor.", "response": "def apply2(self, func, *args, **kwargs):\n        \"\"\"\n        Apply a function on the wrapped tensor. The tensor\n        will be the second argument of func.\n\n        This is because many symbolic functions\n        (such as tensorpack's layers) takes 'scope' as the first argument.\n\n        Returns:\n            LinearWrap: ``LinearWrap(func(args[0], self.tensor(), *args[1:], **kwargs))``.\n        \"\"\"\n        ret = func(args[0], self._t, *(args[1:]), **kwargs)\n        return LinearWrap(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef guided_relu():\n    from tensorflow.python.ops import gen_nn_ops   # noqa\n\n    @tf.RegisterGradient(\"GuidedReLU\")\n    def GuidedReluGrad(op, grad):\n        return tf.where(0. < grad,\n                        gen_nn_ops._relu_grad(grad, op.outputs[0]),\n                        tf.zeros(grad.get_shape()))\n\n    g = tf.get_default_graph()\n    with g.gradient_override_map({'Relu': 'GuidedReLU'}):\n        yield", "response": "A context where the gradient of the relu is replaced by a guided back - propagation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the saliency map of the input.", "response": "def saliency_map(output, input, name=\"saliency_map\"):\n    \"\"\"\n    Produce a saliency map as described in the paper:\n    `Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n    <https://arxiv.org/abs/1312.6034>`_.\n    The saliency map is the gradient of the max element in output w.r.t input.\n\n    Returns:\n        tf.Tensor: the saliency map. Has the same shape as input.\n    \"\"\"\n    max_outp = tf.reduce_max(output, 1)\n    saliency_op = tf.gradients(max_outp, input)[:][0]\n    return tf.identity(saliency_op, name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Conv2D(\n        inputs,\n        filters,\n        kernel_size,\n        strides=(1, 1),\n        padding='same',\n        data_format='channels_last',\n        dilation_rate=(1, 1),\n        activation=None,\n        use_bias=True,\n        kernel_initializer=None,\n        bias_initializer=tf.zeros_initializer(),\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        split=1):\n    \"\"\"\n    A wrapper around `tf.layers.Conv2D`.\n    Some differences to maintain backward-compatibility:\n\n    1. Default kernel initializer is variance_scaling_initializer(2.0).\n    2. Default padding is 'same'.\n    3. Support 'split' argument to do group conv. Note that this is not efficient.\n\n    Variable Names:\n\n    * ``W``: weights\n    * ``b``: bias\n    \"\"\"\n    if kernel_initializer is None:\n        if get_tf_version_tuple() <= (1, 12):\n            kernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0)\n        else:\n            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')\n    dilation_rate = shape2d(dilation_rate)\n\n    if split == 1 and dilation_rate == [1, 1]:\n        # tf.layers.Conv2D has bugs with dilations (https://github.com/tensorflow/tensorflow/issues/26797)\n        with rename_get_variable({'kernel': 'W', 'bias': 'b'}):\n            layer = tf.layers.Conv2D(\n                filters,\n                kernel_size,\n                strides=strides,\n                padding=padding,\n                data_format=data_format,\n                dilation_rate=dilation_rate,\n                activation=activation,\n                use_bias=use_bias,\n                kernel_initializer=kernel_initializer,\n                bias_initializer=bias_initializer,\n                kernel_regularizer=kernel_regularizer,\n                bias_regularizer=bias_regularizer,\n                activity_regularizer=activity_regularizer,\n                _reuse=tf.get_variable_scope().reuse)\n            ret = layer.apply(inputs, scope=tf.get_variable_scope())\n            ret = tf.identity(ret, name='output')\n\n        ret.variables = VariableHolder(W=layer.kernel)\n        if use_bias:\n            ret.variables.b = layer.bias\n\n    else:\n        # group conv implementation\n        data_format = get_data_format(data_format, keras_mode=False)\n        in_shape = inputs.get_shape().as_list()\n        channel_axis = 3 if data_format == 'NHWC' else 1\n        in_channel = in_shape[channel_axis]\n        assert in_channel is not None, \"[Conv2D] Input cannot have unknown channel!\"\n        assert in_channel % split == 0\n\n        assert kernel_regularizer is None and bias_regularizer is None and activity_regularizer is None, \\\n            \"Not supported by group conv or dilated conv!\"\n\n        out_channel = filters\n        assert out_channel % split == 0\n        assert dilation_rate == [1, 1] or get_tf_version_tuple() >= (1, 5), 'TF>=1.5 required for dilated conv.'\n\n        kernel_shape = shape2d(kernel_size)\n        filter_shape = kernel_shape + [in_channel / split, out_channel]\n        stride = shape4d(strides, data_format=data_format)\n\n        kwargs = dict(data_format=data_format)\n        if get_tf_version_tuple() >= (1, 5):\n            kwargs['dilations'] = shape4d(dilation_rate, data_format=data_format)\n\n        W = tf.get_variable(\n            'W', filter_shape, initializer=kernel_initializer)\n\n        if use_bias:\n            b = tf.get_variable('b', [out_channel], initializer=bias_initializer)\n\n        if split == 1:\n            conv = tf.nn.conv2d(inputs, W, stride, padding.upper(), **kwargs)\n        else:\n            conv = None\n            if get_tf_version_tuple() >= (1, 13):\n                try:\n                    conv = tf.nn.conv2d(inputs, W, stride, padding.upper(), **kwargs)\n                except ValueError:\n                    log_once(\"CUDNN group convolution support is only available with \"\n                             \"https://github.com/tensorflow/tensorflow/pull/25818 . \"\n                             \"Will fall back to a loop-based slow implementation instead!\", 'warn')\n            if conv is None:\n                inputs = tf.split(inputs, split, channel_axis)\n                kernels = tf.split(W, split, 3)\n                outputs = [tf.nn.conv2d(i, k, stride, padding.upper(), **kwargs)\n                           for i, k in zip(inputs, kernels)]\n                conv = tf.concat(outputs, channel_axis)\n\n        ret = tf.nn.bias_add(conv, b, data_format=data_format) if use_bias else conv\n        if activation is not None:\n            ret = activation(ret)\n        ret = tf.identity(ret, name='output')\n\n        ret.variables = VariableHolder(W=W)\n        if use_bias:\n            ret.variables.b = b\n    return ret", "response": "A wrapper around tf. layers. Conv2D."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Conv2DTranspose(\n        inputs,\n        filters,\n        kernel_size,\n        strides=(1, 1),\n        padding='same',\n        data_format='channels_last',\n        activation=None,\n        use_bias=True,\n        kernel_initializer=None,\n        bias_initializer=tf.zeros_initializer(),\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None):\n    \"\"\"\n    A wrapper around `tf.layers.Conv2DTranspose`.\n    Some differences to maintain backward-compatibility:\n\n    1. Default kernel initializer is variance_scaling_initializer(2.0).\n    2. Default padding is 'same'\n\n    Variable Names:\n\n    * ``W``: weights\n    * ``b``: bias\n    \"\"\"\n    if kernel_initializer is None:\n        if get_tf_version_tuple() <= (1, 12):\n            kernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0)\n        else:\n            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')\n\n    if get_tf_version_tuple() <= (1, 12):\n        with rename_get_variable({'kernel': 'W', 'bias': 'b'}):\n            layer = tf.layers.Conv2DTranspose(\n                filters,\n                kernel_size,\n                strides=strides,\n                padding=padding,\n                data_format=data_format,\n                activation=activation,\n                use_bias=use_bias,\n                kernel_initializer=kernel_initializer,\n                bias_initializer=bias_initializer,\n                kernel_regularizer=kernel_regularizer,\n                bias_regularizer=bias_regularizer,\n                activity_regularizer=activity_regularizer,\n                _reuse=tf.get_variable_scope().reuse)\n            ret = layer.apply(inputs, scope=tf.get_variable_scope())\n            ret = tf.identity(ret, name='output')\n        ret.variables = VariableHolder(W=layer.kernel)\n        if use_bias:\n            ret.variables.b = layer.bias\n    else:\n        # Our own implementation, to avoid Keras bugs. https://github.com/tensorflow/tensorflow/issues/25946\n        assert kernel_regularizer is None and bias_regularizer is None and activity_regularizer is None, \\\n            \"Unsupported arguments due to Keras bug in TensorFlow 1.13\"\n        data_format = get_data_format(data_format, keras_mode=False)\n        shape_dyn = tf.shape(inputs)\n        strides2d = shape2d(strides)\n        channels_in = inputs.shape[1 if data_format == 'NCHW' else 3]\n        if data_format == 'NCHW':\n            channels_in = inputs.shape[1]\n            out_shape_dyn = tf.stack(\n                [shape_dyn[0], filters,\n                 shape_dyn[2] * strides2d[0],\n                 shape_dyn[3] * strides2d[1]])\n            out_shape3_sta = [filters,\n                              None if inputs.shape[2] is None else inputs.shape[2] * strides2d[0],\n                              None if inputs.shape[3] is None else inputs.shape[3] * strides2d[1]]\n        else:\n            channels_in = inputs.shape[-1]\n            out_shape_dyn = tf.stack(\n                [shape_dyn[0],\n                 shape_dyn[1] * strides2d[0],\n                 shape_dyn[2] * strides2d[1],\n                 filters])\n            out_shape3_sta = [None if inputs.shape[1] is None else inputs.shape[1] * strides2d[0],\n                              None if inputs.shape[2] is None else inputs.shape[2] * strides2d[1],\n                              filters]\n\n        kernel_shape = shape2d(kernel_size)\n        W = tf.get_variable('W', kernel_shape + [filters, channels_in], initializer=kernel_initializer)\n        if use_bias:\n            b = tf.get_variable('b', [filters], initializer=bias_initializer)\n        conv = tf.nn.conv2d_transpose(\n            inputs, W, out_shape_dyn,\n            shape4d(strides, data_format=data_format),\n            padding=padding.upper(),\n            data_format=data_format)\n        conv.set_shape(tf.TensorShape([None] + out_shape3_sta))\n\n        ret = tf.nn.bias_add(conv, b, data_format=data_format) if use_bias else conv\n        if activation is not None:\n            ret = activation(ret)\n        ret = tf.identity(ret, name='output')\n\n        ret.variables = VariableHolder(W=W)\n        if use_bias:\n            ret.variables.b = b\n\n    return ret", "response": "A wrapper around tf. layers. Conv2DTranspose that creates a 2D convolutional layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup_graph(self):\n        all_vars = tfv1.global_variables() + tfv1.local_variables()\n        for v in all_vars:\n            if v.name == self.var_name:\n                self.var = v\n                break\n        else:\n            raise ValueError(\"{} is not a variable in the graph!\".format(self.var_name))", "response": "Will setup the assign operator for that variable."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_value_to_set(self):\n        ret = self._get_value_to_set()\n        if ret is not None and ret != self._last_value:\n            if self.epoch_num != self._last_epoch_set:  # Print this message at most once every epoch\n                if self._last_value is None:\n                    logger.info(\"[HyperParamSetter] At global_step={}, {} is set to {:.6f}\".format(\n                        self.global_step, self.param.readable_name, ret))\n                else:\n                    logger.info(\"[HyperParamSetter] At global_step={}, {} changes from {:.6f} to {:.6f}\".format(\n                        self.global_step, self.param.readable_name, self._last_value, ret))\n            self._last_epoch_set = self.epoch_num\n            self._last_value = ret\n        return ret", "response": "Returns the value to assign to the variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the value to be set at a given point.", "response": "def _get_value_to_set_at_point(self, point):\n        \"\"\"\n        Using schedule, compute the value to be set at a given point.\n        \"\"\"\n        laste, lastv = None, None\n        for e, v in self.schedule:\n            if e == point:\n                return v    # meet the exact boundary, return directly\n            if e > point:\n                break\n            laste, lastv = e, v\n        if laste is None or laste == e:\n            # hasn't reached the first scheduled point, or reached the end of all scheduled points\n            return None\n        if self.interp is None:\n            # If no interpolation, nothing to do.\n            return None\n        v = (point - laste) * 1. / (e - laste) * (v - lastv) + lastv\n        return v"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_graph(self, image, label):\n\n        # In tensorflow, inputs to convolution function are assumed to be\n        # NHWC. Add a single channel here.\n        image = tf.expand_dims(image, 3)\n\n        image = image * 2 - 1   # center the pixels values at zero\n        # The context manager `argscope` sets the default option for all the layers under\n        # this context. Here we use 32 channel convolution with shape 3x3\n        with argscope(Conv2D, kernel_size=3, activation=tf.nn.relu, filters=32):\n            logits = (LinearWrap(image)\n                      .Conv2D('conv0')\n                      .MaxPooling('pool0', 2)\n                      .Conv2D('conv1')\n                      .Conv2D('conv2')\n                      .MaxPooling('pool1', 2)\n                      .Conv2D('conv3')\n                      .FullyConnected('fc0', 512, activation=tf.nn.relu)\n                      .Dropout('dropout', rate=0.5)\n                      .FullyConnected('fc1', 10, activation=tf.identity)())\n\n        # a vector of length B with loss of each sample\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name='cross_entropy_loss')  # the average cross-entropy loss\n\n        correct = tf.cast(tf.nn.in_top_k(predictions=logits, targets=label, k=1), tf.float32, name='correct')\n        accuracy = tf.reduce_mean(correct, name='accuracy')\n\n        # This will monitor training error & accuracy (in a moving average fashion). The value will be automatically\n        # 1. written to tensosrboard\n        # 2. written to stat.json\n        # 3. printed after each epoch\n        train_error = tf.reduce_mean(1 - correct, name='train_error')\n        summary.add_moving_summary(train_error, accuracy)\n\n        # Use a regex to find parameters to apply weight decay.\n        # Here we apply a weight decay on all W (weight matrix) of all fc layers\n        # If you don't like regex, you can certainly define the cost in any other methods.\n        wd_cost = tf.multiply(1e-5,\n                              regularize_cost('fc.*/W', tf.nn.l2_loss),\n                              name='regularize_loss')\n        total_cost = tf.add_n([wd_cost, cost], name='total_cost')\n        summary.add_moving_summary(cost, wd_cost, total_cost)\n\n        # monitor histogram of all weight (of conv and fc layers) in tensorboard\n        summary.add_param_summary(('.*/W', ['histogram', 'rms']))\n        # the function should return the total cost to be optimized\n        return total_cost", "response": "This function should build the graph for the context manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a caffe parameter name to a tensorflow parameter name as a base64 encoded version of the name.", "response": "def name_conversion(caffe_layer_name):\n    \"\"\" Convert a caffe parameter name to a tensorflow parameter name as\n        defined in the above model \"\"\"\n    # beginning & end mapping\n    NAME_MAP = {'bn_conv1/beta': 'conv0/bn/beta',\n                'bn_conv1/gamma': 'conv0/bn/gamma',\n                'bn_conv1/mean/EMA': 'conv0/bn/mean/EMA',\n                'bn_conv1/variance/EMA': 'conv0/bn/variance/EMA',\n                'conv1/W': 'conv0/W', 'conv1/b': 'conv0/b',\n                'fc1000/W': 'linear/W', 'fc1000/b': 'linear/b'}\n    if caffe_layer_name in NAME_MAP:\n        return NAME_MAP[caffe_layer_name]\n\n    s = re.search('([a-z]+)([0-9]+)([a-z]+)_', caffe_layer_name)\n    if s is None:\n        s = re.search('([a-z]+)([0-9]+)([a-z]+)([0-9]+)_', caffe_layer_name)\n        layer_block_part1 = s.group(3)\n        layer_block_part2 = s.group(4)\n        assert layer_block_part1 in ['a', 'b']\n        layer_block = 0 if layer_block_part1 == 'a' else int(layer_block_part2)\n    else:\n        layer_block = ord(s.group(3)) - ord('a')\n    layer_type = s.group(1)\n    layer_group = s.group(2)\n\n    layer_branch = int(re.search('_branch([0-9])', caffe_layer_name).group(1))\n    assert layer_branch in [1, 2]\n    if layer_branch == 2:\n        layer_id = re.search('_branch[0-9]([a-z])/', caffe_layer_name).group(1)\n        layer_id = ord(layer_id) - ord('a') + 1\n\n    TYPE_DICT = {'res': 'conv{}', 'bn': 'conv{}/bn'}\n    layer_type = TYPE_DICT[layer_type].format(layer_id if layer_branch == 2 else 'shortcut')\n\n    tf_name = caffe_layer_name[caffe_layer_name.index('/'):]\n    tf_name = 'group{}/block{}/{}'.format(\n        int(layer_group) - 2, layer_block, layer_type) + tf_name\n    return tf_name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remap_variables(fn):\n    def custom_getter(getter, *args, **kwargs):\n        v = getter(*args, **kwargs)\n        return fn(v)\n    return custom_getter_scope(custom_getter)", "response": "A custom getter that maps all the variables by fn."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef freeze_variables(stop_gradient=True, skip_collection=False):\n    def custom_getter(getter, *args, **kwargs):\n        trainable = kwargs.get('trainable', True)\n        name = args[0] if len(args) else kwargs.get('name')\n        if skip_collection:\n            kwargs['trainable'] = False\n        v = getter(*args, **kwargs)\n        if skip_collection:\n            tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, v)\n        if trainable and stop_gradient:\n            v = tf.stop_gradient(v, name='freezed_' + name)\n        return v\n    return custom_getter_scope(custom_getter)", "response": "Returns a context that can be used to freeze variables in a single language."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a caffe model.", "response": "def load_caffe(model_desc, model_file):\n    \"\"\"\n    Load a caffe model. You must be able to ``import caffe`` to use this\n    function.\n    Args:\n        model_desc (str): path to caffe model description file (.prototxt).\n        model_file (str): path to caffe model parameter file (.caffemodel).\n    Returns:\n        dict: the parameters.\n    \"\"\"\n    with change_env('GLOG_minloglevel', '2'):\n        import caffe\n        caffe.set_mode_cpu()\n        net = caffe.Net(model_desc, model_file, caffe.TEST)\n    param_dict = CaffeLayerProcessor(net).process()\n    logger.info(\"Model loaded from caffe. Params: \" +\n                \", \".join(sorted(param_dict.keys())))\n    return param_dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_caffe_pb():\n    dir = get_dataset_path('caffe')\n    caffe_pb_file = os.path.join(dir, 'caffe_pb2.py')\n    if not os.path.isfile(caffe_pb_file):\n        download(CAFFE_PROTO_URL, dir)\n        assert os.path.isfile(os.path.join(dir, 'caffe.proto'))\n\n        if sys.version_info.major == 3:\n            cmd = \"protoc --version\"\n            version, ret = subproc_call(cmd, timeout=3)\n            if ret != 0:\n                sys.exit(1)\n            try:\n                version = version.decode('utf-8')\n                version = float('.'.join(version.split(' ')[1].split('.')[:2]))\n                assert version >= 2.7, \"Require protoc>=2.7 for Python3\"\n            except Exception:\n                logger.exception(\"protoc --version gives: \" + str(version))\n                raise\n\n        cmd = 'cd {} && protoc caffe.proto --python_out .'.format(dir)\n        ret = os.system(cmd)\n        assert ret == 0, \\\n            \"Command `{}` failed!\".format(cmd)\n        assert os.path.isfile(caffe_pb_file), caffe_pb_file\n    import imp\n    return imp.load_source('caffepb', caffe_pb_file)", "response": "Get caffe protobuf.\n    Returns:\n        The imported caffe protobuf module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_dict(self):\n        return {k: v.to_dict() if isinstance(v, AttrDict) else v\n                for k, v in self.__dict__.items() if not k.startswith('_')}", "response": "Convert to a nested dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_args(self, args):\n        for cfg in args:\n            keys, v = cfg.split('=', maxsplit=1)\n            keylist = keys.split('.')\n\n            dic = self\n            for i, k in enumerate(keylist[:-1]):\n                assert k in dir(dic), \"Unknown config key: {}\".format(keys)\n                dic = getattr(dic, k)\n            key = keylist[-1]\n\n            oldv = getattr(dic, key)\n            if not isinstance(oldv, str):\n                v = eval(v)\n            setattr(dic, key, v)", "response": "Update from command line args."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a corresponding model loader by looking at the file name.", "response": "def get_model_loader(filename):\n    \"\"\"\n    Get a corresponding model loader by looking at the file name.\n\n    Returns:\n        SessInit: either a :class:`DictRestore` (if name ends with 'npy/npz') or\n        :class:`SaverRestore` (otherwise).\n    \"\"\"\n    assert isinstance(filename, six.string_types), filename\n    filename = os.path.expanduser(filename)\n    if filename.endswith('.npy'):\n        assert tf.gfile.Exists(filename), filename\n        return DictRestore(np.load(filename, encoding='latin1').item())\n    elif filename.endswith('.npz'):\n        assert tf.gfile.Exists(filename), filename\n        obj = np.load(filename)\n        return DictRestore(dict(obj))\n    else:\n        return SaverRestore(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_checkpoint_vars(model_path):\n        reader = tf.train.NewCheckpointReader(model_path)\n        reader = CheckpointReaderAdapter(reader)    # use an adapter to standardize the name\n        ckpt_vars = reader.get_variable_to_shape_map().keys()\n        return reader, set(ckpt_vars)", "response": "return a tf. train. CheckpointReader and a set of strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enable_argscope_for_function(func, log_shape=True):\n\n    assert callable(func), \"func should be a callable\"\n\n    @wraps(func)\n    def wrapped_func(*args, **kwargs):\n        actual_args = copy.copy(get_arg_scope()[func.__name__])\n        actual_args.update(kwargs)\n        out_tensor = func(*args, **actual_args)\n        in_tensor = args[0]\n\n        ctx = get_current_tower_context()\n        name = func.__name__ if 'name' not in kwargs else kwargs['name']\n        if log_shape:\n            if ('tower' not in ctx.ns_name.lower()) or ctx.is_main_training_tower:\n                # we assume the first parameter is the most interesting\n                if isinstance(out_tensor, tuple):\n                    out_tensor_descr = out_tensor[0]\n                else:\n                    out_tensor_descr = out_tensor\n                logger.info('%20s: %20s -> %20s' %\n                            (name, in_tensor.shape.as_list(),\n                             out_tensor_descr.shape.as_list()))\n\n        return out_tensor\n    # argscope requires this property\n    wrapped_func.symbolic_function = None\n    return wrapped_func", "response": "Decorator for function to support argscope for all the functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enable_argscope_for_module(module, log_shape=True):\n    if is_tfv2() and module == tf.layers:\n        module = tf.compat.v1.layers\n    for name, obj in getmembers(module):\n        if isfunction(obj):\n            setattr(module, name, enable_argscope_for_function(obj,\n                    log_shape=log_shape))", "response": "Enable argscope for all functions of a given module."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visualize_tensors(name, imgs, scale_func=lambda x: (x + 1.) * 128., max_outputs=1):\n    xy = scale_func(tf.concat(imgs, axis=2))\n    xy = tf.cast(tf.clip_by_value(xy, 0, 255), tf.uint8, name='viz')\n    tf.summary.image(name, xy, max_outputs=30)", "response": "Generate tensor for TensorBoard for visualization operation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_input(img):\n    # split the image into left + right pairs\n    s = img.shape[0]\n    assert img.shape[1] == 2 * s\n    input, output = img[:, :s, :], img[:, s:, :]\n    if args.mode == 'BtoA':\n        input, output = output, input\n    if IN_CH == 1:\n        input = cv2.cvtColor(input, cv2.COLOR_RGB2GRAY)[:, :, np.newaxis]\n    if OUT_CH == 1:\n        output = cv2.cvtColor(output, cv2.COLOR_RGB2GRAY)[:, :, np.newaxis]\n    return [input, output]", "response": "split the input image into left + right pairs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a (b, 1) logits", "response": "def discriminator(self, inputs, outputs):\n        \"\"\" return a (b, 1) logits\"\"\"\n        l = tf.concat([inputs, outputs], 3)\n        with argscope(Conv2D, kernel_size=4, strides=2, activation=BNLReLU):\n            l = (LinearWrap(l)\n                 .Conv2D('conv0', NF, activation=tf.nn.leaky_relu)\n                 .Conv2D('conv1', NF * 2)\n                 .Conv2D('conv2', NF * 4)\n                 .Conv2D('conv3', NF * 8, strides=1, padding='VALID')\n                 .Conv2D('convlast', 1, strides=1, padding='VALID', activation=tf.identity)())\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the root mean square of tensor x.", "response": "def rms(x, name=None):\n    \"\"\"\n    Returns:\n        root mean square of tensor x.\n    \"\"\"\n    if name is None:\n        name = x.op.name + '/rms'\n        with tfv1.name_scope(None):   # name already contains the scope\n            return tf.sqrt(tf.reduce_mean(tf.square(x)), name=name)\n    return tf.sqrt(tf.reduce_mean(tf.square(x)), name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the gaussian weight of the current object.", "response": "def get_gaussian_weight(self, anchor):\n        \"\"\"\n        Args:\n            anchor: coordinate of the center\n        \"\"\"\n        ret = np.zeros(self.shape, dtype='float32')\n\n        y, x = np.mgrid[:self.shape[0], :self.shape[1]]\n        y = y.astype('float32') / ret.shape[0] - anchor[0]\n        x = x.astype('float32') / ret.shape[1] - anchor[1]\n        g = np.exp(-(x**2 + y ** 2) / self.sigma)\n        # cv2.imshow(\" \", g)\n        # cv2.waitKey()\n        return g"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npadding a tensor in H with p padding for W", "response": "def pad(x, p=3):\n    \"\"\"Pad tensor in H, W\n\n    Remarks:\n        TensorFlow uses \"ceil(input_spatial_shape[i] / strides[i])\" rather than explicit padding\n        like Caffe, pyTorch does. Hence, we need to pad here beforehand.\n\n    Args:\n        x (tf.tensor): incoming tensor\n        p (int, optional): padding for H, W\n\n    Returns:\n        tf.tensor: padded tensor\n    \"\"\"\n    return tf.pad(x, [[0, 0], [0, 0], [p, p], [p, p]])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the correlation cost volume of a single object.", "response": "def correlation(ina, inb,\n                kernel_size, max_displacement,\n                stride_1, stride_2,\n                pad, data_format):\n    \"\"\"\n    Correlation Cost Volume computation.\n\n    This is a fallback Python-only implementation, specialized just for FlowNet2.\n    It takes a lot of memory and is slow.\n\n    If you know to compile a custom op yourself, it's better to use the cuda implementation here:\n    https://github.com/PatWie/tensorflow-recipes/tree/master/OpticalFlow/user_ops\n    \"\"\"\n    assert pad == max_displacement\n    assert kernel_size == 1\n    assert data_format == 'NCHW'\n    assert max_displacement % stride_2 == 0\n    assert stride_1 == 1\n\n    D = int(max_displacement / stride_2 * 2) + 1  # D^2 == number of correlations per spatial location\n\n    b, c, h, w = ina.shape.as_list()\n\n    inb = tf.pad(inb, [[0, 0], [0, 0], [pad, pad], [pad, pad]])\n\n    res = []\n    for k1 in range(0, D):\n        start_h = k1 * stride_2\n        for k2 in range(0, D):\n            start_w = k2 * stride_2\n            s = tf.slice(inb, [0, 0, start_h, start_w], [-1, -1, h, w])\n            ans = tf.reduce_mean(ina * s, axis=1, keepdims=True)\n            res.append(ans)\n    res = tf.concat(res, axis=1)   # ND^2HW\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resize(x, mode, factor=4):\n    assert mode in ['bilinear', 'nearest'], mode\n    shp = tf.shape(x)[2:] * factor\n    # NCHW -> NHWC\n    x = tf.transpose(x, [0, 2, 3, 1])\n    if mode == 'bilinear':\n        x = tf.image.resize_bilinear(x, shp, align_corners=True)\n    else:\n        # better approximation of what Caffe is doing\n        x = tf.image.resize_nearest_neighbor(x, shp, align_corners=False)\n    # NHWC -> NCHW\n    return tf.transpose(x, [0, 3, 1, 2])", "response": "Resizes input tensor with unkown input - shape by a factor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef graph_structure(self, x1x2):\n        with argscope([tf.layers.conv2d], activation=lambda x: tf.nn.leaky_relu(x, 0.1),\n                      padding='valid', strides=2, kernel_size=3,\n                      data_format='channels_first'), \\\n            argscope([tf.layers.conv2d_transpose], padding='same', activation=tf.identity,\n                     data_format='channels_first', strides=2, kernel_size=4):\n\n            # extract features\n            x = tf.layers.conv2d(pad(x1x2, 3), 64, kernel_size=7, name='conv1')\n            conv2 = tf.layers.conv2d(pad(x, 2), 128, kernel_size=5, name='conv2')\n            conv3 = tf.layers.conv2d(pad(conv2, 2), 256, kernel_size=5, name='conv3')\n\n            conv2a, _ = tf.split(conv2, 2, axis=0)\n            conv3a, conv3b = tf.split(conv3, 2, axis=0)\n\n            corr = correlation(conv3a, conv3b,\n                               kernel_size=1,\n                               max_displacement=20,\n                               stride_1=1,\n                               stride_2=2,\n                               pad=20, data_format='NCHW')\n            corr = tf.nn.leaky_relu(corr, 0.1)\n\n            conv_redir = tf.layers.conv2d(conv3a, 32, kernel_size=1, strides=1, name='conv_redir')\n\n            in_conv3_1 = tf.concat([conv_redir, corr], axis=1, name='in_conv3_1')\n            conv3_1 = tf.layers.conv2d(pad(in_conv3_1, 1), 256, name='conv3_1', strides=1)\n\n            x = tf.layers.conv2d(pad(conv3_1, 1), 512, name='conv4')\n            conv4 = tf.layers.conv2d(pad(x, 1), 512, name='conv4_1', strides=1)\n            x = tf.layers.conv2d(pad(conv4, 1), 512, name='conv5')\n            conv5 = tf.layers.conv2d(pad(x, 1), 512, name='conv5_1', strides=1)\n            x = tf.layers.conv2d(pad(conv5, 1), 1024, name='conv6')\n            conv6 = tf.layers.conv2d(pad(x, 1), 1024, name='conv6_1', strides=1)\n\n            flow6 = tf.layers.conv2d(pad(conv6, 1), 2, name='predict_flow6', strides=1, activation=tf.identity)\n            flow6_up = tf.layers.conv2d_transpose(flow6, 2, name='upsampled_flow6_to_5')\n            x = tf.layers.conv2d_transpose(conv6, 512, name='deconv5', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            # return flow6\n            concat5 = tf.concat([conv5, x, flow6_up], axis=1, name='concat5')\n            flow5 = tf.layers.conv2d(pad(concat5, 1), 2, name='predict_flow5', strides=1, activation=tf.identity)\n            flow5_up = tf.layers.conv2d_transpose(flow5, 2, name='upsampled_flow5_to_4')\n            x = tf.layers.conv2d_transpose(concat5, 256, name='deconv4', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat4 = tf.concat([conv4, x, flow5_up], axis=1, name='concat4')\n            flow4 = tf.layers.conv2d(pad(concat4, 1), 2, name='predict_flow4', strides=1, activation=tf.identity)\n            flow4_up = tf.layers.conv2d_transpose(flow4, 2, name='upsampled_flow4_to_3')\n            x = tf.layers.conv2d_transpose(concat4, 128, name='deconv3', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat3 = tf.concat([conv3_1, x, flow4_up], axis=1, name='concat3')\n            flow3 = tf.layers.conv2d(pad(concat3, 1), 2, name='predict_flow3', strides=1, activation=tf.identity)\n            flow3_up = tf.layers.conv2d_transpose(flow3, 2, name='upsampled_flow3_to_2')\n            x = tf.layers.conv2d_transpose(concat3, 64, name='deconv2', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat2 = tf.concat([conv2a, x, flow3_up], axis=1, name='concat2')\n            flow2 = tf.layers.conv2d(pad(concat2, 1), 2, name='predict_flow2', strides=1, activation=tf.identity)\n\n            return tf.identity(flow2, name='flow2')", "response": "A function that returns a graph of the 2CHW flow."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef draw_annotation(img, boxes, klass, is_crowd=None):\n    labels = []\n    assert len(boxes) == len(klass)\n    if is_crowd is not None:\n        assert len(boxes) == len(is_crowd)\n        for cls, crd in zip(klass, is_crowd):\n            clsname = cfg.DATA.CLASS_NAMES[cls]\n            if crd == 1:\n                clsname += ';Crowd'\n            labels.append(clsname)\n    else:\n        for cls in klass:\n            labels.append(cfg.DATA.CLASS_NAMES[cls])\n    img = viz.draw_boxes(img, boxes, labels)\n    return img", "response": "Draw annotation boxes from boxes and klass."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw top3 proposals for each gt.", "response": "def draw_proposal_recall(img, proposals, proposal_scores, gt_boxes):\n    \"\"\"\n    Draw top3 proposals for each gt.\n    Args:\n        proposals: NPx4\n        proposal_scores: NP\n        gt_boxes: NG\n    \"\"\"\n    box_ious = np_iou(gt_boxes, proposals)    # ng x np\n    box_ious_argsort = np.argsort(-box_ious, axis=1)\n    good_proposals_ind = box_ious_argsort[:, :3]   # for each gt, find 3 best proposals\n    good_proposals_ind = np.unique(good_proposals_ind.ravel())\n\n    proposals = proposals[good_proposals_ind, :]\n    tags = list(map(str, proposal_scores[good_proposals_ind]))\n    img = viz.draw_boxes(img, proposals, tags)\n    return img, good_proposals_ind"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw_predictions(img, boxes, scores):\n    if len(boxes) == 0:\n        return img\n    labels = scores.argmax(axis=1)\n    scores = scores.max(axis=1)\n    tags = [\"{},{:.2f}\".format(cfg.DATA.CLASS_NAMES[lb], score) for lb, score in zip(labels, scores)]\n    return viz.draw_boxes(img, boxes, tags)", "response": "Draw predictions on the given boxes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndraw final outputs of the image.", "response": "def draw_final_outputs(img, results):\n    \"\"\"\n    Args:\n        results: [DetectionResult]\n    \"\"\"\n    if len(results) == 0:\n        return img\n\n    # Display in largest to smallest order to reduce occlusion\n    boxes = np.asarray([r.box for r in results])\n    areas = np_area(boxes)\n    sorted_inds = np.argsort(-areas)\n\n    ret = img\n    tags = []\n\n    for result_id in sorted_inds:\n        r = results[result_id]\n        if r.mask is not None:\n            ret = draw_mask(ret, r.mask)\n\n    for r in results:\n        tags.append(\n            \"{},{:.2f}\".format(cfg.DATA.CLASS_NAMES[r.class_id], r.score))\n    ret = viz.draw_boxes(ret, boxes, tags)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\noverlay a mask on top of the image.", "response": "def draw_mask(im, mask, alpha=0.5, color=None):\n    \"\"\"\n    Overlay a mask on top of the image.\n\n    Args:\n        im: a 3-channel uint8 image in BGR\n        mask: a binary 1-channel image of the same size\n        color: if None, will choose automatically\n    \"\"\"\n    if color is None:\n        color = PALETTE_RGB[np.random.choice(len(PALETTE_RGB))][::-1]\n    im = np.where(np.repeat((mask > 0)[:, :, None], 3, axis=2),\n                  im * (1 - alpha) + color * alpha, im)\n    im = im.astype('uint8')\n    return im"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_dataflow_zmq(df, addr, hwm=50, format=None, bind=False):\n    assert format in [None, 'zmq_op', 'zmq_ops']\n    if format is None:\n        dump_fn = dumps\n    else:\n        from zmq_ops import dump_arrays\n        dump_fn = dump_arrays\n\n    ctx = zmq.Context()\n    socket = ctx.socket(zmq.PUSH)\n    socket.set_hwm(hwm)\n    if bind:\n        socket.bind(addr)\n    else:\n        socket.connect(addr)\n    try:\n        df.reset_state()\n        logger.info(\"Serving data to {} with {} format ...\".format(\n            addr, 'default' if format is None else 'zmq_ops'))\n        INTERVAL = 200\n        q = deque(maxlen=INTERVAL)\n\n        try:\n            total = len(df)\n        except NotImplementedError:\n            total = 0\n        tqdm_args = get_tqdm_kwargs(leave=True, smoothing=0.8)\n        tqdm_args['bar_format'] = tqdm_args['bar_format'] + \"{postfix}\"\n        while True:\n            with tqdm.trange(total, **tqdm_args) as pbar:\n                for dp in df:\n                    start = time.time()\n                    socket.send(dump_fn(dp), copy=False)\n                    q.append(time.time() - start)\n                    pbar.update(1)\n                    if pbar.n % INTERVAL == 0:\n                        avg = \"{:.3f}\".format(sum(q) / len(q))\n                        pbar.set_postfix({'AvgSendLat': avg})\n    finally:\n        logger.info(\"Exiting send_dataflow_zmq ...\")\n        socket.setsockopt(zmq.LINGER, 0)\n        socket.close()\n        if not ctx.closed:\n            ctx.destroy(0)", "response": "This function will loop over the dataflow and send each datapoint to a ZMQ socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump_dataflow_to_process_queue(df, size, nr_consumer):\n    q = mp.Queue(size)\n\n    class EnqueProc(mp.Process):\n\n        def __init__(self, df, q, nr_consumer):\n            super(EnqueProc, self).__init__()\n            self.df = df\n            self.q = q\n\n        def run(self):\n            self.df.reset_state()\n            try:\n                for idx, dp in enumerate(self.df):\n                    self.q.put((idx, dp))\n            finally:\n                for _ in range(nr_consumer):\n                    self.q.put((DIE, None))\n\n    proc = EnqueProc(df, q, nr_consumer)\n    return q, proc", "response": "Convert a DataFlow to a multiprocessing. Queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngrab the current 3 - channel image from the ALE screen.", "response": "def _grab_raw_image(self):\n        \"\"\"\n        :returns: the current 3-channel image\n        \"\"\"\n        m = self.ale.getScreenRGB()\n        return m.reshape((self.height, self.width, 3))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a gray - scale image", "response": "def _current_state(self):\n        \"\"\"\n        :returns: a gray-scale (h, w) uint8 image\n        \"\"\"\n        ret = self._grab_raw_image()\n        # max-pooled over the last screen\n        ret = np.maximum(ret, self.last_raw_screen)\n        if self.viz:\n            if isinstance(self.viz, float):\n                cv2.imshow(self.windowname, ret)\n                cv2.waitKey(int(self.viz * 1000))\n        ret = ret.astype('float32')\n        # 0.299,0.587.0.114. same as rgb2y in torch/image\n        ret = cv2.cvtColor(ret, cv2.COLOR_RGB2GRAY)[:, :]\n        return ret.astype('uint8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clip_boxes(boxes, window, name=None):\n    boxes = tf.maximum(boxes, 0.0)\n    m = tf.tile(tf.reverse(window, [0]), [2])    # (4,)\n    boxes = tf.minimum(boxes, tf.cast(m, tf.float32), name=name)\n    return boxes", "response": "Clip boxes in a window."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_bbox_target(box_predictions, anchors):\n    orig_shape = tf.shape(anchors)\n    box_pred_txtytwth = tf.reshape(box_predictions, (-1, 2, 2))\n    box_pred_txty, box_pred_twth = tf.split(box_pred_txtytwth, 2, axis=1)\n    # each is (...)x1x2\n    anchors_x1y1x2y2 = tf.reshape(anchors, (-1, 2, 2))\n    anchors_x1y1, anchors_x2y2 = tf.split(anchors_x1y1x2y2, 2, axis=1)\n\n    waha = anchors_x2y2 - anchors_x1y1\n    xaya = (anchors_x2y2 + anchors_x1y1) * 0.5\n\n    clip = np.log(config.PREPROC.MAX_SIZE / 16.)\n    wbhb = tf.exp(tf.minimum(box_pred_twth, clip)) * waha\n    xbyb = box_pred_txty * waha + xaya\n    x1y1 = xbyb - wbhb * 0.5\n    x2y2 = xbyb + wbhb * 0.5    # (...)x1x2\n    out = tf.concat([x1y1, x2y2], axis=-2)\n    return tf.reshape(out, orig_shape)", "response": "Decode the bbox target."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode_bbox_target(boxes, anchors):\n    anchors_x1y1x2y2 = tf.reshape(anchors, (-1, 2, 2))\n    anchors_x1y1, anchors_x2y2 = tf.split(anchors_x1y1x2y2, 2, axis=1)\n    waha = anchors_x2y2 - anchors_x1y1\n    xaya = (anchors_x2y2 + anchors_x1y1) * 0.5\n\n    boxes_x1y1x2y2 = tf.reshape(boxes, (-1, 2, 2))\n    boxes_x1y1, boxes_x2y2 = tf.split(boxes_x1y1x2y2, 2, axis=1)\n    wbhb = boxes_x2y2 - boxes_x1y1\n    xbyb = (boxes_x2y2 + boxes_x1y1) * 0.5\n\n    # Note that here not all boxes are valid. Some may be zero\n    txty = (xbyb - xaya) / waha\n    twth = tf.log(wbhb / waha)  # may contain -inf for invalid boxes\n    encoded = tf.concat([txty, twth], axis=1)  # (-1x2x2)\n    return tf.reshape(encoded, tf.shape(boxes))", "response": "Encode the target of a new cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef crop_and_resize(image, boxes, box_ind, crop_size, pad_border=True):\n    assert isinstance(crop_size, int), crop_size\n    boxes = tf.stop_gradient(boxes)\n\n    # TF's crop_and_resize produces zeros on border\n    if pad_border:\n        # this can be quite slow\n        image = tf.pad(image, [[0, 0], [0, 0], [1, 1], [1, 1]], mode='SYMMETRIC')\n        boxes = boxes + 1\n\n    @under_name_scope()\n    def transform_fpcoor_for_tf(boxes, image_shape, crop_shape):\n        \"\"\"\n        The way tf.image.crop_and_resize works (with normalized box):\n        Initial point (the value of output[0]): x0_box * (W_img - 1)\n        Spacing: w_box * (W_img - 1) / (W_crop - 1)\n        Use the above grid to bilinear sample.\n\n        However, what we want is (with fpcoor box):\n        Spacing: w_box / W_crop\n        Initial point: x0_box + spacing/2 - 0.5\n        (-0.5 because bilinear sample (in my definition) assumes floating point coordinate\n         (0.0, 0.0) is the same as pixel value (0, 0))\n\n        This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize\n\n        Returns:\n            y1x1y2x2\n        \"\"\"\n        x0, y0, x1, y1 = tf.split(boxes, 4, axis=1)\n\n        spacing_w = (x1 - x0) / tf.cast(crop_shape[1], tf.float32)\n        spacing_h = (y1 - y0) / tf.cast(crop_shape[0], tf.float32)\n\n        imshape = [tf.cast(image_shape[0] - 1, tf.float32), tf.cast(image_shape[1] - 1, tf.float32)]\n        nx0 = (x0 + spacing_w / 2 - 0.5) / imshape[1]\n        ny0 = (y0 + spacing_h / 2 - 0.5) / imshape[0]\n\n        nw = spacing_w * tf.cast(crop_shape[1] - 1, tf.float32) / imshape[1]\n        nh = spacing_h * tf.cast(crop_shape[0] - 1, tf.float32) / imshape[0]\n\n        return tf.concat([ny0, nx0, ny0 + nh, nx0 + nw], axis=1)\n\n    # Expand bbox to a minium size of 1\n    # boxes_x1y1, boxes_x2y2 = tf.split(boxes, 2, axis=1)\n    # boxes_wh = boxes_x2y2 - boxes_x1y1\n    # boxes_center = tf.reshape((boxes_x2y2 + boxes_x1y1) * 0.5, [-1, 2])\n    # boxes_newwh = tf.maximum(boxes_wh, 1.)\n    # boxes_x1y1new = boxes_center - boxes_newwh * 0.5\n    # boxes_x2y2new = boxes_center + boxes_newwh * 0.5\n    # boxes = tf.concat([boxes_x1y1new, boxes_x2y2new], axis=1)\n\n    image_shape = tf.shape(image)[2:]\n    boxes = transform_fpcoor_for_tf(boxes, image_shape, [crop_size, crop_size])\n    image = tf.transpose(image, [0, 2, 3, 1])   # nhwc\n    ret = tf.image.crop_and_resize(\n        image, boxes, tf.cast(box_ind, tf.int32),\n        crop_size=[crop_size, crop_size])\n    ret = tf.transpose(ret, [0, 3, 1, 2])   # ncss\n    return ret", "response": "Returns a new image with the given boxes and the size and size."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nalign the ROI to the nearest bin", "response": "def roi_align(featuremap, boxes, resolution):\n    \"\"\"\n    Args:\n        featuremap: 1xCxHxW\n        boxes: Nx4 floatbox\n        resolution: output spatial resolution\n\n    Returns:\n        NxCx res x res\n    \"\"\"\n    # sample 4 locations per roi bin\n    ret = crop_and_resize(\n        featuremap, boxes,\n        tf.zeros([tf.shape(boxes)[0]], dtype=tf.int32),\n        resolution * 2)\n    ret = tf.nn.avg_pool(ret, [1, 1, 2, 2], [1, 1, 2, 2], padding='SAME', data_format='NCHW')\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef narrow_to(self, featuremap):\n        shape2d = tf.shape(featuremap)[2:]  # h,w\n        slice3d = tf.concat([shape2d, [-1]], axis=0)\n        slice4d = tf.concat([shape2d, [-1, -1]], axis=0)\n        boxes = tf.slice(self.boxes, [0, 0, 0, 0], slice4d)\n        gt_labels = tf.slice(self.gt_labels, [0, 0, 0], slice3d)\n        gt_boxes = tf.slice(self.gt_boxes, [0, 0, 0, 0], slice4d)\n        return RPNAnchors(boxes, gt_labels, gt_boxes)", "response": "Slice anchors to the spatial size of this featuremap."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an augmented image parameters.", "response": "def _get_augment_params(self, img):\n        center = img.shape[1::-1] * self._rand_range(\n            self.center_range[0], self.center_range[1], (2,))\n        deg = self._rand_range(-self.max_deg, self.max_deg)\n        if self.step_deg:\n            deg = deg // self.step_deg * self.step_deg\n        \"\"\"\n        The correct center is shape*0.5-0.5. This can be verified by:\n\n        SHAPE = 7\n        arr = np.random.rand(SHAPE, SHAPE)\n        orig = arr\n        c = SHAPE * 0.5 - 0.5\n        c = (c, c)\n        for k in range(4):\n            mat = cv2.getRotationMatrix2D(c, 90, 1)\n            arr = cv2.warpAffine(arr, mat, arr.shape)\n        assert np.all(arr == orig)\n        \"\"\"\n        mat = cv2.getRotationMatrix2D(tuple(center - 0.5), deg, 1)\n        return WarpAffineTransform(\n            mat, img.shape[1::-1], interp=self.interp,\n            borderMode=self.border, borderValue=self.border_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef largest_rotated_rect(w, h, angle):\n        angle = angle / 180.0 * math.pi\n        if w <= 0 or h <= 0:\n            return 0, 0\n\n        width_is_longer = w >= h\n        side_long, side_short = (w, h) if width_is_longer else (h, w)\n\n        # since the solutions for angle, -angle and 180-angle are all the same,\n        # if suffices to look at the first quadrant and the absolute values of sin,cos:\n        sin_a, cos_a = abs(math.sin(angle)), abs(math.cos(angle))\n        if side_short <= 2. * sin_a * cos_a * side_long:\n            # half constrained case: two crop corners touch the longer side,\n            #   the other two corners are on the mid-line parallel to the longer line\n            x = 0.5 * side_short\n            wr, hr = (x / sin_a, x / cos_a) if width_is_longer else (x / cos_a, x / sin_a)\n        else:\n            # fully constrained case: crop touches all 4 sides\n            cos_2a = cos_a * cos_a - sin_a * sin_a\n            wr, hr = (w * cos_a - h * sin_a) / cos_2a, (h * cos_a - w * sin_a) / cos_2a\n        return int(np.round(wr)), int(np.round(hr))", "response": "Get largest rectangle after rotation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies a mapping on certain argument before calling the original function. Args: maps (dict): {argument_name: map_func}", "response": "def map_arg(**maps):\n    \"\"\"\n    Apply a mapping on certain argument before calling the original function.\n\n    Args:\n        maps (dict): {argument_name: map_func}\n    \"\"\"\n    def deco(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            if six.PY2:\n                argmap = inspect.getcallargs(func, *args, **kwargs)\n            else:\n                # getcallargs was deprecated since 3.5\n                sig = inspect.signature(func)\n                argmap = sig.bind_partial(*args, **kwargs).arguments\n            for k, map_func in six.iteritems(maps):\n                if k in argmap:\n                    argmap[k] = map_func(argmap[k])\n            return func(**argmap)\n        return wrapper\n    return deco"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking memoized, but keep one cache per default graph.", "response": "def graph_memoized(func):\n    \"\"\"\n    Like memoized, but keep one cache per default graph.\n    \"\"\"\n\n    # TODO it keeps the graph alive\n    from ..compat import tfv1\n    GRAPH_ARG_NAME = '__IMPOSSIBLE_NAME_FOR_YOU__'\n\n    @memoized\n    def func_with_graph_arg(*args, **kwargs):\n        kwargs.pop(GRAPH_ARG_NAME)\n        return func(*args, **kwargs)\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        assert GRAPH_ARG_NAME not in kwargs, \"No Way!!\"\n        graph = tfv1.get_default_graph()\n        kwargs[GRAPH_ARG_NAME] = graph\n        return func_with_graph_arg(*args, **kwargs)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef memoized_ignoreargs(func):\n    def wrapper(*args, **kwargs):\n        if func not in _MEMOIZED_NOARGS:\n            res = func(*args, **kwargs)\n            _MEMOIZED_NOARGS[func] = res\n            return res\n        return _MEMOIZED_NOARGS[func]\n    return wrapper", "response": "A decorator that performs memoization ignoring the arguments used to call\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shape2d(a):\n    if type(a) == int:\n        return [a, a]\n    if isinstance(a, (list, tuple)):\n        assert len(a) == 2\n        return list(a)\n    raise RuntimeError(\"Illegal shape: {}\".format(a))", "response": "Ensure a 2D shape."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shape4d(a, data_format='NHWC'):\n    s2d = shape2d(a)\n    if get_data_format(data_format, False) == 'NHWC':\n        return [1] + s2d + [1]\n    else:\n        return [1, 1] + s2d", "response": "Ensuer a 4D shape to use with 4D symbolic functions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef call_only_once(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        # cannot use hasattr here, because hasattr tries to getattr, which\n        # fails if func is a property\n        assert func.__name__ in dir(self), \"call_only_once can only be used on method or property!\"\n\n        if not hasattr(self, '_CALL_ONLY_ONCE_CACHE'):\n            cache = self._CALL_ONLY_ONCE_CACHE = set()\n        else:\n            cache = self._CALL_ONLY_ONCE_CACHE\n\n        cls = type(self)\n        # cannot use ismethod(), because decorated method becomes a function\n        is_method = inspect.isfunction(getattr(cls, func.__name__))\n        assert func not in cache, \\\n            \"{} {}.{} can only be called once per object!\".format(\n                'Method' if is_method else 'Property',\n                cls.__name__, func.__name__)\n        cache.add(func)\n\n        return func(*args, **kwargs)\n\n    return wrapper", "response": "Decorator that can only be called once per object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef memoized_method(func):\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        assert func.__name__ in dir(self), \"memoized_method can only be used on method!\"\n\n        if not hasattr(self, '_MEMOIZED_CACHE'):\n            cache = self._MEMOIZED_CACHE = {}\n        else:\n            cache = self._MEMOIZED_CACHE\n\n        key = (func, ) + args[1:] + tuple(kwargs)\n        ret = cache.get(key, None)\n        if ret is not None:\n            return ret\n        value = func(*args, **kwargs)\n        cache[key] = value\n        return value\n\n    return wrapper", "response": "A decorator that performs memoization on methods. It stores the cache on the object instance itself."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a context which opens and caches a new name scope and reenter an existing one.", "response": "def cached_name_scope(name, top_level=True):\n    \"\"\"\n    Return a context which either opens and caches a new name scope,\n    or reenter an existing one.\n\n    Args:\n        top_level(bool): if True, the name scope will always be top-level.\n            It will not be nested under any existing name scope of the caller.\n    \"\"\"\n    if not top_level:\n        current_ns = tf.get_default_graph().get_name_scope()\n        if current_ns:\n            name = current_ns + '/' + name\n    ns = _get_cached_ns(name)\n    with tf.name_scope(ns):\n        yield ns"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_grad_list(grad_list):\n        nvars = [len(k) for k in grad_list]\n\n        def basename(x):\n            return re.sub('tower[0-9]+/', '', x.op.name)\n\n        if len(set(nvars)) != 1:\n            names_per_gpu = [set([basename(k[1]) for k in grad_and_vars]) for grad_and_vars in grad_list]\n            inters = copy.copy(names_per_gpu[0])\n            for s in names_per_gpu:\n                inters &= s\n            for s in names_per_gpu:\n                s -= inters\n            logger.error(\"Unique trainable variables on towers: \" + pprint.pformat(names_per_gpu))\n            raise ValueError(\"Number of gradients from each tower is different! \" + str(nvars))", "response": "Checks that the gradients of a tower are unique."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call_for_each_tower(\n            towers, func, devices=None, use_vs=None):\n        \"\"\"\n        Run `func` on all GPUs (towers) and return the results.\n\n        Args:\n            towers (list[int]): a list of GPU id.\n            func: a lambda to be called inside each tower\n            devices: a list of devices to be used. By default will use '/gpu:{tower}'\n            use_vs (list[bool]): list of use_vs to passed to TowerContext\n\n        Returns:\n            List of outputs of ``func``, evaluated on each tower.\n        \"\"\"\n\n        ret = []\n        if devices is not None:\n            assert len(devices) == len(towers)\n        if use_vs is not None:\n            assert len(use_vs) == len(towers)\n\n        tower_names = ['tower{}'.format(idx) for idx in range(len(towers))]\n\n        for idx, t in enumerate(towers):\n            device = devices[idx] if devices is not None else '/gpu:{}'.format(t)\n            usevs = use_vs[idx] if use_vs is not None else False\n            reuse = not usevs and idx > 0\n            with tfv1.device(device), _maybe_reuse_vs(reuse), TrainTowerContext(\n                    tower_names[idx],\n                    vs_name=tower_names[idx] if usevs else '',\n                    index=idx, total=len(towers)):\n                if len(str(device)) < 10:   # a device function doesn't have good string description\n                    logger.info(\"Building graph for training tower {} on device {} ...\".format(idx, device))\n                else:\n                    logger.info(\"Building graph for training tower {} ...\".format(idx))\n\n                # When use_vs is True, use LOCAL_VARIABLES,\n                # so these duplicated variables won't be saved by default.\n                with override_to_local_variable(enable=usevs):\n                    ret.append(func())\n        return ret", "response": "Calls func on all GPUs and returns the results."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(self, grad_list, get_opt_fn):\n        assert len(grad_list) == len(self.towers)\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        # debug tower performance (without update):\n        # ops = [k[0] for k in grad_list[1]] + [k[0] for k in grad_list[0]]\n        # self.train_op = tf.group(*ops)\n        # return\n\n        self.grads = aggregate_grads(grad_list, colocation=True)\n        # grads = grad_list[0]\n\n        opt = get_opt_fn()\n        if self.ps_device == 'cpu':\n            with tf.device('/cpu:0'):\n                train_op = opt.apply_gradients(self.grads, name='train_op')\n        else:\n            train_op = opt.apply_gradients(self.grads, name='train_op')\n        return train_op", "response": "Builds the training op for the current tower and returns the training op for the new tower."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall the function tower_fn on each tower.", "response": "def call_for_each_tower(self, tower_fn):\n        \"\"\"\n        Call the function `tower_fn` under :class:`TowerContext` for each tower.\n\n        Returns:\n            a list, contains the return values of `tower_fn` on each tower.\n        \"\"\"\n        # if tower_fn returns [(grad, var), ...], this returns #GPU x #VAR x 2\n        return DataParallelBuilder.build_on_towers(\n            self.towers,\n            tower_fn,\n            # use no variable scope for the first tower\n            use_vs=[False] + [True] * (len(self.towers) - 1))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(self, grad_list, get_opt_fn):\n        assert len(grad_list) == len(self.towers)\n        raw_devices = ['/gpu:{}'.format(k) for k in self.towers]\n\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        dtypes = set([x[0].dtype.base_dtype for x in grad_list[0]])\n        dtypes_nccl_supported = [tf.float32, tf.float64]\n        if get_tf_version_tuple() >= (1, 8):\n            dtypes_nccl_supported.append(tf.float16)\n        valid_for_nccl = all([k in dtypes_nccl_supported for k in dtypes])\n        if self._mode == 'nccl' and not valid_for_nccl:\n            logger.warn(\"Cannot use mode='nccl' because some gradients have unsupported types. Fallback to mode='cpu'\")\n            self._mode = 'cpu'\n\n        if self._mode in ['nccl', 'hierarchical']:\n            all_grads, all_vars = split_grad_list(grad_list)\n            # use allreduce from tf-benchmarks\n            # from .batch_allreduce import AllReduceSpecAlgorithm\n            # algo = AllReduceSpecAlgorithm('nccl', list(range(8)), 0, 10)\n            # all_grads, warmup_ops = algo.batch_all_reduce(all_grads, 1, True, False)\n            # print(\"WARMUP OPS\", warmup_ops)\n\n            if self._mode == 'nccl':\n                all_grads = allreduce_grads(all_grads, average=self._average)  # #gpu x #param\n            else:\n                packer = GradientPacker(len(raw_devices))\n                succ = packer.compute_strategy(all_grads[0])\n                if succ:\n                    packed_grads = packer.pack_all(all_grads, raw_devices)\n                    packed_grads_aggr = allreduce_grads_hierarchical(\n                        packed_grads, raw_devices, average=self._average)\n                    all_grads = packer.unpack_all(packed_grads_aggr, raw_devices)\n                else:\n                    all_grads = allreduce_grads_hierarchical(all_grads, raw_devices, average=self._average)\n\n            self.grads = merge_grad_list(all_grads, all_vars)\n        elif self._mode == 'cpu':\n            agg_grad_and_vars = aggregate_grads(\n                grad_list, colocation=False,\n                devices=['/cpu:0'], average=self._average)    # #param x 2\n            self.grads = []  # #gpu x #param x 2\n            for grad_and_vars in grad_list:   # grad_and_vars: #paramx2\n                # take v from each tower, and g from average.\n                self.grads.append(\n                    [(g, v) for (_, v), (g, _) in zip(grad_and_vars, agg_grad_and_vars)])\n\n        train_ops = []\n        opt = get_opt_fn()\n        with tf.name_scope('apply_gradients'):\n            for idx, grad_and_vars in enumerate(self.grads):\n                with tf.device(raw_devices[idx]):\n                    # apply_gradients may create variables. Make them LOCAL_VARIABLES\n                    with override_to_local_variable(enable=idx > 0):\n                        train_ops.append(opt.apply_gradients(\n                            grad_and_vars, name='apply_grad_{}'.format(idx)))\n        train_op = tf.group(*train_ops, name='train_op')\n\n        with tf.name_scope('sync_variables'):\n            post_init_op = SyncMultiGPUReplicatedBuilder.get_post_init_ops()\n        return train_op, post_init_op", "response": "Builds the set of all - reduced GPUs for the current training environment."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of post - initialization operations that are required to initialize the variable.", "response": "def get_post_init_ops():\n        \"\"\"\n        Copy values of variables on GPU 0 to other GPUs.\n        \"\"\"\n        # literally all variables, because it's better to sync optimizer-internal variables as well\n        all_vars = tf.global_variables() + tf.local_variables()\n        var_by_name = dict([(v.name, v) for v in all_vars])\n        trainable_names = set([x.name for x in tf.trainable_variables()])\n        post_init_ops = []\n\n        def log_failure(name, reason):\n            logger.warn(\"[ReplicatedTrainer] Do not know how to sync variable '{}' across GPUs. \"\n                        \"Reason: {} \".format(name, reason))\n            assert name not in trainable_names, \\\n                \"The aforementioned variable is trainable, so this is probably a fatal error.\"\n            logger.warn(\n                \"[ReplicatedTrainer] This variable is non-trainable. \"\n                \"Ignore this warning if you know it's OK to leave it out-of-sync.\")\n\n        for v in all_vars:\n            if not v.name.startswith('tower'):\n                continue\n            if v.name.startswith('tower0'):\n                # in this trainer, the master name doesn't have the towerx/ prefix\n                log_failure(v.name, \"Name should not have prefix 'tower0' in this trainer!\")\n                continue        # TODO some vars (EMA) may still startswith tower0\n\n            split_name = v.name.split('/')\n            prefix = split_name[0]\n            realname = '/'.join(split_name[1:])\n            if prefix in realname:\n                log_failure(v.name, \"Prefix {} appears multiple times in its name!\".format(prefix))\n                continue\n            copy_from = var_by_name.get(realname)\n            if copy_from is not None:\n                post_init_ops.append(v.assign(copy_from.read_value()))\n            else:\n                log_failure(v.name, \"Cannot find {} in the graph!\".format(realname))\n        logger.info(\n            \"'sync_variables_from_main_tower' includes {} operations.\".format(len(post_init_ops)))\n        return tf.group(*post_init_ops, name='sync_variables_from_main_tower')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call_for_each_tower(self, tower_fn):\n        ps_device = 'cpu' if len(self.towers) >= 4 else 'gpu'\n\n        raw_devices = ['/gpu:{}'.format(k) for k in self.towers]\n        if ps_device == 'gpu':\n            devices = [LeastLoadedDeviceSetter(d, raw_devices) for d in raw_devices]\n        else:\n            devices = [tf.train.replica_device_setter(\n                worker_device=d, ps_device='/cpu:0', ps_tasks=1) for d in raw_devices]\n\n        return DataParallelBuilder.build_on_towers(self.towers, tower_fn, devices)", "response": "Call the function tower_fn on each tower."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild the training op for the given list of gradients.", "response": "def build(self, grad_list, get_opt_fn):\n        \"\"\"\n        Args:\n            grad_list ([[(grad, var), ...], ...]): #GPU lists to be reduced. Each is the gradients computed on each GPU.\n            get_opt_fn (-> tf.train.Optimizer): callable which returns an optimizer\n\n        Returns:\n            tf.Operation: the training op\n        \"\"\"\n        assert len(grad_list) == len(self.towers)\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        if self._scale_gradient and len(self.towers) > 1:\n            # pretend to average the grads, in order to make async and\n            # sync have consistent effective learning rate\n            gradproc = ScaleGradient(('.*', 1.0 / len(self.towers)), verbose=False)\n            grad_list = [gradproc.process(gv) for gv in grad_list]\n        # Ngpu x Nvar x 2\n\n        train_ops = []\n        opt = get_opt_fn()\n        with tf.name_scope('async_apply_gradients'):\n            for i, grad_and_vars in enumerate(zip(*grad_list)):\n                # Ngpu x 2\n                v = grad_and_vars[0][1]\n                with tf.device(v.device):\n                    # will call apply_gradients (therefore gradproc) multiple times\n                    train_ops.append(opt.apply_gradients(\n                        grad_and_vars, name='apply_grad_{}'.format(i)))\n        return tf.group(*train_ops, name='train_op')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef humanize_time_delta(sec):\n    if sec < 0:\n        logger.warn(\"humanize_time_delta() obtains negative seconds!\")\n        return \"{:.3g} seconds\".format(sec)\n    if sec == 0:\n        return \"0 second\"\n    time = datetime(2000, 1, 1) + timedelta(seconds=int(sec))\n    units = ['day', 'hour', 'minute', 'second']\n    vals = [int(sec // 86400), time.hour, time.minute, time.second]\n    if sec < 60:\n        vals[-1] = sec\n\n    def _format(v, u):\n        return \"{:.3g} {}{}\".format(v, u, \"s\" if v > 1 else \"\")\n\n    ans = []\n    for v, u in zip(vals, units):\n        if v > 0:\n            ans.append(_format(v, u))\n    return \" \".join(ans)", "response": "Humanize timedelta given in seconds\n    Returns a string that represents the time delta in seconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges the environment variable name to val.", "response": "def change_env(name, val):\n    \"\"\"\n    Args:\n        name(str), val(str):\n\n    Returns:\n        a context where the environment variable ``name`` being set to\n        ``val``. It will be set back after the context exits.\n    \"\"\"\n    oldval = os.environ.get(name, None)\n    os.environ[name] = val\n    yield\n    if oldval is None:\n        del os.environ[name]\n    else:\n        os.environ[name] = oldval"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_rng(obj=None):\n    seed = (id(obj) + os.getpid() +\n            int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\"))) % 4294967295\n    if _RNG_SEED is not None:\n        seed = _RNG_SEED\n    return np.random.RandomState(seed)", "response": "Get a good RNG seeded with time pid and the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute_only_once():\n    f = inspect.currentframe().f_back\n    ident = (f.f_code.co_filename, f.f_lineno)\n    if ident in _EXECUTE_HISTORY:\n        return False\n    _EXECUTE_HISTORY.add(ident)\n    return True", "response": "This function is called only once."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_tqdm_kwargs(**kwargs):\n    default = dict(\n        smoothing=0.5,\n        dynamic_ncols=True,\n        ascii=True,\n        bar_format='{l_bar}{bar}|{n_fmt}/{total_fmt}[{elapsed}<{remaining},{rate_noinv_fmt}]'\n    )\n\n    try:\n        # Use this env var to override the refresh interval setting\n        interval = float(os.environ['TENSORPACK_PROGRESS_REFRESH'])\n    except KeyError:\n        interval = _pick_tqdm_interval(kwargs.get('file', sys.stderr))\n\n    default['mininterval'] = interval\n    default.update(kwargs)\n    return default", "response": "Returns default arguments to be used with tqdm.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the full path of a library.", "response": "def find_library_full_path(name):\n    \"\"\"\n    Similar to `from ctypes.util import find_library`, but try\n    to return full path if possible.\n    \"\"\"\n    from ctypes.util import find_library\n\n    if os.name == \"posix\" and sys.platform == \"darwin\":\n        # on Mac, ctypes already returns full path\n        return find_library(name)\n\n    def _use_proc_maps(name):\n        \"\"\"\n        Find so from /proc/pid/maps\n        Only works with libraries that has already been loaded.\n        But this is the most accurate method -- it finds the exact library that's being used.\n        \"\"\"\n        procmap = os.path.join('/proc', str(os.getpid()), 'maps')\n        if not os.path.isfile(procmap):\n            return None\n        with open(procmap, 'r') as f:\n            for line in f:\n                line = line.strip().split(' ')\n                sofile = line[-1]\n\n                basename = os.path.basename(sofile)\n                if 'lib' + name + '.so' in basename:\n                    if os.path.isfile(sofile):\n                        return os.path.realpath(sofile)\n\n    # The following two methods come from https://github.com/python/cpython/blob/master/Lib/ctypes/util.py\n    def _use_ld(name):\n        \"\"\"\n        Find so with `ld -lname -Lpath`.\n        It will search for files in LD_LIBRARY_PATH, but not in ldconfig.\n        \"\"\"\n        cmd = \"ld -t -l{} -o {}\".format(name, os.devnull)\n        ld_lib_path = os.environ.get('LD_LIBRARY_PATH', '')\n        for d in ld_lib_path.split(':'):\n            cmd = cmd + \" -L \" + d\n        result, ret = subproc_call(cmd + '|| true')\n        expr = r'[^\\(\\)\\s]*lib%s\\.[^\\(\\)\\s]*' % re.escape(name)\n        res = re.search(expr, result.decode('utf-8'))\n        if res:\n            res = res.group(0)\n            if not os.path.isfile(res):\n                return None\n            return os.path.realpath(res)\n\n    def _use_ldconfig(name):\n        \"\"\"\n        Find so in `ldconfig -p`.\n        It does not handle LD_LIBRARY_PATH.\n        \"\"\"\n        with change_env('LC_ALL', 'C'), change_env('LANG', 'C'):\n            ldconfig, ret = subproc_call(\"ldconfig -p\")\n            ldconfig = ldconfig.decode('utf-8')\n            if ret != 0:\n                return None\n        expr = r'\\s+(lib%s\\.[^\\s]+)\\s+\\(.*=>\\s+(.*)' % (re.escape(name))\n        res = re.search(expr, ldconfig)\n        if not res:\n            return None\n        else:\n            ret = res.group(2)\n            return os.path.realpath(ret)\n\n    if sys.platform.startswith('linux'):\n        return _use_proc_maps(name) or _use_ld(name) or _use_ldconfig(name) or find_library(name)\n\n    return find_library(name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(df, path, write_frequency=5000):\n        assert isinstance(df, DataFlow), type(df)\n        isdir = os.path.isdir(path)\n        if isdir:\n            assert not os.path.isfile(os.path.join(path, 'data.mdb')), \"LMDB file exists!\"\n        else:\n            assert not os.path.isfile(path), \"LMDB file {} exists!\".format(path)\n        db = lmdb.open(path, subdir=isdir,\n                       map_size=1099511627776 * 2, readonly=False,\n                       meminit=False, map_async=True)    # need sync() at the end\n        size = _reset_df_and_get_size(df)\n        with get_tqdm(total=size) as pbar:\n            idx = -1\n\n            # LMDB transaction is not exception-safe!\n            # although it has a context manager interface\n            txn = db.begin(write=True)\n            for idx, dp in enumerate(df):\n                txn.put(u'{:08}'.format(idx).encode('ascii'), dumps(dp))\n                pbar.update()\n                if (idx + 1) % write_frequency == 0:\n                    txn.commit()\n                    txn = db.begin(write=True)\n            txn.commit()\n\n            keys = [u'{:08}'.format(k).encode('ascii') for k in range(idx + 1)]\n            with db.begin(write=True) as txn:\n                txn.put(b'__keys__', dumps(keys))\n\n            logger.info(\"Flushing database ...\")\n            db.sync()\n        db.close()", "response": "Save a DataFlow to a LMDB file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(path, shuffle=True):\n        df = LMDBData(path, shuffle=shuffle)\n        return MapData(df, lambda dp: loads(dp[1]))", "response": "Load a map from a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(df, path):\n        buffer = []\n        size = _reset_df_and_get_size(df)\n        with get_tqdm(total=size) as pbar:\n            for dp in df:\n                buffer.append(dp)\n                pbar.update()\n        np.savez_compressed(path, buffer=np.asarray(buffer, dtype=np.object))", "response": "Save the data flow to npz file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the data flow to a file.", "response": "def save(df, path):\n        \"\"\"\n        Args:\n            df (DataFlow): the DataFlow to serialize.\n            path (str): output tfrecord file.\n        \"\"\"\n        if os.environ.get('TENSORPACK_COMPATIBLE_SERIALIZE', 'msgpack') == 'msgpack':\n            def _dumps(dp):\n                return dumps(dp)\n        else:\n            def _dumps(dp):\n                return dumps(dp).to_pybytes()\n\n        size = _reset_df_and_get_size(df)\n        with tf.python_io.TFRecordWriter(path) as writer, get_tqdm(total=size) as pbar:\n            for dp in df:\n                writer.write(_dumps(dp))\n                pbar.update()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(path, size=None):\n        gen = tf.python_io.tf_record_iterator(path)\n        ds = DataFromGenerator(gen)\n        ds = MapData(ds, loads)\n        if size is not None:\n            ds = FixedSizeData(ds, size)\n        return ds", "response": "Load a TFRecord file into a DataFlow."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the data flow to an hdf5 file.", "response": "def save(df, path, data_paths):\n        \"\"\"\n        Args:\n            df (DataFlow): the DataFlow to serialize.\n            path (str): output hdf5 file.\n            data_paths (list[str]): list of h5 paths. It should have the same\n                length as each datapoint, and each path should correspond to one\n                component of the datapoint.\n        \"\"\"\n        size = _reset_df_and_get_size(df)\n        buffer = defaultdict(list)\n\n        with get_tqdm(total=size) as pbar:\n            for dp in df:\n                assert len(dp) == len(data_paths), \"Datapoint has {} components!\".format(len(dp))\n                for k, el in zip(data_paths, dp):\n                    buffer[k].append(el)\n                pbar.update()\n\n        with h5py.File(path, 'w') as hf, get_tqdm(total=len(data_paths)) as pbar:\n            for data_path in data_paths:\n                hf.create_dataset(data_path, data=buffer[data_path])\n                pbar.update()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompiles the Keras train model.", "response": "def compile(self, optimizer, loss, metrics=None):\n        \"\"\"\n        Args:\n            optimizer (tf.train.Optimizer):\n            loss, metrics: string or list of strings\n        \"\"\"\n        if isinstance(loss, six.string_types):\n            loss = [loss]\n        if metrics is None:\n            metrics = []\n        if isinstance(metrics, six.string_types):\n            metrics = [metrics]\n\n        self._stats_to_inference = loss + metrics + [TOTAL_LOSS_NAME]\n        setup_keras_trainer(\n            self.trainer, get_model=self.get_model,\n            input_signature=self.input_signature,\n            target_signature=self.target_signature,\n            input=self.input,\n            optimizer=optimizer,\n            loss=loss,\n            metrics=metrics)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit(self, validation_data=None, **kwargs):\n        callbacks = kwargs.pop('callbacks', [])\n        if validation_data is not None:\n            # There is no way to guess where users want this callback. So we have to choose one.\n            # MinSaver may need results from this callback,\n            # so we put this callback at first.\n            callbacks.insert(0, InferenceRunner(\n                validation_data, ScalarStats(self._stats_to_inference)))\n        self.trainer.train_with_defaults(callbacks=callbacks, **kwargs)", "response": "Train the trainer with the given data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the three quantization functions fw fa fg for weights activations and gradients respectively.", "response": "def get_dorefa(bitW, bitA, bitG):\n    \"\"\"\n    Return the three quantization functions fw, fa, fg, for weights, activations and gradients respectively\n    \"\"\"\n    def quantize(x, k):\n        n = float(2 ** k - 1)\n\n        @tf.custom_gradient\n        def _quantize(x):\n            return tf.round(x * n) / n, lambda dy: dy\n\n        return _quantize(x)\n\n    def fw(x):\n        if bitW == 32:\n            return x\n\n        if bitW == 1:   # BWN\n            E = tf.stop_gradient(tf.reduce_mean(tf.abs(x)))\n\n            @tf.custom_gradient\n            def _sign(x):\n                return tf.where(tf.equal(x, 0), tf.ones_like(x), tf.sign(x / E)) * E, lambda dy: dy\n\n            return _sign(x)\n\n        x = tf.tanh(x)\n        x = x / tf.reduce_max(tf.abs(x)) * 0.5 + 0.5\n        return 2 * quantize(x, bitW) - 1\n\n    def fa(x):\n        if bitA == 32:\n            return x\n        return quantize(x, bitA)\n\n    def fg(x):\n        if bitG == 32:\n            return x\n\n        @tf.custom_gradient\n        def _identity(input):\n            def grad_fg(x):\n                rank = x.get_shape().ndims\n                assert rank is not None\n                maxx = tf.reduce_max(tf.abs(x), list(range(1, rank)), keep_dims=True)\n                x = x / maxx\n                n = float(2**bitG - 1)\n                x = x * 0.5 + 0.5 + tf.random_uniform(\n                    tf.shape(x), minval=-0.5 / n, maxval=0.5 / n)\n                x = tf.clip_by_value(x, 0.0, 1.0)\n                x = quantize(x, bitG) - 0.5\n                return x * maxx * 2\n\n            return input, grad_fg\n\n        return _identity(x)\n    return fw, fa, fg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimplementing Trained Ternary Quantization: https://arxiv.org/abs/1612.01064 Code modified from the authors' at: https://github.com/czhu95/ternarynet/blob/master/examples/Ternary-Net/ternary.py", "response": "def ternarize(x, thresh=0.05):\n    \"\"\"\n    Implemented Trained Ternary Quantization:\n    https://arxiv.org/abs/1612.01064\n\n    Code modified from the authors' at:\n    https://github.com/czhu95/ternarynet/blob/master/examples/Ternary-Net/ternary.py\n    \"\"\"\n    shape = x.get_shape()\n\n    thre_x = tf.stop_gradient(tf.reduce_max(tf.abs(x)) * thresh)\n\n    w_p = tf.get_variable('Wp', initializer=1.0, dtype=tf.float32)\n    w_n = tf.get_variable('Wn', initializer=1.0, dtype=tf.float32)\n\n    tf.summary.scalar(w_p.op.name + '-summary', w_p)\n    tf.summary.scalar(w_n.op.name + '-summary', w_n)\n\n    mask = tf.ones(shape)\n    mask_p = tf.where(x > thre_x, tf.ones(shape) * w_p, mask)\n    mask_np = tf.where(x < -thre_x, tf.ones(shape) * w_n, mask_p)\n    mask_z = tf.where((x < thre_x) & (x > - thre_x), tf.zeros(shape), mask)\n\n    @tf.custom_gradient\n    def _sign_mask(x):\n        return tf.sign(x) * mask_z, lambda dy: dy\n\n    w = _sign_mask(x)\n\n    w = w * mask_np\n\n    tf.summary.histogram(w.name, w)\n    return w"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef interactive_imshow(img, lclick_cb=None, rclick_cb=None, **kwargs):\n    name = 'tensorpack_viz_window'\n    cv2.imshow(name, img)\n\n    def mouse_cb(event, x, y, *args):\n        if event == cv2.EVENT_LBUTTONUP and lclick_cb is not None:\n            lclick_cb(img, x, y)\n        elif event == cv2.EVENT_RBUTTONUP and rclick_cb is not None:\n            rclick_cb(img, x, y)\n    cv2.setMouseCallback(name, mouse_cb)\n    key = cv2.waitKey(-1)\n    while key >= 128:\n        key = cv2.waitKey(-1)\n    key = chr(key & 0xff)\n    cb_name = 'key_cb_' + key\n    if cb_name in kwargs:\n        kwargs[cb_name](img)\n    elif key == 'q':\n        cv2.destroyWindow(name)\n    elif key == 'x':\n        sys.exit()\n    elif key == 's':\n        cv2.imwrite('out.png', img)\n    elif key in ['+', '=']:\n        img = cv2.resize(img, None, fx=1.3, fy=1.3, interpolation=cv2.INTER_CUBIC)\n        interactive_imshow(img, lclick_cb, rclick_cb, **kwargs)\n    elif key == '-':\n        img = cv2.resize(img, None, fx=0.7, fy=0.7, interpolation=cv2.INTER_CUBIC)\n        interactive_imshow(img, lclick_cb, rclick_cb, **kwargs)", "response": "Function to show an image in the current window."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stack_patches(\n        patch_list, nr_row, nr_col, border=None,\n        pad=False, bgcolor=255, viz=False, lclick_cb=None):\n    \"\"\"\n    Stacked patches into grid, to produce visualizations like the following:\n\n    .. image:: https://github.com/tensorpack/tensorpack/raw/master/examples/GAN/demo/BEGAN-CelebA-samples.jpg\n\n    Args:\n        patch_list(list[ndarray] or ndarray): NHW or NHWC images in [0,255].\n        nr_row(int), nr_col(int): rows and cols of the grid.\n            ``nr_col * nr_row`` must be no less than ``len(patch_list)``.\n        border(int): border length between images.\n            Defaults to ``0.05 * min(patch_width, patch_height)``.\n        pad (boolean): when `patch_list` is a list, pad all patches to the maximum height and width.\n            This option allows stacking patches of different shapes together.\n        bgcolor(int or 3-tuple): background color in [0, 255]. Either an int\n            or a BGR tuple.\n        viz(bool): whether to use :func:`interactive_imshow` to visualize the results.\n        lclick_cb: A callback function ``f(patch, patch index in patch_list)``\n            to get called when a patch get clicked in imshow.\n\n    Returns:\n        np.ndarray: the stacked image.\n    \"\"\"\n    if pad:\n        patch_list = _pad_patch_list(patch_list, bgcolor)\n    patch_list = _preprocess_patch_list(patch_list)\n\n    if lclick_cb is not None:\n        viz = True\n    ph, pw = patch_list.shape[1:3]\n\n    canvas = Canvas(ph, pw, nr_row, nr_col,\n                    patch_list.shape[-1], border, bgcolor)\n\n    if lclick_cb is not None:\n        def lclick_callback(img, x, y):\n            idx = canvas.get_patchid_from_coord(x, y)\n            lclick_cb(patch_list[idx], idx)\n    else:\n        lclick_callback = None\n\n    canvas.draw_patches(patch_list)\n    if viz:\n        interactive_imshow(canvas.canvas, lclick_cb=lclick_callback)\n    return canvas.canvas", "response": "Stacked patches into a single NHW or NHWC grid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_stack_patches(patch_list,\n                      nr_row=None, nr_col=None, border=None,\n                      max_width=1000, max_height=1000,\n                      bgcolor=255, viz=False, lclick_cb=None):\n    \"\"\"\n    Similar to :func:`stack_patches` but with a generator interface.\n    It takes a much-longer list and yields stacked results one by one.\n    For example, if ``patch_list`` contains 1000 images and ``nr_row==nr_col==10``,\n    this generator yields 10 stacked images.\n\n    Args:\n        nr_row(int), nr_col(int): rows and cols of each result.\n        max_width(int), max_height(int): Maximum allowed size of the\n            stacked image. If ``nr_row/nr_col`` are None, this number\n            will be used to infer the rows and cols. Otherwise the option is\n            ignored.\n        patch_list, border, viz, lclick_cb: same as in :func:`stack_patches`.\n\n    Yields:\n        np.ndarray: the stacked image.\n    \"\"\"\n    # setup parameters\n    patch_list = _preprocess_patch_list(patch_list)\n    if lclick_cb is not None:\n        viz = True\n    ph, pw = patch_list.shape[1:3]\n\n    if border is None:\n        border = int(0.05 * min(ph, pw))\n    if nr_row is None:\n        nr_row = int(max_height / (ph + border))\n    if nr_col is None:\n        nr_col = int(max_width / (pw + border))\n    canvas = Canvas(ph, pw, nr_row, nr_col, patch_list.shape[-1], border, bgcolor)\n\n    nr_patch = nr_row * nr_col\n    start = 0\n\n    if lclick_cb is not None:\n        def lclick_callback(img, x, y):\n            idx = canvas.get_patchid_from_coord(x, y)\n            idx = idx + start\n            if idx < end:\n                lclick_cb(patch_list[idx], idx)\n    else:\n        lclick_callback = None\n\n    while True:\n        end = start + nr_patch\n        cur_list = patch_list[start:end]\n        if not len(cur_list):\n            return\n        canvas.draw_patches(cur_list)\n        if viz:\n            interactive_imshow(canvas.canvas, lclick_cb=lclick_callback)\n        yield canvas.canvas\n        start = end", "response": "Generates stacked images from a list of patches."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dump_dataflow_images(df, index=0, batched=True,\n                         number=1000, output_dir=None,\n                         scale=1, resize=None, viz=None,\n                         flipRGB=False):\n    \"\"\"\n    Dump or visualize images of a :class:`DataFlow`.\n\n    Args:\n        df (DataFlow): the DataFlow.\n        index (int): the index of the image component.\n        batched (bool): whether the component contains batched images (NHW or\n            NHWC) or not (HW or HWC).\n        number (int): how many datapoint to take from the DataFlow.\n        output_dir (str): output directory to save images, default to not save.\n        scale (float): scale the value, usually either 1 or 255.\n        resize (tuple or None): tuple of (h, w) to resize the images to.\n        viz (tuple or None): tuple of (h, w) determining the grid size to use\n            with :func:`gen_stack_patches` for visualization. No visualization will happen by\n            default.\n        flipRGB (bool): apply a RGB<->BGR conversion or not.\n    \"\"\"\n    if output_dir:\n        mkdir_p(output_dir)\n    if viz is not None:\n        viz = shape2d(viz)\n        vizsize = viz[0] * viz[1]\n    if resize is not None:\n        resize = tuple(shape2d(resize))\n    vizlist = []\n\n    df.reset_state()\n    cnt = 0\n    while True:\n        for dp in df:\n            if not batched:\n                imgbatch = [dp[index]]\n            else:\n                imgbatch = dp[index]\n            for img in imgbatch:\n                cnt += 1\n                if cnt == number:\n                    return\n                if scale != 1:\n                    img = img * scale\n                if resize is not None:\n                    img = cv2.resize(img, resize)\n                if flipRGB:\n                    img = img[:, :, ::-1]\n                if output_dir:\n                    fname = os.path.join(output_dir, '{:03d}.jpg'.format(cnt))\n                    cv2.imwrite(fname, img)\n                if viz is not None:\n                    vizlist.append(img)\n            if viz is not None and len(vizlist) >= vizsize:\n                stack_patches(\n                    vizlist[:vizsize],\n                    nr_row=viz[0], nr_col=viz[1], viz=True)\n                vizlist = vizlist[vizsize:]", "response": "Dump or visualize images of a DataFlow."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef intensity_to_rgb(intensity, cmap='cubehelix', normalize=False):\n    assert intensity.ndim == 2, intensity.shape\n    intensity = intensity.astype(\"float\")\n\n    if normalize:\n        intensity -= intensity.min()\n        intensity /= intensity.max()\n\n    cmap = plt.get_cmap(cmap)\n    intensity = cmap(intensity)[..., :3]\n    return intensity.astype('float32') * 255.0", "response": "Convert a 1 - channel matrix of intensities to an RGB image employing a colormap."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw a text on an image.", "response": "def draw_text(img, pos, text, color, font_scale=0.4):\n    \"\"\"\n    Draw text on an image.\n\n    Args:\n        pos (tuple): x, y; the position of the text\n        text (str):\n        font_scale (float):\n        color (tuple): a 3-tuple BGR color in [0, 255]\n    \"\"\"\n    img = img.astype(np.uint8)\n    x0, y0 = int(pos[0]), int(pos[1])\n    # Compute text size.\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    ((text_w, text_h), _) = cv2.getTextSize(text, font, font_scale, 1)\n    # Place text background.\n    if x0 + text_w > img.shape[1]:\n        x0 = img.shape[1] - text_w\n    if y0 - int(1.15 * text_h) < 0:\n        y0 = int(1.15 * text_h)\n    back_topleft = x0, y0 - int(1.3 * text_h)\n    back_bottomright = x0 + text_w, y0\n    cv2.rectangle(img, back_topleft, back_bottomright, color, -1)\n    # Show text.\n    text_bottomleft = x0, y0 - int(0.25 * text_h)\n    cv2.putText(img, text, text_bottomleft, font, font_scale, (222, 222, 222), lineType=cv2.LINE_AA)\n    return img"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws a set of boxes in order of size.", "response": "def draw_boxes(im, boxes, labels=None, color=None):\n    \"\"\"\n    Args:\n        im (np.ndarray): a BGR image in range [0,255]. It will not be modified.\n        boxes (np.ndarray): a numpy array of shape Nx4 where each row is [x1, y1, x2, y2].\n        labels: (list[str] or None)\n        color: a 3-tuple BGR color (in range [0, 255])\n\n    Returns:\n        np.ndarray: a new image.\n    \"\"\"\n    boxes = np.asarray(boxes, dtype='int32')\n    if labels is not None:\n        assert len(labels) == len(boxes), \"{} != {}\".format(len(labels), len(boxes))\n    areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n    sorted_inds = np.argsort(-areas)    # draw large ones first\n    assert areas.min() > 0, areas.min()\n    # allow equal, because we are not very strict about rounding error here\n    assert boxes[:, 0].min() >= 0 and boxes[:, 1].min() >= 0 \\\n        and boxes[:, 2].max() <= im.shape[1] and boxes[:, 3].max() <= im.shape[0], \\\n        \"Image shape: {}\\n Boxes:\\n{}\".format(str(im.shape), str(boxes))\n\n    im = im.copy()\n    if color is None:\n        color = (15, 128, 15)\n    if im.ndim == 2 or (im.ndim == 3 and im.shape[2] == 1):\n        im = cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)\n    for i in sorted_inds:\n        box = boxes[i, :]\n        if labels is not None:\n            im = draw_text(im, (box[0], box[1]), labels[i], color=color)\n        cv2.rectangle(im, (box[0], box[1]), (box[2], box[3]),\n                      color=color, thickness=1)\n    return im"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a list of 8 - dimensional points into a list of boxes.", "response": "def point8_to_box(points):\n    \"\"\"\n    Args:\n        points: (nx4)x2\n    Returns:\n        nx4 boxes (x1y1x2y2)\n    \"\"\"\n    p = points.reshape((-1, 4, 2))\n    minxy = p.min(axis=1)   # nx2\n    maxxy = p.max(axis=1)   # nx2\n    return np.concatenate((minxy, maxxy), axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert polygons to binary masks.", "response": "def segmentation_to_mask(polys, height, width):\n    \"\"\"\n    Convert polygons to binary masks.\n\n    Args:\n        polys: a list of nx2 float array. Each array contains many (x, y) coordinates.\n\n    Returns:\n        a binary matrix of (height, width)\n    \"\"\"\n    polys = [p.flatten().tolist() for p in polys]\n    assert len(polys) > 0, \"Polygons are empty!\"\n\n    import pycocotools.mask as cocomask\n    rles = cocomask.frPyObjects(polys, height, width)\n    rle = cocomask.merge(rles)\n    return cocomask.decode(rle)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clip_boxes(boxes, shape):\n    orig_shape = boxes.shape\n    boxes = boxes.reshape([-1, 4])\n    h, w = shape\n    boxes[:, [0, 1]] = np.maximum(boxes[:, [0, 1]], 0)\n    boxes[:, 2] = np.minimum(boxes[:, 2], w)\n    boxes[:, 3] = np.minimum(boxes[:, 3], h)\n    return boxes.reshape(orig_shape)", "response": "Clip a set of boxes to a new shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfilters out the n - tuple boxes that are inside the given shape.", "response": "def filter_boxes_inside_shape(boxes, shape):\n    \"\"\"\n    Args:\n        boxes: (nx4), float\n        shape: (h, w)\n\n    Returns:\n        indices: (k, )\n        selection: (kx4)\n    \"\"\"\n    assert boxes.ndim == 2, boxes.shape\n    assert len(shape) == 2, shape\n    h, w = shape\n    indices = np.where(\n        (boxes[:, 0] >= 0) &\n        (boxes[:, 1] >= 0) &\n        (boxes[:, 2] <= w) &\n        (boxes[:, 3] <= h))[0]\n    return indices, boxes[indices, :]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GlobalAvgPooling(x, data_format='channels_last'):\n    assert x.shape.ndims == 4\n    data_format = get_data_format(data_format)\n    axis = [1, 2] if data_format == 'channels_last' else [2, 3]\n    return tf.reduce_mean(x, axis, name='output')", "response": "Global average pooling as in the paper Network In Network\n    <http://arxiv. org / abs / 1312. 4400 > _."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_savename_from_varname(\n        varname, varname_prefix=None,\n        savename_prefix=None):\n    \"\"\"\n    Args:\n        varname(str): a variable name in the graph\n        varname_prefix(str): an optional prefix that may need to be removed in varname\n        savename_prefix(str): an optional prefix to append to all savename\n    Returns:\n        str: the name used to save the variable\n    \"\"\"\n    name = varname\n    if varname_prefix is not None \\\n            and name.startswith(varname_prefix):\n        name = name[len(varname_prefix) + 1:]\n    if savename_prefix is not None:\n        name = savename_prefix + '/' + name\n    return name", "response": "Returns the name used to save a variable in the graph\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump_session_params(path):\n    # save variables that are GLOBAL, and either TRAINABLE or MODEL\n    var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    var.extend(tf.get_collection(tf.GraphKeys.MODEL_VARIABLES))\n    # TODO dedup\n    assert len(set(var)) == len(var), \"TRAINABLE and MODEL variables have duplication!\"\n    gvars = set([k.name for k in tf.global_variables()])\n    var = [v for v in var if v.name in gvars]\n    result = {}\n    for v in var:\n        result[v.name] = v.eval()\n    save_chkpt_vars(result, path)", "response": "Dump all TRAINABLE + MODEL variables to a dict and save as npz format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves variables in dic to path.", "response": "def save_chkpt_vars(dic, path):\n    \"\"\"\n    Save variables in dic to path.\n\n    Args:\n        dic: {name: value}\n        path: save as npz if the name ends with '.npz', otherwise save as a checkpoint.\n    \"\"\"\n    logger.info(\"Variables to save to {}:\".format(path))\n    keys = sorted(list(dic.keys()))\n    logger.info(pprint.pformat(keys))\n\n    assert not path.endswith('.npy')\n    if path.endswith('.npz'):\n        np.savez_compressed(path, **dic)\n    else:\n        with tf.Graph().as_default(), \\\n                tf.Session() as sess:\n            for k, v in six.iteritems(dic):\n                k = get_op_tensor_name(k)[0]\n                _ = tf.Variable(name=k, initial_value=v)    # noqa\n            sess.run(tf.global_variables_initializer())\n            saver = tf.train.Saver()\n            saver.save(sess, path, write_meta_graph=False)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_checkpoint_path(model_path):\n    if os.path.basename(model_path) == model_path:\n        model_path = os.path.join('.', model_path)  # avoid #4921 and #6142\n    if os.path.basename(model_path) == 'checkpoint':\n        assert tfv1.gfile.Exists(model_path), model_path\n        model_path = tf.train.latest_checkpoint(os.path.dirname(model_path))\n        # to be consistent with either v1 or v2\n\n    # fix paths if provided a wrong one\n    new_path = model_path\n    if '00000-of-00001' in model_path:\n        new_path = model_path.split('.data')[0]\n    elif model_path.endswith('.index'):\n        new_path = model_path.split('.index')[0]\n    if new_path != model_path:\n        logger.info(\n            \"Checkpoint path {} is auto-corrected to {}.\".format(model_path, new_path))\n        model_path = new_path\n    assert tfv1.gfile.Exists(model_path) or tfv1.gfile.Exists(model_path + '.index'), model_path\n    return model_path", "response": "This function is used to get the path to the checkpoint file for a single entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading all variables from a checkpoint to a dict.", "response": "def load_chkpt_vars(model_path):\n    \"\"\" Load all variables from a checkpoint to a dict.\n\n    Args:\n        model_path(str): path to a checkpoint.\n\n    Returns:\n        dict: a name:value dict\n    \"\"\"\n    model_path = get_checkpoint_path(model_path)\n    reader = tfv1.train.NewCheckpointReader(model_path)\n    var_names = reader.get_variable_to_shape_map().keys()\n    result = {}\n    for n in var_names:\n        result[n] = reader.get_tensor(n)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine if the variable name is a training variable.", "response": "def is_training_name(name):\n    \"\"\"\n    **Guess** if this variable is only used in training.\n    Only used internally to avoid too many logging. Do not use it.\n    \"\"\"\n    # TODO: maybe simply check against TRAINABLE_VARIABLES and MODEL_VARIABLES?\n    # TODO or use get_slot_names()\n    name = get_op_tensor_name(name)[0]\n    if name.endswith('/Adam') or name.endswith('/Adam_1'):\n        return True\n    if name.endswith('/Momentum'):\n        return True\n    if name.endswith('/Adadelta') or name.endswith('/Adadelta_1'):\n        return True\n    if name.endswith('/RMSProp') or name.endswith('/RMSProp_1'):\n        return True\n    if name.endswith('/Adagrad'):\n        return True\n    if name.startswith('EMA/') or '/EMA/' in name:  # all the moving average summaries\n        return True\n    if name.startswith('AccumGrad') or name.endswith('/AccumGrad'):\n        return True\n    if name.startswith('apply_gradients'):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef relaxed_value_for_var(value, var):\n        assert isinstance(var, tf.Variable)\n        name = var.op.name\n\n        # check incompatible shape\n        varshape = tuple(var.get_shape().as_list())\n        if varshape != value.shape:\n            # TODO only allow reshape when shape different by empty axis\n            if np.prod(varshape) != np.prod(value.shape):\n                raise ValueError(\n                    \"Trying to load a tensor of shape {} into the variable '{}' whose shape is {}.\".format(\n                        value.shape, name, varshape))\n            logger.warn(\"The tensor is reshaped from {} to {} when assigned to '{}'\".format(\n                value.shape, varshape, name))\n            value = value.reshape(varshape)\n\n        # fix some common type incompatibility problems, but not all\n        def upcast(vartype, valtype):\n            # vartype: a tf dtype\n            # valtype: a numpy dtype\n            # allow up-casting\n            if vartype == tf.float64 and valtype == np.float32:\n                return np.float64\n            if vartype in [tf.int64, tf.int32] and valtype in [np.int32, np.int16, np.int8]:\n                return np.int64 if vartype == tf.int64 else np.int32\n            return None\n\n        if hasattr(value, 'dtype'):\n            vartype = var.dtype.as_numpy_dtype\n            if vartype != value.dtype:\n                msg = \"Variable {} has dtype {} but was given a value of dtype {}.\".format(name, vartype, value.dtype)\n                newtype = upcast(var.dtype.base_dtype, value.dtype)\n                if newtype is not None:\n                    value = newtype(value)\n                    logger.warn(msg + \" Load it after casting!\")\n                else:\n                    assert vartype == value.dtype, msg\n        return value", "response": "Returns a relaxed version of value for the given variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, prms):\n        with self.sess.as_default():\n            fetches = []\n            feeds = {}\n            for name, value in six.iteritems(prms):\n                assert name in self.name_map\n                var = self.name_map[name]\n                fetches.append(var.initializer)\n                # This is the implementation of `var.load`\n                feeds[var.initializer.inputs[1]] = SessionUpdate.relaxed_value_for_var(value, var)\n            self.sess.run(fetches, feed_dict=feeds)", "response": "Updates the set of all the related variables in prms."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_distributed_session_creator(server):\n\n    server_def = server.server_def\n    is_chief = (server_def.job_name == 'worker') and (server_def.task_index == 0)\n\n    init_op = tf.global_variables_initializer()\n    local_init_op = tf.local_variables_initializer()\n    ready_op = tf.report_uninitialized_variables()\n    ready_for_local_init_op = tf.report_uninitialized_variables(tf.global_variables())\n    sm = tf.train.SessionManager(\n        local_init_op=local_init_op,\n        ready_op=ready_op,\n        ready_for_local_init_op=ready_for_local_init_op,\n        graph=tf.get_default_graph())\n\n    # to debug wrong variable collection\n    # from pprint import pprint\n    # print(\"GLOBAL:\")\n    # pprint([(k.name, k.device) for k in tf.global_variables()])\n    # print(\"LOCAL:\")\n    # pprint([(k.name, k.device) for k in tf.local_variables()])\n\n    class _Creator(tf.train.SessionCreator):\n        def create_session(self):\n            if is_chief:\n                return sm.prepare_session(master=server.target, init_op=init_op)\n            else:\n                tf.logging.set_verbosity(tf.logging.INFO)   # print message about uninitialized vars\n                ret = sm.wait_for_session(master=server.target)\n                tf.logging.set_verbosity(tf.logging.WARN)\n                return ret\n\n    return _Creator()", "response": "Returns a freshly distributed session creator for the given server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget number of GPUs available in the current environment.", "response": "def get_num_gpu():\n    \"\"\"\n    Returns:\n        int: #available GPUs in CUDA_VISIBLE_DEVICES, or in the system.\n    \"\"\"\n\n    def warn_return(ret, message):\n        try:\n            import tensorflow as tf\n        except ImportError:\n            return ret\n\n        built_with_cuda = tf.test.is_built_with_cuda()\n        if not built_with_cuda and ret > 0:\n            logger.warn(message + \"But TensorFlow was not built with CUDA support and could not use GPUs!\")\n        return ret\n\n    env = os.environ.get('CUDA_VISIBLE_DEVICES', None)\n    if env:\n        return warn_return(len(env.split(',')), \"Found non-empty CUDA_VISIBLE_DEVICES. \")\n    output, code = subproc_call(\"nvidia-smi -L\", timeout=5)\n    if code == 0:\n        output = output.decode('utf-8')\n        return warn_return(len(output.strip().split('\\n')), \"Found nvidia-smi. \")\n    try:\n        # Use NVML to query device properties\n        with NVMLContext() as ctx:\n            return warn_return(ctx.num_devices(), \"NVML found nvidia devices. \")\n    except Exception:\n        # Fallback\n        logger.info(\"Loading local devices by TensorFlow ...\")\n\n        try:\n            import tensorflow as tf\n            # available since TF 1.14\n            gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n        except AttributeError:\n            from tensorflow.python.client import device_lib\n            local_device_protos = device_lib.list_local_devices()\n            # Note this will initialize all GPUs and therefore has side effect\n            # https://github.com/tensorflow/tensorflow/issues/8136\n            gpu_devices = [x.name for x in local_device_protos if x.device_type == 'GPU']\n        return len(gpu_devices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nputs a tf. Summary.", "response": "def put_summary(self, summary):\n        \"\"\"\n        Put a `tf.Summary`.\n        \"\"\"\n        if isinstance(summary, six.binary_type):\n            summary = tf.Summary.FromString(summary)\n        assert isinstance(summary, tf.Summary), type(summary)\n\n        # TODO other types\n        for val in summary.value:\n            if val.WhichOneof('value') == 'simple_value':\n                val.tag = re.sub('tower[0-9]+/', '', val.tag)   # TODO move to subclasses\n\n                # TODO This hack is still needed, seem to disappear only when\n                # compiled from source.\n                suffix = '-summary'  # tensorflow#6150, tensorboard#59\n                if val.tag.endswith(suffix):\n                    val.tag = val.tag[:-len(suffix)]\n\n                self._dispatch(lambda m: m.process_scalar(val.tag, val.simple_value))\n\n        self._dispatch(lambda m: m.process_summary(summary))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put_image(self, name, val):\n        assert isinstance(val, np.ndarray)\n        arr = image_to_nhwc(val)\n        self._dispatch(lambda m: m.process_image(name, arr))\n        s = create_image_summary(name, arr)\n        self._dispatch(lambda m: m.process_summary(s))", "response": "Put an image.\n\n        Args:\n            name (str):\n            val (np.ndarray): 2D, 3D (HWC) or 4D (NHWC) numpy array of images\n                in range [0,255]. If channel is 3, assumed to be RGB."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nputs an event into the internal state.", "response": "def put_event(self, evt):\n        \"\"\"\n        Put an :class:`tf.Event`.\n        `step` and `wall_time` fields of :class:`tf.Event` will be filled automatically.\n\n        Args:\n            evt (tf.Event):\n        \"\"\"\n        evt.step = self.global_step\n        evt.wall_time = time.time()\n        self._dispatch(lambda m: m.process_event(evt))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the existing stats. json file and return the loaded list of statistics if found. Returns None if not found.", "response": "def load_existing_json():\n        \"\"\"\n        Look for an existing json under :meth:`logger.get_logger_dir()` named \"stats.json\",\n        and return the loaded list of statistics if found. Returns None otherwise.\n        \"\"\"\n        dir = logger.get_logger_dir()\n        fname = os.path.join(dir, JSONWriter.FILENAME)\n        if tf.gfile.Exists(fname):\n            with open(fname) as f:\n                stats = json.load(f)\n                assert isinstance(stats, list), type(stats)\n                return stats\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _trigger(self):\n        if len(self._stat_now):\n            self._stat_now['epoch_num'] = self.epoch_num\n            self._stat_now['global_step'] = self.global_step\n\n            self._stats.append(self._stat_now)\n            self._stat_now = {}\n            self._write_stat()", "response": "Add stats to json and dump to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sample(img, coords):\n    shape = img.get_shape().as_list()[1:]   # h, w, c\n    batch = tf.shape(img)[0]\n    shape2 = coords.get_shape().as_list()[1:3]  # h2, w2\n    assert None not in shape2, coords.get_shape()\n    max_coor = tf.constant([shape[0] - 1, shape[1] - 1], dtype=tf.float32)\n\n    coords = tf.clip_by_value(coords, 0., max_coor)  # borderMode==repeat\n    coords = tf.cast(coords, tf.int32)\n\n    batch_index = tf.range(batch, dtype=tf.int32)\n    batch_index = tf.reshape(batch_index, [-1, 1, 1, 1])\n    batch_index = tf.tile(batch_index, [1, shape2[0], shape2[1], 1])    # bxh2xw2x1\n    indices = tf.concat([batch_index, coords], axis=3)  # bxh2xw2x3\n    sampled = tf.gather_nd(img, indices)\n    return sampled", "response": "Sample from the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GridSample(inputs, borderMode='repeat'):\n    image, mapping = inputs\n    assert image.get_shape().ndims == 4 and mapping.get_shape().ndims == 4\n    input_shape = image.get_shape().as_list()[1:]\n    assert None not in input_shape, \\\n        \"Images in GridSample layer must have fully-defined shape\"\n    assert borderMode in ['repeat', 'constant']\n\n    orig_mapping = mapping\n    mapping = tf.maximum(mapping, 0.0)\n    lcoor = tf.floor(mapping)\n    ucoor = lcoor + 1\n\n    diff = mapping - lcoor\n    neg_diff = 1.0 - diff  # bxh2xw2x2\n\n    lcoory, lcoorx = tf.split(lcoor, 2, 3)\n    ucoory, ucoorx = tf.split(ucoor, 2, 3)\n\n    lyux = tf.concat([lcoory, ucoorx], 3)\n    uylx = tf.concat([ucoory, lcoorx], 3)\n\n    diffy, diffx = tf.split(diff, 2, 3)\n    neg_diffy, neg_diffx = tf.split(neg_diff, 2, 3)\n\n    ret = tf.add_n([sample(image, lcoor) * neg_diffx * neg_diffy,\n                    sample(image, ucoor) * diffx * diffy,\n                    sample(image, lyux) * neg_diffy * diffx,\n                    sample(image, uylx) * diffy * neg_diffx], name='sampled')\n    if borderMode == 'constant':\n        max_coor = tf.constant([input_shape[0] - 1, input_shape[1] - 1], dtype=tf.float32)\n        mask = tf.greater_equal(orig_mapping, 0.0)\n        mask2 = tf.less_equal(orig_mapping, max_coor)\n        mask = tf.logical_and(mask, mask2)  # bxh2xw2x2\n        mask = tf.reduce_all(mask, [3])  # bxh2xw2 boolean\n        mask = tf.expand_dims(mask, 3)\n        ret = ret * tf.cast(mask, tf.float32)\n    return tf.identity(ret, name='output')", "response": "Sample the images using bilinear interpolation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enable_call_trace():\n    def tracer(frame, event, arg):\n        if event == 'call':\n            co = frame.f_code\n            func_name = co.co_name\n            if func_name == 'write' or func_name == 'print':\n                # ignore write() calls from print statements\n                return\n            func_line_no = frame.f_lineno\n            func_filename = co.co_filename\n            caller = frame.f_back\n            if caller:\n                caller_line_no = caller.f_lineno\n                caller_filename = caller.f_code.co_filename\n                print('Call to `%s` on line %s:%s from %s:%s' %\n                      (func_name, func_filename, func_line_no,\n                       caller_filename, caller_line_no))\n            return\n    sys.settrace(tracer)", "response": "Enable trace for calls to any function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_default_prefetch(input_source_or_dataflow, trainer):\n    if not isinstance(input_source_or_dataflow, InputSource):\n        # to mimic same behavior of the old trainer interface\n        if type(trainer) == SimpleTrainer:\n            input = FeedInput(input_source_or_dataflow)\n        else:\n            logger.info(\"Automatically applying QueueInput on the DataFlow.\")\n            input = QueueInput(input_source_or_dataflow)\n    else:\n        input = input_source_or_dataflow\n    if hasattr(trainer, 'devices'):\n        towers = trainer.devices\n        if len(towers) > 1:\n            # seem to only improve on >1 GPUs\n            assert not isinstance(trainer, SimpleTrainer)\n\n            if isinstance(input, FeedfreeInput) and \\\n               not isinstance(input, (StagingInput, DummyConstantInput)):\n                logger.info(\"Automatically applying StagingInput on the DataFlow.\")\n                input = StagingInput(input)\n    return input", "response": "Applies a set of default prefetch rules to make a fast : class : InputSource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef launch_train_with_config(config, trainer):\n    if is_tfv2():\n        tfv1.disable_eager_execution()\n\n    assert isinstance(trainer, SingleCostTrainer), trainer\n    assert isinstance(config, TrainConfig), config\n    assert config.model is not None\n    assert config.dataflow is not None or config.data is not None\n\n    model = config.model\n    input = config.data or config.dataflow\n    input = apply_default_prefetch(input, trainer)\n\n    # This is the only place where the `ModelDesc` abstraction is useful.\n    # We should gradually stay away from this unuseful abstraction.\n    # TowerFuncWrapper is a better abstraction (similar to tf.defun in the future)\n    trainer.setup_graph(\n        model.get_input_signature(), input,\n        model.build_graph, model.get_optimizer)\n    _check_unused_regularization()\n    trainer.train_with_defaults(\n        callbacks=config.callbacks,\n        monitors=config.monitors,\n        session_creator=config.session_creator,\n        session_init=config.session_init,\n        steps_per_epoch=config.steps_per_epoch,\n        starting_epoch=config.starting_epoch,\n        max_epoch=config.max_epoch,\n        extra_callbacks=config.extra_callbacks)", "response": "Launch a training on a single - level model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a property from the loop object.", "response": "def _get_property(name):\n    \"\"\"\n    Delegate property to self.loop\n    \"\"\"\n    ret = property(\n        lambda self: getattr(self.loop, name))\n    if six.PY3:     # __doc__ is readonly in Py2\n        try:\n            ret.__doc__ = getattr(TrainLoop, name).__doc__\n        except AttributeError:\n            pass\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconfiguring the loop given the settings.", "response": "def config(self, steps_per_epoch, starting_epoch, max_epoch):\n        \"\"\"\n        Configure the loop given the settings.\n        \"\"\"\n        self.starting_epoch = int(starting_epoch)\n        self.max_epoch = int(max_epoch)\n        self.steps_per_epoch = int(steps_per_epoch)\n        # Allow empty epoch (no steps), if we want to run the callbacks only.\n        assert self.steps_per_epoch >= 0 and self.max_epoch >= 0\n\n        self._epoch_num = starting_epoch - 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a callback to the trainer.", "response": "def _register_callback(self, cb):\n        \"\"\"\n        Register callbacks to the trainer.\n        It can only be called before :meth:`Trainer.train()`.\n\n        Args:\n            cb (Callback or [Callback]): a callback or a list of callbacks\n\n        Returns:\n            succeed or not\n        \"\"\"\n        if isinstance(cb, (list, tuple)):\n            for x in cb:\n                self._register_callback(x)\n            return\n        assert isinstance(cb, Callback), cb\n        assert not isinstance(self._callbacks, Callbacks), \\\n            \"Cannot register more callbacks after trainer was setup!\"\n        if not self.is_chief and cb.chief_only:\n            logger.warn(\"Callback {} is chief-only, skipped.\".format(str(cb)))\n            return False\n        else:\n            self._callbacks.append(cb)\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_step(self):\n        if not hasattr(self, 'train_op'):\n            raise NotImplementedError(\n                \"Please either set `Trainer.train_op` or provide an implementation \"\n                \"of Trainer.run_step()!\")\n        self.hooked_sess.run(self.train_op)", "response": "Runs the next iteration of the Trainer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup_callbacks(self, callbacks, monitors):\n        assert isinstance(callbacks, list), callbacks\n        assert isinstance(monitors, list), monitors\n        describe_trainable_vars()   # TODO weird\n\n        self.register_callback(MaintainStepCounter())\n        for cb in callbacks:\n            self.register_callback(cb)\n        for cb in self._callbacks:\n            assert not isinstance(cb, MonitorBase), \"Monitor cannot be pre-registered for now!\"\n        registered_monitors = []\n        for m in monitors:\n            if self.register_callback(m):\n                registered_monitors.append(m)\n        self.monitors = Monitors(registered_monitors)\n        self.register_callback(self.monitors)   # monitors is also a callback\n\n        # some final operations that might modify the graph\n        logger.info(\"Setup callbacks graph ...\")\n        self._callbacks = Callbacks(self._callbacks)\n        self._callbacks.setup_graph(weakref.proxy(self))", "response": "Setup callbacks and monitors. Must be called after the main graph is built."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize(self, session_creator, session_init):\n        assert isinstance(session_creator, tfv1.train.SessionCreator), session_creator\n        assert isinstance(session_init, SessionInit), session_init\n        session_init._setup_graph()\n\n        logger.info(\"Creating the session ...\")\n\n        self.sess = session_creator.create_session()\n        self.initialize_hooks()\n\n        if self.is_chief:\n            logger.info(\"Initializing the session ...\")\n            session_init._run_init(self.sess)\n        else:\n            if not isinstance(session_init, JustCurrentSession):\n                logger.warn(\"This is not a chief worker, 'session_init' was ignored!\")\n\n        self.sess.graph.finalize()\n        logger.info(\"Graph Finalized.\")", "response": "Initialize the current session and set self. sess."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initialize_hooks(self):\n        hooks = self._callbacks.get_hooks()\n        self.hooked_sess = tfv1.train.MonitoredSession(\n            session_creator=ReuseSessionCreator(self.sess), hooks=hooks)", "response": "Create SessionRunHooks for all callbacks and hook it onto self. hooked_sess."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main_loop(self, steps_per_epoch, starting_epoch, max_epoch):\n        with self.sess.as_default():\n            self.loop.config(steps_per_epoch, starting_epoch, max_epoch)\n            self.loop.update_global_step()\n            try:\n                self._callbacks.before_train()\n                # refresh global step (might have changed by callbacks) TODO ugly\n                # what if gs is changed later?\n                self.loop.update_global_step()\n                for self.loop._epoch_num in range(\n                        self.loop.starting_epoch, self.loop.max_epoch + 1):\n                    logger.info(\"Start Epoch {} ...\".format(self.loop.epoch_num))\n                    self._callbacks.before_epoch()\n                    start_time = time.time()\n                    for self.loop._local_step in range(self.loop.steps_per_epoch):\n                        if self.hooked_sess.should_stop():\n                            return\n                        self.run_step()  # implemented by subclass\n                        self._callbacks.trigger_step()\n                    self._callbacks.after_epoch()\n                    logger.info(\"Epoch {} (global_step {}) finished, time:{}.\".format(\n                        self.loop.epoch_num, self.loop.global_step, humanize_time_delta(time.time() - start_time)))\n\n                    # trigger epoch outside the timing region.\n                    self._callbacks.trigger_epoch()\n                logger.info(\"Training has finished!\")\n            except (StopTraining, tf.errors.OutOfRangeError) as e:\n                logger.info(\"Training was stopped by exception {}.\".format(str(e)))\n            except KeyboardInterrupt:\n                logger.info(\"Detected Ctrl-C and exiting main loop.\")\n                raise\n            finally:\n                self._callbacks.after_train()\n                self.hooked_sess.close()", "response": "Run the main training loop.\n\n        Args:\n            steps_per_epoch, starting_epoch, max_epoch (int):"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrain the log file for the current session.", "response": "def train(self,\n              callbacks, monitors,\n              session_creator, session_init,\n              steps_per_epoch, starting_epoch=1, max_epoch=9999999):\n        \"\"\"\n        Implemented by three lines:\n\n        .. code-block:: python\n\n            self.setup_callbacks(callbacks, monitors)\n            self.initialize(session_creator, session_init)\n            self.main_loop(steps_per_epoch, starting_epoch, max_epoch)\n\n        You can call those methods by yourself to have better control on details if needed.\n        \"\"\"\n        self.setup_callbacks(callbacks, monitors)\n        self.initialize(session_creator, session_init)\n        self.main_loop(steps_per_epoch, starting_epoch, max_epoch)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef train_with_defaults(\n            self, _sentinel=None,\n            callbacks=None, monitors=None,\n            session_creator=None, session_init=None,\n            steps_per_epoch=None, starting_epoch=1, max_epoch=9999999,\n            extra_callbacks=None):\n        \"\"\"\n        Same as :meth:`train()`, except:\n\n        1. Add `extra_callbacks` to callbacks. The default value for\n           `extra_callbacks` is :meth:`DEFAULT_CALLBACKS()`.\n        2. Default value for `monitors` is :meth:`DEFAULT_MONITORS()`.\n        3. Provide default values for every option except `steps_per_epoch`.\n        \"\"\"\n        assert _sentinel is None, \"Please call `train_with_defaults` with keyword arguments only!\"\n        callbacks = copy.copy(callbacks or [])\n        monitors = DEFAULT_MONITORS() if monitors is None else monitors\n        extra_callbacks = DEFAULT_CALLBACKS() if extra_callbacks is None else extra_callbacks\n        callbacks.extend(extra_callbacks)\n\n        assert steps_per_epoch is not None\n        session_creator = session_creator or NewSessionCreator()\n        session_init = session_init or JustCurrentSession()\n\n        self.train(callbacks, monitors,\n                   session_creator, session_init,\n                   steps_per_epoch, starting_epoch, max_epoch)", "response": "Train the log file with default values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_sess_config(mem_fraction=0.99):\n    conf = tfv1.ConfigProto()\n\n    conf.allow_soft_placement = True\n    # conf.log_device_placement = True\n\n    conf.intra_op_parallelism_threads = 1\n    conf.inter_op_parallelism_threads = 0\n    # TF benchmark use cpu_count() - gpu_thread_count(), e.g. 80 - 8 * 2\n    # Didn't see much difference.\n\n    conf.gpu_options.per_process_gpu_memory_fraction = mem_fraction\n\n    # This hurt performance of large data pipeline:\n    # https://github.com/tensorflow/benchmarks/commit/1528c46499cdcff669b5d7c006b7b971884ad0e6\n    # conf.gpu_options.force_gpu_compatible = True\n\n    conf.gpu_options.allow_growth = True\n\n    # from tensorflow.core.protobuf import rewriter_config_pb2 as rwc\n    # conf.graph_options.rewrite_options.memory_optimization = \\\n    #     rwc.RewriterConfig.HEURISTICS\n\n    # May hurt performance?\n    # conf.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n    # conf.graph_options.place_pruned_graph = True\n    return conf", "response": "Returns a tf. ConfigProto to use as default session config."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the global_step variable in the current graph. Create it if it doesn t exist.", "response": "def get_global_step_var():\n    \"\"\"\n    Returns:\n        tf.Tensor: the global_step variable in the current graph. Create if doesn't exist.\n    \"\"\"\n    scope = tfv1.VariableScope(reuse=False, name='')  # the root vs\n    with tfv1.variable_scope(scope):\n        var = tfv1.train.get_or_create_global_step()\n    return var"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tensors_by_names(names):\n    ret = []\n    G = tfv1.get_default_graph()\n    for n in names:\n        opn, varn = get_op_tensor_name(n)\n        ret.append(G.get_tensor_by_name(varn))\n    return ret", "response": "Get a list of tensors in the default graph by a list of names."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_op_or_tensor_by_name(name):\n    G = tfv1.get_default_graph()\n\n    def f(n):\n        if len(n) >= 3 and n[-2] == ':':\n            return G.get_tensor_by_name(n)\n        else:\n            return G.get_operation_by_name(n)\n\n    if not isinstance(name, list):\n        return f(name)\n    else:\n        return list(map(f, name))", "response": "Get either tf. Operation of tf. Tensor from names."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_sync_queues_and_barrier(self, name, dependencies):\n        self._sync_queue_counter += 1\n        with tf.device(self.sync_queue_devices[self._sync_queue_counter % len(self.sync_queue_devices)]):\n            sync_queues = [\n                tf.FIFOQueue(self.num_worker, [tf.bool], shapes=[[]],\n                             shared_name='%s%s' % (name, i))\n                for i in range(self.num_worker)]\n            queue_ops = []\n            # For each other worker, add an entry in a queue, signaling that it can finish this step.\n            token = tf.constant(False)\n            with tf.control_dependencies(dependencies):\n                for i, q in enumerate(sync_queues):\n                    if i != self.task_index:\n                        queue_ops.append(q.enqueue(token))\n\n            # Drain tokens off queue for this worker, one for each other worker.\n            queue_ops.append(\n                sync_queues[self.task_index].dequeue_many(len(sync_queues) - 1))\n\n            return tf.group(*queue_ops, name=name)", "response": "Adds ops to enqueue on all worker queues and drain tokens off all workers."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates shadow variables on PS and replace variables in avg_grads with these shadow variables.", "response": "def _apply_shadow_vars(avg_grads):\n        \"\"\"\n        Create shadow variables on PS, and replace variables in avg_grads\n        by these shadow variables.\n\n        Args:\n            avg_grads: list of (grad, var) tuples\n        \"\"\"\n        ps_var_grads = []\n        for grad, var in avg_grads:\n            assert var.name.startswith('tower'), var.name\n            my_name = '/'.join(var.name.split('/')[1:])\n            my_name = get_op_tensor_name(my_name)[0]\n            new_v = tf.get_variable(my_name, dtype=var.dtype.base_dtype,\n                                    initializer=var.initial_value,\n                                    trainable=True)\n            # (g, v) to be applied, where v is global (ps vars)\n            ps_var_grads.append((grad, new_v))\n        return ps_var_grads"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _shadow_model_variables(shadow_vars):\n        G = tf.get_default_graph()\n        curr_shadow_vars = set([v.name for v in shadow_vars])\n        model_vars = tf.model_variables()\n        shadow_model_vars = []\n        for v in model_vars:\n            assert v.name.startswith('tower'), \"Found some MODEL_VARIABLES created outside of the tower function!\"\n            stripped_op_name, stripped_var_name = get_op_tensor_name(re.sub('^tower[0-9]+/', '', v.name))\n            if stripped_op_name in curr_shadow_vars:\n                continue\n            try:\n                G.get_tensor_by_name(stripped_var_name)\n                logger.warn(\"Model Variable {} also appears in other collections.\".format(stripped_var_name))\n                continue\n            except KeyError:\n                pass\n            new_v = tf.get_variable(stripped_op_name, dtype=v.dtype.base_dtype,\n                                    initializer=v.initial_value,\n                                    trainable=False)\n\n            curr_shadow_vars.add(stripped_op_name)  # avoid duplicated shadow_model_vars\n            shadow_vars.append(new_v)\n            shadow_model_vars.append((new_v, v))  # only need to sync model_var from one tower\n        return shadow_model_vars", "response": "Create shadow vars for model_variables as well and add to the list of shadow_vars."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build(self, get_grad_fn, get_opt_fn):\n        with override_to_local_variable():\n            get_global_step_var()\n\n        get_opt_fn = memoized(get_opt_fn)\n        # Build the optimizer first, before entering any tower.\n        # This makes sure that learning_rate is a global variable (what we expect)\n        get_opt_fn()    # TODO get_opt_fn called before main graph was built\n\n        # Ngpu * Nvar * 2\n        grad_list = DataParallelBuilder.build_on_towers(\n            self.towers, get_grad_fn,\n            devices=self.raw_devices,\n            use_vs=[True] * len(self.towers))  # open vs at each tower\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        avg_grads = aggregate_grads(\n            grad_list, colocation=False, devices=self.raw_devices)\n        with tf.device(self.param_server_device):\n            ps_var_grads = DistributedReplicatedBuilder._apply_shadow_vars(avg_grads)\n            var_update_ops = self._apply_gradients_and_copy(\n                get_opt_fn(), grad_list, ps_var_grads)\n            self._shadow_vars = [v for (__, v) in ps_var_grads]\n            self._shadow_model_vars = DistributedReplicatedBuilder._shadow_model_variables(self._shadow_vars)\n\n        # TODO add options to synchronize less\n        main_fetch = tf.group(*var_update_ops, name='main_fetches')\n        train_op = self._add_sync_queues_and_barrier(\n            'post_copy_barrier', [main_fetch])\n\n        # initial local_vars syncing\n        with tf.name_scope('initial_sync_variables'):\n            initial_sync_op = self._get_initial_sync_op()\n        if len(self._shadow_model_vars) and self.is_chief:\n            with tf.name_scope('sync_model_variables'):\n                model_sync_op = self._get_sync_model_vars_op()\n        else:\n            model_sync_op = None\n        return train_op, initial_sync_op, model_sync_op", "response": "Builds the current state of the tower and returns the current state of the tower."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply averaged gradients to ps vars and then copy the updated_value back to each tower.", "response": "def _apply_gradients_and_copy(self, opt, raw_grad_list, ps_var_grads):\n        \"\"\"\n        Apply averaged gradients to ps vars, and then copy the updated\n        variables back to each tower.\n\n        Args:\n            raw_grad_list: Ngpu x Nvar x 2 gradient list from all towers\n            ps_var_grads: Nvar x 2 (grad, ps_var)\n\n        Returns:\n            list of copy ops\n        \"\"\"\n        # TODO do this for variables together?\n        with tf.name_scope('apply_gradients'):\n            var_update_ops = []\n            for vid, (g, v) in enumerate(ps_var_grads):\n                # TODO do we put momentum variables into local or global?\n                apply_gradient_op = opt.apply_gradients([(g, v)])\n                barrier = self._add_sync_queues_and_barrier(\n                    'param_update_barrier_{}'.format(vid), [apply_gradient_op])\n                with tf.control_dependencies([barrier]), \\\n                        tf.device(self.cpu_device):\n                    updated_value = v.read_value()\n                    for towerid in range(self.nr_gpu):\n                        var_update_ops.append(\n                            raw_grad_list[towerid][vid][1].assign(updated_value))\n            return var_update_ops"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the op to copy - initialized all local variables from PS.", "response": "def _get_initial_sync_op(self):\n        \"\"\"\n        Get the op to copy-initialized all local variables from PS.\n        \"\"\"\n        def strip_port(s):\n            if s.endswith(':0'):\n                return s[:-2]\n            return s\n        local_vars = tf.local_variables()\n        local_var_by_name = dict([(strip_port(v.name), v) for v in local_vars])\n        ops = []\n        nr_shadow_vars = len(self._shadow_vars)\n        for v in self._shadow_vars:\n            vname = strip_port(v.name)\n            for i in range(self.nr_gpu):\n                name = 'tower%s/%s' % (i, vname)\n                assert name in local_var_by_name, \\\n                    \"Shadow variable {} doesn't match a corresponding local variable!\".format(v.name)\n                copy_to = local_var_by_name[name]\n                # logger.info(\"{} -> {}\".format(v.name, copy_to.name))\n                ops.append(copy_to.assign(v.read_value()))\n        return tf.group(*ops, name='sync_{}_variables_from_ps'.format(nr_shadow_vars))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the op to sync local model_variables to PS.", "response": "def _get_sync_model_vars_op(self):\n        \"\"\"\n        Get the op to sync local model_variables to PS.\n        \"\"\"\n        ops = []\n        for (shadow_v, local_v) in self._shadow_model_vars:\n            ops.append(shadow_v.assign(local_v.read_value()))\n        assert len(ops)\n        return tf.group(*ops, name='sync_{}_model_variables_to_ps'.format(len(ops)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_tensors_inputs(placeholders, tensors, names):\n    assert len(tensors) == len(names), \\\n        \"Input tensors {} and input names {} have different length!\".format(\n            tensors, names)\n    ret = copy.copy(placeholders)\n    placeholder_names = [p.name for p in placeholders]\n    for name, tensor in zip(names, tensors):\n        tensorname = get_op_tensor_name(name)[1]\n        try:\n            idx = placeholder_names.index(tensorname)\n        except ValueError:\n            logger.error(\"Name {} is not a model input!\".format(tensorname))\n            raise\n        ret[idx] = tensor\n    return ret", "response": "Returns a list of tensors that are used for the tower function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of objects with matching names.", "response": "def get_sublist_by_names(lst, names):\n    \"\"\"\n    Args:\n        lst (list): list of objects with \"name\" property.\n\n    Returns:\n        list: a sublist of objects, matching names\n    \"\"\"\n    orig_names = [p.name for p in lst]\n    ret = []\n    for name in names:\n        try:\n            idx = orig_names.index(name)\n        except ValueError:\n            logger.error(\"Name {} doesn't appear in lst {}!\".format(\n                name, str(orig_names)))\n            raise\n        ret.append(lst[idx])\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rpn_head(featuremap, channel, num_anchors):\n    with argscope(Conv2D, data_format='channels_first',\n                  kernel_initializer=tf.random_normal_initializer(stddev=0.01)):\n        hidden = Conv2D('conv0', featuremap, channel, 3, activation=tf.nn.relu)\n\n        label_logits = Conv2D('class', hidden, num_anchors, 1)\n        box_logits = Conv2D('box', hidden, 4 * num_anchors, 1)\n        # 1, NA(*4), im/16, im/16 (NCHW)\n\n        label_logits = tf.transpose(label_logits, [0, 2, 3, 1])  # 1xfHxfWxNA\n        label_logits = tf.squeeze(label_logits, 0)  # fHxfWxNA\n\n        shp = tf.shape(box_logits)  # 1x(NAx4)xfHxfW\n        box_logits = tf.transpose(box_logits, [0, 2, 3, 1])  # 1xfHxfWx(NAx4)\n        box_logits = tf.reshape(box_logits, tf.stack([shp[2], shp[3], num_anchors, 4]))  # fHxfWxNAx4\n    return label_logits, box_logits", "response": "Returns the label and box logits for the current class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):\n    with tf.device('/cpu:0'):\n        valid_mask = tf.stop_gradient(tf.not_equal(anchor_labels, -1))\n        pos_mask = tf.stop_gradient(tf.equal(anchor_labels, 1))\n        nr_valid = tf.stop_gradient(tf.count_nonzero(valid_mask, dtype=tf.int32), name='num_valid_anchor')\n        nr_pos = tf.identity(tf.count_nonzero(pos_mask, dtype=tf.int32), name='num_pos_anchor')\n        # nr_pos is guaranteed >0 in C4. But in FPN. even nr_valid could be 0.\n\n        valid_anchor_labels = tf.boolean_mask(anchor_labels, valid_mask)\n    valid_label_logits = tf.boolean_mask(label_logits, valid_mask)\n\n    with tf.name_scope('label_metrics'):\n        valid_label_prob = tf.nn.sigmoid(valid_label_logits)\n        summaries = []\n        with tf.device('/cpu:0'):\n            for th in [0.5, 0.2, 0.1]:\n                valid_prediction = tf.cast(valid_label_prob > th, tf.int32)\n                nr_pos_prediction = tf.reduce_sum(valid_prediction, name='num_pos_prediction')\n                pos_prediction_corr = tf.count_nonzero(\n                    tf.logical_and(\n                        valid_label_prob > th,\n                        tf.equal(valid_prediction, valid_anchor_labels)),\n                    dtype=tf.int32)\n                placeholder = 0.5   # A small value will make summaries appear lower.\n                recall = tf.cast(tf.truediv(pos_prediction_corr, nr_pos), tf.float32)\n                recall = tf.where(tf.equal(nr_pos, 0), placeholder, recall, name='recall_th{}'.format(th))\n                precision = tf.cast(tf.truediv(pos_prediction_corr, nr_pos_prediction), tf.float32)\n                precision = tf.where(tf.equal(nr_pos_prediction, 0),\n                                     placeholder, precision, name='precision_th{}'.format(th))\n                summaries.extend([precision, recall])\n        add_moving_summary(*summaries)\n\n    # Per-level loss summaries in FPN may appear lower due to the use of a small placeholder.\n    # But the total RPN loss will be fine.  TODO make the summary op smarter\n    placeholder = 0.\n    label_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.cast(valid_anchor_labels, tf.float32), logits=valid_label_logits)\n    label_loss = tf.reduce_sum(label_loss) * (1. / cfg.RPN.BATCH_PER_IM)\n    label_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name='label_loss')\n\n    pos_anchor_boxes = tf.boolean_mask(anchor_boxes, pos_mask)\n    pos_box_logits = tf.boolean_mask(box_logits, pos_mask)\n    delta = 1.0 / 9\n    box_loss = tf.losses.huber_loss(\n        pos_anchor_boxes, pos_box_logits, delta=delta,\n        reduction=tf.losses.Reduction.SUM) / delta\n    box_loss = box_loss * (1. / cfg.RPN.BATCH_PER_IM)\n    box_loss = tf.where(tf.equal(nr_pos, 0), placeholder, box_loss, name='box_loss')\n\n    add_moving_summary(label_loss, box_loss, nr_valid, nr_pos)\n    return [label_loss, box_loss]", "response": "Function that computes the loss of a single class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_rpn_proposals(boxes, scores, img_shape,\n                           pre_nms_topk, post_nms_topk=None):\n    \"\"\"\n    Sample RPN proposals by the following steps:\n    1. Pick top k1 by scores\n    2. NMS them\n    3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output.\n\n    Args:\n        boxes: nx4 float dtype, the proposal boxes. Decoded to floatbox already\n        scores: n float, the logits\n        img_shape: [h, w]\n        pre_nms_topk, post_nms_topk (int): See above.\n\n    Returns:\n        boxes: kx4 float\n        scores: k logits\n    \"\"\"\n    assert boxes.shape.ndims == 2, boxes.shape\n    if post_nms_topk is None:\n        post_nms_topk = pre_nms_topk\n\n    topk = tf.minimum(pre_nms_topk, tf.size(scores))\n    topk_scores, topk_indices = tf.nn.top_k(scores, k=topk, sorted=False)\n    topk_boxes = tf.gather(boxes, topk_indices)\n    topk_boxes = clip_boxes(topk_boxes, img_shape)\n\n    topk_boxes_x1y1x2y2 = tf.reshape(topk_boxes, (-1, 2, 2))\n    topk_boxes_x1y1, topk_boxes_x2y2 = tf.split(topk_boxes_x1y1x2y2, 2, axis=1)\n    # nx1x2 each\n    wbhb = tf.squeeze(topk_boxes_x2y2 - topk_boxes_x1y1, axis=1)\n    valid = tf.reduce_all(wbhb > cfg.RPN.MIN_SIZE, axis=1)  # n,\n    topk_valid_boxes_x1y1x2y2 = tf.boolean_mask(topk_boxes_x1y1x2y2, valid)\n    topk_valid_scores = tf.boolean_mask(topk_scores, valid)\n\n    # TODO not needed\n    topk_valid_boxes_y1x1y2x2 = tf.reshape(\n        tf.reverse(topk_valid_boxes_x1y1x2y2, axis=[2]),\n        (-1, 4), name='nms_input_boxes')\n    nms_indices = tf.image.non_max_suppression(\n        topk_valid_boxes_y1x1y2x2,\n        topk_valid_scores,\n        max_output_size=post_nms_topk,\n        iou_threshold=cfg.RPN.PROPOSAL_NMS_THRESH)\n\n    topk_valid_boxes = tf.reshape(topk_valid_boxes_x1y1x2y2, (-1, 4))\n    proposal_boxes = tf.gather(topk_valid_boxes, nms_indices)\n    proposal_scores = tf.gather(topk_valid_scores, nms_indices)\n    tf.sigmoid(proposal_scores, name='probs')  # for visualization\n    return tf.stop_gradient(proposal_boxes, name='boxes'), tf.stop_gradient(proposal_scores, name='scores')", "response": "Generate the RPN proposals for a set of boxes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef MergeAllSummaries(period=0, run_alone=False, key=None):\n    if key is None:\n        key = tf.GraphKeys.SUMMARIES\n    period = int(period)\n    if run_alone:\n        return MergeAllSummaries_RunAlone(period, key)\n    else:\n        return MergeAllSummaries_RunWithOp(period, key)", "response": "This callback is enabled by default."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef append(self, exp):\n        if self._curr_size < self.max_size:\n            self._assign(self._curr_pos, exp)\n            self._curr_pos = (self._curr_pos + 1) % self.max_size\n            self._curr_size += 1\n        else:\n            self._assign(self._curr_pos, exp)\n            self._curr_pos = (self._curr_pos + 1) % self.max_size", "response": "Append an entry to the internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsampling from the internal state", "response": "def sample(self, idx):\n        \"\"\" return a tuple of (s,r,a,o),\n            where s is of shape self._output_shape, which is\n            [H, W, (hist_len+1) * channel] if input is (H, W, channel)\"\"\"\n        idx = (self._curr_pos + idx) % self._curr_size\n        k = self.history_len + 1\n        if idx + k <= self._curr_size:\n            state = self.state[idx: idx + k]\n            reward = self.reward[idx: idx + k]\n            action = self.action[idx: idx + k]\n            isOver = self.isOver[idx: idx + k]\n        else:\n            end = idx + k - self._curr_size\n            state = self._slice(self.state, idx, end)\n            reward = self._slice(self.reward, idx, end)\n            action = self._slice(self.action, idx, end)\n            isOver = self._slice(self.isOver, idx, end)\n        ret = self._pad_sample(state, reward, action, isOver)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the environment for one step.", "response": "def step(self, exploration):\n        \"\"\"\n        Run the environment for one step.\n        If the episode ends, store the entire episode to the replay memory.\n        \"\"\"\n        old_s = self._current_ob\n        if self.rng.rand() <= exploration:\n            act = self.rng.choice(range(self.num_actions))\n        else:\n            history = self.recent_state()\n            history.append(old_s)\n            history = np.stack(history, axis=-1)  # state_shape + (Hist,)\n\n            # assume batched network\n            history = np.expand_dims(history, axis=0)\n            q_values = self.predictor(history)[0][0]  # this is the bottleneck\n            act = np.argmax(q_values)\n\n        self._current_ob, reward, isOver, info = self.player.step(act)\n        self._current_game_score.feed(reward)\n        self._current_episode.append(Experience(old_s, act, reward, isOver))\n\n        if isOver:\n            flush_experience = True\n            if 'ale.lives' in info:  # if running Atari, do something special\n                if info['ale.lives'] != 0:\n                    # only record score and flush experience\n                    # when a whole game is over (not when an episode is over)\n                    flush_experience = False\n            self.player.reset()\n\n            if flush_experience:\n                self.total_scores.append(self._current_game_score.sum)\n                self._current_game_score.reset()\n\n                # Ensure that the whole episode of experience is continuous in the replay buffer\n                with self.memory.writer_lock:\n                    for exp in self._current_episode:\n                        self.memory.append(exp)\n                self._current_episode.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the recent state of the environment.", "response": "def recent_state(self):\n        \"\"\"\n        Get the recent state (with stacked history) of the environment.\n\n        Returns:\n            a list of ``hist_len-1`` elements, each of shape ``self.state_shape``\n        \"\"\"\n        expected_len = self.history_len - 1\n        if len(self._current_episode) >= expected_len:\n            return [k.state for k in self._current_episode[-expected_len:]]\n        else:\n            states = [np.zeros(self.state_shape, dtype=self.dtype)] * (expected_len - len(self._current_episode))\n            states.extend([k.state for k in self._current_episode])\n            return states"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes one step in any of the runners.", "response": "def step(self, exploration):\n        \"\"\"\n        Execute one step in any of the runners.\n        \"\"\"\n        if len(self._runners) > 1:\n            self._populate_job_queue.put(exploration)\n        else:\n            self._runners[0].step(exploration)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets the stats of the runners to their mean and max values.", "response": "def reset_stats(self):\n        \"\"\"\n        Returns:\n            mean, max: two stats of the runners, to be added to backend\n        \"\"\"\n        scores = list(itertools.chain.from_iterable([v.total_scores for v in self._runners]))\n        for v in self._runners:\n            v.total_scores.clear()\n\n        try:\n            return np.mean(scores), np.max(scores)\n        except Exception:\n            logger.exception(\"Cannot compute total scores in EnvRunner.\")\n            return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log(self):\n\n        \"\"\" log the time of some heavy callbacks \"\"\"\n        if self.tot < 3:\n            return\n        msgs = []\n        for name, t in self.times:\n            if t / self.tot > 0.3 and t > 1:\n                msgs.append(name + \": \" + humanize_time_delta(t))\n        logger.info(\n            \"Callbacks took {:.3f} sec in total. {}\".format(\n                self.tot, '; '.join(msgs)))", "response": "log the time of some heavy callbacks"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef TowerContext(tower_name, is_training, vs_name=''):\n    if is_training:\n        return TrainTowerContext(tower_name, vs_name=vs_name)\n    else:\n        return PredictTowerContext(tower_name, vs_name=vs_name)", "response": "Returns a context for a tower function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a TowerTensorHandles containing only the training towers.", "response": "def training(self):\n        \"\"\"\n        Returns:\n            A :class:`TowerTensorHandles`, containing only the training towers.\n        \"\"\"\n        handles = [h for h in self._handles if h.is_training]\n        return TowerTensorHandles(handles)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a TowerTensorHandles containing only the inference towers.", "response": "def inference(self):\n        \"\"\"\n        Returns:\n            A :class:`TowerTensorHandles`, containing only the inference towers.\n        \"\"\"\n        handles = [h for h in self._handles if not h.is_training]\n        return TowerTensorHandles(handles)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a tensor in this tower.", "response": "def get_tensor(self, name):\n        \"\"\"\n        Get a tensor in this tower. The name can be:\n\n        1. The name of the tensor without any tower prefix.\n\n        2. A name in the input signature, if it is used when building the tower.\n\n        In the second case, this method will return the tensor that's used as the corresponding\n        input to the tower. Note that this tensor may have a different name (e.g. may be an output of a queue).\n        \"\"\"\n        name = get_op_tensor_name(name)[1]\n        if len(self.ns_name):\n            name_with_ns = self.ns_name + \"/\" + name\n        else:\n            name_with_ns = name\n\n        try:\n            ret = get_op_or_tensor_by_name(name_with_ns)\n        except KeyError:\n            if name in self._extra_tensor_names:\n                return self._extra_tensor_names[name]\n            raise\n        else:\n            if name in self._extra_tensor_names:\n                mapped_tensor = self._extra_tensor_names[name]\n                logger.info(\n                    \"'{}' may refer to both the Tensor/Placeholder '{}' or the input to the tower '{}'.\".format(\n                        name, ret.name, mapped_tensor.name) +\n                    \" Assuming it is the input '{}'.\".format(mapped_tensor.name))\n                return mapped_tensor\n            return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_variable(self, name):\n        name = get_op_tensor_name(name)[1]\n        if len(self.vs_name):\n            name_with_vs = self.vs_name + \"/\" + name\n        else:\n            name_with_vs = name\n        return get_op_or_tensor_by_name(name_with_vs)", "response": "Get a variable used in this tower."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_collection(self, key=None, name=None):\n        if name is not None:\n            logger.warn(\"TowerTensorHandle.get_collection(name=..) was renamed to (key=..) !\")\n            key = name\n        return self._ctx.get_collection_in_tower(key)", "response": "Get the collection of items in the tower."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mkdir_p(dirname):\n    assert dirname is not None\n    if dirname == '' or os.path.isdir(dirname):\n        return\n    try:\n        os.makedirs(dirname)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise e", "response": "Like mkdir - p but do nothing if the dir exists"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading a single file from a URL to a directory.", "response": "def download(url, dir, filename=None, expect_size=None):\n    \"\"\"\n    Download URL to a directory.\n    Will figure out the filename automatically from URL, if not given.\n    \"\"\"\n    mkdir_p(dir)\n    if filename is None:\n        filename = url.split('/')[-1]\n    fpath = os.path.join(dir, filename)\n\n    if os.path.isfile(fpath):\n        if expect_size is not None and os.stat(fpath).st_size == expect_size:\n            logger.info(\"File {} exists! Skip download.\".format(filename))\n            return fpath\n        else:\n            logger.warn(\"File {} exists. Will overwrite with a new download!\".format(filename))\n\n    def hook(t):\n        last_b = [0]\n\n        def inner(b, bsize, tsize=None):\n            if tsize is not None:\n                t.total = tsize\n            t.update((b - last_b[0]) * bsize)\n            last_b[0] = b\n        return inner\n    try:\n        with tqdm.tqdm(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n            fpath, _ = urllib.request.urlretrieve(url, fpath, reporthook=hook(t))\n        statinfo = os.stat(fpath)\n        size = statinfo.st_size\n    except IOError:\n        logger.error(\"Failed to download {}\".format(url))\n        raise\n    assert size > 0, \"Downloaded an empty file from {}!\".format(url)\n\n    if expect_size is not None and size != expect_size:\n        logger.error(\"File downloaded from {} does not match the expected size!\".format(url))\n        logger.error(\"You may have downloaded a broken file, or the upstream may have modified the file.\")\n\n    # TODO human-readable size\n    logger.info('Succesfully downloaded ' + filename + \". \" + str(size) + ' bytes.')\n    return fpath"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recursive_walk(rootdir):\n    for r, dirs, files in os.walk(rootdir):\n        for f in files:\n            yield os.path.join(r, f)", "response": "Recursively walks a directory and returns all files in it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the path to some dataset under the TENSORPACK_DATASET.", "response": "def get_dataset_path(*args):\n    \"\"\"\n    Get the path to some dataset under ``$TENSORPACK_DATASET``.\n\n    Args:\n        args: strings to be joined to form path.\n\n    Returns:\n        str: path to the dataset.\n    \"\"\"\n    d = os.environ.get('TENSORPACK_DATASET', None)\n    if d is None:\n        d = os.path.join(os.path.expanduser('~'), 'tensorpack_data')\n        if execute_only_once():\n            logger.warn(\"Env var $TENSORPACK_DATASET not set, using {} for datasets.\".format(d))\n        if not os.path.isdir(d):\n            mkdir_p(d)\n            logger.info(\"Created the directory {}.\".format(d))\n    assert os.path.isdir(d), d\n    return os.path.join(d, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of all the collections in the backup.", "response": "def backup_collection(keys=None):\n    \"\"\"\n    Args:\n        keys (list): list of collection keys to backup.\n            Defaults to all keys in the graph.\n\n    Returns:\n        dict: the backup\n    \"\"\"\n    if keys is None:\n        keys = tf.get_default_graph().get_all_collection_keys()\n    ret = {}\n    assert isinstance(keys, (list, tuple, set))\n    for k in keys:\n        ret[k] = copy(tf.get_collection(k))\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrestores from a collection backup.", "response": "def restore_collection(backup):\n    \"\"\"\n    Restore from a collection backup.\n\n    Args:\n        backup (dict):\n    \"\"\"\n    for k, v in six.iteritems(backup):\n        del tf.get_collection_ref(k)[:]\n        tf.get_collection_ref(k).extend(v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets items from this collection that are added in the current tower.", "response": "def get_collection_in_tower(self, key):\n        \"\"\"\n        Get items from this collection that are added in the current tower.\n        \"\"\"\n        new = tf.get_collection(key)\n        old = set(self.original.get(key, []))\n        # persist the order in new\n        return [x for x in new if x not in old]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ptb_producer(raw_data, batch_size, num_steps, name=None):\n  with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n\n    data_len = tf.size(raw_data)\n    batch_len = data_len // batch_size\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n                      [batch_size, batch_len])\n\n    epoch_size = (batch_len - 1) // num_steps\n    assertion = tf.assert_positive(\n        epoch_size,\n        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n    with tf.control_dependencies([assertion]):\n      epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n    x = tf.strided_slice(data, [0, i * num_steps],\n                         [batch_size, (i + 1) * num_steps])\n    x.set_shape([batch_size, num_steps])\n    y = tf.strided_slice(data, [0, i * num_steps + 1],\n                         [batch_size, (i + 1) * num_steps + 1])\n    y.set_shape([batch_size, num_steps])\n    return x, y", "response": "Iterate on the raw PTB data and produce Tensors that are drawn from these batches."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the directory for global logging.", "response": "def set_logger_dir(dirname, action=None):\n    \"\"\"\n    Set the directory for global logging.\n\n    Args:\n        dirname(str): log directory\n        action(str): an action of [\"k\",\"d\",\"q\"] to be performed\n            when the directory exists. Will ask user by default.\n\n                \"d\": delete the directory. Note that the deletion may fail when\n                the directory is used by tensorboard.\n\n                \"k\": keep the directory. This is useful when you resume from a\n                previous training and want the directory to look as if the\n                training was not interrupted.\n                Note that this option does not load old models or any other\n                old states for you. It simply does nothing.\n\n    \"\"\"\n    global LOG_DIR, _FILE_HANDLER\n    if _FILE_HANDLER:\n        # unload and close the old file handler, so that we may safely delete the logger directory\n        _logger.removeHandler(_FILE_HANDLER)\n        del _FILE_HANDLER\n\n    def dir_nonempty(dirname):\n        # If directory exists and nonempty (ignore hidden files), prompt for action\n        return os.path.isdir(dirname) and len([x for x in os.listdir(dirname) if x[0] != '.'])\n\n    if dir_nonempty(dirname):\n        if not action:\n            _logger.warn(\"\"\"\\\nLog directory {} exists! Use 'd' to delete it. \"\"\".format(dirname))\n            _logger.warn(\"\"\"\\\nIf you're resuming from a previous run, you can choose to keep it.\nPress any other key to exit. \"\"\")\n        while not action:\n            action = input(\"Select Action: k (keep) / d (delete) / q (quit):\").lower().strip()\n        act = action\n        if act == 'b':\n            backup_name = dirname + _get_time_str()\n            shutil.move(dirname, backup_name)\n            info(\"Directory '{}' backuped to '{}'\".format(dirname, backup_name))  # noqa: F821\n        elif act == 'd':\n            shutil.rmtree(dirname, ignore_errors=True)\n            if dir_nonempty(dirname):\n                shutil.rmtree(dirname, ignore_errors=False)\n        elif act == 'n':\n            dirname = dirname + _get_time_str()\n            info(\"Use a new log directory {}\".format(dirname))  # noqa: F821\n        elif act == 'k':\n            pass\n        else:\n            raise OSError(\"Directory {} exits!\".format(dirname))\n    LOG_DIR = dirname\n    from .fs import mkdir_p\n    mkdir_p(dirname)\n    _set_file(os.path.join(dirname, 'log.log'))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nuse :func:`logger.set_logger_dir` to set log directory to \"./train_log/{scriptname}:{name}\". \"scriptname\" is the name of the main python file currently running", "response": "def auto_set_dir(action=None, name=None):\n    \"\"\"\n    Use :func:`logger.set_logger_dir` to set log directory to\n    \"./train_log/{scriptname}:{name}\". \"scriptname\" is the name of the main python file currently running\"\"\"\n    mod = sys.modules['__main__']\n    basename = os.path.basename(mod.__file__)\n    auto_dirname = os.path.join('train_log', basename[:basename.rfind('.')])\n    if name:\n        auto_dirname += '_%s' % name if os.name == 'nt' else ':%s' % name\n    set_logger_dir(auto_dirname, action=action)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a base class - balanced cross entropy loss.", "response": "def class_balanced_sigmoid_cross_entropy(logits, label, name='cross_entropy_loss'):\n    \"\"\"\n    The class-balanced cross entropy loss,\n    as in `Holistically-Nested Edge Detection\n    <http://arxiv.org/abs/1504.06375>`_.\n\n    Args:\n        logits: of shape (b, ...).\n        label: of the same shape. the ground truth in {0,1}.\n    Returns:\n        class-balanced cross entropy loss.\n    \"\"\"\n    with tf.name_scope('class_balanced_sigmoid_cross_entropy'):\n        y = tf.cast(label, tf.float32)\n\n        count_neg = tf.reduce_sum(1. - y)\n        count_pos = tf.reduce_sum(y)\n        beta = count_neg / (count_neg + count_pos)\n\n        pos_weight = beta / (1 - beta)\n        cost = tf.nn.weighted_cross_entropy_with_logits(logits=logits, targets=y, pos_weight=pos_weight)\n        cost = tf.reduce_mean(cost * (1 - beta))\n        zero = tf.equal(count_pos, 0.0)\n    return tf.where(zero, 0.0, cost, name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef CaffeBilinearUpSample(x, shape):\n    inp_shape = x.shape.as_list()\n    ch = inp_shape[1]\n    assert ch == 1, \"This layer only works for channel=1\"\n    # for a version that supports >1 channels, see:\n    # https://github.com/tensorpack/tensorpack/issues/1040#issuecomment-452798180\n\n    shape = int(shape)\n    filter_shape = 2 * shape\n\n    def bilinear_conv_filler(s):\n        \"\"\"\n        s: width, height of the conv filter\n        https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/filler.hpp#L219-L268\n        \"\"\"\n        f = np.ceil(float(s) / 2)\n        c = float(2 * f - 1 - f % 2) / (2 * f)\n        ret = np.zeros((s, s), dtype='float32')\n        for x in range(s):\n            for y in range(s):\n                ret[x, y] = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n        return ret\n\n    w = bilinear_conv_filler(filter_shape)\n    w = np.repeat(w, ch * ch).reshape((filter_shape, filter_shape, ch, ch))\n\n    weight_var = tf.constant(w, tf.float32,\n                             shape=(filter_shape, filter_shape, ch, ch),\n                             name='bilinear_upsample_filter')\n    x = tf.pad(x, [[0, 0], [0, 0], [shape - 1, shape - 1], [shape - 1, shape - 1]], mode='SYMMETRIC')\n    out_shape = tf.shape(x) * tf.constant([1, 1, shape, shape], tf.int32)\n    deconv = tf.nn.conv2d_transpose(x, weight_var, out_shape,\n                                    [1, 1, shape, shape], 'SAME', data_format='NCHW')\n    edge = shape * (shape - 1)\n    deconv = deconv[:, :, edge:-edge, edge:-edge]\n\n    if inp_shape[2]:\n        inp_shape[2] *= shape\n    if inp_shape[3]:\n        inp_shape[3] *= shape\n    deconv.set_shape(inp_shape)\n    return deconv", "response": "A function that bilinearly upsamples the input images."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreset the state of the current object to its initial state.", "response": "def reset_state(self):\n        \"\"\"\n        All forked dataflows should only be reset **once and only once** in spawned processes.\n        Subclasses should call this method with super.\n        \"\"\"\n        assert not self._reset_done, \"reset_state() was called twice! This violates the API of DataFlow!\"\n        self._reset_done = True\n\n        # __del__ not guaranteed to get called at exit\n        atexit.register(del_weakref, weakref.ref(self))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_compatible_with(self, spec_or_tensor):\n    return (self._dtype.is_compatible_with(spec_or_tensor.dtype) and\n            self._shape.is_compatible_with(spec_or_tensor.shape))", "response": "Returns True if spec_or_tensor is compatible with this TensorSpec."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a description of the current model parameters.", "response": "def describe_trainable_vars():\n    \"\"\"\n    Print a description of the current model parameters.\n    Skip variables starting with \"tower\", as they are just duplicates built by data-parallel logic.\n    \"\"\"\n    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    if len(train_vars) == 0:\n        logger.warn(\"No trainable variables in the graph!\")\n        return\n    total = 0\n    total_bytes = 0\n    data = []\n    for v in train_vars:\n        if v.name.startswith('tower'):\n            continue\n        shape = v.get_shape()\n        ele = shape.num_elements()\n        if ele is None:\n            logger.warn(\"Shape of variable {} is not fully defined but {}.\".format(v.name, shape))\n            ele = 0\n        try:\n            shape = shape.as_list()\n        except ValueError:\n            shape = '<unknown>'\n\n        total += ele\n        total_bytes += ele * v.dtype.size\n        data.append([v.name, shape, ele, v.device, v.dtype.base_dtype.name])\n    headers = ['name', 'shape', '#elements', 'device', 'dtype']\n\n    dtypes = list(set([x[4] for x in data]))\n    if len(dtypes) == 1 and dtypes[0] == \"float32\":\n        # don't log the dtype if all vars are float32 (default dtype)\n        for x in data:\n            del x[4]\n        del headers[4]\n\n    devices = set([x[3] for x in data])\n    if len(devices) == 1:\n        # don't log the device if all vars on the same device\n        for x in data:\n            del x[3]\n        del headers[3]\n\n    table = tabulate(data, headers=headers)\n\n    size_mb = total_bytes / 1024.0**2\n    summary_msg = colored(\n        \"\\nNumber of trainable variables: {}\".format(len(data)) +\n        \"\\nNumber of parameters (elements): {}\".format(total) +\n        \"\\nStorage space needed for all trainable variables: {:.02f}MB\".format(size_mb),\n        'cyan')\n    logger.info(colored(\"List of Trainable Variables: \\n\", 'cyan') + table + summary_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_shape_str(tensors):\n    if isinstance(tensors, (list, tuple)):\n        for v in tensors:\n            assert isinstance(v, (tf.Tensor, tf.Variable)), \"Not a tensor: {}\".format(type(v))\n        shape_str = \",\".join(\n            map(lambda x: str(x.get_shape().as_list()), tensors))\n    else:\n        assert isinstance(tensors, (tf.Tensor, tf.Variable)), \"Not a tensor: {}\".format(type(tensors))\n        shape_str = str(tensors.get_shape().as_list())\n    return shape_str", "response": "Internally used by layer registry to print shapes of inputs and outputs of layers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contrastive_loss(left, right, y, margin, extra=False, scope=\"constrastive_loss\"):\n    with tf.name_scope(scope):\n        y = tf.cast(y, tf.float32)\n\n        delta = tf.reduce_sum(tf.square(left - right), 1)\n        delta_sqrt = tf.sqrt(delta + 1e-10)\n\n        match_loss = delta\n        missmatch_loss = tf.square(tf.nn.relu(margin - delta_sqrt))\n\n        loss = tf.reduce_mean(0.5 * (y * match_loss + (1 - y) * missmatch_loss))\n\n        if extra:\n            num_pos = tf.count_nonzero(y)\n            num_neg = tf.count_nonzero(1 - y)\n            pos_dist = tf.where(tf.equal(num_pos, 0), 0.,\n                                tf.reduce_sum(y * delta_sqrt) / tf.cast(num_pos, tf.float32),\n                                name=\"pos-dist\")\n            neg_dist = tf.where(tf.equal(num_neg, 0), 0.,\n                                tf.reduce_sum((1 - y) * delta_sqrt) / tf.cast(num_neg, tf.float32),\n                                name=\"neg-dist\")\n            return loss, pos_dist, neg_dist\n        else:\n            return loss", "response": "r Computes the contrastive loss for the given binary labels."}
