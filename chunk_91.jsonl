{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_mean_error(x1, x2=-1, function=\"MSE\"):\n    if function == \"MSE\":\n        return MSE(x1, x2)\n    elif function == \"MAE\":\n        return MAE(x1, x2)\n    elif function == \"RMSE\":\n        return RMSE(x1, x2)\n    else:\n        raise ValueError('The provided error function is not known')", "response": "This function returns the desired mean error value from the first series or error of the second series."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef adapt(self, d, x):\n        y = np.dot(self.w, x)\n        e = d - y\n        R1 = np.dot(np.dot(np.dot(self.R,x),x.T),self.R)\n        R2 = self.mu + np.dot(np.dot(x,self.R),x.T)\n        self.R = 1/self.mu * (self.R - R1/R2)\n        dw = np.dot(self.R, x.T) * e\n        self.w += dw", "response": "Adapt weights according one desired value and its input."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef adapt(self, d, x):\n        # create input matrix and target vector\n        self.x_mem[:,1:] = self.x_mem[:,:-1]\n        self.x_mem[:,0] = x\n        self.d_mem[1:] = self.d_mem[:-1]\n        self.d_mem[0] = d\n        # estimate output and error\n        self.y_mem = np.dot(self.x_mem.T, self.w)\n        self.e_mem = self.d_mem - self.y_mem\n        # update\n        dw_part1 = np.dot(self.x_mem.T, self.x_mem) + self.ide_eps\n        dw_part2 = np.linalg.solve(dw_part1, self.ide)\n        dw = np.dot(self.x_mem, np.dot(dw_part2, self.e_mem))\n        self.w += self.mu * dw", "response": "Adapt weights according one desired value and its input."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, d, x):\n        # measure the data and check if the dimmension agree\n        N = len(x)\n        if not len(d) == N:\n            raise ValueError('The length of vector d and matrix x must agree.')  \n        self.n = len(x[0])\n        # prepare data\n        try:    \n            x = np.array(x)\n            d = np.array(d)\n        except:\n            raise ValueError('Impossible to convert x or d to a numpy array')\n        # create empty arrays\n        y = np.zeros(N)\n        e = np.zeros(N)\n        self.w_history = np.zeros((N,self.n))\n        # adaptation loop\n        for k in range(N):\n            self.w_history[k,:] = self.w\n            # create input matrix and target vector\n            self.x_mem[:,1:] = self.x_mem[:,:-1]\n            self.x_mem[:,0] = x[k]\n            self.d_mem[1:] = self.d_mem[:-1]\n            self.d_mem[0] = d[k]\n            # estimate output and error\n            self.y_mem = np.dot(self.x_mem.T, self.w)\n            self.e_mem = self.d_mem - self.y_mem\n            y[k] = self.y_mem[0]\n            e[k] = self.e_mem[0]\n            # update\n            dw_part1 = np.dot(self.x_mem.T, self.x_mem) + self.ide_eps\n            dw_part2 = np.linalg.solve(dw_part1, self.ide)\n            dw = np.dot(self.x_mem, np.dot(dw_part2, self.e_mem))\n            self.w += self.mu * dw           \n        return y, e, self.w_history", "response": "This function filters multiple samples in a row."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef LDA_base(x, labels):\n    classes = np.array(tuple(set(labels)))\n    cols = x.shape[1]\n    # mean values for every class\n    means = np.zeros((len(classes), cols))\n    for i, cl in enumerate(classes):\n        means[i] = np.mean(x[labels==cl], axis=0)\n    # scatter matrices\n    scatter_within = np.zeros((cols, cols))\n    for cl, mean in zip(classes, means):\n        scatter_class = np.zeros((cols, cols))\n        for row in x[labels == cl]:\n            dif = row - mean\n            scatter_class += np.dot(dif.reshape(cols, 1), dif.reshape(1, cols))\n        scatter_within += scatter_class\n    total_mean = np.mean(x, axis=0)\n    scatter_between = np.zeros((cols, cols))\n    for cl, mean in zip(classes, means):\n        dif = mean - total_mean\n        dif_product = np.dot(dif.reshape(cols, 1), dif.reshape(1, cols))\n        scatter_between += x[labels == cl, :].shape[0] * dif_product\n    # eigenvalues and eigenvectors from scatter matrices\n    scatter_product = np.dot(np.linalg.inv(scatter_within), scatter_between)\n    eigen_values, eigen_vectors = np.linalg.eig(scatter_product)\n    return eigen_values, eigen_vectors", "response": "Base function used for Linear Discriminant Analysis."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef LDA_discriminants(x, labels):\n    # validate inputs\n    try:    \n        x = np.array(x)\n    except:\n        raise ValueError('Impossible to convert x to a numpy array.')\n    # make the LDA\n    eigen_values, eigen_vectors = LDA_base(x, labels)\n    return eigen_values[(-eigen_values).argsort()]", "response": "Linear Discriminant Analysis helper for determination how many columns of data should be reduced."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadapt weights according one desired value and its input.", "response": "def adapt(self, d, x):\n        \"\"\"\n        Adapt weights according one desired value and its input.\n\n        **Args:**\n\n        * `d` : desired value (float)\n\n        * `x` : input array (1-dimensional array)\n        \"\"\"\n        y = np.dot(self.w, x)\n        e = d - y\n        self.eps = self.eps - self.ro * self.mu * e * self.last_e * \\\n            np.dot(x, self.last_x) / \\\n            (np.dot(self.last_x, self.last_x) + self.eps)**2\n        nu = self.mu / (self.eps + np.dot(x, x))\n        self.w += nu * e * x        \n        self.last_e = e"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadapt weights according one desired value and its input.", "response": "def adapt(self, d, x):\n        \"\"\"\n        Adapt weights according one desired value and its input.\n\n        Args:\n\n        * `d` : desired value (float)\n\n        * `x` : input array (1-dimensional array)\n        \"\"\"\n        self.update_memory_x(x)\n        m_d, m_x = self.read_memory()\n        # estimate\n        y = np.dot(self.w, x-m_x) + m_d\n        e = d - y\n        nu = self.mu / (self.eps + np.dot(x-m_x, x-m_x))\n        dw = nu * e * (x-m_x)\n        self.w += dw\n        self.update_memory_d(d)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_memory(self):\n        if self.mem_empty == True:\n            if self.mem_idx == 0:\n                m_x = np.zeros(self.n)\n                m_d = 0\n            else:\n                m_x = np.mean(self.mem_x[:self.mem_idx+1], axis=0)\n                m_d = np.mean(self.mem_d[:self.mem_idx])\n        else:\n            m_x = np.mean(self.mem_x, axis=0)\n            m_d = np.mean(np.delete(self.mem_d, self.mem_idx))\n        self.mem_idx += 1\n        if self.mem_idx > len(self.mem_x)-1:\n            self.mem_idx = 0\n            self.mem_empty = False\n        return m_d, m_x", "response": "This function read mean value of target d and input vector x from history\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction that filters data with selected adaptive filter.", "response": "def filter_data(d, x, model=\"lms\", **kwargs):\n    \"\"\"\n    Function that filter data with selected adaptive filter.\n    \n    **Args:**\n\n    * `d` : desired value (1 dimensional array)\n\n    * `x` : input matrix (2-dimensional array). Rows are samples, columns are\n      input arrays.\n        \n    **Kwargs:**\n    \n    * Any key argument that can be accepted with selected filter model. \n      For more information see documentation of desired adaptive filter.\n\n    **Returns:**\n\n    * `y` : output value (1 dimensional array).\n      The size corresponds with the desired value.\n\n    * `e` : filter error for every sample (1 dimensional array). \n      The size corresponds with the desired value.\n\n    * `w` : history of all weights (2 dimensional array).\n      Every row is set of the weights for given sample.\n    \n    \"\"\"\n    # overwrite n with correct size\n    kwargs[\"n\"] = x.shape[1]\n    # create filter according model\n    if model in [\"LMS\", \"lms\"]:\n        f = FilterLMS(**kwargs)\n    elif model in [\"NLMS\", \"nlms\"]:\n        f = FilterNLMS(**kwargs)\n    elif model in [\"RLS\", \"rls\"]:\n        f = FilterRLS(**kwargs)\n    elif model in [\"GNGD\", \"gngd\"]:\n        f = FilterGNGD(**kwargs)\n    elif model in [\"AP\", \"ap\"]:\n        f = FilterAP(**kwargs)\n    elif model in [\"LMF\", \"lmf\"]:\n        f = FilterLMF(**kwargs)\n    elif model in [\"NLMF\", \"nlmf\"]:\n        f = FilterNLMF(**kwargs)\n    else:\n        raise ValueError('Unknown model of filter {}'.format(model))\n    # calculate and return the values\n    y, e, w = f.run(d, x)\n    return y, e, w"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction that filters data with selected adaptive filter model.", "response": "def AdaptiveFilter(model=\"lms\", **kwargs):\n    \"\"\"\n    Function that filter data with selected adaptive filter.\n    \n    **Args:**\n\n    * `d` : desired value (1 dimensional array)\n\n    * `x` : input matrix (2-dimensional array). Rows are samples, columns are \n      input arrays.\n        \n    **Kwargs:**\n    \n    * Any key argument that can be accepted with selected filter model.\n      For more information see documentation of desired adaptive filter.\n    \n    * It should be at least filter size `n`.  \n\n    **Returns:**\n\n    * `y` : output value (1 dimensional array).\n      The size corresponds with the desired value.\n\n    * `e` : filter error for every sample (1 dimensional array). \n      The size corresponds with the desired value.\n\n    * `w` : history of all weights (2 dimensional array).\n      Every row is set of the weights for given sample.\n    \n    \"\"\"\n    # check if the filter size was specified\n    if not \"n\" in kwargs:\n        raise ValueError('Filter size is not defined (n=?).')    \n    # create filter according model\n    if model in [\"LMS\", \"lms\"]:\n        f = FilterLMS(**kwargs)\n    elif model in [\"NLMS\", \"nlms\"]:\n        f = FilterNLMS(**kwargs)\n    elif model in [\"RLS\", \"rls\"]:\n        f = FilterRLS(**kwargs)\n    elif model in [\"GNGD\", \"gngd\"]:\n        f = FilterGNGD(**kwargs)\n    elif model in [\"AP\", \"ap\"]:\n        f = FilterAP(**kwargs)\n    elif model in [\"LMF\", \"lmf\"]:\n        f = FilterLMF(**kwargs)\n    elif model in [\"NLMF\", \"nlmf\"]:\n        f = FilterNLMF(**kwargs)\n    else:\n        raise ValueError('Unknown model of filter {}'.format(model))\n    # return filter\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, w, e):\n        if len(w.shape) == 1:\n            e = self.activation(self.y, f=self.f, der=True) * e * w\n            dw = self.mu * np.outer(e, self.x)\n        else:\n            e = self.activation(self.y, f=self.f, der=True) * (1 - self.y) * np.dot(e, w)\n            dw = self.mu * np.outer(e, self.x)\n        w = self.w[:,1:]\n        self.w += dw\n        return w, e", "response": "This function makes an update according provided target and the last used input vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions for training of the MLP.", "response": "def train(self, x, d, epochs=10, shuffle=False):\n        \"\"\"\n        Function for batch training of MLP.\n\n        **Args:**\n\n        * `x` : input array (2-dimensional array).\n            Every row represents one input vector (features).\n\n        * `d` : input array (n-dimensional array).\n            Every row represents target for one input vector.\n            Target can be one or more values (in case of multiple outputs).\n\n        **Kwargs:**\n        \n        * `epochs` : amount of epochs (int). That means how many times\n            the MLP will iterate over the passed set of data (`x`, `d`).\n\n        * `shuffle` : if true, the order of inputs and outpust are shuffled (bool).\n            That means the pairs input-output are in different order in every epoch.\n\n        **Returns:**\n        \n        * `e`: output vector (m-dimensional array). Every row represents\n            error (or errors) for an input and output in given epoch.\n            The size of this array is length of provided data times\n            amount of epochs (`N*epochs`).\n\n        * `MSE` : mean squared error (1-dimensional array). Every value\n            stands for MSE of one epoch.\n            \n        \"\"\"\n        # measure the data and check if the dimmension agree\n        N = len(x)\n        if not len(d) == N:\n            raise ValueError('The length of vector d and matrix x must agree.')  \n        if not len(x[0]) == self.n_input:\n            raise ValueError('The number of network inputs is not correct.')\n        if self.outputs == 1:\n            if not len(d.shape) == 1:\n                raise ValueError('For one output MLP the d must have one dimension')\n        else:\n            if not d.shape[1] == self.outputs:\n                raise ValueError('The number of outputs must agree with number of columns in d')\n        try:    \n            x = np.array(x)\n            d = np.array(d)\n        except:\n            raise ValueError('Impossible to convert x or d to a numpy array')\n        # create empty arrays\n        if self.outputs == 1:\n            e = np.zeros(epochs*N)\n        else:\n            e = np.zeros((epochs*N, self.outputs))\n        MSE = np.zeros(epochs)\n        # shuffle data if demanded\n        if shuffle:\n            randomize = np.arange(len(x))\n            np.random.shuffle(randomize)\n            x = x[randomize]\n            d = d[randomize]\n        # adaptation loop\n        for epoch in range(epochs):\n            for k in range(N):\n                self.predict(x[k])\n                e[(epoch*N)+k] = self.update(d[k])\n            MSE[epoch] = np.sum(e[epoch*N:(epoch+1)*N-1]**2) / N\n        return e, MSE"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, x):\n        # measure the data and check if the dimmension agree\n        try:    \n            x = np.array(x)\n        except:\n            raise ValueError('Impossible to convert x to a numpy array')\n        N = len(x)   \n        # create empty arrays     \n        if self.outputs == 1:\n            y = np.zeros(N)\n        else:\n            y = np.zeros((N, self.outputs))\n        # predict data in loop        \n        for k in range(N):\n            y[k] = self.predict(x[k])\n        return y", "response": "Function for batch usage of already trained and tested MLP."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict(self, x):\n        # forward pass to hidden layers\n        for l in self.layers:\n            x = l.predict(x)\n        self.x[1:] = x\n        # forward pass to output layer\n        if self.outputs == 1:\n            self.y = np.dot(self.w, self.x)\n        else: \n            self.y = np.sum(self.w*self.x, axis=1)\n        return self.y", "response": "This function predicts the state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef PCA_components(x):\n    # validate inputs\n    try:    \n        x = np.array(x)\n    except:\n        raise ValueError('Impossible to convert x to a numpy array.')\n    # eigen values and eigen vectors of data covariance matrix\n    eigen_values, eigen_vectors = np.linalg.eig(np.cov(x.T))\n    # sort eigen vectors according biggest eigen value\n    eigen_order = eigen_vectors.T[(-eigen_values).argsort()]\n    # form output - order the eigenvalues\n    return eigen_values[(-eigen_values).argsort()]", "response": "Principal Component Analysis helper to check out eigenvalues of components."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef PCA(x, n=False):\n    # select n if not provided\n    if not n:\n        n = x.shape[1] - 1   \n    # validate inputs\n    try:    \n        x = np.array(x)\n    except:\n        raise ValueError('Impossible to convert x to a numpy array.')\n    assert type(n) == int, \"Provided n is not an integer.\"\n    assert x.shape[1] > n, \"The requested n is bigger than \\\n        number of features in x.\"\n    # eigen values and eigen vectors of data covariance matrix\n    eigen_values, eigen_vectors = np.linalg.eig(np.cov(x.T))\n    # sort eigen vectors according biggest eigen value\n    eigen_order = eigen_vectors.T[(-eigen_values).argsort()]\n    # form output - reduced x matrix\n    return eigen_order[:n].dot(x.T).T", "response": "Principal Component Analysis Function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean_axis(axis):\n    axis.get_xaxis().set_ticks([])\n    axis.get_yaxis().set_ticks([])\n    for spine in list(axis.spines.values()):\n        spine.set_visible(False)", "response": "Remove ticks tick labels and frame from axis"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_seaborn_colorbar(dfr, classes):\n    levels = sorted(list(set(classes.values())))\n    paldict = {\n        lvl: pal\n        for (lvl, pal) in zip(\n            levels,\n            sns.cubehelix_palette(\n                len(levels), light=0.9, dark=0.1, reverse=True, start=1, rot=-2\n            ),\n        )\n    }\n    lvl_pal = {cls: paldict[lvl] for (cls, lvl) in list(classes.items())}\n    col_cb = pd.Series(dfr.index).map(lvl_pal)\n    # The col_cb Series index now has to match the dfr.index, but\n    # we don't create the Series with this (and if we try, it\n    # fails) - so change it with this line\n    col_cb.index = dfr.index\n    return col_cb", "response": "Return a colorbar representing classes for a Seaborn plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns labels guaranteed to correspond to the dataframe.", "response": "def get_safe_seaborn_labels(dfr, labels):\n    \"\"\"Returns labels guaranteed to correspond to the dataframe.\"\"\"\n    if labels is not None:\n        return [labels.get(i, i) for i in dfr.index]\n    return [i for i in dfr.index]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_seaborn_clustermap(dfr, params, title=None, annot=True):\n    fig = sns.clustermap(\n        dfr,\n        cmap=params.cmap,\n        vmin=params.vmin,\n        vmax=params.vmax,\n        col_colors=params.colorbar,\n        row_colors=params.colorbar,\n        figsize=(params.figsize, params.figsize),\n        linewidths=params.linewidths,\n        xticklabels=params.labels,\n        yticklabels=params.labels,\n        annot=annot,\n    )\n    fig.cax.yaxis.set_label_position(\"left\")\n    if title:\n        fig.cax.set_ylabel(title)\n\n    # Rotate ticklabels\n    fig.ax_heatmap.set_xticklabels(fig.ax_heatmap.get_xticklabels(), rotation=90)\n    fig.ax_heatmap.set_yticklabels(fig.ax_heatmap.get_yticklabels(), rotation=0)\n\n    # Return clustermap\n    return fig", "response": "Returns a Seaborn clustermap."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn seaborn heatmap with cluster dendrograms.", "response": "def heatmap_seaborn(dfr, outfilename=None, title=None, params=None):\n    \"\"\"Returns seaborn heatmap with cluster dendrograms.\n\n    - dfr - pandas DataFrame with relevant data\n    - outfilename - path to output file (indicates output format)\n    \"\"\"\n    # Decide on figure layout size: a minimum size is required for\n    # aesthetics, and a maximum to avoid core dumps on rendering.\n    # If we hit the maximum size, we should modify font size.\n    maxfigsize = 120\n    calcfigsize = dfr.shape[0] * 1.1\n    figsize = min(max(8, calcfigsize), maxfigsize)\n    if figsize == maxfigsize:\n        scale = maxfigsize / calcfigsize\n        sns.set_context(\"notebook\", font_scale=scale)\n\n    # Add a colorbar?\n    if params.classes is None:\n        col_cb = None\n    else:\n        col_cb = get_seaborn_colorbar(dfr, params.classes)\n\n    # Labels are defined before we build the clustering\n    # If a label mapping is missing, use the key text as fall back\n    params.labels = get_safe_seaborn_labels(dfr, params.labels)\n\n    # Add attributes to parameter object, and draw heatmap\n    params.colorbar = col_cb\n    params.figsize = figsize\n    params.linewidths = 0.25\n    fig = get_seaborn_clustermap(dfr, params, title=title)\n\n    # Save to file\n    if outfilename:\n        fig.savefig(outfilename)\n\n    # Return clustermap\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a dendrogram and corresponding gridspec attached to the figure.", "response": "def add_mpl_dendrogram(dfr, fig, heatmap_gs, orientation=\"col\"):\n    \"\"\"Return a dendrogram and corresponding gridspec, attached to the fig\n\n    Modifies the fig in-place. Orientation is either 'row' or 'col' and\n    determines location and orientation of the rendered dendrogram.\n    \"\"\"\n    # Row or column axes?\n    if orientation == \"row\":\n        dists = distance.squareform(distance.pdist(dfr))\n        spec = heatmap_gs[1, 0]\n        orient = \"left\"\n        nrows, ncols = 1, 2\n        height_ratios = [1]\n    else:  # Column dendrogram\n        dists = distance.squareform(distance.pdist(dfr.T))\n        spec = heatmap_gs[0, 1]\n        orient = \"top\"\n        nrows, ncols = 2, 1\n        height_ratios = [1, 0.15]\n\n    # Create row dendrogram axis\n    gspec = gridspec.GridSpecFromSubplotSpec(\n        nrows,\n        ncols,\n        subplot_spec=spec,\n        wspace=0.0,\n        hspace=0.1,\n        height_ratios=height_ratios,\n    )\n    dend_axes = fig.add_subplot(gspec[0, 0])\n    dend = sch.dendrogram(\n        sch.linkage(distance.squareform(dists), method=\"complete\"),\n        color_threshold=np.inf,\n        orientation=orient,\n    )\n    clean_axis(dend_axes)\n    return {\"dendrogram\": dend, \"gridspec\": gspec}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn axis for Matplotlib heatmap.", "response": "def get_mpl_heatmap_axes(dfr, fig, heatmap_gs):\n    \"\"\"Return axis for Matplotlib heatmap.\"\"\"\n    # Create heatmap axis\n    heatmap_axes = fig.add_subplot(heatmap_gs[1, 1])\n    heatmap_axes.set_xticks(np.linspace(0, dfr.shape[0] - 1, dfr.shape[0]))\n    heatmap_axes.set_yticks(np.linspace(0, dfr.shape[0] - 1, dfr.shape[0]))\n    heatmap_axes.grid(False)\n    heatmap_axes.xaxis.tick_bottom()\n    heatmap_axes.yaxis.tick_right()\n    return heatmap_axes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd colourbars to Matplotlib heatmap.", "response": "def add_mpl_colorbar(dfr, fig, dend, params, orientation=\"row\"):\n    \"\"\"Add class colorbars to Matplotlib heatmap.\"\"\"\n    for name in dfr.index[dend[\"dendrogram\"][\"leaves\"]]:\n        if name not in params.classes:\n            params.classes[name] = name\n\n    # Assign a numerical value to each class, for mpl\n    classdict = {cls: idx for (idx, cls) in enumerate(params.classes.values())}\n\n    # colourbar\n    cblist = []\n    for name in dfr.index[dend[\"dendrogram\"][\"leaves\"]]:\n        try:\n            cblist.append(classdict[params.classes[name]])\n        except KeyError:\n            cblist.append(classdict[name])\n    colbar = pd.Series(cblist)\n\n    # Create colourbar axis - could capture if needed\n    if orientation == \"row\":\n        cbaxes = fig.add_subplot(dend[\"gridspec\"][0, 1])\n        cbaxes.imshow(\n            [[cbar] for cbar in colbar.values],\n            cmap=plt.get_cmap(pyani_config.MPL_CBAR),\n            interpolation=\"nearest\",\n            aspect=\"auto\",\n            origin=\"lower\",\n        )\n    else:\n        cbaxes = fig.add_subplot(dend[\"gridspec\"][1, 0])\n        cbaxes.imshow(\n            [colbar],\n            cmap=plt.get_cmap(pyani_config.MPL_CBAR),\n            interpolation=\"nearest\",\n            aspect=\"auto\",\n            origin=\"lower\",\n        )\n    clean_axis(cbaxes)\n    return colbar"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds labels to Matplotlib heatmap axes in - place.", "response": "def add_mpl_labels(heatmap_axes, rowlabels, collabels, params):\n    \"\"\"Add labels to Matplotlib heatmap axes, in-place.\"\"\"\n    if params.labels:\n        # If a label mapping is missing, use the key text as fall back\n        rowlabels = [params.labels.get(lab, lab) for lab in rowlabels]\n        collabels = [params.labels.get(lab, lab) for lab in collabels]\n    xlabs = heatmap_axes.set_xticklabels(collabels)\n    ylabs = heatmap_axes.set_yticklabels(rowlabels)\n    for label in xlabs:  # Rotate column labels\n        label.set_rotation(90)\n    for labset in (xlabs, ylabs):  # Smaller font\n        for label in labset:\n            label.set_fontsize(8)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding colour scale to heatmap.", "response": "def add_mpl_colorscale(fig, heatmap_gs, ax_map, params, title=None):\n    \"\"\"Add colour scale to heatmap.\"\"\"\n    # Set tick intervals\n    cbticks = [params.vmin + e * params.vdiff for e in (0, 0.25, 0.5, 0.75, 1)]\n    if params.vmax > 10:\n        exponent = int(floor(log10(params.vmax))) - 1\n        cbticks = [int(round(e, -exponent)) for e in cbticks]\n\n    scale_subplot = gridspec.GridSpecFromSubplotSpec(\n        1, 3, subplot_spec=heatmap_gs[0, 0], wspace=0.0, hspace=0.0\n    )\n    scale_ax = fig.add_subplot(scale_subplot[0, 1])\n    cbar = fig.colorbar(ax_map, scale_ax, ticks=cbticks)\n    if title:\n        cbar.set_label(title, fontsize=6)\n    cbar.ax.yaxis.set_ticks_position(\"left\")\n    cbar.ax.yaxis.set_label_position(\"left\")\n    cbar.ax.tick_params(labelsize=6)\n    cbar.outline.set_linewidth(0)\n    return cbar"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns matplotlib heatmap with cluster dendrograms.", "response": "def heatmap_mpl(dfr, outfilename=None, title=None, params=None):\n    \"\"\"Returns matplotlib heatmap with cluster dendrograms.\n\n    - dfr - pandas DataFrame with relevant data\n    - outfilename - path to output file (indicates output format)\n    - params - a list of parameters for plotting: [colormap, vmin, vmax]\n    - labels - dictionary of alternative labels, keyed by default sequence\n               labels\n    - classes - dictionary of sequence classes, keyed by default sequence\n                labels\n    \"\"\"\n    # Layout figure grid and add title\n    # Set figure size by the number of rows in the dataframe\n    figsize = max(8, dfr.shape[0] * 0.175)\n    fig = plt.figure(figsize=(figsize, figsize))\n    # if title:\n    #     fig.suptitle(title)\n    heatmap_gs = gridspec.GridSpec(\n        2, 2, wspace=0.0, hspace=0.0, width_ratios=[0.3, 1], height_ratios=[0.3, 1]\n    )\n\n    # Add column and row dendrograms/axes to figure\n    coldend = add_mpl_dendrogram(dfr, fig, heatmap_gs, orientation=\"col\")\n    rowdend = add_mpl_dendrogram(dfr, fig, heatmap_gs, orientation=\"row\")\n\n    # Add heatmap axes to figure, with rows/columns as in the dendrograms\n    heatmap_axes = get_mpl_heatmap_axes(dfr, fig, heatmap_gs)\n    ax_map = heatmap_axes.imshow(\n        dfr.iloc[rowdend[\"dendrogram\"][\"leaves\"], coldend[\"dendrogram\"][\"leaves\"]],\n        interpolation=\"nearest\",\n        cmap=params.cmap,\n        origin=\"lower\",\n        vmin=params.vmin,\n        vmax=params.vmax,\n        aspect=\"auto\",\n    )\n\n    # Are there class colourbars to add?\n    if params.classes is not None:\n        add_mpl_colorbar(dfr, fig, coldend, params, orientation=\"col\")\n        add_mpl_colorbar(dfr, fig, rowdend, params, orientation=\"row\")\n\n    # Add heatmap labels\n    add_mpl_labels(\n        heatmap_axes,\n        dfr.index[rowdend[\"dendrogram\"][\"leaves\"]],\n        dfr.index[coldend[\"dendrogram\"][\"leaves\"]],\n        params,\n    )\n\n    # Add colour scale\n    add_mpl_colorscale(fig, heatmap_gs, ax_map, params, title)\n\n    # Return figure output, and write, if required\n    plt.subplots_adjust(top=0.85)  # Leave room for title\n    # fig.set_tight_layout(True)\n    # We know that there is a UserWarning here about tight_layout and\n    # using the Agg renderer on OSX, so catch and ignore it, for cleanliness.\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        heatmap_gs.tight_layout(fig, h_pad=0.1, w_pad=0.5)\n    if outfilename:\n        fig.savefig(outfilename)\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and runs the jobs in the passed jobgraph in a single thread.", "response": "def run_dependency_graph(jobgraph, workers=None, logger=None):\n    \"\"\"Creates and runs pools of jobs based on the passed jobgraph.\n\n    - jobgraph - list of jobs, which may have dependencies.\n    - verbose - flag for multiprocessing verbosity\n    - logger - a logger module logger (optional)\n\n    The strategy here is to loop over each job in the list of jobs (jobgraph),\n    and create/populate a series of Sets of commands, to be run in\n    reverse order with multiprocessing_run as asynchronous pools.\n    \"\"\"\n    cmdsets = []\n    for job in jobgraph:\n        cmdsets = populate_cmdsets(job, cmdsets, depth=1)\n\n    # Put command sets in reverse order, and submit to multiprocessing_run\n    cmdsets.reverse()\n    cumretval = 0\n    for cmdset in cmdsets:\n        if logger:  # Try to be informative, if the logger module is being used\n            logger.info(\"Command pool now running:\")\n            for cmd in cmdset:\n                logger.info(cmd)\n        cumretval += multiprocessing_run(cmdset, workers)\n        if logger:  # Try to be informative, if the logger module is being used\n            logger.info(\"Command pool done.\")\n    return cumretval"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef populate_cmdsets(job, cmdsets, depth):\n    if len(cmdsets) < depth:\n        cmdsets.append(set())\n    cmdsets[depth-1].add(job.command)\n    if len(job.dependencies) == 0:\n        return cmdsets\n    for j in job.dependencies:\n        cmdsets = populate_cmdsets(j, cmdsets, depth+1)\n    return cmdsets", "response": "Creates a list of sets containing jobs at different depths of the\n    dependency tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndistribute passed command - line jobs using multiprocessing.", "response": "def multiprocessing_run(cmdlines, workers=None):\n    \"\"\"Distributes passed command-line jobs using multiprocessing.\n\n    - cmdlines - an iterable of command line strings\n\n    Returns the sum of exit codes from each job that was run. If\n    all goes well, this should be 0. Anything else and the calling\n    function should act accordingly.\n    \"\"\"\n    # Run jobs\n    # If workers is None or greater than the number of cores available,\n    # it will be set to the maximum number of cores\n    pool = multiprocessing.Pool(processes=workers)\n    results = [pool.apply_async(subprocess.run, (str(cline), ),\n                                {'shell': sys.platform != \"win32\",\n                                 'stdout': subprocess.PIPE,\n                                 'stderr': subprocess.PIPE})\n               for cline in cmdlines]\n    pool.close()\n    pool.join()\n    return sum([r.get().returncode for r in results])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_input_files(dirname, *ext):\n    filelist = [f for f in os.listdir(dirname) if\n                os.path.splitext(f)[-1] in ext]\n    return [os.path.join(dirname, f) for f in filelist]", "response": "Returns a list of files in passed directory filtered by extension."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_sequence_lengths(fastafilenames):\n    tot_lengths = {}\n    for fn in fastafilenames:\n        tot_lengths[os.path.splitext(os.path.split(fn)[-1])[0]] = \\\n            sum([len(s) for s in SeqIO.parse(fn, 'fasta')])\n    return tot_lengths", "response": "Returns a dictionary of sequence lengths keyed by organism."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_cmdline():\n    parser = ArgumentParser(prog=\"average_nucleotide_identity.py\")\n    parser.add_argument(\n        \"--version\", action=\"version\", version=\"%(prog)s: pyani \" + VERSION\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--outdir\",\n        dest=\"outdirname\",\n        action=\"store\",\n        default=None,\n        required=True,\n        help=\"Output directory (required)\",\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--indir\",\n        dest=\"indirname\",\n        action=\"store\",\n        default=None,\n        required=True,\n        help=\"Input directory name (required)\",\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        dest=\"verbose\",\n        action=\"store_true\",\n        default=False,\n        help=\"Give verbose output\",\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--force\",\n        dest=\"force\",\n        action=\"store_true\",\n        default=False,\n        help=\"Force file overwriting\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--fragsize\",\n        dest=\"fragsize\",\n        action=\"store\",\n        default=FRAGSIZE,\n        type=int,\n        help=\"Sequence fragment size for ANIb \" \"(default %i)\" % FRAGSIZE,\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--logfile\",\n        dest=\"logfile\",\n        action=\"store\",\n        default=None,\n        help=\"Logfile location\",\n    )\n    parser.add_argument(\n        \"--skip_nucmer\",\n        dest=\"skip_nucmer\",\n        action=\"store_true\",\n        default=False,\n        help=\"Skip NUCmer runs, for testing \" + \"(e.g. if output already present)\",\n    )\n    parser.add_argument(\n        \"--skip_blastn\",\n        dest=\"skip_blastn\",\n        action=\"store_true\",\n        default=False,\n        help=\"Skip BLASTN runs, for testing \" + \"(e.g. if output already present)\",\n    )\n    parser.add_argument(\n        \"--noclobber\",\n        dest=\"noclobber\",\n        action=\"store_true\",\n        default=False,\n        help=\"Don't nuke existing files\",\n    )\n    parser.add_argument(\n        \"--nocompress\",\n        dest=\"nocompress\",\n        action=\"store_true\",\n        default=False,\n        help=\"Don't compress/delete the comparison output\",\n    )\n    parser.add_argument(\n        \"-g\",\n        \"--graphics\",\n        dest=\"graphics\",\n        action=\"store_true\",\n        default=False,\n        help=\"Generate heatmap of ANI\",\n    )\n    parser.add_argument(\n        \"--gformat\",\n        dest=\"gformat\",\n        action=\"store\",\n        default=\"pdf,png,eps\",\n        help=\"Graphics output format(s) [pdf|png|jpg|svg] \"\n        \"(default pdf,png,eps meaning three file formats)\",\n    )\n    parser.add_argument(\n        \"--gmethod\",\n        dest=\"gmethod\",\n        action=\"store\",\n        default=\"mpl\",\n        choices=[\"mpl\", \"seaborn\"],\n        help=\"Graphics output method (default mpl)\",\n    )\n    parser.add_argument(\n        \"--labels\",\n        dest=\"labels\",\n        action=\"store\",\n        default=None,\n        help=\"Path to file containing sequence labels\",\n    )\n    parser.add_argument(\n        \"--classes\",\n        dest=\"classes\",\n        action=\"store\",\n        default=None,\n        help=\"Path to file containing sequence classes\",\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--method\",\n        dest=\"method\",\n        action=\"store\",\n        default=\"ANIm\",\n        choices=[\"ANIm\", \"ANIb\", \"ANIblastall\", \"TETRA\"],\n        help=\"ANI method (default ANIm)\",\n    )\n    parser.add_argument(\n        \"--scheduler\",\n        dest=\"scheduler\",\n        action=\"store\",\n        default=\"multiprocessing\",\n        choices=[\"multiprocessing\", \"SGE\"],\n        help=\"Job scheduler (default multiprocessing, i.e. locally)\",\n    )\n    parser.add_argument(\n        \"--workers\",\n        dest=\"workers\",\n        action=\"store\",\n        default=None,\n        type=int,\n        help=\"Number of worker processes for multiprocessing \"\n        \"(default zero, meaning use all available cores)\",\n    )\n    parser.add_argument(\n        \"--SGEgroupsize\",\n        dest=\"sgegroupsize\",\n        action=\"store\",\n        default=10000,\n        type=int,\n        help=\"Number of jobs to place in an SGE array group \" \"(default 10000)\",\n    )\n    parser.add_argument(\n        \"--SGEargs\",\n        dest=\"sgeargs\",\n        action=\"store\",\n        default=None,\n        type=str,\n        help=\"Additional arguments for qsub\",\n    )\n    parser.add_argument(\n        \"--maxmatch\",\n        dest=\"maxmatch\",\n        action=\"store_true\",\n        default=False,\n        help=\"Override MUMmer to allow all NUCmer matches\",\n    )\n    parser.add_argument(\n        \"--nucmer_exe\",\n        dest=\"nucmer_exe\",\n        action=\"store\",\n        default=pyani_config.NUCMER_DEFAULT,\n        help=\"Path to NUCmer executable\",\n    )\n    parser.add_argument(\n        \"--filter_exe\",\n        dest=\"filter_exe\",\n        action=\"store\",\n        default=pyani_config.FILTER_DEFAULT,\n        help=\"Path to delta-filter executable\",\n    )\n    parser.add_argument(\n        \"--blastn_exe\",\n        dest=\"blastn_exe\",\n        action=\"store\",\n        default=pyani_config.BLASTN_DEFAULT,\n        help=\"Path to BLASTN+ executable\",\n    )\n    parser.add_argument(\n        \"--makeblastdb_exe\",\n        dest=\"makeblastdb_exe\",\n        action=\"store\",\n        default=pyani_config.MAKEBLASTDB_DEFAULT,\n        help=\"Path to BLAST+ makeblastdb executable\",\n    )\n    parser.add_argument(\n        \"--blastall_exe\",\n        dest=\"blastall_exe\",\n        action=\"store\",\n        default=pyani_config.BLASTALL_DEFAULT,\n        help=\"Path to BLASTALL executable\",\n    )\n    parser.add_argument(\n        \"--formatdb_exe\",\n        dest=\"formatdb_exe\",\n        action=\"store\",\n        default=pyani_config.FORMATDB_DEFAULT,\n        help=\"Path to BLAST formatdb executable\",\n    )\n    parser.add_argument(\n        \"--write_excel\",\n        dest=\"write_excel\",\n        action=\"store_true\",\n        default=False,\n        help=\"Write Excel format output tables\",\n    )\n    parser.add_argument(\n        \"--rerender\",\n        dest=\"rerender\",\n        action=\"store_true\",\n        default=False,\n        help=\"Rerender graphics output without recalculation\",\n    )\n    parser.add_argument(\n        \"--subsample\",\n        dest=\"subsample\",\n        action=\"store\",\n        default=None,\n        help=\"Subsample a percentage [0-1] or specific \"\n        + \"number (1-n) of input sequences\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        dest=\"seed\",\n        action=\"store\",\n        default=None,\n        help=\"Set random seed for reproducible subsampling.\",\n    )\n    parser.add_argument(\n        \"--jobprefix\",\n        dest=\"jobprefix\",\n        action=\"store\",\n        default=\"ANI\",\n        help=\"Prefix for SGE jobs (default ANI).\",\n    )\n    return parser.parse_args()", "response": "Parse command - line arguments for script."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the last exception as a string or use in logging.", "response": "def last_exception():\n    \"\"\" Returns last exception as a string, or use in logging.\n    \"\"\"\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    return \"\".join(traceback.format_exception(exc_type, exc_value, exc_traceback))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_outdir():\n    if os.path.exists(args.outdirname):\n        if not args.force:\n            logger.error(\n                \"Output directory %s would overwrite existing \" + \"files (exiting)\",\n                args.outdirname,\n            )\n            sys.exit(1)\n        elif args.noclobber:\n            logger.warning(\n                \"NOCLOBBER: not actually deleting directory %s\", args.outdirname\n            )\n        else:\n            logger.info(\n                \"Removing directory %s and everything below it\", args.outdirname\n            )\n            shutil.rmtree(args.outdirname)\n    logger.info(\"Creating directory %s\", args.outdirname)\n    try:\n        os.makedirs(args.outdirname)  # We make the directory recursively\n        # Depending on the choice of method, a subdirectory will be made for\n        # alignment output files\n        if args.method != \"TETRA\":\n            os.makedirs(os.path.join(args.outdirname, ALIGNDIR[args.method]))\n    except OSError:\n        # This gets thrown if the directory exists. If we've forced overwrite/\n        # delete and we're not clobbering, we let things slide\n        if args.noclobber and args.force:\n            logger.info(\"NOCLOBBER+FORCE: not creating directory\")\n        else:\n            logger.error(last_exception)\n            sys.exit(1)", "response": "Make the output directory if required."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compress_delete_outdir(outdir):\n    # Compress output in .tar.gz file and remove raw output\n    tarfn = outdir + \".tar.gz\"\n    logger.info(\"\\tCompressing output from %s to %s\", outdir, tarfn)\n    with tarfile.open(tarfn, \"w:gz\") as fh:\n        fh.add(outdir)\n    logger.info(\"\\tRemoving output directory %s\", outdir)\n    shutil.rmtree(outdir)", "response": "Compress the contents of the passed directory to. tar. gz and delete."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating ANIm dataframes for each input file and returns the result dataframes in the output directory.", "response": "def calculate_anim(infiles, org_lengths):\n    \"\"\"Returns ANIm result dataframes for files in input directory.\n\n    - infiles - paths to each input file\n    - org_lengths - dictionary of input sequence lengths, keyed by sequence\n\n    Finds ANI by the ANIm method, as described in Richter et al (2009)\n    Proc Natl Acad Sci USA 106: 19126-19131 doi:10.1073/pnas.0906412106.\n\n    All FASTA format files (selected by suffix) in the input directory\n    are compared against each other, pairwise, using NUCmer (which must\n    be in the path). NUCmer output is stored in the output directory.\n\n    The NUCmer .delta file output is parsed to obtain an alignment length\n    and similarity error count for every unique region alignment between\n    the two organisms, as represented by the sequences in the FASTA files.\n\n    These are processed to give matrices of aligned sequence lengths,\n    average nucleotide identity (ANI) percentages, coverage (aligned\n    percentage of whole genome), and similarity error cound for each pairwise\n    comparison.\n    \"\"\"\n    logger.info(\"Running ANIm\")\n    logger.info(\"Generating NUCmer command-lines\")\n    deltadir = os.path.join(args.outdirname, ALIGNDIR[\"ANIm\"])\n    logger.info(\"Writing nucmer output to %s\", deltadir)\n    # Schedule NUCmer runs\n    if not args.skip_nucmer:\n        joblist = anim.generate_nucmer_jobs(\n            infiles,\n            args.outdirname,\n            nucmer_exe=args.nucmer_exe,\n            filter_exe=args.filter_exe,\n            maxmatch=args.maxmatch,\n            jobprefix=args.jobprefix,\n        )\n        if args.scheduler == \"multiprocessing\":\n            logger.info(\"Running jobs with multiprocessing\")\n            if args.workers is None:\n                logger.info(\"(using maximum number of available \" + \"worker threads)\")\n            else:\n                logger.info(\"(using %d worker threads, if available)\", args.workers)\n            cumval = run_mp.run_dependency_graph(\n                joblist, workers=args.workers, logger=logger\n            )\n            logger.info(\"Cumulative return value: %d\", cumval)\n            if 0 < cumval:\n                logger.warning(\n                    \"At least one NUCmer comparison failed. \" + \"ANIm may fail.\"\n                )\n            else:\n                logger.info(\"All multiprocessing jobs complete.\")\n        else:\n            logger.info(\"Running jobs with SGE\")\n            logger.info(\"Jobarray group size set to %d\", args.sgegroupsize)\n            run_sge.run_dependency_graph(\n                joblist,\n                logger=logger,\n                jgprefix=args.jobprefix,\n                sgegroupsize=args.sgegroupsize,\n                sgeargs=args.sgeargs,\n            )\n    else:\n        logger.warning(\"Skipping NUCmer run (as instructed)!\")\n\n    # Process resulting .delta files\n    logger.info(\"Processing NUCmer .delta files.\")\n    results = anim.process_deltadir(deltadir, org_lengths, logger=logger)\n    if results.zero_error:  # zero percentage identity error\n        if not args.skip_nucmer and args.scheduler == \"multiprocessing\":\n            if 0 < cumval:\n                logger.error(\n                    \"This has possibly been a NUCmer run failure, \"\n                    + \"please investigate\"\n                )\n                logger.error(last_exception())\n                sys.exit(1)\n            else:\n                logger.error(\n                    \"This is possibly due to a NUCmer comparison \"\n                    + \"being too distant for use. Please consider \"\n                    + \"using the --maxmatch option.\"\n                )\n                logger.error(\n                    \"This is alternatively due to NUCmer run \"\n                    + \"failure, analysis will continue, but please \"\n                    + \"investigate.\"\n                )\n    if not args.nocompress:\n        logger.info(\"Compressing/deleting %s\", deltadir)\n        compress_delete_outdir(deltadir)\n\n    # Return processed data from .delta files\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating TETRA for each input file.", "response": "def calculate_tetra(infiles):\n    \"\"\"Calculate TETRA for files in input directory.\n\n    - infiles - paths to each input file\n    - org_lengths - dictionary of input sequence lengths, keyed by sequence\n\n    Calculates TETRA correlation scores, as described in:\n\n    Richter M, Rossello-Mora R (2009) Shifting the genomic gold standard for\n    the prokaryotic species definition. Proc Natl Acad Sci USA 106:\n    19126-19131. doi:10.1073/pnas.0906412106.\n\n    and\n\n    Teeling et al. (2004) Application of tetranucleotide frequencies for the\n    assignment of genomic fragments. Env. Microbiol. 6(9): 938-947.\n    doi:10.1111/j.1462-2920.2004.00624.x\n    \"\"\"\n    logger.info(\"Running TETRA.\")\n    # First, find Z-scores\n    logger.info(\"Calculating TETRA Z-scores for each sequence.\")\n    tetra_zscores = {}\n    for filename in infiles:\n        logger.info(\"Calculating TETRA Z-scores for %s\", filename)\n        org = os.path.splitext(os.path.split(filename)[-1])[0]\n        tetra_zscores[org] = tetra.calculate_tetra_zscore(filename)\n    # Then calculate Pearson correlation between Z-scores for each sequence\n    logger.info(\"Calculating TETRA correlation scores.\")\n    tetra_correlations = tetra.calculate_correlations(tetra_zscores)\n    return tetra_correlations"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate ANIb for files in input directory.", "response": "def unified_anib(infiles, org_lengths):\n    \"\"\"Calculate ANIb for files in input directory.\n\n    - infiles - paths to each input file\n    - org_lengths - dictionary of input sequence lengths, keyed by sequence\n\n    Calculates ANI by the ANIb method, as described in Goris et al. (2007)\n    Int J Syst Evol Micr 57: 81-91. doi:10.1099/ijs.0.64483-0. There are\n    some minor differences depending on whether BLAST+ or legacy BLAST\n    (BLASTALL) methods are used.\n\n    All FASTA format files (selected by suffix) in the input directory are\n    used to construct BLAST databases, placed in the output directory.\n    Each file's contents are also split into sequence fragments of length\n    options.fragsize, and the multiple FASTA file that results written to\n    the output directory. These are BLASTNed, pairwise, against the\n    databases.\n\n    The BLAST output is interrogated for all fragment matches that cover\n    at least 70% of the query sequence, with at least 30% nucleotide\n    identity over the full length of the query sequence. This is an odd\n    choice and doesn't correspond to the twilight zone limit as implied by\n    Goris et al. We persist with their definition, however.  Only these\n    qualifying matches contribute to the total aligned length, and total\n    aligned sequence identity used to calculate ANI.\n\n    The results are processed to give matrices of aligned sequence length\n    (aln_lengths.tab), similarity error counts (sim_errors.tab), ANIs\n    (perc_ids.tab), and minimum aligned percentage (perc_aln.tab) of\n    each genome, for each pairwise comparison. These are written to the\n    output directory in plain text tab-separated format.\n    \"\"\"\n    logger.info(\"Running %s\", args.method)\n    blastdir = os.path.join(args.outdirname, ALIGNDIR[args.method])\n    logger.info(\"Writing BLAST output to %s\", blastdir)\n    # Build BLAST databases and run pairwise BLASTN\n    if not args.skip_blastn:\n        # Make sequence fragments\n        logger.info(\"Fragmenting input files, and writing to %s\", args.outdirname)\n        # Fraglengths does not get reused with BLASTN\n        fragfiles, fraglengths = anib.fragment_fasta_files(\n            infiles, blastdir, args.fragsize\n        )\n        # Export fragment lengths as JSON, in case we re-run with --skip_blastn\n        with open(os.path.join(blastdir, \"fraglengths.json\"), \"w\") as outfile:\n            json.dump(fraglengths, outfile)\n\n        # Which executables are we using?\n        # if args.method == \"ANIblastall\":\n        #    format_exe = args.formatdb_exe\n        #    blast_exe = args.blastall_exe\n        # else:\n        #    format_exe = args.makeblastdb_exe\n        #    blast_exe = args.blastn_exe\n\n        # Run BLAST database-building and executables from a jobgraph\n        logger.info(\"Creating job dependency graph\")\n        jobgraph = anib.make_job_graph(\n            infiles, fragfiles, anib.make_blastcmd_builder(args.method, blastdir)\n        )\n        # jobgraph = anib.make_job_graph(infiles, fragfiles, blastdir,\n        #                               format_exe, blast_exe, args.method,\n        #                               jobprefix=args.jobprefix)\n        if args.scheduler == \"multiprocessing\":\n            logger.info(\"Running jobs with multiprocessing\")\n            logger.info(\"Running job dependency graph\")\n            if args.workers is None:\n                logger.info(\"(using maximum number of available \" + \"worker threads)\")\n            else:\n                logger.info(\"(using %d worker threads, if available)\", args.workers)\n            cumval = run_mp.run_dependency_graph(\n                jobgraph, workers=args.workers, logger=logger\n            )\n            if 0 < cumval:\n                logger.warning(\n                    \"At least one BLAST run failed. \" + \"%s may fail.\", args.method\n                )\n            else:\n                logger.info(\"All multiprocessing jobs complete.\")\n        else:\n            run_sge.run_dependency_graph(jobgraph, logger=logger)\n            logger.info(\"Running jobs with SGE\")\n    else:\n        # Import fragment lengths from JSON\n        if args.method == \"ANIblastall\":\n            with open(os.path.join(blastdir, \"fraglengths.json\"), \"rU\") as infile:\n                fraglengths = json.load(infile)\n        else:\n            fraglengths = None\n        logger.warning(\"Skipping BLASTN runs (as instructed)!\")\n\n    # Process pairwise BLASTN output\n    logger.info(\"Processing pairwise %s BLAST output.\", args.method)\n    try:\n        data = anib.process_blast(\n            blastdir, org_lengths, fraglengths=fraglengths, mode=args.method\n        )\n    except ZeroDivisionError:\n        logger.error(\"One or more BLAST output files has a problem.\")\n        if not args.skip_blastn:\n            if 0 < cumval:\n                logger.error(\n                    \"This is possibly due to BLASTN run failure, \"\n                    + \"please investigate\"\n                )\n            else:\n                logger.error(\n                    \"This is possibly due to a BLASTN comparison \"\n                    + \"being too distant for use.\"\n                )\n        logger.error(last_exception())\n    if not args.nocompress:\n        logger.info(\"Compressing/deleting %s\", blastdir)\n        compress_delete_outdir(blastdir)\n\n    # Return processed BLAST data\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(results):\n    logger.info(\"Writing %s results to %s\", args.method, args.outdirname)\n    if args.method == \"TETRA\":\n        out_excel = os.path.join(args.outdirname, TETRA_FILESTEMS[0]) + \".xlsx\"\n        out_csv = os.path.join(args.outdirname, TETRA_FILESTEMS[0]) + \".tab\"\n        if args.write_excel:\n            results.to_excel(out_excel, index=True)\n        results.to_csv(out_csv, index=True, sep=\"\\t\")\n\n    else:\n        for dfr, filestem in results.data:\n            out_excel = os.path.join(args.outdirname, filestem) + \".xlsx\"\n            out_csv = os.path.join(args.outdirname, filestem) + \".tab\"\n            logger.info(\"\\t%s\", filestem)\n            if args.write_excel:\n                dfr.to_excel(out_excel, index=True)\n            dfr.to_csv(out_csv, index=True, sep=\"\\t\")", "response": "Write results to an Excel - format file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw(filestems, gformat):\n    # Draw heatmaps\n    for filestem in filestems:\n        fullstem = os.path.join(args.outdirname, filestem)\n        outfilename = fullstem + \".%s\" % gformat\n        infilename = fullstem + \".tab\"\n        df = pd.read_csv(infilename, index_col=0, sep=\"\\t\")\n        logger.info(\"Writing heatmap to %s\", outfilename)\n        params = pyani_graphics.Params(\n            params_mpl(df)[filestem],\n            pyani_tools.get_labels(args.labels),\n            pyani_tools.get_labels(args.classes),\n        )\n        if args.gmethod == \"mpl\":\n            pyani_graphics.heatmap_mpl(\n                df, outfilename=outfilename, title=filestem, params=params\n            )\n        elif args.gmethod == \"seaborn\":\n            pyani_graphics.heatmap_seaborn(\n                df, outfilename=outfilename, title=filestem, params=params\n            )", "response": "Draw the results of the ANIb and TETRA results for the given filestems."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subsample_input(infiles):\n    logger.info(\"--subsample: %s\", args.subsample)\n    try:\n        samplesize = float(args.subsample)\n    except TypeError:  # Not a number\n        logger.error(\n            \"--subsample must be int or float, got %s (exiting)\", type(args.subsample)\n        )\n        sys.exit(1)\n    if samplesize <= 0:  # Not a positive value\n        logger.error(\"--subsample must be positive value, got %s\", str(args.subsample))\n        sys.exit(1)\n    if int(samplesize) > 1:\n        logger.info(\"Sample size integer > 1: %d\", samplesize)\n        k = min(int(samplesize), len(infiles))\n    else:\n        logger.info(\"Sample size proportion in (0, 1]: %.3f\", samplesize)\n        k = int(min(samplesize, 1.0) * len(infiles))\n    logger.info(\"Randomly subsampling %d sequences for analysis\", k)\n    if args.seed:\n        logger.info(\"Setting random seed with: %s\", args.seed)\n        random.seed(args.seed)\n    else:\n        logger.warning(\"Subsampling without specified random seed!\")\n        logger.warning(\"Subsampling may NOT be easily reproducible!\")\n    return random.sample(infiles, k)", "response": "Returns a random subsample of the input files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait(self, interval=SGE_WAIT):\n        finished = False\n        while not finished:\n            time.sleep(interval)\n            interval = min(2 * interval, 60)\n            finished = os.system(\"qstat -j %s > /dev/null\" % (self.name))", "response": "Wait until the job finishes and poll SGE on its status."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_script(self):\n        self.script = \"\"        # Holds the script string\n        total = 1               # total number of jobs in this group\n\n        # for now, SGE_TASK_ID becomes TASK_ID, but we base it at zero\n        self.script += \"\"\"let \"TASK_ID=$SGE_TASK_ID - 1\"\\n\"\"\"\n\n        # build the array definitions; force ordering for Python3.5 tests\n        for key in sorted(self.arguments.keys()):\n            values = self.arguments[key]\n            line = (\"%s_ARRAY=( \" % (key))\n            for value in values:\n                line += value\n                line += \" \"\n            line += \" )\\n\"\n            self.script += line\n            total *= len(values)\n        self.script += \"\\n\"\n\n        # now, build the decoding logic in the script; force ordering\n        for key in sorted(self.arguments.keys()):\n            count = len(self.arguments[key])\n            self.script += \"\"\"let \"%s_INDEX=$TASK_ID %% %d\"\\n\"\"\" % (key, count)\n            self.script += \"\"\"%s=${%s_ARRAY[$%s_INDEX]}\\n\"\"\" % (key, key, key)\n            self.script += \"\"\"let \"TASK_ID=$TASK_ID / %d\"\\n\"\"\" % (count)\n\n        # now, add the command to run the job\n        self.script += \"\\n\"\n        self.script += self.command\n        self.script += \"\\n\"\n\n        # set the number of tasks in this group\n        self.tasks = total", "response": "Generate the script that will run the jobs in the JobGroup with the passed arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a list of Jobs describing NUCmer command - lines for each pair of FASTA files.", "response": "def generate_nucmer_jobs(\n    filenames,\n    outdir=\".\",\n    nucmer_exe=pyani_config.NUCMER_DEFAULT,\n    filter_exe=pyani_config.FILTER_DEFAULT,\n    maxmatch=False,\n    jobprefix=\"ANINUCmer\",\n):\n    \"\"\"Return a list of Jobs describing NUCmer command-lines for ANIm\n\n    - filenames - a list of paths to input FASTA files\n    - outdir - path to output directory\n    - nucmer_exe - location of the nucmer binary\n    - maxmatch - Boolean flag indicating to use NUCmer's -maxmatch option\n\n    Loop over all FASTA files, generating Jobs describing NUCmer command lines\n    for each pairwise comparison.\n    \"\"\"\n    ncmds, fcmds = generate_nucmer_commands(\n        filenames, outdir, nucmer_exe, filter_exe, maxmatch\n    )\n    joblist = []\n    for idx, ncmd in enumerate(ncmds):\n        njob = pyani_jobs.Job(\"%s_%06d-n\" % (jobprefix, idx), ncmd)\n        fjob = pyani_jobs.Job(\"%s_%06d-f\" % (jobprefix, idx), fcmds[idx])\n        fjob.add_dependency(njob)\n        # joblist.append(njob)  # not required: dependency in fjob\n        joblist.append(fjob)\n    return joblist"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_nucmer_commands(\n    filenames,\n    outdir=\".\",\n    nucmer_exe=pyani_config.NUCMER_DEFAULT,\n    filter_exe=pyani_config.FILTER_DEFAULT,\n    maxmatch=False,\n):\n    \"\"\"Return a tuple of lists of NUCmer command-lines for ANIm\n\n    The first element is a list of NUCmer commands, the second a list\n    of delta_filter_wrapper.py commands. These are ordered such that\n    commands are paired. The NUCmer commands should be run before\n    the delta-filter commands.\n\n    - filenames - a list of paths to input FASTA files\n    - outdir - path to output directory\n    - nucmer_exe - location of the nucmer binary\n    - maxmatch - Boolean flag indicating to use NUCmer's -maxmatch option\n\n    Loop over all FASTA files generating NUCmer command lines for each\n    pairwise comparison.\n    \"\"\"\n    nucmer_cmdlines, delta_filter_cmdlines = [], []\n    for idx, fname1 in enumerate(filenames[:-1]):\n        for fname2 in filenames[idx + 1 :]:\n            ncmd, dcmd = construct_nucmer_cmdline(\n                fname1, fname2, outdir, nucmer_exe, filter_exe, maxmatch\n            )\n            nucmer_cmdlines.append(ncmd)\n            delta_filter_cmdlines.append(dcmd)\n    return (nucmer_cmdlines, delta_filter_cmdlines)", "response": "Generates NUCmer command - lines for ANIm\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef construct_nucmer_cmdline(\n    fname1,\n    fname2,\n    outdir=\".\",\n    nucmer_exe=pyani_config.NUCMER_DEFAULT,\n    filter_exe=pyani_config.FILTER_DEFAULT,\n    maxmatch=False,\n):\n    \"\"\"Returns a tuple of NUCmer and delta-filter commands\n\n    The split into a tuple was made necessary by changes to SGE/OGE. The\n    delta-filter command must now be run as a dependency of the NUCmer\n    command, and be wrapped in a Python script to capture STDOUT.\n\n    NOTE: This command-line writes output data to a subdirectory of the passed\n    outdir, called \"nucmer_output\".\n\n    - fname1 - query FASTA filepath\n    - fname2 - subject FASTA filepath\n    - outdir - path to output directory\n    - maxmatch - Boolean flag indicating whether to use NUCmer's -maxmatch\n    option. If not, the -mum option is used instead\n    \"\"\"\n    outsubdir = os.path.join(outdir, pyani_config.ALIGNDIR[\"ANIm\"])\n    outprefix = os.path.join(\n        outsubdir,\n        \"%s_vs_%s\"\n        % (\n            os.path.splitext(os.path.split(fname1)[-1])[0],\n            os.path.splitext(os.path.split(fname2)[-1])[0],\n        ),\n    )\n    if maxmatch:\n        mode = \"--maxmatch\"\n    else:\n        mode = \"--mum\"\n    nucmercmd = \"{0} {1} -p {2} {3} {4}\".format(\n        nucmer_exe, mode, outprefix, fname1, fname2\n    )\n    filtercmd = \"delta_filter_wrapper.py \" + \"{0} -1 {1} {2}\".format(\n        filter_exe, outprefix + \".delta\", outprefix + \".filter\"\n    )\n    return (nucmercmd, filtercmd)", "response": "Constructs a tuple of NUCmer and delta - filter commands for a single object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_delta(filename):\n    aln_length, sim_errors = 0, 0\n    for line in [l.strip().split() for l in open(filename, \"r\").readlines()]:\n        if line[0] == \"NUCMER\" or line[0].startswith(\">\"):  # Skip headers\n            continue\n        # We only process lines with seven columns:\n        if len(line) == 7:\n            aln_length += abs(int(line[1]) - int(line[0]))\n            sim_errors += int(line[4])\n    return aln_length, sim_errors", "response": "Parses the file containing the alignment length and similarity errors for each unique - matched region."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_deltadir(delta_dir, org_lengths, logger=None):\n    # Process directory to identify input files - as of v0.2.4 we use the\n    # .filter files that result from delta-filter (1:1 alignments)\n    deltafiles = pyani_files.get_input_files(delta_dir, \".filter\")\n\n    # Hold data in ANIResults object\n    results = ANIResults(list(org_lengths.keys()), \"ANIm\")\n\n    # Fill diagonal NA values for alignment_length with org_lengths\n    for org, length in list(org_lengths.items()):\n        results.alignment_lengths[org][org] = length\n\n    # Process .delta files assuming that the filename format holds:\n    # org1_vs_org2.delta\n    for deltafile in deltafiles:\n        qname, sname = os.path.splitext(os.path.split(deltafile)[-1])[0].split(\"_vs_\")\n\n        # We may have .delta files from other analyses in the same directory\n        # If this occurs, we raise a warning, and skip the .delta file\n        if qname not in list(org_lengths.keys()):\n            if logger:\n                logger.warning(\n                    \"Query name %s not in input \" % qname\n                    + \"sequence list, skipping %s\" % deltafile\n                )\n            continue\n        if sname not in list(org_lengths.keys()):\n            if logger:\n                logger.warning(\n                    \"Subject name %s not in input \" % sname\n                    + \"sequence list, skipping %s\" % deltafile\n                )\n            continue\n        tot_length, tot_sim_error = parse_delta(deltafile)\n        if tot_length == 0 and logger is not None:\n            if logger:\n                logger.warning(\n                    \"Total alignment length reported in \" + \"%s is zero!\" % deltafile\n                )\n        query_cover = float(tot_length) / org_lengths[qname]\n        sbjct_cover = float(tot_length) / org_lengths[sname]\n\n        # Calculate percentage ID of aligned length. This may fail if\n        # total length is zero.\n        # The ZeroDivisionError that would arise should be handled\n        # Common causes are that a NUCmer run failed, or that a very\n        # distant sequence was included in the analysis.\n        try:\n            perc_id = 1 - float(tot_sim_error) / tot_length\n        except ZeroDivisionError:\n            perc_id = 0  # set arbitrary value of zero identity\n            results.zero_error = True\n\n        # Populate dataframes: when assigning data from symmetrical MUMmer\n        # output, both upper and lower triangles will be populated\n        results.add_tot_length(qname, sname, tot_length)\n        results.add_sim_errors(qname, sname, tot_sim_error)\n        results.add_pid(qname, sname, perc_id)\n        results.add_coverage(qname, sname, query_cover, sbjct_cover)\n    return results", "response": "Processes the. delta files in the passed directory and returns a tuple of ANIm results for the results of the NUCmer analysis."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_cmdline():\n    parser = ArgumentParser(prog=\"genbank_get_genomes_by_taxon.py\")\n    parser.add_argument(\n        \"-o\",\n        \"--outdir\",\n        dest=\"outdirname\",\n        required=True,\n        action=\"store\",\n        default=None,\n        help=\"Output directory (required)\")\n    parser.add_argument(\n        \"-t\",\n        \"--taxon\",\n        dest=\"taxon\",\n        action=\"store\",\n        default=None,\n        help=\"NCBI taxonomy ID\")\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        dest=\"verbose\",\n        action=\"store_true\",\n        default=False,\n        help=\"Give verbose output\")\n    parser.add_argument(\n        \"-f\",\n        \"--force\",\n        dest=\"force\",\n        action=\"store_true\",\n        default=False,\n        help=\"Force file overwriting\")\n    parser.add_argument(\n        \"--noclobber\",\n        dest=\"noclobber\",\n        action=\"store_true\",\n        default=False,\n        help=\"Don't nuke existing files\")\n    parser.add_argument(\n        \"-l\",\n        \"--logfile\",\n        dest=\"logfile\",\n        action=\"store\",\n        default=None,\n        help=\"Logfile location\")\n    parser.add_argument(\n        \"--format\",\n        dest=\"format\",\n        action=\"store\",\n        default=\"fasta\",\n        help=\"Output file format [gbk|fasta]\")\n    parser.add_argument(\n        \"--email\",\n        dest=\"email\",\n        required=True,\n        action=\"store\",\n        default=None,\n        help=\"Email associated with NCBI queries (required)\")\n    parser.add_argument(\n        \"--retries\",\n        dest=\"retries\",\n        action=\"store\",\n        default=20,\n        type=int,\n        help=\"Number of Entrez retry attempts per request.\")\n    parser.add_argument(\n        \"--batchsize\",\n        dest=\"batchsize\",\n        action=\"store\",\n        default=10000,\n        type=int,\n        help=\"Entrez record return batch size\")\n    parser.add_argument(\n        \"--timeout\",\n        dest=\"timeout\",\n        action=\"store\",\n        default=10,\n        type=int,\n        help=\"Timeout for URL connection (s)\")\n    return parser.parse_args()", "response": "Parse command - line arguments and return a list of genomes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets contact email for NCBI.", "response": "def set_ncbi_email():\n    \"\"\"Set contact email for NCBI.\"\"\"\n    Entrez.email = args.email\n    logger.info(\"Set NCBI contact email to %s\", args.email)\n    Entrez.tool = \"genbank_get_genomes_by_taxon.py\""}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef entrez_retry(func, *fnargs, **fnkwargs):\n    tries, success = 0, False\n    while not success and tries < args.retries:\n        try:\n            output = func(*fnargs, **fnkwargs)\n            success = True\n        except (HTTPError, URLError):\n            tries += 1\n            logger.warning(\"Entrez query %s(%s, %s) failed (%d/%d)\", func,\n                           fnargs, fnkwargs, tries + 1, args.retries)\n            logger.warning(last_exception())\n    if not success:\n        logger.error(\"Too many Entrez failures (exiting)\")\n        sys.exit(1)\n    return output", "response": "Retries the passed function up to the number of times specified\n    by args. retries\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef entrez_batch_webhistory(record, expected, batchsize, *fnargs, **fnkwargs):\n    results = []\n    for start in range(0, expected, batchsize):\n        batch_handle = entrez_retry(\n            Entrez.efetch,\n            retstart=start,\n            retmax=batchsize,\n            webenv=record[\"WebEnv\"],\n            query_key=record[\"QueryKey\"],\n            *fnargs,\n            **fnkwargs)\n        batch_record = Entrez.read(batch_handle, validate=False)\n        results.extend(batch_record)\n    return results", "response": "Recovers the Entrez webhistory data from a prior NCBI webhistory search using Efetch. Returns all results as a list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a set of NCBI UIDs associated with the passed taxon.", "response": "def get_asm_uids(taxon_uid):\n    \"\"\"Returns a set of NCBI UIDs associated with the passed taxon.\n\n    This query at NCBI returns all assemblies for the taxon subtree\n    rooted at the passed taxon_uid.\n    \"\"\"\n    query = \"txid%s[Organism:exp]\" % taxon_uid\n    logger.info(\"Entrez ESearch with query: %s\", query)\n\n    # Perform initial search for assembly UIDs with taxon ID as query.\n    # Use NCBI history for the search.\n    handle = entrez_retry(\n        Entrez.esearch,\n        db=\"assembly\",\n        term=query,\n        format=\"xml\",\n        usehistory=\"y\")\n    record = Entrez.read(handle, validate=False)\n    result_count = int(record['Count'])\n    logger.info(\"Entrez ESearch returns %d assembly IDs\", result_count)\n\n    # Recover assembly UIDs from the web history\n    asm_ids = entrez_batch_webhistory(\n        record, result_count, 250, db=\"assembly\", retmode=\"xml\")\n    logger.info(\"Identified %d unique assemblies\", len(asm_ids))\n    return asm_ids"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract filestem from Entrez eSummary data.", "response": "def extract_filestem(data):\n    \"\"\"Extract filestem from Entrez eSummary data.\n\n    Function expects esummary['DocumentSummarySet']['DocumentSummary'][0]\n\n    Some illegal characters may occur in AssemblyName - for these, a more\n    robust regex replace/escape may be required. Sadly, NCBI don't just\n    use standard percent escapes, but instead replace certain\n    characters with underscores: white space, slash, comma, hash, brackets.\n    \"\"\"\n    escapes = re.compile(r\"[\\s/,#\\(\\)]\")\n    escname = re.sub(escapes, '_', data['AssemblyName'])\n    return '_'.join([data['AssemblyAccession'], escname])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the NCBI AssemblyAccession AssemblyName and AssemblyName for the given assembly UID.", "response": "def get_ncbi_asm(asm_uid, fmt='fasta'):\n    \"\"\"Returns the NCBI AssemblyAccession and AssemblyName for the assembly\n    with passed UID, and organism data for class/label files also, as well\n    as accession, so we can track whether downloads fail because only the\n    most recent version is available..\n\n    AssemblyAccession and AssemblyName are data fields in the eSummary record,\n    and correspond to downloadable files for each assembly at\n    ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GC[AF]/nnn/nnn/nnn/<AA>_<AN>\n    where <AA> is AssemblyAccession, and <AN> is AssemblyName, and the choice\n    of GCA vs GCF, and the three values of nnn are taken from <AA>\n    \"\"\"\n    logger.info(\"Identifying assembly information from NCBI for %s\", asm_uid)\n\n    # Obtain full eSummary data for the assembly\n    summary = Entrez.read(\n        entrez_retry(\n            Entrez.esummary, db=\"assembly\", id=asm_uid, report=\"full\"),\n        validate=False)\n\n    # Extract filestem from assembly data\n    data = summary['DocumentSummarySet']['DocumentSummary'][0]\n    filestem = extract_filestem(data)\n\n    # Report interesting things from the summary for those interested\n    logger.info(\"\\tOrganism: %s\", data['Organism'])\n    logger.info(\"\\tTaxid: %s\", data['SpeciesTaxid'])\n    logger.info(\"\\tAccession: %s\", data['AssemblyAccession'])\n    logger.info(\"\\tName: %s\", data['AssemblyName'])\n    # NOTE: Maybe parse out the assembly stats here, in future?\n\n    # Get class and label text\n    organism = data['SpeciesName']\n    try:\n        strain = data['Biosource']['InfraspeciesList'][0]['Sub_value']\n    except (KeyError, IndexError):\n        # we consider this an error/incompleteness in the NCBI metadata\n        strain = \"\"\n\n    # Create label and class strings\n    genus, species = organism.split(' ', 1)\n    labeltxt = \"%s_genomic\\t%s %s %s\" % (filestem, genus[0] + '.', species,\n                                         strain)\n    classtxt = \"%s_genomic\\t%s\" % (filestem, organism)\n    logger.info(\"\\tLabel: %s\", labeltxt)\n    logger.info(\"\\tClass: %s\", classtxt)\n\n    # Download and extract genome assembly\n    try:\n        fastafilename = retrieve_asm_contigs(filestem, fmt=fmt)\n    except NCBIDownloadException:\n        # This is a little hacky. Sometimes, RefSeq assemblies are\n        # suppressed (presumably because they are non-redundant),\n        # but the GenBank assembly persists. In those cases, we\n        # *assume* (because it may not be true) that the corresponding\n        # genbank sequence shares the same accession number, except\n        # that GCF is replaced by GCA\n        gbfilestem = re.sub('^GCF_', 'GCA_', filestem)\n        logger.warning(\"Could not download %s, trying %s\", filestem,\n                       gbfilestem)\n        try:\n            fastafilename = retrieve_asm_contigs(gbfilestem, fmt=fmt)\n        except NCBIDownloadException:\n            fastafilename = None\n\n    return (fastafilename, classtxt, labeltxt, data['AssemblyAccession'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retrieve_asm_contigs(filestem,\n                         ftpstem=\"ftp://ftp.ncbi.nlm.nih.gov/genomes/all\",\n                         fmt='fasta'):\n    \"\"\"Downloads an assembly sequence to a local directory.\n\n    The filestem corresponds to <AA>_<AN>, where <AA> and <AN> are\n    AssemblyAccession and AssemblyName: data fields in the eSummary record.\n    These correspond to downloadable files for each assembly at\n    ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GC[AF]/nnn/nnn/nnn/<AA>_<AN>/\n    where <AA> is AssemblyAccession, and <AN> is AssemblyName. The choice\n    of GCA vs GCF, and the values of nnn, are derived from <AA>\n\n    The files in this directory all have the stem <AA>_<AN>_<suffix>, where\n    suffixes are:\n    assembly_report.txt\n    assembly_stats.txt\n    feature_table.txt.gz\n    genomic.fna.gz\n    genomic.gbff.gz\n    genomic.gff.gz\n    protein.faa.gz\n    protein.gpff.gz\n    rm_out.gz\n    rm.run\n    wgsmaster.gbff.gz\n\n    This function downloads the genomic_fna.gz file, and extracts it in the\n    output directory name specified when the script is called.\n    \"\"\"\n    logger.info(\"Retrieving assembly sequence for %s\", filestem)\n\n    # Define format suffix\n    logger.info(\"%s format requested\", fmt)\n    if fmt == 'fasta':\n        suffix = \"genomic.fna.gz\"\n    elif fmt == 'gbk':\n        suffix = 'genomic.gbff.gz'\n\n    # Compile URL\n    gc, aa, an = tuple(filestem.split('_', 2))\n    aaval = aa.split('.')[0]\n    subdirs = '/'.join([aa[i:i + 3] for i in range(0, len(aaval), 3)])\n\n    url = \"{0}/{1}/{2}/{3}/{3}_{4}\".format(ftpstem, gc, subdirs, filestem,\n                                           suffix)\n    logger.info(\"Using URL: %s\", url)\n\n    # Get data info\n    try:\n        response = urlopen(url, timeout=args.timeout)\n    except HTTPError:\n        logger.error(\"Download failed for URL: %s\\n%s\", url, last_exception())\n        raise NCBIDownloadException()\n    except URLError as e:\n        if isinstance(e.reason, timeout):\n            logger.error(\"Download timed out for URL: %s\\n%s\", url,\n                         last_exception())\n        else:\n            logger.error(\"Download failed for URL: %s\\n%s\", url,\n                         last_exception())\n        raise NCBIDownloadException()\n    except timeout:\n        # TODO: Does this ever happen?\n        logger.error(\"Download timed out for URL: %s\\n%s\", url,\n                     last_exception())\n        raise NCBIDownloadException()\n    else:\n        logger.info(\"Opened URL\")\n\n    # Issue 108 highlighted an fsize return problem\n    try:\n        fsize = int(response.info().get(\"Content-length\"))\n    except TypeError:  # Thrown if no content length returned\n        raise NCBIDownloadException()\n    else:\n        logger.info(\"Parsed file content size: %d.\", fsize)\n\n    # Download data\n    outfname = os.path.join(args.outdirname, '_'.join([filestem, suffix]))\n    if os.path.exists(outfname):\n        logger.warning(\"Output file %s exists, not downloading\", outfname)\n    else:\n        logger.info(\"Downloading %s (%d bytes)\", url, fsize)\n        bsize = 1048576  # buffer size\n        fsize_dl = 0  # bytes downloaded\n        try:\n            with open(outfname, \"wb\") as ofh:\n                while True:\n                    buffer = response.read(bsize)\n                    if not buffer:\n                        break\n                    fsize_dl += len(buffer)\n                    ofh.write(buffer)\n                    status = r\"%10d  [%3.2f%%]\" % (fsize_dl,\n                                                   fsize_dl * 100. / fsize)\n                    logger.info(status)\n        except:\n            logger.error(\"Download failed for %s\", url)\n            logger.error(last_exception())\n            raise NCBIDownloadException()\n\n    # Extract data\n    ename = os.path.splitext(outfname)[0]  # Strips only .gz from filename\n    # The code below would munge the extracted filename to suit the expected\n    # class/label from the old version of this script.\n    # The .gz file downloaded from NCBI has format\n    # <assembly UID>_<string>_genomic.fna.gz - which we would extract to\n    # <assembly UID>.fna\n    #regex = \".{3}_[0-9]{9}.[0-9]\"\n    #outparts = os.path.split(outfname)\n    # print(outparts[0])\n    #print(re.match(regex, outparts[-1]).group())\n    # ename = os.path.join(outparts[0],\n    #                     re.match(regex, outparts[-1]).group() + '.fna')\n    if os.path.exists(ename):\n        logger.warning(\"Output file %s exists, not extracting\", ename)\n    else:\n        try:\n            logger.info(\"Extracting archive %s to %s\", outfname, ename)\n            with open(ename, 'w') as efh:\n                subprocess.call(\n                    ['gunzip', '-c', outfname],\n                    stdout=efh)  # can be subprocess.run in Py3.5\n                logger.info(\"Archive extracted to %s\", ename)\n        except:\n            logger.error(\"Extracting archive %s failed\", outfname)\n            logger.error(last_exception())\n            raise NCBIDownloadException()\n\n    return ename", "response": "Downloads an assembly sequence from NCBI and extracts it in the local directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting contigs out to a single FASTA file in the script s designated output directory.", "response": "def write_contigs(asm_uid, contig_uids, batchsize=10000):\n    \"\"\"Writes assembly contigs out to a single FASTA file in the script's\n    designated output directory.\n\n    FASTA records are returned, as GenBank and even GenBankWithParts format\n    records don't reliably give correct sequence in all cases.\n\n    The script returns two strings for each assembly, a 'class' and a 'label'\n    string - this is for use with, e.g. pyani.\n    \"\"\"\n    # Has duplicate code with get_class_label_info() - needs refactoring\n    logger.info(\"Collecting contig data for %s\", asm_uid)\n    # Assembly record - get binomial and strain names\n    asm_record = Entrez.read(\n        entrez_retry(\n            Entrez.esummary, db='assembly', id=asm_uid, rettype='text'),\n        validate=False)\n    asm_organism = asm_record['DocumentSummarySet']['DocumentSummary'][0][\n        'SpeciesName']\n    try:\n        asm_strain = asm_record['DocumentSummarySet']['DocumentSummary'][0][\n            'Biosource']['InfraspeciesList'][0]['Sub_value']\n    except KeyError:\n        asm_strain = \"\"\n    # Assembly UID (long form) for the output filename\n    outfilename = \"%s.fasta\" % os.path.join(args.outdirname, asm_record[\n        'DocumentSummarySet']['DocumentSummary'][0]['AssemblyAccession'])\n\n    # Create label and class strings\n    genus, species = asm_organism.split(' ', 1)\n\n    # Get FASTA records for contigs\n    logger.info(\"Downloading FASTA records for assembly %s (%s)\", asm_uid,\n                ' '.join([genus[0] + '.', species, asm_strain]))\n    # We're doing an explicit outer retry loop here because we want to confirm\n    # we have the correct data, as well as test for Entrez connection errors,\n    # which is all the entrez_retry function does.\n    tries, success = 0, False\n    while not success and tries < args.retries:\n        records = []  # Holds all return records\n        # We may need to batch contigs\n        query_uids = ','.join(contig_uids)\n        try:\n            for start in range(0, len(contig_uids), batchsize):\n                logger.info(\"Batch: %d-%d\", start, start + batchsize)\n                records.extend(\n                    list(\n                        SeqIO.parse(\n                            entrez_retry(\n                                Entrez.efetch,\n                                db='nucleotide',\n                                id=query_uids,\n                                rettype='fasta',\n                                retmode='text',\n                                retstart=start,\n                                retmax=batchsize), 'fasta')))\n            tries += 1\n            # Check only that correct number of records returned.\n            if len(records) == len(contig_uids):\n                success = True\n            else:\n                logger.warning(\"%d contigs expected, %d contigs returned\",\n                               len(contig_uids), len(records))\n                logger.warning(\"FASTA download for assembly %s failed\",\n                               asm_uid)\n                logger.warning(\"try %d/20\", tries)\n            # Could also check expected assembly sequence length?\n            logger.info(\"Downloaded genome size: %d\",\n                        sum([len(r) for r in records]))\n        except:\n            logger.warning(\"FASTA download for assembly %s failed\", asm_uid)\n            logger.warning(last_exception())\n            logger.warning(\"try %d/20\", tries)\n    if not success:\n        # Could place option on command-line to stop or continue here.\n        logger.error(\"Failed to download records for %s (continuing)\", asm_uid)\n\n    # Write contigs to file\n    retval = SeqIO.write(records, outfilename, 'fasta')\n    logger.info(\"Wrote %d contigs to %s\", retval, outfilename)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef logreport_downloaded(accession, skippedlist, accessiondict, uidaccdict):\n    for vid in accessiondict[accession.split('.')[0]]:\n        if vid in skippedlist:\n            status = \"NOT DOWNLOADED\"\n        else:\n            status = \"DOWNLOADED\"\n        logger.warning(\"\\t\\t%s: %s - %s\", vid, uidaccdict[vid], status)", "response": "Reports to logger whether alternative assemblies for an accession that was missing have been downloaded"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates TETRA Z - scores for each input file.", "response": "def calculate_tetra_zscores(infilenames):\n    \"\"\"Returns dictionary of TETRA Z-scores for each input file.\n\n    - infilenames - collection of paths to sequence files\n    \"\"\"\n    org_tetraz = {}\n    for filename in infilenames:\n        org = os.path.splitext(os.path.split(filename)[-1])[0]\n        org_tetraz[org] = calculate_tetra_zscore(filename)\n    return org_tetraz"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_tetra_zscore(filename):\n    # For the Teeling et al. method, the Z-scores require us to count\n    # mono, di, tri and tetranucleotide sequences - these are stored\n    # (in order) in the counts tuple\n    counts = (collections.defaultdict(int), collections.defaultdict(int),\n              collections.defaultdict(int), collections.defaultdict(int))\n    for rec in SeqIO.parse(filename, 'fasta'):\n        for seq in [str(rec.seq).upper(),\n                    str(rec.seq.reverse_complement()).upper()]:\n            # The Teeling et al. algorithm requires us to consider\n            # both strand orientations, so monocounts are easy\n            for base in ('G', 'C', 'T', 'A'):\n                counts[0][base] += seq.count(base)\n            # For di, tri and tetranucleotide counts, loop over the\n            # sequence and its reverse complement, until near the end:\n            for i in range(len(seq[:-4])):\n                din, tri, tetra = seq[i:i+2], seq[i:i+3], seq[i:i+4]\n                counts[1][str(din)] += 1\n                counts[2][str(tri)] += 1\n                counts[3][str(tetra)] += 1\n            # Then clean up the straggling bit at the end:\n            counts[2][str(seq[-4:-1])] += 1\n            counts[2][str(seq[-3:])] += 1\n            counts[1][str(seq[-4:-2])] += 1\n            counts[1][str(seq[-3:-1])] += 1\n            counts[1][str(seq[-2:])] += 1\n    # Following Teeling (2004), calculate expected frequencies for each\n    # tetranucleotide; we ignore ambiguity symbols\n    tetra_exp = {}\n    for tet in [tetn for tetn in counts[3] if tetra_clean(tetn)]:\n        tetra_exp[tet] = 1. * counts[2][tet[:3]] * counts[2][tet[1:]] / \\\n                         counts[1][tet[1:3]]\n    # Following Teeling (2004) we approximate the std dev and Z-score for each\n    # tetranucleotide\n    tetra_sd = {}\n    tetra_z = {}\n    for tet, exp in list(tetra_exp.items()):\n        den = counts[1][tet[1:3]]\n        tetra_sd[tet] = math.sqrt(exp * (den - counts[2][tet[:3]]) *\n                                  (den - counts[2][tet[1:]]) / (den * den))\n        try:\n            tetra_z[tet] = (counts[3][tet] - exp)/tetra_sd[tet]\n        except ZeroDivisionError:\n            # To record if we hit a zero in the estimation of variance\n            # zeroes = [k for k, v in list(tetra_sd.items()) if v == 0]\n            tetra_z[tet] = 1 / (counts[1][tet[1:3]] * counts[1][tet[1:3]])\n    return tetra_z", "response": "Calculates TETRA Z - score for the input file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate Pearson correlation coefficient for each element of the sequence ID sequence.", "response": "def calculate_correlations(tetra_z):\n    \"\"\"Returns dataframe of Pearson correlation coefficients.\n\n    - tetra_z - dictionary of Z-scores, keyed by sequence ID\n\n    Calculates Pearson correlation coefficient from Z scores for each\n    tetranucleotide. This is done longhand here, which is fast enough,\n    but for robustness we might want to do something else... (TODO).\n\n    Note that we report a correlation by this method, rather than a\n    percentage identity.\n    \"\"\"\n    orgs = sorted(tetra_z.keys())\n    correlations = pd.DataFrame(index=orgs, columns=orgs,\n                                dtype=float).fillna(1.0)\n    for idx, org1 in enumerate(orgs[:-1]):\n        for org2 in orgs[idx+1:]:\n            assert sorted(tetra_z[org1].keys()) == sorted(tetra_z[org2].keys())\n            tets = sorted(tetra_z[org1].keys())\n            zscores = [[tetra_z[org1][t] for t in tets],\n                       [tetra_z[org2][t] for t in tets]]\n            zmeans = [sum(zscore)/len(zscore) for zscore in zscores]\n            zdiffs = [[z - zmeans[0] for z in zscores[0]],\n                      [z - zmeans[1] for z in zscores[1]]]\n            diffprods = sum([zdiffs[0][i] * zdiffs[1][i] for i in\n                             range(len(zdiffs[0]))])\n            zdiffs2 = [sum([z * z for z in zdiffs[0]]),\n                       sum([z * z for z in zdiffs[1]])]\n            correlations[org1][org2] = diffprods / \\\n                                       math.sqrt(zdiffs2[0] * zdiffs2[1])\n            correlations[org2][org1] = correlations[org1][org2]\n    return correlations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of alternative sequence labels from a file.", "response": "def get_labels(filename, logger=None):\n    \"\"\"Returns a dictionary of alternative sequence labels, or None\n\n    - filename - path to file containing tab-separated table of labels\n\n    Input files should be formatted as <key>\\t<label>, one pair per line.\n    \"\"\"\n    labeldict = {}\n    if filename is not None:\n        if logger:\n            logger.info(\"Reading labels from %s\", filename)\n        with open(filename, \"r\") as ifh:\n            count = 0\n            for line in ifh.readlines():\n                count += 1\n                try:\n                    key, label = line.strip().split(\"\\t\")\n                except ValueError:\n                    if logger:\n                        logger.warning(\"Problem with class file: %s\", filename)\n                        logger.warning(\"%d: %s\", (count, line.strip()))\n                        logger.warning(\"(skipping line)\")\n                    continue\n                else:\n                    labeldict[key] = label\n    return labeldict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_tot_length(self, qname, sname, value, sym=True):\n        self.alignment_lengths.loc[qname, sname] = value\n        if sym:\n            self.alignment_lengths.loc[sname, qname] = value", "response": "Add a total length value to self. alignment_lengths."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a similarity error value to self. similarity_errors.", "response": "def add_sim_errors(self, qname, sname, value, sym=True):\n        \"\"\"Add a similarity error value to self.similarity_errors.\"\"\"\n        self.similarity_errors.loc[qname, sname] = value\n        if sym:\n            self.similarity_errors.loc[sname, qname] = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a percentage identity value to self. percentage_identity.", "response": "def add_pid(self, qname, sname, value, sym=True):\n        \"\"\"Add a percentage identity value to self.percentage_identity.\"\"\"\n        self.percentage_identity.loc[qname, sname] = value\n        if sym:\n            self.percentage_identity.loc[sname, qname] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds percentage coverage values to self. alignment_coverage.", "response": "def add_coverage(self, qname, sname, qcover, scover=None):\n        \"\"\"Add percentage coverage values to self.alignment_coverage.\"\"\"\n        self.alignment_coverage.loc[qname, sname] = qcover\n        if scover:\n            self.alignment_coverage.loc[sname, qname] = scover"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data(self):\n        stemdict = {\n            \"ANIm\": pyani_config.ANIM_FILESTEMS,\n            \"ANIb\": pyani_config.ANIB_FILESTEMS,\n            \"ANIblastall\": pyani_config.ANIBLASTALL_FILESTEMS,\n        }\n        return zip(\n            (\n                self.alignment_lengths,\n                self.percentage_identity,\n                self.alignment_coverage,\n                self.similarity_errors,\n                self.hadamard,\n            ),\n            stemdict[self.mode],\n        )", "response": "Return list of ( dataframe filestem ) tuples."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning database format build command", "response": "def build_db_cmd(self, fname):\n        \"\"\"Return database format/build command\"\"\"\n        return self.funcs.db_func(fname, self.outdir, self.exes.format_exe)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a list of files and returns a list of filenames and a dictionary containing the names of the fragments that are generated by the fragment_fasta_files function.", "response": "def fragment_fasta_files(infiles, outdirname, fragsize):\n    \"\"\"Chops sequences of the passed files into fragments, returns filenames.\n\n    - infiles - paths to each input sequence file\n    - outdirname - path to output directory\n    - fragsize - the size of sequence fragments\n\n    Takes every sequence from every file in infiles, and splits them into\n    consecutive fragments of length fragsize, (with any trailing sequences\n    being included, even if shorter than fragsize), and writes the resulting\n    set of sequences to a file with the same name in the output directory.\n    All fragments are named consecutively and uniquely (within a file) as\n    fragNNNNN. Sequence description fields are retained.\n    \"\"\"\n    outfnames = []\n    for fname in infiles:\n        outstem, outext = os.path.splitext(os.path.split(fname)[-1])\n        outfname = os.path.join(outdirname, outstem) + \"-fragments\" + outext\n        outseqs = []\n        count = 0\n        for seq in SeqIO.parse(fname, \"fasta\"):\n            idx = 0\n            while idx < len(seq):\n                count += 1\n                newseq = seq[idx : idx + fragsize]\n                newseq.id = \"frag%05d\" % count\n                outseqs.append(newseq)\n                idx += fragsize\n        outfnames.append(outfname)\n        SeqIO.write(outseqs, outfname, \"fasta\")\n    return outfnames, get_fraglength_dict(outfnames)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of sequence fragment lengths keyed by sequence name.", "response": "def get_fraglength_dict(fastafiles):\n    \"\"\"Returns dictionary of sequence fragment lengths, keyed by query name.\n\n    - fastafiles - list of FASTA input whole sequence files\n\n    Loops over input files and, for each, produces a dictionary with fragment\n    lengths, keyed by sequence ID. These are returned as a dictionary with\n    the keys being query IDs derived from filenames.\n    \"\"\"\n    fraglength_dict = {}\n    for filename in fastafiles:\n        qname = os.path.split(filename)[-1].split(\"-fragments\")[0]\n        fraglength_dict[qname] = get_fragment_lengths(filename)\n    return fraglength_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_fragment_lengths(fastafile):\n    fraglengths = {}\n    for seq in SeqIO.parse(fastafile, \"fasta\"):\n        fraglengths[seq.id] = len(seq)\n    return fraglengths", "response": "Returns dictionary of sequence fragment lengths keyed by fragment ID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn dictionary of db - building commands keyed by dbname.", "response": "def build_db_jobs(infiles, blastcmds):\n    \"\"\"Returns dictionary of db-building commands, keyed by dbname.\"\"\"\n    dbjobdict = {}  # Dict of database construction jobs, keyed by filename\n    # Create dictionary of database building jobs, keyed by db name\n    # defining jobnum for later use as last job index used\n    for idx, fname in enumerate(infiles):\n        dbjobdict[blastcmds.get_db_name(fname)] = pyani_jobs.Job(\n            \"%s_db_%06d\" % (blastcmds.prefix, idx), blastcmds.build_db_cmd(fname)\n        )\n    return dbjobdict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_blastcmd_builder(\n    mode, outdir, format_exe=None, blast_exe=None, prefix=\"ANIBLAST\"\n):\n    \"\"\"Returns BLASTcmds object for construction of BLAST commands.\"\"\"\n    if mode == \"ANIb\":  # BLAST/formatting executable depends on mode\n        blastcmds = BLASTcmds(\n            BLASTfunctions(construct_makeblastdb_cmd, construct_blastn_cmdline),\n            BLASTexes(\n                format_exe or pyani_config.MAKEBLASTDB_DEFAULT,\n                blast_exe or pyani_config.BLASTN_DEFAULT,\n            ),\n            prefix,\n            outdir,\n        )\n    else:\n        blastcmds = BLASTcmds(\n            BLASTfunctions(construct_formatdb_cmd, construct_blastall_cmdline),\n            BLASTexes(\n                format_exe or pyani_config.FORMATDB_DEFAULT,\n                blast_exe or pyani_config.BLASTALL_DEFAULT,\n            ),\n            prefix,\n            outdir,\n        )\n    return blastcmds", "response": "Returns BLASTcmds object for construction of BLAST commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_job_graph(infiles, fragfiles, blastcmds):\n    joblist = []  # Holds list of job dependency graphs\n\n    # Get dictionary of database-building jobs\n    dbjobdict = build_db_jobs(infiles, blastcmds)\n\n    # Create list of BLAST executable jobs, with dependencies\n    jobnum = len(dbjobdict)\n    for idx, fname1 in enumerate(fragfiles[:-1]):\n        for fname2 in fragfiles[idx + 1 :]:\n            jobnum += 1\n            jobs = [\n                pyani_jobs.Job(\n                    \"%s_exe_%06d_a\" % (blastcmds.prefix, jobnum),\n                    blastcmds.build_blast_cmd(fname1, fname2.replace(\"-fragments\", \"\")),\n                ),\n                pyani_jobs.Job(\n                    \"%s_exe_%06d_b\" % (blastcmds.prefix, jobnum),\n                    blastcmds.build_blast_cmd(fname2, fname1.replace(\"-fragments\", \"\")),\n                ),\n            ]\n            jobs[0].add_dependency(dbjobdict[fname1.replace(\"-fragments\", \"\")])\n            jobs[1].add_dependency(dbjobdict[fname2.replace(\"-fragments\", \"\")])\n            joblist.extend(jobs)\n\n    # Return the dependency graph\n    return joblist", "response": "Return a list of Job objects that are dependent on the passed input sequence files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_blastdb_commands(filenames, outdir, blastdb_exe=None, mode=\"ANIb\"):\n    if mode == \"ANIb\":\n        construct_db_cmdline = construct_makeblastdb_cmd\n    else:\n        construct_db_cmdline = construct_formatdb_cmd\n    if blastdb_exe is None:\n        cmdlines = [construct_db_cmdline(fname, outdir) for fname in filenames]\n    else:\n        cmdlines = [\n            construct_db_cmdline(fname, outdir, blastdb_exe) for fname in filenames\n        ]\n    return cmdlines", "response": "Return a list of makeblastdb command - lines for ANIb or ANIblastall ArcGIS"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a single makeblastdb command.", "response": "def construct_makeblastdb_cmd(\n    filename, outdir, blastdb_exe=pyani_config.MAKEBLASTDB_DEFAULT\n):\n    \"\"\"Returns a single makeblastdb command.\n\n    - filename - input filename\n    - blastdb_exe - path to the makeblastdb executable\n    \"\"\"\n    title = os.path.splitext(os.path.split(filename)[-1])[0]\n    outfilename = os.path.join(outdir, os.path.split(filename)[-1])\n    return (\n        \"{0} -dbtype nucl -in {1} -title {2} -out {3}\".format(\n            blastdb_exe, filename, title, outfilename\n        ),\n        outfilename,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef construct_formatdb_cmd(filename, outdir, blastdb_exe=pyani_config.FORMATDB_DEFAULT):\n    title = os.path.splitext(os.path.split(filename)[-1])[0]\n    newfilename = os.path.join(outdir, os.path.split(filename)[-1])\n    shutil.copy(filename, newfilename)\n    return (\n        \"{0} -p F -i {1} -t {2}\".format(blastdb_exe, newfilename, title),\n        newfilename,\n    )", "response": "Constructs a single formatdb command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the list of blastn command - lines for ANIm .", "response": "def generate_blastn_commands(filenames, outdir, blast_exe=None, mode=\"ANIb\"):\n    \"\"\"Return a list of blastn command-lines for ANIm\n\n    - filenames - a list of paths to fragmented input FASTA files\n    - outdir - path to output directory\n    - blastn_exe - path to BLASTN executable\n\n    Assumes that the fragment sequence input filenames have the form\n    ACCESSION-fragments.ext, where the corresponding BLAST database filenames\n    have the form ACCESSION.ext. This is the convention followed by the\n    fragment_FASTA_files() function above.\n    \"\"\"\n    if mode == \"ANIb\":\n        construct_blast_cmdline = construct_blastn_cmdline\n    else:\n        construct_blast_cmdline = construct_blastall_cmdline\n    cmdlines = []\n    for idx, fname1 in enumerate(filenames[:-1]):\n        dbname1 = fname1.replace(\"-fragments\", \"\")\n        for fname2 in filenames[idx + 1 :]:\n            dbname2 = fname2.replace(\"-fragments\", \"\")\n            if blast_exe is None:\n                cmdlines.append(construct_blast_cmdline(fname1, dbname2, outdir))\n                cmdlines.append(construct_blast_cmdline(fname2, dbname1, outdir))\n            else:\n                cmdlines.append(\n                    construct_blast_cmdline(fname1, dbname2, outdir, blast_exe)\n                )\n                cmdlines.append(\n                    construct_blast_cmdline(fname2, dbname1, outdir, blast_exe)\n                )\n    return cmdlines"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef construct_blastn_cmdline(\n    fname1, fname2, outdir, blastn_exe=pyani_config.BLASTN_DEFAULT\n):\n    \"\"\"Returns a single blastn command.\n\n    - filename - input filename\n    - blastn_exe - path to BLASTN executable\n    \"\"\"\n    fstem1 = os.path.splitext(os.path.split(fname1)[-1])[0]\n    fstem2 = os.path.splitext(os.path.split(fname2)[-1])[0]\n    fstem1 = fstem1.replace(\"-fragments\", \"\")\n    prefix = os.path.join(outdir, \"%s_vs_%s\" % (fstem1, fstem2))\n    cmd = (\n        \"{0} -out {1}.blast_tab -query {2} -db {3} \"\n        + \"-xdrop_gap_final 150 -dust no -evalue 1e-15 \"\n        + \"-max_target_seqs 1 -outfmt '6 qseqid sseqid length mismatch \"\n        + \"pident nident qlen slen qstart qend sstart send positive \"\n        + \"ppos gaps' -task blastn\"\n    )\n    return cmd.format(blastn_exe, prefix, fname1, fname2)", "response": "Returns a single blastn command line."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef construct_blastall_cmdline(\n    fname1, fname2, outdir, blastall_exe=pyani_config.BLASTALL_DEFAULT\n):\n    \"\"\"Returns a single blastall command.\n\n    - blastall_exe - path to BLASTALL executable\n    \"\"\"\n    fstem1 = os.path.splitext(os.path.split(fname1)[-1])[0]\n    fstem2 = os.path.splitext(os.path.split(fname2)[-1])[0]\n    fstem1 = fstem1.replace(\"-fragments\", \"\")\n    prefix = os.path.join(outdir, \"%s_vs_%s\" % (fstem1, fstem2))\n    cmd = (\n        \"{0} -p blastn -o {1}.blast_tab -i {2} -d {3} \"\n        + \"-X 150 -q -1 -F F -e 1e-15 \"\n        + \"-b 1 -v 1 -m 8\"\n    )\n    return cmd.format(blastall_exe, prefix, fname1, fname2)", "response": "Returns a single blastall command line."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_blast(\n    blast_dir,\n    org_lengths,\n    fraglengths=None,\n    mode=\"ANIb\",\n    identity=0.3,\n    coverage=0.7,\n    logger=None,\n):\n    \"\"\"Returns a tuple of ANIb results for .blast_tab files in the output dir.\n\n    - blast_dir - path to the directory containing .blast_tab files\n    - org_lengths - the base count for each input sequence\n    - fraglengths - dictionary of query sequence fragment lengths, only\n    needed for BLASTALL output\n    - mode - parsing BLASTN+ or BLASTALL output?\n    - logger - a logger for messages\n\n    Returns the following pandas dataframes in an ANIResults object;\n    query sequences are rows, subject sequences are columns:\n\n    - alignment_lengths - non-symmetrical: total length of alignment\n    - percentage_identity - non-symmetrical: ANIb (Goris) percentage identity\n    - alignment_coverage - non-symmetrical: coverage of query\n    - similarity_errors - non-symmetrical: count of similarity errors\n\n    May throw a ZeroDivisionError if one or more BLAST runs failed, or a\n    very distant sequence was included in the analysis.\n    \"\"\"\n    # Process directory to identify input files\n    blastfiles = pyani_files.get_input_files(blast_dir, \".blast_tab\")\n    # Hold data in ANIResults object\n    results = ANIResults(list(org_lengths.keys()), mode)\n\n    # Fill diagonal NA values for alignment_length with org_lengths\n    for org, length in list(org_lengths.items()):\n        results.alignment_lengths[org][org] = length\n\n    # Process .blast_tab files assuming that the filename format holds:\n    # org1_vs_org2.blast_tab:\n    for blastfile in blastfiles:\n        qname, sname = os.path.splitext(os.path.split(blastfile)[-1])[0].split(\"_vs_\")\n\n        # We may have BLAST files from other analyses in the same directory\n        # If this occurs, we raise a warning, and skip the file\n        if qname not in list(org_lengths.keys()):\n            if logger:\n                logger.warning(\n                    \"Query name %s not in input \" % qname\n                    + \"sequence list, skipping %s\" % blastfile\n                )\n            continue\n        if sname not in list(org_lengths.keys()):\n            if logger:\n                logger.warning(\n                    \"Subject name %s not in input \" % sname\n                    + \"sequence list, skipping %s\" % blastfile\n                )\n            continue\n        resultvals = parse_blast_tab(blastfile, fraglengths, identity, coverage, mode)\n        query_cover = float(resultvals[0]) / org_lengths[qname]\n\n        # Populate dataframes: when assigning data, we need to note that\n        # we have asymmetrical data from BLAST output, so only the\n        # upper triangle is populated\n        results.add_tot_length(qname, sname, resultvals[0], sym=False)\n        results.add_sim_errors(qname, sname, resultvals[1], sym=False)\n        results.add_pid(qname, sname, 0.01 * resultvals[2], sym=False)\n        results.add_coverage(qname, sname, query_cover)\n    return results", "response": "Processes the. blast_tab files in the output directory and returns a tuple of ANIResults objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the BLASTN file and return a tuple containing the alignment length similarity errors mean_pid", "response": "def parse_blast_tab(filename, fraglengths, identity, coverage, mode=\"ANIb\"):\n    \"\"\"Returns (alignment length, similarity errors, mean_pid) tuple\n    from .blast_tab\n\n    - filename - path to .blast_tab file\n\n    Calculate the alignment length and total number of similarity errors (as\n    we would with ANIm), as well as the Goris et al.-defined mean identity\n    of all valid BLAST matches for the passed BLASTALL alignment .blast_tab\n    file.\n\n    '''ANI between the query genome and the reference genome was calculated as\n    the mean identity of all BLASTN matches that showed more than 30% overall\n    sequence identity (recalculated to an identity along the entire sequence)\n    over an alignable region of at least 70% of their length.\n    '''\n    \"\"\"\n    # Assuming that the filename format holds org1_vs_org2.blast_tab:\n    qname = os.path.splitext(os.path.split(filename)[-1])[0].split(\"_vs_\")[0]\n    # Load output as dataframe\n    if mode == \"ANIblastall\":\n        qfraglengths = fraglengths[qname]\n        columns = [\n            \"sid\",\n            \"blast_pid\",\n            \"blast_alnlen\",\n            \"blast_mismatch\",\n            \"blast_gaps\",\n            \"q_start\",\n            \"q_end\",\n            \"s_start\",\n            \"s_end\",\n            \"e_Value\",\n            \"bit_score\",\n        ]\n    else:\n        columns = [\n            \"sbjct_id\",\n            \"blast_alnlen\",\n            \"blast_mismatch\",\n            \"blast_pid\",\n            \"blast_identities\",\n            \"qlen\",\n            \"slen\",\n            \"q_start\",\n            \"q_end\",\n            \"s_start\",\n            \"s_end\",\n            \"blast_pos\",\n            \"ppos\",\n            \"blast_gaps\",\n        ]\n    # We may receive an empty BLASTN output file, if there are no significant\n    # regions of homology. This causes pandas to throw an error on CSV import.\n    # To get past this, we create an empty dataframe with the appropriate\n    # columns.\n    try:\n        data = pd.read_csv(filename, header=None, sep=\"\\t\", index_col=0)\n        data.columns = columns\n    except pd.io.common.EmptyDataError:\n        data = pd.DataFrame(columns=columns)\n    # Add new column for fragment length, only for BLASTALL\n    if mode == \"ANIblastall\":\n        data[\"qlen\"] = pd.Series(\n            [qfraglengths[idx] for idx in data.index], index=data.index\n        )\n    # Add new columns for recalculated alignment length, proportion, and\n    # percentage identity\n    data[\"ani_alnlen\"] = data[\"blast_alnlen\"] - data[\"blast_gaps\"]\n    data[\"ani_alnids\"] = data[\"ani_alnlen\"] - data[\"blast_mismatch\"]\n    data[\"ani_coverage\"] = data[\"ani_alnlen\"] / data[\"qlen\"]\n    data[\"ani_pid\"] = data[\"ani_alnids\"] / data[\"qlen\"]\n    # Filter rows on 'ani_coverage' > 0.7, 'ani_pid' > 0.3\n    filtered = data[(data[\"ani_coverage\"] > coverage) & (data[\"ani_pid\"] > identity)]\n    # Dedupe query hits, so we only take the best hit\n    filtered = filtered.groupby(filtered.index).first()\n    # Replace NaNs with zero\n    filtered = filtered.fillna(value=0)  # Needed if no matches\n    # The ANI value is then the mean percentage identity.\n    # We report total alignment length and the number of similarity errors\n    # (mismatches and gaps), as for ANIm\n    # NOTE: We report the mean of 'blast_pid' for concordance with JSpecies\n    # Despite this, the concordance is not exact. Manual inspection during\n    # development indicated that a handful of fragments are differentially\n    # filtered out in JSpecies and this script. This is often on the basis\n    # of rounding differences (e.g. coverage being close to 70%).\n    # NOTE: If there are no hits, then ani_pid will be nan - we replace this\n    # with zero if that happens\n    ani_pid = filtered[\"blast_pid\"].mean()\n    if pd.isnull(ani_pid):  # Happens if there are no matches in ANIb\n        ani_pid = 0\n    aln_length = filtered[\"ani_alnlen\"].sum()\n    sim_errors = filtered[\"blast_mismatch\"].sum() + filtered[\"blast_gaps\"].sum()\n    filtered.to_csv(filename + \".dataframe\", sep=\"\\t\")\n    return aln_length, sim_errors, ani_pid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit a passed iterable into chunks of a given size.", "response": "def split_seq(iterable, size):\n    \"\"\"Splits a passed iterable into chunks of a given size.\"\"\"\n    elm = iter(iterable)\n    item = list(itertools.islice(elm, size))\n    while item:\n        yield item\n        item = list(itertools.islice(elm, size))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of jobs from a passed jobgraph.", "response": "def build_joblist(jobgraph):\n    \"\"\"Returns a list of jobs, from a passed jobgraph.\"\"\"\n    jobset = set()\n    for job in jobgraph:\n        jobset = populate_jobset(job, jobset, depth=1)\n    return list(jobset)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compile_jobgroups_from_joblist(joblist, jgprefix, sgegroupsize):\n    jobcmds = defaultdict(list)\n    for job in joblist:\n        jobcmds[job.command.split(' ', 1)[0]].append(job.command)\n    jobgroups = []\n    for cmds in list(jobcmds.items()):\n        # Break arglist up into batches of sgegroupsize (default: 10,000)\n        sublists = split_seq(cmds[1], sgegroupsize)\n        count = 0\n        for sublist in sublists:\n            count += 1\n            sge_jobcmdlist = ['\\\"%s\\\"' % jc for jc in sublist]\n            jobgroups.append(JobGroup(\"%s_%d\" % (jgprefix, count),\n                                      \"$cmds\",\n                                      arguments={'cmds': sge_jobcmdlist}))\n    return jobgroups", "response": "Return list of jobgroups rather than list of jobs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_dependency_graph(jobgraph, logger=None, jgprefix=\"ANIm_SGE_JG\",\n                         sgegroupsize=10000, sgeargs=None):\n    \"\"\"Creates and runs GridEngine scripts for jobs based on the passed\n    jobgraph.\n\n    - jobgraph - list of jobs, which may have dependencies.\n    - verbose - flag for multiprocessing verbosity\n    - logger - a logger module logger (optional)\n    - jgprefix - a prefix for the submitted jobs, in the scheduler\n    - sgegroupsize - the maximum size for an array job submission\n    - sgeargs - additional arguments to qsub\n\n    The strategy here is to loop over each job in the list of jobs (jobgraph),\n    and create/populate a series of Sets of commands, to be run in\n    reverse order with multiprocessing_run as asynchronous pools.\n\n    The strategy here is to loop over each job in the dependency graph, and\n    add the job to a new list of jobs, swapping out the Job dependency for\n    the name of the Job on which it depends.\n    \"\"\"\n    joblist = build_joblist(jobgraph)\n\n    # Try to be informative by telling the user what jobs will run\n    dep_count = 0  # how many dependencies are there\n    if logger:\n        logger.info(\"Jobs to run with scheduler\")\n        for job in joblist:\n            logger.info(\"{0}: {1}\".format(job.name, job.command))\n            if len(job.dependencies):\n                dep_count += len(job.dependencies)\n                for dep in job.dependencies:\n                    logger.info(\"\\t[^ depends on: %s]\" % dep.name)\n    logger.info(\"There are %d job dependencies\" % dep_count)\n\n    # If there are no job dependencies, we can use an array (or series of\n    # arrays) to schedule our jobs. This cuts down on problems with long\n    # job lists choking up the queue.\n    if dep_count == 0:\n        logger.info(\"Compiling jobs into JobGroups\")\n        joblist = compile_jobgroups_from_joblist(joblist, jgprefix,\n                                                 sgegroupsize)\n\n    # Send jobs to scheduler\n    logger.info(\"Running jobs with scheduler...\")\n    logger.info(\"Jobs passed to scheduler in order:\")\n    for job in joblist:\n        logger.info(\"\\t%s\" % job.name)\n    build_and_submit_jobs(os.curdir, joblist, sgeargs)\n    logger.info(\"Waiting for SGE-submitted jobs to finish (polling)\")\n    for job in joblist:\n        job.wait()", "response": "Creates and runs GridEngine scripts for jobs based on the passed jobgraph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npopulate the jobset with jobs at the given depth.", "response": "def populate_jobset(job, jobset, depth):\n    \"\"\" Creates a set of jobs, containing jobs at difference depths of the\n    dependency tree, retaining dependencies as strings, not Jobs.\n    \"\"\"\n    jobset.add(job)\n    if len(job.dependencies) == 0:\n        return jobset\n    for j in job.dependencies:\n        jobset = populate_jobset(j, jobset, depth+1)\n    return jobset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_directories(root_dir):\n    # If the root directory doesn't exist, create it\n    if not os.path.exists(root_dir):\n        os.mkdir(root_dir)\n\n    # Create subdirectories\n    directories = [os.path.join(root_dir, subdir) for subdir in\n                   (\"output\", \"stderr\", \"stdout\", \"jobs\")]\n    for dirname in directories:\n        os.makedirs(dirname, exist_ok=True)", "response": "Constructs the directories that are needed to create the SGE\nCTYPE."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs the script for each passed Job in the jobs iterable", "response": "def build_job_scripts(root_dir, jobs):\n    \"\"\"Constructs the script for each passed Job in the jobs iterable\n\n    - root_dir      Path to output directory\n    \"\"\"\n    # Loop over the job list, creating each job script in turn, and then adding\n    # scriptPath to the Job object\n    for job in jobs:\n        scriptpath = os.path.join(root_dir, \"jobs\", job.name)\n        with open(scriptpath, \"w\") as scriptfile:\n            scriptfile.write(\"#!/bin/sh\\n#$ -S /bin/bash\\n%s\\n\" % job.script)\n        job.scriptpath = scriptpath"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubmit the passed list of jobs to the Grid Engine server using the passed directory as the root for scheduler output.", "response": "def submit_safe_jobs(root_dir, jobs, sgeargs=None):\n    \"\"\"Submit the passed list of jobs to the Grid Engine server, using the\n    passed directory as the root for scheduler output.\n\n    - root_dir      Path to output directory\n    - jobs          Iterable of Job objects\n    \"\"\"\n    # Loop over each job, constructing SGE command-line based on job settings\n    for job in jobs:\n        job.out = os.path.join(root_dir, \"stdout\")\n        job.err = os.path.join(root_dir, \"stderr\")\n\n        # Add the job name, current working directory, and SGE stdout/stderr\n        # directories to the SGE command line\n        args = \" -N %s \" % (job.name)\n        args += \" -cwd \"\n        args += \" -o %s -e %s \" % (job.out, job.err)\n\n        # If a queue is specified, add this to the SGE command line\n        # LP: This has an undeclared variable, not sure why - delete?\n        #if job.queue is not None and job.queue in local_queues:\n        #    args += local_queues[job.queue]\n\n        # If the job is actually a JobGroup, add the task numbering argument\n        if isinstance(job, JobGroup):\n            args += \"-t 1:%d \" % (job.tasks)\n\n        # If there are dependencies for this job, hold the job until they are\n        # complete\n        if len(job.dependencies) > 0:\n            args += \"-hold_jid \"\n            for dep in job.dependencies:\n                args += dep.name + \",\"\n            args = args[:-1]\n\n        # Build the qsub SGE commandline (passing local environment)\n        qsubcmd = (\"%s -V %s %s\" %\n                   (pyani_config.QSUB_DEFAULT, args, job.scriptpath))\n        if sgeargs is not None:\n            qsubcmd = \"%s %s\" % (qsubcmd, sgeargs)\n        os.system(qsubcmd)               # Run the command\n        job.submitted = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsubmits each of the passed jobs to the SGE server using the passed directory as root for SGE output.", "response": "def submit_jobs(root_dir, jobs, sgeargs=None):\n    \"\"\" Submit each of the passed jobs to the SGE server, using the passed\n    directory as root for SGE output.\n\n    - root_dir       Path to output directory\n    - jobs           List of Job objects\n    \"\"\"\n    waiting = list(jobs)                 # List of jobs still to be done\n    # Loop over the list of pending jobs, while there still are any\n    while len(waiting) > 0:\n        # extract submittable jobs\n        submittable = extract_submittable_jobs(waiting)\n        # run those jobs\n        submit_safe_jobs(root_dir, submittable, sgeargs)\n        # remove those from the waiting list\n        for job in submittable:\n            waiting.remove(job)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_and_submit_jobs(root_dir, jobs, sgeargs=None):\n    # If the passed set of jobs is not a list, turn it into one. This makes the\n    # use of a single JobGroup a little more intutitive\n    if not isinstance(jobs, list):\n        jobs = [jobs]\n\n    # Build and submit the passed jobs\n    build_directories(root_dir)        # build all necessary directories\n    build_job_scripts(root_dir, jobs)  # build job scripts\n    submit_jobs(root_dir, jobs, sgeargs)", "response": "Builds and submits the passed iterable of Jobs to SGE and then submits them to the SGE s\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef params_mpl(df):\n    return {'ANIb_alignment_lengths': ('afmhot', df.values.min(),\n                                       df.values.max()),\n            'ANIb_percentage_identity': ('spbnd_BuRd', 0, 1),\n            'ANIb_alignment_coverage': ('BuRd', 0, 1),\n            'ANIb_hadamard': ('hadamard_BuRd', 0, 1),\n            'ANIb_similarity_errors': ('afmhot', df.values.min(),\n                                       df.values.max()),\n            'ANIm_alignment_lengths': ('afmhot', df.values.min(),\n                                       df.values.max()),\n            'ANIm_percentage_identity': ('spbnd_BuRd', 0, 1),\n            'ANIm_alignment_coverage': ('BuRd', 0, 1),\n            'ANIm_hadamard': ('hadamard_BuRd', 0, 1),\n            'ANIm_similarity_errors': ('afmhot', df.values.min(),\n                                       df.values.max()),\n            'TETRA_correlations': ('spbnd_BuRd', 0, 1),\n            'ANIblastall_alignment_lengths': ('afmhot', df.values.min(),\n                                              df.values.max()),\n            'ANIblastall_percentage_identity': ('spbnd_BuRd', 0, 1),\n            'ANIblastall_alignment_coverage': ('BuRd', 0, 1),\n            'ANIblastall_hadamard': ('hadamard_BuRd', 0, 1),\n            'ANIblastall_similarity_errors': ('afmhot', df.values.min(),\n                                              df.values.max())}", "response": "Returns dict of matplotlib parameters dependent on dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading a file from the datasets_url and save it to target_dir unless the file already exists.", "response": "def download_file(fname, target_dir=None, force=False):\n    \"\"\"Download fname from the datasets_url, and save it to target_dir,\n    unless the file already exists, and force is False.\n\n    Parameters\n    ----------\n    fname : str\n        Name of the file to download\n\n    target_dir : str\n        Directory where to store the file\n\n    force : bool\n        Force downloading the file, if it already exists\n\n    Returns\n    -------\n    fname : str\n        Full path of the downloaded file\n    \"\"\"\n    target_dir = target_dir or temporary_dir()\n    target_fname = os.path.join(target_dir, fname)\n\n    if force or not os.path.isfile(target_fname):\n        url = urljoin(datasets_url, fname)\n        urlretrieve(url, target_fname)\n\n    return target_fname"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_idx(fd):\n    DATA_TYPES = {0x08: 'B',  # unsigned byte\n                  0x09: 'b',  # signed byte\n                  0x0b: 'h',  # short (2 bytes)\n                  0x0c: 'i',  # int (4 bytes)\n                  0x0d: 'f',  # float (4 bytes)\n                  0x0e: 'd'}  # double (8 bytes)\n\n    header = fd.read(4)\n    if len(header) != 4:\n        raise IdxDecodeError('Invalid IDX file, '\n                             'file empty or does not contain a full header.')\n\n    zeros, data_type, num_dimensions = struct.unpack('>HBB', header)\n\n    if zeros != 0:\n        raise IdxDecodeError('Invalid IDX file, '\n                             'file must start with two zero bytes. '\n                             'Found 0x%02x' % zeros)\n\n    try:\n        data_type = DATA_TYPES[data_type]\n    except KeyError:\n        raise IdxDecodeError('Unknown data type '\n                             '0x%02x in IDX file' % data_type)\n\n    dimension_sizes = struct.unpack('>' + 'I' * num_dimensions,\n                                    fd.read(4 * num_dimensions))\n\n    data = array.array(data_type, fd.read())\n    data.byteswap()  # looks like array.array reads data as little endian\n\n    expected_items = functools.reduce(operator.mul, dimension_sizes)\n    if len(data) != expected_items:\n        raise IdxDecodeError('IDX file has wrong number of items. '\n                             'Expected: %d. Found: %d' % (expected_items,\n                                                          len(data)))\n\n    return numpy.array(data).reshape(dimension_sizes)", "response": "Parse an IDX file and return it as a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload the IDX file named fname from dataset_url and return it as a numpy array.", "response": "def download_and_parse_mnist_file(fname, target_dir=None, force=False):\n    \"\"\"Download the IDX file named fname from the URL specified in dataset_url\n    and return it as a numpy array.\n\n    Parameters\n    ----------\n    fname : str\n        File name to download and parse\n\n    target_dir : str\n        Directory where to store the file\n\n    force : bool\n        Force downloading the file, if it already exists\n\n    Returns\n    -------\n    data : numpy.ndarray\n        Numpy array with the dimensions and the data in the IDX file\n    \"\"\"\n    fname = download_file(fname, target_dir=target_dir, force=force)\n    fopen = gzip.open if os.path.splitext(fname)[1] == '.gz' else open\n    with fopen(fname, 'rb') as fd:\n        return parse_idx(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists [ Transaction]: The current stack of Transactions.", "response": "def _transactions(self):\n        \"list[Transaction]: The current stack of Transactions.\"\n        transactions = getattr(self._state, \"transactions\", None)\n        if transactions is None:\n            transactions = self._state.transactions = []\n        return transactions"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transactional(*, adapter=None, retries=3, propagation=Transaction.Propagation.Nested):\n    def decorator(fn):\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            nonlocal adapter\n            adapter = adapter or get_adapter()\n            attempts, cause = 0, None\n            while attempts <= retries:\n                attempts += 1\n                transaction = adapter.transaction(propagation)\n\n                try:\n                    transaction.begin()\n                    res = fn(*args, **kwargs)\n                    transaction.commit()\n                    return res\n\n                except TransactionFailed as e:\n                    cause = e\n                    continue\n\n                except Exception as e:\n                    transaction.rollback()\n                    raise e\n\n                finally:\n                    transaction.end()\n\n            raise RetriesExceeded(cause)\n        return inner\n    return decorator", "response": "Decorator for functions that run inside a Datastore transaction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of results to fetch per batch.", "response": "def batch_size(self):\n        \"\"\"int: The number of results to fetch per batch.  Clamped to\n        limit if limit is set and is smaller than the given batch\n        size.\n        \"\"\"\n        batch_size = self.get(\"batch_size\", DEFAULT_BATCH_SIZE)\n        if self.limit is not None:\n            return min(self.limit, batch_size)\n        return batch_size"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetch_next_page(self):\n        for page in self:\n            return page\n        else:\n            return Page(self._resultset.cursor, iter(()))", "response": "Fetch the next Page of results."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef count(self, *, page_size=DEFAULT_BATCH_SIZE, **options):\n        entities = 0\n        options = QueryOptions(self).replace(keys_only=True)\n        for page in self.paginate(page_size=page_size, **options):\n            entities += len(list(page))\n        return entities", "response": "Counts the number of entities that match this query."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self, *, page_size=DEFAULT_BATCH_SIZE, **options):\n        from .model import delete_multi\n\n        deleted = 0\n        options = QueryOptions(self).replace(keys_only=True)\n        for page in self.paginate(page_size=page_size, **options):\n            keys = list(page)\n            deleted += len(keys)\n            delete_multi(keys)\n\n        return deleted", "response": "Deletes all the entities that match this query."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning this query and get the first result.", "response": "def get(self, **options):\n        \"\"\"Run this query and get the first result.\n\n        Parameters:\n          \\**options(QueryOptions, optional)\n\n        Returns:\n          Model: An entity or None if there were no results.\n        \"\"\"\n        sub_query = self.with_limit(1)\n        options = QueryOptions(sub_query).replace(batch_size=1)\n        for result in sub_query.run(**options):\n            return result\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun this query and return a page iterator.", "response": "def paginate(self, *, page_size, **options):\n        \"\"\"Run this query and return a page iterator.\n\n        Parameters:\n          page_size(int): The number of entities to fetch per page.\n          \\**options(QueryOptions, optional)\n\n        Returns:\n          Pages: An iterator for this query's pages of results.\n        \"\"\"\n        return Pages(self._prepare(), page_size, QueryOptions(self, **options))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_false(entity, prop, name):\n    \"bool: True if the value of a property is False.\"\n    return is_not_empty(entity, prop, name) and name in entity._data and not bool(getattr(entity, name))", "response": "bool : True if the value of a property is False."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef namespace(namespace):\n    try:\n        current_namespace = _namespace.current\n    except AttributeError:\n        current_namespace = None\n\n    set_namespace(namespace)\n    try:\n        yield\n    finally:\n        set_namespace(current_namespace)", "response": "Context manager for stacking the current thread - local default\n    namespace."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlooking up the model instance for a given Datastore kind.", "response": "def lookup_model_by_kind(kind):\n    \"\"\"Look up the model instance for a given Datastore kind.\n\n    Parameters:\n      kind(str)\n\n    Raises:\n      RuntimeError: If a model for the given kind has not been\n        defined.\n\n    Returns:\n      model: The model class.\n    \"\"\"\n    model = _known_models.get(kind)\n    if model is None:\n        raise RuntimeError(f\"Model for kind {kind!r} not found.\")\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_multi(keys):\n    if not keys:\n        return\n\n    adapter = None\n    for key in keys:\n        if key.is_partial:\n            raise RuntimeError(f\"Key {key!r} is partial.\")\n\n        model = lookup_model_by_kind(key.kind)\n        if adapter is None:\n            adapter = model._adapter\n\n        model.pre_delete_hook(key)\n\n    adapter.delete_multi(keys)\n    for key in keys:\n        # Micro-optimization to avoid calling get_model.  This is OK\n        # to do here because we've already proved that a model for\n        # that kind exists in the previous block.\n        model = _known_models[key.kind]\n        model.post_delete_hook(key)", "response": "Delete a set of entitites from Datastore by their\n    respective keys."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a set of entities from Datastore by their respective keys.", "response": "def get_multi(keys):\n    \"\"\"Get a set of entities from Datastore by their respective keys.\n\n    Note:\n      This uses the adapter that is tied to the first model in the\n      list.  If the keys have disparate adapters this function may\n      behave in unexpected ways.\n\n    Warning:\n      You must pass a **list** and not a generator or some other kind\n      of iterable to this function as it has to iterate over the list\n      of keys multiple times.\n\n    Parameters:\n      keys(list[anom.Key]): The list of keys whose entities to get.\n\n    Raises:\n      RuntimeError: If the given set of keys have models that use\n        a disparate set of adapters or if any of the keys are\n        partial.\n\n    Returns:\n      list[Model]: Entities that do not exist are going to be None\n      in the result list.  The order of results matches the order\n      of the input keys.\n    \"\"\"\n    if not keys:\n        return []\n\n    adapter = None\n    for key in keys:\n        if key.is_partial:\n            raise RuntimeError(f\"Key {key!r} is partial.\")\n\n        model = lookup_model_by_kind(key.kind)\n        if adapter is None:\n            adapter = model._adapter\n\n        model.pre_get_hook(key)\n\n    entities_data, entities = adapter.get_multi(keys), []\n    for key, entity_data in zip(keys, entities_data):\n        if entity_data is None:\n            entities.append(None)\n            continue\n\n        # Micro-optimization to avoid calling get_model.  This is OK\n        # to do here because we've already proved that a model for\n        # that kind exists in the previous block.\n        model = _known_models[key.kind]\n        entity = model._load(key, entity_data)\n        entities.append(entity)\n        entity.post_get_hook()\n\n    return entities"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npersisting a list of entities to the Datastore.", "response": "def put_multi(entities):\n    \"\"\"Persist a set of entities to Datastore.\n\n    Note:\n      This uses the adapter that is tied to the first Entity in the\n      list.  If the entities have disparate adapters this function may\n      behave in unexpected ways.\n\n    Warning:\n      You must pass a **list** and not a generator or some other kind\n      of iterable to this function as it has to iterate over the list\n      of entities multiple times.\n\n    Parameters:\n      entities(list[Model]): The list of entities to persist.\n\n    Raises:\n      RuntimeError: If the given set of models use a disparate set of\n        adapters.\n\n    Returns:\n      list[Model]: The list of persisted entitites.\n    \"\"\"\n    if not entities:\n        return []\n\n    adapter, requests = None, []\n    for entity in entities:\n        if adapter is None:\n            adapter = entity._adapter\n\n        entity.pre_put_hook()\n        requests.append(PutRequest(entity.key, entity.unindexed_properties, entity))\n\n    keys = adapter.put_multi(requests)\n    for key, entity in zip(keys, entities):\n        entity.key = key\n        entity.post_put_hook()\n\n    return entities"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding up a Datastore key from a path.", "response": "def from_path(cls, *path, namespace=None):\n        \"\"\"Build up a Datastore key from a path.\n\n        Parameters:\n          \\*path(tuple[str or int]): The path segments.\n          namespace(str): An optional namespace for the key. This is\n            applied to each key in the tree.\n\n        Returns:\n          anom.Key: The Datastore represented by the given path.\n        \"\"\"\n        parent = None\n        for i in range(0, len(path), 2):\n            parent = cls(*path[i:i + 2], parent=parent, namespace=namespace)\n\n        return parent"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef str_id(self):\n        \"str: This key's string id.\"\n        id_or_name = self.id_or_name\n        if id_or_name is not None and isinstance(id_or_name, str):\n            return id_or_name\n        return None", "response": "str : This key s string id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(self, value):\n        if isinstance(value, self._types):\n            return value\n\n        elif self.optional and value is None:\n            return [] if self.repeated else None\n\n        elif self.repeated and isinstance(value, (tuple, list)) and all(isinstance(x, self._types) for x in value):\n            return value\n\n        else:\n            raise TypeError(f\"Value of type {classname(value)} assigned to {classname(self)} property.\")", "response": "Validates that the value of the property is a valid value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare_to_store(self, entity, value):\n        if value is None and not self.optional:\n            raise RuntimeError(f\"Property {self.name_on_model} requires a value.\")\n        return value", "response": "Prepare value for storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unindexed_properties(self):\n        \"tuple[str]: The names of all the unindexed properties on this entity.\"\n        properties = ()\n        for name, prop in self._properties.items():\n            if isinstance(prop, EmbedLike):\n                embedded_entity = getattr(self, name, None)\n                if embedded_entity:\n                    properties += prop.get_unindexed_properties(embedded_entity)\n\n            elif not prop.indexed or prop.indexed_if and not prop.indexed_if(self, prop, name):\n                properties += (prop.name_on_entity,)\n\n        return properties", "response": "tuple[str]: The names of all the unindexed properties on this entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(cls, id_or_name, *, parent=None, namespace=None):\n        return Key(cls, id_or_name, parent=parent, namespace=namespace).get()", "response": "Get an entity by id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate the given dictionary of keys and set the value of the current application s config variable to the value of the specified type.", "response": "def eval(self, keys):\n        \"\"\"\n        Examples:\n            Specify type literal for key.\n\n            >>> env.eval({MAIL_PORT: int})\n        \"\"\"\n        for k, v in keys.items():  # pylint: disable=invalid-name\n            if k in self.app.config:\n                try:\n                    val = ast.literal_eval(self.app.config[k])\n                    if isinstance(val, v):\n                        if self.verbose_mode:\n                            print(\n                                \" * Casting a specified var as literal:\"\n                                \" {0} => {1}\".format(k, v)\n                            )\n                        self.app.config[k] = val\n                    else:\n                        print(\n                            \" ! Does not match with specified type:\"\n                            \" {0} => {1}\".format(k, v))\n                except (ValueError, SyntaxError):\n                    print(\" ! Could not evaluate as literal type:\"\n                          \" {0} => {1}\".format(k, v))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef alias(self, maps):\n        for k, v in maps.items():  # pylint: disable=invalid-name\n            if self.verbose_mode:\n                print(\n                    \" * Mapping a specified var as a alias:\"\n                    \" {0} -> {1}\".format(v, k))\n            self.app.config[v] = self.app.config[k]", "response": "Make alias var as.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_django_adminopt_node(env, sig, signode):\n    from sphinx.domains.std import option_desc_re\n    count = 0\n    firstname = ''\n    for m in option_desc_re.finditer(sig):\n        optname, args = m.groups()\n        if count:\n            signode += addnodes.desc_addname(', ', ', ')\n        signode += addnodes.desc_name(optname, optname)\n        signode += addnodes.desc_addname(args, args)\n        if not count:\n            firstname = optname\n        count += 1\n    if not count:\n        for m in simple_option_desc_re.finditer(sig):\n            optname, args = m.groups()\n            if count:\n                signode += addnodes.desc_addname(', ', ', ')\n            signode += addnodes.desc_name(optname, optname)\n            signode += addnodes.desc_addname(args, args)\n            if not count:\n                firstname = optname\n            count += 1\n    if not firstname:\n        raise ValueError\n    return firstname", "response": "A copy of sphinx. directives. CmdoptionDesc. parse_signature"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of stop words for a given language", "response": "def get_stop_words(language, cache=True):\n    \"\"\"\n    :type language: basestring\n\n    :rtype: list\n    \"\"\"\n    try:\n        language = LANGUAGE_MAPPING[language]\n    except KeyError:\n        if language not in AVAILABLE_LANGUAGES:\n            raise StopWordError('{0}\" language is unavailable.'.format(\n                language\n            ))\n\n    if cache and language in STOP_WORDS_CACHE:\n        return STOP_WORDS_CACHE[language]\n\n    language_filename = os.path.join(STOP_WORDS_DIR, language + '.txt')\n    try:\n        with open(language_filename, 'rb') as language_file:\n            stop_words = [line.decode('utf-8').strip()\n                          for line in language_file.readlines()]\n            stop_words = apply_filters(stop_words, language)\n    except IOError:\n        raise StopWordError(\n            '{0}\" file is unreadable, check your installation.'.format(\n                language_filename\n            )\n        )\n\n    if cache:\n        STOP_WORDS_CACHE[language] = stop_words\n\n    return stop_words"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_filters(stopwords, language):\n    if language in _filters:\n        for func in _filters[language]:\n            stopwords = func(stopwords)\n\n    for func in _filters[None]:\n        stopwords = func(stopwords, language)\n\n    return stopwords", "response": "Apply registered filters to stopwords"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_filter(func, language=None):\n    if language not in _filters:\n        _filters[language] = []\n    _filters[language].append(func)", "response": "Register a filter function for specific language."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_filter(func, language=None):\n    if not (language in _filters and func in _filters[language]):\n        return False\n    _filters[language].remove(func)\n    return True", "response": "Removes a filter function from the list of filters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_response(self, request, response):\n\n        if hasattr(request, 'COUNTRY_CODE'):\n            response.set_cookie(\n                key=constants.COUNTRY_COOKIE_NAME,\n                value=request.COUNTRY_CODE,\n                max_age=settings.LANGUAGE_COOKIE_AGE,\n                path=settings.LANGUAGE_COOKIE_PATH,\n                domain=settings.LANGUAGE_COOKIE_DOMAIN\n            )\n        return response", "response": "Sets the language cookie for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_option(\n            self, name, value, label, selected, index,\n            subindex=None, attrs=None):\n        \"\"\"Patch to use nicer ids.\"\"\"\n        index = str(index) if subindex is None else \"%s%s%s\" % (\n            index, self.id_separator, subindex)\n        if attrs is None:\n            attrs = {}\n        option_attrs = self.build_attrs(\n            self.attrs, attrs) if self.option_inherits_attrs else {}\n        if selected:\n            option_attrs.update(self.checked_attribute)\n        if 'id' in option_attrs:\n            if self.use_nice_ids:\n                option_attrs['id'] = \"%s%s%s\" % (\n                    option_attrs['id'],\n                    self.id_separator,\n                    slugify(label.lower())\n                    )\n            else:\n                option_attrs['id'] = self.id_for_label(\n                    option_attrs['id'], index)\n        return {\n            'name': name,\n            'value': value,\n            'label': label,\n            'selected': selected,\n            'index': index,\n            'attrs': option_attrs,\n            'type': self.input_type,\n            'template_name': self.option_template_name,\n            'wrap_label': True,\n            }", "response": "Patch to use nicer ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef current_version():\n    filepath = os.path.abspath(\n        project_root / \"directory_components\" / \"version.py\")\n    version_py = get_file_string(filepath)\n    regex = re.compile(Utils.get_version)\n    if regex.search(version_py) is not None:\n        current_version = regex.search(version_py).group(0)\n        print(color(\n            \"Current directory-components version: {}\".format(current_version),\n            fg='blue', style='bold'))\n        get_update_info()\n    else:\n        print(color(\n            'Error finding directory-components version.',\n            fg='red', style='bold'))", "response": "Get current version of directory - components."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_file_string(filepath):\n    with open(os.path.abspath(filepath)) as f:\n        return f.read()", "response": "Get string from file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef replace_in_dirs(version):\n    print(color(\n        \"Upgrading directory-components dependency in all repos...\",\n        fg='blue', style='bold'))\n    for dirname in Utils.dirs:\n        replace = \"directory-components=={}\".format(version)\n        replace_in_files(dirname, replace)\n    done(version)", "response": "Look through dirs and run replace_in_files in each."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplacing current version with new version in requirements. in files.", "response": "def replace_in_files(dirname, replace):\n    \"\"\"Replace current version with new version in requirements files.\"\"\"\n    filepath = os.path.abspath(dirname / \"requirements.in\")\n    if os.path.isfile(filepath) and header_footer_exists(filepath):\n        replaced = re.sub(Utils.exp, replace, get_file_string(filepath))\n        with open(filepath, \"w\") as f:\n            f.write(replaced)\n        print(color(\n            \"Written to file: {}\".format(filepath),\n            fg='magenta', style='bold'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef header_footer_exists(filepath):\n    with open(filepath) as f:\n        return re.search(Utils.exp, f.read())", "response": "Check if the header - footer file exists in requirements files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef abut (source,*args):\n\n    if type(source) not in [ListType,TupleType]:\n        source = [source]\n    for addon in args:\n        if type(addon) not in [ListType,TupleType]:\n            addon = [addon]\n        if len(addon) < len(source):                # is source list longer?\n            if len(source) % len(addon) == 0:        # are they integer multiples?\n                repeats = len(source)/len(addon)    # repeat addon n times\n                origadd = copy.deepcopy(addon)\n                for i in range(repeats-1):\n                    addon = addon + origadd\n            else:\n                repeats = len(source)/len(addon)+1  # repeat addon x times,\n                origadd = copy.deepcopy(addon)      #    x is NOT an integer\n                for i in range(repeats-1):\n                    addon = addon + origadd\n                    addon = addon[0:len(source)]\n        elif len(source) < len(addon):                # is addon list longer?\n            if len(addon) % len(source) == 0:        # are they integer multiples?\n                repeats = len(addon)/len(source)    # repeat source n times\n                origsour = copy.deepcopy(source)\n                for i in range(repeats-1):\n                    source = source + origsour\n            else:\n                repeats = len(addon)/len(source)+1  # repeat source x times,\n                origsour = copy.deepcopy(source)    #   x is NOT an integer\n                for i in range(repeats-1):\n                    source = source + origsour\n                source = source[0:len(addon)]\n\n        source = simpleabut(source,addon)\n    return source", "response": "This command is used to combine two lists of lists as long as the longest list is at least as long as the longest list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconcatenate two lists as columns and returns the result. '2D' lists are also accomodated for either argument (source or addon). This DOES NOT repeat either list to make the 2 lists of equal length. Beware of list pairs with different lengths ... the resulting list will be the length of the FIRST list passed. Usage: simpleabut(source,addon) where source, addon=list (or list-of-lists) Returns: a list of lists as long as source, with source on the 'left' and addon on the 'right'", "response": "def simpleabut (source, addon):\n    \"\"\"\nConcatenates two lists as columns and returns the result.  '2D' lists\nare also accomodated for either argument (source or addon).  This DOES NOT\nrepeat either list to make the 2 lists of equal length.  Beware of list pairs\nwith different lengths ... the resulting list will be the length of the\nFIRST list passed.\n\nUsage:   simpleabut(source,addon)  where source, addon=list (or list-of-lists)\nReturns: a list of lists as long as source, with source on the 'left' and\n                 addon on the 'right'\n\"\"\"\n    if type(source) not in [ListType,TupleType]:\n        source = [source]\n    if type(addon) not in [ListType,TupleType]:\n        addon = [addon]\n    minlen = min(len(source),len(addon))\n    list = copy.deepcopy(source)                # start abut process\n    if type(source[0]) not in [ListType,TupleType]:\n        if type(addon[0]) not in [ListType,TupleType]:\n            for i in range(minlen):\n                list[i] = [source[i]] + [addon[i]]        # source/addon = column\n        else:\n            for i in range(minlen):\n                list[i] = [source[i]] + addon[i]        # addon=list-of-lists\n    else:\n        if type(addon[0]) not in [ListType,TupleType]:\n            for i in range(minlen):\n                list[i] = source[i] + [addon[i]]        # source=list-of-lists\n        else:\n            for i in range(minlen):\n                list[i] = source[i] + addon[i]        # source/addon = list-of-lists\n    source = list\n    return source"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef colex (listoflists,cnums):\n    global index\n    column = 0\n    if type(cnums) in [ListType,TupleType]:   # if multiple columns to get\n        index = cnums[0]\n        column = [x[index] for x in listoflists]\n        for col in cnums[1:]:\n            index = col\n            column = abut(column,[x[index] for x in listoflists])\n    elif type(cnums) == StringType:              # if an 'x[3:]' type expr.\n        evalstring = 'map(lambda x: x'+cnums+', listoflists)'\n        column = eval(evalstring)\n    else:                                     # else it's just 1 col to get\n        index = cnums\n        column = [x[index] for x in listoflists]\n    return column", "response": "This function returns a list of lists corresponding to the columns specified by cnums."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollapse a list of lists with all unique items appearing in a column.", "response": "def collapse (listoflists,keepcols,collapsecols,fcn1=None,fcn2=None,cfcn=None):\n     \"\"\"\nAverages data in collapsecol, keeping all unique items in keepcols\n(using unique, which keeps unique LISTS of column numbers), retaining the\nunique sets of values in keepcols, the mean for each.  Setting fcn1\nand/or fcn2 to point to a function rather than None (e.g., stats.sterr, len)\nwill append those results (e.g., the sterr, N) after each calculated mean.\ncfcn is the collapse function to apply (defaults to mean, defined here in the\npstat module to avoid circular imports with stats.py, but harmonicmean or\nothers could be passed).\n\nUsage:    collapse (listoflists,keepcols,collapsecols,fcn1=None,fcn2=None,cfcn=None)\nReturns: a list of lists with all unique permutations of entries appearing in\n     columns (\"conditions\") specified by keepcols, abutted with the result of\n     cfcn (if cfcn=None, defaults to the mean) of each column specified by\n     collapsecols.\n\"\"\"\n     def collmean (inlist):\n         s = 0\n         for item in inlist:\n             s = s + item\n         return s/float(len(inlist))\n\n     if type(keepcols) not in [ListType,TupleType]:\n         keepcols = [keepcols]\n     if type(collapsecols) not in [ListType,TupleType]:\n         collapsecols = [collapsecols]\n     if cfcn == None:\n         cfcn = collmean\n     if keepcols == []:\n         means = [0]*len(collapsecols)\n         for i in range(len(collapsecols)):\n             avgcol = colex(listoflists,collapsecols[i])\n             means[i] = cfcn(avgcol)\n             if fcn1:\n                 try:\n                     test = fcn1(avgcol)\n                 except:\n                     test = 'N/A'\n                     means[i] = [means[i], test]\n             if fcn2:\n                 try:\n                     test = fcn2(avgcol)\n                 except:\n                     test = 'N/A'\n                 try:\n                     means[i] = means[i] + [len(avgcol)]\n                 except TypeError:\n                     means[i] = [means[i],len(avgcol)]\n         return means\n     else:\n         values = colex(listoflists,keepcols)\n         uniques = unique(values)\n         uniques.sort()\n         newlist = []\n         if type(keepcols) not in [ListType,TupleType]:  keepcols = [keepcols]\n         for item in uniques:\n             if type(item) not in [ListType,TupleType]:  item =[item]\n             tmprows = linexand(listoflists,keepcols,item)\n             for col in collapsecols:\n                 avgcol = colex(tmprows,col)\n                 item.append(cfcn(avgcol))\n                 if fcn1 != None:\n                     try:\n                         test = fcn1(avgcol)\n                     except:\n                         test = 'N/A'\n                     item.append(test)\n                 if fcn2 != None:\n                     try:\n                         test = fcn2(avgcol)\n                     except:\n                         test = 'N/A'\n                     item.append(test)\n                 newlist.append(item)\n         return newlist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flat(l):\n    newl = []\n    for i in range(len(l)):\n        for j in range(len(l[i])):\n            newl.append(l[i][j])\n    return newl", "response": "Returns the flattened version of a 2D list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the rows of a list of lists where columnlist = valuelist and columnlist = columnlist.", "response": "def linexand (listoflists,columnlist,valuelist):\n    \"\"\"\nReturns the rows of a list of lists where col (from columnlist) = val\n(from valuelist) for EVERY pair of values (columnlist[i],valuelists[i]).\nlen(columnlist) must equal len(valuelist).\n\nUsage:   linexand (listoflists,columnlist,valuelist)\nReturns: the rows of listoflists where columnlist[i]=valuelist[i] for ALL i\n\"\"\"\n    if type(columnlist) not in [ListType,TupleType]:\n        columnlist = [columnlist]\n    if type(valuelist) not in [ListType,TupleType]:\n        valuelist = [valuelist]\n    criterion = ''\n    for i in range(len(columnlist)):\n        if type(valuelist[i])==StringType:\n            critval = '\\'' + valuelist[i] + '\\''\n        else:\n            critval = str(valuelist[i])\n        criterion = criterion + ' x['+str(columnlist[i])+']=='+critval+' and'\n    criterion = criterion[0:-3]         # remove the \"and\" after the last crit\n    function = 'filter(lambda x: '+criterion+',listoflists)'\n    lines = eval(function)\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef linedelimited (inlist,delimiter):\n    outstr = ''\n    for item in inlist:\n        if type(item) != StringType:\n            item = str(item)\n        outstr = outstr + item + delimiter\n    outstr = outstr[0:-1]\n    return outstr", "response": "Returns a string composed of elements in inlist with each element\nseparated by delimiter. Used by function writedelimited."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string composed of elements in inlist with each element right - aligned in columns of colsize.", "response": "def lineincols (inlist,colsize):\n    \"\"\"\nReturns a string composed of elements in inlist, with each element\nright-aligned in columns of (fixed) colsize.\n\nUsage:   lineincols (inlist,colsize)   where colsize is an integer\n\"\"\"\n    outstr = ''\n    for item in inlist:\n        if type(item) != StringType:\n            item = str(item)\n        size = len(item)\n        if size <= colsize:\n            for i in range(colsize-size):\n                outstr = outstr + ' '\n            outstr = outstr + item\n        else:\n            outstr = outstr + item[0:colsize+1]\n    return outstr"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lineincustcols (inlist,colsizes):\n    outstr = ''\n    for i in range(len(inlist)):\n        if type(inlist[i]) != StringType:\n            item = str(inlist[i])\n        else:\n            item = inlist[i]\n        size = len(item)\n        if size <= colsizes[i]:\n            for j in range(colsizes[i]-size):\n                outstr = outstr + ' '\n            outstr = outstr + item\n        else:\n            outstr = outstr + item[0:colsizes[i]+1]\n    return outstr", "response": "This function returns a string composed of elements in inlist with each element\nright - aligned in a column of width specified by a sequence colsizes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a 1D list to a single long string for file output, using the string.join function. Usage: list2string (inlist,delimit=' ') Returns: the string created from inlist", "response": "def list2string (inlist,delimit=' '):\n    \"\"\"\nConverts a 1D list to a single long string for file output, using\nthe string.join function.\n\nUsage:   list2string (inlist,delimit=' ')\nReturns: the string created from inlist\n\"\"\"\n    stringlist = [makestr(_) for _ in inlist]\n    return string.join(stringlist,delimit)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting a list of lists in columns customized by the max size of items in col plus extra number of spaces.", "response": "def printcc (lst,extra=2):\n    \"\"\"\nPrints a list of lists in columns, customized by the max size of items\nwithin the columns (max size of items in col, plus 'extra' number of spaces).\nUse 'dashes' or '\\\\n' in the list-of-lists to print dashes or blank lines,\nrespectively.\n\nUsage:   printcc (lst,extra=2)\nReturns: None\n\"\"\"\n    if type(lst[0]) not in [ListType,TupleType]:\n        lst = [lst]\n    rowstokill = []\n    list2print = copy.deepcopy(lst)\n    for i in range(len(lst)):\n        if lst[i] == ['\\n'] or lst[i]=='\\n' or lst[i]=='dashes' or lst[i]=='' or lst[i]==['']:\n            rowstokill = rowstokill + [i]\n    rowstokill.reverse()   # delete blank rows from the end\n    for row in rowstokill:\n        del list2print[row]\n    maxsize = [0]*len(list2print[0])\n    for col in range(len(list2print[0])):\n        items = colex(list2print,col)\n        items = [makestr(_) for _ in items]\n        maxsize[col] = max(map(len, items)) + extra\n    for row in lst:\n        if row == ['\\n'] or row == '\\n' or row == '' or row == ['']:\n            print()\n        elif row == ['dashes'] or row == 'dashes':\n            dashes = [0]*len(maxsize)\n            for j in range(len(maxsize)):\n                dashes[j] = '-'*(maxsize[j]-2)\n            print(lineincustcols(dashes,maxsize))\n        else:\n            print(lineincustcols(row,maxsize))\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pl (listoflists):\n    for row in listoflists:\n        if row[-1] == '\\n':\n            print(row, end=' ')\n        else:\n            print(row)\n    return None", "response": "A function that prints a list of lists 1 list at a time."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replace (inlst,oldval,newval):\n    lst = inlst*1\n    for i in range(len(lst)):\n        if type(lst[i]) not in [ListType,TupleType]:\n            if lst[i]==oldval: lst[i]=newval\n        else:\n            lst[i] = replace(lst[i],oldval,newval)\n    return lst", "response": "Replaces all occurrences of oldval with newval recursively."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recode (inlist,listmap,cols=None):\n    lst = copy.deepcopy(inlist)\n    if cols != None:\n        if type(cols) not in [ListType,TupleType]:\n            cols = [cols]\n        for col in cols:\n            for row in range(len(lst)):\n                try:\n                    idx = colex(listmap,0).index(lst[row][col])\n                    lst[row][col] = listmap[idx][1]\n                except ValueError:\n                    pass\n    else:\n        for row in range(len(lst)):\n            for col in range(len(lst)):\n                try:\n                    idx = colex(listmap,0).index(lst[row][col])\n                    lst[row][col] = listmap[idx][1]\n                except ValueError:\n                    pass\n    return lst", "response": "recode the values in a list to a new set of values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngoing through each element in a 1D or 2D inlist, and applies the following function to all elements of FloatType ... round(element,digits). Usage: roundlist(inlist,digits) Returns: list with rounded floats", "response": "def roundlist (inlist,digits):\n    \"\"\"\nGoes through each element in a 1D or 2D inlist, and applies the following\nfunction to all elements of FloatType ... round(element,digits).\n\nUsage:   roundlist(inlist,digits)\nReturns: list with rounded floats\n\"\"\"\n    if type(inlist[0]) in [IntType, FloatType]:\n        inlist = [inlist]\n    l = inlist*1\n    for i in range(len(l)):\n        for j in range(len(l[i])):\n            if type(l[i][j])==FloatType:\n                l[i][j] = round(l[i][j],digits)\n    return l"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsorting a list of lists on the column specified in the sequence sortcols.", "response": "def sortby(listoflists,sortcols):\n    \"\"\"\nSorts a list of lists on the column(s) specified in the sequence\nsortcols.\n\nUsage:   sortby(listoflists,sortcols)\nReturns: sorted list, unchanged column ordering\n\"\"\"\n    newlist = abut(colex(listoflists,sortcols),listoflists)\n    newlist.sort()\n    try:\n        numcols = len(sortcols)\n    except TypeError:\n        numcols = 1\n    crit = '[' + str(numcols) + ':]'\n    newlist = colex(newlist,crit)\n    return newlist"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unique (inlist):\n    uniques = []\n    for item in inlist:\n        if item not in uniques:\n            uniques.append(item)\n    return uniques", "response": "Returns all unique items in the passed list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef duplicates(inlist):\n    dups = []\n    for i in range(len(inlist)):\n        if inlist[i] in inlist[i+1:]:\n            dups.append(inlist[i])\n    return dups", "response": "Returns duplicate items in the FIRST dimension of the passed list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nonrepeats(inlist):\n    nonrepeats = []\n    for i in range(len(inlist)):\n        if inlist.count(inlist[i]) == 1:\n            nonrepeats.append(inlist[i])\n    return nonrepeats", "response": "Returns items that are NOT duplicated in the first dim of the passed list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the geometric mean of the values in the passed list.", "response": "def lgeometricmean (inlist):\n    \"\"\"\nCalculates the geometric mean of the values in the passed list.\nThat is:  n-th root of (x1 * x2 * ... * xn).  Assumes a '1D' list.\n\nUsage:   lgeometricmean(inlist)\n\"\"\"\n    mult = 1.0\n    one_over_n = 1.0/len(inlist)\n    for item in inlist:\n        mult = mult * pow(item,one_over_n)\n    return mult"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lharmonicmean (inlist):\n    sum = 0\n    for item in inlist:\n        sum = sum + 1.0/item\n    return len(inlist) / sum", "response": "Calculates the harmonic mean of the values in the passed list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the arithematic mean of the values in the passed list.", "response": "def lmean (inlist):\n    \"\"\"\nReturns the arithematic mean of the values in the passed list.\nAssumes a '1D' list, but will function on the 1st dim of an array(!).\n\nUsage:   lmean(inlist)\n\"\"\"\n    sum = 0\n    for item in inlist:\n        sum = sum + item\n    return sum/float(len(inlist))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lmedian (inlist,numbins=1000):\n    (hist, smallest, binsize, extras) = histogram(inlist,numbins) # make histog\n    cumhist = cumsum(hist)              # make cumulative histogram\n    for i in range(len(cumhist)):        # get 1st(!) index holding 50%ile score\n        if cumhist[i]>=len(inlist)/2.0:\n            cfbin = i\n            break\n    LRL = smallest + binsize*cfbin        # get lower read limit of that bin\n    cfbelow = cumhist[cfbin-1]\n    freq = float(hist[cfbin])                # frequency IN the 50%ile bin\n    median = LRL + ((len(inlist)/2.0 - cfbelow)/float(freq))*binsize  # median formula\n    return median", "response": "Returns the computed median value of a list of numbers given the number of bins to use for the histogram."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef litemfreq(inlist):\n    scores = pstat.unique(inlist)\n    scores.sort()\n    freq = []\n    for item in scores:\n        freq.append(inlist.count(item))\n    return pstat.abut(scores, freq)", "response": "Returns a list of pairs. Each pair consists of one of the scores in inlist\nand it s frequency count."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn (i) a list of histogram bin counts, (ii) the smallest value of the histogram binning, and (iii) the bin width (the last 2 are not necessarily integers). Default number of bins is 10. If no sequence object is given for defaultreallimits, the routine picks (usually non-pretty) bins spanning all the numbers in the inlist. Usage: lhistogram (inlist, numbins=10, defaultreallimits=None,suppressoutput=0) Returns: list of bin values, lowerreallimit, binsize, extrapoints", "response": "def lhistogram (inlist,numbins=10,defaultreallimits=None,printextras=0):\n    \"\"\"\nReturns (i) a list of histogram bin counts, (ii) the smallest value\nof the histogram binning, and (iii) the bin width (the last 2 are not\nnecessarily integers).  Default number of bins is 10.  If no sequence object\nis given for defaultreallimits, the routine picks (usually non-pretty) bins\nspanning all the numbers in the inlist.\n\nUsage:   lhistogram (inlist, numbins=10, defaultreallimits=None,suppressoutput=0)\nReturns: list of bin values, lowerreallimit, binsize, extrapoints\n\"\"\"\n    if (defaultreallimits != None):\n        if type(defaultreallimits) not in [ListType,TupleType] or len(defaultreallimits)==1: # only one limit given, assumed to be lower one & upper is calc'd\n            lowerreallimit = defaultreallimits\n            upperreallimit = 1.0001 * max(inlist)\n        else: # assume both limits given\n            lowerreallimit = defaultreallimits[0]\n            upperreallimit = defaultreallimits[1]\n        binsize = (upperreallimit-lowerreallimit)/float(numbins)\n    else:     # no limits given for histogram, both must be calc'd\n        estbinwidth=(max(inlist)-min(inlist))/float(numbins) + 1 # 1=>cover all\n        binsize = ((max(inlist)-min(inlist)+estbinwidth))/float(numbins)\n        lowerreallimit = min(inlist) - binsize/2 #lower real limit,1st bin\n    bins = [0]*(numbins)\n    extrapoints = 0\n    for num in inlist:\n        try:\n            if (num-lowerreallimit) < 0:\n                extrapoints = extrapoints + 1\n            else:\n                bintoincrement = int((num-lowerreallimit)/float(binsize))\n                bins[bintoincrement] = bins[bintoincrement] + 1\n        except:\n            extrapoints = extrapoints + 1\n    if (extrapoints > 0 and printextras == 1):\n        print('\\nPoints outside given histogram range =',extrapoints)\n    return (bins, lowerreallimit, binsize, extrapoints)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lsem (inlist):\n    sd = stdev(inlist)\n    n = len(inlist)\n    return sd/math.sqrt(n)", "response": "Returns the estimated standard error of the mean ( sx - bar of the the\nvalues in the passed list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lz (inlist, score):\n    z = (score-mean(inlist))/samplestdev(inlist)\n    return z", "response": "Returns the z - score for a given input score given that score and the input list in which that score came."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lzs (inlist):\n    zscores = []\n    for item in inlist:\n        zscores.append(z(inlist,item))\n    return zscores", "response": "Returns a list of z - scores one for each item in the passed list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ltrimboth (l,proportiontocut):\n    lowercut = int(proportiontocut*len(l))\n    uppercut = len(l) - lowercut\n    return l[lowercut:uppercut]", "response": "Returns a list of items from BOTH ends of a passed sequence l with the given proportion of items from the given sequence l."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of items from ONE end of the passed n - item list l.", "response": "def ltrim1 (l,proportiontocut,tail='right'):\n    \"\"\"\nSlices off the passed proportion of items from ONE end of the passed\nlist (i.e., if proportiontocut=0.1, slices off 'leftmost' or 'rightmost'\n10% of scores).  Slices off LESS if proportion results in a non-integer\nslice index (i.e., conservatively slices off proportiontocut).\n\nUsage:   ltrim1 (l,proportiontocut,tail='right')  or set tail='left'\nReturns: trimmed version of list l\n\"\"\"\n    if tail == 'right':\n        lowercut = 0\n        uppercut = len(l) - int(proportiontocut*len(l))\n    elif tail == 'left':\n        lowercut = int(proportiontocut*len(l))\n        uppercut = len(l)\n    return l[lowercut:uppercut]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates a Pearson correlation coefficient and the associated probability value.", "response": "def lpearsonr(x,y):\n    \"\"\"\nCalculates a Pearson correlation coefficient and the associated\nprobability value.  Taken from Heiman's Basic Statistics for the Behav.\nSci (2nd), p.195.\n\nUsage:   lpearsonr(x,y)      where x and y are equal-length lists\nReturns: Pearson's r value, two-tailed p-value\n\"\"\"\n    TINY = 1.0e-30\n    if len(x) != len(y):\n        raise ValueError('Input values not paired in pearsonr.  Aborting.')\n    n = len(x)\n    x = [float(_) for _ in x]\n    y = [float(_) for _ in y]\n    xmean = mean(x)\n    ymean = mean(y)\n    r_num = n*(summult(x,y)) - sum(x)*sum(y)\n    r_den = math.sqrt((n*ss(x) - square_of_sums(x))*(n*ss(y)-square_of_sums(y)))\n    r = (r_num / r_den)  # denominator already a float\n    df = n-2\n    t = r*math.sqrt(df/((1.0-r+TINY)*(1.0+r+TINY)))\n    prob = betai(0.5*df,0.5,df/float(df+t*t))\n    return r, prob"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating a Spearman rank-order correlation coefficient. Taken from Heiman's Basic Statistics for the Behav. Sci (1st), p.192. Usage: lspearmanr(x,y) where x and y are equal-length lists Returns: Spearman's r, two-tailed p-value", "response": "def lspearmanr(x,y):\n    \"\"\"\nCalculates a Spearman rank-order correlation coefficient.  Taken\nfrom Heiman's Basic Statistics for the Behav. Sci (1st), p.192.\n\nUsage:   lspearmanr(x,y)      where x and y are equal-length lists\nReturns: Spearman's r, two-tailed p-value\n\"\"\"\n    TINY = 1e-30\n    if len(x) != len(y):\n        raise ValueError('Input values not paired in spearmanr.  Aborting.')\n    n = len(x)\n    rankx = rankdata(x)\n    ranky = rankdata(y)\n    dsq = sumdiffsquared(rankx,ranky)\n    rs = 1 - 6*dsq / float(n*(n**2-1))\n    t = rs * math.sqrt((n-2) / ((rs+1.0)*(1.0-rs)))\n    df = n-2\n    probrs = betai(0.5*df,0.5,df/(df+t*t))  # t already a float\n# probability values for rs are from part 2 of the spearman function in\n# Numerical Recipies, p.510.  They are close to tables, but not exact. (?)\n    return rs, probrs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lpointbiserialr(x,y):\n    TINY = 1e-30\n    if len(x) != len(y):\n        raise ValueError('INPUT VALUES NOT PAIRED IN pointbiserialr.  ABORTING.')\n    data = pstat.abut(x,y)\n    categories = pstat.unique(x)\n    if len(categories) != 2:\n        raise ValueError(\"Exactly 2 categories required for pointbiserialr().\")\n    else:   # there are 2 categories, continue\n        codemap = pstat.abut(categories,range(2))\n        recoded = pstat.recode(data,codemap,0)\n        x = pstat.linexand(data,0,categories[0])\n        y = pstat.linexand(data,0,categories[1])\n        xmean = mean(pstat.colex(x,1))\n        ymean = mean(pstat.colex(y,1))\n        n = len(data)\n        adjust = math.sqrt((len(x)/float(n))*(len(y)/float(n)))\n        rpb = (ymean - xmean)/samplestdev(pstat.colex(data,1))*adjust\n        df = n-2\n        t = rpb*math.sqrt(df/((1.0-rpb+TINY)*(1.0+rpb+TINY)))\n        prob = betai(0.5*df,0.5,df/(df+t*t))  # t already a float\n        return rpb, prob", "response": "Calculates a point - biserial correlation coefficient and the associated\nprobability value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lkendalltau(x,y):\n    n1 = 0\n    n2 = 0\n    iss = 0\n    for j in range(len(x)-1):\n        for k in range(j,len(y)):\n            a1 = x[j] - x[k]\n            a2 = y[j] - y[k]\n            aa = a1 * a2\n            if (aa):             # neither list has a tie\n                n1 = n1 + 1\n                n2 = n2 + 1\n                if aa > 0:\n                    iss = iss + 1\n                else:\n                    iss = iss -1\n            else:\n                if (a1):\n                    n1 = n1 + 1\n                else:\n                    n2 = n2 + 1\n    tau = iss / math.sqrt(n1*n2)\n    svar = (4.0*len(x)+10.0) / (9.0*len(x)*(len(x)-1))\n    z = tau / math.sqrt(svar)\n    prob = erfcc(abs(z)/1.4142136)\n    return tau, prob", "response": "Calculates the Kendall s tau... correlation of ordinal data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates a regression line on x,y pairs. Usage: llinregress(x,y) x,y are equal-length lists of x-y coordinates Returns: slope, intercept, r, two-tailed prob, sterr-of-estimate", "response": "def llinregress(x,y):\n    \"\"\"\nCalculates a regression line on x,y pairs.  \n\nUsage:   llinregress(x,y)      x,y are equal-length lists of x-y coordinates\nReturns: slope, intercept, r, two-tailed prob, sterr-of-estimate\n\"\"\"\n    TINY = 1.0e-20\n    if len(x) != len(y):\n        raise ValueError('Input values not paired in linregress.  Aborting.')\n    n = len(x)\n    x = [float(_) for _ in x]\n    y = [float(_) for _ in y]\n    xmean = mean(x)\n    ymean = mean(y)\n    r_num = float(n*(summult(x,y)) - sum(x)*sum(y))\n    r_den = math.sqrt((n*ss(x) - square_of_sums(x))*(n*ss(y)-square_of_sums(y)))\n    r = r_num / r_den\n    z = 0.5*math.log((1.0+r+TINY)/(1.0-r+TINY))\n    df = n-2\n    t = r*math.sqrt(df/((1.0-r+TINY)*(1.0+r+TINY)))\n    prob = betai(0.5*df,0.5,df/(df+t*t))\n    slope = r_num / float(n*ss(x) - square_of_sums(x))\n    intercept = ymean - slope*xmean\n    sterrest = math.sqrt(1-r*r)*samplestdev(y)\n    return slope, intercept, r, prob, sterrest"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions that calculates a one - way chi square for list of observed frequencies and returns the result.", "response": "def lchisquare(f_obs,f_exp=None):\n    \"\"\"\nCalculates a one-way chi square for list of observed frequencies and returns\nthe result.  If no expected frequencies are given, the total N is assumed to\nbe equally distributed across all groups.\n\nUsage:   lchisquare(f_obs, f_exp=None)   f_obs = list of observed cell freq.\nReturns: chisquare-statistic, associated p-value\n\"\"\"\n    k = len(f_obs)                 # number of groups\n    if f_exp == None:\n        f_exp = [sum(f_obs)/float(k)] * len(f_obs) # create k bins with = freq.\n    chisq = 0\n    for i in range(len(f_obs)):\n        chisq = chisq + (f_obs[i]-f_exp[i])**2 / float(f_exp[i])\n    return chisq, chisqprob(chisq, k-1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lks_2samp (data1,data2):\n    j1 = 0\n    j2 = 0\n    fn1 = 0.0\n    fn2 = 0.0\n    n1 = len(data1)\n    n2 = len(data2)\n    en1 = n1\n    en2 = n2\n    d = 0.0\n    data1.sort()\n    data2.sort()\n    while j1 < n1 and j2 < n2:\n        d1=data1[j1]\n        d2=data2[j2]\n        if d1 <= d2:\n            fn1 = (j1)/float(en1)\n            j1 = j1 + 1\n        if d2 <= d1:\n            fn2 = (j2)/float(en2)\n            j2 = j2 + 1\n        dt = (fn2-fn1)\n        if math.fabs(dt) > math.fabs(d):\n            d = dt\n    try:\n        en = math.sqrt(en1*en2/float(en1+en2))\n        prob = ksprob((en+0.12+0.11/en)*abs(d))\n    except:\n        prob = 1.0\n    return d, prob", "response": "A function that computes the Kolmogorov - Smirnof statistic on 2 samples."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates a Mann - Whitney U statistic on the provided scores and returns the result.", "response": "def lmannwhitneyu(x,y):\n    \"\"\"\nCalculates a Mann-Whitney U statistic on the provided scores and\nreturns the result.  Use only when the n in each condition is < 20 and\nyou have 2 independent samples of ranks.  NOTE: Mann-Whitney U is\nsignificant if the u-obtained is LESS THAN or equal to the critical\nvalue of U found in the tables.  Equivalent to Kruskal-Wallis H with\njust 2 groups.\n\nUsage:   lmannwhitneyu(data)\nReturns: u-statistic, one-tailed p-value (i.e., p(z(U)))\n\"\"\"\n    n1 = len(x)\n    n2 = len(y)\n    ranked = rankdata(x+y)\n    rankx = ranked[0:n1]       # get the x-ranks\n    ranky = ranked[n1:]        # the rest are y-ranks\n    u1 = n1*n2 + (n1*(n1+1))/2.0 - sum(rankx)  # calc U for x\n    u2 = n1*n2 - u1                            # remainder is U for y\n    bigu = max(u1,u2)\n    smallu = min(u1,u2)\n    T = math.sqrt(tiecorrect(ranked))  # correction factor for tied scores\n    if T == 0:\n        raise ValueError('All numbers are identical in lmannwhitneyu')\n    sd = math.sqrt(T*n1*n2*(n1+n2+1)/12.0)\n    z = abs((bigu-n1*n2/2.0) / sd)  # normal approximation for prob calc\n    return smallu, 1.0 - zprob(z)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ltiecorrect(rankvals):\n    sorted,posn = shellsort(rankvals)\n    n = len(sorted)\n    T = 0.0\n    i = 0\n    while (i<n-1):\n        if sorted[i] == sorted[i+1]:\n            nties = 1\n            while (i<n-1) and (sorted[i] == sorted[i+1]):\n                nties = nties +1\n                i = i +1\n            T = T + nties**3 - nties\n        i = i+1\n    T = T / float(n**3-n)\n    return 1.0 - T", "response": "This function calculates the T correction factor for the current state of the A - T object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the rank sums statistic on the provided scores and returns the result. Use only when the n in each condition is > 20 and you have 2 independent samples of ranks. Usage: lranksums(x,y) Returns: a z-statistic, two-tailed p-value", "response": "def lranksums(x,y):\n    \"\"\"\nCalculates the rank sums statistic on the provided scores and\nreturns the result.  Use only when the n in each condition is > 20 and you\nhave 2 independent samples of ranks.\n\nUsage:   lranksums(x,y)\nReturns: a z-statistic, two-tailed p-value\n\"\"\"\n    n1 = len(x)\n    n2 = len(y)\n    alldata = x+y\n    ranked = rankdata(alldata)\n    x = ranked[:n1]\n    y = ranked[n1:]\n    s = sum(x)\n    expected = n1*(n1+n2+1) / 2.0\n    z = (s - expected) / math.sqrt(n1*n2*(n1+n2+1)/12.0)\n    prob = 2*(1.0 -zprob(abs(z)))\n    return z, prob"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lkruskalwallish(*args):\n    args = list(args)\n    n = [0]*len(args)\n    all = []\n    n = [len(_) for _ in args]\n    for i in range(len(args)):\n        all = all + args[i]\n    ranked = rankdata(all)\n    T = tiecorrect(ranked)\n    for i in range(len(args)):\n        args[i] = ranked[0:n[i]]\n        del ranked[0:n[i]]\n    rsums = []\n    for i in range(len(args)):\n        rsums.append(sum(args[i])**2)\n        rsums[i] = rsums[i] / float(n[i])\n    ssbn = sum(rsums)\n    totaln = sum(n)\n    h = 12.0 / (totaln*(totaln+1)) * ssbn - 3*(totaln+1)\n    df = len(args) - 1\n    if T == 0:\n        raise ValueError('All numbers are identical in lkruskalwallish')\n    h = h / float(T)\n    return h, chisqprob(h,df)", "response": "Function lkruskalwallish This function calculates the Kruskal - Wallis H - test for 3 or more independent samples\nand returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lfriedmanchisquare(*args):\n    k = len(args)\n    if k < 3:\n        raise ValueError('Less than 3 levels.  Friedman test not appropriate.')\n    n = len(args[0])\n    data = pstat.abut(*tuple(args))\n    for i in range(len(data)):\n        data[i] = rankdata(data[i])\n    ssbn = 0\n    for i in range(k):\n        ssbn = ssbn + sum(args[i])**2\n    chisq = 12.0 / (k*n*(k+1)) * ssbn - 3*n*(k+1)\n    return chisq, chisqprob(chisq,k-1)", "response": "This function calculates the Friedman Chi - Square test for repeated\nmeasures and returns the result along with the associated probability\nvalue."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lchisqprob(chisq,df):\n    BIG = 20.0\n    def ex(x):\n        BIG = 20.0\n        if x < -BIG:\n            return 0.0\n        else:\n            return math.exp(x)\n\n    if chisq <=0 or df < 1:\n        return 1.0\n    a = 0.5 * chisq\n    if df%2 == 0:\n        even = 1\n    else:\n        even = 0\n    if df > 1:\n        y = ex(-a)\n    if even:\n        s = y\n    else:\n        s = 2.0 * zprob(-math.sqrt(chisq))\n    if (df > 2):\n        chisq = 0.5 * (df - 1.0)\n        if even:\n            z = 1.0\n        else:\n            z = 0.5\n        if a > BIG:\n            if even:\n                e = 0.0\n            else:\n                e = math.log(math.sqrt(math.pi))\n            c = math.log(a)\n            while (z <= chisq):\n                e = math.log(z) + e\n                s = s + ex(c*z-a-e)\n                z = z + 1.0\n            return s\n        else:\n            if even:\n                e = 1.0\n            else:\n                e = 1.0 / math.sqrt(math.pi) / math.sqrt(a)\n            c = 0.0\n            while (z <= chisq):\n                e = e * (a/float(z))\n                c = c + e\n                z = z + 1.0\n            return (c*y+s)\n    else:\n        return s", "response": "Returns the probability value associated with the provided chisq and df."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lerfcc(x):\n    z = abs(x)\n    t = 1.0 / (1.0+0.5*z)\n    ans = t * math.exp(-z*z-1.26551223 + t*(1.00002368+t*(0.37409196+t*(0.09678418+t*(-0.18628806+t*(0.27886807+t*(-1.13520398+t*(1.48851587+t*(-0.82215223+t*0.17087277)))))))))\n    if x >= 0:\n        return ans\n    else:\n        return 2.0 - ans", "response": "Returns the complementary error function erfc with fractional\nerror everywhere less than 1. 2e - 7."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the probability of a given z value in the current language.", "response": "def lzprob(z):\n    \"\"\"\nReturns the area under the normal curve 'to the left of' the given z value.\nThus, \n    for z<0, zprob(z) = 1-tail probability\n    for z>0, 1.0-zprob(z) = 1-tail probability\n    for any z, 2.0*(1.0-zprob(abs(z))) = 2-tail probability\nAdapted from z.c in Gary Perlman's |Stat.\n\nUsage:   lzprob(z)\n\"\"\"\n    Z_MAX = 6.0    # maximum meaningful z-value\n    if z == 0.0:\n        x = 0.0\n    else:\n        y = 0.5 * math.fabs(z)\n        if y >= (Z_MAX*0.5):\n            x = 1.0\n        elif (y < 1.0):\n            w = y*y\n            x = ((((((((0.000124818987 * w\n                        -0.001075204047) * w +0.005198775019) * w\n                      -0.019198292004) * w +0.059054035642) * w\n                    -0.151968751364) * w +0.319152932694) * w\n                  -0.531923007300) * w +0.797884560593) * y * 2.0\n        else:\n            y = y - 2.0\n            x = (((((((((((((-0.000045255659 * y\n                             +0.000152529290) * y -0.000019538132) * y\n                           -0.000676904986) * y +0.001390604284) * y\n                         -0.000794620820) * y -0.002034254874) * y\n                       +0.006549791214) * y -0.010557625006) * y\n                     +0.011630447319) * y -0.009279453341) * y\n                   +0.005353579108) * y -0.002141268741) * y\n                 +0.000535310849) * y +0.999936657524\n    if z > 0.0:\n        prob = ((x+1.0)*0.5)\n    else:\n        prob = ((1.0-x)*0.5)\n    return prob"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lksprob(alam):\n    fac = 2.0\n    sum = 0.0\n    termbf = 0.0\n    a2 = -2.0*alam*alam\n    for j in range(1,201):\n        term = fac*math.exp(a2*j*j)\n        sum = sum + term\n        if math.fabs(term) <= (0.001*termbf) or math.fabs(term) < (1.0e-8*sum):\n            return sum\n        fac = -fac\n        termbf = math.fabs(term)\n    return 1.0", "response": "Computes a Kolmolgorov - Smirnov t - test significance level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the (1-tailed) significance level (p-value) of an F statistic given the degrees of freedom for the numerator (dfR-dfF) and the degrees of freedom for the denominator (dfF). Usage: lfprob(dfnum, dfden, F) where usually dfnum=dfbn, dfden=dfwn", "response": "def lfprob (dfnum, dfden, F):\n    \"\"\"\nReturns the (1-tailed) significance level (p-value) of an F\nstatistic given the degrees of freedom for the numerator (dfR-dfF) and\nthe degrees of freedom for the denominator (dfF).\n\nUsage:   lfprob(dfnum, dfden, F)   where usually dfnum=dfbn, dfden=dfwn\n\"\"\"\n    p = betai(0.5*dfden, 0.5*dfnum, dfden/float(dfden+dfnum*F))\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the gamma function of xx.", "response": "def lgammln(xx):\n    \"\"\"\nReturns the gamma function of xx.\n    Gamma(z) = Integral(0,infinity) of t^(z-1)exp(-t) dt.\n(Adapted from: Numerical Recipies in C.)\n\nUsage:   lgammln(xx)\n\"\"\"\n\n    coeff = [76.18009173, -86.50532033, 24.01409822, -1.231739516,\n             0.120858003e-2, -0.536382e-5]\n    x = xx - 1.0\n    tmp = x + 5.5\n    tmp = tmp - (x+0.5)*math.log(tmp)\n    ser = 1.0\n    for j in range(len(coeff)):\n        x = x + 1\n        ser = ser + coeff[j]/x\n    return -tmp + math.log(2.50662827465*ser)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lbetai(a,b,x):\n    if (x<0.0 or x>1.0):\n        raise ValueError('Bad x in lbetai')\n    if (x==0.0 or x==1.0):\n        bt = 0.0\n    else:\n        bt = math.exp(gammln(a+b)-gammln(a)-gammln(b)+a*math.log(x)+b*\n                      math.log(1.0-x))\n    if (x<(a+1.0)/(a+b+2.0)):\n        return bt*betacf(a,b,x)/float(a)\n    else:\n        return 1.0-bt*betacf(b,a,1.0-x)/float(b)", "response": "Returns the incomplete beta function of a given number of samples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a 1-way ANOVA, returning an F-value and probability given any number of groups. From Heiman, pp.394-7. Usage: F_oneway(*lists) where *lists is any number of lists, one per treatment group Returns: F value, one-tailed p-value", "response": "def lF_oneway(*lists):\n    \"\"\"\nPerforms a 1-way ANOVA, returning an F-value and probability given\nany number of groups.  From Heiman, pp.394-7.\n\nUsage:   F_oneway(*lists)    where *lists is any number of lists, one per\n                                  treatment group\nReturns: F value, one-tailed p-value\n\"\"\"\n    a = len(lists)           # ANOVA on 'a' groups, each in it's own list\n    means = [0]*a\n    vars = [0]*a\n    ns = [0]*a\n    alldata = []\n    tmp = [N.array(_) for _ in lists]\n    means = [amean(_) for _ in tmp]\n    vars = [avar(_) for _ in tmp]\n    ns = [len(_) for _ in lists]\n    for i in range(len(lists)):\n        alldata = alldata + lists[i]\n    alldata = N.array(alldata)\n    bign = len(alldata)\n    sstot = ass(alldata)-(asquare_of_sums(alldata)/float(bign))\n    ssbn = 0\n    for list in lists:\n        ssbn = ssbn + asquare_of_sums(N.array(list))/float(len(list))\n    ssbn = ssbn - (asquare_of_sums(alldata)/float(bign))\n    sswn = sstot-ssbn\n    dfbn = a-1\n    dfwn = bign - a\n    msb = ssbn/float(dfbn)\n    msw = sswn/float(dfwn)\n    f = msb/msw\n    prob = fprob(dfbn,dfwn,f)\n    return f, prob"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lF_value (ER,EF,dfnum,dfden):\n    return ((ER-EF)/float(dfnum) / (EF/float(dfden)))", "response": "Returns the F - statistic value given the following parameters ER EF dfnum and dfden"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting a list of lists to a file in columns customized by the maximal size of items within the columns.", "response": "def writecc (listoflists,file,writetype='w',extra=2):\n    \"\"\"\nWrites a list of lists to a file in columns, customized by the max\nsize of items within the columns (max size of items in col, +2 characters)\nto specified file.  File-overwrite is the default.\n\nUsage:   writecc (listoflists,file,writetype='w',extra=2)\nReturns: None\n\"\"\"\n    if type(listoflists[0]) not in [ListType,TupleType]:\n        listoflists = [listoflists]\n    outfile = open(file,writetype)\n    rowstokill = []\n    list2print = copy.deepcopy(listoflists)\n    for i in range(len(listoflists)):\n        if listoflists[i] == ['\\n'] or listoflists[i]=='\\n' or listoflists[i]=='dashes':\n            rowstokill = rowstokill + [i]\n    rowstokill.reverse()\n    for row in rowstokill:\n        del list2print[row]\n    maxsize = [0]*len(list2print[0])\n    for col in range(len(list2print[0])):\n        items = pstat.colex(list2print,col)\n        items = [pstat.makestr(_) for _ in items]\n        maxsize[col] = max(map(len,items)) + extra\n    for row in listoflists:\n        if row == ['\\n'] or row == '\\n':\n            outfile.write('\\n')\n        elif row == ['dashes'] or row == 'dashes':\n            dashes = [0]*len(maxsize)\n            for j in range(len(maxsize)):\n                dashes[j] = '-'*(maxsize[j]-2)\n            outfile.write(pstat.lineincustcols(dashes,maxsize))\n        else:\n            outfile.write(pstat.lineincustcols(row,maxsize))\n        outfile.write('\\n')\n    outfile.close()\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsimulate a counting system from an n - dimensional list", "response": "def lincr(l,cap):        # to increment a list up to a max-list of 'cap'\n    \"\"\"\nSimulate a counting system from an n-dimensional list.\n\nUsage:   lincr(l,cap)   l=list to increment, cap=max values for each list pos'n\nReturns: next set of values for list l, OR -1 (if overflow)\n\"\"\"\n    l[0] = l[0] + 1     # e.g., [0,0,0] --> [2,4,3] (=cap)\n    for i in range(len(l)):\n        if l[i] > cap[i] and i < len(l)-1: # if carryover AND not done\n            l[i] = 0\n            l[i+1] = l[i+1] + 1\n        elif l[i] > cap[i] and i == len(l)-1: # overflow past last column, must be finished\n            l = -1\n    return l"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list consisting of the cumulative sum of the items in the passed list.", "response": "def lcumsum (inlist):\n    \"\"\"\nReturns a list consisting of the cumulative sum of the items in the\npassed list.\n\nUsage:   lcumsum(inlist)\n\"\"\"\n    newlist = copy.deepcopy(inlist)\n    for i in range(1,len(newlist)):\n        newlist[i] = newlist[i] + newlist[i-1]\n    return newlist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lss(inlist):\n    ss = 0\n    for item in inlist:\n        ss = ss + item*item\n    return ss", "response": "Adds up these squares and\nreturns the result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lsummult (list1,list2):\n    if len(list1) != len(list2):\n        raise ValueError(\"Lists not equal length in summult.\")\n    s = 0\n    for item1,item2 in pstat.abut(list1,list2):\n        s = s + item1*item2\n    return s", "response": "Returns the sum of all resulting multiplications in list1 and list2."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lsumdiffsquared(x,y):\n    sds = 0\n    for i in range(len(x)):\n        sds = sds + (x[i]-y[i])**2\n    return sds", "response": "Takes pairwise differences of the values in lists x and y and returns the sum of these squares."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting or writes a file stats for two groups.", "response": "def outputpairedstats(fname,writemode,name1,n1,m1,se1,min1,max1,name2,n2,m2,se2,min2,max2,statname,stat,prob):\n    \"\"\"\nPrints or write to a file stats for two groups, using the name, n,\nmean, sterr, min and max for each group, as well as the statistic name,\nits value, and the associated p-value.\n\nUsage:   outputpairedstats(fname,writemode,\n                           name1,n1,mean1,stderr1,min1,max1,\n                           name2,n2,mean2,stderr2,min2,max2,\n                           statname,stat,prob)\nReturns: None\n\"\"\"\n    suffix = ''                       # for *s after the p-value\n    try:\n        x = prob.shape\n        prob = prob[0]\n    except:\n        pass\n    if  prob < 0.001:  suffix = '  ***'\n    elif prob < 0.01:  suffix = '  **'\n    elif prob < 0.05:  suffix = '  *'\n    title = [['Name','N','Mean','SD','Min','Max']]\n    lofl = title+[[name1,n1,round(m1,3),round(math.sqrt(se1),3),min1,max1],\n                  [name2,n2,round(m2,3),round(math.sqrt(se2),3),min2,max2]]\n    if type(fname)!=StringType or len(fname)==0:\n        print()\n        print(statname)\n        print()\n        pstat.printcc(lofl)\n        print()\n        try:\n            if stat.shape == ():\n                stat = stat[0]\n            if prob.shape == ():\n                prob = prob[0]\n        except:\n            pass\n        print('Test statistic = ',round(stat,3),'   p = ',round(prob,3),suffix)\n        print()\n    else:\n        file = open(fname,writemode)\n        file.write('\\n'+statname+'\\n\\n')\n        file.close()\n        writecc(lofl,fname,'a')\n        file = open(fname,'a')\n        try:\n            if stat.shape == ():\n                stat = stat[0]\n            if prob.shape == ():\n                prob = prob[0]\n        except:\n            pass\n        file.write(pstat.list2string(['\\nTest statistic = ',round(stat,4),'   p = ',round(prob,4),suffix,'\\n\\n']))\n        file.close()\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GeneReader( fh, format='gff' ):\n\n    known_formats = ( 'gff', 'gtf', 'bed')\n    if format not in known_formats: \n        print('%s format not in %s' % (format, \",\".join( known_formats )), file=sys.stderr)\n        raise Exception('?')\n\n    if format == 'bed':\n        for line in fh:    \n            f = line.strip().split()\n            chrom = f[0]\n            chrom_start = int(f[1])\n            name = f[4]\n            strand = f[5]\n            cdsStart = int(f[6])\n            cdsEnd = int(f[7])\n            blockCount = int(f[9])\n            blockSizes = [ int(i) for i in f[10].strip(',').split(',') ]\n            blockStarts = [ chrom_start + int(i) for i in f[11].strip(',').split(',') ]\n\n            # grab cdsStart - cdsEnd\n            gene_exons = []\n            for base,offset in zip( blockStarts, blockSizes ):\n                exon_start = base\n                exon_end = base+offset\n                gene_exons.append( (exon_start, exon_end) )\n            yield chrom, strand, gene_exons, name\n    genelist = {}\n    grouplist = []\n    if format == 'gff' or format == 'gtf':\n        for line in fh:\n            if line.startswith('#'): continue\n            fields = line.strip().split('\\t')\n            if len( fields ) < 9: continue\n\n            # fields\n\n            chrom = fields[0]\n            ex_st = int( fields[3] ) - 1 # make zero-centered\n            ex_end = int( fields[4] ) #+ 1 # make exclusive\n            strand = fields[6]\n\n            if format == 'gtf':\n                group = fields[8].split(';')[0]\n            else:\n                group = fields[8]\n\n            if group not in grouplist: grouplist.append( group )\n            if group not in genelist:\n                genelist[group] = (chrom, strand, [])\n            exons_i = 2\n            genelist[group][exons_i].append( ( ex_st, ex_end ) )\n\n        sp = lambda a,b: cmp( a[0], b[0] )\n\n        #for gene in genelist.values():\n        for gene in grouplist:\n            chrom, strand, gene_exons = genelist[ gene ]\n            gene_exons = bitset_union( gene_exons )\n            yield chrom, strand, gene_exons, gene", "response": "Generator that yields the gene list and gene exons of all the blocks in the file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CDSReader( fh, format='gff' ):\n\n    known_formats = ( 'gff', 'gtf', 'bed')\n    if format not in known_formats: \n        print('%s format not in %s' % (format, \",\".join( known_formats )), file=sys.stderr)\n        raise Exception('?')\n\n    if format == 'bed':\n        for line in fh:    \n            f = line.strip().split()\n            chrom = f[0]\n            chrom_start = int(f[1])\n            name = f[4]\n            strand = f[5]\n            cdsStart = int(f[6])\n            cdsEnd = int(f[7])\n            blockCount = int(f[9])\n            blockSizes = [ int(i) for i in f[10].strip(',').split(',') ]\n            blockStarts = [ chrom_start + int(i) for i in f[11].strip(',').split(',') ]\n\n            # grab cdsStart - cdsEnd\n            cds_exons = []\n            cds_seq = ''\n            genome_seq_index = []\n            for base,offset in zip( blockStarts, blockSizes ):\n                if (base + offset) < cdsStart: continue\n                if base > cdsEnd: continue\n                exon_start = max( base, cdsStart )\n                exon_end = min( base+offset, cdsEnd ) \n                cds_exons.append( (exon_start, exon_end) )\n            yield chrom, strand, cds_exons, name\n\n    genelist = {}\n    grouplist = []\n    if format == 'gff' or format == 'gtf':\n        for line in fh:\n            if line.startswith('#'): continue\n            fields = line.strip().split('\\t')\n            if len( fields ) < 9: continue\n            if fields[2] not in ('CDS', 'stop_codon', 'start_codon'): continue\n\n            # fields\n\n            chrom = fields[0]\n            ex_st = int( fields[3] ) - 1 # make zero-centered\n            ex_end = int( fields[4] ) #+ 1 # make exclusive\n            strand = fields[6]\n\n            if format == 'gtf':\n                group = fields[8].split(';')[0]\n            else:\n                group = fields[8]\n\n            if group not in grouplist: grouplist.append( group )\n            if group not in genelist:\n                genelist[group] = (chrom, strand, [])\n            \n            genelist[group][2].append( ( ex_st, ex_end ) )\n\n        sp = lambda a,b: cmp( a[0], b[0] )\n\n        #for gene in genelist.values():\n        for gene in grouplist:\n            chrom, strand, cds_exons = genelist[ gene ]\n            seqlen = sum([ a[1]-a[0] for a in cds_exons ])\n            overhang = seqlen % 3\n            if overhang > 0:\n                #print >>sys.stderr, \"adjusting \", gene  \n                if strand == '+': \n                    cds_exons[-1] = ( cds_exons[-1][0], cds_exons[-1][1] - overhang )\n                else:\n                    cds_exons[0] = ( cds_exons[0][0] + overhang, cds_exons[0][1] )\n            cds_exons = bitset_union( cds_exons )\n            yield chrom, strand, cds_exons, gene", "response": "Generator that reads a CDS file and returns a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef FeatureReader( fh, format='gff', alt_introns_subtract=\"exons\", gtf_parse=None):\n\n    known_formats = ( 'gff', 'gtf', 'bed')\n    if format not in known_formats: \n        print('%s format not in %s' % (format, \",\".join( known_formats )), file=sys.stderr)\n        raise Exception('?')\n\n    if format == 'bed':\n        for line in fh:    \n            f = line.strip().split()\n            chrom = f[0]\n            chrom_start = int(f[1])\n            name = f[4]\n            strand = f[5]\n            cdsStart = int(f[6])\n            cdsEnd = int(f[7])\n            blockCount = int(f[9])\n            blockSizes = [ int(i) for i in f[10].strip(',').split(',') ]\n            blockStarts = [ chrom_start + int(i) for i in f[11].strip(',').split(',') ]\n\n            # grab cdsStart - cdsEnd\n            cds_exons = []\n            exons = []\n            \n            cds_seq = ''\n            genome_seq_index = []\n            for base,offset in zip( blockStarts, blockSizes ):\n                if (base + offset) < cdsStart: continue\n                if base > cdsEnd: continue\n                # exons\n                exon_start = base\n                exon_end = base+offset\n                exons.append( (exon_start, exon_end) )\n                # cds exons\n                exon_start = max( base, cdsStart )\n                exon_end = min( base+offset, cdsEnd ) \n                cds_exons.append( (exon_start, exon_end) )\n            cds_exons = bitset_union( cds_exons )\n            exons = bitset_union( exons )\n            introns = bitset_complement( exons )\n            yield chrom, strand, cds_exons, introns, exons, name\n\n    genelist = {}\n    grouplist = []\n    if format == 'gff' or format == 'gtf':\n        for line in fh:\n            if line.startswith('#'): continue\n            fields = line.strip().split('\\t')\n            if len( fields ) < 9: continue\n\n            # fields\n\n            chrom = fields[0]\n            ex_st = int( fields[3] ) - 1 # make zero-centered\n            ex_end = int( fields[4] ) #+ 1 # make exclusive\n            strand = fields[6]\n\n            if format == 'gtf':\n                if not gtf_parse:\n                    group = fields[8].split(';')[0]\n                else:\n                    group = gtf_parse( fields[8] )\n            else:\n                group = fields[8]\n\n            # Results are listed in the same order as encountered\n            if group not in grouplist: grouplist.append( group )\n\n            if group not in genelist:\n                # chrom, strand, cds_exons, introns, exons, cds_start, cds_end\n                genelist[group] = [chrom, strand, [], [], [], None, None]\n            \n            if fields[2] == 'exon':\n                genelist[group][4].append( ( ex_st, ex_end ) )\n\n            elif fields[2] in ('CDS', 'stop_codon', 'start_codon'):\n                genelist[group][2].append( ( ex_st, ex_end ) )\n\n                if fields[2] == 'start_codon':\n                    if strand == '+': genelist[group][5] = ex_st\n                    else: genelist[group][5] = ex_end\n                if fields[2] == 'stop_codon':\n                    if strand == '+': genelist[group][5] = ex_end\n                    else: genelist[group][5] = ex_st\n\n            elif fields[2] == 'intron':\n                genelist[group][3].append( ( ex_st, ex_end ) )\n\n        for gene in grouplist:\n            chrom, strand, cds_exons, introns, exons, cds_start, cds_end = genelist[ gene ]\n\n            cds_exons = bitset_union( cds_exons )\n            exons = bitset_union( exons )\n\n            # assure that cds exons were within the cds range\n            if cds_start is not None and cds_end is not None:\n                if strand == '+':\n                    cds_exons = bitset_intersect( cds_exons, [(cds_start,cds_end)] )\n                else:\n                    cds_exons = bitset_intersect( cds_exons, [(cds_end,cds_start)] )\n\n            # assure that introns are non-overlapping with themselves or exons\n            if alt_introns_subtract:\n                if alt_introns_subtract == 'exons':\n                    introns = bitset_subtract( introns, exons )\n                if alt_introns_subtract == 'cds_exons':\n                    introns = bitset_subtract( introns, cds_exons )\n            else: introns = bitset_union( introns )\n\n            # assure CDS is a multiple of 3, trim from last exon if necessary\n            seqlen = sum([ a[1]-a[0] for a in cds_exons ])\n            overhang = seqlen % 3\n            if overhang > 0:\n                if strand == '+': \n                    cds_exons[-1] = ( cds_exons[-1][0], cds_exons[-1][1] - overhang )\n                else:\n                    cds_exons[0] = ( cds_exons[0][0] + overhang, cds_exons[0][1] )\n\n            yield chrom, strand, cds_exons, introns, exons, gene", "response": "Generator that reads a file - like object and yields a sequence of all the features in it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef throw_random_gap_list( lengths, mask, save_interval_func, allow_overlap=False ):\n    # Use mask to find the gaps;  gaps is a list of (length,start,end)\n    lengths = [length for length in lengths if length > 0]\n    min_length = min( lengths )\n    gaps = []\n    start = end = 0\n    while 1:\n        start = mask.next_clear( end )\n        if start == mask.size: break\n        end = mask.next_set( start )\n        if end-start >= min_length:\n            gaps.append( ( end-start, start, None ) )\n    # Sort (long regions first)    \n    gaps.sort()\n    gaps.reverse()\n    # Throw\n    throw_random_private( lengths, gaps, save_interval_func, allow_overlap, three_args=False )", "response": "Generate a list of non - overlapping random intervals from a length \nAddon distribution."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef throw_random_intervals( lengths, regions, save_interval_func=None, allow_overlap=False ):\n    # Copy regions\n    regions = [( x[1]-x[0], x[0], x ) for x in regions]\n    # Sort (long regions first)    \n    regions.sort()\n    regions.reverse()\n    # Throw\n    if (save_interval_func != None):\n        throw_random_private( lengths, regions, save_interval_func, allow_overlap )\n        return\n    else:\n        intervals = []\n        save_interval_func = lambda s, e, rgn: intervals.append( overwrite_start_end ( s, e, rgn ) )\n        throw_random_private( lengths, regions, save_interval_func, allow_overlap )\n        return intervals", "response": "Generates a set of non - overlapping random intervals from a length \n    distribution."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef throw_random_private( lengths, regions, save_interval_func, allow_overlap=False, three_args=True ):\n\n    # Implementation:\n    #   We keep a list of the regions, sorted from largest to smallest.  We then\n    #   place each length by following steps:\n    #     (1) construct a candidate counts array (cc array)\n    #     (2) choose a candidate at random\n    #     (3) find region containing that candidate\n    #     (4) map candidate to position in that region\n    #     (5) split region if not allowing overlaps\n    #     (6) report placed segment\n    #\n    #   The cc array is only constructed if there's a change (different length\n    #   to place, or the region list has changed).  It contains, for each\n    #   region, the total number of number of candidate positions in regions\n    #   *preceding* it in the region list:\n    #     cc[i] = sum over k in 0..(i-1) of length[i] - L + 1\n    #   where N is the number of regions and L is the length being thrown.\n    #   At the same time, we determine the total number of candidates (the total\n    #   number of places the current length can be placed) and the index range\n    #   of regions into which the length will fit.\n    #\n    #   example:\n    #     for L = 20\n    #     i =           0   1   2   3   4   5   6   7   8   9\n    #     length[i] =  96  66  56  50  48  40  29  17  11   8\n    #     cc[i] =       0  77 124 161 192 221 242   X   X   X\n    #     candidates = 252\n    #     lo_rgn = 0\n    #     hi_rgn = 6\n    #\n    #   The candidate is chosen in (0..candidates-1).  The candidate counts\n    #   array allows us to do a binary search to locate the region that holds that\n    #   candidate.  Continuing the example above, we choose a random candidate\n    #   s in (0..251).  If s happens to be in (124..160), it will be mapped to\n    #   region 2 at start position s-124.\n    #\n    #   During the binary search, if we are looking at region 3, if s < cc[3]\n    #   then the desired region is region 2 or lower.  Otherwise it is region 3 or\n    #   higher.\n\n    min_length = min( lengths )\n    prev_length = None # (force initial cc array construction)\n    cc = [0] * (len( regions ) + len(lengths) - 1)\n    num_thrown = 0\n    for length in lengths:\n        # construct cc array (only needed if length has changed or region list has\n        # changed)\n        if length != prev_length:\n            prev_length = length\n            assert len( cc ) >= len( regions )\n            candidates = 0\n            hi_rgn = 0\n            for region in regions:\n                rgn_len = region[0]\n                if rgn_len < length:\n                    break\n                cc[hi_rgn] = candidates\n                candidates += rgn_len - length + 1\n                hi_rgn += 1\n            if candidates == 0:\n                raise MaxtriesException( \"No region can fit an interval of length %d (we threw %d of %d)\" \\\n                                       % ( length, num_thrown,len( lengths ) ) )\n            hi_rgn -= 1\n        # Select a candidate\n        s = random.randrange( candidates )\n        #..\n        #..for ix in range( len( regions ) ):\n        #..    region = regions[ix]\n        #..    if ix <= hi_rgn: print \"%2s: %5s %5s %5s\" % ( ix, region[1], region[0], cc[ix] )\n        #..    else:            print \"%2s: %5s %5s %5s\" % ( ix, region[1], region[0], \"X\" )\n        #..print \"s = %s (of %s candidates)\" % ( s, candidates )\n        # Locate region containing that candidate, by binary search\n        lo = 0\n        hi = hi_rgn\n        while hi > lo:\n            mid = (lo + hi + 1) / 2     # (we round up to prevent infinite loop)\n            if s < cc[mid]: hi = mid-1  # (s <  num candidates from 0..mid-1)\n            else:           lo = mid    # (s >= num candidates from 0..mid-1)\n        s -= cc[lo]\n        # If we are not allowing overlaps we will remove the placed interval\n        # from the region list\n        if allow_overlap:\n            rgn_length, rgn_start, rgn_extra = regions[lo]\n        else:\n            # Remove the chosen region and split\n            rgn_length, rgn_start, rgn_extra = regions.pop( lo )\n            rgn_end = rgn_start + rgn_length\n            assert s >= 0\n            assert rgn_start + s + length <= rgn_end, \"Expected: %d + %d + %d == %d <= %d\" % ( rgn_start, s, length, rgn_start + s + length, rgn_end )\n            regions.reverse()\n            if s >= min_length:\n                bisect.insort( regions, ( s, rgn_start, rgn_extra ) )\n            if s + length <= rgn_length - min_length:\n                bisect.insort( regions, ( rgn_length - ( s + length ), rgn_start + s + length, rgn_extra ) )\n            regions.reverse()\n            prev_length = None # (force cc array construction)\n        # Save the new interval\n        if (three_args):\n            save_interval_func( rgn_start + s, rgn_start + s + length, rgn_extra )\n        else:\n            save_interval_func( rgn_start + s, rgn_start + s + length )\n        num_thrown += 1", "response": "This function is used to throw random intervals into a single segment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch a subsequence starting at position start with length length.", "response": "def get(self, start, length):\n        \"\"\"\n        Fetch subsequence starting at position `start` with length `length`. \n        This method is picky about parameters, the requested interval must \n        have non-negative length and fit entirely inside the NIB sequence,\n        the returned string will contain exactly 'length' characters, or an\n        AssertionError will be generated.\n        \"\"\"\n        # Check parameters\n        assert length >= 0, \"Length must be non-negative (got %d)\" % length \n        assert start >= 0,\"Start must be greater than 0 (got %d)\" % start\n        assert start + length <= self.length, \\\n            \"Interval beyond end of sequence (%s..%s > %s)\" % ( start, start + length, self.length )\n        # Fetch sequence and reverse complement if necesary\n        if not self.revcomp:\n            return self.raw_fetch( start, length )\n        if self.revcomp == \"-3'\":\n            return self.reverse_complement(self.raw_fetch(start,length))\n        assert self.revcomp == \"-5'\", \"unrecognized reverse complement scheme\"\n        start = self.length - (start+length)\n        return self.reverse_complement(self.raw_fetch(start,length))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_scoring_scheme( f, gap_open, gap_extend, gap1=\"-\", gap2=None, **kwargs ):\n    close_it = False\n    if (type(f) == str):\n        f = file(f,\"rt\")\n        close_it = True\n    ss = build_scoring_scheme(\"\".join([line for line in f]),gap_open, gap_extend, gap1=gap1, gap2=gap2, **kwargs)\n    if (close_it):\n        f.close()\n    return ss", "response": "Read a scoring scheme from a file containing a blastz style text blob."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a scoring scheme from a BLASTZ - style text blob.", "response": "def build_scoring_scheme( s, gap_open, gap_extend, gap1=\"-\", gap2=None, **kwargs ):\n    \"\"\"\n    Initialize scoring scheme from a blastz style text blob, first line\n    specifies the bases for each row/col, subsequent lines contain the\n    corresponding scores.  Slaw extensions allow for unusual and/or\n    asymmetric alphabets.  Symbols can be two digit hex, and each row\n    begins with symbol.  Note that a row corresponds to a symbol in text1\n    and a column to a symbol in text2.\n\n    examples:\n\n       blastz                       slaw\n\n          A    C    G    T               01   02    A    C    G    T\n         91 -114  -31 -123          01  200 -200  -50  100  -50  100\n       -114  100 -125  -31          02 -200  200  100  -50  100  -50\n        -31 -125  100 -114\n       -123  -31 -114   91\n    \"\"\"\n    # perform initial parse to determine alphabets and locate scores\n    bad_matrix = \"invalid scoring matrix\"\n    s = s.rstrip( \"\\n\" )\n    lines = s.split( \"\\n\" )\n    rows  = []\n    symbols2 = lines.pop(0).split()\n    symbols1 = None\n    rows_have_syms = False\n    a_la_blastz = True\n    for i, line in enumerate( lines ):\n        row_scores = line.split()\n        if len( row_scores ) == len( symbols2 ):        # blastz-style row\n            if symbols1 == None:\n                if len( lines ) != len( symbols2 ):\n                    raise bad_matrix\n                symbols1 = symbols2\n            elif (rows_have_syms):\n                raise bad_matrix\n        elif len( row_scores ) == len( symbols2 ) + 1:  # row starts with symbol\n            if symbols1 == None:\n                symbols1 = []\n                rows_have_syms = True\n                a_la_blastz = False\n            elif not rows_have_syms:\n                raise bad_matrix\n            symbols1.append( row_scores.pop(0) )\n        else:\n            raise bad_matrix\n        rows.append( row_scores )\n    # convert alphabets from strings to characters\n    try:\n        alphabet1 = [sym_to_char( sym ) for sym in symbols1]\n        alphabet2 = [sym_to_char( sym ) for sym in symbols2]\n    except ValueError:\n        raise bad_matrix\n    if (alphabet1 != symbols1) or (alphabet2 != symbols2):\n        a_la_blastz = False\n    if a_la_blastz:\n        alphabet1 = [ch.upper() for ch in alphabet1]\n        alphabet2 = [ch.upper() for ch in alphabet2]\n    # decide if rows and/or columns should reflect case\n    if a_la_blastz:\n        foldcase1 = foldcase2 = True\n    else:\n        foldcase1 = \"\".join( alphabet1 ) == \"ACGT\"\n        foldcase2 = \"\".join( alphabet2 ) == \"ACGT\"\n    # create appropriately sized matrix\n    text1_range = text2_range = 128\n    if ord( max( alphabet1 ) ) >= 128: text1_range = 256\n    if ord( max( alphabet2 ) ) >= 128: text2_range = 256\n    typecode = int32\n    for i, row_scores in enumerate( rows ):\n        for j, score in enumerate( map( int_or_float, row_scores ) ):\n            if type( score ) == float: \n                typecode = float32\n    if type( gap_open ) == float: \n        typecode = float32\n    if type( gap_extend ) == float: \n        typecode = float32\n    ss = ScoringScheme( gap_open, gap_extend, alphabet1=alphabet1, alphabet2=alphabet2, gap1=gap1, gap2=gap2, text1_range=text1_range, text2_range=text2_range, typecode=typecode, **kwargs )\n    # fill matrix\n    for i, row_scores in enumerate( rows ):\n        for j, score in enumerate( map( int_or_float, row_scores ) ):\n            ss.set_score( ord( alphabet1[i] ), ord( alphabet2[j] ), score )\n            if foldcase1 and foldcase2:\n                ss.set_score( ord( alphabet1[i].lower() ), ord( alphabet2[j].upper() ), score )\n                ss.set_score( ord( alphabet1[i].upper() ), ord( alphabet2[j].lower() ), score )\n                ss.set_score( ord( alphabet1[i].lower() ), ord( alphabet2[j].lower() ), score )\n            elif foldcase1:\n                ss.set_score( ord( alphabet1[i].lower() ), ord( alphabet2[j]         ), score )\n            elif foldcase2:\n                ss.set_score( ord( alphabet1[i]         ), ord( alphabet2[j].lower() ), score )\n    return ss"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the cumulative score of each position in text1 and text2.", "response": "def accumulate_scores( scoring_scheme, text1, text2, skip_ref_gaps=False ):\n    \"\"\"\n    Return cumulative scores for each position in alignment as a 1d array.\n    \n    If `skip_ref_gaps` is False positions in returned array correspond to each\n    column in alignment, if True they correspond to each non-gap position (each\n    base) in text1.\n    \"\"\"\n    if skip_ref_gaps:\n        rval = zeros( len( text1 ) - text1.count( scoring_scheme.gap1 ) )\n    else:\n        rval = zeros( len( text1 ) )\n    score = 0\n    pos = 0\n    last_gap_a = last_gap_b = False\n    for i in range( len( text1 ) ):\n        a = text1[i]\n        b = text2[i]\n        # Ignore gap/gap pair\n        if a == scoring_scheme.gap1 and b == scoring_scheme.gap2: \n            continue\n        # Gap in first species\n        elif a == scoring_scheme.gap1:\n            score -= scoring_scheme.gap_extend\n            if not last_gap_a:\n               score -= scoring_scheme.gap_open\n               last_gap_a = True\n               last_gap_b = False\n        # Gap in second species\n        elif b == scoring_scheme.gap2:\n            score -= scoring_scheme.gap_extend\n            if not last_gap_b:\n               score -= scoring_scheme.gap_open\n               last_gap_a = False\n               last_gap_b = True\n        # Aligned base\n        else:   \n            score += scoring_scheme._get_score((ord(a),ord(b)))\n            last_gap_a = last_gap_b = False\n        if not( skip_ref_gaps ) or a != scoring_scheme.gap1:\n            rval[pos] = score\n            pos += 1\n    return rval"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shuffle_columns( a ):\n    mask = range( a.text_size )\n    random.shuffle( mask )\n    for c in a.components:\n        c.text = ''.join( [ c.text[i] for i in mask ] )", "response": "Randomize the columns of an alignment"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a slice of the alignment corresponding to an interval in a specific component.", "response": "def slice_by_component( self, component_index, start, end ):\n        \"\"\"\n        Return a slice of the alignment, corresponding to an coordinate interval in a specific component.\n\n        component_index is one of\n            an integer offset into the components list\n            a string indicating the src of the desired component\n            a component\n\n        start and end are relative to the + strand, regardless of the component's strand.\n\n        \"\"\"\n        if type( component_index ) == type( 0 ):\n            ref = self.components[ component_index ]\n        elif type( component_index ) == type( \"\" ):\n            ref = self.get_component_by_src( component_index )\n        elif type( component_index ) == Component:\n            ref = component_index\n        else:\n            raise ValueError( \"can't figure out what to do\" )\n        start_col = ref.coord_to_col( start )\n        end_col = ref.coord_to_col( end )\n        if (ref.strand == '-'):\n            (start_col,end_col) = (end_col,start_col)\n        return self.slice( start_col, end_col )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_all_gap_columns( self ):\n        seqs = []\n        for c in self.components:\n            try:\n                seqs.append( list( c.text ) )\n            except TypeError:\n                seqs.append( None )\n        i = 0\n        text_size = self.text_size\n        while i < text_size:\n            all_gap = True\n            for seq in seqs:\n                if seq is None: continue\n                if seq[i] != '-': all_gap = False\n            if all_gap:\n                for seq in seqs:\n                    if seq is None: continue\n                    del seq[i]\n                text_size -= 1\n            else:\n                i += 1\n        for i in range( len( self.components ) ):\n            if seqs[i] is None: continue\n            self.components[i].text = ''.join( seqs[i] )\n        self.text_size = text_size", "response": "Remove any columns containing only gaps from alignment components."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a slice of the component corresponding to a coordinate interval.", "response": "def slice_by_coord( self, start, end ):\n        \"\"\"\n        Return the slice of the component corresponding to a coordinate interval.\n\n        start and end are relative to the + strand, regardless of the component's strand.\n\n        \"\"\"\n        start_col = self.coord_to_col( start )\n        end_col = self.coord_to_col( end )\n        if (self.strand == '-'):\n            (start_col,end_col) = (end_col,start_col)\n        return self.slice( start_col, end_col )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef coord_to_col( self, pos ):\n        start,end = self.get_forward_strand_start(),self.get_forward_strand_end()\n        if pos < start or pos > end:\n            raise ValueError(\"Range error: %d not in %d-%d\" % ( pos, start, end ))\n        if not self.index:\n            self.index = list()\n            if (self.strand == '-'):\n                # nota bene: for - strand self.index[x] maps to one column\n                # higher than is actually associated with the position;  thus\n                # when slice_by_component() and slice_by_coord() flip the ends,\n                # the resulting slice is correct\n                for x in range( len(self.text)-1,-1,-1 ):\n                    if not self.text[x] == '-':\n                        self.index.append( x + 1 )\n                self.index.append( 0 )\n            else:\n                for x in range( len(self.text) ):\n                    if not self.text[x] == '-':\n                        self.index.append(x)\n                self.index.append( len(self.text) )\n        x = None\n        try:\n            x = self.index[ pos - start ]\n        except:\n            raise Exception(\"Error in index.\")\n        return x", "response": "Return the alignment column index corresponding to coordinate pos."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef thread( mafs, species ):\n    for m in mafs:\n        new_maf = deepcopy( m )\n        new_components = get_components_for_species( new_maf, species )\t\n        if new_components: \n            remove_all_gap_columns( new_components )          \n            new_maf.components = new_components\n            new_maf.score = 0.0\n            new_maf.text_size = len(new_components[0].text)\n            yield new_maf", "response": "This function is used to restrict a list of alignments to a given list of species."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_components_for_species( alignment, species ):\n    # If the number of components in the alignment is less that the requested number\n    # of species we can immediately fail\n    if len( alignment.components ) < len( species ): return None\n    # Otherwise, build an index of components by species, then lookup \n    index = dict( [ ( c.src.split( '.' )[0], c ) for c in alignment.components ] )\n    try: return [ index[s] for s in species ]\n    except: return None", "response": "Return the component for each species in the list species or None if no components are available."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_all_gap_columns( components ):\n    seqs = [ list( c.text ) for c in components ]\n    i = 0\n    text_size = len( seqs[0] )\n    while i < text_size:\n        all_gap = True\n        for seq in seqs:\n            if seq[i] != '-': all_gap = False\n        if all_gap:\n            for seq in seqs: del seq[i]\n            text_size -= 1\n        else:\n            i += 1\n    for i in range( len( components ) ):\n        components[i].text = ''.join( seqs[i] )", "response": "Remove any columns containing only gaps from a set of alignment components."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_next_maf( file, species_to_lengths=None, parse_e_rows=False ):\n    alignment = Alignment(species_to_lengths=species_to_lengths)\n    # Attributes line\n    line = readline( file, skip_blank=True )\n    if not line: return None\n    fields = line.split() \n    if fields[0] != 'a': raise Exception(\"Expected 'a ...' line\")\n    alignment.attributes = parse_attributes( fields[1:] )\n    if 'score' in alignment.attributes:\n        alignment.score = alignment.attributes['score']\n        del alignment.attributes['score']\n    else:\n        alignment.score = 0\n    # Sequence lines\n    last_component = None\n    while 1:\n        line = readline( file )\n        # EOF or Blank line terminates alignment components\n        if not line or line.isspace(): break\n        if line.isspace(): break \n        # Parse row\n        fields = line.split()\n        if fields[0] == 's':\n            # An 's' row contains sequence for a component\n            component = Component()\n            component.src = fields[1]\n            component.start = int( fields[2] )\n            component.size = int( fields[3] )\n            component.strand = fields[4]\n            component.src_size = int( fields[5] )\n            if len(fields) > 6: component.text = fields[6].strip()\n            # Add to set\n            alignment.add_component( component )\n            last_component = component\n        elif fields[0] == 'e':\n            # An 'e' row, when no bases align for a given species this tells\n            # us something about the synteny \n            if parse_e_rows:\n                component = Component()\n                component.empty = True\n                component.src = fields[1]\n                component.start = int( fields[2] )\n                component.size = int( fields[3] )\n                component.strand = fields[4]\n                component.src_size = int( fields[5] )\n                component.text = None\n                synteny = fields[6].strip()\n                assert len( synteny ) == 1, \\\n                    \"Synteny status in 'e' rows should be denoted with a single character code\"\n                component.synteny_empty = synteny\n                alignment.add_component( component )\n                last_component = component\n        elif fields[0] == 'i':\n            # An 'i' row, indicates left and right synteny status for the \n            # previous component, we hope ;)\n            assert fields[1] == last_component.src, \"'i' row does not follow matching 's' row\"\n            last_component.synteny_left = ( fields[2], int( fields[3] ) )\n            last_component.synteny_right = ( fields[4], int( fields[5] ) )\n        elif fields[0] == 'q':\n            assert fields[1] == last_component.src, \"'q' row does not follow matching 's' row\"\n            # TODO: Should convert this to an integer array?\n            last_component.quality = fields[2]\n            \n    return alignment", "response": "Read the next MAF block from file and return as an Alignment instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef readline( file, skip_blank=False ):\n    while 1:\n        line = file.readline()\n        #print \"every line: %r\" % line\n        if not line: return None \n        if line[0] != '#' and not ( skip_blank and line.isspace() ):\n            return line", "response": "Read a line from a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse list of key = value strings into a dict", "response": "def parse_attributes( fields ):\n    \"\"\"Parse list of key=value strings into a dict\"\"\"\n    attributes = {}\n    for field in fields:\n        pair = field.split( '=' )\n        attributes[ pair[0] ] = pair[1]\n    return attributes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_dict( self, key=\"id\" ):\n        rval = {}\n        for motif in self:\n            rval[ getattr( motif, key ) ] = motif\n        return rval", "response": "Return a dictionary containing all motifs with the given key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a TRANSFAC record out of lines and return a motif.", "response": "def parse_record( self, lines ):\n        \"\"\"\n        Parse a TRANSFAC record out of `lines` and return a motif.\n        \"\"\"\n        # Break lines up\n        temp_lines = []\n        for line in lines:\n            fields = line.rstrip( \"\\r\\n\" ).split( None, 1 )\n            if len( fields ) == 1:\n                fields.append( \"\" )\n            temp_lines.append( fields )\n        lines = temp_lines\n        # Fill in motif from lines\n        motif = TransfacMotif()\n        current_line = 0\n        while 1:\n            # Done parsing if no more lines to consume\n            if current_line >= len( lines ):\n                break\n            # Remove prefix and first separator from line\n            prefix, rest = lines[ current_line ]\n            # No action for this prefix, just ignore the line\n            if prefix not in self.parse_actions:\n                current_line += 1\n                continue\n            # Get action for line\n            action = self.parse_actions[ prefix ]\n            # Store a single line value\n            if action[0] == \"store_single\":\n                key = action[1]\n                setattr( motif, key, rest )\n                current_line += 1\n            # Add a single line value to a list\n            if action[0] == \"store_single_list\":\n                key = action[1]\n                if not getattr( motif, key ):\n                    setattr( motif, key, [] )\n                getattr( motif, key ).append( rest )\n                current_line += 1\n            # Add a single line value to a dictionary\n            if action[0] == \"store_single_key_value\":\n                key = action[1]\n                k, v = rest.strip().split( '=', 1 )\n                if not getattr( motif, key ):\n                    setattr( motif, key, {} )\n                getattr( motif, key )[k] = v\n                current_line += 1\n            # Store a block of text\n            if action[0] == \"store_block\":\n                key = action[1]\n                value = []\n                while current_line < len( lines ) and lines[ current_line ][0] == prefix:\n                    value.append( lines[current_line][1] )\n                    current_line += 1\n                setattr( motif, key, str.join( \"\\n\", value ) )\n            # Store a matrix\n            if action[0] == \"store_matrix\":\n                # First line is alphabet\n                alphabet = rest.split()\n                alphabet_size = len( alphabet )\n                rows = []\n                pattern = \"\"\n                current_line += 1\n                # Next lines are the rows of the matrix (we allow 0 rows)\n                while current_line < len( lines ):\n                    prefix, rest = lines[ current_line ]\n                    # Prefix should be a two digit 0 padded row number\n                    if not prefix.isdigit():\n                        break\n                    # The first `alphabet_size` fields are the row values\n                    values = rest.split()\n                    rows.append( [ float(_) for _ in values[:alphabet_size] ] )\n                    # TRANSFAC includes an extra column with the IUPAC code\n                    if len( values ) > alphabet_size:\n                        pattern += values[alphabet_size]\n                    current_line += 1\n                # Only store the pattern if it is the correct length (meaning\n                # that every row had an extra field)\n                if len( pattern ) != len( rows ):\n                    pattern = None\n                matrix = FrequencyMatrix.from_rows( alphabet, rows )\n                setattr( motif, action[1], matrix )\n        # Only return a motif if we saw at least ID or AC or NA\n        if motif.id or motif.accession or motif.name:\n            return motif"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncloning a bitset containing only the given bits.", "response": "def bit_clone( bits ):\n    \"\"\"\n    Clone a bitset\n    \"\"\"\n    new = BitSet( bits.size )\n    new.ior( bits )\n    return new"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying multiple times to run throw_random", "response": "def throw_random( lengths, mask ):\n    \"\"\"\n    Try multiple times to run 'throw_random'\n    \"\"\"\n    saved = None\n    for i in range( maxtries ):\n        try:\n            return throw_random_bits( lengths, mask )\n        except MaxtriesException as e:\n            saved = e\n            continue\n    raise e"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_bits( region_start, region_length, intervals ):\n    bits = BitSet( region_length )\n    for chr, start, stop in intervals:\n        bits.set_range( start - region_start, stop - start )\n    return bits", "response": "Convert a set of intervals overlapping a region of a chromosome into a bitset of the given length."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the length distribution of all contiguous runs of set bits from", "response": "def interval_lengths( bits ):\n    \"\"\"\n    Get the length distribution of all contiguous runs of set bits from\n    \"\"\"\n    end = 0\n    while 1:\n        start = bits.next_set( end )\n        if start == bits.size: break\n        end = bits.next_clear( start )\n        yield end - start"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef count_overlap( bits1, bits2 ):\n    b = BitSet( bits1.size )\n    b |= bits1\n    b &= bits2\n    return b.count_range( 0, b.size )", "response": "Count the number of bits that overlap between two sets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef overlapping_in_bed( fname, r_chr, r_start, r_stop ):\n    rval = []\n    for line in open( fname ):\n        if line.startswith( \"#\" ) or line.startswith( \"track\" ):\n            continue\n        fields = line.split()\n        chr, start, stop = fields[0], int( fields[1] ), int( fields[2] )\n        if chr == r_chr and start < r_stop and stop >= r_start:\n            rval.append( ( chr, max( start, r_start ), min( stop, r_stop ) ) )\n    return rval", "response": "Get from a bed all intervals that overlap the region defined by r_chr r_start r_stop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tile_interval( sources, index, ref_src, start, end, seq_db=None ):\n    # First entry in sources should also be on the reference species\n    assert sources[0].split('.')[0] == ref_src.split('.')[0], \\\n        \"%s != %s\" % ( sources[0].split('.')[0], ref_src.split('.')[0] )\n    base_len = end - start\n    blocks = index.get( ref_src, start, end )\n    # From low to high score\n    blocks.sort(key=lambda t: t.score)\n    mask = [ -1 ] * base_len\n    ref_src_size = None\n    for i, block in enumerate( blocks ):\n        ref = block.get_component_by_src_start( ref_src )\n        ref_src_size = ref.src_size\n        assert ref.strand == \"+\"\n        slice_start = max( start, ref.start )\n        slice_end = min( end, ref.end )\n        for j in range( slice_start, slice_end ):\n            mask[j-start] = i\n    tiled = []\n    for i in range( len( sources ) ):\n        tiled.append( [] )\n    for ss, ee, index in intervals_from_mask( mask ):\n        # Interval with no covering alignments\n        if index < 0:\n            # Get sequence if available, otherwise just use 'N'\n            if seq_db:\n                tiled[0].append( bx.seq.nib.NibFile( open( seq_db[ ref_src ] ) ).get( start+ss, ee-ss ) )\n            else:\n                tiled[0].append( \"N\" * (ee-ss) )\n            # Gaps in all other species\n            for row in tiled[1:]:\n                row.append( \"-\" * ( ee - ss ) )\n        else:\n            slice_start = start + ss\n            slice_end = start + ee\n            block = blocks[index]\n            ref = block.get_component_by_src_start( ref_src )\n            sliced = block.slice_by_component( ref, slice_start, slice_end )\n            sliced = sliced.limit_to_species( sources )\n            sliced.remove_all_gap_columns()\n            for i, src in enumerate( sources ):\n                comp = sliced.get_component_by_src_start( src )\n                if comp:\n                    tiled[i].append( comp.text )\n                else:\n                    tiled[i].append( \"-\" * sliced.text_size )\n    return [ \"\".join( t ) for t in tiled ]", "response": "Tile maf blocks onto an interval."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef MafMotifSelect(mafblock,pwm,motif=None,threshold=0):\n\n    if motif != None and len(motif) != len(pwm): \n        raise Exception(\"pwm and motif must be the same length\")\n    # generic alignment\n    alignlist = [ c.text for c in mafblock.components ]\n    align = pwmx.Align( alignlist )\n    nrows,ncols = align.dims\n    #chr,chr_start,chr_stop = align.headers[0]\n    # required sequence length\n    minSeqLen = len( motif )\n    # record the text sizes from the alignment rows\n    align_match_lens = []\n\n    for start in range(ncols - minSeqLen):\n        if align.rows[0][start] == '-': continue\n        subseq = \"\"\n        pwm_score_vec = []\n        motif_score_vec = []\n        max_cols = 0\n        for ir in range(nrows):\n            expanded = align.rows[ir].count( '-', start, minSeqLen)\n            subtext = align.rows[ir][ start : minSeqLen+expanded ]\n            max_cols = max( len(subtext), max_cols )\n            subseq = subtext.replace('-','')\n            revseq = pwmx.reverse_complement(subseq)\n            # pwm score\n            nill,f_score = pwm.score_seq( subseq )[0]\n            r_score, nill = pwm.score_seq( revseq )[0]\n            pwm_score_vec.append( max(f_score, r_score) )\n            # consensus score\n            if motif is not None:\n                for_score = int( pwmx.match_consensus(subseq,motif) )\n                rev_score = int( pwmx.match_consensus(revseq,motif) )\n                motif_score_vec.append( max(for_score, rev_score) )\n        #check threshold\n        try:\n            assert not isnan(max(pwm_score_vec) )\n            assert not isnan(max(motif_score_vec) )\n        except:\n            print(pwm_score_vec, motif_score_vec, file=sys.stderr)\n            print(len(subseq), len(pwm), file=sys.stderr)\n        if max(pwm_score_vec) < threshold: continue\n        if max(motif_score_vec) < threshold: continue\n        # chop block\n        col_start = start\n        col_end = max_cols + 1\n        motifmaf = mafblock.slice( col_start, col_end )\n        yield motifmaf, pwm_score_vec, motif_score_vec\n                \n    \"\"\"\n    for ir in range(nrows):\n        # scan alignment row for motif subsequences\n        for start in range(ncols):\n            if align.rows[ir][start] == '-': continue\n            elif align.rows[ir][start] == 'n': continue\n            elif align.rows[ir][start] == 'N': continue\n            # gather enough subseq for motif\n            for ic in range(start,ncols):\n                char = align.rows[ir][ic].upper()\n                if char == '-' or char == 'N': continue\n                else: subseq += char\n                if len(subseq) == minSeqLen: \n                    revseq = pwmx.reverse_complement( subseq )\n                    align_match_lens.append( ic )\n                    # pwm score\n                    nill,f_score = pwm.score_seq( subseq )[0]\n                    r_score, nill = pwm.score_seq( revseq )[0]\n                    pwm_score_vec.append( max(f_score, r_score) )\n                    # consensus score\n                    if motif is not None:\n                        for_score = int( pwmx.match_consensus(subseq,motif) )\n                        rev_score = int( pwmx.match_consensus(revseq,motif) )\n                        motif_score_vec.append( max(for_score, rev_score) )\n                    #check threshold\n                    try:\n                        assert not isnan(max(pwm_score_vec) )\n                        assert not isnan(max(motif_score_vec) )\n                    except:\n                        print >>sys.stderr, pwm_score_vec, motif_score_vec\n                        print >>sys.stderr, len(subseq), len(pwm)\n                    if max(pwm_score_vec) < threshold: continue\n                    if max(motif_score_vec) < threshold: continue\n                    # chop block\n                    col_start = start\n                    col_end = max( align_match_lens ) + 1\n                    motifmaf = mafblock.slice( col_start, col_end )\n\n                    print subseq,revseq,ic\n                    print align_match_lens\n                    yield motifmaf, pwm_score_vec, motif_score_vec\n        \"\"\"", "response": "Select the motif from the mafblock."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a pyparsing parser for newick format trees", "response": "def create_parser():\n    \"\"\"\n    Create a 'pyparsing' parser for newick format trees roughly based on the\n    grammar here:\n        http://evolution.genetics.washington.edu/phylip/newick_doc.html\n\n    Problems:\n        - Is a single leaf a valid tree?\n        - Branch length on root? Doesn't make sense to me, and forces the root\n          to be an edge.\n    \"\"\"\n    # Basic tokens\n    real = Combine( Word( \"+-\" + nums, nums ) +\n                    Optional( \".\" + Optional( Word( nums ) ) ) +\n                    Optional( CaselessLiteral( \"E\" ) + Word( \"+-\" + nums, nums ) ) )\n    lpar = Suppress( \"(\" )\n    rpar = Suppress( \")\" )\n    colon = Suppress( \":\" )\n    semi = Suppress( \";\" )\n    quot = Suppress( \"'\" )\n    # Labels are either unquoted or single quoted, if unquoted underscores will be replaced with spaces\n    quoted_label = QuotedString( \"'\", None, \"''\" ).setParseAction( lambda s, l, t: t[0] )\n    simple_label = Word( alphas + nums + \"_.\" ).setParseAction( lambda s, l, t: t[0].replace( \"_\", \" \" ) )\n    label = quoted_label | simple_label\n    # Branch length is a real number (note though that exponents are not in the spec!)\n    branch_length = real.setParseAction( lambda s, l, t: float( t[0] ) )\n    # Need to forward declare this due to circularity\n    node_list = Forward()\n    # A node might have an list of edges (for a subtree), a label, and/or a branch length\n    node = ( Optional( node_list, None ) + Optional( label, \"\" ) + Optional( colon + branch_length, None ) ) \\\n        .setParseAction( lambda s, l, t: Edge( t[2], Tree( t[1] or None, t[0] ) ) )\n    node_list << ( lpar + delimitedList( node ) + rpar ) \\\n        .setParseAction( lambda s, l, t: [ t.asList() ] )\n    # The root cannot have a branch length\n    tree = ( node_list + Optional( label, \"\" ) + semi )\\\n        .setParseAction( lambda s, l, t: Tree( t[1] or None, t[0] ) )\n    # Return the outermost element\n    return tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_fill_char( maf_status ):\n    ## assert maf_status not in ( maf.MAF_CONTIG_NESTED_STATUS, maf.MAF_NEW_NESTED_STATUS, \n    ##                            maf.MAF_MAYBE_NEW_NESTED_STATUS ), \\\n    ##     \"Nested rows do not make sense in a single coverage MAF (or do they?)\"\n    if maf_status in ( maf.MAF_NEW_STATUS, maf.MAF_MAYBE_NEW_STATUS, \n                       maf.MAF_NEW_NESTED_STATUS, maf.MAF_MAYBE_NEW_NESTED_STATUS ):\n        return \"*\"\n    elif maf_status in ( maf.MAF_INVERSE_STATUS, maf.MAF_INSERT_STATUS ):\n        return \"=\"\n    elif maf_status in ( maf.MAF_CONTIG_STATUS, maf.MAF_CONTIG_NESTED_STATUS ):\n        return \"#\"\n    elif maf_status == maf.MAF_MISSING_STATUS:\n        return \"X\"\n    else:\n        raise ValueError(\"Unknwon maf status\")", "response": "Returns the character that should be used to fill between blocks having a given status"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nguesses the fill character for the given left component and right component.", "response": "def guess_fill_char( left_comp, right_comp ):\n    \"\"\"\n    For the case where there is no annotated synteny we will try to guess it\n    \"\"\"\n    # No left component, obiously new\n    return \"*\"\n    # First check that the blocks have the same src (not just species) and \n    # orientation\n    if ( left_comp.src == right_comp.src and left_comp.strand != right_comp.strand ): \n        # Are they completely contiguous? Easy to call that a gap\n        if left_comp.end == right_comp.start: \n            return \"-\"\n        # TODO: should be able to make some guesses about short insertions\n        # here\n    # All other cases we have no clue about\n    return \"*\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_all_gap_columns( texts ):\n    seqs = [ list( t ) for t in texts ]\n    i = 0\n    text_size = len( texts[0] )\n    while i < text_size:\n        all_gap = True\n        for seq in seqs:\n            if seq[i] not in ( '-', '#', '*', '=', 'X', '@' ): \n                all_gap = False\n        if all_gap:\n            for seq in seqs: \n                del seq[i]\n            text_size -= 1\n        else:\n            i += 1\n    return [ ''.join( s ) for s in seqs ]", "response": "Remove any columns containing only gaps from alignment texts\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the cross product of the arguments", "response": "def cross_lists(*sets):\n    \"\"\"Return the cross product of the arguments\"\"\"\n    wheels = [iter(_) for _ in sets]\n    digits = [next(it) for it in wheels]\n    while True:\n        yield digits[:]\n        for i in range(len(digits)-1, -1, -1):\n            try:\n                digits[i] = next(wheels[i])\n                break\n            except StopIteration:\n                wheels[i] = iter(sets[i])\n                digits[i] = next(wheels[i])\n        else:\n            break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the length file and returns a hash from sequence name to length.", "response": "def read_lengths_file( name ):\n    \"\"\"\n    Returns a hash from sequence name to length.\n    \"\"\"\n\n    chrom_to_length = {}\n    f = file ( name, \"rt\" )\n    for line in f:\n        line = line.strip()\n        if line == '' or line[0] == '#': continue\n        try:\n            fields = line.split()\n            if len(fields) != 2: raise\n            chrom = fields[0]\n            length = int( fields[1] )\n        except:\n            raise ValueError(\"bad length file line: %s\" % line)\n        if chrom in chrom_to_length and length != chrom_to_length[chrom]:\n            raise ValueError(\"%s has more than one length!\" % chrom)\n        chrom_to_length[chrom] = length\n    f.close()\n    return chrom_to_length"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef IntervalReader( f ):\n    current_chrom = None\n    current_pos = None\n    current_step = None\n\n    # always for wiggle data\n    strand = '+'\n\n    mode = \"bed\"\n\n    for line in f:\n        if line.isspace() or line.startswith( \"track\" ) or line.startswith( \"#\" ) or line.startswith( \"browser\" ):\n            continue\n        elif line.startswith( \"variableStep\" ):\n            header = parse_header( line )\n            current_chrom = header['chrom']\n            current_pos = None\n            current_step = None\n            if 'span' in header: current_span = int( header['span'] )\n            else: current_span = 1\n            mode = \"variableStep\"\n        elif line.startswith( \"fixedStep\" ):\n            header = parse_header( line )\n            current_chrom = header['chrom']\n            current_pos = int( header['start'] ) - 1\n            current_step = int( header['step'] )\n            if 'span' in header: current_span = int( header['span'] )\n            else: current_span = 1\n            mode = \"fixedStep\"\n        elif mode == \"bed\":\n            fields = line.split()\n            if len( fields ) > 3:\n                if len( fields ) > 5:\n                    yield fields[0], int( fields[1] ), int( fields[2] ), fields[5], float( fields[3] )\n                else:\n                    yield fields[0], int( fields[1] ), int( fields[2] ), strand, float( fields[3] )\n        elif mode == \"variableStep\":\n            fields = line.split()\n            pos = int( fields[0] ) - 1\n            yield current_chrom, pos, pos + current_span, strand, float( fields[1] )\n        elif mode == \"fixedStep\":\n            yield current_chrom, current_pos, current_pos + current_span, strand, float( line.split()[0] )\n            current_pos += current_step\n        else:\n            raise ValueError(\"Unexpected input line: %s\" % line.strip())", "response": "Iterator yielding chrom start end strand value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_and_unpack( self, format, byte_count=None ):\n        pattern = \"%s%s\" % ( self.endian_code, format )\n        if byte_count is None:\n            byte_count = struct.calcsize( pattern )\n        return struct.unpack( pattern, self.file.read( byte_count ) )", "response": "Read enough bytes to unpack according to format and return the tuple of unpacked values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_c_string( self ):\n        rval = []\n        while 1:\n            ch = self.file.read(1)\n            assert len( ch ) == 1, \"Unexpected end of file\"\n            if ch == b'\\0':\n                break\n            rval.append( ch )\n        return b''.join( rval )", "response": "Read a zero terminated C style string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pack_and_write( self, format, value ):\n        pattern = \"%s%s\" % ( self.endian_code, format )\n        return self.file.write( struct.pack( pattern, value ) )", "response": "Write the value to the file according to format and return the packed tuple of the packed values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_c_string( self, value ):\n        self.file.write( value )\n        self.file.write( b'\\0' )", "response": "Write a C string to the underlying file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding the last element of the list of blocks that are adjacent to each other.", "response": "def fuse_list( mafs ):\n    \"\"\"\n    Try to fuse a list of blocks by progressively fusing each adjacent pair.\n    \"\"\"\n    last = None\n    for m in mafs:\n        if last is None:\n            last = m\n        else:\n            fused = fuse( last, m )\n            if fused:\n                last = fused\n            else:\n                yield last\n                last = m\n    if last:\n        yield last"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattempt to fuse two blocks. Returns a new block if they can be fused returns None otherwise.", "response": "def fuse( m1, m2 ):\n    \"\"\"\n    Attempt to fuse two blocks. If they can be fused returns a new block, \n    otherwise returns None.\n      \n    Example:\n      \n    >>> import bx.align.maf\n      \n    >>> block1 = bx.align.maf.from_string( '''\n    ... a score=0.0\n    ... s hg18.chr10 52686 44 + 135374737 GTGCTAACTTACTGCTCCACAGAAAACATCAATTCTGCTCATGC\n    ... s panTro1.chrUn_random 208115356 44 - 240967748 GTGCTAACTGACTGCTCCAGAGAAAACATCAATTCTGTTCATGT\n    ... ''' )\n    \n    >>> block2 = bx.align.maf.from_string( '''\n    ... a score=0.0\n    ... s hg18.chr10 52730 69 + 135374737 GCAGGTACAATTCATCAAGAAAGGAATTACAACTTCAGAAATGTGTTCAAAATATATCCATACTTTGAC\n    ... s panTro1.chrUn_random 208115400 69 - 240967748 GCAGCTACTATTCATCAAGAAAGGGATTACAACTTCAGAAATGTGTTCAAAGTGTATCCATACTTTGAT\n    ... ''' )\n    \n    >>> fused = fuse( block1, block2 )\n    \n    >>> print(fused)\n    a score=0.0\n    s hg18.chr10 52686 113 + 135374737 GTGCTAACTTACTGCTCCACAGAAAACATCAATTCTGCTCATGCGCAGGTACAATTCATCAAGAAAGGAATTACAACTTCAGAAATGTGTTCAAAATATATCCATACTTTGAC\n    s panTro1.chrUn_random 208115356 113 - 240967748 GTGCTAACTGACTGCTCCAGAGAAAACATCAATTCTGTTCATGTGCAGCTACTATTCATCAAGAAAGGGATTACAACTTCAGAAATGTGTTCAAAGTGTATCCATACTTTGAT\n    <BLANKLINE>\n    \"\"\"\n    # Check if the blocks are adjacent, return none if not.\n    if len( m1.components ) != len( m2.components ): return None\n    for c1, c2 in zip( m1.components, m2.components ):\n        if c1.src != c2.src: return None\n        if c1.strand != c2.strand: return None\n        if c1.end != c2.start: return None\n    # Try to fuse:\n    n = deepcopy( m1 )\n    for c1, c2 in zip( n.components, m2.components ):\n        c1.text += c2.text\n        c1.size += c2.size\n    n.text_size = len( n.components[0].text )\n    return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a nested list expression that can be used to parse the nested list of items within the specified nested list.", "response": "def nestedExpr(opener=\"(\", closer=\")\", content=None, ignoreExpr=quotedString):\n    \"\"\"Helper method for defining nested lists enclosed in opening and closing\n       delimiters (\"(\" and \")\" are the default).\n\n       Parameters:\n        - opener - opening character for a nested list (default=\"(\"); can also be a pyparsing expression\n        - closer - closing character for a nested list (default=\")\"); can also be a pyparsing expression\n        - content - expression for items within the nested lists (default=None)\n        - ignoreExpr - expression for ignoring opening and closing delimiters (default=quotedString)\n\n       If an expression is not provided for the content argument, the nested\n       expression will capture all whitespace-delimited content between delimiters\n       as a list of separate values.\n\n       Use the ignoreExpr argument to define expressions that may contain\n       opening or closing characters that should not be treated as opening\n       or closing characters for nesting, such as quotedString or a comment\n       expression.  Specify multiple expressions using an Or or MatchFirst.\n       The default is quotedString, but if no expressions are to be ignored,\n       then pass None for this argument.\n    \"\"\"\n    if opener == closer:\n        raise ValueError(\"opening and closing strings cannot be the same\")\n    if content is None:\n        if isinstance(opener,basestring) and isinstance(closer,basestring):\n            if ignoreExpr is not None:\n                content = (Combine(OneOrMore(~ignoreExpr +\n                                CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS,exact=1))\n                            ).setParseAction(lambda t:t[0].strip()))\n            else:\n                content = (empty+CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS).setParseAction(lambda t:t[0].strip()))\n        else:\n            raise ValueError(\"opening and closing arguments must be strings if no content expression is given\")\n    ret = Forward()\n    if ignoreExpr is not None:\n        ret << Group( Suppress(opener) + ZeroOrMore( ignoreExpr | ret | content ) + Suppress(closer) )\n    else:\n        ret << Group( Suppress(opener) + ZeroOrMore( ret | content )  + Suppress(closer) )\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _normalizeParseActionArgs( f ):\n        STAR_ARGS = 4\n\n        try:\n            restore = None\n            if isinstance(f,type):\n                restore = f\n                f = f.__init__\n            if not _PY3K:\n                codeObj = f.func_code\n            else:\n                codeObj = f.code\n            if codeObj.co_flags & STAR_ARGS:\n                return f\n            numargs = codeObj.co_argcount\n            if not _PY3K:\n                if hasattr(f,\"im_self\"):\n                    numargs -= 1\n            else:\n                if hasattr(f,\"__self__\"):\n                    numargs -= 1\n            if restore:\n                f = restore\n        except AttributeError:\n            try:\n                if not _PY3K:\n                    call_im_func_code = f.__call__.im_func.func_code\n                else:\n                    call_im_func_code = f.__code__\n\n                # not a function, must be a callable object, get info from the\n                # im_func binding of its bound __call__ method\n                if call_im_func_code.co_flags & STAR_ARGS:\n                    return f\n                numargs = call_im_func_code.co_argcount\n                if not _PY3K:\n                    if hasattr(f.__call__,\"im_self\"):\n                        numargs -= 1\n                else:\n                    if hasattr(f.__call__,\"__self__\"):\n                        numargs -= 0\n            except AttributeError:\n                if not _PY3K:\n                    call_func_code = f.__call__.func_code\n                else:\n                    call_func_code = f.__call__.__code__\n                # not a bound method, get info directly from __call__ method\n                if call_func_code.co_flags & STAR_ARGS:\n                    return f\n                numargs = call_func_code.co_argcount\n                if not _PY3K:\n                    if hasattr(f.__call__,\"im_self\"):\n                        numargs -= 1\n                else:\n                    if hasattr(f.__call__,\"__self__\"):\n                        numargs -= 1\n\n\n        #~ print (\"adding function %s with %d args\" % (f.func_name,numargs))\n        if numargs == 3:\n            return f\n        else:\n            if numargs > 3:\n                def tmp(s,l,t):\n                    return f(f.__call__.__self__, s,l,t)\n            if numargs == 2:\n                def tmp(s,l,t):\n                    return f(l,t)\n            elif numargs == 1:\n                def tmp(s,l,t):\n                    return f(t)\n            else: #~ numargs == 0:\n                def tmp(s,l,t):\n                    return f()\n            try:\n                tmp.__name__ = f.__name__\n            except (AttributeError,TypeError):\n                # no need for special handling if attribute doesnt exist\n                pass\n            try:\n                tmp.__doc__ = f.__doc__\n            except (AttributeError,TypeError):\n                # no need for special handling if attribute doesnt exist\n                pass\n            try:\n                tmp.__dict__.update(f.__dict__)\n            except (AttributeError,TypeError):\n                # no need for special handling if attribute doesnt exist\n                pass\n            return tmp", "response": "Internal method used to decorate parse actions that take fewer than 3 arguments f and return f."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute the parse expression with the given string.", "response": "def parseString( self, instring, parseAll=False ):\n        \"\"\"Execute the parse expression with the given string.\n           This is the main interface to the client code, once the complete\n           expression has been built.\n\n           If you want the grammar to require that the entire input string be\n           successfully parsed, then set parseAll to True (equivalent to ending\n           the grammar with StringEnd()).\n\n           Note: parseString implicitly calls expandtabs() on the input string,\n           in order to report proper column numbers in parse actions.\n           If the input string contains tabs and\n           the grammar uses parse actions that use the loc argument to index into the\n           string being parsed, you can ensure you have a consistent view of the input\n           string by:\n            - calling parseWithTabs on your grammar before calling parseString\n              (see L{I{parseWithTabs}<parseWithTabs>})\n            - define your parse action using the full (s,loc,toks) signature, and\n              reference the input string using the parse action's s argument\n            - explictly expand the tabs in your input string before calling\n              parseString\n        \"\"\"\n        ParserElement.resetCache()\n        if not self.streamlined:\n            self.streamline()\n            #~ self.saveAsList = True\n        for e in self.ignoreExprs:\n            e.streamline()\n        if not self.keepTabs:\n            instring = instring.expandtabs()\n        loc, tokens = self._parse( instring, 0 )\n        if parseAll:\n            StringEnd()._parse( instring, loc )\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transformString( self, instring ):\n        out = []\n        lastE = 0\n        # force preservation of <TAB>s, to minimize unwanted transformation of string, and to\n        # keep string locs straight between transformString and scanString\n        self.keepTabs = True\n        for t,s,e in self.scanString( instring ):\n            out.append( instring[lastE:s] )\n            if t:\n                if isinstance(t,ParseResults):\n                    out += t.asList()\n                elif isinstance(t,list):\n                    out += t\n                else:\n                    out.append(t)\n            lastE = e\n        out.append(instring[lastE:])\n        return \"\".join(map(_ustr,out))", "response": "Extension to scanString to modify matching text with modified tokens that may be returned from a parse action."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef searchString( self, instring, maxMatches=_MAX_INT ):\n        return ParseResults([ t for t,s,e in self.scanString( instring, maxMatches ) ])", "response": "A simple string search that returns a list of ParseResults that contains the tokens found by the given parse expression."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parseFile( self, file_or_filename ):\n        try:\n            file_contents = file_or_filename.read()\n        except AttributeError:\n            f = open(file_or_filename, \"rb\")\n            file_contents = f.read()\n            f.close()\n        return self.parseString(file_contents)", "response": "Execute the parse expression on the given file or filename."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_from_epo(cls, trg_comp, qr_comp, trg_chrom_sizes, qr_chrom_sizes):\n\n        # size, target, query arrays\n        S, T, Q = [], [], []\n\n        #the target strand of the chain must be on the forward strand\n        trg_intervals = trg_comp.intervals(reverse = trg_comp.strand == '-')\n        qr_intervals = qr_comp.intervals(reverse = trg_comp.strand == '-')\n        if len(trg_intervals) == 0 or len(qr_intervals) == 0:\n            log.warning(\"deletion/insertion only intervals\")\n            return None\n        A, B = rem_dash(trg_intervals, qr_intervals)\n        # correct for when cigar starts/ends with dashes (in number of bases)\n        tr_start_correction = max(B[0][0] - A[0][0], 0)\n        tr_end_correction = max(A[-1][1] - B[-1][1], 0)\n        qr_start_correction = max(A[0][0] - B[0][0], 0)\n        qr_end_correction = max(B[-1][1] - A[-1][1], 0)\n\n        a, b = A.pop(0), B.pop(0)\n\n        # intervals are 0-base, halfo-open => lengths = coordinate difference\n        while A or B:\n            if a[1] < b[1]:\n                T.append(0); Q.append( A[0][0] - a[1] ); S.append( min(a[1], b[1]) - max(a[0], b[0]) )\n                a = A.pop(0)\n            elif b[1] < a[1]:\n                Q.append(0); T.append( B[0][0] - b[1] ); S.append( min(a[1], b[1]) - max(a[0], b[0]) )\n                b = B.pop(0)\n            elif A and B:\n                assert 1 > 2, \"there are dash columns\"\n            else:\n                break\n        S.append( min(a[1], b[1]) - max(a[0], b[0]) )\n        assert len(T) == len(Q) == len(S) - 1, \"(S, T, Q) = (%d, %d, %d)\" % tuple(map(len, (S, T, Q)))\n\n        tSize = trg_chrom_sizes[trg_comp.chrom]\n        qSize = qr_chrom_sizes[qr_comp.chrom]\n        ## UCSC coordinates are 0-based, half-open and e! coordinates are 1-base, closed\n        ## chain_start = epo_start - 1 and chain_end = epo_end\n        if qr_comp.strand == '+':\n            chain = Chain(0,\n                    trg_comp.chrom, tSize, \"+\",\n                    (trg_comp.start - 1) + tr_start_correction, trg_comp.end  - tr_end_correction,\n                    qr_comp.chrom, qSize, (qr_comp.strand == trg_comp.strand and '+' or '-'),\n                    (qr_comp.start - 1) + qr_start_correction, qr_comp.end  - qr_end_correction,\n                    qr_comp.gabid)\n        else:\n            chain = Chain(0,\n                    trg_comp.chrom, tSize, \"+\",\n                    (trg_comp.start - 1) + tr_start_correction, trg_comp.end - tr_end_correction,\n                    qr_comp.chrom, qSize, (qr_comp.strand == trg_comp.strand and '+' or '-'),\n                    (qr_comp.start - 1) + qr_end_correction, qr_comp.end - qr_start_correction,\n                    qr_comp.gabid)\n\n        # strand correction. in UCSC coordinates this is: size - coord\n        if chain.qStrand == '-':\n            chain = chain._replace(qEnd = chain.qSize - chain.qStart,\n                    qStart = chain.qSize - chain.qEnd)\n\n        assert chain.tEnd - chain.tStart  == sum(S) + sum(T), \"[%s] %d != %d\" % (str(chain),\n                chain.tEnd - chain.tStart, sum(S) + sum(T))\n        assert chain.qEnd - chain.qStart == sum(S) + sum(Q), \"[%s] %d != %d\" % (str(chain),\n                chain.qEnd - chain.qStart, sum(S) + sum(Q))\n        return chain, S, T, Q", "response": "crate a chain of collinear rings from the given components."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the slice entry ( in a bed6 format AS IS in the chain header", "response": "def slice(self, who):\n        \"return the slice entry (in a bed6 format), AS IS in the chain header\"\n\n        assert who in ('t', 'q'), \"who should be 't' or 'q'\"\n\n        if who == 't':\n            return (self.tName, self.tStart, self.tEnd, self.id, self.score, self.tStrand)\n        else:\n            return (self.qName, self.qStart, self.qEnd, self.id, self.score, self.qStrand)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bedInterval(self, who):\n        \"return a BED6 entry, thus DOES coordinate conversion for minus strands\"\n\n        if who == 't':\n            st, en = self.tStart, self.tEnd\n            if self.tStrand == '-':\n                st, en = self.tSize-en, self.tSize-st\n            return (self.tName, st, en, self.id, self.score, self.tStrand)\n        else:\n            st, en = self.qStart, self.qEnd\n            if self.qStrand == '-':\n                st, en = self.qSize-en, self.qSize-st\n                assert en-st == self.qEnd - self.qStart\n            return (self.qName, st, en, self.id, self.score, self.qStrand)", "response": "return a BED6 entry thus DOES coordinate conversion for minus strands"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_file(cls, path, pickle=False):\n\n        fname = path\n        if fname.endswith(\".gz\"):\n            fname = path[:-3]\n\n        if fname.endswith('.pkl'):\n            #you asked for the pickled file. I'll give it to you\n            log.debug(\"loading pickled file %s ...\" % fname)\n            return cPickle.load( open(fname, \"rb\") )\n        elif os.path.isfile(\"%s.pkl\" % fname):\n            #there is a cached version I can give to you\n            log.info(\"loading pickled file %s.pkl ...\" % fname)\n            if os.stat(path).st_mtime > os.stat(\"%s.pkl\" % fname).st_mtime:\n                log.critical(\"*** pickled file %s.pkl is not up to date ***\" % (path))\n            return cPickle.load( open(\"%s.pkl\" % fname, \"rb\") )\n\n        data = fastLoadChain(path, cls._strfactory)\n        if pickle and not os.path.isfile('%s.pkl' % fname):\n            log.info(\"pckling to %s.pkl\" % (fname))\n            with open('%s.pkl' % fname, 'wb') as fd:\n                cPickle.dump(data, fd)\n        return data", "response": "parse a. chain file into a list of the type L { Chain }..."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_epo(cls, fname):\n\n        data = {}\n        with open(fname) as fd:\n            for el in (cls._strfactory(_) for _ in fd):\n                if el:\n                    data.setdefault(el.gabid, []).append( el )\n        log.info(\"parsed %d elements from %s\" % (len(data), fname))\n        return data", "response": "Load an entire EPO file into a dictionary of the type gab_id = > list of Epoitems"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates the cigar entry", "response": "def cigar_iter(self, reverse):\n        \"\"\"self.cigar => [(length, type) ... ] iterate the cigar\n\n        :param reverse: whether to iterate in the reverse direction (right-to-left)\n        :type reverse: boolean\n\n        :return a list of pairs of the type [(length, M/D) ..]\n        \"\"\"\n\n        l = 0\n        P = self.cigar_pattern\n\n        data = []\n        cigar = self.cigar\n        parsed_cigar = re.findall(P, cigar)\n        if reverse:\n            parsed_cigar = parsed_cigar[::-1]\n        for _l, t in parsed_cigar:\n            # 1M is encoded as M\n            l = (_l and int(_l) or 1) # int(_l) cannot be 0\n            data.append( (l, t) )\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef intervals(self, reverse, thr=0):\n\n        d = [(thr,thr)]\n        dl = 0\n        for tup in self.cigar_iter(reverse):\n            if tup[1] == \"D\":\n                dl = tup[0]\n            else:\n                s = d[-1][1] + dl\n                d.append( (s, s+tup[0]) )\n\n        assert d[0] == (thr, thr)\n        # assert that nr. of Ms in the interval == sum of produced intervals\n        assert sum( t[0] for t in self.cigar_iter(False) if t[1] == \"M\" ) == sum( t[1]-t[0] for t in d )\n\n        d_sum = sum( t[1]-t[0] for t in d )\n        assert self.end - self.start + 1 == d_sum, \"[ (%d, %d) = %d ] != %d\" % (self.start, self.end,\n                self.end-self.start+1, d_sum)\n        return d[1:]", "response": "return a list of half - open intervals representing the match regions of the cigar."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\njoining together alignment blocks to create a semi human projected local alignment (small reference sequence deletions are kept as supported by the local alignment).", "response": "def do_interval( sources, index, out, ref_src, start, end, seq_db, missing_data, strand ):\n    \"\"\"\n    Join together alignment blocks to create a semi human projected local \n    alignment (small reference sequence deletions are kept as supported by \n    the local alignment).\n    \"\"\"\n    ref_src_size = None\n    # Make sure the reference component is also the first in the source list\n    assert sources[0].split('.')[0] == ref_src.split('.')[0], \"%s != %s\" \\\n        % ( sources[0].split('.')[0], ref_src.split('.')[0] )\n    # Determine the overall length of the interval\n    base_len = end - start\n    # Counter for the last reference species base we have processed\n    last_stop = start\n    # Rows in maf blocks come in in arbitrary order, we'll convert things\n    # to the destred order of the tiled block\n    source_to_index = dict( ( name, i ) for ( i, name ) in enumerate( sources ) )\n    # This gets all the maf blocks overlapping our interval of interest\n    # NOTE: Unlike maf_tile we're expecting \n    # things to be single coverage in the reference species, so we won't \n    # sort by score and lay down.\n    blocks = index.get( ref_src, start, end )\n    # The last component seen for each species onto which we are tiling\n    last_components = [ None ] * len( sources )\n    last_status = [ None ] * len( sources )\n    cols_needing_fill = [ 0 ] * len( sources )\n    # The list of strings in which we build up the tiled alignment\n    tiled_rows = [ \"\" for i in range( len( sources ) ) ]\n    # Enumerate the (ordered) list of blocks\n    for i, block in enumerate( blocks ):\n        # Check for overlap in reference species\n        ref = block.get_component_by_src_start( ref_src )\n        if ref.start < last_stop:\n            if ref.end < last_stop: \n                continue\n            block = block.slice_by_component( ref, last_stop, min( end, ref.end ) )\n            ref = block.get_component_by_src_start( ref_src )\n        block = block.slice_by_component( ref, max( start, ref.start ), min( end, ref.end ) )\n        ref = block.get_component_by_src_start( ref_src )\n        # print block\n        assert last_components[0] is None or ref.start >= last_components[0].end, \\\n            \"MAF must be sorted and single coverage in reference species!\"\n        assert ref.strand == \"+\", \\\n            \"MAF must have all reference species blocks on the plus strand\"\n        # Store the size of the reference sequence for building fake block   \n        if ref_src_size is None:\n            ref_src_size = ref.src_size\n        # Handle the reference component seperately, it has no synteny status\n        # but we will try to fill in missing sequence\n        if ref.start > last_stop:\n            # Need to fill in some reference sequence\n            chunk_len = ref.start - last_stop\n            text = bx.seq.nib.NibFile( open( seq_db[ ref_src ] ) ).get( last_stop, chunk_len ) \n            tiled_rows[0] += text\n            for source in sources[1:]:\n                cols_needing_fill[ source_to_index[ source ] ] += chunk_len\n        # Do reference component\n        chunk_len = len( ref.text )\n        tiled_rows[0] += ref.text\n        # Do each other component\n        for source in sources[1:]:\n            source_index = source_to_index[ source ]\n            comp = block.get_component_by_src_start( source )\n            if comp:\n                if comp.synteny_left is None:\n                    left_status, left_length = None, -1\n                else:\n                    left_status, left_length = comp.synteny_left\n                if comp.synteny_right is None:\n                    right_status, right_length = None, -1\n                else:\n                    right_status, right_length = comp.synteny_right\n                # We have a component, do we need to do some filling?\n                cols_to_fill = cols_needing_fill[ source_index ]\n                if cols_to_fill > 0:\n                    # Adjacent components should have matching status\n                    ## assert last_status[ source_index ] is None or last_status[ source_index ] == left_status, \\\n                    ##     \"left status (%s) does not match right status (%s) of last component for %s\" \\\n                    ##         % ( left_status, last_status[ source_index ], source )\n                    if left_status is None:\n                        fill_char = guess_fill_char( last_components[source_index], comp )\n                    else:\n                        fill_char = get_fill_char( left_status )\n                    tiled_rows[ source_index ] += ( fill_char * cols_to_fill )\n                    cols_needing_fill[ source_index ] = 0\n                # Okay, filled up to current position, now append the text\n                tiled_rows[ source_index ] += comp.text\n                assert len( tiled_rows[ source_index ] ) == len( tiled_rows[ 0 ] ), \\\n                    \"length of tiled row should match reference row\"\n                last_components[ source_index ] = comp\n                last_status[ source_index ] = right_status\n            else:\n                # No component, we'll have to fill this region when we know\n                # the status\n                cols_needing_fill[ source_index ] += chunk_len\n        last_stop = ref.end\n    # No more components, clean up the ends\n    if last_stop < end:\n        # Need to fill in some reference sequence\n        chunk_len = end - last_stop\n        tiled_rows[0] += bx.seq.nib.NibFile( open( seq_db[ ref_src ] ) ).get( last_stop, chunk_len ) \n        for source in sources[1:]:\n            cols_needing_fill[ source_to_index[ source ] ] += chunk_len\n    # Any final filling that needs to be done?\n    for source in sources[1:]:\n        source_index = source_to_index[ source ]\n        fill_needed = cols_needing_fill[ source_index ]\n        if fill_needed > 0:\n            if last_components[ source_index ] is None:\n                # print >>sys.stderr, \"Never saw any components for %s, filling with @\" % source\n                fill_char = '@'\n            else:\n                if last_status[ source_index ] is None:\n                    fill_char = '*'\n                else:\n                    fill_char = get_fill_char( last_status[ source_index ] )\n            tiled_rows[ source_index ] += fill_char * fill_needed\n        assert len( tiled_rows[ source_index ] ) == len( tiled_rows[ 0 ] ), \\\n            \"length of tiled row should match reference row\"\n    # Okay, now make up the fake alignment from the tiled rows.\n    tiled_rows = remove_all_gap_columns( tiled_rows )\n    a = align.Alignment()\n    for i, name in enumerate( sources ):\n        text = \"\".join( tiled_rows[i] )\n        size = len( text ) - text.count( \"-\" )\n        if i == 0:\n            if ref_src_size is None: ref_src_size = bx.seq.nib.NibFile( open( seq_db[ ref_src ] ) ).length\n            c = align.Component( ref_src, start, end-start, \"+\", ref_src_size, text )\n        else:\n            c = align.Component( name + \".fake\", 0, size, \"?\", size, text )\n        a.add_component( c )\n    if strand == '-':\n        a = a.reverse_complement()\n    out.write( a )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef binned_bitsets_from_file( f, chrom_col=0, start_col=1, end_col=2, strand_col=5, upstream_pad=0, downstream_pad=0, lens={} ):\n    last_chrom = None\n    last_bitset = None\n    bitsets = dict() \n    for line in f:\n        if line.startswith(\"#\") or line.isspace():\n            continue\n        fields = line.split()\n        strand = \"+\"\n        if len(fields) > strand_col:\n            if fields[strand_col] == \"-\": strand = \"-\"\n        chrom = fields[chrom_col]\n        if chrom != last_chrom:\n            if chrom not in bitsets:\n                if chrom in lens:\n                    size = lens[chrom]\n                else:\n                    size = MAX\n                bitsets[chrom] = BinnedBitSet( size ) \n            last_chrom = chrom\n            last_bitset = bitsets[chrom]\n        start, end = int( fields[start_col] ), int( fields[end_col] )\n        if upstream_pad: start = max( 0, start - upstream_pad )\n        if downstream_pad: end = min( size, end + downstream_pad )\n        if start > end: warn( \"Interval start after end!\" )\n        last_bitset.set_range( start, end-start )\n    return bitsets", "response": "Read a file into a dictionary of bitsets."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef binned_bitsets_proximity( f, chrom_col=0, start_col=1, end_col=2, strand_col=5, upstream=0, downstream=0 ):\n    last_chrom = None\n    last_bitset = None\n    bitsets = dict()\n    for line in f:\n        if line.startswith(\"#\"): continue\n#        print \"input=%s\" % ( line ),\n        fields = line.split()\n        strand = \"+\"\n        if len(fields) >= strand_col + 1:\n            if fields[strand_col] == \"-\": strand = \"-\"\n        chrom = fields[chrom_col]\n        if chrom != last_chrom:\n            if chrom not in bitsets:\n                bitsets[chrom] = BinnedBitSet( MAX )\n            last_chrom = chrom\n            last_bitset = bitsets[chrom]\n        start, end = int( fields[start_col] ), int( fields[end_col] )\n        if strand == \"+\":\n            if upstream: start = max( 0, start - upstream )\n            if downstream: end = min( MAX, end + downstream )\n        if strand == \"-\":\n            if upstream: end = min( MAX, end + upstream )\n            if downstream: start = max( 0, start - downstream )\n#        print \"set: start=%d\\tend=%d\" % ( start, end )\n        if end-start > 0:\n            last_bitset.set_range( start, end-start )\n    return bitsets", "response": "Read a file into a dictionary of bitsets with proximity between start and end columns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a list into a dictionary of bitsets", "response": "def binned_bitsets_from_list( list=[] ):\n    \"\"\"Read a list into a dictionary of bitsets\"\"\"\n    last_chrom = None\n    last_bitset = None\n    bitsets = dict()\n    for l in list:\n        chrom = l[0]\n        if chrom != last_chrom:\n            if chrom not in bitsets:\n                bitsets[chrom] = BinnedBitSet(MAX)\n            last_chrom = chrom\n            last_bitset = bitsets[chrom]\n        start, end = int( l[1] ), int( l[2] )\n        last_bitset.set_range( start, end - start )\n    return bitsets"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a file by chrom name into a bitset", "response": "def binned_bitsets_by_chrom( f, chrom, chrom_col=0, start_col=1, end_col=2):\n    \"\"\"Read a file by chrom name into a bitset\"\"\"\n    bitset = BinnedBitSet( MAX )\n    for line in f:\n        if line.startswith(\"#\"): continue\n        fields = line.split()\n        if fields[chrom_col] == chrom:\n            start, end = int( fields[start_col] ), int( fields[end_col] )\n            bitset.set_range( start, end-start )\n    return bitset"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a list of alignment blocks chop out the portion of the block that overlaps the interval [ start end ) in the sequence blocks.", "response": "def chop_list( blocks, src, start, end ):\n    \"\"\"\n    For each alignment block in the sequence `blocks`, chop out the portion\n    of the block that overlaps the interval [`start`,`end`) in the\n    component/species named `src`.\n    \"\"\"\n    new_blocks = []\n    for block in blocks: \n        ref = block.get_component_by_src( src )\n        # If the reference component is on the '-' strand we should complement the interval\n        if ref.strand == '-':\n            slice_start = max( ref.src_size - end, ref.start )\n            slice_end = max( ref.src_size - start, ref.end )\n        else:\n            slice_start = max( start, ref.start )\n            slice_end = min( end, ref.end )\n        sliced = block.slice_by_component( ref, slice_start, slice_end ) \n        good = True\n        for c in sliced.components: \n            if c.size < 1: \n                good = False\n        if good:\n            new_blocks.append( sliced )\n    return new_blocks"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _double_as_bytes(dval):\n    \"Use struct.unpack to decode a double precision float into eight bytes\"\n    tmp = list(struct.unpack('8B',struct.pack('d', dval)))\n    if not _big_endian:\n        tmp.reverse()\n    return tmp", "response": "Use struct. unpack to decode a double precision float into eight bytes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting the _mantissa bits from a double - precision floating - point value.", "response": "def _mantissa(dval):\n    \"\"\"Extract the _mantissa bits from a double-precision floating\n    point value.\"\"\"\n\n    bb = _double_as_bytes(dval)\n    mantissa =  bb[1] & 0x0f << 48\n    mantissa += bb[2] << 40\n    mantissa += bb[3] << 32\n    mantissa += bb[4]\n    return mantissa"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining whether the mantissa bits of the given double are all zero.", "response": "def _zero_mantissa(dval):\n    \"\"\"Determine whether the mantissa bits of the given double are all\n    zero.\"\"\"\n    bb = _double_as_bytes(dval)\n    return ((bb[1] & 0x0f) | reduce(operator.or_, bb[2:])) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a wiggle file and return a dict keyed by chromosome.", "response": "def load_scores_wiggle( fname ):\n    \"\"\"\n    Read a wiggle file and return a dict of BinnedArray objects keyed \n    by chromosome.\n    \"\"\"\n    scores_by_chrom = dict()\n    for chrom, pos, val in bx.wiggle.Reader( misc.open_compressed( fname ) ):\n        if chrom not in scores_by_chrom:\n            scores_by_chrom[chrom] = BinnedArray()\n        scores_by_chrom[chrom][pos] = val\n    return scores_by_chrom"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the subset of offsets needed to contain intervals over max_size.", "response": "def offsets_for_max_size( max_size ):\n    \"\"\"\n    Return the subset of offsets needed to contain intervals over (0,max_size)\n    \"\"\"\n    for i, max in enumerate( reversed( BIN_OFFSETS_MAX ) ):\n        if max_size < max:\n            break\n    else:\n        raise Exception( \"%d is larger than the maximum possible size (%d)\" % ( max_size, BIN_OFFSETS_MAX[0] ) )\n    return BIN_OFFSETS[ ( len(BIN_OFFSETS) - i - 1 ) : ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bin_for_range( start, end, offsets=None ):\n    if offsets is None:\n        offsets = BIN_OFFSETS\n    start_bin, end_bin = start, max(start, end - 1)\n    start_bin >>= BIN_FIRST_SHIFT\n    end_bin >>= BIN_FIRST_SHIFT\n    for offset in offsets:\n        if start_bin == end_bin:\n            return offset + start_bin\n        else:\n            start_bin >>= BIN_NEXT_SHIFT\n            end_bin >>= BIN_NEXT_SHIFT\n    raise Exceptionn(\"Interval (%d,%d) out of range\")", "response": "Find the smallest bin that can contain the given interval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an empty index for intervals in the range min max", "response": "def new( self, min, max ):\n        \"\"\"Create an empty index for intervals in the range min, max\"\"\"\n        # Ensure the range will fit given the shifting strategy\n        assert MIN <= min <= max <= MAX\n        self.min = min\n        self.max = max\n        # Determine offsets to use\n        self.offsets = offsets_for_max_size( max )\n        # Determine the largest bin we will actually use\n        self.bin_count = bin_for_range( max - 1, max, offsets = self.offsets ) + 1\n        # Create empty bins\n        self.bins = [ [] for i in range( self.bin_count ) ]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add( self, start, end, val ):\n        insort( self.bins[ bin_for_range( start, end, offsets=self.offsets ) ], ( start, end, val ) )\n        assert val >= 0\n        self.max_val = max(self.max_val,val)", "response": "Add the interval start end with associated value val to the index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef seek( self, offset, whence=0 ):\n        # Determine absolute target position\n        if whence == 0:\n            target_pos = offset\n        elif whence == 1:\n            target_pos = self.file_pos + offset\n        elif whence == 2:\n            target_pos = self.size - offset\n        else:\n            raise Exception( \"Invalid `whence` argument: %r\", whence )\n        # Check if this is a noop\n        if target_pos == self.file_pos:\n            return    \n        # Verify it is valid\n        assert 0 <= target_pos < self.size, \"Attempt to seek outside file\"\n        # Move the position\n        self.file_pos = target_pos\n        # Mark as dirty, the next time a read is done we need to actually\n        # move the position in the bzip2 file\n        self.dirty = True", "response": "Move the file pointer to a particular offset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mtime(self, key):\n        if key not in self.__dict:\n            raise CacheKeyError(key)\n        else:\n            node = self.__dict[key]\n            return node.mtime", "response": "Return the last modification time for the cache record with key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the calling class name and dictionary", "response": "def class_space(classlevel=3):\n    \"returns the calling class' name and dictionary\"\n    frame = sys._getframe(classlevel)\n    classname = frame.f_code.co_name\n    classdict = frame.f_locals\n    return classname, classdict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning one property for each key - value pair in kwds ;", "response": "def _attribute(permission='rwd', **kwds):\n    \"\"\"returns one property for each (key,value) pair in kwds;\n       each property provides the specified level of access(permission):\n           'r': readable, 'w':writable, 'd':deletable\n    \"\"\"\n    classname, classdict = class_space()\n    def _property(attrname, default):\n        propname, attrname = attrname, mangle(classname, attrname)\n        fget, fset, fdel, doc = None, None, None, propname\n        if 'r' in permission:\n            def fget(self):\n                value = default\n                try: value = getattr(self, attrname)\n                except AttributeError: setattr(self, attrname, default)\n                return value\n        if 'w' in permission:\n            def fset(self, value):\n                setattr(self, attrname, value)\n        if 'd' in permission:\n            def fdel(self): \n                try: delattr(self, attrname)\n                except AttributeError: pass\n                # calling fget can restore this attribute, so remove property \n                delattr(self.__class__, propname)\n        return property(fget=fget, fset=fset, fdel=fdel, doc=doc)\n        \n    for attrname, default in kwds.items():\n        classdict[attrname] = _property(attrname, default)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_a_stanza(self):\n\t\t# 's' line -- score, 1 field\n\t\tline = self.fetch_line(report=\" in a-stanza\")\n\t\tfields = line.split()\n\t\tassert (fields[0] == \"s\"), \"s line expected in a-stanza (line %d, \\\"%s\\\")\" \\\n\t\t\t\t\t\t\t\t % (self.lineNumber,line)\n\t\ttry:    score = int(fields[1])\n\t\texcept: score = float(fields[1])\n\n\t\t# 'b' line -- begin positions in seqs, 2 fields\n\t\tline = self.fetch_line(report=\" in a-stanza\")\n\t\tfields = line.split()\n\t\tassert (fields[0] == \"b\"), \"b line expected in a-stanza (line %d, \\\"%s\\\")\" \\\n\t\t\t\t\t\t\t\t % (self.lineNumber,line)\n\t\tbeg1 = int(fields[1]) - 1\n\t\tbeg2 = int(fields[2]) - 1\n\n\t\t# 'e' line -- end positions in seqs, 2 fields\n\t\tline = self.fetch_line(report=\" in a-stanza\")\n\t\tfields = line.split()\n\t\tassert (fields[0] == \"e\"), \"e line expected in a-stanza (line %d, \\\"%s\\\")\" \\\n\t\t\t\t\t\t\t\t % (self.lineNumber,line)\n\t\tlen1 = int(fields[1]) - beg1\n\t\tlen2 = int(fields[2]) - beg2\n\n\t\t# 'l' lines\n\t\tpieces = []\n\t\twhile (True):\n\t\t\tline = self.fetch_line(report=\" in a-stanza\")\n\t\t\tfields = line.split()\n\t\t\tif (fields[0] != \"l\"):\n\t\t\t\tbreak\n\t\t\tstart1  = int(fields[1]) - 1\n\t\t\tstart2  = int(fields[2]) - 1\n\t\t\tlength  = int(fields[3]) - start1\n\t\t\tlength2 = int(fields[4]) - start2\n\t\t\ttry:    pctId = int(fields[5])\n\t\t\texcept: pctId = float(fields[5])\n\t\t\tassert (length2 == length), \"length mismatch in a-stanza\"\n\t\t\tpieces.append((start1+self.seq1_start,start2+self.seq2_start,length,pctId))\n\t\tassert (line == \"}\"), \"improper a-stanza terminator (line %d, \\\"%s\\\")\" \\\n\t\t\t\t\t\t\t% (self.lineNumber,line)\n\t\treturn (score,pieces)", "response": "returns the pair ( score pieces ) where pieces is a list of ungapped segments with start1 start2 length pctId"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_alignment(self,score,pieces):\n\t \t# build text\n\t\tself.open_seqs()\n\t\ttext1 = text2 = \"\"\n\t\tend1  = end2  = None\n\t\tfor (start1,start2,length,pctId) in pieces:\n\t\t\tif (end1 != None):\n\t\t\t\tif (start1 == end1): # insertion in sequence 2\n\t\t\t\t\ttext1 += self.seq1_gap * (start2-end2)\n\t\t\t\t\ttext2 += self.seq2_file.get(end2,start2-end2)\n\t\t\t\telse: # insertion in sequence 1\n\t\t\t\t\ttext1 += self.seq1_file.get(end1,start1-end1)\n\t\t\t\t\ttext2 += self.seq2_gap * (start1-end1)\n\n\t\t\ttext1 += self.seq1_file.get(start1,length)\n\t\t\ttext2 += self.seq2_file.get(start2,length)\n\t\t\tend1 = start1 + length\n\t\t\tend2 = start2 + length\n\t\t# create alignment\n\t\tstart1 = pieces[0][0]\n\t\tstart2 = pieces[0][1]\n\t\tend1   = pieces[-1][0] + pieces[-1][2]\n\t\tend2   = pieces[-1][1] + pieces[-1][2]\n\t\tsize1  = end1 - start1\n\t\tsize2  = end2 - start2\n\t\ta = Alignment(score=score,species_to_lengths=self.species_to_lengths)\n\t\t#if (self.seq1_strand == \"-\"): start1 = self.seq1_file.length - end1\n\t\ta.add_component(Component(self.seq1_src,start1,size1,self.seq1_strand,text=text1))\n\t\t#if (self.seq2_strand == \"-\"): start2 = self.seq2_file.length - end2\n\t\ta.add_component(Component(self.seq2_src,start2,size2,self.seq2_strand,text=text2))\n\t\treturn a", "response": "converts a score and pieces to an alignment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bits_clear_in_range( bits, range_start, range_end ):\n    end = range_start\n    while 1:\n        start = bits.next_clear( end )\n        if start >= range_end: break\n        end = min( bits.next_set( start ), range_end )\n        yield start, end", "response": "Yield start end tuples for each span of clear bits in range_start range_end"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iterprogress( sized_iterable ):\n    pb = ProgressBar( 0, len( sized_iterable ) )\n    for i, value in enumerate( sized_iterable ):\n        yield value\n        pb.update_and_print( i, sys.stderr )", "response": "Iterate something printing progress bar to stdout"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_file( Class, dict, file, is_little_endian=True ):\n        io = BinaryFileWriter( file, is_little_endian=is_little_endian )\n        start_offset = io.tell()\n        # Header is of fixed length\n        io.seek( start_offset + ( 8 * 256 ) )\n        # For each item, key and value length (written as length prefixed\n        # strings). We also calculate the subtables on this pass.\n        # NOTE: This requires the key and value be byte strings, support for\n        #       dealing with encoding specific value types should be\n        #       added to this wrapper\n        subtables = [ [] for i in range(256) ]\n        for key, value in dict.items():\n            pair_offset = io.tell()\n            io.write_uint32( len( key ) )\n            io.write_uint32( len( value ) )\n            io.write( key )\n            io.write( value )\n            hash = cdbhash( key )\n            subtables[ hash % 256 ].append( ( hash, pair_offset ) )\n        # Save the offset where the subtables will start\n        subtable_offset = io.tell()\n        # Write subtables\n        for subtable in subtables:\n            if len( subtable ) > 0:\n                # Construct hashtable to be twice the size of the number\n                # of items in the subtable, and built it in memory\n                ncells = len( subtable ) * 2\n                cells = [ (0,0) for i in range( ncells ) ]\n                for hash, pair_offset in subtable:\n                    index = ( hash >> 8 ) % ncells\n                    while cells[index][1] != 0:\n                        index = ( index + 1 ) % ncells\n                    # Guaranteed to find a non-empty cell\n                    cells[index] = ( hash, pair_offset )\n                # Write subtable\n                for hash, pair_offset in cells:\n                    io.write_uint32( hash )\n                    io.write_uint32( pair_offset )\n        # Go back and write the header\n        end_offset = io.tell()\n        io.seek( start_offset )\n        index = subtable_offset\n        for subtable in subtables:\n            io.write_uint32( index )\n            io.write_uint32( len( subtable * 2 ) )\n            # For each cell in the subtable, a hash and a pointer to a value\n            index += ( len( subtable ) * 2 ) * 8\n        # Leave fp at end of cdb\n        io.seek( end_offset )", "response": "This function writes out the CDB structure of the given dictionary to a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_len( f ):\n    mapping = dict()\n    for line in f:\n        fields = line.split()\n        mapping[ fields[0] ] = int( fields[1] )\n    return mapping", "response": "Read a LEN file and return a mapping from chromosome to length"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the logo height using the method of the Schneider TD Stephens RM. Sequence logos : a new way to display consensus sequences. Nucleic Acids Res. 1990 Oct 25 ; 18 ( 20 ) - 6097 - 100.", "response": "def freqs_to_heights( matrix ):\n    \"\"\"\n    Calculate logo height using the method of:\n    \n    Schneider TD, Stephens RM. \"Sequence logos: a new way to display consensus \n    sequences.\" Nucleic Acids Res. 1990 Oct 25;18(20):6097-100.\n    \"\"\"\n    # Columns are sequence positions, rows are symbol counts/frequencies\n    f = matrix.values.transpose()\n    n, m = f.shape\n    # Ensure normalized\n    f = f / sum( f, axis=0 )\n    # Shannon entropy (the where replaces 0 with 1 so that '0 log 0 == 0')\n    H = - sum( f * log2( where( f, f, 1 ) ), axis=0 )\n    # Height\n    return transpose( f * ( log2( n ) - H ) )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eps_logo( matrix, base_width, height, colors=DNA_DEFAULT_COLORS ):\n    alphabet = matrix.sorted_alphabet\n    rval = StringIO()\n    # Read header ans substitute in width / height\n    header = Template( pkg_resources.resource_string( __name__, \"template.ps\" ) )\n    rval.write( header.substitute( bounding_box_width = ceil( base_width * matrix.width ) + PAD,\n                                   bounding_box_height = ceil( height ) + PAD ) )\n    # Determine heights\n    heights = freqs_to_heights( matrix )\n    height_scale = height / log2( len( alphabet ) )\n    # Draw each \"row\" of the matrix\n    for i, row in enumerate( heights ):\n        x = ( i * base_width )\n        y = 0\n        for j, base_height in enumerate( row ):\n            char = alphabet[j]\n            page_height = height_scale * base_height\n            # print matrix.alphabet[j], base_height, height_scale, page_height\n            if page_height > 1:\n                # Draw letter\n                rval.write(  \"%s setrgbcolor\\n\" % colors.get( char, '0 0 0' ) )\n                rval.write( \"%3.2f \" % x )\n                rval.write( \"%3.2f \" % y )\n                rval.write( \"%3.2f \" % ( x + base_width ) )\n                rval.write( \"%3.2f \" % ( y + page_height ) )\n                rval.write( \"(%s) textInBox\\n\" % char )\n            y += page_height\n    rval.write( \"showpage\" )\n    return rval.getvalue()", "response": "Returns an EPS document containing a sequence logo for each row of the matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms the coordinates of this elem into the other species. elem intersects this chain s ginterval.", "response": "def transform(elem, chain_CT_CQ, max_gap):\n    \"\"\"transform the coordinates of this elem into the other species.\n\n    elem intersects this chain's ginterval.\n    :return: a list of the type [(to_chr, start, end, elem[id]) ... ]\"\"\"\n    (chain, CT, CQ) = chain_CT_CQ\n    start, end = max(elem['start'], chain.tStart) - chain.tStart, min(elem['end'], chain.tEnd) - chain.tStart\n\n    assert np.all( (CT[:,1] - CT[:,0]) == (CQ[:,1] - CQ[:,0]) )\n    to_chrom = chain.qName\n    to_gab_start = chain.qStart\n\n    start_idx = np.where( CT[:,1] > start )[0][0]\n    end_idx = np.where( CT[:,0] < end )[0][-1]\n\n    if start_idx > end_idx: #maps to a gap region on the other species\n        return []\n\n    ## apply the gap threshold\n    if max_gap >= 0 and start_idx < end_idx - 1:\n        if np.max(CT[(start_idx+1):end_idx,0] - CT[start_idx:(end_idx-1),1]) > max_gap or np.max(CQ[(start_idx+1):end_idx,0] - CQ[start_idx:(end_idx-1),1]) > max_gap:\n            return []\n\n    assert start < CT[start_idx, 1]\n    assert  CT[end_idx, 0] < end\n    to_start = CQ[start_idx, 0] + max(0, start - CT[start_idx,0]) # correct if on middle of interval\n    to_end = CQ[end_idx, 1] - max(0, CT[end_idx, 1] - end)        # idem\n\n    if start_idx == end_idx: #elem falls in a single run of matches\n        slices = [(to_start, to_end)]\n    else:\n        slices = [(to_start, CQ[start_idx,1])]\n        slices += [(CQ[i,0], CQ[i,1]) for i in range(start_idx+1, end_idx)]\n        slices.append( (CQ[end_idx,0], to_end) )\n    if chain.qStrand == '-':\n        Sz = chain.qEnd - chain.qStart\n        slices =  [(Sz-t[1], Sz-t[0]) for t in slices]\n    return [(to_chrom, to_gab_start + t[0], to_gab_start + t[1], elem['id']) for t in slices]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transform_file(ELEMS, ofname, EPO, TREE, opt):\n    \"transform/map the elements of this file and dump the output on 'ofname'\"\n\n    BED4_FRM = \"%s\\t%d\\t%d\\t%s\\n\"\n    log.info(\"%s (%d) elements ...\" % (opt.screen and \"screening\" or \"transforming\", ELEMS.shape[0]))\n    with open(ofname, 'w') as out_fd:\n        if opt.screen:\n            for elem in ELEMS.flat:\n                matching_blocks = [attrgetter(\"value\")(_) for _ in TREE.find(elem['chrom'], elem['start'], elem['end'])]\n                assert set( matching_blocks ) <= set( EPO.keys() )\n                if matching_blocks:\n                    out_fd.write(BED4_FRM % elem)\n        else:\n            for chrom in set( ELEMS['chrom'] ):\n                transform_by_chrom(EPO,\n                        ELEMS[ELEMS['chrom'] == chrom],\n                        TREE, chrom, opt, out_fd)\n    log.info(\"DONE!\")", "response": "transform the elements of this file and dump the output on ofname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading features from a file.", "response": "def loadFeatures(path, opt):\n    \"\"\"\n    Load features. For BED, only BED4 columns are loaded.\n    For narrowPeak, all columns are loaded.\n    \"\"\"\n\n    log.info(\"loading from %s ...\" % path)\n    data = []\n    if opt.in_format == \"BED\":        \n        with open(path) as fd:\n            for line in fd:\n                cols = line.split()\n                data.append( (cols[0], int(cols[1]), int(cols[2]), cols[3]) )\n        data = np.array(data, dtype=elem_t)\n    else:\n        with open(path) as fd:\n            for line in fd:\n                cols = line.split()\n                data.append( (cols[0], int(cols[1]), int(cols[2]), cols[3], int(cols[4]),\n                              cols[5], float(cols[6]), float(cols[7]), float(cols[8]),\n                              int(cols[-1])+int(cols[1])) )\n        data = np.array(data, dtype=narrowPeak_t)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninsert an element into the tree", "response": "def add(self, chrom, element):\n        \"\"\"insert an element. use this method as the IntervalTree one.\n        this will simply call the IntervalTree.add method on the right tree\n\n        :param chrom: chromosome\n        :param element: the argument of IntervalTree.insert_interval\n        :return: None\n        \"\"\"\n\n        self._trees.setdefault(chrom, IntervalTree()).insert_interval( element )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find(self, chrom, start, end):\n\n        tree = self._trees.get( chrom, None )\n        if tree:\n            return tree.find( start, end )\n        #return always a list\n        return []", "response": "find the intersecting elements\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_rows( Class, alphabet, rows ):\n        # Sorted alphabet\n        sorted_alphabet = sorted( alphabet )\n        # Character to index mapping (initialized to -1)\n        char_to_index = zeros( (256), int16 ) - 1\n        for i, ch  in enumerate( sorted_alphabet ):\n            char_to_index[ ord(ch) ] = i\n        # Array\n        values = zeros( ( len( rows) , len( alphabet ) ), float32 )\n        for i, row in enumerate( rows ):\n            assert len( row ) == len( alphabet )\n            for ch, val in zip( alphabet, row ):\n                values[i, char_to_index[ord(ch)]] = val\n        # Matrix\n        matrix = Class()\n        matrix.alphabet = alphabet\n        matrix.sorted_alphabet = sorted_alphabet\n        matrix.char_to_index = char_to_index\n        matrix.values = values\n        return matrix", "response": "Create a new matrix from a list of rows."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_from_other( Class, other, values=None ):\n        m = Class()\n        m.alphabet = other.alphabet\n        m.sorted_alphabet = other.sorted_alphabet\n        m.char_to_index = other.char_to_index\n        if values is not None:\n            m.values = values\n        else:\n            m.values = other.values\n        return m", "response": "Create a new Matrix with the \n        values taken from other."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reverse_complement( self ):\n        rval = copy( self )\n        # Conveniently enough, reversing rows and columns is exactly what we\n        # want, since this results in A swapping with T and C swapping with G.\n        rval.values = self.values[::-1,::-1].copy()\n        return rval", "response": "Create the reverse complement of this matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a standard logodds scoring matrix.", "response": "def to_logodds_scoring_matrix( self, background=None, correction=DEFAULT_CORRECTION ):\n        \"\"\"\n        Create a standard logodds scoring matrix.\n        \"\"\"\n        alphabet_size = len( self.alphabet )\n        if background is None:\n            background = ones( alphabet_size, float32 ) / alphabet_size\n        # Row totals as a one column array\n        totals = numpy.sum( self.values, 1 )[:,newaxis]\n        values = log2( maximum( self.values, correction ) ) \\\n               - log2( totals ) \\\n               - log2( maximum( background, correction ) )\n        return ScoringMatrix.create_from_other( self, values.astype( float32 ) )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_stormo_scoring_matrix( self, background=None ):\n        alphabet_size = len( self.alphabet )\n        if background is None:\n            background = ones( alphabet_size, float32 ) / alphabet_size\n        # Row totals as a one column array\n        totals = numpy.sum( self.values, 1 )[:,newaxis]\n        values = log2( self.values + background ) \\\n               - log2( totals + 1 ) - log2( background )\n        return ScoringMatrix.create_from_other( self, values.astype( float32 ) )", "response": "Create a Stormo scoring matrix from this count matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscoring each valid position in string using this scoring matrix.", "response": "def score_string( self, string ):\n        \"\"\"\n        Score each valid position in `string` using this scoring matrix. \n        Positions which were not scored are set to nan.\n        \"\"\"\n        rval = zeros( len( string ), float32 )\n        rval[:] = nan\n        _pwm.score_string( self.values, self.char_to_index, string, rval )\n        return rval"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _calc_resp(password_hash, server_challenge):\n        # padding with zeros to make the hash 21 bytes long\n        password_hash += b'\\x00' * (21 - len(password_hash))\n\n        res = b''\n        dobj = DES(DES.key56_to_key64(password_hash[0:7]))\n        res = res + dobj.encrypt(server_challenge[0:8])\n\n        dobj = DES(DES.key56_to_key64(password_hash[7:14]))\n        res = res + dobj.encrypt(server_challenge[0:8])\n\n        dobj = DES(DES.key56_to_key64(password_hash[14:21]))\n        res = res + dobj.encrypt(server_challenge[0:8])\n        return res", "response": "Generate the LM response given a 16 - byte password hash and the CHALLENGE_MESSAGE_COOKIE."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encrypt(self, data, pad=True):\n        encrypted_data = b\"\"\n        for i in range(0, len(data), 8):\n            block = data[i:i + 8]\n            block_length = len(block)\n            if block_length != 8 and pad:\n                block += b\"\\x00\" * (8 - block_length)\n            elif block_length != 8:\n                raise ValueError(\"DES encryption must be a multiple of 8 \"\n                                 \"bytes\")\n            encrypted_data += self._encode_block(block)\n\n        return encrypted_data", "response": "Encrypts the data based on the key it was initialised with."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decrypt(self, data):\n        decrypted_data = b\"\"\n        for i in range(0, len(data), 8):\n            block = data[i:i + 8]\n            block_length = len(block)\n            if block_length != 8:\n                raise ValueError(\"DES decryption must be a multiple of 8 \"\n                                 \"bytes\")\n\n            decrypted_data += self._decode_block(block)\n\n        return decrypted_data", "response": "Decrypts the data based on the key it was initialised with."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef key56_to_key64(key):\n        if len(key) != 7:\n            raise ValueError(\"DES 7-byte key is not 7 bytes in length, \"\n                             \"actual: %d\" % len(key))\n\n        new_key = b\"\"\n        for i in range(0, 8):\n            if i == 0:\n                new_value = struct.unpack(\"B\", key[i:i+1])[0]\n            elif i == 7:\n                new_value = struct.unpack(\"B\", key[6:7])[0]\n                new_value = (new_value << 1) & 0xFF\n            else:\n                new_value = struct.unpack(\"B\", key[i - 1:i])[0]\n                next_value = struct.unpack(\"B\", key[i:i + 1])[0]\n                new_value = ((new_value << (8 - i)) & 0xFF) | next_value >> i\n\n            # clear the last bit so the count isn't off\n            new_value = new_value & ~(1 << 0)\n\n            # set the last bit if the number of set bits are even\n            new_value = new_value | int(not DES.bit_count(new_value) & 0x1)\n            new_key += struct.pack(\"B\", new_value)\n\n        return new_key", "response": "This takes in an array of 7 - byte strings and converts it to a bytes string of 8 - byte strings with the odd parity bit being set to every 8 bits."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a one way Lan Manager hash of the password", "response": "def _lmowfv1(password):\n    \"\"\"\n    [MS-NLMP] v28.0 2016-07-14\n\n    3.3.1 NTLM v1 Authentication\n    Same function as LMOWFv1 in document to create a one way hash of the\n    password. Only used in NTLMv1 auth without session security\n\n    :param password: The password or hash of the user we are trying to\n        authenticate with\n    :return res: A Lan Manager hash of the password supplied\n    \"\"\"\n    # if the password is a hash, return the LM hash\n    if re.match(r'^[a-fA-F\\d]{32}:[a-fA-F\\d]{32}$', password):\n        lm_hash = binascii.unhexlify(password.split(':')[0])\n        return lm_hash\n\n    # fix the password to upper case and length to 14 bytes\n    password = password.upper()\n    lm_pw = password.encode('utf-8')\n    padding_size = 0 if len(lm_pw) >= 14 else (14 - len(lm_pw))\n    lm_pw += b\"\\x00\" * padding_size\n\n    # do hash\n    magic_str = b\"KGS!@#$%\"  # page 56 in [MS-NLMP v28.0]\n\n    res = b\"\"\n    dobj = DES(DES.key56_to_key64(lm_pw[0:7]))\n    res += dobj.encrypt(magic_str)\n\n    dobj = DES(DES.key56_to_key64(lm_pw[7:14]))\n    res += dobj.encrypt(magic_str)\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a one way hash of the password", "response": "def _ntowfv1(password):\n    \"\"\"\n    [MS-NLMP] v28.0 2016-07-14\n\n    3.3.1 NTLM v1 Authentication\n    Same function as NTOWFv1 in document to create a one way hash of the\n    password. Only used in NTLMv1 auth without session security\n\n    :param password: The password or hash of the user we are trying to\n        authenticate with\n    :return digest: An NT hash of the password supplied\n    \"\"\"\n\n    # if the password is a hash, return the NT hash\n    if re.match(r'^[a-fA-F\\d]{32}:[a-fA-F\\d]{32}$', password):\n        nt_hash = binascii.unhexlify(password.split(':')[1])\n        return nt_hash\n\n    digest = hashlib.new('md4', password.encode('utf-16-le')).digest()\n    return digest"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures that the method is the same signature as the parent method.", "response": "def visit_Method(self, method):\n        \"\"\"\n        Ensure method has the same signature matching method on parent interface.\n\n        :param method: L{quarkc.ast.Method} instance.\n        \"\"\"\n        resolved_method = method.resolved.type\n\n        def get_params(method, extra_bindings):\n            # The Method should already be the resolved version.\n            result = []\n            for param in method.params:\n                resolved_param = texpr(param.resolved.type, param.resolved.bindings, extra_bindings)\n                result.append(resolved_param.id)\n            return result\n\n        def get_return_type(method, extra_bindings):\n            # The Method should already be the resolved version.\n            return texpr(method.type.resolved.type, method.type.resolved.bindings,\n                         extra_bindings).id\n\n        def signature(method, return_type, params):\n            return \"%s %s(%s)\" % (return_type, method.name.text, \", \".join(params))\n\n        # Ensure the method has the same signature as matching methods on parent\n        # interfaces:\n        interfaces = list(t for t in method.clazz.bases if isinstance(t.resolved.type, Interface))\n        for interface in interfaces:\n            interfaceTypeExpr = interface.resolved\n            for definition in interfaceTypeExpr.type.definitions:\n                if definition.name.text == method.name.text:\n                    resolved_definition = definition.resolved.type\n                    method_params = get_params(resolved_method, method.clazz.resolved.bindings)\n                    definition_params = get_params(resolved_definition, interfaceTypeExpr.bindings)\n                    method_return = get_return_type(resolved_method, method.clazz.resolved.bindings)\n                    definition_return = get_return_type(resolved_definition, interfaceTypeExpr.bindings)\n\n                    if method_params != definition_params or method_return != definition_return:\n                        self.errors.append(\n                            \"%s: method signature '%s' on %s does not match method '%s' on interface %s\" % (\n                                lineinfo(method), signature(resolved_method, method_return, method_params),\n                                method.clazz.resolved.type.id,\n                                signature(resolved_definition, definition_return, definition_params),\n                                interface.resolved.type.id))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a quark file and return the root entry.", "response": "def urlparse(self, url, top=True, text=None, include=False, recurse=True):\n        \"\"\"\n        Parse a quark file and, optionally, its recursive dependencies.\n\n        A quark file (main.q) is loaded via urlparse() can have two kinds of\n        dependencies, `use a.q` or `include b.q`. For the `use` case each file\n        is added as a separate top-level root to self.roots. For the `include`\n        case the file is added to the *current* root that is including it, so\n        it's added as a child of `self.root`.\n\n        There are  two forms  of caching:  CACHE is  a shared  dictionary across\n        class instances of parsed roots.  Additionally .qc file are written with\n        pickled versions of  loaded roots. Given that both of  them store a root\n        these forms  of caching are only  relevant to top-level quark  files and\n        files referenced using `use`. Files  loaded with `include` should bypass\n        the  caching mechanism  since they  need to  be loaded  as child  of the\n        parent root.\n        \"\"\"\n        if os.path.exists(url):\n            url = os.path.abspath(url)\n\n        urlc = compiled_quark(url)\n        if not include and url in self.CACHE:\n            self.log.debug(\"loading from cache: %s\", url)\n            root = self.CACHE[url]\n            self.roots.add(root)\n            if recurse:\n                for u in root.uses:\n                    assert u in self.CACHE, (url, u, self.CACHE.keys())\n                    self.roots.add(self.CACHE[u])\n            if not include: self.entries[url] = root.files[0]\n            return root.files[0]\n        elif not include and recurse and os.path.exists(url) and is_newer(urlc, url, __file__):\n            self.log.debug(\"loading from: %sc\", url)\n            with open(urlc) as fd:\n                try:\n                    unp = pickle.Unpickler(fd)\n                    deps = unp.load()\n                    if is_newer(urlc, *deps):\n                        roots = unp.load()\n                        # Check for the end record in case we\n                        # encounter a partially written file.\n                        end = unp.load()\n                        if end == ARCHIVE_END:\n                            for root in roots:\n                                self.CACHE[root.url] = root\n                                self.roots.add(root)\n                            if not include: self.entries[url] = roots[0].files[0]\n                            return roots[0].files[0]\n                except EOFError:\n                    pass\n\n        old = None\n        if not include and url not in self.roots:\n            old = self.root\n            self.root = Root(url)\n            self.roots.add(self.root)\n\n        try:\n            if text is None:\n                try:\n                    text = self.read(url)\n                except IOError, e:\n                    if top:\n                        raise CompileError(e)\n                    else:\n                        raise\n            self.log.debug(\"parsing %s\", url)\n            file = self.parse(url, text)\n\n            if recurse:\n                for u in file.uses.values():\n                    qurl = join(url, u.url)\n                    self.perform_use(qurl, u)\n                    assert qurl in self.CACHE, (url, qurl, self.CACHE.keys())\n                for inc in file.includes.values():\n                    qurl = join(url, inc.url)\n                    if qurl.endswith(\".q\"):\n                        self.perform_quark_include(qurl, inc)\n                    else:\n                        self.perform_native_include(qurl, inc)\n                if not include:\n                    self.CACHE[url] = self.root\n            if not include: self.entries[url] = file\n            return file\n        finally:\n            if old: self.root = old"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_doc(node):\n    res = \" \".join(get_doc_annotations(node))\n    if not res:\n        res = \"(%s)\" % node.__class__.__name__.lower()\n    return res", "response": "Return a node s documentation as a string pulling from annotations\n    and constructing a simple fake as needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a node s code", "response": "def get_code(node, coder=Coder()):\n    \"\"\"\n    Return a node's code\n    \"\"\"\n    return cgi.escape(str(coder.code(node)), quote=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_environ(self):\n        SimpleHandler.setup_environ(self)\n        self.environ['ws4py.socket'] = get_connection(self.environ['wsgi.input'])\n        self.http_version = self.environ['SERVER_PROTOCOL'].rsplit('/')[-1]", "response": "Setup the environ dictionary and add the\n        `'ws4py. socket' key. Its associated value\n        is the real socket underlying socket."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef finish_response(self):\n        # force execution of the result iterator until first actual content\n        rest = iter(self.result)\n        first = list(itertools.islice(rest, 1))\n        self.result = itertools.chain(first, rest)\n        # now it's safe to look if environ was modified\n        ws = None\n        if self.environ:\n            self.environ.pop('ws4py.socket', None)\n            ws = self.environ.pop('ws4py.websocket', None)\n\n        try:\n            SimpleHandler.finish_response(self)\n        except:\n            if ws:\n                ws.close(1011, reason='Something broke')\n            raise\n        else:\n            if ws:\n                self.request_handler.server.link_websocket_to_server(ws)", "response": "Completes the response and performs the following tasks."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle(self):\n        self.raw_requestline = self.rfile.readline()\n        if not self.parse_request(): # An error code has been sent, just exit\n            return\n\n        # next line is where we'd have expect a configuration key somehow\n        handler = self.WebSocketWSGIHandler(\n            self.rfile, self.wfile, self.get_stderr(), self.get_environ()\n        )\n        handler.request_handler = self      # backpointer for logging\n        handler.run(self.server.get_app())", "response": "This method is called by the server when we receive a request from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef right_associative_infix_rule(operator, grammar_rule):\n    def semantic_action(self, node, (result, remaining)):\n        while remaining:\n            op, rhs = remaining.pop(0)\n            result = operator(Attr(result, Name(self.aliases[op])), [rhs], op)\n        return result\n    return g.rule(grammar_rule)(semantic_action)", "response": "Semantic action for rules like A = B C B = C."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef configure(self, voltage_range=RANGE_32V, gain=GAIN_AUTO,\n                  bus_adc=ADC_12BIT, shunt_adc=ADC_12BIT):\n        \"\"\" Configures and calibrates how the INA219 will take measurements.\n\n        Arguments:\n        voltage_range -- The full scale voltage range, this is either 16V\n            or 32V represented by one of the following constants;\n            RANGE_16V, RANGE_32V (default).\n        gain -- The gain which controls the maximum range of the shunt\n            voltage represented by one of the following constants;\n            GAIN_1_40MV, GAIN_2_80MV, GAIN_4_160MV,\n            GAIN_8_320MV, GAIN_AUTO (default).\n        bus_adc -- The bus ADC resolution (9, 10, 11, or 12-bit) or\n            set the number of samples used when averaging results\n            represent by one of the following constants; ADC_9BIT,\n            ADC_10BIT, ADC_11BIT, ADC_12BIT (default),\n            ADC_2SAMP, ADC_4SAMP, ADC_8SAMP, ADC_16SAMP,\n            ADC_32SAMP, ADC_64SAMP, ADC_128SAMP\n        shunt_adc -- The shunt ADC resolution (9, 10, 11, or 12-bit) or\n            set the number of samples used when averaging results\n            represent by one of the following constants; ADC_9BIT,\n            ADC_10BIT, ADC_11BIT, ADC_12BIT (default),\n            ADC_2SAMP, ADC_4SAMP, ADC_8SAMP, ADC_16SAMP,\n            ADC_32SAMP, ADC_64SAMP, ADC_128SAMP\n        \"\"\"\n        self.__validate_voltage_range(voltage_range)\n        self._voltage_range = voltage_range\n\n        if self._max_expected_amps is not None:\n            if gain == self.GAIN_AUTO:\n                self._auto_gain_enabled = True\n                self._gain = self._determine_gain(self._max_expected_amps)\n            else:\n                self._gain = gain\n        else:\n            if gain != self.GAIN_AUTO:\n                self._gain = gain\n            else:\n                self._auto_gain_enabled = True\n                self._gain = self.GAIN_1_40MV\n\n        logging.info('gain set to %.2fV' % self.__GAIN_VOLTS[self._gain])\n\n        logging.debug(\n            self.__LOG_MSG_1 %\n            (self._shunt_ohms, self.__BUS_RANGE[voltage_range],\n             self.__GAIN_VOLTS[self._gain],\n             self.__max_expected_amps_to_string(self._max_expected_amps),\n             bus_adc, shunt_adc))\n\n        self._calibrate(\n            self.__BUS_RANGE[voltage_range], self.__GAIN_VOLTS[self._gain],\n            self._max_expected_amps)\n        self._configure(voltage_range, self._gain, bus_adc, shunt_adc)", "response": "Configures and calibrates the INA219 ADCs for the specified ADCs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wake(self):\n        configuration = self._read_configuration()\n        self._configuration_register(configuration | 0x0007)\n        # 40us delay to recover from powerdown (p14 of spec)\n        time.sleep(0.00004)", "response": "Wake the INA219 from power down mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _return_response_and_status_code(response, json_results=True):\n    if response.status_code == requests.codes.ok:\n        return dict(results=response.json() if json_results else response.content, response_code=response.status_code)\n    elif response.status_code == 400:\n        return dict(\n                error='package sent is either malformed or not within the past 24 hours.',\n                response_code=response.status_code)\n    elif response.status_code == 204:\n        return dict(\n            error='You exceeded the public API request rate limit (4 requests of any nature per minute)',\n            response_code=response.status_code)\n    elif response.status_code == 403:\n        return dict(\n            error='You tried to perform calls to functions for which you require a Private API key.',\n            response_code=response.status_code)\n    elif response.status_code == 404:\n        return dict(error='File not found.', response_code=response.status_code)\n    else:\n        return dict(response_code=response.status_code)", "response": "Output the requests response content or content as json and status code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_comments(self, resource, comment, timeout=None):\n        params = {'apikey': self.api_key, 'resource': resource, 'comment': comment}\n\n        try:\n            response = requests.post(self.base + 'comments/put', params=params, proxies=self.proxies, timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response)", "response": "Post a comment on a file or URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the report for a specific IP address.", "response": "def get_ip_report(self, this_ip, timeout=None):\n        \"\"\" Get IP address reports.\n\n        :param this_ip: a valid IPv4 address in dotted quad notation, for the time being only IPv4 addresses are\n                        supported.\n        :param timeout: The amount of time in seconds the request should wait before timing out.\n\n        :return: JSON response\n        \"\"\"\n        params = {'apikey': self.api_key, 'ip': this_ip}\n\n        try:\n            response = requests.get(self.base + 'ip-address/report',\n                                    params=params,\n                                    proxies=self.proxies,\n                                    timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting information about a given domain.", "response": "def get_domain_report(self, this_domain, timeout=None):\n        \"\"\" Get information about a given domain.\n\n        :param this_domain: a domain name.\n        :param timeout: The amount of time in seconds the request should wait before timing out.\n\n        :return: JSON response\n        \"\"\"\n        params = {'apikey': self.api_key, 'domain': this_domain}\n\n        try:\n            response = requests.get(self.base + 'domain/report', params=params, proxies=self.proxies, timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsubmits a file to be scanned by VirusTotal.", "response": "def scan_file(self,\n                  this_file,\n                  notify_url=None,\n                  notify_changes_only=None,\n                  from_disk=True,\n                  filename=None,\n                  timeout=None):\n        \"\"\" Submit a file to be scanned by VirusTotal.\n\n        Allows you to send a file for scanning with VirusTotal. Before performing your submissions we encourage you to\n        retrieve the latest report on the files, if it is recent enough you might want to save time and bandwidth by\n        making use of it. File size limit is 32MB, in order to submmit files up to 200MB in size you must request a\n        special upload URL.\n\n        :param this_file: The file to be uploaded.\n        :param notify_url: A URL to which a POST notification should be sent when the scan finishes.\n        :param notify_changes_only: Used in conjunction with notify_url. Indicates if POST notifications should be\n                                    sent only if the scan results differ from the previous analysis.\n        :param from_disk: If True we read the file contents from disk using this_file as filepath. If False this_file\n                          is the actual file object.\n        :param filename: Specify the filename, this overwrites the filename if we read a file from disk.\n        :param timeout: The amount of time in seconds the request should wait before timing out.\n\n        :return: JSON response that contains scan_id and permalink.\n        \"\"\"\n        params = {'apikey': self.api_key}\n        if from_disk:\n            if not filename:\n                filename = os.path.basename(this_file)\n            files = {'file': (filename, open(this_file, 'rb').read())}\n        else:\n            if filename:\n                files = {'file': (filename, this_file)}\n            else:\n                files = {'file': this_file}\n\n        try:\n            response = requests.post(\n                self.base + 'file/scan', files=files, params=params, proxies=self.proxies, timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a special URL for submitted files bigger than 32MB.", "response": "def get_upload_url(self, timeout=None):\n        \"\"\" Get a special URL for submitted files bigger than 32MB.\n\n        In order to submit files bigger than 32MB you need to obtain a special upload URL to which you\n        can POST files up to 200MB in size. This API generates such a URL.\n\n        :param timeout: The amount of time in seconds the request should wait before timing out.\n\n        :return: JSON special upload URL to which you can POST files up to 200MB in size.\n        \"\"\"\n        params = {'apikey': self.api_key}\n\n        try:\n            response = requests.get(self.base + 'file/scan/upload_url',\n                                    params=params,\n                                    proxies=self.proxies,\n                                    timeout=timeout)\n            if response.status_code == requests.codes.ok:\n                return response.json().get('upload_url')\n            else:\n                return dict(response_code=response.status_code)\n        except requests.RequestException as e:\n            return dict(error=str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the scan results for a given file.", "response": "def get_file_report(self, resource, allinfo=1, timeout=None):\n        \"\"\" Get the scan results for a file.\n\n        Retrieves a concluded file scan report for a given file. Unlike the public API, this call allows you to also\n        access all the information we have on a particular file (VirusTotal metadata, signature information, structural\n        information, etc.) by using the allinfo parameter described later on.\n\n        :param resource: An md5/sha1/sha256 hash of a file for which you want to retrieve the most recent antivirus\n        report. You may also specify a scan_id (sha256-timestamp as returned by the scan API) to access a specific\n        report. You can also specify a CSV list made up of a combination of hashes and scan_ids (up to 25 items),\n        this allows you to perform a batch request with just one single call.\n        :param allinfo: (optional) If specified and set to one, the call will return additional info, other than the\n        antivirus results, on the file being queried. This additional info includes the output of several tools acting\n        on the file (PDFiD, ExifTool, sigcheck, TrID, etc.), metadata regarding VirusTotal submissions (number of\n        unique sources that have sent the file in the past, first seen date, last seen date, etc.), the output of\n        in-house technologies such as a behavioural sandbox, etc.\n        :param timeout: The amount of time in seconds the request should wait before timing out.\n\n        :return: JSON response\n        \"\"\"\n        params = {'apikey': self.api_key, 'resource': resource, 'allinfo': allinfo}\n\n        try:\n            response = requests.get(self.base + 'file/report', params=params, proxies=self.proxies, timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef file_search(self, query, offset=None, timeout=None):\n        params = dict(apikey=self.api_key, query=query, offset=offset)\n\n        try:\n            response = requests.get(self.base + 'file/search', params=params, proxies=self.proxies, timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response)", "response": "Search for samples.\n\n        In addition to retrieving all information on a particular file, VirusTotal allows you to perform what we\n        call \"advanced reverse searches\". Reverse searches take you from a file property to a list of files that\n        match that property. For example, this functionality enables you to retrieve all those files marked by at\n        least one antivirus vendor as Zbot, or all those files that have a size under 90KB and are detected by at\n        least 10 antivirus solutions, or all those PDF files that have an invalid XREF section, etc.\n\n        This API is equivalent to VirusTotal Intelligence advanced searches. A very wide variety of search modifiers\n        are available, including: file size, file type, first submission date to VirusTotal, last submission date to\n        VirusTotal, number of positives, dynamic behavioural properties, binary content, submission file name, and a\n        very long etcetera. The full list of search modifiers allowed for file search queries is documented at:\n        https://www.virustotal.com/intelligence/help/file-search/#search-modifiers\n\n        NOTE:\n        Daily limited! No matter what API step you have licensed, this API call is limited to 50K requests per day.\n        If you need any more, chances are you are approaching your engineering problem erroneously and you can\n        probably solve it using the file distribution call. Do not hesitate to contact us with your particular\n        use case.\n\n        EXAMPLE:\n        search_options = 'type:peexe size:90kb+ positives:5+ behaviour:\"taskkill\"'\n\n        :param query: A search modifier compliant file search query.\n        :param offset: (optional) The offset value returned by a previously issued identical query, allows you to\n        paginate over the results. If not specified the first 300 matching files sorted according to last submission\n        date to VirusTotal in a descending fashion will be returned.\n        :param timeout: The amount of time in seconds the request should wait before timing out.\n\n        :return: JSON response -  By default the list returned contains at most 300 hashes, ordered according to\n        last submission date to VirusTotal in a descending fashion."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_file_clusters(self, this_date, timeout=None):\n        params = {'apikey': self.api_key, 'date': this_date}\n\n        try:\n            response = requests.get(self.base + 'file/clusters', params=params, proxies=self.proxies, timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response)", "response": "This method returns a JSON object containing the file similarity clusters for a given time frame."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_url_distribution(self, after=None, reports='true', limit=1000, timeout=None):\n\n        params = {'apikey': self.api_key, 'after': after, 'reports': reports, 'limit': limit}\n\n        try:\n            response = requests.get(self.base + 'url/distribution',\n                                    params=params,\n                                    proxies=self.proxies,\n                                    timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response)", "response": "This method returns a live feed with the lastest URLs submitted to VirusTotal along with their detection ratio."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a live file feed with the latest files submitted to VirusTotal.", "response": "def get_url_feed(self, package=None, timeout=None):\n        \"\"\" Get a live file feed with the latest files submitted to VirusTotal.\n\n        Allows you to retrieve a live feed of reports on absolutely all URLs scanned by VirusTotal. This API requires\n        you to stay relatively synced with the live submissions as only a backlog of 24 hours is provided at any given\n        point in time.\n\n        This API returns a bzip2 compressed tarball. For per-minute packages the compressed package contains a unique\n        file, the file contains a json per line, this json is a full report on a given URL processed by VirusTotal\n        during the given time window. The URL report follows the exact same format as the response of the URL report\n        API if the allinfo=1 parameter is provided. For hourly packages, the tarball contains 60 files, one per each\n        minute of the window.\n\n        :param package: Indicates a time window to pull reports on all items received during such window.\n                        Only per-minute and hourly windows are allowed, the format is %Y%m%dT%H%M (e.g. 20160304T0900)\n                        or %Y%m%dT%H (e.g. 20160304T09). Time is expressed in UTC.\n        :param timeout: The amount of time in seconds the request should wait before timing out.\n\n        :return: BZIP2 response: please see https://www.virustotal.com/en/documentation/private-api/#file-feed\n        \"\"\"\n        if package is None:\n            now = datetime.utcnow()\n            five_minutes_ago = now - timedelta(\n                minutes=now.minute % 5 + 5, seconds=now.second, microseconds=now.microsecond)\n            package = five_minutes_ago.strftime('%Y%m%dT%H%M')\n\n        params = {'apikey': self.api_key, 'package': package}\n\n        try:\n            response = requests.get(self.base + 'url/feed', params=params, proxies=self.proxies, timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response, json_results=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the hashes from a file search API call.", "response": "def get_hashes_from_search(self, query, page=None, timeout=None):\n        \"\"\" Get the scan results for a file.\n\n        Even if you do not have a Private Mass API key that you can use, you can still automate VirusTotal Intelligence\n        searches pretty much in the same way that the searching for files api call works.\n\n        :param query: a VirusTotal Intelligence search string in accordance with the file search documentation .\n            <https://www.virustotal.com/intelligence/help/file-search/>\n        :param page: the next_page property of the results of a previously issued query to this API. This parameter\n            should not be provided if it is the very first query to the API, i.e. if we are retrieving the\n            first page of results.\n        :param timeout: The amount of time in seconds the request should wait before timing out.\n\n        apikey: the API key associated to a VirusTotal Community account with VirusTotal Intelligence privileges.\n        \"\"\"\n        params = {'query': query, 'apikey': self.api_key, 'page': page}\n\n        try:\n            response = requests.get(self.base + 'search/programmatic/',\n                                    params=params,\n                                    proxies=self.proxies,\n                                    timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_file(self, file_hash, save_file_at, timeout=None):\n        params = {'hash': file_hash, 'apikey': self.api_key}\n\n        try:\n            response = requests.get(self.base + 'download/',\n                                    params=params,\n                                    proxies=self.proxies,\n                                    stream=True,\n                                    timeout=timeout)\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        if response.status_code == requests.codes.ok:\n            self.save_downloaded_file(file_hash, save_file_at, response.content)\n\n        return _return_response_and_status_code(response, json_results=False)", "response": "Get the scan results for a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all file report pages.", "response": "def get_all_file_report_pages(self, query):\n        \"\"\" Get File Report (All Pages).\n\n        :param query: a VirusTotal Intelligence search string in accordance with the file search documentation.\n        :return: All JSON responses appended together.\n        \"\"\"\n        responses = []\n        r = self.get_hashes_from_search(query)\n        responses.append(r)\n\n        if ('results' in r.keys()) and ('next_page' in r['results'].keys()):\n            next_page = r['results']['next_page']\n        else:\n            next_page = None\n\n        while next_page:\n            r = self.get_hashes_from_search(query, next_page)\n\n            if ('results' in r.keys()) and ('next_page' in r['results'].keys()):\n                next_page = r['results']['next_page']\n            else:\n                next_page = None\n\n            responses.append(r)\n        return dict(results=responses)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the next page of the notifications feed for this resource.", "response": "def get_intel_notifications_feed(self, page=None, timeout=None):\n        \"\"\" Get notification feed in JSON for further processing.\n\n        :param page: the next_page property of the results of a previously issued query to this API. This parameter\n            should not be provided if it is the very first query to the API, i.e. if we are retrieving the\n            first page of results.\n        :param timeout: The amount of time in seconds the request should wait before timing out.\n        :returns: The next page identifier, The results (JSON is possible with .json())\n        \"\"\"\n        params = {'apikey': self.api_key, 'next': page}\n        try:\n            response = requests.get(self.base + 'hunting/notifications-feed/',\n                                    params=params,\n                                    proxies=self.proxies,\n                                    timeout=timeout)\n            # VT returns an empty result, len(content)==0, and status OK if there are no pending notifications.\n            # To keep the API consistent we generate an empty object instead.\n            # This might not be necessary with a later release of the VTI API. (bug has been submitted)\n            if len(response.content) == 0:\n                response.__dict__['_content'] = \\\n                    b'{\"notifications\":[],\"verbose_msg\":\"No pending notification\",\"result\":0,\"next\":null}'\n        except requests.RequestException as e:\n            return dict(error=str(e))\n\n        return _return_response_and_status_code(response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning botocore. credential. Credential object.", "response": "def get_credentials(self):\n        \"\"\"\n        Returns botocore.credential.Credential object.\n        \"\"\"\n        return Credentials(access_key=self.aws_access_key_id,\n                           secret_key=self.aws_secret_access_key,\n                           token=self.aws_session_token)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_membership(self, group):\n        user_groups = self.request.user.groups.values_list(\"name\", flat=True)\n        if isinstance(group, (list, tuple)):\n            for req_group in group:\n                if req_group in user_groups:\n                    return True\n\n        is_member = group in user_groups\n        if not is_member:\n            messages.add_message(self.request, messages.ERROR, 'You do not have sufficient permissions to do that.')\n\n        return is_member", "response": "Check if user is member of the specified group."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef form_valid(self, form):\n\n        ret = super(HookCreate, self).form_valid(form)\n\n        # Good to make note of that\n        messages.add_message(self.request, messages.SUCCESS, 'Hook %s created' % self.object.url)\n\n        return ret", "response": "After the form is valid lets let people know that the hook is created"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef form_valid(self, form):\n\n        form_valid_from_parent = super(HostCreate, self).form_valid(form)\n        messages.success(self.request, 'Host {} Successfully Created'.format(self.object))\n\n        return form_valid_from_parent", "response": "First call the parent s form valid then let the user know it worked."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(self, *args, **kwargs):\n\n        existing_ssh = models.SSHConfig.objects.all()\n\n        if existing_ssh.exists():\n            return self.get_view()\n\n        remote_user = self.request.POST.get('remote_user', 'root')\n\n        create_ssh_config(remote_user=remote_user)\n\n        return self.get_view()", "response": "Create the SSH file & then return the normal get method..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nput s a text file on the server", "response": "def update_sandbox_site(comment_text):\n    \"\"\"put's a text file on the server\"\"\"\n\n    file_to_deliver = NamedTemporaryFile(delete=False)\n\n    file_text = \"Deployed at: {} <br /> Comment: {}\".format(datetime.datetime.now().strftime('%c'), cgi.escape(comment_text))\n\n    file_to_deliver.write(file_text)\n    file_to_deliver.close()\n\n    put(file_to_deliver.name, '/var/www/html/index.html', use_sudo=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef web_hooks(self, include_global=True):\n        from fabric_bolt.web_hooks.models import Hook\n        ors = [Q(project=self)]\n\n        if include_global:\n            ors.append(Q(project=None))\n\n        hooks = Hook.objects.filter(reduce(operator.or_, ors))\n\n        return hooks", "response": "Get all web hooks for this project. Includes global hooks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of all the configuration keys that are currently set for this project.", "response": "def get_queryset_configurations(self, **kwargs):\n        \"\"\"\n        Really we just want to do a simple SQL statement like this (but oh the ORM):\n\n        SELECT Distinct(Coalesce(stage.key, project.key)) AS key,\n        (CASE WHEN stage.key IS NOT null THEN stage.data_type ELSE project.data_type END) AS data_type,\n        (CASE WHEN stage.key IS NOT null THEN stage.value ELSE project.value END) AS value,\n        (CASE WHEN stage.key IS NOT null THEN stage.value_number ELSE project.value_number END) AS value_number,\n        (CASE WHEN stage.key IS NOT null THEN stage.value_boolean ELSE project.value_boolean END) AS value_boolean,\n        (CASE WHEN stage.key IS NOT null THEN stage.prompt_me_for_input ELSE project.prompt_me_for_input END) AS prompt_me_for_input,\n        (CASE WHEN stage.key IS NOT null THEN stage.sensitive_value ELSE project.sensitive_value END) AS sensitive_value\n        FROM projects_configuration AS project\n        LEFT JOIN projects_configuration AS stage ON stage.project_id = project.project_id\n            AND project.key = stage.key AND stage.stage_id = STAGE_ID_HERE\n        WHERE project.project_id = PROJECT_ID_HERE AND (project.stage_id is null OR project.stage_id = STAGE_ID_HERE)\n        \"\"\"\n        queryset_list = []\n        current_configs = []\n\n        # Create stage specific configurations dictionary\n        for stage in self.stage_configurations().filter(**kwargs):\n            queryset_list.append(stage)\n            current_configs.append(stage.key)\n\n        for project in self.project.project_configurations().filter(**kwargs):\n            if not project.key in current_configs:\n                queryset_list.append(project)\n                current_configs.append(project.key)\n\n        return queryset_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_configurations(self):\n\n        project_configurations_dictionary = {}\n        project_configurations = self.project.project_configurations()\n\n        # Create project specific configurations dictionary\n        for config in project_configurations:\n            project_configurations_dictionary[config.key] = config\n\n        stage_configurations_dictionary = {}\n        stage_configurations = self.stage_configurations()\n\n        # Create stage specific configurations dictionary\n        for s in stage_configurations:\n            stage_configurations_dictionary[s.key] = s\n\n        # override project specific configuration with the ones in the stage if they are there\n        project_configurations_dictionary.update(stage_configurations_dictionary)\n\n        # Return the updated configurations\n        return project_configurations_dictionary", "response": "Generates a dictionary that s made up of the configurations on the project and the stage."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_absolute_url(self):\n\n        # Determine if this configuration is on a stage\n        if self.stage:\n            # Stage specific configurations go back to the stage view\n            url = reverse('projects_stage_view', args=(self.project.pk, self.stage.pk))\n        else:\n            # Project specific configurations go back to the project page\n            url = self.project.get_absolute_url()\n\n        return url", "response": "Determine where I am coming from and where I am going"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the proper value based on the data type", "response": "def get_value(self):\n        \"\"\"Determine the proper value based on the data_type\"\"\"\n\n        if self.data_type == self.BOOLEAN_TYPE:\n            return self.value_boolean\n        elif self.data_type == self.NUMBER_TYPE:\n            return self.value_number\n        elif self.data_type == self.SSH_KEY_TYPE:\n            return self.value_ssh_key.private_key_file._get_path()\n        else:\n            return self.value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the value of the attribute based on the data type", "response": "def set_value(self, value):\n        \"\"\"Determine the proper value based on the data_type\"\"\"\n\n        if self.data_type == self.BOOLEAN_TYPE:\n            self.value_boolean = bool(value)\n        elif self.data_type == self.NUMBER_TYPE:\n            self.value_number = float(value)\n        else:\n            self.value = value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend a line of text to the output instantly.", "response": "def add_output(self, line):\n        \"\"\"\n        Appends {line} of output to the output instantly. (directly hits the database)\n        :param line: the line of text to append\n        :return: None\n        \"\"\"\n        Deployment.objects.filter(pk=self.id).update(output=CF('output')+line)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nappends a line of text to the input instantly.", "response": "def add_input(self, line):\n        \"\"\"\n        Appends {line} of input to the input instantly. (directly hits the database)\n        :param line: the line of text to append\n        :return: None\n        \"\"\"\n        Deployment.objects.filter(pk=self.id).update(input=CF('input')+line)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_next_input(self):\n\n        # TODO: could override input if we get input coming in at the same time\n        all_input = Deployment.objects.get(pk=self.id).input or ''\n        lines = all_input.splitlines()\n        first_line = lines[0] if len(lines) else None\n        lines = lines[1:] if len(lines) > 1 else []\n        Deployment.objects.filter(pk=self.id).update(input='\\n'.join(lines))\n        return first_line", "response": "Returns the next input line of the input\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gravatar(self, size=20):\n        default = \"mm\"\n\n        gravatar_url = \"//www.gravatar.com/avatar/\" + hashlib.md5(self.email.lower()).hexdigest() + \"?\"\n        gravatar_url += urllib.urlencode({'d': default, 's': str(size)})\n\n        return gravatar_url", "response": "Construct a gravatar image address for the user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the model instance with the correct Auth Group based on the user level question.", "response": "def save(self, commit=True):\n        \"\"\"\n        Save the model instance with the correct Auth Group based on the user_level question\n        \"\"\"\n        instance = super(UserChangeForm, self).save(commit=commit)\n\n        if commit:\n            self.set_permissions(instance)\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the model instance with the correct Auth Group based on the user level question", "response": "def save(self, commit=True):\n        \"\"\"\n        Save the model instance with the correct Auth Group based on the user_level question\n        \"\"\"\n        instance = super(UserCreationForm, self).save(commit=commit)\n        random_password = ''.join(random.choice(string.ascii_uppercase + string.digits) for x in range(32))\n        instance.set_password(random_password)\n        instance.save()\n\n        email_form = PasswordResetForm({'email': self.cleaned_data['email']})\n        email_form.is_valid()\n        email_form.save(email_template_name='accounts/welcome_email.html')\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the urls we need to post to", "response": "def hooks(self, project):\n        \"\"\" Look up the urls we need to post to\"\"\"\n\n        return self.get_queryset().filter(\n            Q(project=None) |\n            Q(project=project)\n        ).distinct('url')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef web_hook_receiver(sender, **kwargs):\n\n    deployment = Deployment.objects.get(pk=kwargs.get('deployment_id'))\n\n    hooks = deployment.web_hooks\n\n    if not hooks:\n        return\n\n    for hook in hooks:\n\n        data = payload_generator(deployment)\n\n        deliver_hook(deployment, hook.url, data)", "response": "Generic receiver for the web hook firing piece."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef full_domain_validator(hostname):\n    HOSTNAME_LABEL_PATTERN = re.compile(\"(?!-)[A-Z\\d-]+(?<!-)$\", re.IGNORECASE)\n\n    if not hostname:\n        return\n    if len(hostname) > 255:\n        raise ValidationError(_(\"The domain name cannot be composed of more than 255 characters.\"))\n    if hostname[-1:] == \".\":\n        hostname = hostname[:-1]  # strip exactly one dot from the right, if present\n    for label in hostname.split(\".\"):\n        if len(label) > 63:\n            raise ValidationError(\n                _(\"The label '%(label)s' is too long (maximum is 63 characters).\") % {'label': label})\n        if not HOSTNAME_LABEL_PATTERN.match(label):\n            raise ValidationError(_(\"Unallowed characters in label '%(label)s'.\") % {'label': label})", "response": "Validate a full domain name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nserialize the object down to Python primitives.", "response": "def serialize_hook(instance):\n    \"\"\"\n    Serialize the object down to Python primitives.\n\n    By default it uses Django's built in serializer.\n    \"\"\"\n\n    if getattr(instance, 'serialize_hook', None) and callable(instance.serialize_hook):\n        return instance.serialize_hook(hook=instance)\n    if getattr(settings, 'HOOK_SERIALIZER', None):\n        serializer = get_module(settings.HOOK_SERIALIZER)\n        return serializer(instance, hook=instance)\n    # if no user defined serializers, fallback to the django builtin!\n    return {\n        'hook': instance.dict(),\n        'data': serializers.serialize('python', [instance])[0]\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndelivers the payload to the target URL.", "response": "def deliver_hook(instance, target, payload_override=None):\n    \"\"\"\n    Deliver the payload to the target URL.\n\n    By default it serializes to JSON and POSTs.\n    \"\"\"\n    payload = payload_override or serialize_hook(instance)\n    if hasattr(settings, 'HOOK_DELIVERER'):\n        deliverer = get_module(settings.HOOK_DELIVERER)\n        deliverer(target, payload, instance=instance)\n    else:\n        client.post(\n            url=target,\n            data=json.dumps(payload, cls=serializers.json.DjangoJSONEncoder),\n            headers={'Content-Type': 'application/json'}\n        )\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npaginates the table using a paginator and creates a page property containing the information for the current page.", "response": "def paginate(self, klass=Paginator, per_page=None, page=1, *args, **kwargs):\n        \"\"\"\n        Paginates the table using a paginator and creates a ``page`` property\n        containing information for the current page.\n\n        :type     klass: Paginator class\n        :param    klass: a paginator class to paginate the results\n        :type  per_page: `int`\n        :param per_page: how many records are displayed on each page\n        :type      page: `int`\n        :param     page: which page should be displayed.\n\n        Extra arguments are passed to the paginator.\n\n        Pagination exceptions (`~django.core.paginator.EmptyPage` and\n        `~django.core.paginator.PageNotAnInteger`) may be raised from this\n        method and should be handled by the caller.\n        \"\"\"\n\n        self.per_page_options = [25, 50, 100, 200]  # This should probably be a passed in option\n\n        self.per_page = per_page = per_page or self._meta.per_page\n\n        self.paginator = klass(self.rows, per_page, *args, **kwargs)\n        self.page = self.paginator.page(page)\n\n        # Calc variables for use in displaying first, adjacent, and last page links\n        adjacent_pages = 1  # This should probably be a passed in option\n\n        # Starting page (first page between the ellipsis)\n        start_page = max(self.page.number - adjacent_pages, 1)\n        if start_page <= 3:\n            start_page = 1\n\n        # Ending page (last page between the ellipsis)\n        end_page = self.page.number + adjacent_pages + 1\n        if end_page >= self.paginator.num_pages - 1:\n            end_page = self.paginator.num_pages + 1\n\n        # Paging vars used in template\n        self.page_numbers = [n for n in range(start_page, end_page) if 0 < n <= self.paginator.num_pages]\n        self.show_first = 1 not in self.page_numbers\n        self.show_last = self.paginator.num_pages not in self.page_numbers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a list of fabric tasks that are available for the project", "response": "def get_fabric_tasks(self, project):\n        \"\"\"\n        Generate a list of fabric tasks that are available\n        \"\"\"\n\n        cache_key = 'project_{}_fabfile_tasks'.format(project.pk)\n        cached_result = cache.get(cache_key)\n\n        if cached_result:\n            return cached_result\n\n        try:\n            fabfile_path, activate_loc = self.get_fabfile_path(project)\n\n            if activate_loc:\n                output = self.check_output(\n                    'source {};fab --list --list-format=short --fabfile={}'.format(activate_loc, fabfile_path),\n                    shell=True\n                )\n            else:\n                output = self.check_output(\n                    'fab --list --list-format=short --fabfile={}'.format(fabfile_path),\n                    shell=True\n                )\n\n            lines = output.splitlines()\n            tasks = []\n            for line in lines:\n                name = line.strip()\n                if activate_loc:\n                    o = self.check_output(\n                        'source {};fab --display={} --fabfile={}'.format(activate_loc, name, fabfile_path),\n                        shell=True\n                    )\n                else:\n                    o = self.check_output(\n                        ['fab', '--display={}'.format(name), '--fabfile={}'.format(fabfile_path)]\n                    )\n\n                tasks.append(self.parse_task_details(name, o))\n\n            cache.set(cache_key, tasks, settings.FABRIC_TASK_CACHE_TIMEOUT)\n        except Exception as e:\n            tasks = []\n\n        return tasks"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the initial data to use for forms on this view.", "response": "def get_initial(self):\n        \"\"\"\n        Returns the initial data to use for forms on this view.\n        \"\"\"\n        initial = super(ProjectCopy, self).get_initial()\n        if self.copy_object:\n            initial.update({'name': '%s copy' % self.copy_object.name,\n                            'description': self.copy_object.description,\n                            'use_repo_fabfile': self.copy_object.use_repo_fabfile,\n                            'fabfile_requirements': self.copy_object.fabfile_requirements,\n                            'repo_url': self.copy_object.repo_url})\n        return initial"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy the configuration of the object to the new object.", "response": "def copy_configurations(self, stages=None):\n        \"\"\"\n        Copy configuretions\n        \"\"\"\n        if stages:\n            confs = stages[0].stage_configurations()\n            new_stage = stages[1]\n        else:\n            confs = self.copy_object.project_configurations()\n            new_stage = None\n\n        for conf in confs:\n            new_conf = deepcopy(conf)\n            new_conf.id = None\n            new_conf.project = self.object\n            new_conf.stage = new_stage\n            new_conf.save()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the project on this configuration after it s valid", "response": "def form_valid(self, form):\n        \"\"\"Set the project on this configuration after it's valid\"\"\"\n\n        self.object = form.save(commit=False)\n        self.object.project = self.project\n\n        if self.kwargs.get('stage_id', None):\n            current_stage = models.Stage.objects.get(pk=self.kwargs.get('stage_id'))\n            self.object.stage = current_stage\n\n        self.object.save()\n\n        # Good to make note of that\n        messages.add_message(self.request, messages.SUCCESS, 'Configuration %s created' % self.object.key)\n\n        return super(ProjectConfigurationCreate, self).form_valid(form)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the url depending on what type of configuration I deleted.", "response": "def get_success_url(self):\n        \"\"\"Get the url depending on what type of configuration I deleted.\"\"\"\n\n        if self.stage_id:\n            url = reverse('projects_stage_view', args=(self.project_id, self.stage_id))\n        else:\n            url = reverse('projects_project_view', args=(self.project_id,))\n\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef form_valid(self, form):\n\n        self.object = form.save(commit=False)\n        self.object.project = self.project\n        self.object.save()\n\n        # Good to make note of that\n        messages.add_message(self.request, messages.SUCCESS, 'Stage %s created' % self.object.name)\n\n        return super(ProjectStageCreate, self).form_valid(form)", "response": "Set the project on this configuration after it s valid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npost data to the target url.", "response": "def run(self, target, payload, instance=None, hook_id=None, **kwargs):\n        \"\"\"\n        target:     the url to receive the payload.\n        payload:    a python primitive data structure\n        instance:   a possibly null \"trigger\" instance\n        hook:       the defining Hook object (useful for removing)\n        \"\"\"\n        self.post_data(target, payload, hook_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert(self, json=\"\", table_attributes='border=\"1\"', clubbing=True, encode=False, escape=True):\n        # table attributes such as class, id, data-attr-*, etc.\n        # eg: table_attributes = 'class = \"table table-bordered sortable\"'\n        self.table_init_markup = \"<table %s>\" % table_attributes\n        self.clubbing = clubbing\n        self.escape = escape\n        json_input = None\n        if not json:\n            json_input = {}\n        elif type(json) in text_types:\n            try:\n                json_input = json_parser.loads(json, object_pairs_hook=OrderedDict)\n            except ValueError as e:\n                #so the string passed here is actually not a json string\n                # - let's analyze whether we want to pass on the error or use the string as-is as a text node\n                if u\"Expecting property name\" in text(e):\n                    #if this specific json loads error is raised, then the user probably actually wanted to pass json, but made a mistake\n                    raise e\n                json_input = json\n        else:\n            json_input = json\n        converted = self.convert_json_node(json_input)\n        if encode:\n            return converted.encode('ascii', 'xmlcharrefreplace')\n        return converted", "response": "Convert JSON to HTML Table format"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a JSON node to HTML.", "response": "def convert_json_node(self, json_input):\n        \"\"\"\n            Dispatch JSON input according to the outermost type and process it\n            to generate the super awesome HTML format.\n            We try to adhere to duck typing such that users can just pass all kinds\n            of funky objects to json2html that *behave* like dicts and lists and other\n            basic JSON types.\n        \"\"\"\n        if type(json_input) in text_types:\n            if self.escape:\n                return cgi.escape(text(json_input))\n            else:\n                return text(json_input)\n        if hasattr(json_input, 'items'):\n            return self.convert_object(json_input)\n        if hasattr(json_input, '__iter__') and hasattr(json_input, '__getitem__'):\n            return self.convert_list(json_input)\n        return text(json_input)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_list(self, list_input):\n        if not list_input:\n            return \"\"\n        converted_output = \"\"\n        column_headers = None\n        if self.clubbing:\n            column_headers = self.column_headers_from_list_of_dicts(list_input)\n        if column_headers is not None:\n            converted_output += self.table_init_markup\n            converted_output += '<thead>'\n            converted_output += '<tr><th>' + '</th><th>'.join(column_headers) + '</th></tr>'\n            converted_output += '</thead>'\n            converted_output += '<tbody>'\n            for list_entry in list_input:\n                converted_output += '<tr><td>'\n                converted_output += '</td><td>'.join([self.convert_json_node(list_entry[column_header]) for column_header in\n                                                     column_headers])\n                converted_output += '</td></tr>'\n            converted_output += '</tbody>'\n            converted_output += '</table>'\n            return converted_output\n\n        #so you don't want or need clubbing eh? This makes @muellermichel very sad... ;(\n        #alright, let's fall back to a basic list here...\n        converted_output = '<ul><li>'\n        converted_output += '</li><li>'.join([self.convert_json_node(child) for child in list_input])\n        converted_output += '</li></ul>'\n        return converted_output", "response": "This function converts a list of items into HTML tables."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_object(self, json_input):\n        if not json_input:\n            return \"\" #avoid empty tables\n        converted_output = self.table_init_markup + \"<tr>\"\n        converted_output += \"</tr><tr>\".join([\n            \"<th>%s</th><td>%s</td>\" %(\n                self.convert_json_node(k),\n                self.convert_json_node(v)\n            )\n            for k, v in json_input.items()\n        ])\n        converted_output += '</tr></table>'\n        return converted_output", "response": "Convert the JSON object to HTML"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the vpn_url and local_url of a given camera and the local_url of the given camera.", "response": "def cameraUrls(self, camera=None, home=None, cid=None):\n        \"\"\"\n        Return the vpn_url and the local_url (if available) of a given camera\n        in order to access to its live feed\n        Can't use the is_local property which is mostly false in case of operator\n        dynamic IP change after presence start sequence\n        \"\"\"\n        local_url = None\n        vpn_url = None\n        if cid:\n            camera_data=self.cameraById(cid)\n        else:\n            camera_data=self.cameraByName(camera=camera, home=home)\n        if camera_data:\n            vpn_url = camera_data['vpn_url']\n            resp = postRequest(vpn_url + '/command/ping')\n            temp_local_url=resp['local_url']\n            try:\n                resp = postRequest(temp_local_url + '/command/ping',timeout=1)\n                if resp and temp_local_url == resp['local_url']:\n                    local_url = temp_local_url\n            except:  # On this particular request, vithout errors from previous requests, error is timeout\n                local_url = None\n        return vpn_url, local_url"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef personsAtHome(self, home=None):\n        if not home: home = self.default_home\n        home_data = self.homeByName(home)\n        atHome = []\n        for p in home_data['persons']:\n            #Only check known persons\n            if 'pseudo' in p:\n                if not p[\"out_of_sight\"]:\n                    atHome.append(p['pseudo'])\n        return atHome", "response": "Return the list of known persons who are currently at home"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getCameraPicture(self, image_id, key):\n        postParams = {\n            \"access_token\" : self.getAuthToken,\n            \"image_id\" : image_id,\n            \"key\" : key\n            }\n        resp = postRequest(_GETCAMERAPICTURE_REQ, postParams)\n        image_type = imghdr.what('NONE.FILE',resp)\n        return resp, image_type", "response": "Download a specific image from the camera"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the camera picture of a person", "response": "def getProfileImage(self, name):\n        \"\"\"\n        Retrieve the face of a given person\n        \"\"\"\n        for p in self.persons:\n            if 'pseudo' in self.persons[p]:\n                if name == self.persons[p]['pseudo']:\n                    image_id = self.persons[p]['face']['id']\n                    key = self.persons[p]['face']['key']\n                    return self.getCameraPicture(image_id, key)\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef updateEvent(self, event=None, home=None):\n        if not home: home=self.default_home\n        if not event:\n            #If not event is provided we need to retrieve the oldest of the last event seen by each camera\n            listEvent = dict()\n            for cam_id in self.lastEvent:\n                listEvent[self.lastEvent[cam_id]['time']] = self.lastEvent[cam_id]\n            event = listEvent[sorted(listEvent)[0]]\n\n        home_data = self.homeByName(home)\n        postParams = {\n            \"access_token\" : self.getAuthToken,\n            \"home_id\" : home_data['id'],\n            \"event_id\" : event['id']\n        }\n        resp = postRequest(_GETEVENTSUNTIL_REQ, postParams)\n        eventList = resp['body']['events_list']\n        for e in eventList:\n            self.events[ e['camera_id'] ][ e['time'] ] = e\n        for camera in self.events:\n            self.lastEvent[camera]=self.events[camera][sorted(self.events[camera])[-1]]", "response": "Update the list of event with the latest ones"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if a specific person has been seen by a camera.", "response": "def personSeenByCamera(self, name, home=None, camera=None):\n        \"\"\"\n        Return True if a specific person has been seen by a camera\n        \"\"\"\n        try:\n            cam_id = self.cameraByName(camera=camera, home=home)['id']\n        except TypeError:\n            logger.warning(\"personSeenByCamera: Camera name or home is unknown\")\n            return False\n        #Check in the last event is someone known has been seen\n        if self.lastEvent[cam_id]['type'] == 'person':\n            person_id = self.lastEvent[cam_id]['person_id']\n            if 'pseudo' in self.persons[person_id]:\n                if self.persons[person_id]['pseudo'] == name:\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef someoneKnownSeen(self, home=None, camera=None):\n        try:\n            cam_id = self.cameraByName(camera=camera, home=home)['id']\n        except TypeError:\n            logger.warning(\"personSeenByCamera: Camera name or home is unknown\")\n            return False\n        #Check in the last event is someone known has been seen\n        if self.lastEvent[cam_id]['type'] == 'person':\n            if self.lastEvent[cam_id]['person_id'] in self._knownPersons():\n                return True\n        return False", "response": "Return True if someone has been seen by camera."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if movement has been detected False otherwise.", "response": "def motionDetected(self, home=None, camera=None):\n        \"\"\"\n        Return True if movement has been detected\n        \"\"\"\n        try:\n            cam_id = self.cameraByName(camera=camera, home=home)['id']\n        except TypeError:\n            logger.warning(\"personSeenByCamera: Camera name or home is unknown\")\n            return False\n        if self.lastEvent[cam_id]['type'] == 'movement':\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call(self, path, query=None, method='GET', data=None,\n             files=None, get_all_pages=False, complete_response=False,\n             retry_on=None, max_retries=0, raw_query=None, retval=None,\n             **kwargs):\n        \"\"\"Make a REST call to the Zendesk web service.\n\n        Parameters:\n        path - Path portion of the Zendesk REST endpoint URL.\n        query - Query parameters in dict form.\n        method - HTTP method to use in making the request.\n        data - POST data or multi-part form data to include.\n        files - Requests style dict of files for multi-part file uploads.\n        get_all_pages - Make multiple requests and follow next_page.\n        complete_response - Return raw request results.\n        retry_on - Specify any exceptions from ACCEPT_RETRIES or non-2xx\n            HTTP codes on which you want to retry request.\n            Note that calling Zendesk.call with get_all_pages=True can make\n            up to (max_retries + 1) * pages.\n            Defaults to empty set, but can be any iterable, exception or int,\n            which will become set with same values you provided.\n        max_retries - How many additional connections to make when\n            first one fails. No effect when retry_on evaluates to False.\n            Defaults to 0.\n        raw_query - Raw query string, starting with '?', that will be\n            appended to the URL path and will completely override / discard\n            any other query parameters. Enables use cases where query\n            parameters need to be repeated in the query string.\n        retval - Request a specific part of the returned response. Valid\n            values are 'content', 'code', 'location', and 'headers'.\n            JSON content is still automatically deserialized if possible.\n            If retval is not specified, then the old behavior of trying\n            to determine an appropriate value to return is used.\n        \"\"\"\n\n        # Rather obscure way to support retry_on per single API call\n        if retry_on and max_retries:\n            try:\n                _retry_on = self._retry_on\n                _max_retries = self._max_retries\n\n                self.retry_on = retry_on\n                self.max_retries = max_retries\n                return self.call(path=path,\n                                 query=query,\n                                 method=method,\n                                 data=data,\n                                 files=files,\n                                 get_all_pages=get_all_pages,\n                                 complete_response=complete_response)\n            finally:\n                self._retry_on = _retry_on\n                self._max_retries = _max_retries\n\n        # Support specifying a mime-type other than application/json\n        mime_type = kwargs.pop('mime_type', 'application/json')\n\n        for key in kwargs.keys():\n            value = kwargs[key]\n            if hasattr(value, '__iter__') and not isinstance(value, str):\n                kwargs[key] = ','.join(map(str, value))\n\n        if query:\n            if kwargs:\n                kwargs.update(query)\n            else:\n                kwargs = query\n\n        if raw_query:\n            path = path + raw_query\n            kwargs = None\n\n        url = self.zdesk_url + path\n\n        if files:\n            # Sending multipart file. data contains parameters.\n            json = None\n            self.headers.pop('Content-Type', None)\n        elif (mime_type == 'application/json' and\n                (method == 'POST' or method == 'PUT')):\n            # Sending JSON data.\n            json = data\n            data = {}\n            self.headers.pop('Content-Type', None)\n        elif (mime_type != 'application/json' and\n                (method == 'POST' or method == 'PUT')):\n            # Uploading an attachment, probably.\n            # Specifying the MIME type is required.\n            json = None\n            self.headers['Content-Type'] = mime_type\n        else:\n            # Probably a GET or DELETE. Not sending JSON or files.\n            json = None\n            self.headers.pop('Content-Type', None)\n\n        results = []\n        all_requests_complete = False\n        request_count = 0\n\n        while not all_requests_complete:\n            # Make an http request\n            # counts request attempts in order to fetch this specific one\n            request_count += 1\n            try:\n                response = self.client.request(method,\n                                               url,\n                                               params=kwargs,\n                                               json=json,\n                                               data=data,\n                                               headers=self.headers,\n                                               files=files,\n                                               **self.client_args)\n            except requests.RequestException:\n                if request_count <= self.max_retries:\n                    # we have to bind response to None in case\n                    # self.client.request raises an exception and\n                    # response holds old requests.Response\n                    # (and possibly its Retry-After header)\n                    response = None\n                    self._handle_retry(response)\n                    continue\n                else:\n                    raise\n\n            # If the response status is not in the 200 range then assume an\n            # error and raise proper exception\n\n            code = response.status_code\n            try:\n                if not 200 <= code < 300 and code != 422:\n                    if code == 401:\n                        raise AuthenticationError(\n                            response.content, code, response)\n                    elif code == 429:\n                        raise RateLimitError(\n                            response.content, code, response)\n                    else:\n                        raise ZendeskError(\n                            response.content, code, response)\n            except ZendeskError:\n                if request_count <= self.max_retries:\n                    self._handle_retry(response)\n                    continue\n                else:\n                    raise\n\n            # Deserialize json content if content exists.\n            # In some cases Zendesk returns ' ' strings.\n            # Also return false non strings (0, [], (), {})\n            if response.content.strip() and 'json' in response.headers['content-type']:\n                content = response.json()\n\n                # set url to the next page if that was returned in the response\n                url = content.get('next_page', None)\n                # url we get above already has the start_time appended to it,\n                # specific to incremental exports\n                kwargs = {}\n            elif response.content.strip() and 'text' in response.headers['content-type']:\n                try:\n                    content = response.json()\n                    # set url to the next page if that was returned in the response\n                    url = content.get('next_page', None)\n                    # url we get above already has the start_time appended to it,\n                    # specific to incremental exports\n                    kwargs = {}\n                except ValueError:\n                    content = response.content\n            else:\n                content = response.content\n                url = None\n\n            if complete_response:\n                results.append({\n                    'response': response,\n                    'content': content,\n                    'status': response.status_code\n                })\n\n            else:\n                if retval == 'content':\n                    results.append(content)\n                elif retval == 'code':\n                    results.append(response.status_code)\n                elif retval == 'location':\n                    results.append(response.headers.get('location'))\n                elif retval == 'headers':\n                    results.append(response.headers)\n                else:\n                    # Attempt to automatically determine the value of\n                    # most interest to return.\n\n                    if response.headers.get('location'):\n                        # Zendesk's response is sometimes the url of a newly\n                        # created user/ticket/group/etc and they pass this through\n                        # 'location'.  Otherwise, the body of 'content'\n                        # has our response.\n                        results.append(response.headers.get('location'))\n                    elif content:\n                        results.append(content)\n                    else:\n                        results.append(responses[response.status_code])\n\n            # if there is a next_page, and we are getting pages, then continue\n            # making requests\n\n            # deal with how incremental export results are returned\n            # there could be two cases\n            # response code == 422 returned when end_time < five minutes recent\n            # or count < 1000\n            # this is an ugly check, and we have to check this just for incremental export end-points\n            # non-incremental load end-points have a 100 item/page limit and return next_page = null for last page\n            # also note that incremental/ticket_metric_events end-point has a 10,000 items per page limit\n            url = None if (url is not None and\n                           'incremental' in url and\n                           content.get('count') < 1000) else url\n            all_requests_complete = not (get_all_pages and url)\n            request_count = 0\n\n        if get_all_pages and complete_response:\n            # Return the list of results from all calls made.\n            # This way even if only one page was present the caller will\n            # always receive back an iterable value, since multiple pages\n            # were requested/expected. This also provides the information for\n            # every call, and saves us from having to try to combine all of\n            # that ourselves in a sensible way.\n            return results\n\n        if len(results) == 1:\n            # regardless as to whether all pages were requested, there was\n            # only one call and set of results, so just send it back.\n            return results[0]\n\n        # Now we need to try to combine or reduce the results:\n\n        hashable = True\n        try:\n            if len(set(results)) == 1:\n                # all responses were the same, so return just the first one.\n                # may have a list of locations or response statuses\n                return results[0]\n        except TypeError:\n            # probably we have a list of content dictionaries.\n            hashable = False\n\n        if hashable:\n            # we have a list of simple objects like strings, but they are not\n            # all the same so send them all back.\n            return results\n\n        # may have a sequence of response contents\n        # (dicts, possibly lists in the future as that is valid json also)\n        combined_dict_results = {}\n        combined_list_results = []\n        for result in results:\n            if isinstance(result, list):\n                # the result of this call returned a list.\n                # extend the combined list with these results.\n                combined_list_results.extend(result)\n            elif isinstance(result, dict):\n                # the result of this call returned a dict. the dict probably\n                # has both simple attributes (strings) and complex attributes\n                # (lists). if the attribute is a list, we will extend the\n                # combined attribute, otherwise we will just take the last\n                # attribute value from the last call.\n                # the end result is a response that looks like one giant call,\n                # to e.g. list tickets, but was actually made by multiple API\n                # calls.\n                for k in result.keys():\n                    v = result[k]\n                    if isinstance(v, list):\n                        try:\n                            combined_dict_results[k].extend(v)\n                        except KeyError:\n                            combined_dict_results[k] = v\n                    else:\n                        combined_dict_results[k] = v\n            else:\n                # returned result is not a dict or a list. don't know how to\n                # deal with this, so just send everything back.\n                return results\n\n        if combined_list_results and combined_dict_results:\n            # there was a mix of list and dict results from the sequence\n            # of calls. this case seems very odd to me if it ever happens.\n            # at any rate, send everything back uncombined\n            return results\n\n        if combined_dict_results:\n            return combined_dict_results\n\n        if combined_list_results:\n            return combined_list_results\n\n        # I don't expect to make it here, but I suppose it could happen if,\n        # perhaps, a sequence of empty dicts were returned or some such.\n        # Send everything back.\n        return results", "response": "Makes a Zendesk REST call to the Zendesk API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_retry(self, resp):\n        exc_t, exc_v, exc_tb = sys.exc_info()\n\n        if exc_t is None:\n            raise TypeError('Must be called in except block.')\n\n        retry_on_exc = tuple(\n            (x for x in self._retry_on if inspect.isclass(x)))\n        retry_on_codes = tuple(\n            (x for x in self._retry_on if isinstance(x, int)))\n\n        if issubclass(exc_t, ZendeskError):\n            code = exc_v.error_code\n            if exc_t not in retry_on_exc and code not in retry_on_codes:\n                six.reraise(exc_t, exc_v, exc_tb)\n        else:\n            if not issubclass(exc_t, retry_on_exc):\n                six.reraise(exc_t, exc_v, exc_tb)\n\n        if resp is not None:\n            try:\n                retry_after = float(resp.headers.get('Retry-After', 0))\n                time.sleep(retry_after)\n            except (TypeError, ValueError):\n                pass\n\n        return True", "response": "Handle any exceptions during API request or raises original Exception\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef account_settings_update(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/account_settings#update-account-settings\"\n        api_path = \"/api/v2/account/settings.json\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the account settings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef account_update(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/accounts#update-account\"\n        api_path = \"/api/v2/account\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the account s metadata."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef activities_list(self, since=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/activity_stream#list-activities\"\n        api_path = \"/api/v2/activities.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if since:\n            api_query.update({\n                \"since\": since,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "List the activities in the given time frame."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef activity_show(self, activity_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/activity_stream#show-activity\"\n        api_path = \"/api/v2/activities/{activity_id}.json\"\n        api_path = api_path.format(activity_id=activity_id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of the activity."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef agent_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/agents#create-agent\"\n        api_path = \"/api/v2/agents\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new agent in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the specified agent s metadata.", "response": "def agent_delete(self, agent_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/agents#delete-agent\"\n        api_path = \"/api/v2/agents/{agent_id}\"\n        api_path = api_path.format(agent_id=agent_id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the details of an agent s metadata.", "response": "def agent_show(self, agent_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/agents#get-agent-by-id\"\n        api_path = \"/api/v2/agents/{agent_id}\"\n        api_path = api_path.format(agent_id=agent_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the metadata for the agent.", "response": "def agent_update(self, agent_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/agents#update-agent\"\n        api_path = \"/api/v2/agents/{agent_id}\"\n        api_path = api_path.format(agent_id=agent_id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the details of an email.", "response": "def agents_email_show(self, email_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/agents#get-agent-by-email-id\"\n        api_path = \"/api/v2/agents/email/{email_id}\"\n        api_path = api_path.format(email_id=email_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef agents_me_update(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/agents#update-requesting-agent\"\n        api_path = \"/api/v2/agents/me\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the current state of the current agent s cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef any_channel_push_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/channel_framework#push-content-to-support\"\n        api_path = \"/api/v2/any_channel/push\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new push record for any channel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate the token for any channel.", "response": "def any_channel_validate_token_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/channel_framework#validate-token\"\n        api_path = \"/api/v2/any_channel/validate_token\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef app_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#create-app\"\n        api_path = \"/api/v2/apps.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create an app in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef app_public_key(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#get-app-public-key\"\n        api_path = \"/api/v2/apps/{id}/public_key.pem\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the public key of an app."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets information about an app.", "response": "def app_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#get-information-about-app\"\n        api_path = \"/api/v2/apps/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apps_installation_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#install-app\"\n        api_path = \"/api/v2/apps/installations.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new installation for the given app."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting an app installation.", "response": "def apps_installation_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#remove-app-installation\"\n        api_path = \"/api/v2/apps/installations/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apps_installation_requirements(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#list-requirements\"\n        api_path = \"/api/v2/apps/installations/{id}/requirements.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the requirements for the given app."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshow the details of an app installation.", "response": "def apps_installation_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#show-app-installation\"\n        api_path = \"/api/v2/apps/installations/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the status of a specific app installation job.", "response": "def apps_installations_job_status_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#get-requirements-install-status\"\n        api_path = \"/api/v2/apps/installations/job_statuses/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apps_job_status_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#get-job-status\"\n        api_path = \"/api/v2/apps/job_statuses/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the status of a specific app job."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreorder apps for a given location.", "response": "def apps_location_installations_reorder(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/app_location_installations#reorder-app-installations-for-location\"\n        api_path = \"/api/v2/apps/location_installations/reorder.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshowing the details of a specific app location.", "response": "def apps_location_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/app_locations#show-location\"\n        api_path = \"/api/v2/apps/locations/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a notification to the App.", "response": "def apps_notify_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#send-notification-to-app\"\n        api_path = \"/api/v2/apps/notify.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupload an app package to the current app.", "response": "def apps_upload_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/apps#upload-app-package\"\n        api_path = \"/api/v2/apps/uploads.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef attachment_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/attachments#show-attachment\"\n        api_path = \"/api/v2/attachments/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef audit_log_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/audit_logs#getting-audit-logs\"\n        api_path = \"/api/v2/audit_logs/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific audit log."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists the audit logs for the current user.", "response": "def audit_logs_list(self, filter_actor_id=None, filter_created_at=None, filter_ip_address=None, filter_source_type=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/audit_logs#listing-audit-logs\"\n        api_path = \"/api/v2/audit_logs.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if filter_actor_id:\n            api_query.update({\n                \"filter[actor_id]\": filter_actor_id,\n            })\n        if filter_created_at:\n            api_query.update({\n                \"filter[created_at]\": filter_created_at,\n            })\n        if filter_ip_address:\n            api_query.update({\n                \"filter[ip_address]\": filter_ip_address,\n            })\n        if filter_source_type:\n            api_query.update({\n                \"filter[source_type]\": filter_source_type,\n            })\n        return self.call(api_path, query=api_query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef automation_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/automations#create-automation\"\n        api_path = \"/api/v2/automations.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new automation for a given resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef automation_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/automations#show-automation\"\n        api_path = \"/api/v2/automations/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of an automation record."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ban_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/bans#create-ban\"\n        api_path = \"/api/v2/bans\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Ban a new resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a ban s base record set.", "response": "def ban_delete(self, ban_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/bans#delete-ban\"\n        api_path = \"/api/v2/bans/{ban_id}\"\n        api_path = api_path.format(ban_id=ban_id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ban_show(self, ban_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/bans#get-ban\"\n        api_path = \"/api/v2/bans/{ban_id}\"\n        api_path = api_path.format(ban_id=ban_id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific ban."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new bookmark for a given resource.", "response": "def bookmark_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/bookmarks#create-bookmark\"\n        api_path = \"/api/v2/bookmarks.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bookmark_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/bookmarks#delete-bookmark\"\n        api_path = \"/api/v2/bookmarks/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a bookmark from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef brand_check_host_mapping(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/brands#check-host-mapping-validity-for-an-existing-brand\"\n        api_path = \"/api/v2/brands/{id}/check_host_mapping.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Check if the given brand has a valid host mapping."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef brand_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/brands#create-brand\"\n        api_path = \"/api/v2/brands.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a brand for the given data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the details of a brand.", "response": "def brand_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/brands#show-a-brand\"\n        api_path = \"/api/v2/brands/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the host mapping validity of a given host.", "response": "def brands_check_host_mapping_list(self, host_mapping=None, subdomain=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/brands#check-host-mapping-validity\"\n        api_path = \"/api/v2/brands/check_host_mapping.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if host_mapping:\n            api_query.update({\n                \"host_mapping\": host_mapping,\n            })\n        if subdomain:\n            api_query.update({\n                \"subdomain\": subdomain,\n            })\n        return self.call(api_path, query=api_query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef business_hours_schedule_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/schedules#create-a-schedule\"\n        api_path = \"/api/v2/business_hours/schedules.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new business hours schedule."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a business hours schedule.", "response": "def business_hours_schedule_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/schedules#delete-a-schedule\"\n        api_path = \"/api/v2/business_hours/schedules/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting a Holiday from the Business hours schedule.", "response": "def business_hours_schedule_holiday_delete(self, schedule_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/schedules#delete-a-holiday\"\n        api_path = \"/api/v2/business_hours/schedules/{schedule_id}/holidays/{id}.json\"\n        api_path = api_path.format(schedule_id=schedule_id, id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a specific holiday by its ID.", "response": "def business_hours_schedule_holiday_show(self, schedule_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/schedules#show-a-holiday\"\n        api_path = \"/api/v2/business_hours/schedules/{schedule_id}/holidays/{id}.json\"\n        api_path = api_path.format(schedule_id=schedule_id, id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating a Holiday in the Business hours schedule.", "response": "def business_hours_schedule_holiday_update(self, schedule_id, id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/schedules#update-a-holiday\"\n        api_path = \"/api/v2/business_hours/schedules/{schedule_id}/holidays/{id}.json\"\n        api_path = api_path.format(schedule_id=schedule_id, id=id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting the holidays for a given business hours schedule.", "response": "def business_hours_schedule_holidays(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/schedules#list-holidays-for-a-schedule\"\n        api_path = \"/api/v2/business_hours/schedules/{id}/holidays.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow the details of a specific business hours schedule.", "response": "def business_hours_schedule_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/schedules#show-a-schedule\"\n        api_path = \"/api/v2/business_hours/schedules/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef channels_twitter_monitored_twitter_handle_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/twitter_channel#getting-monitored-twitter-handle\"\n        api_path = \"/api/v2/channels/twitter/monitored_twitter_handles/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a monitored twitter handle."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new Tweet from a Tweet.", "response": "def channels_twitter_ticket_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/twitter_channel#create-ticket-from-tweet\"\n        api_path = \"/api/v2/channels/twitter/tickets.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef channels_twitter_ticket_statuses(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/twitter_channel#getting-twicket-status\"\n        api_path = \"/api/v2/channels/twitter/tickets/{id}/statuses.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the statuses of a twitter ticket."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new display for a given agent and ticket.", "response": "def channels_voice_agent_ticket_display_create(self, agent_id, ticket_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/partner_edition#open-ticket-in-agents-browser\"\n        api_path = \"/api/v2/channels/voice/agents/{agent_id}/tickets/{ticket_id}/display.json\"\n        api_path = api_path.format(agent_id=agent_id, ticket_id=ticket_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new display for a user in an agent.", "response": "def channels_voice_agent_user_display_create(self, agent_id, user_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/partner_edition#open-a-users-profile-in-an-agents-browser\"\n        api_path = \"/api/v2/channels/voice/agents/{agent_id}/users/{user_id}/display.json\"\n        api_path = api_path.format(agent_id=agent_id, user_id=user_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing the availability of a channel.", "response": "def channels_voice_availability_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/availabilities#show-availability\"\n        api_path = \"/api/v2/channels/voice/availabilities/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the details of a greeting category.", "response": "def channels_voice_greeting_category_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/greetings#show-greeting-category\"\n        api_path = \"/api/v2/channels/voice/greeting_categories/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef channels_voice_greeting_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/greetings#create-greetings\"\n        api_path = \"/api/v2/channels/voice/greetings.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new greeting for the given resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a greeting from the given ID.", "response": "def channels_voice_greeting_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/greetings#delete-greeting\"\n        api_path = \"/api/v2/channels/voice/greetings/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef channels_voice_greeting_recording(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/greetings#get-greeting-audio-file\"\n        api_path = \"/api/v2/channels/voice/greetings/{id}/recording.mp3\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the recording for a given greeting."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the details of a dedicated greeting.", "response": "def channels_voice_greeting_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/greetings#show-greeting\"\n        api_path = \"/api/v2/channels/voice/greetings/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef channels_voice_phone_number_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/phone_numbers#create-phone-numbers\"\n        api_path = \"/api/v2/channels/voice/phone_numbers.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new phone number for the given channel."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef channels_voice_phone_number_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/phone_numbers#show-phone-number\"\n        api_path = \"/api/v2/channels/voice/phone_numbers/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific channel s phone number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef channels_voice_ticket_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/voice-api/partner_edition#create-ticket\"\n        api_path = \"/api/v2/channels/voice/tickets.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new ticket for the given data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chat_delete(self, chat_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/chats#delete-chat\"\n        api_path = \"/api/v2/chats/{chat_id}\"\n        api_path = api_path.format(chat_id=chat_id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete the last available cache for a chat."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the current user s base key cache for a chat.", "response": "def chat_show(self, chat_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/chats#get-chat\"\n        api_path = \"/api/v2/chats/{chat_id}\"\n        api_path = api_path.format(chat_id=chat_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the base data for the current channel.", "response": "def chat_update(self, chat_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/chats#update-chat\"\n        api_path = \"/api/v2/chats/{chat_id}\"\n        api_path = api_path.format(chat_id=chat_id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef chats_search(self, q=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/chats#search-chats\"\n        api_path = \"/api/v2/chats/search\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if q:\n            api_query.update({\n                \"q\": q,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Search for the chats in the system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef community_post_comment_delete(self, post_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/post_comments#delete-comment\"\n        api_path = \"/api/v2/community/posts/{post_id}/comments/{id}.json\"\n        api_path = api_path.format(post_id=post_id, id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a comment from a post."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new down comment for a post.", "response": "def community_post_comment_down_create(self, post_id, id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/votes#create-vote\"\n        api_path = \"/api/v2/community/posts/{post_id}/comments/{id}/down.json\"\n        api_path = api_path.format(post_id=post_id, id=id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef community_post_comment_show(self, post_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/post_comments#show-comment\"\n        api_path = \"/api/v2/community/posts/{post_id}/comments/{id}.json\"\n        api_path = api_path.format(post_id=post_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a comment in a post."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef community_post_comment_votes(self, post_id, comment_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/votes#list-votes\"\n        api_path = \"/api/v2/community/posts/{post_id}/comments/{comment_id}/votes.json\"\n        api_path = api_path.format(post_id=post_id, comment_id=comment_id)\n        return self.call(api_path, **kwargs)", "response": "Get the list of votes for a given comment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef community_post_comments(self, post_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/post_comments#list-comments\"\n        api_path = \"/api/v2/community/posts/{post_id}/comments.json\"\n        api_path = api_path.format(post_id=post_id)\n        return self.call(api_path, **kwargs)", "response": "List the comments for a post."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef community_post_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/posts#create-post\"\n        api_path = \"/api/v2/community/posts.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new post in the community."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef community_post_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/posts#delete-post\"\n        api_path = \"/api/v2/community/posts/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a post from the community."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow the details of a post.", "response": "def community_post_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/posts#show-post\"\n        api_path = \"/api/v2/community/posts/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef community_post_subscription_create(self, post_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#create-post-subscription\"\n        api_path = \"/api/v2/community/posts/{post_id}/subscriptions.json\"\n        api_path = api_path.format(post_id=post_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new subscription for a post."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef community_post_subscription_show(self, post_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#show-post-subscription\"\n        api_path = \"/api/v2/community/posts/{post_id}/subscriptions/{id}.json\"\n        api_path = api_path.format(post_id=post_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a specific post subscription."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef community_post_subscriptions(self, post_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#list-post-subscriptions\"\n        api_path = \"/api/v2/community/posts/{post_id}/subscriptions.json\"\n        api_path = api_path.format(post_id=post_id)\n        return self.call(api_path, **kwargs)", "response": "List the subscriptions for a post."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef community_post_votes(self, post_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/votes#list-votes\"\n        api_path = \"/api/v2/community/posts/{post_id}/votes.json\"\n        api_path = api_path.format(post_id=post_id)\n        return self.call(api_path, **kwargs)", "response": "List the votes for a given post."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef community_posts_list(self, filter_by=None, sort_by=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/posts#list-posts\"\n        api_path = \"/api/v2/community/posts.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if filter_by:\n            api_query.update({\n                \"filter_by\": filter_by,\n            })\n        if sort_by:\n            api_query.update({\n                \"sort_by\": sort_by,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "List the community posts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsearching for community posts.", "response": "def community_posts_search(self, query=None, topic=None, updated_after=None, updated_before=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/search#filtering-by-date\"\n        api_path = \"/api/v2/community/posts/search.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if query:\n            api_query.update({\n                \"query\": query,\n            })\n        if topic:\n            api_query.update({\n                \"topic\": topic,\n            })\n        if updated_after:\n            api_query.update({\n                \"updated_after\": updated_after,\n            })\n        if updated_before:\n            api_query.update({\n                \"updated_before\": updated_before,\n            })\n        return self.call(api_path, query=api_query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a topic in the community.", "response": "def community_topic_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/topics#create-topic\"\n        api_path = \"/api/v2/community/topics.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a topic from the community.", "response": "def community_topic_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/topics#delete-topic\"\n        api_path = \"/api/v2/community/topics/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef community_topic_posts(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/posts#list-posts\"\n        api_path = \"/api/v2/community/topics/{id}/posts.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "List the posts for a topic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef community_topic_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/topics#show-topic\"\n        api_path = \"/api/v2/community/topics/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific topic."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new subscription for a topic.", "response": "def community_topic_subscription_create(self, topic_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#create-topic-subscription\"\n        api_path = \"/api/v2/community/topics/{topic_id}/subscriptions.json\"\n        api_path = api_path.format(topic_id=topic_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a topic subscription.", "response": "def community_topic_subscription_delete(self, topic_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#delete-topic-subscription\"\n        api_path = \"/api/v2/community/topics/{topic_id}/subscriptions/{id}.json\"\n        api_path = api_path.format(topic_id=topic_id, id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the details of a specific topic subscription.", "response": "def community_topic_subscription_show(self, topic_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#show-topic-subscription\"\n        api_path = \"/api/v2/community/topics/{topic_id}/subscriptions/{id}.json\"\n        api_path = api_path.format(topic_id=topic_id, id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists the subscriptions for a topic.", "response": "def community_topic_subscriptions(self, topic_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#list-topic-subscriptions\"\n        api_path = \"/api/v2/community/topics/{topic_id}/subscriptions.json\"\n        api_path = api_path.format(topic_id=topic_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist the comments for the given user.", "response": "def community_user_comments(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/post_comments#list-comments\"\n        api_path = \"/api/v2/community/users/{id}/comments.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef community_user_posts(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/posts#list-posts\"\n        api_path = \"/api/v2/community/users/{id}/posts.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "List the user s posts."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef core_oauth_token_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/grant_type_tokens#create-token-for-grant-type\"\n        api_path = \"/oauth/tokens\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new OAuth token for a grant type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a ticket by ID.", "response": "def deleted_ticket_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#delete-ticket-permanently\"\n        api_path = \"/api/v2/deleted_tickets/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a user from the cluster.", "response": "def deleted_user_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#permanently-delete-user\"\n        api_path = \"/api/v2/deleted_users/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deleted_user_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#show-deleted-user\"\n        api_path = \"/api/v2/deleted_users/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a deleted user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new department.", "response": "def department_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/departments#create-department\"\n        api_path = \"/api/v2/departments\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef department_delete(self, department_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/departments#delete-department-by-id\"\n        api_path = \"/api/v2/departments/{department_id}\"\n        api_path = api_path.format(department_id=department_id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a specific department."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef department_show(self, department_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/departments#get-department-by-id\"\n        api_path = \"/api/v2/departments/{department_id}\"\n        api_path = api_path.format(department_id=department_id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific department."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the data for a specific department.", "response": "def department_update(self, department_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/departments#update-department-by-id\"\n        api_path = \"/api/v2/departments/{department_id}\"\n        api_path = api_path.format(department_id=department_id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a departments record by its name.", "response": "def departments_name_delete(self, name, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/departments#delete-department-by-name\"\n        api_path = \"/api/v2/departments/name/{name}\"\n        api_path = api_path.format(name=name)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef departments_name_show(self, name, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/departments#get-department-by-name\"\n        api_path = \"/api/v2/departments/name/{name}\"\n        api_path = api_path.format(name=name)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific departments resource."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new dynamic content item.", "response": "def dynamic_content_item_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/dynamic_content#create-item\"\n        api_path = \"/api/v2/dynamic_content/items.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a dynamic content item.", "response": "def dynamic_content_item_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/dynamic_content#delete-item\"\n        api_path = \"/api/v2/dynamic_content/items/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dynamic_content_item_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/dynamic_content#show-item\"\n        api_path = \"/api/v2/dynamic_content/items/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a dynamic content item."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dynamic_content_item_variant_delete(self, item_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/dynamic_content#delete-variant\"\n        api_path = \"/api/v2/dynamic_content/items/{item_id}/variants/{id}.json\"\n        api_path = api_path.format(item_id=item_id, id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a variant from a dynamic content item."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dynamic_content_item_variant_show(self, item_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/dynamic_content#show-variant\"\n        api_path = \"/api/v2/dynamic_content/items/{item_id}/variants/{id}.json\"\n        api_path = api_path.format(item_id=item_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Show a single variant for a given item."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate a variant in the dynamic content.", "response": "def dynamic_content_item_variant_update(self, item_id, id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/dynamic_content#update-variant\"\n        api_path = \"/api/v2/dynamic_content/items/{item_id}/variants/{id}.json\"\n        api_path = api_path.format(item_id=item_id, id=id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist the variants of a dynamic content item.", "response": "def dynamic_content_item_variants(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/dynamic_content#list-variants\"\n        api_path = \"/api/v2/dynamic_content/items/{id}/variants.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dynamic_content_items_show_many(self, identifiers=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/dynamic_content#show-many-items\"\n        api_path = \"/api/v2/dynamic_content/items/show_many.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if identifiers:\n            api_query.update({\n                \"identifiers\": identifiers,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Show many items in dynamic content."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef end_user_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/end_user#show-user\"\n        api_path = \"/api/v2/end_users/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a user s metadata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef goal_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/goals#create-goal\"\n        api_path = \"/api/v2/goals\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new goal."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete the current node s cluster cache.", "response": "def goal_delete(self, goal_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/goals#delete-goal\"\n        api_path = \"/api/v2/goals/{goal_id}\"\n        api_path = api_path.format(goal_id=goal_id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the details of a specific branching goal.", "response": "def goal_show(self, goal_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/goals#get-goal\"\n        api_path = \"/api/v2/goals/{goal_id}\"\n        api_path = api_path.format(goal_id=goal_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef goal_update(self, goal_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/goals#update-goal\"\n        api_path = \"/api/v2/goals/{goal_id}\"\n        api_path = api_path.format(goal_id=goal_id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the current state of a specific goal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new graphql request.", "response": "def graphql_request_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/conversations-api#api-path\"\n        api_path = \"/graphql/request\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new group.", "response": "def group_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/groups#create-group\"\n        api_path = \"/api/v2/groups.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting the group s metadata.", "response": "def group_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/groups#delete-group\"\n        api_path = \"/api/v2/groups/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef group_membership_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/group_memberships#create-membership\"\n        api_path = \"/api/v2/group_memberships.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new group membership for a resource."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes the group membership for a specific resource.", "response": "def group_membership_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/group_memberships#delete-membership\"\n        api_path = \"/api/v2/group_memberships/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef group_membership_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/group_memberships#show-membership\"\n        api_path = \"/api/v2/group_memberships/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a group membership."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting the memberships of a group.", "response": "def group_memberships(self, group_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/group_memberships#list-memberships\"\n        api_path = \"/api/v2/groups/{group_id}/memberships.json\"\n        api_path = api_path.format(group_id=group_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting the group memberships that are assigned to the current user.", "response": "def group_memberships_assignable(self, group_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/group_memberships#list-assignable-memberships\"\n        api_path = \"/api/v2/groups/{group_id}/memberships/assignable.json\"\n        api_path = api_path.format(group_id=group_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef group_memberships_create_many(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/group_memberships#bulk-create-memberships\"\n        api_path = \"/api/v2/group_memberships/create_many.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create many group memberships for a given set of group resources."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes many group memberships.", "response": "def group_memberships_destroy_many(self, ids=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/group_memberships#bulk-delete-memberships\"\n        api_path = \"/api/v2/group_memberships/destroy_many.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if ids:\n            api_query.update({\n                \"ids\": ids,\n            })\n        return self.call(api_path, query=api_query, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef group_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/groups#show-group\"\n        api_path = \"/api/v2/groups/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a group s base cache entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a comment from the help center.", "response": "def help_center_article_comment_delete(self, article_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/comments#delete-comment\"\n        api_path = \"/api/v2/help_center/articles/{article_id}/comments/{id}.json\"\n        api_path = api_path.format(article_id=article_id, id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef help_center_article_comment_down_create(self, article_id, id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/votes#create-vote\"\n        api_path = \"/api/v2/help_center/articles/{article_id}/comments/{id}/down.json\"\n        api_path = api_path.format(article_id=article_id, id=id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new comment down in the help center."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef help_center_article_comment_votes(self, article_id, comment_id, locale=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/votes#list-votes\"\n        api_path = \"/api/v2/help_center/articles/{article_id}/comments/{comment_id}/votes.json\"\n        api_path = api_path.format(article_id=article_id, comment_id=comment_id)\n        if locale:\n            api_opt_path = \"/api/v2/help_center/{locale}/articles/{article_id}/comments/{comment_id}/votes.json\"\n            api_path = api_opt_path.format(article_id=article_id, comment_id=comment_id, locale=locale)\n        return self.call(api_path, **kwargs)", "response": "List the votes for a specific comment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef help_center_article_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/articles#archive-article\"\n        api_path = \"/api/v2/help_center/articles/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete an article from the help center."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef help_center_article_labels(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/labels#list-article-labels\"\n        api_path = \"/api/v2/help_center/articles/{id}/labels.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "List the labels for an article."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef help_center_article_show(self, locale, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/articles#show-article\"\n        api_path = \"/api/v2/help_center/{locale}/articles/{id}.json\"\n        api_path = api_path.format(locale=locale, id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of an article in the help center."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef help_center_article_subscription_show(self, article_id, id, locale=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#show-article-subscription\"\n        api_path = \"/api/v2/help_center/articles/{article_id}/subscriptions/{id}.json\"\n        api_path = api_path.format(article_id=article_id, id=id)\n        if locale:\n            api_opt_path = \"/api/v2/help_center/{locale}/articles/{article_id}/subscriptions/{id}.json\"\n            api_path = api_opt_path.format(article_id=article_id, id=id, locale=locale)\n        return self.call(api_path, **kwargs)", "response": "Show the details of an article subscription."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists the subscriptions for an article.", "response": "def help_center_article_subscriptions(self, article_id, locale=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#list-article-subscriptions\"\n        api_path = \"/api/v2/help_center/articles/{article_id}/subscriptions.json\"\n        api_path = api_path.format(article_id=article_id)\n        if locale:\n            api_opt_path = \"/api/v2/help_center/{locale}/articles/{article_id}/subscriptions.json\"\n            api_path = api_opt_path.format(article_id=article_id, locale=locale)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a translation for an article.", "response": "def help_center_article_translation_create(self, article_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#create-translation\"\n        api_path = \"/api/v2/help_center/articles/{article_id}/translations.json\"\n        api_path = api_path.format(article_id=article_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow the translation for the given article and locale.", "response": "def help_center_article_translation_show(self, article_id, locale, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#show-translation\"\n        api_path = \"/api/v2/help_center/articles/{article_id}/translations/{locale}.json\"\n        api_path = api_path.format(article_id=article_id, locale=locale)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the translation for an article.", "response": "def help_center_article_translation_update(self, article_id, locale, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#update-translation\"\n        api_path = \"/api/v2/help_center/articles/{article_id}/translations/{locale}.json\"\n        api_path = api_path.format(article_id=article_id, locale=locale)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef help_center_article_translations(self, article_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#list-translations\"\n        api_path = \"/api/v2/help_center/articles/{article_id}/translations.json\"\n        api_path = api_path.format(article_id=article_id)\n        return self.call(api_path, **kwargs)", "response": "List the translations for an article."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef help_center_article_translations_missing(self, article_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#list-missing-translations\"\n        api_path = \"/api/v2/help_center/articles/{article_id}/translations/missing.json\"\n        api_path = api_path.format(article_id=article_id)\n        return self.call(api_path, **kwargs)", "response": "List missing translations for an article."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef help_center_articles_attachment_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/article_attachments#create-unassociated-attachment\"\n        api_path = \"/api/v2/help_center/articles/attachments.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create an unassociated attachment for the given article."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef help_center_articles_attachment_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/article_attachments#delete-article-attachment\"\n        api_path = \"/api/v2/help_center/articles/attachments/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete an article attachment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef help_center_articles_label_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/labels#show-label\"\n        api_path = \"/api/v2/help_center/articles/labels/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a label."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef help_center_articles_labels_list(self, locale=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/labels#list-all-labels\"\n        api_path = \"/api/v2/help_center/articles/labels.json\"\n        if locale:\n            api_opt_path = \"/api/v2/help_center/{locale}/articles/labels.json\"\n            api_path = api_opt_path.format(locale=locale)\n        return self.call(api_path, **kwargs)", "response": "List all labels for a given locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef help_center_categories_list(self, locale=None, sort_by=None, sort_order=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/categories#list-categories\"\n        api_path = \"/api/v2/help_center/categories.json\"\n        if locale:\n            api_opt_path = \"/api/v2/help_center/{locale}/categories.json\"\n            api_path = api_opt_path.format(locale=locale)\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if sort_by:\n            api_query.update({\n                \"sort_by\": sort_by,\n            })\n        if sort_order:\n            api_query.update({\n                \"sort_order\": sort_order,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "List the categories in the help center."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef help_center_category_create(self, data, locale=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/categories#create-category\"\n        api_path = \"/api/v2/help_center/categories.json\"\n        if locale:\n            api_opt_path = \"/api/v2/help_center/{locale}/categories.json\"\n            api_path = api_opt_path.format(locale=locale)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new category in the help center."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a Category from the Help Center.", "response": "def help_center_category_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/categories#delete-category\"\n        api_path = \"/api/v2/help_center/categories/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a translation for a given category.", "response": "def help_center_category_translation_create(self, category_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#create-translation\"\n        api_path = \"/api/v2/help_center/categories/{category_id}/translations.json\"\n        api_path = api_path.format(category_id=category_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the translation for a specific category.", "response": "def help_center_category_translation_update(self, category_id, locale, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#update-translation\"\n        api_path = \"/api/v2/help_center/categories/{category_id}/translations/{locale}.json\"\n        api_path = api_path.format(category_id=category_id, locale=locale)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef help_center_category_translations(self, category_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#list-translations\"\n        api_path = \"/api/v2/help_center/categories/{category_id}/translations.json\"\n        api_path = api_path.format(category_id=category_id)\n        return self.call(api_path, **kwargs)", "response": "List the translations for a given category."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef help_center_category_translations_missing(self, category_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#list-missing-translations\"\n        api_path = \"/api/v2/help_center/categories/{category_id}/translations/missing.json\"\n        api_path = api_path.format(category_id=category_id)\n        return self.call(api_path, **kwargs)", "response": "List missing translations for a given category."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef help_center_category_update(self, id, data, locale=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/categories#update-category\"\n        api_path = \"/api/v2/help_center/categories/{id}.json\"\n        api_path = api_path.format(id=id)\n        if locale:\n            api_opt_path = \"/api/v2/help_center/{locale}/categories/{id}.json\"\n            api_path = api_opt_path.format(id=id, locale=locale)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the data for a specific category."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting the articles in the Incremental article store.", "response": "def help_center_incremental_articles_list(self, start_time=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/articles#list-articles\"\n        api_path = \"/api/v2/help_center/incremental/articles.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if start_time:\n            api_query.update({\n                \"start_time\": start_time,\n            })\n        return self.call(api_path, query=api_query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists the articles in the section.", "response": "def help_center_section_articles(self, id, locale=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/articles#list-articles\"\n        api_path = \"/api/v2/help_center/sections/{id}/articles.json\"\n        api_path = api_path.format(id=id)\n        if locale:\n            api_opt_path = \"/api/v2/help_center/{locale}/sections/{id}/articles.json\"\n            api_path = api_opt_path.format(id=id, locale=locale)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a section from the help center.", "response": "def help_center_section_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/sections#delete-section\"\n        api_path = \"/api/v2/help_center/sections/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef help_center_section_subscription_delete(self, section_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#delete-section-subscription\"\n        api_path = \"/api/v2/help_center/sections/{section_id}/subscriptions/{id}.json\"\n        api_path = api_path.format(section_id=section_id, id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a section subscription."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshowing the details of a specific section subscription.", "response": "def help_center_section_subscription_show(self, section_id, id, locale=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#show-section-subscription\"\n        api_path = \"/api/v2/help_center/sections/{section_id}/subscriptions/{id}.json\"\n        api_path = api_path.format(section_id=section_id, id=id)\n        if locale:\n            api_opt_path = \"/api/v2/help_center/{locale}/sections/{section_id}/subscriptions/{id}.json\"\n            api_path = api_opt_path.format(section_id=section_id, id=id, locale=locale)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist the subscriptions for a specific section.", "response": "def help_center_section_subscriptions(self, section_id, locale=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#list-section-subscriptions\"\n        api_path = \"/api/v2/help_center/sections/{section_id}/subscriptions.json\"\n        api_path = api_path.format(section_id=section_id)\n        if locale:\n            api_opt_path = \"/api/v2/help_center/{locale}/sections/{section_id}/subscriptions.json\"\n            api_path = api_opt_path.format(section_id=section_id, locale=locale)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new translation for a section.", "response": "def help_center_section_translation_create(self, section_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#create-translation\"\n        api_path = \"/api/v2/help_center/sections/{section_id}/translations.json\"\n        api_path = api_path.format(section_id=section_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef help_center_section_translation_update(self, section_id, locale, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#update-translation\"\n        api_path = \"/api/v2/help_center/sections/{section_id}/translations/{locale}.json\"\n        api_path = api_path.format(section_id=section_id, locale=locale)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the translation for a specific locale in a section."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists the translations for a section.", "response": "def help_center_section_translations(self, section_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#list-translations\"\n        api_path = \"/api/v2/help_center/sections/{section_id}/translations.json\"\n        api_path = api_path.format(section_id=section_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef help_center_section_translations_missing(self, section_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#list-missing-translations\"\n        api_path = \"/api/v2/help_center/sections/{section_id}/translations/missing.json\"\n        api_path = api_path.format(section_id=section_id)\n        return self.call(api_path, **kwargs)", "response": "List missing translations for a section."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a translation from the Help Center.", "response": "def help_center_translation_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/translations#delete-translation\"\n        api_path = \"/api/v2/help_center/translations/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef help_center_user_articles(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/articles#list-articles\"\n        api_path = \"/api/v2/help_center/users/{id}/articles.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "List the user s articles."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef help_center_user_comments(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/comments#list-comments\"\n        api_path = \"/api/v2/help_center/users/{id}/comments.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "List the comments for the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef help_center_user_segment_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/user_segments#create-user-segment\"\n        api_path = \"/api/v2/help_center/user_segments.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new user segment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef help_center_user_segment_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/user_segments#delete-user-segment\"\n        api_path = \"/api/v2/help_center/user_segments/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a user segment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef help_center_user_segment_sections(self, user_segment_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/user_segments#list-sections-using-a-user-segment\"\n        api_path = \"/api/v2/help_center/user_segments/{user_segment_id}/sections.json\"\n        api_path = api_path.format(user_segment_id=user_segment_id)\n        return self.call(api_path, **kwargs)", "response": "List the user segment sections."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef help_center_user_segment_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/user_segments#show-user-segment\"\n        api_path = \"/api/v2/help_center/user_segments/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a user segment."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef help_center_user_segment_topics(self, user_segment_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/user_segments#list-topics-using-a-user-segment\"\n        api_path = \"/api/v2/help_center/user_segments/{user_segment_id}/topics.json\"\n        api_path = api_path.format(user_segment_id=user_segment_id)\n        return self.call(api_path, **kwargs)", "response": "List the topics for a user segment."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist the user s subscriptions.", "response": "def help_center_user_subscriptions(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#list-subscriptions-by-user\"\n        api_path = \"/api/v2/help_center/users/{user_id}/subscriptions.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef help_center_user_user_segments(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/user_segments#list-user-segments\"\n        api_path = \"/api/v2/help_center/users/{user_id}/user_segments.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)", "response": "List the user segments for the given user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists the user s current user s available votes.", "response": "def help_center_user_votes(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/votes#list-votes\"\n        api_path = \"/api/v2/help_center/users/{user_id}/votes.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a single resource from the help center.", "response": "def help_center_vote_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/votes#delete-vote\"\n        api_path = \"/api/v2/help_center/votes/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef help_center_vote_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/help_center/votes#show-vote\"\n        api_path = \"/api/v2/help_center/votes/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef imports_ticket(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_import#ticket-import\"\n        api_path = \"/api/v2/imports/tickets.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Import a ticket from a set of data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new set of tickets for a given set of items.", "response": "def imports_tickets_create_many(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_import#ticket-bulk-import\"\n        api_path = \"/api/v2/imports/tickets/create_many.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsamples the current set of items from the incremental export.", "response": "def incremental_sample(self, item, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/incremental_export#incremental-sample-export\"\n        api_path = \"/api/v2/incremental/{item}/sample.json\"\n        api_path = api_path.format(item=item)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist the incremental tickets for the current user.", "response": "def incremental_tickets_list(self, include=None, start_time=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/incremental_export#start-time\"\n        api_path = \"/api/v2/incremental/tickets.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if include:\n            api_query.update({\n                \"include\": include,\n            })\n        if start_time:\n            api_query.update({\n                \"start_time\": start_time,\n            })\n        return self.call(api_path, query=api_query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow the status of a specific job.", "response": "def job_status_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/job_statuses#show-job-status\"\n        api_path = \"/api/v2/job_statuses/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the details of a locale.", "response": "def locale_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/locales#show-locale\"\n        api_path = \"/api/v2/locales/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply a macro to a specific resource.", "response": "def macro_apply(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/macros#show-changes-to-ticket\"\n        api_path = \"/api/v2/macros/{id}/apply.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef macro_attachment_create(self, macro_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/macros#create-macro-attachment\"\n        api_path = \"/api/v2/macros/{macro_id}/attachments.json\"\n        api_path = api_path.format(macro_id=macro_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new macro attachment."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the list of attachments for a given macro.", "response": "def macro_attachments(self, macro_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/macros#list-macro-attachments\"\n        api_path = \"/api/v2/macros/{macro_id}/attachments.json\"\n        api_path = api_path.format(macro_id=macro_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new macro.", "response": "def macro_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/macros#create-macro\"\n        api_path = \"/api/v2/macros.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef macro_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/macros#delete-macro\"\n        api_path = \"/api/v2/macros/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a macro from the API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the details of a specific macro.", "response": "def macro_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/macros#show-macro\"\n        api_path = \"/api/v2/macros/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new macro attachment.", "response": "def macros_attachment_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/macros#create-unassociated-macro-attachment\"\n        api_path = \"/api/v2/macros/attachments.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef macros_attachment_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/macros#show-macro-attachment\"\n        api_path = \"/api/v2/macros/attachments/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific macro attachment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef macros_update_many(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/macros#update-many-macros\"\n        api_path = \"/api/v2/macros/update_many.json\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the MACROs for a given set of MACROs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nps_survey_invitation_show(self, survey_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/invitations#show-invitation\"\n        api_path = \"/api/v2/nps/surveys/{survey_id}/invitations/{id}.json\"\n        api_path = api_path.format(survey_id=survey_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of an invitation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nps_survey_invitations(self, survey_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/invitations#list-invitations\"\n        api_path = \"/api/v2/nps/surveys/{survey_id}/invitations.json\"\n        api_path = api_path.format(survey_id=survey_id)\n        return self.call(api_path, **kwargs)", "response": "List the invitations for a given survey."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nps_survey_preview(self, id, locale=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/surveys#preview-survey\"\n        api_path = \"/api/v2/nps/surveys/{id}/preview\"\n        api_path = api_path.format(id=id)\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if locale:\n            api_query.update({\n                \"locale\": locale,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Preview a survey s current state."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows the details of a single recipient.", "response": "def nps_survey_recipient_show(self, survey_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/recipients#show-recipient\"\n        api_path = \"/api/v2/nps/surveys/{survey_id}/recipients/{id}.json\"\n        api_path = api_path.format(survey_id=survey_id, id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists the recipients for a given survey.", "response": "def nps_survey_recipients(self, survey_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/recipients#list-recipients\"\n        api_path = \"/api/v2/nps/surveys/{survey_id}/recipients.json\"\n        api_path = api_path.format(survey_id=survey_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nps_survey_recipients_search(self, survey_id, email=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/recipients#search-recipients\"\n        api_path = \"/api/v2/nps/surveys/{survey_id}/recipients/search.json\"\n        api_path = api_path.format(survey_id=survey_id)\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if email:\n            api_query.update({\n                \"email\": email,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Search for the set of recipients for a given survey."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new response for a given survey.", "response": "def nps_survey_response_create(self, survey_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/responses#create-response\"\n        api_path = \"/api/v2/nps/surveys/{survey_id}/responses.json\"\n        api_path = api_path.format(survey_id=survey_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nps_survey_response_show(self, survey_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/responses#show-response\"\n        api_path = \"/api/v2/nps/surveys/{survey_id}/responses/{id}.json\"\n        api_path = api_path.format(survey_id=survey_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a response."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the response for a given survey.", "response": "def nps_survey_response_update(self, survey_id, id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/responses#update-response\"\n        api_path = \"/api/v2/nps/surveys/{survey_id}/responses/{id}.json\"\n        api_path = api_path.format(survey_id=survey_id, id=id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef nps_survey_responses(self, survey_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/responses#list-responses\"\n        api_path = \"/api/v2/nps/surveys/{survey_id}/responses.json\"\n        api_path = api_path.format(survey_id=survey_id)\n        return self.call(api_path, **kwargs)", "response": "List the available NPS response types for a given survey."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nps_survey_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/surveys#show-survey\"\n        api_path = \"/api/v2/nps/surveys/{id}\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Return the details of a single NPS Survey."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the data for the NPS Survey 1.", "response": "def nps_surveys_1_update(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/nps-api/surveys#update-survey\"\n        api_path = \"/api/v2/nps/surveys/1\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef oauth_client_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/oauth_clients#create-client\"\n        api_path = \"/api/v2/oauth/clients.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new OAuth client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting an OAuth client.", "response": "def oauth_client_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/oauth_clients#delete-client\"\n        api_path = \"/api/v2/oauth/clients/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the information of an OAuth client.", "response": "def oauth_client_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/oauth_clients#show-client\"\n        api_path = \"/api/v2/oauth/clients/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef oauth_token_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/oauth_tokens#create-token\"\n        api_path = \"/api/v2/oauth/tokens.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new OAuth token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete an OAuth token.", "response": "def oauth_token_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/oauth_tokens#revoke-token\"\n        api_path = \"/api/v2/oauth/tokens/{id}\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the details of an OAuth token.", "response": "def oauth_token_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/oauth_tokens#show-token\"\n        api_path = \"/api/v2/oauth/tokens/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an organization field.", "response": "def organization_field_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_fields#create-organization-fields\"\n        api_path = \"/api/v2/organization_fields.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes an organization field.", "response": "def organization_field_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_fields#delete-organization-field\"\n        api_path = \"/api/v2/organization_fields/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef organization_field_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_fields#show-organization-field\"\n        api_path = \"/api/v2/organization_fields/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of an organization field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new organization membership for a resource.", "response": "def organization_membership_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_memberships#create-membership\"\n        api_path = \"/api/v2/organization_memberships.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef organization_membership_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_memberships#delete-membership\"\n        api_path = \"/api/v2/organization_memberships/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete the organization membership for the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef organization_membership_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_memberships#show-membership\"\n        api_path = \"/api/v2/organization_memberships/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of the organization membership for a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates many organization memberships for a given set of organization resources.", "response": "def organization_memberships_create_many(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_memberships#create-many-memberships\"\n        api_path = \"/api/v2/organization_memberships/create_many.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef organization_organization_memberships(self, organization_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_memberships#list-memberships\"\n        api_path = \"/api/v2/organizations/{organization_id}/organization_memberships.json\"\n        api_path = api_path.format(organization_id=organization_id)\n        return self.call(api_path, **kwargs)", "response": "List the organization memberships for the given organization."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef organization_related(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organizations#show-organizations-related-information\"\n        api_path = \"/api/v2/organizations/{id}/related.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the related organizations for a given organization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist the requests in the organization.", "response": "def organization_requests(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/requests#list-requests\"\n        api_path = \"/api/v2/organizations/{id}/requests.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef organization_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organizations#show-organization\"\n        api_path = \"/api/v2/organizations/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of the organization with the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an organization subscription.", "response": "def organization_subscription_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_subscriptions#create-organization-subscription\"\n        api_path = \"/api/v2/organization_subscriptions.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef organization_subscription_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_subscriptions#delete-organization-subscription\"\n        api_path = \"/api/v2/organization_subscriptions/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete an organization subscription."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef organization_subscription_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_subscriptions#show-organization-subscription\"\n        api_path = \"/api/v2/organization_subscriptions/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of an organization subscription."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef organization_subscriptions(self, organization_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_subscriptions#list-organization-subscriptions\"\n        api_path = \"/api/v2/organizations/{organization_id}/subscriptions.json\"\n        api_path = api_path.format(organization_id=organization_id)\n        return self.call(api_path, **kwargs)", "response": "List the subscriptions for the given organization."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef organization_tags(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tags#show-tags\"\n        api_path = \"/api/v2/organizations/{id}/tags.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the tags for the organization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting the tags for the organization.", "response": "def organization_tags_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tags#remove-tags\"\n        api_path = \"/api/v2/organizations/{id}/tags.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef organization_tickets(self, organization_id, external_id=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#allowed-for\"\n        api_path = \"/api/v2/organizations/{organization_id}/tickets.json\"\n        api_path = api_path.format(organization_id=organization_id)\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if external_id:\n            api_query.update({\n                \"external_id\": external_id,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Get the list of available items for the organization."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef organization_users(self, id, permission_set=None, role=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#list-users\"\n        api_path = \"/api/v2/organizations/{id}/users.json\"\n        api_path = api_path.format(id=id)\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if permission_set:\n            api_query.update({\n                \"permission_set\": permission_set,\n            })\n        if role:\n            api_query.update({\n                \"role\": role,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "List the users in an organization."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef organizations_create_or_update(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organizations#create-or-update-organization\"\n        api_path = \"/api/v2/organizations/create_or_update.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create or update the organization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching for organizations by external_id.", "response": "def organizations_search(self, external_id=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organizations#search-organizations-by-external-id\"\n        api_path = \"/api/v2/organizations/search.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if external_id:\n            api_query.update({\n                \"external_id\": external_id,\n            })\n        return self.call(api_path, query=api_query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef problems_autocomplete(self, data, text=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#autocomplete-problems\"\n        api_path = \"/api/v2/problems/autocomplete.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if text:\n            api_query.update({\n                \"text\": text,\n            })\n        return self.call(api_path, query=api_query, method=\"POST\", data=data, **kwargs)", "response": "Autocomplete for a set of problems."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new Support Address", "response": "def recipient_address_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/support_addresses#create-support-address\"\n        api_path = \"/api/v2/recipient_addresses.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a specific recipient address.", "response": "def recipient_address_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/support_addresses#delete-recipient-address\"\n        api_path = \"/api/v2/recipient_addresses/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets information about a specific recipient address.", "response": "def recipient_address_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/support_addresses#show-support-address\"\n        api_path = \"/api/v2/recipient_addresses/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request_comment_show(self, request_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/requests#getting-comments\"\n        api_path = \"/api/v2/requests/{request_id}/comments/{id}.json\"\n        api_path = api_path.format(request_id=request_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific comment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef request_comments(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/requests#listing-comments\"\n        api_path = \"/api/v2/requests/{id}/comments.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the comments for a specific request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new request in the cluster.", "response": "def request_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/requests#create-request\"\n        api_path = \"/api/v2/requests.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/requests#show-request\"\n        api_path = \"/api/v2/requests/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef requests_list(self, status=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/requests#list-requests\"\n        api_path = \"/api/v2/requests.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if status:\n            api_query.update({\n                \"status\": status,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "List the available requests for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef requests_search(self, cc_id=None, organization_id=None, query=None, status=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/requests#search-requests\"\n        api_path = \"/api/v2/requests/search.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if cc_id:\n            api_query.update({\n                \"cc_id\": cc_id,\n            })\n        if organization_id:\n            api_query.update({\n                \"organization_id\": organization_id,\n            })\n        if query:\n            api_query.update({\n                \"query\": query,\n            })\n        if status:\n            api_query.update({\n                \"status\": status,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Search for the requests in the specified organization."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resource_collection_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/resource_collections#create-a-resource-collection\"\n        api_path = \"/api/v2/resource_collections.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a resource collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resource_collection_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/resource_collections#delete-a-resource-collection\"\n        api_path = \"/api/v2/resource_collections/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a resource collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resource_collection_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/resource_collections#retrieve-a-resource-collection\"\n        api_path = \"/api/v2/resource_collections/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get a specific resource collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resource_collections_update(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/resource_collections#update-a-resource-collection\"\n        api_path = \"/api/v2/resource_collections.json\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the resource collection with the given data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new role for the current user.", "response": "def role_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/roles#create-role\"\n        api_path = \"/api/v2/roles\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a role from the cache.", "response": "def role_delete(self, role_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/roles#delete-role\"\n        api_path = \"/api/v2/roles/{role_id}\"\n        api_path = api_path.format(role_id=role_id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the details of a role s base resources.", "response": "def role_show(self, role_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/roles#get-role\"\n        api_path = \"/api/v2/roles/{role_id}\"\n        api_path = api_path.format(role_id=role_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the role s metadata.", "response": "def role_update(self, role_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/roles#update-role\"\n        api_path = \"/api/v2/roles/{role_id}\"\n        api_path = api_path.format(role_id=role_id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing the details of a specific satisfaction rating.", "response": "def satisfaction_rating_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/satisfaction_ratings#show-satisfaction-rating\"\n        api_path = \"/api/v2/satisfaction_ratings/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef satisfaction_ratings_list(self, score=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/satisfaction_ratings#list-satisfaction-ratings\"\n        api_path = \"/api/v2/satisfaction_ratings.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if score:\n            api_query.update({\n                \"score\": score,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "List the satisfaction ratings for a given user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the details of a specific satisfaction reason for a specific resource.", "response": "def satisfaction_reason_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/satisfaction_reasons#show-reason-for-satisfaction-rating\"\n        api_path = \"/api/v2/satisfaction_reasons/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(self, query=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/search#list-search-results\"\n        api_path = \"/api/v2/search.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if query:\n            api_query.update({\n                \"query\": query,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Search for the available resource types."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new Shared Agreement", "response": "def sharing_agreement_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/sharing_agreements#create-sharing-agreement\"\n        api_path = \"/api/v2/sharing_agreements.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a Shared Agreement.", "response": "def sharing_agreement_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/sharing_agreements#delete-a-sharing-agreement\"\n        api_path = \"/api/v2/sharing_agreements/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sharing_agreement_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/sharing_agreements#show-a-sharing-agreement\"\n        api_path = \"/api/v2/sharing_agreements/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show a specific sharing agreement."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new key - value entry in the registry.", "response": "def shortcut_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/shortcuts#create-shortcut\"\n        api_path = \"/api/v2/shortcuts\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shortcut_delete(self, shortcut_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/shortcuts#delete-shortcut\"\n        api_path = \"/api/v2/shortcuts/{shortcut_id}\"\n        api_path = api_path.format(shortcut_id=shortcut_id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a specific resource from the Shortcuts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the details of a specific key - value entry in the system.", "response": "def shortcut_show(self, shortcut_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/shortcuts#get-shortcut\"\n        api_path = \"/api/v2/shortcuts/{shortcut_id}\"\n        api_path = api_path.format(shortcut_id=shortcut_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shortcut_update(self, shortcut_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/shortcuts#update-shortcut\"\n        api_path = \"/api/v2/shortcuts/{shortcut_id}\"\n        api_path = api_path.format(shortcut_id=shortcut_id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the data for a given key - value entry in the given key - value store."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a skill for a given resource.", "response": "def skill_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/skills#create-skill\"\n        api_path = \"/api/v2/skills\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a skill by its ID.", "response": "def skill_delete(self, skill_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/skills#delete-skill-by-id\"\n        api_path = \"/api/v2/skills/{skill_id}\"\n        api_path = api_path.format(skill_id=skill_id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef skill_show(self, skill_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/skills#get-skill-by-id\"\n        api_path = \"/api/v2/skills/{skill_id}\"\n        api_path = api_path.format(skill_id=skill_id)\n        return self.call(api_path, **kwargs)", "response": "Get a specific skill by its ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the skill with the given data.", "response": "def skill_update(self, skill_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/skills#update-skill-by-id\"\n        api_path = \"/api/v2/skills/{skill_id}\"\n        api_path = api_path.format(skill_id=skill_id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef skills_name_delete(self, name, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/skills#delete-skill-by-name\"\n        api_path = \"/api/v2/skills/name/{name}\"\n        api_path = api_path.format(name=name)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a skill by its name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef skills_name_show(self, name, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/skills#get-skill-by-name\"\n        api_path = \"/api/v2/skills/name/{name}\"\n        api_path = api_path.format(name=name)\n        return self.call(api_path, **kwargs)", "response": "Get a specific skill by its name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef skills_name_update(self, name, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/skills#update-skill-by-name\"\n        api_path = \"/api/v2/skills/name/{name}\"\n        api_path = api_path.format(name=name)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the skill by name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef skip_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_skips#record-a-new-skip-for-the-current-user\"\n        api_path = \"/api/v2/skips.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new skip for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreorders the SLA policies for a given set of attributes.", "response": "def slas_policies_reorder(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/sla_policies#reorder-sla-policies\"\n        api_path = \"/api/v2/slas/policies/reorder.json\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new SLA policy.", "response": "def slas_policy_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/sla_policies#create-sla-policy\"\n        api_path = \"/api/v2/slas/policies\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting a SLA policy.", "response": "def slas_policy_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/sla_policies#delete-sla-policy\"\n        api_path = \"/api/v2/slas/policies/{id}\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow the details of a specific SLA policy.", "response": "def slas_policy_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/sla_policies#get-sla-policy\"\n        api_path = \"/api/v2/slas/policies/{id}\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the status of a single agent in a single stream.", "response": "def stream_agent_show(self, metric_key, department_id=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/apis#get-single-agent-status\"\n        api_path = \"/stream/agents/{metric_key}\"\n        api_path = api_path.format(metric_key=metric_key)\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if department_id:\n            api_query.update({\n                \"department_id\": department_id,\n            })\n        return self.call(api_path, query=api_query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stream_agents_list(self, department_id=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/apis#get-all-agents-status\"\n        api_path = \"/stream/agents\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if department_id:\n            api_query.update({\n                \"department_id\": department_id,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Get a list of all available agents."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stream_chat_show(self, metric_key, department_id=None, window=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/apis#get-single-chat-metric\"\n        api_path = \"/stream/chats/{metric_key}\"\n        api_path = api_path.format(metric_key=metric_key)\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if department_id:\n            api_query.update({\n                \"department_id\": department_id,\n            })\n        if window:\n            api_query.update({\n                \"window\": window,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Get the information for a single chat metric."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stream_chats_list(self, department_id=None, window=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/apis#get-all-chat-metrics\"\n        api_path = \"/stream/chats\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if department_id:\n            api_query.update({\n                \"department_id\": department_id,\n            })\n        if window:\n            api_query.update({\n                \"window\": window,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Stream the chats in the chat."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstreams the list of chats that have been missed.", "response": "def stream_chats_missed_chats_list(self, window=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/apis#example-response\"\n        api_path = \"/stream/chats/missed_chats\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if window:\n            api_query.update({\n                \"window\": window,\n            })\n        return self.call(api_path, query=api_query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a suspended ticket.", "response": "def suspended_ticket_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/suspended_tickets#delete-suspended-ticket\"\n        api_path = \"/api/v2/suspended_tickets/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef suspended_ticket_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/suspended_tickets#show-suspended-ticket\"\n        api_path = \"/api/v2/suspended_tickets/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a suspended ticket."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new target.", "response": "def target_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/targets#create-target\"\n        api_path = \"/api/v2/targets.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef target_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/targets#delete-target\"\n        api_path = \"/api/v2/targets/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a target from the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef target_failure_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/target_failures#show-target-failure\"\n        api_path = \"/api/v2/target_failures/{id}\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a specific target failure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef target_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/targets#show-target\"\n        api_path = \"/api/v2/targets/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ticket_audit_show(self, ticket_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_audits#show-audit\"\n        api_path = \"/api/v2/tickets/{ticket_id}/audits/{id}.json\"\n        api_path = api_path.format(ticket_id=ticket_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a specific audit entry."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ticket_audits(self, ticket_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_audits#list-audits-for-a-ticket\"\n        api_path = \"/api/v2/tickets/{ticket_id}/audits.json\"\n        api_path = api_path.format(ticket_id=ticket_id)\n        return self.call(api_path, **kwargs)", "response": "List the available audits for a ticket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ticket_audits_list(self, cursor=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_audits#list-all-ticket-audits\"\n        api_path = \"/api/v2/ticket_audits.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if cursor:\n            api_query.update({\n                \"cursor\": cursor,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "List all Ticket Audits."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ticket_collaborators(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#list-collaborators-for-a-ticket\"\n        api_path = \"/api/v2/tickets/{id}/collaborators.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "List the Collaborators for a Ticket."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ticket_comment_make_private(self, ticket_id, id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_comments#make-comment-private\"\n        api_path = \"/api/v2/tickets/{ticket_id}/comments/{id}/make_private.json\"\n        api_path = api_path.format(ticket_id=ticket_id, id=id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Make a comment private in the given ticket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of comments for a given ticket.", "response": "def ticket_comments(self, ticket_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_comments#list-comments\"\n        api_path = \"/api/v2/tickets/{ticket_id}/comments.json\"\n        api_path = api_path.format(ticket_id=ticket_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new ticket.", "response": "def ticket_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#create-ticket\"\n        api_path = \"/api/v2/tickets.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ticket_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#delete-ticket\"\n        api_path = \"/api/v2/tickets/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a ticket from the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ticket_field_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_fields#create-ticket-field\"\n        api_path = \"/api/v2/ticket_fields.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a new ticket field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting a Ticket Field.", "response": "def ticket_field_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_fields#delete-ticket-field\"\n        api_path = \"/api/v2/ticket_fields/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new option in a ticket field.", "response": "def ticket_field_option_create(self, field_id, id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_fields#create-or-update-a-ticket-field-option\"\n        api_path = \"/api/v2/ticket_fields/{field_id}/options/{id}.json\"\n        api_path = api_path.format(field_id=field_id, id=id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow a specific option in a ticket field.", "response": "def ticket_field_option_show(self, field_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_fields#show-a-ticket-field-option\"\n        api_path = \"/api/v2/ticket_fields/{field_id}/options/{id}.json\"\n        api_path = api_path.format(field_id=field_id, id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ticket_field_options(self, field_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_fields#list-ticket-field-options\"\n        api_path = \"/api/v2/ticket_fields/{field_id}/options.json\"\n        api_path = api_path.format(field_id=field_id)\n        return self.call(api_path, **kwargs)", "response": "Get the options for a specific ticket field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ticket_field_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_fields#show-ticket-field\"\n        api_path = \"/api/v2/ticket_fields/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a specific ticket field."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new ticket form.", "response": "def ticket_form_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_forms#create-ticket-forms\"\n        api_path = \"/api/v2/ticket_forms.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ticket_form_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_forms#delete-ticket-form\"\n        api_path = \"/api/v2/ticket_forms/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a Ticket Form"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the details of a specific ticket form.", "response": "def ticket_form_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_forms#show-ticket-form\"\n        api_path = \"/api/v2/ticket_forms/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreorders Ticket Forms for a given set of ticket_forms.", "response": "def ticket_forms_reorder(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_forms#reorder-ticket-forms\"\n        api_path = \"/api/v2/ticket_forms/reorder.json\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the details of the Ticket Incidents for the given ID.", "response": "def ticket_incidents(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#listing-ticket-incidents\"\n        api_path = \"/api/v2/tickets/{id}/incidents.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ticket_macro_apply(self, ticket_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/macros#show-ticket-after-changes\"\n        api_path = \"/api/v2/tickets/{ticket_id}/macros/{id}/apply.json\"\n        api_path = api_path.format(ticket_id=ticket_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Apply a macro to a ticket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the details of a specific ticket metric.", "response": "def ticket_metric_show(self, ticket_metric_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_metrics#show-ticket-metrics\"\n        api_path = \"/api/v2/ticket_metrics/{ticket_metric_id}.json\"\n        api_path = api_path.format(ticket_metric_id=ticket_metric_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the metrics for a given ticket.", "response": "def ticket_metrics(self, ticket_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_metrics#show-ticket-metrics\"\n        api_path = \"/api/v2/tickets/{ticket_id}/metrics.json\"\n        api_path = api_path.format(ticket_id=ticket_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ticket_related(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#ticket-related-information\"\n        api_path = \"/api/v2/tickets/{id}/related.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the related ticket information for a given ticket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Satisfaction Rating for a Ticket.", "response": "def ticket_satisfaction_rating_create(self, ticket_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/satisfaction_ratings#create-a-satisfaction-rating\"\n        api_path = \"/api/v2/tickets/{ticket_id}/satisfaction_rating.json\"\n        api_path = api_path.format(ticket_id=ticket_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the details of a specific ticket.", "response": "def ticket_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#show-ticket\"\n        api_path = \"/api/v2/tickets/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the ticket skip list for the current account.", "response": "def ticket_skips(self, ticket_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_skips#list-skips-for-the-current-account\"\n        api_path = \"/api/v2/tickets/{ticket_id}/skips.json\"\n        api_path = api_path.format(ticket_id=ticket_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ticket_tags(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tags#show-tags\"\n        api_path = \"/api/v2/tickets/{id}/tags.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the tags for a given ticket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting the tags for a given ticket.", "response": "def ticket_tags_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tags#remove-tags\"\n        api_path = \"/api/v2/tickets/{id}/tags.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tickets_list(self, external_id=None, include=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#json-format\"\n        api_path = \"/api/v2/tickets.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if external_id:\n            api_query.update({\n                \"external_id\": external_id,\n            })\n        if include:\n            api_query.update({\n                \"include\": include,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "List the tickets for the specified external_id and include."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new trigger for a given resource.", "response": "def trigger_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/triggers#create-trigger\"\n        api_path = \"/api/v2/triggers\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a specific trigger by ID.", "response": "def trigger_delete_by_id(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/triggers#delete-trigger\"\n        api_path = \"/api/v2/triggers/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a resource from the cluster store by its name.", "response": "def trigger_delete_by_trigger_name(self, trigger_name, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/triggers#delete-trigger\"\n        api_path = \"/api/v2/triggers/{trigger_name}\"\n        api_path = api_path.format(trigger_name=trigger_name)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the details of a specific revision for a specific trigger.", "response": "def trigger_revision_show(self, trigger_id, revision_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/triggers#getting-revisions\"\n        api_path = \"/api/v2/triggers/{trigger_id}/revisions/{revision_id}.json\"\n        api_path = api_path.format(trigger_id=trigger_id, revision_id=revision_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the list of revision records for a given trigger.", "response": "def trigger_revisions(self, trigger_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/triggers#list-trigger-revisions\"\n        api_path = \"/api/v2/triggers/{trigger_id}/revisions.json\"\n        api_path = api_path.format(trigger_id=trigger_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trigger_show_by_id(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/triggers#getting-triggers\"\n        api_path = \"/api/v2/triggers/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get a single trigger by its ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trigger_show_by_trigger_name(self, trigger_name, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/triggers#get-a-trigger\"\n        api_path = \"/api/v2/triggers/{trigger_name}\"\n        api_path = api_path.format(trigger_name=trigger_name)\n        return self.call(api_path, **kwargs)", "response": "Get the information of a specific trigger by its name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef trigger_update_by_id(self, id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/triggers#update-trigger\"\n        api_path = \"/api/v2/triggers/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the data for a specific trigger by ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the data for a specific trigger by its name.", "response": "def trigger_update_by_trigger_name(self, trigger_name, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/triggers#update-trigger\"\n        api_path = \"/api/v2/triggers/{trigger_name}\"\n        api_path = api_path.format(trigger_name=trigger_name)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreorders the triggers for a given set of resources.", "response": "def triggers_reorder(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/triggers#reorder-triggers\"\n        api_path = \"/api/v2/triggers/reorder.json\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef triggers_update_many(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/triggers#update-many-triggers\"\n        api_path = \"/api/v2/triggers/update_many.json\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the triggers for a given set of resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads a new file to the specified file.", "response": "def upload_create(self, data, filename=None, token=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/attachments#upload-files\"\n        api_path = \"/api/v2/uploads.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if filename:\n            api_query.update({\n                \"filename\": filename,\n            })\n        if token:\n            api_query.update({\n                \"token\": token,\n            })\n        return self.call(api_path, query=api_query, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting the current image from the store.", "response": "def upload_delete(self, token, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/attachments#delete-upload\"\n        api_path = \"/api/v2/uploads/{token}.json\"\n        api_path = api_path.format(token=token)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the compliance deletion statuses for a user.", "response": "def user_compliance_deletion_statuses(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#show-compliance-deletion-statuses\"\n        api_path = \"/api/v2/users/{id}/compliance_deletion_statuses.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#create-user\"\n        api_path = \"/api/v2/users.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create a user in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a user from the database.", "response": "def user_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#delete-user\"\n        api_path = \"/api/v2/users/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new user field.", "response": "def user_field_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_fields#create-user-fields\"\n        api_path = \"/api/v2/user_fields.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_field_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_fields#delete-user-field\"\n        api_path = \"/api/v2/user_fields/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a user field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new option for a user field.", "response": "def user_field_option_create(self, field_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_fields#create-or-update-a-user-field-option\"\n        api_path = \"/api/v2/user_fields/{field_id}/options.json\"\n        api_path = api_path.format(field_id=field_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete an option from a user field.", "response": "def user_field_option_delete(self, field_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_fields#delete-user-field-option\"\n        api_path = \"/api/v2/user_fields/{field_id}/options/{id}.json\"\n        api_path = api_path.format(field_id=field_id, id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_field_option_show(self, field_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_fields#show-a-user-field-option\"\n        api_path = \"/api/v2/user_fields/{field_id}/options/{id}.json\"\n        api_path = api_path.format(field_id=field_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Show a specific user field option."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist the options for a user field.", "response": "def user_field_options(self, field_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_fields#list-user-field-options\"\n        api_path = \"/api/v2/user_fields/{field_id}/options.json\"\n        api_path = api_path.format(field_id=field_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshowing the details of a user field.", "response": "def user_field_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_fields#show-user-field\"\n        api_path = \"/api/v2/user_fields/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_fields_reorder(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_fields#reorder-user-field\"\n        api_path = \"/api/v2/user_fields/reorder.json\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Reorder the user fields of a user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_group_membership_make_default(self, user_id, membership_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/group_memberships#set-membership-as-default\"\n        api_path = \"/api/v2/users/{user_id}/group_memberships/{membership_id}/make_default.json\"\n        api_path = api_path.format(user_id=user_id, membership_id=membership_id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Set the membership as default for a user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the details of a group membership entry.", "response": "def user_group_membership_show(self, user_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/group_memberships#show-membership\"\n        api_path = \"/api/v2/users/{user_id}/group_memberships/{id}.json\"\n        api_path = api_path.format(user_id=user_id, id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef user_group_memberships(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/group_memberships#list-memberships\"\n        api_path = \"/api/v2/users/{user_id}/group_memberships.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)", "response": "List the group memberships for a user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the user s group info.", "response": "def user_groups(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/groups#list-groups\"\n        api_path = \"/api/v2/users/{user_id}/groups.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting the user identities.", "response": "def user_identities(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_identities#list-identities\"\n        api_path = \"/api/v2/users/{user_id}/identities.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_identity_delete(self, user_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_identities#delete-identity\"\n        api_path = \"/api/v2/users/{user_id}/identities/{id}.json\"\n        api_path = api_path.format(user_id=user_id, id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete a user identity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_identity_make_primary(self, user_id, id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_identities#make-identity-primary\"\n        api_path = \"/api/v2/users/{user_id}/identities/{id}/make_primary\"\n        api_path = api_path.format(user_id=user_id, id=id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Make the identity primary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_identity_show(self, user_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/user_identities#show-identity\"\n        api_path = \"/api/v2/users/{user_id}/identities/{id}.json\"\n        api_path = api_path.format(user_id=user_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the details of a user identity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new organization membership for a user.", "response": "def user_organization_membership_create(self, user_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_memberships#create-membership\"\n        api_path = \"/api/v2/users/{user_id}/organization_memberships.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the user s membership as default.", "response": "def user_organization_membership_make_default(self, id, membership_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_memberships#set-membership-as-default\"\n        api_path = \"/api/v2/users/{id}/organization_memberships/{membership_id}/make_default.json\"\n        api_path = api_path.format(id=id, membership_id=membership_id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_organization_membership_show(self, user_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_memberships#show-membership\"\n        api_path = \"/api/v2/users/{user_id}/organization_memberships/{id}.json\"\n        api_path = api_path.format(user_id=user_id, id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the organization membership of a user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist the organization memberships for a user.", "response": "def user_organization_memberships(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_memberships#list-memberships\"\n        api_path = \"/api/v2/users/{user_id}/organization_memberships.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting the organization subscriptions for a user.", "response": "def user_organization_subscriptions(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organization_subscriptions#list-organization-subscriptions\"\n        api_path = \"/api/v2/users/{user_id}/organization_subscriptions.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting the user organizations.", "response": "def user_organizations(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/organizations#list-organizations\"\n        api_path = \"/api/v2/users/{user_id}/organizations.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_password_requirements(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#get-a-list-of-password-requirements\"\n        api_path = \"/api/v2/users/{user_id}/password/requirements.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)", "response": "Get a list of password requirements for a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_related(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#user-related-information\"\n        api_path = \"/api/v2/users/{id}/related.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the related users."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef user_requests(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/requests#list-requests\"\n        api_path = \"/api/v2/users/{id}/requests.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "List the user s requests."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the details of a user s session.", "response": "def user_session_show(self, user_id, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/sessions#show-session\"\n        api_path = \"/api/v2/users/{user_id}/sessions/{id}.json\"\n        api_path = api_path.format(user_id=user_id, id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef user_sessions(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/sessions#list-sessions\"\n        api_path = \"/api/v2/users/{user_id}/sessions.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)", "response": "List the user s active sessions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_sessions_delete(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/sessions#bulk-deleting-sessions\"\n        api_path = \"/api/v2/users/{user_id}/sessions.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete the user s current session."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the user s metadata.", "response": "def user_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#show-user\"\n        api_path = \"/api/v2/users/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_skips(self, user_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/ticket_skips#list-skips-for-the-current-account\"\n        api_path = \"/api/v2/users/{user_id}/skips.json\"\n        api_path = api_path.format(user_id=user_id)\n        return self.call(api_path, **kwargs)", "response": "Get the ticket skip list for the user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef user_tags(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tags#show-tags\"\n        api_path = \"/api/v2/users/{id}/tags.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Get the tags for a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef user_tags_delete(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tags#remove-tags\"\n        api_path = \"/api/v2/users/{id}/tags.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, method=\"DELETE\", **kwargs)", "response": "Delete the user s tags."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_tickets_assigned(self, user_id, external_id=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/tickets#allowed-for\"\n        api_path = \"/api/v2/users/{user_id}/tickets/assigned.json\"\n        api_path = api_path.format(user_id=user_id)\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if external_id:\n            api_query.update({\n                \"external_id\": external_id,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Get a list of all the available user tickets for the specified user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef users_create_or_update(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#create-or-update-user\"\n        api_path = \"/api/v2/users/create_or_update.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Create or update a user s cluster cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating or update many users.", "response": "def users_create_or_update_many(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#create-or-update-many-users\"\n        api_path = \"/api/v2/users/create_or_update_many.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist the users in the hierarchy.", "response": "def users_list(self, permission_set=None, role=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#list-users\"\n        api_path = \"/api/v2/users.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if permission_set:\n            api_query.update({\n                \"permission_set\": permission_set,\n            })\n        if role:\n            api_query.update({\n                \"role\": role,\n            })\n        return self.call(api_path, query=api_query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef users_me(self, include=None, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#show-the-currently-authenticated-user\"\n        api_path = \"/api/v2/users/me.json\"\n        api_query = {}\n        if \"query\" in kwargs.keys():\n            api_query.update(kwargs[\"query\"])\n            del kwargs[\"query\"]\n        if include:\n            api_query.update({\n                \"include\": include,\n            })\n        return self.call(api_path, query=api_query, **kwargs)", "response": "Return the list of users that have the currently authenticated user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef users_me_merge(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#merge-self-with-another-user\"\n        api_path = \"/api/v2/users/me/merge.json\"\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Merge the user s identity and other user s identity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting a new user from a set of data.", "response": "def users_request_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/users#request-user-create\"\n        api_path = \"/api/v2/users/request_create.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the number of views in a given resource.", "response": "def view_count(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/views#get-view-count\"\n        api_path = \"/api/v2/views/{id}/count.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new view in the cluster.", "response": "def view_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/views#create-view\"\n        api_path = \"/api/v2/views.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexporting the view with the given ID.", "response": "def view_export(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/views#export-view\"\n        api_path = \"/api/v2/views/{id}/export.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef view_show(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/views#show-view\"\n        api_path = \"/api/v2/views/{id}.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)", "response": "Show the details of a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists the tickets from a view.", "response": "def view_tickets(self, id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/views#list-tickets-from-a-view\"\n        api_path = \"/api/v2/views/{id}/tickets.json\"\n        api_path = api_path.format(id=id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npreviews the current view for a given resource.", "response": "def views_preview(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/views#previewing-views\"\n        api_path = \"/api/v2/views/preview.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef views_preview_count(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/core/views#preview-count\"\n        api_path = \"/api/v2/views/preview/count.json\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)", "response": "Count the number of items in the current view."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a visitor for the given resource.", "response": "def visitor_create(self, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/visitors#create-visitor\"\n        api_path = \"/api/v2/visitors\"\n        return self.call(api_path, method=\"POST\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the information for a given visitor.", "response": "def visitor_show(self, visitor_id, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/visitors#get-a-visitor\"\n        api_path = \"/api/v2/visitors/{visitor_id}\"\n        api_path = api_path.format(visitor_id=visitor_id)\n        return self.call(api_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visitor_update(self, visitor_id, data, **kwargs):\n        \"https://developer.zendesk.com/rest_api/docs/chat/visitors#update-visitor\"\n        api_path = \"/api/v2/visitors/{visitor_id}\"\n        api_path = api_path.format(visitor_id=visitor_id)\n        return self.call(api_path, method=\"PUT\", data=data, **kwargs)", "response": "Update the metadata for a given visitor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef include_notify_js_variables(context):\n    ctx = {\n        'user': context['request'].user,\n        'update_notification': reverse('notifications:update'),\n        'mark_notification': reverse('notifications:mark'),\n        'mark_all_notification': reverse('notifications:mark_all'),\n        'delete_notification': reverse('notifications:delete'),\n\n        'nf_list_class_selector': notify_settings.NF_LIST_CLASS_SELECTOR,\n        'nf_class_selector': notify_settings.SINGLE_NF_CLASS_SELECTOR,\n        'nf_box_list_class_selector': notify_settings.NF_BOX_CLASS_SELECTOR,\n        'nf_box_class_selector': notify_settings.SINGLE_NF_BOX_CLASS_SELECTOR,\n\n        'nf_mark_selector': notify_settings.MARK_NF_CLASS_SELECTOR,\n        'nf_mark_all_selector': notify_settings.MARK_ALL_NF_CLASS_SELECTOR,\n        'nf_delete_selector': notify_settings.DELETE_NF_CLASS_SELECTOR,\n\n        'nf_read_class': notify_settings.READ_NF_CLASS,\n        'nf_unread_class': notify_settings.UNREAD_NF_CLASS,\n\n        'nf_update_time_interval': notify_settings.UPDATE_TIME_INTERVAL,\n    }\n    return ctx", "response": "Include all JS variables required by the notify. js file on the HTML page around <script > tags."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate rendered HTML content using supplied notifications.", "response": "def generate_html(self, notifications):\n        \"\"\"\n        Generates rendered HTML content using supplied notifications.\n        :param notifications: Notification QuerySet Object\n        :return: Rendered HTML.\n        \"\"\"\n        html_chunks = []\n        for nf in notifications:\n            extra = nf.as_json() if self.target == 'box' else {}\n            html = render_notification(nf, render_target=self.target, **extra)\n            html_chunks.append(html)\n        if not html_chunks:\n            html_chunks.append(_(\"<b>No notifications yet.</b>\"))\n        html_string = '\\n'.join(html_chunks)\n        return html_string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender the template tag", "response": "def render(self, context):\n        \"\"\"\n        Render method of the template tag, returns generated html content to\n        the parent template where it was called.\n        :param context: Template context.\n        :return: Generated HTML content using notification queryset object.\n        \"\"\"\n        notifications = self.obj.resolve(context)\n        return self.generate_html(notifications)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmarks all notifications as unread for a user.", "response": "def unread_all(self, user=None):\n        \"\"\"\n        Marks all notifications as unread for a user (if supplied)\n\n        :param user: Notification recipient.\n\n        :return: Updates QuerySet as unread.\n        \"\"\"\n        qs = self.read()\n        if user:\n            qs = qs.filter(recipient=user)\n        qs.update(read=False)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmarking all notifications as read for a user.", "response": "def read_all(self, user=None):\n        \"\"\"\n        Marks all notifications as read for a user (if supplied)\n\n        :param user: Notification recipient.\n\n        :return: Updates QuerySet as read.\n        \"\"\"\n        qs = self.unread()\n        if user:\n            qs = qs.filter(recipient=user)\n        qs.update(read=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef active_all(self, user=None):\n        qs = self.deleted()\n        if user:\n            qs = qs.filter(recipient=user)\n        qs.update(deleted=False)", "response": "Method to soft - delete all notifications of a User"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prefetch_relations(weak_queryset):\n    from django.contrib.contenttypes.models import ContentType\n    from django.contrib.contenttypes.fields import GenericForeignKey\n\n    # reverse model's generic foreign keys into a dict:\n    # { 'field_name': generic.GenericForeignKey instance, ... }\n    gfks = {}\n    for name, gfk in weak_queryset.model.__dict__.items():\n        if not isinstance(gfk, GenericForeignKey):\n            continue\n        gfks[name] = gfk\n\n    data = {}\n    for weak_model in weak_queryset:\n        for gfk_name, gfk_field in gfks.items():\n            related_content_type_id = getattr(\n                weak_model,\n                gfk_field.model._meta.get_field(gfk_field.ct_field).get_attname())\n            if not related_content_type_id:\n                continue\n            related_content_type = ContentType.objects.get_for_id(related_content_type_id)\n            related_object_id = int(getattr(weak_model, gfk_field.fk_field))\n\n            if related_content_type not in data.keys():\n                data[related_content_type] = []\n            data[related_content_type].append(related_object_id)\n\n    for content_type, object_ids in data.items():\n        model_class = content_type.model_class()\n        models = prefetch_relations(model_class.objects.filter(pk__in=object_ids).select_related())\n        for model in models:\n            for weak_model in weak_queryset:\n                for gfk_name, gfk_field in gfks.items():\n                    related_content_type_id = getattr(\n                        weak_model,\n                        gfk_field.model._meta.get_field(gfk_field.ct_field).get_attname())\n                    if not related_content_type_id:\n                        continue\n                    related_content_type = ContentType.objects.get_for_id(related_content_type_id)\n                    related_object_id = int(getattr(weak_model, gfk_field.fk_field))\n\n                    if related_object_id != model.pk:\n                        continue\n                    if related_content_type != content_type:\n                        continue\n\n                    setattr(weak_model, gfk_name, model)\n\n    return weak_queryset", "response": "This is a simple example that uses the prefetch_relations method to create a new object for each action."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef notifications(request):\n    notification_list = request.user.notifications.active().prefetch()\n    return render(request, 'notifications/all.html',\n                  {'notifications': notification_list})", "response": "Returns a rendered page of all notifications for the logged - in user."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles marking of individual notifications as read or unread.", "response": "def mark(request):\n    \"\"\"\n    Handles marking of individual notifications as read or unread.\n    Takes ``notification id`` and mark ``action`` as POST data.\n\n    :param request: HTTP request context.\n\n    :returns: Response to mark action of supplied notification ID.\n    \"\"\"\n    notification_id = request.POST.get('id', None)\n    action = request.POST.get('action', None)\n    success = True\n\n    if notification_id:\n        try:\n            notification = Notification.objects.get(pk=notification_id,\n                                                    recipient=request.user)\n            if action == 'read':\n                notification.mark_as_read()\n                msg = _(\"Marked as read\")\n            elif action == 'unread':\n                notification.mark_as_unread()\n                msg = _(\"Marked as unread\")\n            else:\n                success = False\n                msg = _(\"Invalid mark action.\")\n        except Notification.DoesNotExist:\n            success = False\n            msg = _(\"Notification does not exists.\")\n    else:\n        success = False\n        msg = _(\"Invalid Notification ID\")\n\n    ctx = {'msg': msg, 'success': success, 'action': action}\n\n    return notification_redirect(request, ctx)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmark all notifications as read or unread depending of POST data.", "response": "def mark_all(request):\n    \"\"\"\n    Marks notifications as either read or unread depending of POST parameters.\n    Takes ``action`` as POST data, it can either be ``read`` or ``unread``.\n\n    :param request: HTTP Request context.\n\n    :return: Response to mark_all action.\n    \"\"\"\n    action = request.POST.get('action', None)\n    success = True\n\n    if action == 'read':\n        request.user.notifications.read_all()\n        msg = _(\"Marked all notifications as read\")\n    elif action == 'unread':\n        request.user.notifications.unread_all()\n        msg = _(\"Marked all notifications as unread\")\n    else:\n        msg = _(\"Invalid mark action\")\n        success = False\n\n    ctx = {'msg': msg, 'success': success, 'action': action}\n\n    return notification_redirect(request, ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a single notification.", "response": "def delete(request):\n    \"\"\"\n    Deletes notification of supplied notification ID.\n\n    Depending on project settings, if ``NOTIFICATIONS_SOFT_DELETE``\n    is set to ``False``, the notifications will be deleted from DB.\n    If not, a soft delete will be performed.\n\n    By default, notifications are deleted softly.\n\n    :param request: HTTP request context.\n\n    :return: Response to delete action on supplied notification ID.\n    \"\"\"\n    notification_id = request.POST.get('id', None)\n    success = True\n\n    if notification_id:\n        try:\n            notification = Notification.objects.get(pk=notification_id,\n                                                    recipient=request.user)\n            soft_delete = getattr(settings, 'NOTIFY_SOFT_DELETE', True)\n            if soft_delete:\n                notification.deleted = True\n                notification.save()\n            else:\n                notification.delete()\n            msg = _(\"Deleted notification successfully\")\n        except Notification.DoesNotExist:\n            success = False\n            msg = _(\"Notification does not exists.\")\n    else:\n        success = False\n        msg = _(\"Invalid Notification ID\")\n\n    ctx = {'msg': msg, 'success': success, }\n\n    return notification_redirect(request, ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle live updating of notifications, follows ajax-polling approach. Read more: http://stackoverflow.com/a/12855533/4726598 Required URL parameters: ``flag``. Explanation: - The ``flag`` parameter carries the last notification ID \\ received by the user's browser. - This ``flag`` is most likely to be generated by using \\ a simple JS/JQuery DOM. Just grab the first element of \\ the notification list. - The element will have a ``data-id`` attribute set to the \\ corresponding notification. - We'll use it's value as the flag parameter. - The view treats the ``last notification flag`` as a model \\ ```filter()`` and fetches all notifications greater than \\ the flag for the user. - Then the a JSON data is prepared with all necessary \\ details such as, ``verb``, ``actor``, ``target`` and their \\ URL etc. The foreignkey are serialized as their \\ default ``__str__`` value. - Everything will be HTML escaped by django's ``escape()``. - Since these notification sent will only serve temporarily \\ on the notification box and will be generated fresh \\ using a whole template, to avoid client-side notification \\ generation using the JSON data, the JSON data will also \\ contain a rendered HTML string so that you can easily \\ do a JQuery ``$yourNotificationBox.prepend()`` on the \\ rendered html string of the notification. - The template used is expected to be different than the \\ template used in full page notification as the css \\ and some other elements are highly likely to be \\ different than the full page notification list. \\ - The template used will be the ``notification type`` of the \\ notification suffixed ``_box.html``. So, if your \\ notification type is ``comment_reply``, the template \\ will be ``comment_reply_box.html``. - This template will be stored in ``notifications/includes/`` \\ of your template directory. - That makes: ``notifications/includes/comment_reply_box.html`` - The rest is self-explanatory. :param request: HTTP request context. :return: Notification updates (if any) in JSON format.", "response": "def notification_update(request):\n    \"\"\"\n    Handles live updating of notifications, follows ajax-polling approach.\n\n    Read more: http://stackoverflow.com/a/12855533/4726598\n\n    Required URL parameters: ``flag``.\n\n    Explanation:\n\n        - The ``flag`` parameter carries the last notification ID \\\n        received by the user's browser.\n\n        - This ``flag`` is most likely to be generated by using \\\n        a simple JS/JQuery DOM. Just grab the first element of \\\n        the notification list.\n\n            - The element will have a ``data-id`` attribute set to the \\\n            corresponding notification.\n            - We'll use it's value as the flag parameter.\n\n        - The view treats the ``last notification flag`` as a model \\\n        ```filter()`` and fetches all notifications greater than \\\n        the flag for the user.\n\n        - Then the a JSON data is prepared with all necessary \\\n        details such as, ``verb``, ``actor``, ``target`` and their \\\n        URL etc. The foreignkey are serialized as their \\\n        default ``__str__`` value.\n\n            - Everything will be HTML escaped by django's ``escape()``.\n\n        - Since these notification sent will only serve temporarily \\\n        on the notification box and will be generated fresh \\\n        using a whole template, to avoid client-side notification \\\n        generation using the JSON data, the JSON data will also \\\n        contain a rendered HTML string so that you can easily \\\n        do a JQuery ``$yourNotificationBox.prepend()`` on the \\\n        rendered html string of the notification.\n\n        - The template used is expected to be different than the \\\n        template used in full page notification as the css \\\n        and some other elements are highly likely to be \\\n        different than the full page notification list. \\\n\n        - The template used will be the ``notification type`` of the \\\n        notification suffixed ``_box.html``. So, if your \\\n        notification type is ``comment_reply``, the template \\\n        will be ``comment_reply_box.html``.\n\n            - This template will be stored in ``notifications/includes/`` \\\n            of  your template directory.\n\n            - That makes: ``notifications/includes/comment_reply_box.html``\n\n        - The rest is self-explanatory.\n\n    :param request: HTTP request context.\n\n    :return: Notification updates (if any) in JSON format.\n    \"\"\"\n    flag = request.GET.get('flag', None)\n    target = request.GET.get('target', 'box')\n    last_notification = int(flag) if flag.isdigit() else None\n\n    if last_notification:\n\n        new_notifications = request.user.notifications.filter(\n            id__gt=last_notification).active().prefetch()\n\n        msg = _(\"Notifications successfully retrieved.\") \\\n            if new_notifications else _(\"No new notifications.\")\n        notification_list = []\n        for nf in new_notifications:\n            notification = nf.as_json()\n            notification_list.append(notification)\n            notification['html'] = render_notification(\n                nf, render_target=target, **notification)\n\n        ctx = {\n            \"retrieved\": len(new_notifications),\n            \"unread_count\": request.user.notifications.unread().count(),\n            \"notifications\": notification_list,\n            \"success\": True,\n            \"msg\": msg,\n        }\n\n        return JsonResponse(ctx)\n\n    else:\n        msg = _(\"Notification flag not sent.\")\n\n    ctx = {\"success\": False, \"msg\": msg}\n    return JsonResponse(ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmarks the supplied notification as read and then redirects to the supplied URL.", "response": "def read_and_redirect(request, notification_id):\n    \"\"\"\n    Marks the supplied notification as read and then redirects\n    to the supplied URL from the ``next`` URL parameter.\n\n    **IMPORTANT**: This is CSRF - unsafe method.\n    Only use it if its okay for you to mark notifications \\\n    as read without a robust check.\n\n    :param request: HTTP request context.\n    :param notification_id: ID of the notification to be marked a read.\n\n    :returns: Redirect response to a valid target url.\n    \"\"\"\n    notification_page = reverse('notifications:all')\n    next_page = request.GET.get('next', notification_page)\n\n    if is_safe_url(next_page):\n        target = next_page\n    else:\n        target = notification_page\n    try:\n        user_nf = request.user.notifications.get(pk=notification_id)\n        if not user_nf.read:\n            user_nf.mark_as_read()\n    except Notification.DoesNotExist:\n        pass\n\n    return HttpResponseRedirect(target)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch current motion state from camera", "response": "def get_motion_detection(self):\n        \"\"\"Fetch current motion state from camera\"\"\"\n        url = ('%s/ISAPI/System/Video/inputs/'\n               'channels/1/motionDetection') % self.root_url\n\n        try:\n            response = self.hik_request.get(url, timeout=CONNECT_TIMEOUT)\n        except (requests.exceptions.RequestException,\n                requests.exceptions.ConnectionError) as err:\n            _LOGGING.error('Unable to fetch MotionDetection, error: %s', err)\n            self.motion_detection = None\n            return self.motion_detection\n\n        if response.status_code == requests.codes.unauthorized:\n            _LOGGING.error('Authentication failed')\n            self.motion_detection = None\n            return self.motion_detection\n\n        if response.status_code != requests.codes.ok:\n            # If we didn't receive 200, abort\n            _LOGGING.debug('Unable to fetch motion detection.')\n            self.motion_detection = None\n            return self.motion_detection\n\n        try:\n            tree = ET.fromstring(response.text)\n            ET.register_namespace(\"\", self.namespace)\n            enabled = tree.find(self.element_query('enabled'))\n\n            if enabled is not None:\n                self._motion_detection_xml = tree\n            self.motion_detection = {'true': True, 'false': False}[enabled.text]\n            return self.motion_detection\n\n        except AttributeError as err:\n            _LOGGING.error('Entire response: %s', response.text)\n            _LOGGING.error('There was a problem: %s', err)\n            self.motion_detection = None\n            return self.motion_detection"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_motion_detection(self, enable):\n        url = ('%s/ISAPI/System/Video/inputs/'\n               'channels/1/motionDetection') % self.root_url\n\n        enabled = self._motion_detection_xml.find(self.element_query('enabled'))\n        if enabled is None:\n            _LOGGING.error(\"Couldn't find 'enabled' in the xml\")\n            _LOGGING.error('XML: %s', ET.tostring(self._motion_detection_xml))\n            return\n\n        enabled.text = 'true' if enable else 'false'\n        xml = ET.tostring(self._motion_detection_xml)\n\n        try:\n            response = self.hik_request.put(url, data=xml, timeout=CONNECT_TIMEOUT)\n        except (requests.exceptions.RequestException,\n                requests.exceptions.ConnectionError) as err:\n            _LOGGING.error('Unable to set MotionDetection, error: %s', err)\n            return\n\n        if response.status_code == requests.codes.unauthorized:\n            _LOGGING.error('Authentication failed')\n            return\n\n        if response.status_code != requests.codes.ok:\n            # If we didn't receive 200, abort\n            _LOGGING.error('Unable to set motion detection: %s', response.text)\n\n        self.motion_detection = enable", "response": "Set desired motion detection state on camera"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters as callback for when a matching device sensor changes.", "response": "def add_update_callback(self, callback, sensor):\n        \"\"\"Register as callback for when a matching device sensor changes.\"\"\"\n        self._updateCallbacks.append([callback, sensor])\n        _LOGGING.debug('Added update callback to %s on %s', callback, sensor)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _do_update_callback(self, msg):\n        for callback, sensor in self._updateCallbacks:\n            if sensor == msg:\n                _LOGGING.debug('Update callback %s for sensor %s',\n                               callback, sensor)\n                callback(msg)", "response": "Call registered update callbacks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the object with the current values of the deviceInfo and available events.", "response": "def initialize(self):\n        \"\"\"Initialize deviceInfo and available events.\"\"\"\n        device_info = self.get_device_info()\n\n        if device_info is None:\n            self.name = None\n            self.cam_id = None\n            self.event_states = None\n            return\n\n        for key in device_info:\n            if key == 'deviceName':\n                self.name = device_info[key]\n            elif key == 'deviceID':\n                if len(device_info[key]) > 10:\n                    self.cam_id = device_info[key]\n                else:\n                    self.cam_id = uuid.uuid4()\n\n        events_available = self.get_event_triggers()\n        if events_available:\n            for event, channel_list in events_available.items():\n                for channel in channel_list:\n                    try:\n                        self.event_states.setdefault(\n                            SENSOR_MAP[event.lower()], []).append(\n                                [False, channel, 0, datetime.datetime.now()])\n                    except KeyError:\n                        # Sensor type doesn't have a known friendly name\n                        # We can't reliably handle it at this time...\n                        _LOGGING.warning(\n                            'Sensor type \"%s\" is unsupported.', event)\n\n            _LOGGING.debug('Initialized Dictionary: %s', self.event_states)\n        else:\n            _LOGGING.debug('No Events available in dictionary.')\n\n        self.get_motion_detection()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the deviceInfo into a dictionary.", "response": "def get_device_info(self):\n        \"\"\"Parse deviceInfo into dictionary.\"\"\"\n        device_info = {}\n        url = '%s/ISAPI/System/deviceInfo' % self.root_url\n        using_digest = False\n\n        try:\n            response = self.hik_request.get(url, timeout=CONNECT_TIMEOUT)\n            if response.status_code == requests.codes.unauthorized:\n                _LOGGING.debug('Basic authentication failed. Using digest.')\n                self.hik_request.auth = HTTPDigestAuth(self.usr, self.pwd)\n                using_digest = True\n                response = self.hik_request.get(url)\n\n            if response.status_code == requests.codes.not_found:\n                # Try alternate URL for deviceInfo\n                _LOGGING.debug('Using alternate deviceInfo URL.')\n                url = '%s/System/deviceInfo' % self.root_url\n                response = self.hik_request.get(url)\n                # Seems to be difference between camera and nvr, they can't seem to\n                # agree if they should 404 or 401 first\n                if not using_digest and response.status_code == requests.codes.unauthorized:\n                    _LOGGING.debug('Basic authentication failed. Using digest.')\n                    self.hik_request.auth = HTTPDigestAuth(self.usr, self.pwd)\n                    using_digest = True\n                    response = self.hik_request.get(url)\n\n        except (requests.exceptions.RequestException,\n                requests.exceptions.ConnectionError) as err:\n            _LOGGING.error('Unable to fetch deviceInfo, error: %s', err)\n            return None\n\n        if response.status_code == requests.codes.unauthorized:\n            _LOGGING.error('Authentication failed')\n            return None\n\n        if response.status_code != requests.codes.ok:\n            # If we didn't receive 200, abort\n            _LOGGING.debug('Unable to fetch device info.')\n            return None\n\n        try:\n            tree = ET.fromstring(response.text)\n            # Try to fetch namespace from XML\n            nmsp = tree.tag.split('}')[0].strip('{')\n            self.namespace = nmsp if nmsp.startswith('http') else XML_NAMESPACE\n            _LOGGING.debug('Using Namespace: %s', self.namespace)\n\n            for item in tree:\n                tag = item.tag.split('}')[1]\n                device_info[tag] = item.text\n\n            return device_info\n\n        except AttributeError as err:\n            _LOGGING.error('Entire response: %s', response.text)\n            _LOGGING.error('There was a problem: %s', err)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops the watchdog and reset the thread.", "response": "def watchdog_handler(self):\n        \"\"\"Take care of threads if wachdog expires.\"\"\"\n        _LOGGING.debug('%s Watchdog expired. Resetting connection.', self.name)\n        self.watchdog.stop()\n        self.reset_thrd.set()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disconnect(self):\n        _LOGGING.debug('Disconnecting from stream: %s', self.name)\n        self.kill_thrd.set()\n        self.thrd.join()\n        _LOGGING.debug('Event stream thread for %s is stopped', self.name)\n        self.kill_thrd.clear()", "response": "Disconnect from the event stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_stream(self, tree):\n        try:\n            etype = SENSOR_MAP[tree.find(\n                self.element_query('eventType')).text.lower()]\n            estate = tree.find(\n                self.element_query('eventState')).text\n            echid = tree.find(\n                self.element_query('channelID'))\n            if echid is None:\n                # Some devices use a different key\n                echid = tree.find(\n                    self.element_query('dynChannelID'))\n            echid = int(echid.text)\n            ecount = tree.find(\n                self.element_query('activePostCount')).text\n        except (AttributeError, KeyError, IndexError) as err:\n            _LOGGING.error('Problem finding attribute: %s', err)\n            return\n\n        # Take care of keep-alive\n        if len(etype) > 0 and etype == 'Video Loss':\n            self.watchdog.pet()\n\n        # Track state if it's in the event list.\n        if len(etype) > 0:\n            state = self.fetch_attributes(etype, echid)\n            if state:\n                # Determine if state has changed\n                # If so, publish, otherwise do nothing\n                estate = (estate == 'active')\n                old_state = state[0]\n                attr = [estate, echid, int(ecount),\n                        datetime.datetime.now()]\n                self.update_attributes(etype, echid, attr)\n\n                if estate != old_state:\n                    self.publish_changes(etype, echid)\n                self.watchdog.pet()", "response": "Process incoming event stream packets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the stale status of the current CAN object.", "response": "def update_stale(self):\n        \"\"\"Update stale active statuses\"\"\"\n        # Some events don't post an inactive XML, only active.\n        # If we don't get an active update for 5 seconds we can\n        # assume the event is no longer active and update accordingly.\n        for etype, echannels in self.event_states.items():\n            for eprop in echannels:\n                if eprop[3] is not None:\n                    sec_elap = ((datetime.datetime.now()-eprop[3])\n                                .total_seconds())\n                    # print('Seconds since last update: {}'.format(sec_elap))\n                    if sec_elap > 5 and eprop[0] is True:\n                        _LOGGING.debug('Updating stale event %s on CH(%s)',\n                                       etype, eprop[1])\n                        attr = [False, eprop[1], eprop[2],\n                                datetime.datetime.now()]\n                        self.update_attributes(etype, eprop[1], attr)\n                        self.publish_changes(etype, eprop[1])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef publish_changes(self, etype, echid):\n        _LOGGING.debug('%s Update: %s, %s',\n                       self.name, etype, self.fetch_attributes(etype, echid))\n        signal = 'ValueChanged.{}'.format(self.cam_id)\n        sender = '{}.{}'.format(etype, echid)\n        if dispatcher:\n            dispatcher.send(signal=signal, sender=sender)\n\n        self._do_update_callback('{}.{}.{}'.format(self.cam_id, etype, echid))", "response": "Post updates for specified event type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetch_attributes(self, event, channel):\n        try:\n            for sensor in self.event_states[event]:\n                if sensor[1] == int(channel):\n                    return sensor\n        except KeyError:\n            return None", "response": "Returns the attribute list for a given event and channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the attribute list for the current event and channel.", "response": "def update_attributes(self, event, channel, attr):\n        \"\"\"Update attribute list for current event/channel.\"\"\"\n        try:\n            for i, sensor in enumerate(self.event_states[event]):\n                if sensor[1] == int(channel):\n                    self.event_states[event][i] = attr\n        except KeyError:\n            _LOGGING.debug('Error updating attributes for: (%s, %s)',\n                           event, channel)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart the watchdog timer.", "response": "def start(self):\n        \"\"\" Starts the watchdog timer. \"\"\"\n        self._timer = Timer(self.time, self.handler)\n        self._timer.daemon = True\n        self._timer.start()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrendering the navigational item using a renderer.", "response": "def render(self, renderer=None, **kwargs):\n        \"\"\"Render the navigational item using a renderer.\n\n        :param renderer: An object implementing the :class:`~.Renderer`\n                         interface.\n        :return: A markupsafe string with the rendered result.\n        \"\"\"\n        return Markup(get_renderer(current_app, renderer)(**kwargs).visit(\n            self))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_object(self, node):\n        if current_app.debug:\n            return tags.comment('no implementation in {} to render {}'.format(\n                self.__class__.__name__,\n                node.__class__.__name__, ))\n        return ''", "response": "Fallback rendering for objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_renderer(app, id, renderer, force=True):\n    renderers = app.extensions.setdefault('nav_renderers', {})\n\n    if force:\n        renderers[id] = renderer\n    else:\n        renderers.setdefault(id, renderer)", "response": "Registers a renderer on the application."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving a renderer. :param app: :class:`~flask.Flask` application to look ``id`` up on :param id: Internal renderer id-string to look up", "response": "def get_renderer(app, id):\n    \"\"\"Retrieve a renderer.\n\n    :param app: :class:`~flask.Flask` application to look ``id`` up on\n    :param id: Internal renderer id-string to look up\n    \"\"\"\n    renderer = app.extensions.get('nav_renderers', {})[id]\n\n    if isinstance(renderer, tuple):\n        mod_name, cls_name = renderer\n        mod = import_module(mod_name)\n\n        cls = mod\n        for name in cls_name.split('.'):\n            cls = getattr(cls, name)\n\n        return cls\n\n    return renderer"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_app(self, app):\n        if not hasattr(app, 'extensions'):\n            app.extensions = {}\n\n        app.extensions['nav'] = self\n        app.add_template_global(self.elems, 'nav')\n\n        # register some renderers\n        for args in self._renderers:\n            register_renderer(app, *args)", "response": "Initialize an application.\n\n        :param app: A :class:`~flask.Flask` app."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef navigation(self, id=None):\n\n        def wrapper(f):\n            self.register_element(id or f.__name__, f)\n            return f\n\n        return wrapper", "response": "Decorator for navbar registration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclass decorator for Renderers. This is a class decorator that returns a function that will be called when the application is initialized.", "response": "def renderer(self, id=None, force=True):\n        \"\"\"Class decorator for Renderers.\n\n        The decorated class will be added to the list of renderers kept by this\n        instance that will be registered on the app upon app initialization.\n\n        :param id: Id for the renderer, defaults to the class name in snake\n                   case.\n        :param force: Whether or not to overwrite existing renderers.\n        \"\"\"\n\n        def _(cls):\n            name = cls.__name__\n            sn = name[0] + re.sub(r'([A-Z])', r'_\\1', name[1:])\n\n            self._renderers.append((id or sn.lower(), cls, force))\n            return cls\n\n        return _"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_time(time):\n    if isinstance(time, datetime.datetime):\n        return time\n    return datetime.datetime.strptime(time, DATETIME_FORMAT_OPENVPN)", "response": "Parses date and time from input string in OpenVPN logging format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Lrr_location(self):\n        return WKT_POINT_FMT.format(lng=float(self.LrrLON), lat=float(self.LrrLAT))", "response": "Return the location of the LRR in the wireless station"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a string to bytes.", "response": "def to_bytes(s):\n    \"\"\"\n    PY2/PY3 compatible way to convert to something cryptography understands\n    \"\"\"\n    if sys.version_info < (3,):\n        return \"\".join(map(chr, s))\n    else:\n        return bytes(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef loramac_decrypt(payload_hex, sequence_counter, key, dev_addr, direction=UP_LINK):\n    key = unhexlify(key)\n    dev_addr = unhexlify(dev_addr)\n    buffer = bytearray(unhexlify(payload_hex))\n    size = len(buffer)\n\n    bufferIndex = 0\n    # block counter\n    ctr = 1\n\n    # output buffer, initialize to input buffer size.\n    encBuffer = [0x00] * size\n\n    cipher = Cipher(algorithms.AES(key), modes.ECB(), backend=default_backend())\n\n    def aes_encrypt_block(aBlock):\n        \"\"\"\n        AES encrypt a block.\n        aes.encrypt expects a string, so we convert the input to string and\n        the return value to bytes again.\n        \"\"\"\n        encryptor = cipher.encryptor()\n\n        return bytearray(encryptor.update(to_bytes(aBlock)) + encryptor.finalize())\n\n    # For the exact definition of this block refer to\n    # 'chapter 4.3.3.1 Encryption in LoRaWAN' in the LoRaWAN specification\n    aBlock = bytearray(\n        [\n            0x01,  # 0 always 0x01\n            0x00,  # 1 always 0x00\n            0x00,  # 2 always 0x00\n            0x00,  # 3 always 0x00\n            0x00,  # 4 always 0x00\n            direction,  # 5 dir, 0 for uplink, 1 for downlink\n            dev_addr[3],  # 6 devaddr, lsb\n            dev_addr[2],  # 7 devaddr\n            dev_addr[1],  # 8 devaddr\n            dev_addr[0],  # 9 devaddr, msb\n            sequence_counter & 0xFF,  # 10 sequence counter (FCntUp) lsb\n            (sequence_counter >> 8) & 0xFF,  # 11 sequence counter\n            (sequence_counter >> 16) & 0xFF,  # 12 sequence counter\n            (sequence_counter >> 24) & 0xFF,  # 13 sequence counter (FCntUp) msb\n            0x00,  # 14 always 0x01\n            0x00,  # 15 block counter\n        ]\n    )\n\n    # complete blocks\n    while size >= 16:\n        aBlock[15] = ctr & 0xFF\n        ctr += 1\n        sBlock = aes_encrypt_block(aBlock)\n        for i in range(16):\n            encBuffer[bufferIndex + i] = buffer[bufferIndex + i] ^ sBlock[i]\n\n        size -= 16\n        bufferIndex += 16\n\n    # partial blocks\n    if size > 0:\n        aBlock[15] = ctr & 0xFF\n        sBlock = aes_encrypt_block(aBlock)\n        for i in range(size):\n            encBuffer[bufferIndex + i] = buffer[bufferIndex + i] ^ sBlock[i]\n\n    return encBuffer", "response": "This function decrypts a payload of a Lora - MAC message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the status log.", "response": "def parse(self):\n        \"\"\"Parses the status log.\n\n        :raises ParsingError: if syntax error found in the log.\n        :return: The :class:`.models.Status` with filled data.\n        \"\"\"\n        status = Status()\n        self.expect_line(Status.client_list.label)\n\n        status.updated_at = self.expect_tuple(Status.updated_at.label)\n        status.client_list.update({\n            text_type(c.real_address): c\n            for c in self._parse_fields(Client, Status.routing_table.label)})\n        status.routing_table.update({\n            text_type(r.virtual_address): r\n            for r in self._parse_fields(Routing, Status.global_stats.label)})\n        status.global_stats = GlobalStats()\n        status.global_stats.max_bcast_mcast_queue_len = self.expect_tuple(\n            GlobalStats.max_bcast_mcast_queue_len.label)\n\n        self.expect_line(self.terminator)\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_status(status_log, encoding='utf-8'):\n    if isinstance(status_log, bytes):\n        status_log = status_log.decode(encoding)\n    parser = LogParser.fromstring(status_log)\n    return parser.parse()", "response": "Parses the status log of OpenVPN."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef version(self):\n        res = self.client.service.Version()\n        return '.'.join([ustr(x) for x in res[0]])", "response": "Return version of the TR DWE."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns available sources of data.", "response": "def sources(self):\n        \"\"\"Return available sources of data.\"\"\"\n        res = self.client.service.Sources(self.userdata, 0)\n        return [ustr(x[0]) for x in res[0]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef request(self, query, source='Datastream',\n                fields=None, options=None, symbol_set=None, tag=None):\n        \"\"\"General function to retrieve one record in raw format.\n\n           query - query string for DWE system. This may be a simple instrument name\n                   or more complicated request. Refer to the documentation for the\n                   format.\n           source - The name of datasource (default: \"Datastream\")\n           fields - Fields to be retrieved (used when the requester does not want all\n                    fields to be delivered).\n           options - Options for specific data source. Many of datasources do not require\n                     opptions string. Refer to the documentation of the specific\n                     datasource for allowed syntax.\n           symbol_set - The symbol set used inside the instrument (used for mapping\n                        identifiers within the request. Refer to the documentation for\n                        the details.\n           tag - User-defined cookie that can be used to match up requests and response.\n                 It will be returned back in the response. The string should not be\n                 longer than 256 characters.\n        \"\"\"\n        if self.show_request:\n            try:\n                print('Request:' + query)\n            except UnicodeEncodeError:\n                print('Request:' + query.encode('utf-8'))\n\n        rd = self.client.factory.create('RequestData')\n        rd.Source = source\n        rd.Instrument = query\n        if fields is not None:\n            rd.Fields = self.client.factory.create('ArrayOfString')\n            rd.Fields.string = fields\n        rd.SymbolSet = symbol_set\n        rd.Options = options\n        rd.Tag = tag\n\n        self.last_response = self.client.service.RequestRecord(self.userdata, rd, 0)\n\n        return self.last_response", "response": "This function is used to request one record in raw format."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts status from the retrieved data and save it as a property of an object.", "response": "def status(self, record=None):\n        \"\"\"Extract status from the retrieved data and save it as a property of an object.\n           If record with data is not specified then the status of previous operation is\n           returned.\n\n           status - dictionary with data source, string with request and status type,\n                    code and message.\n\n           status['StatusType']: 'Connected' - the data is fine\n                                 'Stale'     - the source is unavailable. It may be\n                                               worthwhile to try again later\n                                 'Failure'   - data could not be obtained (e.g. the\n                                               instrument is incorrect)\n                                 'Pending'   - for internal use only\n           status['StatusCode']: 0 - 'No Error'\n                                 1 - 'Disconnected'\n                                 2 - 'Source Fault'\n                                 3 - 'Network Fault'\n                                 4 - 'Access Denied' (user does not have permissions)\n                                 5 - 'No Such Item' (no instrument with given name)\n                                 11 - 'Blocking Timeout'\n                                 12 - 'Internal'\n        \"\"\"\n        if record is not None:\n            self.last_status = {'Source': ustr(record['Source']),\n                                'StatusType': ustr(record['StatusType']),\n                                'StatusCode': record['StatusCode'],\n                                'StatusMessage': ustr(record['StatusMessage']),\n                                'Request': ustr(record['Instrument'])}\n        return self.last_status"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_record(self, raw, indx=0):\n        suffix = '' if indx == 0 else '_%i' % (indx + 1)\n\n        # Parsing status\n        status = self.status(raw)\n\n        # Testing if no errors\n        if status['StatusType'] != 'Connected':\n            if self.raise_on_error:\n                raise DatastreamException('%s (error %i): %s --> \"%s\"' %\n                                          (status['StatusType'], status['StatusCode'],\n                                           status['StatusMessage'], status['Request']))\n            else:\n                self._test_status_and_warn()\n                return pd.DataFrame(), {}\n\n        record = self.extract_data(raw)\n        get_field = lambda fldname: record[fldname + suffix]\n\n        try:\n            error = get_field('INSTERROR')\n            if self.raise_on_error:\n                raise DatastreamException('Error: %s --> \"%s\"' %\n                                          (error, status['Request']))\n            else:\n                self.last_status['StatusMessage'] = error\n                self.last_status['StatusType'] = 'INSTERROR'\n                self._test_status_and_warn()\n                metadata = {'Frequency': '', 'Currency': '', 'DisplayName': '',\n                            'Symbol': '', 'Status': error}\n        except KeyError:\n            # Parsing metadata of the symbol\n            # NB! currency might be returned as symbol thus \"unicode\" should be used\n            metadata = {'Frequency': ustr(get_field('FREQUENCY')),\n                        'Currency': ustr(get_field('CCY')),\n                        'DisplayName': ustr(get_field('DISPNAME')),\n                        'Symbol': ustr(get_field('SYMBOL')),\n                        'Status': 'OK'}\n\n        # Fields with data\n        if suffix == '':\n            fields = [ustr(x) for x in record if '_' not in x]\n        else:\n            fields = [ustr(x) for x in record if suffix in x]\n\n        # Filter metadata\n        meta_fields = ['CCY', 'DISPNAME', 'FREQUENCY', 'SYMBOL', 'DATE', 'INSTERROR']\n        fields = [x.replace(suffix, '') for x in fields\n                  if not any([y in x for y in meta_fields])]\n\n        if 'DATE' + suffix in record:\n            date = record['DATE' + suffix]\n        elif 'DATE' in record:\n            date = record['DATE']\n        else:\n            date = None\n\n        if len(fields) > 0 and date is not None:\n            # Check if we have a single value or a series\n            if isinstance(date, dt.datetime):\n                data = pd.DataFrame({x: [get_field(x)] for x in fields},\n                                    index=[date])\n            else:\n                data = pd.DataFrame({x: get_field(x)[0] for x in fields},\n                                    index=date[0])\n        else:\n            data = pd.DataFrame()\n\n        metadata = pd.DataFrame(metadata, index=[indx])\n        metadata = metadata[['Symbol', 'DisplayName', 'Currency', 'Frequency', 'Status']]\n        return data, metadata", "response": "Parse the raw data and return a pandas. DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_record_static(self, raw):\n        # Parsing status\n        status = self.status(raw)\n\n        # Testing if no errors\n        if status['StatusType'] != 'Connected':\n            if self.raise_on_error:\n                raise DatastreamException('%s (error %i): %s --> \"%s\"' %\n                                          (status['StatusType'], status['StatusCode'],\n                                           status['StatusMessage'], status['Request']))\n            else:\n                self._test_status_and_warn()\n                return pd.DataFrame(), {}\n\n        # Convert record to dict\n        record = self.extract_data(raw)\n\n        try:\n            error = record['INSTERROR']\n            if self.raise_on_error:\n                raise DatastreamException('Error: %s --> \"%s\"' %\n                                          (error, status['Request']))\n            else:\n                self.last_status['StatusMessage'] = error\n                self.last_status['StatusType'] = 'INSTERROR'\n                self._test_status_and_warn()\n                return pd.DataFrame(), {'Status': error, 'Date': None}\n        except KeyError:\n            metadata = {'Status': 'OK', 'Date': ''}\n\n        # All fields that are available\n        fields = [x for x in record if '_' not in x]\n        metadata['Date'] = record['DATE']\n        fields.remove('DATE')\n\n        # Number of elements\n        num = len([x[0] for x in record if 'SYMBOL' in x])\n\n        # field naming 'CCY', 'CCY_2', 'CCY_3', ...\n        fld_name = lambda field, indx: field if indx == 0 else field + '_%i' % (indx + 1)\n\n        # Construct pd.DataFrame\n        res = pd.DataFrame({fld: [record[fld_name(fld, ind)]\n                                  if fld_name(fld, ind) in record else ''\n                                  for ind in range(num)]\n                            for fld in fields})\n        return res, metadata", "response": "Parse raw data that is retrieved by static request and return pandas. DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a request string for querying TR DWE.", "response": "def construct_request(ticker, fields=None, date=None,\n                          date_from=None, date_to=None, freq=None):\n        \"\"\"Construct a request string for querying TR DWE.\n\n           tickers - ticker or symbol\n           fields  - list of fields.\n           date    - date for a single-date query\n           date_from, date_to - date range (used only if \"date\" is not specified)\n           freq    - frequency of data: daily('D'), weekly('W') or monthly('M')\n                     Use here 'REP' for static requests\n\n           Some of available fields:\n           P  - adjusted closing price\n           PO - opening price\n           PH - high price\n           PL - low price\n           VO - volume, which is expressed in 1000's of shares.\n           UP - unadjusted price\n           OI - open interest\n\n           MV - market value\n           EPS - earnings per share\n           DI - dividend index\n           MTVB - market to book value\n           PTVB - price to book value\n           ...\n\n           The full list of data fields is available at http://dtg.tfn.com/.\n        \"\"\"\n        if isinstance(ticker, basestring):\n            request = ticker\n        elif hasattr(ticker, '__len__'):\n            request = ','.join(ticker)\n        else:\n            raise ValueError('ticker should be either string or list/array of strings')\n        if fields is not None:\n            if isinstance(fields, basestring):\n                request += '~=' + fields\n            elif isinstance(fields, list) and len(fields) > 0:\n                request += '~=' + ','.join(fields)\n        if date is not None:\n            request += '~@' + pd.to_datetime(date).strftime('%Y-%m-%d')\n        else:\n            if date_from is not None:\n                request += '~' + pd.to_datetime(date_from).strftime('%Y-%m-%d')\n            if date_to is not None:\n                request += '~:' + pd.to_datetime(date_to).strftime('%Y-%m-%d')\n        if freq is not None:\n            request += '~' + freq\n        return request"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetch(self, tickers, fields=None, date=None, date_from=None, date_to=None,\n              freq='D', only_data=True, static=False):\n        \"\"\"Fetch data from TR DWE.\n\n           tickers - ticker or list of tickers\n           fields  - list of fields.\n           date    - date for a single-date query\n           date_from, date_to - date range (used only if \"date\" is not specified)\n           freq    - frequency of data: daily('D'), weekly('W') or monthly('M')\n           only_data - if True then metadata will not be returned\n           static  - if True \"static\" request is created (i.e. not a series).\n                     In this case 'date_from', 'date_to' and 'freq' are ignored\n\n           In case list of tickers is requested, a MultiIndex-dataframe is returned.\n\n           Some of available fields:\n           P  - adjusted closing price\n           PO - opening price\n           PH - high price\n           PL - low price\n           VO - volume, which is expressed in 1000's of shares.\n           UP - unadjusted price\n           OI - open interest\n\n           MV - market value\n           EPS - earnings per share\n           DI - dividend index\n           MTVB - market to book value\n           PTVB - price to book value\n           ...\n\n           The full list of data fields is available at http://dtg.tfn.com/.\n        \"\"\"\n        if static:\n            query = self.construct_request(tickers, fields, date, freq='REP')\n        else:\n            query = self.construct_request(tickers, fields, date, date_from, date_to, freq)\n\n        raw = self.request(query)\n\n        if static:\n            data, metadata = self.parse_record_static(raw)\n        elif isinstance(tickers, basestring) or len(tickers) == 1:\n            data, metadata = self.parse_record(raw)\n        elif hasattr(tickers, '__len__'):\n            metadata = pd.DataFrame()\n            data = {}\n            for indx in range(len(tickers)):\n                dat, meta = self.parse_record(raw, indx)\n                data[tickers[indx]] = dat\n                metadata = metadata.append(meta, ignore_index=False)\n\n            data = pd.concat(data)\n        else:\n            raise DatastreamException(('First argument should be either ticker or '\n                                       'list of tickers'))\n\n        if only_data:\n            return data\n        else:\n            return data, metadata", "response": "Fetch data from TR DWE."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets Open High Low Close prices and daily Volume for a given ticker.", "response": "def get_OHLCV(self, ticker, date=None, date_from=None, date_to=None):\n        \"\"\"Get Open, High, Low, Close prices and daily Volume for a given ticker.\n\n           ticker  - ticker or symbol\n           date    - date for a single-date query\n           date_from, date_to - date range (used only if \"date\" is not specified)\n\n           Returns pandas.Dataframe with data. If error occurs, then it is printed as\n           a warning.\n        \"\"\"\n        data, meta = self.fetch(ticker + \"~OHLCV\", None, date,\n                                date_from, date_to, 'D', only_data=False)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of all constituents of a given index.", "response": "def get_constituents(self, index_ticker, date=None, only_list=False):\n        \"\"\" Get a list of all constituents of a given index.\n\n            index_ticker - Datastream ticker for index\n            date         - date for which list should be retrieved (if None then\n                           list of present constituents is retrieved)\n            only_list    - request only list of symbols. By default the method\n                           retrieves many extra fields with information (various\n                           mnemonics and codes). This might pose some problems\n                           for large indices like Russel-3000. If only_list=True,\n                           then only the list of symbols and names are retrieved.\n        \"\"\"\n        if date is not None:\n            str_date = pd.to_datetime(date).strftime('%m%y')\n        else:\n            str_date = ''\n        # Note: ~XREF is equal to the following large request\n        # ~REP~=DSCD,EXMNEM,GEOG,GEOGC,IBTKR,INDC,INDG,INDM,INDX,INDXEG,INDXFS,INDXL,\n        #       INDXS,ISIN,ISINID,LOC,MNEM,NAME,SECD,TYPE\n        fields = '~REP~=NAME' if only_list else '~XREF'\n        query = 'L' + index_ticker + str_date + fields\n        raw = self.request(query)\n\n        res, metadata = self.parse_record_static(raw)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_epit_vintage_matrix(self, mnemonic, date_from='1951-01-01', date_to=None):\n        # Get first available date from the REL1 series\n        rel1 = self.fetch(mnemonic, 'REL1', date_from=date_from, date_to=date_to)\n        date_0 = rel1.dropna().index[0]\n\n        # All release dates\n        reld123 = self.fetch(mnemonic, ['RELD1', 'RELD2', 'RELD3'],\n                             date_from=date_0, date_to=date_to).dropna(how='all')\n\n        # Fetch all vintages\n        res = {}\n        for date in reld123.index:\n            try:\n                _tmp = self.fetch(mnemonic, 'RELV', date_from=date_0, date_to=date).dropna()\n            except DatastreamException:\n                continue\n            res[date] = _tmp\n        return pd.concat(res).RELV.unstack()", "response": "Construct the vintage matrix for a given economic series."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_epit_revisions(self, mnemonic, period, relh50=False):\n        if relh50:\n            data = self.fetch(mnemonic, 'RELH50', date=period, static=True)\n        else:\n            data = self.fetch(mnemonic, 'RELH', date=period, static=True)\n        data = data.iloc[0]\n\n        # Parse the response\n        res = {data.loc['RELHD%02d' % i]: data.loc['RELHV%02d' % i]\n               for i in range(1, 51 if relh50 else 21)\n               if data.loc['RELHD%02d' % i] != ''}\n        res = pd.Series(res, name=data.loc['RELHP  ']).sort_index()\n        return res", "response": "Return initial estimate and first revisions of a given economic time\n            series and a given period."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate the given validation level.", "response": "def check_validation_level(validation_level):\n    \"\"\"\n    Validate the given validation level\n\n    :type validation_level: ``int``\n    :param validation_level: validation level (see :class:`hl7apy.consts.VALIDATION_LEVEL`)\n    :raises: :exc:`hl7apy.exceptions.UnknownValidationLevel` if the given validation level is unsupported\n    \"\"\"\n    if validation_level not in (VALIDATION_LEVEL.QUIET, VALIDATION_LEVEL.STRICT, VALIDATION_LEVEL.TOLERANT):\n        raise UnknownValidationLevel"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_library(version):\n    check_version(version)\n    module_name = SUPPORTED_LIBRARIES[version]\n    lib = sys.modules.get(module_name)\n    if lib is None:\n        lib = importlib.import_module(module_name)\n    return lib", "response": "Load the correct module according to the version"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_reference(name, element_type, version):\n    lib = load_library(version)\n    ref = lib.get(name, element_type)\n    return ref", "response": "Loads the reference of an element of the given type name and version and returns the reference structure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind an element of the given name and version into the given types and return its reference structure.", "response": "def find_reference(name, element_types, version):\n    \"\"\"\n    Look for an element of the given name and version into the given types and return its reference structure\n\n    :type name: ``str``\n    :param name: the element name to look for (e.g. 'MSH')\n    :type types: ``list`` or ``tuple``\n    :param types: the element classes where to look for the element (e.g. (Group, Segment))\n    :type version: ``str``\n    :param version: the version of the library where to search the element (e.g. '2.6')\n    :rtype: ``dict``\n    :return: a dictionary describing the element structure\n    :raise: :class:`hl7apy.exceptions.ChildNotFound` if the element has not been found\n\n    >>> from hl7apy.core import Message, Segment\n    >>> find_reference('UNKNOWN', (Segment, ), '2.5')  # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n    ...\n    ChildNotFound: No child named UNKNOWN\n    >>> find_reference('ADT_A01', (Segment,),  '2.5')  # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n    ...\n    ChildNotFound: No child named ADT_A01\n    >>> r = find_reference('ADT_A01', (Message,),  '2.5')\n    >>> print('%s %s' % (r['name'], r['cls']))\n    ADT_A01 <class 'hl7apy.core.Message'>\n    \"\"\"\n    lib = load_library(version)\n    ref = lib.find(name, element_types)\n    return ref"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds a node in the hierarchy where.", "response": "def find(name, where):\n    \"\"\"\n    >>> from hl7apy.core import Segment\n    >>> from hl7apy import find_reference\n    >>> find_reference('UNKNOWN', (Segment, ), '2.3.1')  # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n    ...\n    ChildNotFound: No child named UNKNOWN\n    \"\"\"\n    for cls in where:\n        try:\n            return {'ref': get(name, cls.__name__), 'name': name, 'cls': cls}\n        except ChildNotFound:\n            pass\n    raise ChildNotFound(name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the datetime object and the format of the date in input", "response": "def get_date_info(value):\n    \"\"\"\n    Returns the datetime object and the format of the date in input\n\n    :type value: `str`\n    \"\"\"\n    fmt = _get_date_format(value)\n    dt_value = _datetime_obj_factory(value, fmt)\n    return dt_value, fmt"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_timestamp_info(value):\n    value, offset = _split_offset(value)\n    fmt, microsec = _get_timestamp_format(value)\n    dt_value = _datetime_obj_factory(value, fmt)\n    return dt_value, fmt, offset, microsec", "response": "Returns the datetime object the format offset and microsecond of the timestamp in input\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the datetime object the format offset and microsecond of the datetime in input", "response": "def get_datetime_info(value):\n    \"\"\"\n    Returns the datetime object, the format, the offset and the microsecond of the datetime in input\n\n    :type value: `str`\n    \"\"\"\n    date_value, offset = _split_offset(value)\n    date_format = _get_date_format(date_value[:8])\n\n    try:\n        timestamp_form, microsec = _get_timestamp_format(date_value[8:])\n    except ValueError:\n        if not date_value[8:]:  # if it's empty\n            timestamp_form, microsec = '', 4\n        else:\n            raise ValueError('{0} is not an HL7 valid date value'.format(value))\n\n    fmt = '{0}{1}'.format(date_format, timestamp_form)\n    dt_value = _datetime_obj_factory(date_value, fmt)\n    return dt_value, fmt, offset, microsec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the given datatype is a base datatype of the specified version", "response": "def is_base_datatype(datatype, version=None):\n    \"\"\"\n    Check if the given datatype is a base datatype of the specified version\n\n    :type datatype: ``str``\n    :param datatype: the datatype (e.g. ST)\n\n    :type version: ``str``\n    :param version: the HL7 version (e.g. 2.5)\n\n    :return: ``True`` if it is a base datatype, ``False`` otherwise\n\n    >>> is_base_datatype('ST')\n    True\n    >>> is_base_datatype('CE')\n    False\n    \"\"\"\n    if version is None:\n        version = get_default_version()\n    lib = load_library(version)\n    return lib.is_base_datatype(datatype)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_ordered_children(self):\n        ordered_keys = self.element.ordered_children if self.element.ordered_children is not None else []\n        children = [self.indexes.get(k, None) for k in ordered_keys]\n        return children", "response": "Return the list of children ordered according to the element structure\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert(self, index, child, by_name_index=-1):\n        if self._can_add_child(child):\n            try:\n                if by_name_index == -1:\n                    self.indexes[child.name].append(child)\n                else:\n                    self.indexes[child.name].insert(by_name_index, child)\n            except KeyError:\n                self.indexes[child.name] = [child]\n            self.list.insert(index, child)", "response": "Adds a child at the given index."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef append(self, child):\n        if self._can_add_child(child):\n            if self.element == child.parent:\n                self._remove_from_traversal_index(child)\n                self.list.append(child)\n                try:\n                    self.indexes[child.name].append(child)\n                except KeyError:\n                    self.indexes[child.name] = [child]\n            elif self.element == child.traversal_parent:\n                try:\n                    self.traversal_indexes[child.name].append(child)\n                except KeyError:\n                    self.traversal_indexes[child.name] = [child]", "response": "Append the given child to the list of nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassigns the value to the child having the given name at the given index.", "response": "def set(self, name, value, index=-1):\n        \"\"\"\n        Assign the ``value`` to the child having the given ``name`` at the ``index`` position\n\n        :type name: ``str``\n        :param name: the child name (e.g. PID)\n\n        :type value: an instance of :class:`Element <hl7apy.core.Element>`, a `str` or an instance of\n            :class:`ElementProxy <hl7apy.core.ElementProxy>`\n        :param value: the child value\n\n        :type index: ``int``\n        :param index: the child position (e.g. 1)\n        \"\"\"\n\n        # just copy the first element of the ElementProxy (e.g. message.pid = message2.pid)\n        if isinstance(value, ElementProxy):\n            value = value[0].to_er7()\n\n        name = name.upper()\n        reference = None if name is None else self.element.find_child_reference(name)\n        child_ref, child_name = (None, None) if reference is None else (reference['ref'], reference['name'])\n\n        if isinstance(value, basestring):  # if the value is a basestring, parse it\n            child = self.element.parse_child(value, child_name=child_name, reference=child_ref)\n        elif isinstance(value, Element):  # it is already an instance of Element\n            child = value\n        elif isinstance(value, BaseDataType):\n            child = self.create_element(name, False, reference)\n            child.value = value\n        else:\n            raise ChildNotValid(value, child_name)\n\n        if child.name != child_name:  # e.g. message.pid = Segment('SPM') is forbidden\n            raise ChildNotValid(value, child_name)\n\n        child_to_remove = self.child_at_index(child_name, index)\n\n        if child_to_remove is None:\n            self.append(child)\n        else:\n            self.replace_child(child_to_remove, child)\n\n        # a set has been called, change the temporary parent to be the actual one\n        self.element.set_parent_to_traversal()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the given child from both the list and the indexes of the given child.", "response": "def remove(self, child):\n        \"\"\"\n        Remove the given child from both child list and child indexes\n\n        :type child: :class:`Element <hl7apy.core.Element>`\n        :param child: an instance of :class:`Element <hl7apy.core.Element>` subclass\n        \"\"\"\n        try:\n            if self.element == child.traversal_parent:\n                self._remove_from_traversal_index(child)\n            else:\n                self._remove_from_index(child)\n                self.list.remove(child)\n        except:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the child having the given name at the given position.", "response": "def remove_by_name(self, name, index=0):\n        \"\"\"\n        Remove the child having the given name at the given position\n\n        :type name: ``str``\n        :param name: child name (e.g. PID)\n\n        :type index: ``int``\n        :param index: child index\n\n        :return: an instance of :class:`Element <hl7apy.core.Element>` subclass\n        \"\"\"\n        child = self.child_at_index(name, index)\n        self.remove(child)\n        return child"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef child_at_index(self, name, index):\n\n        def _finder(n, i):\n            try:\n                return self.indexes[n][i]\n            except (KeyError, IndexError):\n                try:\n                    return self.traversal_indexes[n][i]\n                except (KeyError, IndexError):\n                    return None\n\n        child = _finder(name, index)\n        child_name = None if name is None else self._find_name(name)\n        if child_name != name:\n            child = _finder(child_name, index)\n        return child", "response": "Return the child named name at the given index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an element with the given name", "response": "def create_element(self, name, traversal_parent=False, reference=None):\n        \"\"\"\n        Create an element having the given name\n\n        :type name: ``str``\n        :param name: the name of the element to be created (e.g. PID)\n\n        :type traversal_parent: ``bool``\n        :param traversal_parent: if ``True``, the parent will be set as temporary for traversal purposes\n\n        :param reference: the new element structure (see :func:`load_reference <hl7apy.load_reference>`)\n\n        :return: an instance of an :class:`hl7apy.core.Element` subclass\n\n        :raises: :exc:`ChildNotFound <hl7apy.exceptions.ChildNotFound>` if the element does not exist\n        \"\"\"\n        if reference is None:\n            reference = self.element.find_child_reference(name)\n        if reference is not None:\n            cls = reference['cls']\n            element_name = reference['name']\n            kwargs = {'reference': reference['ref'],\n                      'validation_level': self.element.validation_level,\n                      'version': self.element.version}\n            if not traversal_parent:\n                kwargs['parent'] = self.element\n            else:\n                kwargs['traversal_parent'] = self.element\n            return cls(element_name, **kwargs)\n        else:\n            raise ChildNotFound(name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the reference of a child having the given name.", "response": "def _find_name(self, name):\n        \"\"\"\n        Find the reference of a child having the given name\n\n        :type name: ``str``\n        :param name: the child name (e.g. PID)\n\n        :return: the element structure (see :func:`load_reference <hl7apy.load_reference>`) or ``None`` if the\n            element has not been found\n        \"\"\"\n        name = name.upper()\n        element = self.element.find_child_reference(name)\n        return element['name'] if element is not None else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _default_child_lookup(self, name):\n        if name in self.indexes or name in self.traversal_indexes:\n            try:\n                return self.proxies[name]\n            except KeyError:\n                self.proxies[name] = ElementProxy(self, name)\n                return self.proxies[name]\n        else:  # child not found in the indexes dictionary (e.g. msh_9.message_code, msh_9.msh_9_1)\n            child_name = self._find_name(name)\n            if child_name is not None:\n                try:\n                    return self.proxies[child_name]\n                except KeyError:\n                    self.proxies[child_name] = ElementProxy(self, child_name)\n                    return self.proxies[child_name]", "response": "Default child lookup method."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the structure of the given element", "response": "def get_structure(element, reference=None):\n        \"\"\"\n        Get the element structure\n\n        :type element: :class:`Element <hl7apy.core.Element>`\n        :param element: element having the given reference structure\n\n        :param reference: the element structure from :func:`load_reference <hl7apy.load_reference>` or from a\n            message profile\n\n        :return: a dictionary containing the structure data\n        \"\"\"\n        if reference is None:\n            try:\n                reference = load_reference(element.name, element.classname, element.version)\n            except (ChildNotFound, KeyError):\n                raise InvalidName(element.classname, element.name)\n        if not isinstance(reference, collections.Sequence):\n            raise Exception\n        return ElementFinder._parse_structure(element, reference)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_structure(element, reference):\n        data = {\n            'reference': reference\n        }\n\n        content_type = reference[0]  # content type can be sequence, choice or leaf\n        if content_type in ('sequence', 'choice'):\n            children = reference[1]\n            ordered_children = []\n            structure = {}\n            structure_by_longname = {}\n            repetitions = {}\n            counters = collections.defaultdict(int)\n            for c in children:\n                child_name, child_ref, cardinality, cls = c\n                k = child_name if child_name not in structure \\\n                    else '{0}_{1}'.format(child_name, counters[child_name])\n                structure[k] = {\"ref\": child_ref, \"name\": k, \"cls\": element.child_classes[cls]}\n                try:\n                    structure_by_longname[child_ref[3]] = structure[k]\n                except IndexError:\n                    pass\n                counters[child_name] += 1\n                repetitions[k] = cardinality\n                ordered_children.append(k)\n            data['repetitions'] = repetitions\n            data['ordered_children'] = ordered_children\n            data['structure_by_name'] = structure\n            data['structure_by_longname'] = structure_by_longname\n\n        if len(reference) > 5:\n            datatype, long_name, table, max_length = reference[2:]\n            data['datatype'] = datatype\n            data['table'] = table\n            data['long_name'] = long_name\n        return data", "response": "Parse the given element and return the structure data for the given a\n            message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the HL7 representation of the object.", "response": "def to_er7(self, encoding_chars=None, trailing_children=False):\n        \"\"\"\n        Returns the HL7 representation of the :class:`Element <hl7apy.core.Element>`. It adds the appropriate\n        separator at the end if needed\n\n        :type encoding_chars: ``dict``\n        :param encoding_chars: The encoding chars to use.\n            If it is ``None`` it uses :attr:`self.encoding_chars`,\n            which by default is the ones return by\n            :func:`get_default_encoding_chars <hl7apy.get_default_encoding_chars>` values\n        :rtype: ``str``\n        :return: the HL7 representation of the :class:`Element <hl7apy.core.Element>`\n        \"\"\"\n        if encoding_chars is None:\n            encoding_chars = self.encoding_chars\n\n        child_class = list(self.child_classes.values())[0]\n        separator = encoding_chars.get(child_class.__name__.upper(), '')\n\n        s = []\n        for child in self._get_children(trailing_children):\n            if child:\n                s.extend(repetition.to_er7(encoding_chars, trailing_children) for repetition in child)\n            else:\n                try:\n                    s.append(self._handle_empty_children(encoding_chars))\n                except NotImplementedError:\n                    pass\n\n        return separator.join(s)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(self, report_file=None):\n        return Validator.validate(self, reference=self.reference, report_file=report_file)", "response": "Validate the HL7 element using the strict validation level."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encoding_chars(self):\n        if self.parent is not None:\n            return self.parent.encoding_chars\n        return get_default_encoding_chars(self.version)", "response": "Return the encoding chars of the current locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the ER7 - encoded version of the attribute value.", "response": "def to_er7(self, encoding_chars=None, trailing_children=False):\n        \"\"\"\n        Return the ER7-encoded string\n\n        :type encoding_chars: ``dict``\n        :param encoding_chars: a dictionary containing the encoding chars or None to use the default\n            (see :func:`get_default_encoding_chars <hl7apy.get_default_encoding_chars>`)\n\n        :type trailing_children: ``bool``\n        :param trailing_children: if ``True``, trailing children will be added even if their value is None\n\n        :return: the ER7-encoded string\n\n        >>> s = SubComponent(\"CE_1\")\n        >>> s.value = \"IDENTIFIER\"\n        >>> print(s.to_er7())\n        IDENTIFIER\n        \"\"\"\n\n        if encoding_chars is None:\n            encoding_chars = self.encoding_chars\n        try:\n            return self.value.to_er7(encoding_chars)\n        except AttributeError:\n            return self.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_subcomponent(self, name):\n        if self.is_unknown() and is_base_datatype(self.datatype):\n            # An unknown component can't have a child\n            raise ChildNotValid(name, self)\n        return self.children.create_element(name)", "response": "Create an instance of a subcomponent with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an instance of a SubComponent to the list of children.", "response": "def add(self, obj):\n        \"\"\"\n        Add an instance of :class:`SubComponent <hl7apy.core.SubComponent>` to the list of children\n\n        :param obj: an instance of :class:`SubComponent <hl7apy.core.SubComponent>`\n\n        >>> c = Component('CX_10')\n        >>> s = SubComponent(name='CWE_1', value='EXAMPLE_ID')\n        >>> s2 = SubComponent(name='CWE_4', value='ALT_ID')\n        >>> c.add(s)\n        >>> c.add(s2)\n        >>> print(c.to_er7())\n        EXAMPLE_ID&&&ALT_ID\n        \"\"\"\n        # base datatype components can't have more than one child\n        if self.name and is_base_datatype(self.datatype, self.version) and \\\n                len(self.children) >= 1:\n            raise MaxChildLimitReached(self, obj, 1)\n\n        # the name is different from the datatype (i.e. the name has been forced to be equal to the datatype)\n        try:\n            if obj.name and obj.name != obj.datatype:\n                try:\n                    if not _valid_child_name(obj.name, self.datatype):\n                        raise ChildNotValid(obj.name, self)\n                except AttributeError:\n                    pass\n        except ChildNotFound:  # obj.datatype causes ChildNotFound for some Elements (Message, Groups etc)\n            raise ChildNotValid(obj, self)\n\n        return super(Component, self).add(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, obj):\n        # base datatype components can't have more than one child\n        if self.name and is_base_datatype(self.datatype, self.version) and \\\n                len(self.children) >= 1:\n            raise MaxChildLimitReached(self, obj, 1)\n\n        return super(Field, self).add(obj)", "response": "Add an instance of Component to the list of children\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_er7(self, encoding_chars=None, trailing_children=False):\n        if encoding_chars is None:\n            encoding_chars = self.encoding_chars\n        if self.is_named('MSH_1'):\n            try:\n                return self.msh_1_1.children[0].value.value\n            except IndexError:\n                return self.msh_1_1.children[0].value\n        elif self.is_named('MSH_2'):\n            try:\n                return self.msh_2_1.children[0].value.value\n            except IndexError:\n                return self.msh_2_1.children[0].value\n        return super(Field, self).to_er7(encoding_chars, trailing_children)", "response": "Return the ER7 - encoded string representation of the ADT field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving component and subcomponent indexes from the given traversal path.", "response": "def _get_traversal_children(self, name):\n        \"\"\"\n        Retrieve component and subcomponent indexes from the given traversal path\n        (e.g. PID_1_2 -> component=2, subcomponent=None)\n        \"\"\"\n        name = name.upper()\n        parts = name.split('_')\n        try:\n            assert 3 <= len(parts) <= 4\n            prefix = \"{0}_{1}\".format(parts[0], parts[1])\n            component = int(parts[2])\n            subcomponent = int(parts[3]) if len(parts) == 4 else None\n        except (AssertionError, ValueError):\n            return None, None\n        else:\n            if prefix != self.name:\n                return None, None\n        return component, subcomponent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_child_reference(self, name):\n        name = name.upper()\n        element = self.structure_by_name.get(name, None) or self.structure_by_longname.get(name, None)\n\n        if element is None:  # not found in self.structure\n            if self.allow_infinite_children and _valid_child_name(name, self.name):\n                if _valid_z_field_name(name):\n                    datatype = 'ST'\n                else:\n                    datatype = 'varies'\n\n                element = {'cls': Field, 'name': name, 'ref': ('leaf', None, datatype, None, None, -1)}\n            else:\n                element = find_reference(name, self.child_classes.values(), self.version)\n                if element:\n                    raise ChildNotValid(name, self)\n                else:\n                    raise ChildNotFound(name)\n        return element", "response": "This method returns a reference to a child of the given name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_er7(self, encoding_chars=None, trailing_children=False):\n        if encoding_chars is None:\n            encoding_chars = self.encoding_chars\n\n        separator = encoding_chars.get('FIELD')\n        repetition = encoding_chars.get('REPETITION')\n        s = [self.name]\n        for child in self._get_children(trailing_children):\n            if child is not None:\n                s.append(repetition.join(item.to_er7(encoding_chars, trailing_children) for item in child))\n            else:\n                try:\n                    s.append(self._handle_empty_children(encoding_chars))\n                except NotImplementedError:\n                    pass\n\n        if self.name == 'MSH' and len(s) > 1:\n            s.pop(1)\n\n        return separator.join(s)", "response": "Return the ER7 - encoded version of the current entry in a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_mllp(self, encoding_chars=None, trailing_children=False):\n        if encoding_chars is None:\n            encoding_chars = self.encoding_chars\n\n        return \"{0}{1}{2}{3}{2}\".format(MLLP_ENCODING_CHARS.SB,\n                                        self.to_er7(encoding_chars, trailing_children),\n                                        MLLP_ENCODING_CHARS.CR,\n                                        MLLP_ENCODING_CHARS.EB)", "response": "Returns the MLLP representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the given element is a valid HL7 message according to the given reference.", "response": "def validate(element, reference=None, report_file=None):\n        \"\"\"\n        Checks if the :class:`Element <hl7apy.core.Element>` is a valid HL7 message according to the reference\n        specified. If the reference is not specified, it will be used the official HL7 structures for the\n        elements.\n        In particular it checks:\n\n        * the maximum and minimum number of occurrences for every child\n        * that children are all allowed\n        * the datatype of fields, components and subcomponents\n        * the values, in particular the length and the adherence with the HL7 table, if one is specified\n\n        It raises the first exception that it finds.\n\n        If :attr:`report_file` is specified, it will create a file with all the errors that occur.\n\n        :param element: :class:`Element <hl7apy.core.Element>`: The element to validate\n        :param reference: the reference to use. Usually is None or a message profile object\n        :param report_file: the name of the report file to create\n\n        :return: The True if everything is ok\n        :raises: :exc:`ValidationError <hl7apy.exceptions.ValidationError>`: when errors occur\n        :raises: :exc:`ValidationWarning <hl7apy.exceptions.ValidationWarning>`: errors concerning the values\n        \"\"\"\n\n        from hl7apy.core import is_base_datatype\n\n        def _check_z_element(el, errs, warns):\n            if el.classname == 'Field':\n                if is_base_datatype(el.datatype, el.version) or \\\n                        el.datatype == 'varies':\n                    return True\n                elif el.datatype is not None:\n                    # if the datatype the is a complex datatype, the z element must follow the correct\n                    # structure of that datatype\n                    # Component just to search in the datatypes....\n                    dt_struct = load_reference(el.datatype, 'Datatypes_Structs', el.version)\n                    ref = ('sequence', dt_struct, el.datatype, None, None, -1)\n                    _check_known_element(el, ref, errs, warns)\n            for c in el.children:\n                _is_valid(c, None, errs, warns)\n            return True\n\n        def _check_repetitions(el, children, cardinality, child_name, errs):\n            children_num = len(children)\n            min_repetitions, max_repetitions = cardinality\n            if max_repetitions != -1:\n                if children_num < min_repetitions:\n                    errs.append(ValidationError(\"Missing required child {}.{}\".format(el.name,\n                                                                                      child_name)))\n                elif children_num > max_repetitions:\n                    errs.append(ValidationError(\"Child limit exceeded {}.{}\".format(child_name,\n                                                                                    el.name)))\n            else:\n                if children_num < min_repetitions:\n                    errs.append(ValidationError(\"Missing required child {}.{}\".format(el.name,\n                                                                                      child_name)))\n\n        def _check_table_compliance(el, ref, warns):\n            table = ref[4]\n            if table is not None:\n                try:\n                    table_ref = load_reference(table, 'Table', el.version)\n                except ChildNotFound:\n                    pass\n                else:\n                    table_children = table_ref[1]\n                    if el.to_er7() not in table_children:\n                        warns.append(ValidationWarning(\"Value {} not in table {} in element {}.{}\".\n                                                       format(el.to_er7(), table, el.parent.name,\n                                                              el.name)))\n\n        def _check_length(el, ref, warns):\n            max_length = ref[5]\n            if -1 < max_length < len(el.to_er7()):\n                warns.append(ValidationWarning(\"Exceeded max length ({}) of {}.{}\".\n                                               format(max_length, el.parent.name, el.name)))\n\n        def _check_datatype(el, ref, errs):\n            ref_datatype = ref[2]\n            if el.datatype != ref_datatype:\n                errs.append(ValidationError(\"Datatype {} is not correct for {}.{} (it must be {})\".\n                                            format(el.datatype, el.parent.name, el.name, ref[1])))\n\n        def _get_valid_children_info(ref):\n            valid_children = {c[0] for c in ref[1]}\n            children_refs = ref[1]\n            return valid_children, children_refs\n\n        def _get_child_reference_info(ref):\n            child_name, cardinality = ref[0], ref[2]\n            return child_name, cardinality\n\n        def _check_known_element(el, ref, errs, warns):\n            if ref is None:\n                try:\n                    ref = load_reference(el.name, el.classname, el.version)\n                except ChildNotFound:\n                    errs.append(ValidationError(\"Invalid element found: {}\".format(el)))\n\n            if ref[0] in ('sequence', 'choice'):\n                element_children = {c.name for c in el.children if not c.is_z_element()}\n                valid_children, valid_children_refs = _get_valid_children_info(ref)\n\n                # check that the children are all allowed children\n                if not element_children <= valid_children:\n                    errs.append(ValidationError(\"Invalid children detected for {}: {}\".\n                                                format(el, list(element_children - valid_children))))\n\n                # iterates the valid children\n                for child_ref in valid_children_refs:\n                    # it gets the structure of the children to check\n                    child_name, cardinality = _get_child_reference_info(child_ref)\n                    try:\n                        # it gets all the occurrences of the children of a type\n                        children = el.children.get(child_name)\n                    except Exception:\n                        # TODO: it is due to the lack of element in the official reference files...  should\n                        # we raise an exception here?\n                        pass\n                    else:\n                        _check_repetitions(el, children, cardinality, child_name, errs)\n                        # calls validation for every children\n                        for c in children:\n                            _is_valid(c, child_ref[1], errs, warns)\n\n                # finally calls validation for z_elements\n                z_children = [c for c in el.children if c.is_z_element()]\n                for c in z_children:\n                    _is_valid(c, None, errs, warns)\n            else:\n                _check_table_compliance(el, ref, warns)\n\n                _check_length(el, ref, warns)\n\n                if el.datatype == 'varies':  # TODO: it should check the real rule\n                    return True\n                _check_datatype(el, ref, errs)\n\n                # For complex datatypes element, the reference is the one of the datatype\n                if not is_base_datatype(el.datatype, el.version):\n                    # Component just to search in the datatypes....\n                    ref = load_reference(el.datatype, 'Datatypes_Structs', el.version)\n                    _is_valid(el, ref, errs, warns)\n\n        def _is_valid(el, ref, errs, warns):\n            if el.is_unknown():\n                errs.append(ValidationError(\"Unknown element found: {}.{}\".format(el.parent, el)))\n                return\n\n            if el.is_z_element():\n                return _check_z_element(el, errs, warns)\n\n            return _check_known_element(el, ref, errs, warns)\n\n        errors = []\n        warnings = []\n\n        _is_valid(element, reference, errors, warnings)\n\n        if report_file is not None:\n            with open(report_file, \"w\") as f:\n                for e in errors:\n                    f.write(\"Error: {}\\n\".format(e))\n                for w in warnings:\n                    f.write(\"Warning: {}\\n\".format(w))\n\n        if errors:\n            raise errors[0]\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_message(message, validation_level=None, find_groups=True, message_profile=None, report_file=None,\n                  force_validation=False):\n    \"\"\"\n    Parse the given ER7-encoded message and return an instance of :class:`Message <hl7apy.core.Message>`.\n\n    :type message: ``str``\n    :param message: the ER7-encoded message to be parsed\n\n    :type validation_level: ``int``\n    :param validation_level: the validation level. Possible values are those defined in\n        :class:`VALIDATION_LEVEL <hl7apy.consts.VALIDATION_LEVEL>` class or ``None`` to use the default\n        validation level (see :func:`set_default_validation_level <hl7apy.set_default_validation_level>`)\n\n    :type find_groups: ``bool``\n    :param find_groups: if ``True``, automatically assign the segments found to the appropriate\n        :class:`Groups <hl7apy.core.Group>` instances. If ``False``, the segments found are assigned as\n        children of the :class:`Message <hl7apy.core.Message>` instance\n        \n    :type force_validation: ``bool``\n    :type force_validation: if ``True``, automatically forces the message validation after the end of the parsing\n\n    :return: an instance of :class:`Message <hl7apy.core.Message>`\n\n    >>> message = \"MSH|^~\\&|GHH_ADT||||20080115153000||OML^O33^OML_O33|0123456789|P|2.5||||AL\\\\rPID|1||\" \\\n    \"566-554-3423^^^GHH^MR||EVERYMAN^ADAM^A|||M|||2222 HOME STREET^^ANN ARBOR^MI^^USA||555-555-2004|||M\\\\r\"\n    >>> m = parse_message(message)\n    >>> print(m)\n    <Message OML_O33>\n    >>> print(m.msh.sending_application.to_er7())\n    GHH_ADT\n    >>> print(m.children)\n    [<Segment MSH>, <Group OML_O33_PATIENT>]\n    \"\"\"\n    message = message.lstrip()\n    encoding_chars, message_structure, version = get_message_info(message)\n    validation_level = _get_validation_level(validation_level)\n\n    try:\n        reference = message_profile[message_structure] if message_profile else None\n    except KeyError:\n        raise MessageProfileNotFound()\n\n    try:\n        m = Message(name=message_structure, reference=reference, version=version,\n                    validation_level=validation_level, encoding_chars=encoding_chars)\n    except InvalidName:\n        m = Message(version=version, validation_level=validation_level,\n                    encoding_chars=encoding_chars)\n\n    try:\n        children = parse_segments(message, m.version, encoding_chars, validation_level, m.reference, find_groups)\n    except AttributeError:  # m.reference can raise i\n        children = parse_segments(message, m.version, encoding_chars, validation_level, find_groups=False)\n\n    m.children = children\n\n    if force_validation:\n        if message_profile is None:\n            Validator.validate(m, report_file=report_file)\n        else:\n            Validator.validate(m, message_profile[message_structure], report_file=report_file)\n\n    return m", "response": "Parse a message and return an instance of the appropriate base class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the given ER7 - encoded segments and return a list of : class : Segment instances.", "response": "def parse_segments(text, version=None, encoding_chars=None, validation_level=None, references=None, find_groups=False):\n    \"\"\"\n    Parse the given ER7-encoded segments and return a list of :class:`hl7apy.core.Segment` instances.\n\n    :type text: ``str``\n    :param text: the ER7-encoded string containing the segments to be parsed\n\n    :type version: ``str``\n    :param version: the HL7 version (e.g. \"2.5\"), or ``None`` to use the default\n        (see :func:`set_default_version <hl7apy.set_default_version>`)\n\n    :type encoding_chars: ``dict``\n    :param encoding_chars: a dictionary containing the encoding chars or None to use the default\n        (see :func:`set_default_encoding_chars <hl7apy.set_default_encoding_chars>`)\n\n    :type validation_level: ``int``\n    :param validation_level: the validation level. Possible values are those defined in\n        :class:`VALIDATION_LEVEL <hl7apy.consts.VALIDATION_LEVEL>` class or ``None`` to use the default\n        validation level (see :func:`validation_level <hl7apy.set_default_validation_level>`)\n\n    :type references: ``list``\n    :param references: A list of the references of the :class:`Segment <hl7apy.core.Segment>`'s children\n\n    :type find_groups: ``bool``\n    :param find_groups: if ``True``, automatically assign the segments found to the appropriate\n        :class:`Groups <hl7apy.core.Group>` instances. If ``False``, the segments found are assigned as\n        children of the :class:`Message <hl7apy.core.Message>` instance\n\n    :return: a list of :class:`Segment <hl7apy.core.Segment>` instances\n\n    >>> segments = \"EVN||20080115153000||||20080114003000\\\\rPID|1||566-554-3423^^^GHH^MR||EVERYMAN^ADAM^A|||M|||\" \\\n    \"2222 HOME STREET^^ANN ARBOR^MI^^USA||555-555-2004|||M\\\\r\"\n    >>> print(parse_segments(segments))\n    [<Segment EVN>, <Segment PID>]\n    \"\"\"\n    version = _get_version(version)\n    encoding_chars = _get_encoding_chars(encoding_chars, version)\n    validation_level = _get_validation_level(validation_level)\n\n    segment_sep = encoding_chars['SEGMENT']\n    segments = []\n\n    parents_refs = [(None, references)]\n    current_parent = None\n    for s in text.split(segment_sep):\n        if len(s) > 0:\n            segment_name = s[:3]\n            for x in xrange(len(parents_refs)):\n                if not find_groups:\n                    segment = parse_segment(s.strip(), version, encoding_chars, validation_level)\n                    segments.append(segment)\n                else:\n                    ref, parents_refs = _get_segment_reference(segment_name, parents_refs)\n                    if ref is None:\n                        # group not found at the current level, go back to the previous level\n                        if current_parent is not None:\n                            parents_refs.pop()\n                            current_parent = current_parent.parent\n                    else:\n                        if current_parent is None and parents_refs[-1][0] is not None or \\\n                           current_parent is not None and parents_refs[-1][0] != current_parent.name:\n                            # create the parents group of the segment\n                            if current_parent is not None:\n                                cur_idx = parents_refs.index((current_parent.name, current_parent.reference))\n                            else:\n                                cur_idx = parents_refs.index((None, references))\n                            for p_ref in parents_refs[cur_idx+1:]:\n                                group = Group(p_ref[0], version=version, reference=p_ref[1],\n                                              validation_level=validation_level)\n                                if current_parent is None:\n                                    segments.append(group)\n                                else:\n                                    current_parent.add(group)\n                                current_parent = group\n                        elif current_parent is not None and segment_name in [c.name for c in current_parent.children] \\\n                                and current_parent.repetitions[segment_name][1] == 1:\n                            # The number of instances allowed is reached so we create another instance of the same\n                            group = Group(current_parent.name, version=version, reference=current_parent.reference,\n                                          validation_level=validation_level)\n\n                            if current_parent.parent is None:\n                                segments.append(group)\n                            else:\n                                current_parent.parent.add(group)\n                            current_parent = group\n\n                        segment = parse_segment(s.strip(), version, encoding_chars, validation_level, ref)\n                        if current_parent is None:\n                            segments.append(segment)\n                        else:\n                            current_parent.add(segment)\n                        break\n    return segments"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_segment(text, version=None, encoding_chars=None, validation_level=None, reference=None):\n    version = _get_version(version)\n    encoding_chars = _get_encoding_chars(encoding_chars, version)\n    validation_level = _get_validation_level(validation_level)\n\n    segment_name = text[:3]\n    text = text[4:] if segment_name != 'MSH' else text[3:]\n    segment = Segment(segment_name, version=version, validation_level=validation_level,\n                      reference=reference)\n    segment.children = parse_fields(text, segment_name, version, encoding_chars, validation_level,\n                                    segment.structure_by_name, segment.allow_infinite_children)\n    return segment", "response": "Parse the given text and return an instance of the class of Segment class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_fields(text, name_prefix=None, version=None, encoding_chars=None, validation_level=None,\n                 references=None, force_varies=False):\n    \"\"\"\n    Parse the given ER7-encoded fields and return a list of :class:`hl7apy.core.Field`.\n\n    :type text: ``str``\n    :param text: the ER7-encoded string containing the fields to be parsed\n\n    :type name_prefix: ``str``\n    :param name_prefix: the field prefix (e.g. MSH)\n\n    :type version: ``str``\n    :param version: the HL7 version (e.g. \"2.5\"), or ``None`` to use the default\n        (see :func:`set_default_version <hl7apy.set_default_version>`)\n\n    :type encoding_chars: ``dict``\n    :param encoding_chars: a dictionary containing the encoding chars or None to use the default\n        (see :func:`set_default_encoding_chars <hl7apy.set_default_encoding_chars>`)\n\n    :type validation_level: ``int``\n    :param validation_level: the validation level. Possible values are those defined in\n        :class:`VALIDATION_LEVEL <hl7apy.consts.VALIDATION_LEVEL>` class or ``None`` to use the default\n        validation level (see :func:`set_default_validation_level <hl7apy.set_default_validation_level>`)\n\n    :type references: ``list``\n    :param references: A list of the references of the :class:`Field <hl7apy.core.Field>`'s children\n\n    :type force_varies: ``bool``\n    :param force_varies: flag that force the fields to use a varies structure when no reference is found.\n        It is used when a segment ends with a field of type varies that thus support infinite children\n\n    :return: a list of :class:`Field <hl7apy.core.Field>` instances\n\n    >>> fields = \"1|NUCLEAR^NELDA^W|SPO|2222 HOME STREET^^ANN ARBOR^MI^^USA\"\n    >>> nk1_fields = parse_fields(fields, name_prefix=\"NK1\")\n    >>> print(nk1_fields)\n    [<Field NK1_1 (SET_ID_NK1) of type SI>, <Field NK1_2 (NAME) of type XPN>, <Field NK1_3 (RELATIONSHIP) of type CE>, \\\n<Field NK1_4 (ADDRESS) of type XAD>]\n    >>> s = Segment(\"NK1\")\n    >>> s.children = nk1_fields\n    >>> print(s.to_er7())\n    NK1|1|NUCLEAR^NELDA^W|SPO|2222 HOME STREET^^ANN ARBOR^MI^^USA\n    >>> unknown_fields = parse_fields(fields)\n    >>> s.children = unknown_fields\n    >>> print(s.to_er7())\n    NK1||||||||||||||||||||||||||||||||||||||||1|NUCLEAR^NELDA^W|SPO|2222 HOME STREET^^ANN ARBOR^MI^^USA\n    \"\"\"\n    version = _get_version(version)\n    encoding_chars = _get_encoding_chars(encoding_chars, version)\n    validation_level = _get_validation_level(validation_level)\n\n    text = text.strip(\"\\r\")\n    field_sep = encoding_chars['FIELD']\n    repetition_sep = encoding_chars['REPETITION']\n    splitted_fields = text.split(field_sep)\n    fields = []\n    for index, field in enumerate(splitted_fields):\n        name = \"{0}_{1}\".format(name_prefix, index+1) if name_prefix is not None else None\n        try:\n            reference = references[name]['ref'] if references is not None else None\n        except KeyError:\n            reference = None\n\n        if field.strip() or name is None:\n            if name == 'MSH_2':\n                fields.append(parse_field(field, name, version, encoding_chars, validation_level,\n                                          reference))\n            else:\n                for rep in field.split(repetition_sep):\n                    fields.append(parse_field(rep, name, version, encoding_chars, validation_level,\n                                              reference, force_varies))\n        elif name == \"MSH_1\":\n            fields.append(parse_field(field_sep, name, version, encoding_chars, validation_level,\n                                      reference))\n    return fields", "response": "Parse the given ER7 - encoded fields and return a list of : class : Field instances."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_field(text, name=None, version=None, encoding_chars=None, validation_level=None,\n                reference=None, force_varies=False):\n    \"\"\"\n    Parse the given ER7-encoded field and return an instance of :class:`Field <hl7apy.core.Field>`.\n\n    :type text: ``str``\n    :param text: the ER7-encoded string containing the fields to be parsed\n\n    :type name: ``str``\n    :param name: the field name (e.g. MSH_7)\n\n    :type version: ``str``\n    :param version: the HL7 version (e.g. \"2.5\"), or ``None`` to use the default\n        (see :func:`set_default_version <hl7apy.set_default_version>`)\n\n    :type encoding_chars: ``dict``\n    :param encoding_chars: a dictionary containing the encoding chars or None to use the default\n        (see :func:`set_default_encoding_chars <hl7apy.set_default_encoding_chars>`)\n\n    :type validation_level: ``int``\n    :param validation_level: the validation level. Possible values are those defined in\n        :class:`VALIDATION_LEVEL <hl7apy.consts.VALIDATION_LEVEL>` class or ``None`` to use the default\n        validation level (see :func:`set_default_validation_level <hl7apy.set_default_validation_level>`)\n\n    :type reference: ``dict``\n    :param reference: a dictionary containing the element structure returned by\n        :func:`load_reference <hl7apy.load_reference>` or :func:`find_reference <hl7apy.find_reference>`\n        or belonging to a message profile\n\n    :type force_varies: ``boolean``\n    :param force_varies: flag that force the fields to use a varies structure when no reference is found.\n        It is used when a segment ends with a field of type varies that thus support infinite children\n\n    :return: an instance of :class:`Field <hl7apy.core.Field>`\n\n    >>> field = \"NUCLEAR^NELDA^W\"\n    >>> nk1_2 = parse_field(field, name=\"NK1_2\")\n    >>> print(nk1_2)\n    <Field NK1_2 (NAME) of type XPN>\n    >>> print(nk1_2.to_er7())\n    NUCLEAR^NELDA^W\n    >>> unknown = parse_field(field)\n    >>> print(unknown)\n    <Field of type None>\n    >>> print(unknown.to_er7())\n    NUCLEAR^NELDA^W\n    \"\"\"\n    version = _get_version(version)\n    encoding_chars = _get_encoding_chars(encoding_chars, version)\n    validation_level = _get_validation_level(validation_level)\n\n    try:\n        field = Field(name, version=version, validation_level=validation_level, reference=reference)\n    except InvalidName:\n        if force_varies:\n            reference = ('leaf', None, 'varies', None, None, -1)\n            field = Field(name, version=version, validation_level=validation_level, reference=reference)\n        else:\n            field = Field(version=version, validation_level=validation_level, reference=reference)\n\n    if name in ('MSH_1', 'MSH_2'):\n        s = SubComponent(datatype='ST', value=text, validation_level=validation_level, version=version)\n        c = Component(datatype='ST', validation_level=validation_level, version=version)\n        c.add(s)\n        field.add(c)\n    else:\n        children = parse_components(text, field.datatype, version, encoding_chars, validation_level,\n                                    field.structure_by_name)\n        if Validator.is_tolerant(validation_level) and is_base_datatype(field.datatype, version) and \\\n                len(children) > 1:\n            field.datatype = None\n        field.children = children\n    return field", "response": "Parse the given ER7 - encoded field and return an instance of : class : Field <hl7apy. core. Field >"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_components(text, field_datatype='ST', version=None, encoding_chars=None,\n                     validation_level=None, references=None):\n    \"\"\"\n    Parse the given ER7-encoded components and return a list of :class:`Component <hl7apy.core.Component>`\n    instances.\n\n    :type text: ``str``\n    :param text: the ER7-encoded string containing the components to be parsed\n\n    :type field_datatype: ``str``\n    :param field_datatype: the datatype of the components (e.g. ST)\n\n    :type version: ``str``\n    :param version: the HL7 version (e.g. \"2.5\"), or ``None`` to use the default\n        (see :func:`set_default_version <hl7apy.set_default_version>`)\n\n    :type encoding_chars: ``dict``\n    :param encoding_chars: a dictionary containing the encoding chars or None to use the default\n        (see :func:`set_default_encoding_chars <hl7apy.set_default_encoding_chars>`)\n\n    :type validation_level: ``int``\n    :param validation_level: the validation level. Possible values are those defined in\n        :class:`VALIDATION_LEVEL <hl7apy.consts.VALIDATION_LEVEL>` class or ``None`` to use the default\n        validation level (see :func:`set_default_validation_level <hl7apy.set_default_validation_level>`)\n\n    :type references: ``list``\n    :param references: A list of the references of the :class:`Component <hl7apy.core.Component>`'s children\n\n    :return: a list of :class:`Component <hl7apy.core.Component>` instances\n\n    >>> components = \"NUCLEAR^NELDA^W^^TEST\"\n    >>> xpn = parse_components(components, field_datatype=\"XPN\")\n    >>> print(xpn)\n    [<Component XPN_1 (FAMILY_NAME) of type FN>, <Component XPN_2 (GIVEN_NAME) of type ST>, \\\n<Component XPN_3 (SECOND_AND_FURTHER_GIVEN_NAMES_OR_INITIALS_THEREOF) of type ST>, \\\n<Component XPN_5 (PREFIX_E_G_DR) of type ST>]\n    >>> print(parse_components(components))\n    [<Component ST (None) of type ST>, <Component ST (None) of type ST>, <Component ST (None) of type ST>, \\\n<Component ST (None) of type ST>, <Component ST (None) of type ST>]\n    \"\"\"\n    version = _get_version(version)\n    encoding_chars = _get_encoding_chars(encoding_chars, version)\n    validation_level = _get_validation_level(validation_level)\n\n    component_sep = encoding_chars['COMPONENT']\n    components = []\n    for index, component in enumerate(text.split(component_sep)):\n        if is_base_datatype(field_datatype, version):\n            component_datatype = field_datatype\n            component_name = None\n        elif field_datatype is None or field_datatype == 'varies':\n            component_datatype = None\n            component_name = 'VARIES_{0}'.format(index+1)\n        else:\n            component_name = \"{0}_{1}\".format(field_datatype, index+1)\n            component_datatype = None\n\n        try:\n            reference = references[component_name]['ref'] \\\n                if None not in (references, component_name) else None\n        except KeyError:\n            reference = None\n\n        if component.strip() or component_name is None or component_name.startswith(\"VARIES_\"):\n            components.append(parse_component(component, component_name, component_datatype,\n                                              version, encoding_chars, validation_level, reference))\n    return components", "response": "Parse the given ER7 - encoded components and return a list of the components that are found in the HL7 file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_component(text, name=None, datatype='ST', version=None, encoding_chars=None,\n                    validation_level=None, reference=None):\n    \"\"\"\n    Parse the given ER7-encoded component and return an instance of\n    :class:`Component <hl7apy.core.Component>`.\n\n    :type text: ``str``\n    :param text: the ER7-encoded string containing the components to be parsed\n\n    :type name: ``str``\n    :param name: the component's name (e.g. XPN_2)\n\n    :type datatype: ``str``\n    :param datatype: the datatype of the component (e.g. ST)\n\n    :type version: ``str``\n    :param version: the HL7 version (e.g. \"2.5\"), or ``None`` to use the default\n        (see :func:`set_default_version <hl7apy.set_default_version>`)\n\n    :type encoding_chars: ``dict``\n    :param encoding_chars: a dictionary containing the encoding chars or None to use the default\n        (see :func:`set_default_encoding_chars <hl7apy.set_default_encoding_chars>`)\n\n    :type validation_level: ``int``\n    :param validation_level: the validation level. Possible values are those defined in\n        :class:`VALIDATION_LEVEL <hl7apy.consts.VALIDATION_LEVEL>` class or ``None`` to use the default\n        validation level (see :func:`set_default_validation_level <hl7apy.set_default_validation_level>`)\n\n    :type reference: ``dict``\n    :param reference: a dictionary containing the element structure returned by\n        :func:`load_reference <hl7apy.load_reference>` or :func:`find_reference <hl7apy.find_reference>`\n        or belonging to a message profile\n\n    :return: an instance of :class:`Component <hl7apy.core.Component>`\n\n    >>> component = \"GATEWAY&1.3.6.1.4.1.21367.2011.2.5.17\"\n    >>> cx_4 = parse_component(component, name=\"CX_4\")\n    >>> print(cx_4)\n    <Component CX_4 (ASSIGNING_AUTHORITY) of type None>\n    >>> print(cx_4.to_er7())\n    GATEWAY&1.3.6.1.4.1.21367.2011.2.5.17\n    >>> print(parse_component(component))\n    <Component ST (None) of type None>\n    \"\"\"\n    version = _get_version(version)\n    encoding_chars = _get_encoding_chars(encoding_chars, version)\n    validation_level = _get_validation_level(validation_level)\n\n    try:\n        component = Component(name, datatype, version=version, validation_level=validation_level,\n                              reference=reference)\n    except InvalidName as e:\n        if Validator.is_strict(validation_level):\n            raise e\n        component = Component(datatype, version=version, validation_level=validation_level,\n                              reference=reference)\n    children = parse_subcomponents(text, component.datatype, version, encoding_chars, validation_level)\n    if Validator.is_tolerant(component.validation_level) and is_base_datatype(component.datatype, version) and \\\n            len(children) > 1:\n        component.datatype = None\n    component.children = children\n    return component", "response": "Parse the given ER7 - encoded component and return an instance of a new object of the classhl7. core. Component class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the given ER7 - encoded subcomponents and return a list of SubComponent instances.", "response": "def parse_subcomponents(text, component_datatype='ST', version=None, encoding_chars=None,\n                        validation_level=None):\n    \"\"\"\n    Parse the given ER7-encoded subcomponents and return a list of\n    :class:`SubComponent <hl7apy.core.SubComponent>` instances.\n\n    :type text: ``str``\n    :param text: the ER7-encoded string containing the components to be parsed\n\n    :type component_datatype: ``str``\n    :param component_datatype: the datatype of the subcomponents (e.g. ST)\n\n    :type version: ``str``\n    :param version: the HL7 version (e.g. \"2.5\"), or ``None`` to use the default\n        (see :func:`set_default_version <hl7apy.set_default_version>`)\n\n    :type encoding_chars: ``dict``\n    :param encoding_chars: a dictionary containing the encoding chars or None to use the default\n        (see :func:`set_default_encoding_chars <hl7apy.set_default_encoding_chars>`)\n\n    :type validation_level: ``int``\n    :param validation_level: the validation level. Possible values are those defined in\n        :class:`VALIDATION_LEVEL <hl7apy.consts.VALIDATION_LEVEL>` class or ``None`` to use the default\n        validation level (see :func:`set_default_validation_level <hl7apy.set_default_validation_level>`)\n\n    :return: a list of :class:`SubComponent <hl7apy.core.SubComponent>` instances\n\n    >>> subcomponents= \"ID&TEST&&AHAH\"\n    >>> cwe = parse_subcomponents(subcomponents, component_datatype=\"CWE\")\n    >>> print(cwe)\n    [<SubComponent CWE_1>, <SubComponent CWE_2>, <SubComponent CWE_4>]\n    >>> c = Component(datatype='CWE')\n    >>> c.children = cwe\n    >>> print(c.to_er7())\n    ID&TEST&&AHAH\n    >>> subs = parse_subcomponents(subcomponents)\n    >>> print(subs)\n    [<SubComponent ST>, <SubComponent ST>, <SubComponent ST>, <SubComponent ST>]\n    >>> c.children = subs\n    >>> print(c.to_er7())\n    &&&&&&&&&ID&TEST&&AHAH\n    \"\"\"\n    version = _get_version(version)\n    encoding_chars = _get_encoding_chars(encoding_chars, version)\n    validation_level = _get_validation_level(validation_level)\n\n    subcomp_sep = encoding_chars['SUBCOMPONENT']\n    subcomponents = []\n    for index, subcomponent in enumerate(text.split(subcomp_sep)):\n        if is_base_datatype(component_datatype, version) or component_datatype is None:\n            subcomponent_name = None\n            subcomponent_datatype = component_datatype if component_datatype is not None else 'ST'\n        else:\n            subcomponent_name = \"{0}_{1}\".format(component_datatype, index+1)\n            subcomponent_datatype = None\n        if subcomponent.strip() or subcomponent_name is None:\n            subcomponents.append(parse_subcomponent(subcomponent, subcomponent_name, subcomponent_datatype,\n                                                    version, validation_level))\n    return subcomponents"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_subcomponent(text, name=None, datatype='ST', version=None, validation_level=None):\n    version = _get_version(version)\n    validation_level = _get_validation_level(validation_level)\n\n    return SubComponent(name=name, datatype=datatype, value=text, version=version,\n                        validation_level=validation_level)", "response": "Parse the given ER7 - encoded component and return an instance of the appropriate class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef datatype_factory(datatype, value, version=None, validation_level=None):\n\n    from hl7apy.validation import Validator\n\n    if validation_level is None:\n        validation_level = get_default_validation_level()\n\n    if version is None:\n        version = get_default_version()\n\n    lib = load_library(version)\n\n    base_datatypes = lib.get_base_datatypes()\n\n    factories = base_datatypes.copy()\n\n    if 'DT' in factories:\n        factories['DT'] = date_factory\n    if 'TM' in factories:\n        factories['TM'] = timestamp_factory\n    if 'DTM' in factories:\n        factories['DTM'] = datetime_factory\n    if 'NM' in factories:\n        factories['NM'] = numeric_factory\n    if 'SI' in factories:\n        factories['SI'] = sequence_id_factory\n\n    try:\n        factory = factories[datatype]\n        if isinstance(factory, FunctionType):\n            return factory(value, base_datatypes[datatype], validation_level=validation_level)\n        return factory(value, validation_level=validation_level)\n    except KeyError:\n        raise InvalidDataType(datatype)\n    except ValueError as e:\n        if Validator.is_strict(validation_level):\n            raise e\n        # TODO: Do we really want this? In that case the parent's datatype must be changed accordingly\n        return factories['ST'](value)", "response": "This function generates a new object according to the given datatype and value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new date object for the given value.", "response": "def date_factory(value, datatype_cls, validation_level=None):\n    \"\"\"\n    Creates a :class:`DT <hl7apy.base_datatypes.DT>` object\n\n    The value in input must be a string parsable with :meth:`datetime.strptime`.\n    The date format is chosen according to the length of the value as stated in this table:\n\n    +-------+-----------+\n    |Length |Format     |\n    +=======+===========+\n    |4      |``%Y``     |\n    |       |           |\n    +-------+-----------+\n    |6      |``%Y%m``   |\n    |       |           |\n    +-------+-----------+\n    |8      |``%Y%m%d`` |\n    |       |           |\n    +-------+-----------+\n\n    Some examples that work are:\n\n    >>> from hl7apy.base_datatypes import DT\n    >>> date_factory(\"1974\", DT) #doctest: +ELLIPSIS\n    <hl7apy.base_datatypes.DT object at 0x...>\n    >>> date_factory(\"198302\", DT) #doctest: +ELLIPSIS\n    <hl7apy.base_datatypes.DT object at 0x...>\n    >>> date_factory(\"19880312\", DT) #doctest: +ELLIPSIS\n    <hl7apy.base_datatypes.DT object at 0x...>\n\n    If the value does not match one of the valid format it raises :exc:`ValueError`\n\n    :type value: ``str``\n    :param value: the value to assign the date object\n\n    :type datatype_cls: `class`\n    :param value: the :class:`DT <hl7apy.base_datatypes.DT>` class to use. It has to be one implementation of\n        the different version modules\n\n    :type validation_level: ``int``\n    :param validation_level: It must be a value from class :attr:`validation_level`\n        :class:`VALIDATION_LEVEL hl7apy.consts.VALIDATION_LEVEL` or ``None`` to use the default value\n\n    :rtype: :class:`hl7apy.base_datatypes.DT`\n    \"\"\"\n\n    dt_value, fmt = get_date_info(value)\n    return datatype_cls(dt_value, fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a :class:`TM <hl7apy.base_datatypes.TM>` object The value in input must be a string parsable with :meth:`datetime.strptime`. It can also have an offset part specified with the format +/-HHMM. The offset can be added with all the allowed format The date format is chosen according to the length of the value as stated in this table: +-------+-----------------+ |Length |Format | +=======+=================+ |2 |``%H`` | +-------+-----------------+ |4 |``%H%M`` | +-------+-----------------+ |6 |``%H%M%S`` | +-------+-----------------+ |10-13 |``%H%M%S.%f`` | +-------+-----------------+ Some examples that work are: >>> from hl7apy.base_datatypes import TM >>> timestamp_factory(\"12\", TM) #doctest: +ELLIPSIS <hl7apy.base_datatypes.TM object at 0x...> >>> timestamp_factory(\"12+0300\", TM) #doctest: +ELLIPSIS <hl7apy.base_datatypes.TM object at 0x...> >>> timestamp_factory(\"1204\", TM) #doctest: +ELLIPSIS <hl7apy.base_datatypes.TM object at 0x...> >>> timestamp_factory(\"120434\", TM) #doctest: +ELLIPSIS <hl7apy.base_datatypes.TM object at 0x...> >>> timestamp_factory(\"120434-0400\", TM) #doctest: +ELLIPSIS <hl7apy.base_datatypes.TM object at 0x...> If the value does not match one of the valid format it raises :exc:ValueError` :type value: ``str`` :param value: the value to assign the date object :type datatype_cls: `class` :param value: the :class:`TM <hl7apy.base_datatypes.TM>` class to use. It has to be one implementation of the different version modules :type validation_level: ``int`` :param validation_level: It must be a value from class :attr:`validation_level` :class:`VALIDATION_LEVEL hl7apy.consts.VALIDATION_LEVEL` or ``None`` to use the default value :rtype: :class:`TM <hl7apy.base_datatypes.TM>`", "response": "def timestamp_factory(value, datatype_cls, validation_level=None):\n    \"\"\"\n    Creates a :class:`TM <hl7apy.base_datatypes.TM>` object\n\n    The value in input must be a string parsable with :meth:`datetime.strptime`.\n    It can also have an offset part specified with the format +/-HHMM.\n    The offset can be added with all the allowed format\n    The date format is chosen according to the length of the value as stated in this table:\n\n    +-------+-----------------+\n    |Length |Format           |\n    +=======+=================+\n    |2      |``%H``           |\n    +-------+-----------------+\n    |4      |``%H%M``         |\n    +-------+-----------------+\n    |6      |``%H%M%S``       |\n    +-------+-----------------+\n    |10-13  |``%H%M%S.%f``    |\n    +-------+-----------------+\n\n    Some examples that work are:\n\n\n    >>> from hl7apy.base_datatypes import TM\n    >>> timestamp_factory(\"12\", TM) #doctest: +ELLIPSIS\n    <hl7apy.base_datatypes.TM object at 0x...>\n    >>> timestamp_factory(\"12+0300\", TM) #doctest: +ELLIPSIS\n    <hl7apy.base_datatypes.TM object at 0x...>\n    >>> timestamp_factory(\"1204\", TM) #doctest: +ELLIPSIS\n    <hl7apy.base_datatypes.TM object at 0x...>\n    >>> timestamp_factory(\"120434\", TM) #doctest: +ELLIPSIS\n    <hl7apy.base_datatypes.TM object at 0x...>\n    >>> timestamp_factory(\"120434-0400\", TM) #doctest: +ELLIPSIS\n    <hl7apy.base_datatypes.TM object at 0x...>\n\n    If the value does not match one of the valid format it raises :exc:ValueError`\n\n    :type value: ``str``\n    :param value: the value to assign the date object\n\n    :type datatype_cls: `class`\n    :param value: the :class:`TM <hl7apy.base_datatypes.TM>` class to use. It has to be one implementation\n        of the different version modules\n\n    :type validation_level: ``int``\n    :param validation_level: It must be a value from class :attr:`validation_level`\n        :class:`VALIDATION_LEVEL hl7apy.consts.VALIDATION_LEVEL` or ``None`` to use the default value\n\n    :rtype: :class:`TM <hl7apy.base_datatypes.TM>`\n    \"\"\"\n\n    dt_value, fmt, offset, microsec = get_timestamp_info(value)\n    return datatype_cls(dt_value, fmt, offset, microsec)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef datetime_factory(value, datatype_cls, validation_level=None):\n\n    dt_value, fmt, offset, microsec = get_datetime_info(value)\n    return datatype_cls(dt_value, fmt, offset, microsec)", "response": "Returns a new object of the given datatype_cls."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef numeric_factory(value, datatype_cls, validation_level=None):\n    if not value:\n        return datatype_cls(validation_level=validation_level)\n    try:\n        return datatype_cls(Decimal(value), validation_level=validation_level)\n    except InvalidOperation:\n        raise ValueError('{0} is not an HL7 valid NM value'.format(value))", "response": "Creates a numeric object from the value in input."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sequence_id_factory(value, datatype_cls, validation_level=None):\n    if not value:\n        return datatype_cls(validation_level=validation_level)\n    try:\n        return datatype_cls(int(value), validation_level=validation_level)\n    except ValueError:\n        raise ValueError('{0} is not an HL7 valid SI value'.format(value))", "response": "Creates a new object of the specified type"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget current app from Flast stack to use.", "response": "def get_app(self):\n        \"\"\"Get current app from Flast stack to use.\n\n        This will allow to ensure which Redis connection to be used when\n        accessing Redis connection public methods via plugin.\n        \"\"\"\n        # First see to connection stack\n        ctx = connection_stack.top\n        if ctx is not None:\n            return ctx.app\n\n        # Next return app from instance cache\n        if self.app is not None:\n            return self.app\n\n        # Something went wrong, in most cases app just not instantiated yet\n        # and we cannot locate it\n        raise RuntimeError(\n            'Flask application not registered on Redis instance '\n            'and no applcation bound to current context')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_app(self, app, config_prefix=None):\n        # Put redis to application extensions\n        if 'redis' not in app.extensions:\n            app.extensions['redis'] = {}\n\n        # Which config prefix to use, custom or default one?\n        self.config_prefix = config_prefix = config_prefix or 'REDIS'\n\n        # No way to do registration two times\n        if config_prefix in app.extensions['redis']:\n            raise ValueError('Already registered config prefix {0!r}.'.\n                             format(config_prefix))\n\n        # Start reading configuration, define converters to use and key func\n        # to prepend config prefix to key value\n        converters = {'port': int}\n        convert = lambda arg, value: (converters[arg](value)\n                                      if arg in converters\n                                      else value)\n        key = lambda param: '{0}_{1}'.format(config_prefix, param)\n\n        # Which redis connection class to use?\n        klass = app.config.get(key('CLASS'), RedisClass)\n\n        # Import connection class if it stil path notation\n        if isinstance(klass, string_types):\n            klass = import_string(klass)\n\n        # Should we use URL configuration\n        url = app.config.get(key('URL'))\n\n        # If should, parse URL and store values to application config to later\n        # reuse if necessary\n        if url:\n            urlparse.uses_netloc.append('redis')\n            url = urlparse.urlparse(url)\n\n            # URL could contains host, port, user, password and db values\n            app.config[key('HOST')] = url.hostname\n            app.config[key('PORT')] = url.port or 6379\n            app.config[key('USER')] = url.username\n            app.config[key('PASSWORD')] = url.password\n            db = url.path.replace('/', '')\n            app.config[key('DB')] = db if db.isdigit() else None\n\n        # Host is not a mandatory key if you want to use connection pool. But\n        # when present and starts with file:// or / use it as unix socket path\n        host = app.config.get(key('HOST'))\n        if host and (host.startswith('file://') or host.startswith('/')):\n            app.config.pop(key('HOST'))\n            app.config[key('UNIX_SOCKET_PATH')] = host\n\n        args = self._build_connection_args(klass)\n        kwargs = dict([(arg, convert(arg, app.config[key(arg.upper())]))\n                       for arg in args\n                       if key(arg.upper()) in app.config])\n\n        # Initialize connection and store it to extensions\n        connection = klass(**kwargs)\n        app.extensions['redis'][config_prefix] = connection\n\n        # Include public methods to current instance\n        self._include_public_methods(connection)", "response": "Initialize the current redis connection and copy all public connection methods to current one."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_connection_args(self, klass):\n        bases = [base for base in klass.__bases__ if base is not object]\n        all_args = []\n        for cls in [klass] + bases:\n            try:\n                args = inspect.getfullargspec(cls.__init__).args\n            except AttributeError:\n                args = inspect.getargspec(cls.__init__).args\n            for arg in args:\n                if arg in all_args:\n                    continue\n                all_args.append(arg)\n        all_args.remove('self')\n        return all_args", "response": "Build the list of connection args spec exclude self from list of possible\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _include_public_methods(self, connection):\n        for attr in dir(connection):\n            value = getattr(connection, attr)\n            if attr.startswith('_') or not callable(value):\n                continue\n            self.__dict__[attr] = self._wrap_public_method(attr)", "response": "Include public methods from current instance to current instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps a public method to be called when accessing the current connection.", "response": "def _wrap_public_method(self, attr):\n        \"\"\"\n        Ensure that plugin will call current connection method when accessing\n        as ``plugin.<public_method>(*args, **kwargs)``.\n        \"\"\"\n        def wrapper(*args, **kwargs):\n            return getattr(self.connection, attr)(*args, **kwargs)\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare(self):\n        '''Prepare to run the docker command'''\n        self.__make_scubadir()\n\n        if self.is_remote_docker:\n            '''\n            Docker is running remotely (e.g. boot2docker on OSX).\n            We don't need to do any user setup whatsoever.\n\n            TODO: For now, remote instances won't have any .scubainit\n\n            See:\n            https://github.com/JonathonReinhart/scuba/issues/17\n            '''\n            raise ScubaError('Remote docker not supported (DOCKER_HOST is set)')\n\n        # Docker is running natively\n        self.__setup_native_run()\n\n        # Apply environment vars from .scuba.yml\n        self.env_vars.update(self.context.environment)", "response": "Prepare to run the docker command"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_env(self, name, val):\n        '''Add an environment variable to the docker run invocation\n        '''\n        if name in self.env_vars:\n            raise KeyError(name)\n        self.env_vars[name] = val", "response": "Add an environment variable to the docker run invocation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a volume to the container invocation", "response": "def add_volume(self, hostpath, contpath, options=None):\n        '''Add a volume (bind-mount) to the docker run invocation\n        '''\n        if options is None:\n            options = []\n        self.volumes.append((hostpath, contpath, options))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __locate_scubainit(self):\n        '''Determine path to scubainit binary\n        '''\n        pkg_path = os.path.dirname(__file__)\n\n        self.scubainit_path = os.path.join(pkg_path, 'scubainit')\n        if not os.path.isfile(self.scubainit_path):\n            raise ScubaError('scubainit not found at \"{}\"'.format(self.scubainit_path))", "response": "Determine path to scubainit binary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __load_config(self):\n        '''Find and load .scuba.yml\n        '''\n\n        # top_path is where .scuba.yml is found, and becomes the top of our bind mount.\n        # top_rel is the relative path from top_path to the current working directory,\n        # and is where we'll set the working directory in the container (relative to\n        # the bind mount point).\n        try:\n            top_path, top_rel = find_config()\n            self.config = load_config(os.path.join(top_path, SCUBA_YML))\n        except ConfigNotFoundError as cfgerr:\n            # SCUBA_YML can be missing if --image was given.\n            # In this case, we assume a default config\n            if not self.image_override:\n                raise ScubaError(str(cfgerr))\n            top_path, top_rel = os.getcwd(), ''\n            self.config = ScubaConfig(image=None)\n        except ConfigError as cfgerr:\n            raise ScubaError(str(cfgerr))\n\n        # Mount scuba root directory at the same path in the container...\n        self.add_volume(top_path, top_path)\n\n        # ...and set the working dir relative to it\n        self.set_workdir(os.path.join(top_path, top_rel))\n\n        self.add_env('SCUBA_ROOT', top_path)", "response": "Find and load. scuba. yml and set the appropriate environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake temp directory where all ancillary files are bind - mounted", "response": "def __make_scubadir(self):\n        '''Make temp directory where all ancillary files are bind-mounted\n        '''\n        self.__scubadir_hostpath = tempfile.mkdtemp(prefix='scubadir')\n        self.__scubadir_contpath = '/.scuba'\n        self.add_volume(self.__scubadir_hostpath, self.__scubadir_contpath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef open_scubadir_file(self, name, mode):\n        '''Opens a file in the 'scubadir'\n\n        This file will automatically be bind-mounted into the container,\n        at a path given by the 'container_path' property on the returned file object.\n        '''\n        path = os.path.join(self.__scubadir_hostpath, name)\n        assert not os.path.exists(path)\n\n        # Make any directories required\n        mkdir_p(os.path.dirname(path))\n\n        f = File(path, mode)\n        f.container_path = os.path.join(self.__scubadir_contpath, name)\n        return f", "response": "Opens a file in the scubadir name with the given mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy_scubadir_file(self, name, source):\n        '''Copies source into the scubadir\n\n        Returns the container-path of the copied file\n        '''\n        dest = os.path.join(self.__scubadir_hostpath, name)\n        assert not os.path.exists(dest)\n        shutil.copy2(source, dest)\n\n        return os.path.join(self.__scubadir_contpath, name)", "response": "Copies source into the scubadir\n        Returns the container - path of the copied file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting args into a shell - quoted command line.", "response": "def format_cmdline(args, maxwidth=80):\n    '''Format args into a shell-quoted command line.\n\n    The result will be wrapped to maxwidth characters where possible,\n    not breaking a single long argument.\n    '''\n\n    # Leave room for the space and backslash at the end of each line\n    maxwidth -= 2\n\n    def lines():\n        line = ''\n        for a in (shell_quote(a) for a in args):\n            # If adding this argument will make the line too long,\n            # yield the current line, and start a new one.\n            if len(line) + len(a) + 1 > maxwidth:\n                yield line\n                line = ''\n\n            # Append this argument to the current line, separating\n            # it by a space from the existing arguments.\n            if line:\n                line += ' ' + a\n            else:\n                line = a\n\n        yield line\n\n    return ' \\\\\\n'.join(lines())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses an environment variable string returning a key - value tuple", "response": "def parse_env_var(s):\n    \"\"\"Parse an environment variable string\n\n    Returns a key-value tuple\n\n    Apply the same logic as `docker run -e`:\n    \"If the operator names an environment variable without specifying a value,\n    then the current value of the named variable is propagated into the\n    container's environment\n    \"\"\"\n    parts = s.split('=', 1)\n    if len(parts) == 2:\n        k, v = parts\n        return (k, v)\n\n    k = parts[0]\n    return (k, os.getenv(k, ''))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __wrap_docker_exec(func):\n    '''Wrap a function to raise DockerExecuteError on ENOENT'''\n    def call(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                raise DockerExecuteError('Failed to execute docker. Is it installed?')\n            raise\n    return call", "response": "Wrap a function to raise DockerExecuteError on ENOENT"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninspecting a docker image Returns a Parsed JSON data", "response": "def docker_inspect(image):\n    '''Inspects a docker image\n\n    Returns: Parsed JSON data\n    '''\n    args = ['docker', 'inspect', '--type', 'image', image]\n    p = Popen(args, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n\n    stdout, stderr = p.communicate()\n    stdout = stdout.decode('utf-8')\n    stderr = stderr.decode('utf-8')\n\n    if not p.returncode == 0:\n        if 'no such image' in stderr.lower():\n            raise NoSuchImageError(image)\n        raise DockerError('Failed to inspect image: {}'.format(stderr.strip()))\n\n    return json.loads(stdout)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_image_command(image):\n    '''Gets the default command for an image'''\n    info = docker_inspect_or_pull(image)\n    try:\n        return info['Config']['Cmd']\n    except KeyError as ke:\n        raise DockerError('Failed to inspect image: JSON result missing key {}'.format(ke))", "response": "Gets the default command for an image"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the image entrypoint", "response": "def get_image_entrypoint(image):\n    '''Gets the image entrypoint'''\n    info = docker_inspect_or_pull(image)\n    try:\n        return info['Config']['Entrypoint']\n    except KeyError as ke:\n        raise DockerError('Failed to inspect image: JSON result missing key {}'.format(ke))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a docker volume option", "response": "def make_vol_opt(hostdir, contdir, options=None):\n    '''Generate a docker volume option'''\n    vol = '--volume={}:{}'.format(hostdir, contdir)\n    if options != None:\n        if isinstance(options, str):\n            options = (options,)\n        vol += ':' + ','.join(options)\n    return vol"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches up the diretcory hierarchy for. scuba. yml and return the absolute path and relative path.", "response": "def find_config():\n    '''Search up the diretcory hierarchy for .scuba.yml\n\n    Returns: path, rel on success, or None if not found\n        path    The absolute path of the directory where .scuba.yml was found\n        rel     The relative path from the directory where .scuba.yml was found\n                to the current directory\n    '''\n    cross_fs = 'SCUBA_DISCOVERY_ACROSS_FILESYSTEM' in os.environ\n    path = os.getcwd()\n\n    rel = ''\n    while True:\n        if os.path.exists(os.path.join(path, SCUBA_YML)):\n            return path, rel\n\n        if not cross_fs and os.path.ismount(path):\n            msg = '{} not found here or any parent up to mount point {}'.format(SCUBA_YML, path) \\\n                   + '\\nStopping at filesystem boundary (SCUBA_DISCOVERY_ACROSS_FILESYSTEM not set).'\n            raise ConfigNotFoundError(msg)\n\n        # Traverse up directory hierarchy\n        path, rest = os.path.split(path)\n        if not rest:\n            raise ConfigNotFoundError('{} not found here or any parent directories'.format(SCUBA_YML))\n\n        # Accumulate the relative path back to where we started\n        rel = os.path.join(rest, rel)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct a new instance of this object from a YAML file.", "response": "def from_yaml(self, node):\n        '''\n        Implementes a !from_yaml constructor with the following syntax:\n            !from_yaml filename key\n\n        Arguments:\n            filename:   Filename of external YAML document from which to load,\n                        relative to the current YAML file.\n            key:        Key from external YAML document to return,\n                        using a dot-separated syntax for nested keys.\n\n        Examples:\n            !from_yaml external.yml pop\n            !from_yaml external.yml foo.bar.pop\n            !from_yaml \"another file.yml\" \"foo bar.snap crackle.pop\"\n        '''\n\n        # Load the content from the node, as a scalar\n        content = self.construct_scalar(node)\n\n        # Split on unquoted spaces\n        try:\n            parts = shlex.split(content)\n        except UnicodeEncodeError:\n            raise yaml.YAMLError('Non-ASCII arguments to !from_yaml are unsupported')\n\n        if len(parts) != 2:\n            raise yaml.YAMLError('Two arguments expected to !from_yaml')\n        filename, key = parts\n\n        # path is relative to the current YAML document\n        path = os.path.join(self._root, filename)\n\n        # Load the other YAML document\n        with open(path, 'r') as f:\n            doc = yaml.load(f, self.__class__)\n\n        # Retrieve the key\n        try:\n            cur = doc\n            for k in key.split('.'):\n                cur = cur[k]\n        except KeyError:\n            raise yaml.YAMLError('Key \"{}\" not found in {}'.format(key, filename))\n        return cur"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_command(self, command):\n        '''Processes a user command using aliases\n\n        Arguments:\n            command     A user command list (e.g. argv)\n\n        Returns: A ScubaContext object with the following attributes:\n            script: a list of command line strings\n            image: the docker image name to use\n        '''\n        result = ScubaContext()\n        result.script = None\n        result.image = self.image\n        result.entrypoint = self.entrypoint\n        result.environment = self.environment.copy()\n\n        if command:\n            alias = self.aliases.get(command[0])\n            if not alias:\n                # Command is not an alias; use it as-is.\n                result.script = [shell_quote_cmd(command)]\n            else:\n                # Using an alias\n                # Does this alias override the image and/or entrypoint?\n                if alias.image:\n                    result.image = alias.image\n                if alias.entrypoint is not None:\n                    result.entrypoint = alias.entrypoint\n\n                # Merge/override the environment\n                if alias.environment:\n                    result.environment.update(alias.environment)\n\n                if len(alias.script) > 1:\n                    # Alias is a multiline script; no additional\n                    # arguments are allowed in the scuba invocation.\n                    if len(command) > 1:\n                        raise ConfigError('Additional arguments not allowed with multi-line aliases')\n                    result.script = alias.script\n\n                else:\n                    # Alias is a single-line script; perform substituion\n                    # and add user arguments.\n                    command.pop(0)\n                    result.script = [alias.script[0] + ' ' + shell_quote_cmd(command)]\n\n            result.script = flatten_list(result.script)\n\n        return result", "response": "Processes a user command using aliases\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open(self, filename):\n        ''' Opens a database file '''\n        # Ensure old file is closed before opening a new one\n        self.close()\n\n        self._f = open(filename, 'rb')\n        self._dbtype = struct.unpack('B', self._f.read(1))[0]\n        self._dbcolumn = struct.unpack('B', self._f.read(1))[0]\n        self._dbyear = struct.unpack('B', self._f.read(1))[0]\n        self._dbmonth = struct.unpack('B', self._f.read(1))[0]\n        self._dbday = struct.unpack('B', self._f.read(1))[0]\n        self._ipv4dbcount = struct.unpack('<I', self._f.read(4))[0]\n        self._ipv4dbaddr = struct.unpack('<I', self._f.read(4))[0]\n        self._ipv6dbcount = struct.unpack('<I', self._f.read(4))[0]\n        self._ipv6dbaddr = struct.unpack('<I', self._f.read(4))[0]\n        self._ipv4indexbaseaddr = struct.unpack('<I', self._f.read(4))[0]\n        self._ipv6indexbaseaddr = struct.unpack('<I', self._f.read(4))[0]", "response": "Opens a database file and returns a new object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the country short name for a given IP", "response": "def get_country_short(self, ip):\n        ''' Get country_short '''\n        rec = self.get_all(ip)\n        return rec and rec.country_short"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_country_long(self, ip):\n        ''' Get country_long '''\n        rec = self.get_all(ip)\n        return rec and rec.country_long", "response": "Get the country long for a given IP"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_region(self, ip):\n        ''' Get region '''\n        rec = self.get_all(ip)\n        return rec and rec.region", "response": "Get the region of the given IP"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_city(self, ip):\n        ''' Get city '''\n        rec = self.get_all(ip)\n        return rec and rec.city", "response": "Get city for ip"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the ISP for a given IP", "response": "def get_isp(self, ip):\n        ''' Get isp '''\n        rec = self.get_all(ip)\n        return rec and rec.isp"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_latitude(self, ip):\n        ''' Get latitude '''\n        rec = self.get_all(ip)\n        return rec and rec.latitude", "response": "Get latitude of a specific IP"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the longitude of a given IP", "response": "def get_longitude(self, ip):\n        ''' Get longitude '''\n        rec = self.get_all(ip)\n        return rec and rec.longitude"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_domain(self, ip):\n        ''' Get domain '''\n        rec = self.get_all(ip)\n        return rec and rec.domain", "response": "Get the domain of the given IP"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_zipcode(self, ip):\n        ''' Get zipcode '''\n        rec = self.get_all(ip)\n        return rec and rec.zipcode", "response": "Get the zipcode associated with a given IP"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_timezone(self, ip):\n        ''' Get timezone '''\n        rec = self.get_all(ip)\n        return rec and rec.timezone", "response": "Get the timezone of the given IP"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget netspeed for a given ip", "response": "def get_netspeed(self, ip):\n        ''' Get netspeed '''\n        rec = self.get_all(ip)\n        return rec and rec.netspeed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_idd_code(self, ip):\n        ''' Get idd_code '''\n        rec = self.get_all(ip)\n        return rec and rec.idd_code", "response": "Get idd code for a given ip"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_area_code(self, ip):\n        ''' Get area_code '''\n        rec = self.get_all(ip)\n        return rec and rec.area_code", "response": "Get area code for given ip"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget weather code for a given IP", "response": "def get_weather_code(self, ip):\n        ''' Get weather_code '''\n        rec = self.get_all(ip)\n        return rec and rec.weather_code"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_weather_name(self, ip):\n        ''' Get weather_name '''\n        rec = self.get_all(ip)\n        return rec and rec.weather_name", "response": "Get weather name for a given IP"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget mcc for a given ip", "response": "def get_mcc(self, ip):\n        ''' Get mcc '''\n        rec = self.get_all(ip)\n        return rec and rec.mcc"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_mnc(self, ip):\n        ''' Get mnc '''\n        rec = self.get_all(ip)\n        return rec and rec.mnc", "response": "Get the mnc for a given IP"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mobile_brand(self, ip):\n        ''' Get mobile_brand '''\n        rec = self.get_all(ip)\n        return rec and rec.mobile_brand", "response": "Get mobile_brand from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_elevation(self, ip):\n        ''' Get elevation '''\n        rec = self.get_all(ip)\n        return rec and rec.elevation", "response": "Get the elevation of a specific IP"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_usage_type(self, ip):\n        ''' Get usage_type '''\n        rec = self.get_all(ip)\n        return rec and rec.usage_type", "response": "Get usage type for a given ip"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse an address and returns the IP version. Raises exception on invalid argument.", "response": "def _parse_addr(self, addr): \n        ''' Parses address and returns IP version. Raises exception on invalid argument '''\n        ipv = 0\n        try:\n            socket.inet_pton(socket.AF_INET6, addr)\n            # Convert ::FFFF:x.y.z.y to IPv4\n            if addr.lower().startswith('::ffff:'):\n                try:\n                    socket.inet_pton(socket.AF_INET, addr)\n                    ipv = 4\n                except:\n                    ipv = 6\n            else:\n                ipv = 6\n        except:\n            socket.inet_pton(socket.AF_INET, addr)\n            ipv = 4\n        return ipv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rates_for_location(self, postal_code, location_deets=None):\n        request = self._get(\"rates/\" + postal_code, location_deets)\n        return self.responder(request)", "response": "Shows the sales tax rates for a given location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow the sales tax that should be collected for a given order.", "response": "def tax_for_order(self, order_deets):\n        \"\"\"Shows the sales tax that should be collected for a given order.\"\"\"\n        request = self._post('taxes', order_deets)\n        return self.responder(request)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist existing order transactions.", "response": "def list_orders(self, params=None):\n        \"\"\"Lists existing order transactions.\"\"\"\n        request = self._get('transactions/orders', params)\n        return self.responder(request)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show_order(self, order_id):\n        request = self._get('transactions/orders/' + str(order_id))\n        return self.responder(request)", "response": "Shows an existing order transaction."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new order transaction.", "response": "def create_order(self, order_deets):\n        \"\"\"Creates a new order transaction.\"\"\"\n        request = self._post('transactions/orders', order_deets)\n        return self.responder(request)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating an existing order transaction.", "response": "def update_order(self, order_id, order_deets):\n        \"\"\"Updates an existing order transaction.\"\"\"\n        request = self._put(\"transactions/orders/\" + str(order_id), order_deets)\n        return self.responder(request)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes an existing order transaction.", "response": "def delete_order(self, order_id):\n        \"\"\"Deletes an existing order transaction.\"\"\"\n        request = self._delete(\"transactions/orders/\" + str(order_id))\n        return self.responder(request)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_refunds(self, params=None):\n        request = self._get('transactions/refunds', params)\n        return self.responder(request)", "response": "Lists existing refund transactions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nshows an existing refund transaction.", "response": "def show_refund(self, refund_id):\n        \"\"\"Shows an existing refund transaction.\"\"\"\n        request = self._get('transactions/refunds/' + str(refund_id))\n        return self.responder(request)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_refund(self, refund_deets):\n        request = self._post('transactions/refunds', refund_deets)\n        return self.responder(request)", "response": "Creates a new refund transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates an existing refund transaction.", "response": "def update_refund(self, refund_id, refund_deets):\n        \"\"\"Updates an existing refund transaction.\"\"\"\n        request = self._put('transactions/refunds/' + str(refund_id), refund_deets)\n        return self.responder(request)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes an existing refund transaction.", "response": "def delete_refund(self, refund_id):\n        \"\"\"Deletes an existing refund transaction.\"\"\"\n        request = self._delete('transactions/refunds/' + str(refund_id))\n        return self.responder(request)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing an existing customer.", "response": "def show_customer(self, customer_id):\n        \"\"\"Shows an existing customer.\"\"\"\n        request = self._get('customers/' + str(customer_id))\n        return self.responder(request)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_customer(self, customer_deets):\n        request = self._post('customers', customer_deets)\n        return self.responder(request)", "response": "Creates a new customer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_customer(self, customer_id, customer_deets):\n        request = self._put(\"customers/\" + str(customer_id), customer_deets)\n        return self.responder(request)", "response": "Updates an existing customer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_customer(self, customer_id):\n        request = self._delete(\"customers/\" + str(customer_id))\n        return self.responder(request)", "response": "Deletes an existing customer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_address(self, address_deets):\n        request = self._post('addresses/validate', address_deets)\n        return self.responder(request)", "response": "Validates a customer address and returns back a collection of addresses matches."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self, vat_deets):\n        request = self._get('validation', vat_deets)\n        return self.responder(request)", "response": "Validates an existing VAT identification number against VIES."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_score(self, terms):\n        assert isinstance(terms, list) or isinstance(terms, tuple)\n        score_li = np.asarray([self._get_score(t) for t in terms])\n        \n        s_pos = np.sum(score_li[score_li > 0])\n        s_neg = -np.sum(score_li[score_li < 0])\n        \n        s_pol = (s_pos-s_neg) * 1.0 / ((s_pos+s_neg)+self.EPSILON)\n        s_sub = (s_pos+s_neg) * 1.0 / (len(score_li)+self.EPSILON)\n        \n        return {self.TAG_POS: s_pos,\n                self.TAG_NEG: s_neg,\n                self.TAG_POL: s_pol,\n                self.TAG_SUB: s_sub}", "response": "Get score for a list of terms."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchoose proper form for plural.", "response": "def choose_plural(amount, variants):\n    \"\"\"\n    Choose proper form for plural.\n\n    Value is a amount, parameters are forms of noun.\n    Forms are variants for 1, 2, 5 nouns. It may be tuple\n    of elements, or string where variants separates each other\n    by comma.\n\n    Examples::\n        {{ some_int|choose_plural:\"\u043f\u0440\u0438\u043c\u0435\u0440,\u043f\u0440\u0438\u043c\u0435\u0440\u0430,\u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432\" }}\n    \"\"\"\n    try:\n        if isinstance(variants, six.string_types):\n            uvariants = smart_text(variants, encoding)\n        else:\n            uvariants = [smart_text(v, encoding) for v in variants]\n        res = numeral.choose_plural(amount, uvariants)\n    except Exception as err:\n        # because filter must die silently\n        try:\n            default_variant = variants\n        except Exception:\n            default_variant = \"\"\n        res = default_value % {'error': err, 'value': default_variant}\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rubles(amount, zero_for_kopeck=False):\n    try:\n        res = numeral.rubles(amount, zero_for_kopeck)\n    except Exception as err:\n        # because filter must die silently\n        res = default_value % {'error': err, 'value': str(amount)}\n    return res", "response": "Converts float value to in - words representation ( for money"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sum_string(amount, gender, items):\n    try:\n        if isinstance(items, six.string_types):\n            uitems = smart_text(items, encoding, default_uvalue)\n        else:\n            uitems = [smart_text(i, encoding) for i in items]\n        res = numeral.sum_string(amount, getattr(numeral, str(gender), None), uitems)\n    except Exception as err:\n        # because tag's renderer must die silently\n        res = default_value % {'error': err, 'value': str(amount)}\n    return res", "response": "Sums the string of a single object in a single flask\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying re. sub to text.", "response": "def _sub_patterns(patterns, text):\n    \"\"\"\n    Apply re.sub to bunch of (pattern, repl)\n    \"\"\"\n    for pattern, repl in patterns:\n        text = re.sub(pattern, repl, text)\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rl_cleanspaces(x):\n    patterns = (\n        # arguments for re.sub: pattern and repl\n        # \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u043f\u0440\u043e\u0431\u0435\u043b \u043f\u0435\u0440\u0435\u0434 \u0437\u043d\u0430\u043a\u0430\u043c\u0438 \u043f\u0440\u0435\u043f\u0438\u043d\u0430\u043d\u0438\u044f\n        (r' +([\\.,?!\\)]+)', r'\\1'),\n        # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043f\u0440\u043e\u0431\u0435\u043b \u043f\u043e\u0441\u043b\u0435 \u0437\u043d\u0430\u043a\u0430 \u043f\u0440\u0435\u043f\u0438\u043d\u0430\u043d\u0438\u044f, \u0435\u0441\u043b\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u0437\u0430 \u043d\u0438\u043c \u043d\u0435\u0442 \u0434\u0440\u0443\u0433\u043e\u0433\u043e\n        (r'([\\.,?!\\)]+)([^\\.!,?\\)]+)', r'\\1 \\2'),\n        # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0440\u043e\u0431\u0435\u043b \u043f\u043e\u0441\u043b\u0435 \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u044e\u0449\u0435\u0439 \u0441\u043a\u043e\u0431\u043a\u0438\n        (r'(\\S+)\\s*(\\()\\s*(\\S+)', r'\\1 (\\3'),\n    )\n    # \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u0434\u0432\u043e\u0439\u043d\u044b\u0435, \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0438 \u043a\u043e\u043d\u0435\u0447\u043d\u044b\u0435 \u043f\u0440\u043e\u0431\u0435\u043b\u044b\n    return os.linesep.join(\n        ' '.join(part for part in line.split(' ') if part)\n        for line in _sub_patterns(patterns, x).split(os.linesep)\n    )", "response": "Clean double spaces trailing spaces heading spaces and punctuations from a single string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a sequence of ellipsis patterns.", "response": "def rl_ellipsis(x):\n    \"\"\"\n    Replace three dots to ellipsis\n    \"\"\"\n\n    patterns = (\n        # \u0435\u0441\u043b\u0438 \u0431\u043e\u043b\u044c\u0448\u0435 \u0442\u0440\u0435\u0445 \u0442\u043e\u0447\u0435\u043a, \u0442\u043e \u043d\u0435 \u0437\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u043d\u0430 \u0442\u0440\u043e\u0435\u0442\u043e\u0447\u0438\u0435\n        # \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u0431\u044b\u043b\u043e \u0433\u043b\u0443\u043f\u044b\u0445 .....->\u2026..\n        (r'([^\\.]|^)\\.\\.\\.([^\\.]|$)', u'\\\\1\\u2026\\\\2'),\n        # \u0435\u0441\u043b\u0438 \u0442\u0440\u043e\u0435\u0442\u043e\u0447\u0438\u0435 \u0432 \u043d\u0430\u0447\u0430\u043b\u0435 \u0441\u0442\u0440\u043e\u043a\u0438 \u0438\u043b\u0438 \u0432\u043e\u0437\u043b\u0435 \u043a\u0430\u0432\u044b\u0447\u043a\u0438 --\n        # \u044d\u0442\u043e \u0446\u0438\u0442\u0430\u0442\u0430, \u043f\u0440\u043e\u0431\u0435\u043b \u043c\u0435\u0436\u0434\u0443 \u0442\u0440\u043e\u0435\u0442\u043e\u0447\u0438\u0435\u043c \u0438 \u043f\u0435\u0440\u0432\u044b\u043c\n        # \u0441\u043b\u043e\u0432\u043e\u043c \u043d\u0443\u0436\u043d\u043e \u0443\u0431\u0440\u0430\u0442\u044c\n        (re.compile(u'(^|\\\\\"|\\u201c|\\xab)\\\\s*\\u2026\\\\s*([\u0410-\u042f\u0430-\u044fA-Za-z])', re.UNICODE), u'\\\\1\\u2026\\\\2'),\n        \n    )\n    return _sub_patterns(patterns, x)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rl_initials(x):\n    return re.sub(\n        re.compile(u'([\u0410-\u042f])\\\\.\\\\s*([\u0410-\u042f])\\\\.\\\\s*([\u0410-\u042f][\u0430-\u044f]+)', re.UNICODE),\n        u'\\\\1.\\\\2.\\u2009\\\\3',\n        x\n    )", "response": "Return a new string that can be used to create a new resource."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreplace dash to long dashes", "response": "def rl_dashes(x):\n    \"\"\"\n    Replace dash to long/medium dashes\n    \"\"\"\n    patterns = (\n        # \u0442\u0438\u0440\u0435\n        (re.compile(u'(^|(.\\\\s))\\\\-\\\\-?(([\\\\s\\u202f].)|$)', re.MULTILINE|re.UNICODE), u'\\\\1\\u2014\\\\3'),\n        # \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b \u043c\u0435\u0436\u0434\u0443 \u0446\u0438\u0444\u0440\u0430\u043c\u0438 - en dash\n        (re.compile(u'(\\\\d[\\\\s\\u2009]*)\\\\-([\\\\s\\u2009]*\\d)', re.MULTILINE|re.UNICODE), u'\\\\1\\u2013\\\\2'),\n        # TODO: \u0430 \u0447\u0442\u043e \u0441 \u043c\u0438\u043d\u0443\u0441\u043e\u043c?\n    )\n    return _sub_patterns(patterns, x)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rl_wordglue(x):\n    patterns = (\n        # \u0447\u0430\u0441\u0442\u0438\u0446\u044b \u0441\u043a\u043b\u0435\u0438\u0432\u0430\u0435\u043c \u0441 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u043c \u0441\u043b\u043e\u0432\u043e\u043c\n        (re.compile(u'(\\\\s+)(\u0436\u0435|\u043b\u0438|\u043b\u044c|\u0431\u044b|\u0431|\u0436|\u043a\u0430)([\\\\.,!\\\\?:;]?\\\\s+)', re.UNICODE), u'\\u202f\\\\2\\\\3'),\n        # \u0441\u043a\u043b\u0435\u0438\u0432\u0430\u0435\u043c \u043a\u043e\u0440\u043e\u0442\u043a\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u0441\u043b\u043e\u0432\u043e\u043c\n        (re.compile(u'\\\\b([a-zA-Z\u0410-\u042f\u0430-\u044f]{1,3})(\\\\s+)', re.UNICODE), u'\\\\1\\u202f'),\n        # \u0441\u043a\u043b\u0435\u0438\u0432\u0430\u0435\u043c \u0442\u0438\u0440\u0435 \u0441 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u043c \u0441\u043b\u043e\u0432\u043e\u043c\n        (re.compile(u'(\\\\s+)([\\u2014\\\\-]+)(\\\\s+)', re.UNICODE), u'\\u202f\\\\2\\\\3'),\n        # \u0441\u043a\u043b\u0435\u0438\u0432\u0430\u0435\u043c \u0434\u0432\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0445 \u0441\u043b\u043e\u0432\u0430 \u0432 \u0430\u0431\u0437\u0430\u0446\u0435 \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0431\u043e\u0439\n        # \u043f\u043e\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0430\u0431\u0437\u0430\u0446\u044b \u0431\u0443\u0434\u0443\u0442 \u043f\u0435\u0440\u0435\u0434\u0430\u0432\u0430\u0442\u044c\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u043e\u0439\n        (re.compile(u'([^\\\\s]+)\\\\s+([^\\\\s]+)$', re.UNICODE), u'\\\\1\\u202f\\\\2'),\n    )\n    return _sub_patterns(patterns, x)", "response": "Return a list of short words with word before and after words."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new sequence of unicode strings that can be used as a mark in a URL.", "response": "def rl_marks(x):\n    \"\"\"\n    Replace +-, (c), (tm), (r), (p), etc by its typographic eqivalents\n    \"\"\"\n    # \u043f\u0440\u043e\u0441\u0442\u044b\u0435 \u0437\u0430\u043c\u0435\u043d\u044b, \u043c\u043e\u0436\u043d\u043e \u0431\u0435\u0437 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043e\u043a\n    replacements = (\n        (u'(r)', u'\\u00ae'), # \u00ae\n        (u'(R)', u'\\u00ae'), # \u00ae\n        (u'(p)', u'\\u00a7'), # \u00a7\n        (u'(P)', u'\\u00a7'), # \u00a7\n        (u'(tm)', u'\\u2122'), # \u2122\n        (u'(TM)', u'\\u2122'), # \u2122\n    )\n    patterns = (\n        # \u043a\u043e\u043f\u0438\u0440\u0430\u0439\u0442 \u0441\u0442\u0430\u0432\u0438\u0442\u0441\u044f \u0434\u043e \u0433\u043e\u0434\u0430: \u00a9 2008 \u042e\u0440\u0438\u0439 \u042e\u0440\u0435\u0432\u0438\u0447\n        (re.compile(u'\\\\([cC\u0441\u0421]\\\\)\\\\s*(\\\\d+)', re.UNICODE), u'\\u00a9\\u202f\\\\1'),\n        (r'([^+])(\\+\\-|\\-\\+)', u'\\\\1\\u00b1'), # \u00b1\n        # \u0433\u0440\u0430\u0434\u0443\u0441\u044b \u0441 \u043c\u0438\u043d\u0443\u0441\u043e\u043c\n        (u'\\\\-(\\\\d+)[\\\\s]*([FC\u0421][^\\\\w])', u'\\u2212\\\\1\\202f\\u00b0\\\\2'), # \u221212 \u00b0C, \u221253 \u00b0F\n        # \u0433\u0440\u0430\u0434\u0443\u0441\u044b \u0431\u0435\u0437 \u043c\u0438\u043d\u0443\u0441\u0430\n        (u'(\\\\d+)[\\\\s]*([FC\u0421][^\\\\w])', u'\\\\1\\u202f\\u00b0\\\\2'), # 12 \u00b0C, 53 \u00b0F\n        # \u00ae \u0438 \u2122 \u043f\u0440\u0438\u043a\u043b\u0435\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u043a \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c\u0443 \u0441\u043b\u043e\u0432\u0443, \u0431\u0435\u0437 \u043f\u0440\u043e\u0431\u0435\u043b\u0430\n        (re.compile(u'([A-Za-z\u0410-\u042f\u0430-\u044f\\\\!\\\\?])\\\\s*(\\xae|\\u2122)', re.UNICODE), u'\\\\1\\\\2'),\n        # No5 -> \u2116 5\n        (re.compile(u'(\\\\s)(No|no|NO|\\u2116)[\\\\s\\u2009]*(\\\\d+)', re.UNICODE), u'\\\\1\\u2116\\u2009\\\\3'),\n    )\n\n    for what, to in replacements:\n        x = x.replace(what, to)\n    return _sub_patterns(patterns, x)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreplace quotes by typographic quotes", "response": "def rl_quotes(x):\n    \"\"\"\n    Replace quotes by typographic quotes\n    \"\"\"\n    \n    patterns = (\n        # \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u044e\u0449\u0438\u0435 \u043a\u0430\u0432\u044b\u0447\u043a\u0438 \u0441\u0442\u0430\u0432\u044f\u0442\u0441\u044f \u043e\u0431\u044b\u0447\u043d\u043e \u0432\u043f\u043b\u043e\u0442\u043d\u0443\u044e \u043a \u0441\u043b\u043e\u0432\u0443 \u0441\u043b\u0435\u0432\u0430\n        # \u0430 \u0437\u0430\u043a\u0440\u044b\u0432\u0430\u044e\u0449\u0438\u0435 -- \u0432\u043f\u043b\u043e\u0442\u043d\u0443\u044e \u0441\u043f\u0440\u0430\u0432\u0430\n        # \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u044e\u0449\u0438\u0435 \u0440\u0443\u0441\u0441\u043a\u0438\u0435 \u043a\u0430\u0432\u044b\u0447\u043a\u0438-\u0451\u043b\u043e\u0447\u043a\u0438\n        (re.compile(r'((?:^|\\s))(\")((?u))', re.UNICODE), u'\\\\1\\xab\\\\3'),\n        # \u0437\u0430\u043a\u0440\u044b\u0432\u0430\u044e\u0449\u0438\u0435 \u0440\u0443\u0441\u0441\u043a\u0438\u0435 \u043a\u0430\u0432\u044b\u0447\u043a\u0438-\u0451\u043b\u043e\u0447\u043a\u0438\n        (re.compile(r'(\\S)(\")((?u))', re.UNICODE), u'\\\\1\\xbb\\\\3'),\n        # \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u044e\u0449\u0438\u0435 \u043a\u0430\u0432\u044b\u0447\u043a\u0438-\u043b\u0430\u043f\u043a\u0438, \u0432\u043c\u0435\u0441\u0442\u043e \u043e\u0434\u0438\u043d\u0430\u0440\u043d\u044b\u0445 \u043a\u0430\u0432\u044b\u0447\u0435\u043a\n        (re.compile(r'((?:^|\\s))(\\')((?u))', re.UNICODE), u'\\\\1\\u201c\\\\3'),\n        # \u0437\u0430\u043a\u0440\u044b\u0432\u0430\u044e\u0449\u0438\u0435 \u043a\u0430\u0432\u044b\u0447\u043a\u0438-\u043b\u0430\u043f\u043a\u0438\n\t(re.compile(r'(\\S)(\\')((?u))', re.UNICODE), u'\\\\1\\u201d\\\\3'),\n    )\n    return _sub_patterns(patterns, x)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef distance_of_time(from_time, accuracy=1):\n    try:\n        to_time = None\n        if conf.settings.USE_TZ:\n            to_time=utils.timezone.now()\n        res = dt.distance_of_time_in_words(from_time, accuracy, to_time)\n    except Exception as err:\n        # because filter must die silently\n        try:\n            default_distance = \"%s seconds\" % str(int(time.time() - from_time))\n        except Exception:\n            default_distance = \"\"\n        res = default_value % {'error': err, 'value': default_distance}\n    return res", "response": "Display distance of time from current time."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ru_strftime(date, format=\"%d.%m.%Y\", inflected_day=False, preposition=False):\n    try:\n        res = dt.ru_strftime(format,\n                             date,\n                             inflected=True,\n                             inflected_day=inflected_day,\n                             preposition=preposition)\n    except Exception as err:\n        # because filter must die silently\n        try:\n            default_date = date.strftime(format)\n        except Exception:\n            default_date = str(date)\n        res = default_value % {'error': err, 'value': default_date}\n    return res", "response": "Russian strftime - returns a date in a random format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the distance of time in words in a base class.", "response": "def distance_of_time_in_words(from_time, accuracy=1, to_time=None):\n    \"\"\"\n    Represents distance of time in words\n\n    @param from_time: source time (in seconds from epoch)\n    @type from_time: C{int}, C{float} or C{datetime.datetime}\n\n    @param accuracy: level of accuracy (1..3), default=1\n    @type accuracy: C{int}\n\n    @param to_time: target time (in seconds from epoch),\n        default=None translates to current time\n    @type to_time: C{int}, C{float} or C{datetime.datetime}\n\n    @return: distance of time in words\n    @rtype: unicode\n\n    @raise ValueError: accuracy is lesser or equal zero\n    \"\"\"\n    current = False\n\n    if to_time is None:\n        current = True\n        to_time = datetime.datetime.now()\n\n    check_positive(accuracy, strict=True)\n\n    if not isinstance(from_time, datetime.datetime):\n        from_time = datetime.datetime.fromtimestamp(from_time)\n\n    if not isinstance(to_time, datetime.datetime):\n        to_time = datetime.datetime.fromtimestamp(to_time)\n\n    if from_time.tzinfo and not to_time.tzinfo:\n        to_time = to_time.replace(tzinfo=from_time.tzinfo)\n\n    dt_delta = to_time - from_time\n    difference = dt_delta.days*86400 + dt_delta.seconds\n\n    minutes_orig = int(abs(difference)/60.0)\n    hours_orig = int(abs(difference)/3600.0)\n    days_orig = int(abs(difference)/86400.0)\n    in_future = from_time > to_time\n\n    words = []\n    values = []\n    alternatives = []\n\n    days = days_orig\n    hours = hours_orig - days_orig*24\n\n    words.append(u\"%d %s\" % (days, numeral.choose_plural(days, DAY_VARIANTS)))\n    values.append(days)\n\n    words.append(u\"%d %s\" %\n                 (hours, numeral.choose_plural(hours, HOUR_VARIANTS)))\n    values.append(hours)\n\n    days == 0 and hours == 1 and current and alternatives.append(u\"\u0447\u0430\u0441\")\n\n    minutes = minutes_orig - hours_orig*60\n\n    words.append(u\"%d %s\" % (minutes,\n                             numeral.choose_plural(minutes, MINUTE_VARIANTS)))\n    values.append(minutes)\n\n    days == 0 and hours == 0 and minutes == 1 and current and \\\n        alternatives.append(u\"\u043c\u0438\u043d\u0443\u0442\u0443\")\n\n    # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0438\u0437 values \u0438 words \u043a\u043e\u043d\u0435\u0447\u043d\u044b\u0435 \u043d\u0443\u043b\u0438\n    while values and not values[-1]:\n        values.pop()\n        words.pop()\n    # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0438\u0437 values \u0438 words \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0443\u043b\u0438\n    while values and not values[0]:\n        values.pop(0)\n        words.pop(0)\n    limit = min(accuracy, len(words))\n    real_words = words[:limit]\n    real_values = values[:limit]\n    # \u0441\u043d\u043e\u0432\u0430 \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u043a\u043e\u043d\u0435\u0447\u043d\u044b\u0435 \u043d\u0443\u043b\u0438\n    while real_values and not real_values[-1]:\n        real_values.pop()\n        real_words.pop()\n        limit -= 1\n\n    real_str = u\" \".join(real_words)\n\n    # \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u043d\u0443\u0436\u043d\u044b \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u0432 real_words \u043e\u0434\u043d\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n    # \u0438, \u0432\u0434\u043e\u0431\u0430\u0432\u043e\u043a, \u0435\u0441\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0442\u0435\u043a\u0443\u0449\u0435\u0435 \u0432\u0440\u0435\u043c\u044f\n    alter_str = limit == 1 and current and alternatives and \\\n        alternatives[0]\n    _result_str = alter_str or real_str\n    result_str = in_future and u\"%s %s\" % (PREFIX_IN, _result_str) \\\n        or u\"%s %s\" % (_result_str, SUFFIX_AGO)\n\n    # \u0435\u0441\u043b\u0438 \u0436\u0435 \u043f\u0440\u043e\u0448\u043b\u043e \u043c\u0435\u043d\u0435\u0435 \u043c\u0438\u043d\u0443\u0442\u044b, \u0442\u043e real_words -- \u043f\u0443\u0441\u0442\u043e\u0439, \u0438 \u043f\u043e\u044d\u0442\u043e\u043c\u0443\n    # \u043d\u0443\u0436\u043d\u043e \u0431\u0440\u0430\u0442\u044c alternatives[0], \u0430 \u043d\u0435 result_str\n    zero_str = minutes == 0 and not real_words and \\\n        (in_future and u\"\u043c\u0435\u043d\u0435\u0435 \u0447\u0435\u043c \u0447\u0435\u0440\u0435\u0437 \u043c\u0438\u043d\u0443\u0442\u0443\"\n         or u\"\u043c\u0435\u043d\u0435\u0435 \u043c\u0438\u043d\u0443\u0442\u044b \u043d\u0430\u0437\u0430\u0434\")\n\n    # \u043d\u0443\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u0447\u0435\u0440\u0430/\u043f\u043e\u0437\u0430\u0432\u0447\u0435\u0440\u0430/\u0437\u0430\u0432\u0442\u0440\u0430/\u043f\u043e\u0441\u043b\u0435\u0437\u0430\u0432\u0442\u0440\u0430\n    # \u0435\u0441\u043b\u0438 days 1..2 \u0438 \u0432 real_words \u043e\u0434\u043d\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n    day_alternatives = DAY_ALTERNATIVES.get(days, False)\n    alternate_day = day_alternatives and current and limit == 1 and \\\n        ((in_future and day_alternatives[1])\n         or day_alternatives[0])\n\n    final_str = not real_words and zero_str or alternate_day or result_str\n\n    return final_str"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ru_strftime(format=u\"%d.%m.%Y\", date=None, inflected=False,\n                inflected_day=False, preposition=False):\n    \"\"\"\n    Russian strftime without locale\n\n    @param format: strftime format, default=u'%d.%m.%Y'\n    @type format: C{unicode}\n\n    @param date: date value, default=None translates to today\n    @type date: C{datetime.date} or C{datetime.datetime}\n\n    @param inflected: is month inflected, default False\n    @type inflected: C{bool}\n\n    @param inflected_day: is day inflected, default False\n    @type inflected: C{bool}\n\n    @param preposition: is preposition used, default False\n        preposition=True automatically implies inflected_day=True\n    @type preposition: C{bool}\n\n    @return: strftime string\n    @rtype: unicode\n    \"\"\"\n    if date is None:\n        date = datetime.datetime.today()\n\n    weekday = date.weekday()\n\n    prepos = preposition and DAY_NAMES[weekday][3] or u\"\"\n\n    month_idx = inflected and 2 or 1\n    day_idx = (inflected_day or preposition) and 2 or 1\n\n    # for russian typography standard,\n    # 1 April 2007, but 01.04.2007\n    if u'%b' in format or u'%B' in format:\n        format = format.replace(u'%d', six.text_type(date.day))\n\n    format = format.replace(u'%a', prepos+DAY_NAMES[weekday][0])\n    format = format.replace(u'%A', prepos+DAY_NAMES[weekday][day_idx])\n    format = format.replace(u'%b', MONTH_NAMES[date.month-1][0])\n    format = format.replace(u'%B', MONTH_NAMES[date.month-1][month_idx])\n\n    # Python 2: strftime's argument must be str\n    # Python 3: strftime's argument str, not a bitestring\n    if six.PY2:\n        # strftime must be str, so encode it to utf8:\n        s_format = format.encode(\"utf-8\")\n        s_res = date.strftime(s_format)\n        # and back to unicode\n        u_res = s_res.decode(\"utf-8\")\n    else:\n        u_res = date.strftime(format)\n    return u_res", "response": "Russian strftime with locale"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget remainder of float i. e. 2. 05 or Decimal. 0. 05", "response": "def _get_float_remainder(fvalue, signs=9):\n    \"\"\"\n    Get remainder of float, i.e. 2.05 -> '05'\n\n    @param fvalue: input value\n    @type fvalue: C{integer types}, C{float} or C{Decimal}\n\n    @param signs: maximum number of signs\n    @type signs: C{integer types}\n\n    @return: remainder\n    @rtype: C{str}\n\n    @raise ValueError: fvalue is negative\n    @raise ValueError: signs overflow\n    \"\"\"\n    check_positive(fvalue)\n    if isinstance(fvalue, six.integer_types):\n        return \"0\"\n    if isinstance(fvalue, Decimal) and fvalue.as_tuple()[2] == 0:\n        # Decimal.as_tuple() -> (sign, digit_tuple, exponent)\n        # \u0435\u0441\u043b\u0438 \u044d\u043a\u0441\u043f\u043e\u043d\u0435\u043d\u0442\u0430 \"0\" -- \u0437\u043d\u0430\u0447\u0438\u0442 \u0434\u0440\u043e\u0431\u043d\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u043d\u0435\u0442\n        return \"0\"\n\n    signs = min(signs, len(FRACTIONS))\n\n    # \u043d\u0443\u0436\u043d\u043e remainder \u0432 \u0441\u0442\u0440\u043e\u043a\u0435, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0434\u0440\u043e\u0431\u043d\u044b\u0435 X.0Y\n    # \u0431\u0443\u0434\u0443\u0442 \"\u043b\u043e\u043c\u0430\u0442\u044c\u0441\u044f\" \u0434\u043e X.Y\n    remainder = str(fvalue).split('.')[1]\n    iremainder = int(remainder)\n    orig_remainder = remainder\n    factor = len(str(remainder)) - signs\n\n    if factor > 0:\n        # \u043f\u043e\u0441\u043b\u0435 \u0437\u0430\u043f\u044f\u0442\u043e\u0439 \u0446\u0438\u0444\u0440 \u0431\u043e\u043b\u044c\u0448\u0435 \u0447\u0435\u043c signs, \u043e\u043a\u0440\u0443\u0433\u043b\u044f\u0435\u043c\n        iremainder = int(round(iremainder / (10.0**factor)))\n    format = \"%%0%dd\" % min(len(remainder), signs)\n\n    remainder = format % iremainder\n\n    if len(remainder) > signs:\n        # \u043f\u0440\u0438 \u043e\u043a\u0440\u0443\u0433\u043b\u0435\u043d\u0438\u0438 \u0446\u0438\u0444\u0440 \u0432\u0438\u0434\u0430 0.998 \u0440\u0443\u0433\u0430\u0442\u044c\u0441\u044f\n        raise ValueError(\"Signs overflow: I can't round only fractional part \\\n                          of %s to fit %s in %d signs\" % \\\n                         (str(fvalue), orig_remainder, signs))\n\n    return remainder"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchoosing a proper case depending on amount of objects.", "response": "def choose_plural(amount, variants):\n    \"\"\"\n    Choose proper case depending on amount\n\n    @param amount: amount of objects\n    @type amount: C{integer types}\n\n    @param variants: variants (forms) of object in such form:\n        (1 object, 2 objects, 5 objects).\n    @type variants: 3-element C{sequence} of C{unicode}\n        or C{unicode} (three variants with delimeter ',')\n\n    @return: proper variant\n    @rtype: C{unicode}\n\n    @raise ValueError: variants' length lesser than 3\n    \"\"\"\n    \n    if isinstance(variants, six.text_type):\n        variants = split_values(variants)\n    check_length(variants, 3)\n    amount = abs(amount)\n    \n    if amount % 10 == 1 and amount % 100 != 11:\n        variant = 0\n    elif amount % 10 >= 2 and amount % 10 <= 4 and \\\n         (amount % 100 < 10 or amount % 100 >= 20):\n        variant = 1\n    else:\n        variant = 2\n    \n    return variants[variant]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget proper case with value", "response": "def get_plural(amount, variants, absence=None):\n    \"\"\"\n    Get proper case with value\n\n    @param amount: amount of objects\n    @type amount: C{integer types}\n\n    @param variants: variants (forms) of object in such form:\n        (1 object, 2 objects, 5 objects).\n    @type variants: 3-element C{sequence} of C{unicode}\n        or C{unicode} (three variants with delimeter ',')\n\n    @param absence: if amount is zero will return it\n    @type absence: C{unicode}\n\n    @return: amount with proper variant\n    @rtype: C{unicode}\n    \"\"\"\n    if amount or absence is None:\n        return u\"%d %s\" % (amount, choose_plural(amount, variants))\n    else:\n        return absence"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_plural_legacy(amount, extra_variants):\n    absence = None\n    if isinstance(extra_variants, six.text_type):\n        extra_variants = split_values(extra_variants)\n    if len(extra_variants) == 4:\n        variants = extra_variants[:3]\n        absence = extra_variants[3]\n    else:\n        variants = extra_variants\n    return get_plural(amount, variants, absence)", "response": "Get proper case with value for legacy variant"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget string for money s amount", "response": "def rubles(amount, zero_for_kopeck=False):\n    \"\"\"\n    Get string for money\n\n    @param amount: amount of money\n    @type amount: C{integer types}, C{float} or C{Decimal}\n\n    @param zero_for_kopeck: If false, then zero kopecks ignored\n    @type zero_for_kopeck: C{bool}\n\n    @return: in-words representation of money's amount\n    @rtype: C{unicode}\n\n    @raise ValueError: amount is negative\n    \"\"\"\n    check_positive(amount)\n\n    pts = []\n    amount = round(amount, 2)\n    pts.append(sum_string(int(amount), 1, (u\"\u0440\u0443\u0431\u043b\u044c\", u\"\u0440\u0443\u0431\u043b\u044f\", u\"\u0440\u0443\u0431\u043b\u0435\u0439\")))\n    remainder = _get_float_remainder(amount, 2)\n    iremainder = int(remainder)\n\n    if iremainder != 0 or zero_for_kopeck:\n        # \u0435\u0441\u043b\u0438 3.1, \u0442\u043e \u044d\u0442\u043e 10 \u043a\u043e\u043f\u0435\u0435\u043a, \u0430 \u043d\u0435 \u043e\u0434\u043d\u0430\n        if iremainder < 10 and len(remainder) == 1:\n            iremainder *= 10\n        pts.append(sum_string(iremainder, 2,\n                              (u\"\u043a\u043e\u043f\u0435\u0439\u043a\u0430\", u\"\u043a\u043e\u043f\u0435\u0439\u043a\u0438\", u\"\u043a\u043e\u043f\u0435\u0435\u043a\")))\n\n    return u\" \".join(pts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef in_words_float(amount, _gender=FEMALE):\n    check_positive(amount)\n\n    pts = []\n    # \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0446\u0435\u043b\u0443\u044e \u0447\u0430\u0441\u0442\u044c\n    pts.append(sum_string(int(amount), 2,\n                          (u\"\u0446\u0435\u043b\u0430\u044f\", u\"\u0446\u0435\u043b\u044b\u0445\", u\"\u0446\u0435\u043b\u044b\u0445\")))\n    # \u0442\u0435\u043f\u0435\u0440\u044c \u0442\u043e, \u0447\u0442\u043e \u043f\u043e\u0441\u043b\u0435 \u0437\u0430\u043f\u044f\u0442\u043e\u0439\n    remainder = _get_float_remainder(amount)\n    signs = len(str(remainder)) - 1\n    pts.append(sum_string(int(remainder), 2, FRACTIONS[signs]))\n\n    return u\" \".join(pts)", "response": "Return in - words reprsentation of float amount."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the in - words representation of a number.", "response": "def in_words(amount, gender=None):\n    \"\"\"\n    Numeral in words\n\n    @param amount: numeral\n    @type amount: C{integer types}, C{float} or C{Decimal}\n\n    @param gender: gender (MALE, FEMALE or NEUTER)\n    @type gender: C{int}\n\n    @return: in-words reprsentation of numeral\n    @rtype: C{unicode}\n\n    raise ValueError: when amount is negative\n    \"\"\"\n    check_positive(amount)\n    if isinstance(amount, Decimal) and amount.as_tuple()[2] == 0:\n        # \u0435\u0441\u043b\u0438 \u0446\u0435\u043b\u043e\u0435,\n        # \u0442.\u0435. Decimal.as_tuple -> (sign, digits tuple, exponent), exponent=0\n        # \u0442\u043e \u043a\u0430\u043a \u0446\u0435\u043b\u043e\u0435\n        amount = int(amount)\n    if gender is None:\n        args = (amount,)\n    else:\n        args = (amount, gender)\n    # \u0435\u0441\u043b\u0438 \u0446\u0435\u043b\u043e\u0435\n    if isinstance(amount, six.integer_types):\n        return in_words_int(*args)\n    # \u0435\u0441\u043b\u0438 \u0434\u0440\u043e\u0431\u043d\u043e\u0435\n    elif isinstance(amount, (float, Decimal)):\n        return in_words_float(*args)\n    # \u043d\u0438 float, \u043d\u0438 int, \u043d\u0438 Decimal\n    else:\n        # \u0434\u043e \u0441\u044e\u0434\u0430 \u043d\u0435 \u0434\u043e\u043b\u0436\u043d\u043e \u0434\u043e\u0439\u0442\u0438\n        raise TypeError(\n            \"amount should be number type (int, long, float, Decimal), got %s\"\n            % type(amount))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting sum in words in words of the given amount.", "response": "def sum_string(amount, gender, items=None):\n    \"\"\"\n    Get sum in words\n\n    @param amount: amount of objects\n    @type amount: C{integer types}\n\n    @param gender: gender of object (MALE, FEMALE or NEUTER)\n    @type gender: C{int}\n\n    @param items: variants of object in three forms:\n        for one object, for two objects and for five objects\n    @type items: 3-element C{sequence} of C{unicode} or\n        just C{unicode} (three variants with delimeter ',')\n\n    @return: in-words representation objects' amount\n    @rtype: C{unicode}\n\n    @raise ValueError: items isn't 3-element C{sequence} or C{unicode}\n    @raise ValueError: amount bigger than 10**11\n    @raise ValueError: amount is negative\n    \"\"\"\n    if isinstance(items, six.text_type):\n        items = split_values(items)\n    if items is None:\n        items = (u\"\", u\"\", u\"\")\n\n    try:\n        one_item, two_items, five_items = items\n    except ValueError:\n        raise ValueError(\"Items must be 3-element sequence\")\n\n    check_positive(amount)\n\n    if amount == 0:\n        if five_items:\n            return u\"\u043d\u043e\u043b\u044c %s\" % five_items\n        else:\n            return u\"\u043d\u043e\u043b\u044c\"\n\n    into = u''\n    tmp_val = amount\n\n    # \u0435\u0434\u0438\u043d\u0438\u0446\u044b\n    into, tmp_val = _sum_string_fn(into, tmp_val, gender, items)\n    # \u0442\u044b\u0441\u044f\u0447\u0438\n    into, tmp_val = _sum_string_fn(into, tmp_val, FEMALE,\n                                    (u\"\u0442\u044b\u0441\u044f\u0447\u0430\", u\"\u0442\u044b\u0441\u044f\u0447\u0438\", u\"\u0442\u044b\u0441\u044f\u0447\"))\n    # \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u044b\n    into, tmp_val = _sum_string_fn(into, tmp_val, MALE,\n                                    (u\"\u043c\u0438\u043b\u043b\u0438\u043e\u043d\", u\"\u043c\u0438\u043b\u043b\u0438\u043e\u043d\u0430\", u\"\u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432\"))\n    # \u043c\u0438\u043b\u043b\u0438\u0430\u0440\u0434\u044b\n    into, tmp_val = _sum_string_fn(into, tmp_val, MALE,\n                                    (u\"\u043c\u0438\u043b\u043b\u0438\u0430\u0440\u0434\", u\"\u043c\u0438\u043b\u043b\u0438\u0430\u0440\u0434\u0430\", u\"\u043c\u0438\u043b\u043b\u0438\u0430\u0440\u0434\u043e\u0432\"))\n    if tmp_val == 0:\n        return into\n    else:\n        raise ValueError(\"Cannot operand with numbers bigger than 10**11\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _sum_string_fn(into, tmp_val, gender, items=None):\n    if items is None:\n        items = (u\"\", u\"\", u\"\")\n    one_item, two_items, five_items = items\n    \n    check_positive(tmp_val)\n\n    if tmp_val == 0:\n        return into, tmp_val\n\n    words = []\n\n    rest = tmp_val % 1000\n    tmp_val = tmp_val // 1000\n    if rest == 0:\n        # \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 \u0442\u0440\u0438 \u0437\u043d\u0430\u043a\u0430 \u043d\u0443\u043b\u0435\u0432\u044b\u0435\n        if into == u\"\":\n            into = u\"%s \" % five_items\n        return into, tmp_val\n\n    # \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u0441 rest\n    end_word = five_items\n\n    # \u0441\u043e\u0442\u043d\u0438\n    words.append(HUNDREDS[rest // 100])\n\n    # \u0434\u0435\u0441\u044f\u0442\u043a\u0438\n    rest = rest % 100\n    rest1 = rest // 10\n    # \u043e\u0441\u043e\u0431\u044b\u0439 \u0441\u043b\u0443\u0447\u0430\u0439 -- tens=1\n    tens = rest1 == 1 and TENS[rest] or TENS[rest1]\n    words.append(tens)\n\n    # \u0435\u0434\u0438\u043d\u0438\u0446\u044b\n    if rest1 < 1 or rest1 > 1:\n        amount = rest % 10\n        end_word = choose_plural(amount, items)\n        words.append(ONES[amount][gender-1])\n    words.append(end_word)\n\n    # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e, \u0447\u0442\u043e \u0443\u0436\u0435 \u0431\u044b\u043b\u043e\n    words.append(into)\n\n    # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0443\u0441\u0442\u044b\u0435 \u043f\u043e\u0434\u0441\u0442\u0440\u043e\u043a\u0438\n    words = filter(lambda x: len(x) > 0, words)\n\n    # \u0441\u043a\u043b\u0435\u0438\u0432\u0430\u0435\u043c \u0438 \u043e\u0442\u0434\u0430\u0435\u043c\n    return u\" \".join(words).strip(), tmp_val", "response": "This function is used to calculate the sum of the in - words representation of a single order object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef slugify(in_string):\n    try:\n        u_in_string = six.text_type(in_string).lower()\n    except UnicodeDecodeError:\n        raise ValueError(\"We expects when in_string is str type,\" + \\\n                         \"it is an ascii, but now it isn't. Use unicode \" + \\\n                         \"in this case.\")\n    # convert & to \"and\"\n    u_in_string = re.sub('\\&amp\\;|\\&', ' and ', u_in_string)\n    # replace spaces by hyphen\n    u_in_string = re.sub('[-\\s]+', '-', u_in_string)\n    # remove symbols that not in alphabet\n    u_in_string = u''.join([symb for symb in u_in_string if symb in ALPHABET])\n    # translify it\n    out_string = translify(u_in_string)\n    # remove non-alpha\n    return re.sub('[^\\w\\s-]', '', out_string).strip().lower()", "response": "Prepare string for slug - based on input string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_length(value, length):\n    _length = len(value)\n    if _length != length:\n        raise ValueError(\"length must be %d, not %d\" % \\\n                         (length, _length))", "response": "Checks length of value\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_positive(value, strict=False):\n    if not strict and value < 0:\n        raise ValueError(\"Value must be positive or zero, not %s\" % str(value))\n    if strict and value <= 0:\n        raise ValueError(\"Value must be positive, not %s\" % str(value))", "response": "Checks if the variable is positive or zero."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_values(ustring, sep=u','):\n    assert isinstance(ustring, six.text_type), \"uvalue must be unicode, not %s\" % type(ustring)\n    # unicode have special mark symbol 0xffff which cannot be used in a regular text,\n    # so we use it to mark a place where escaped column was\n    ustring_marked = ustring.replace(u'\\,', u'\\uffff')\n    items = tuple([i.strip().replace(u'\\uffff', u',') for i in ustring_marked.split(sep)])\n    return items", "response": "Splits unicode string with separator Csep but skips escaped separator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransform the records with the patch.", "response": "def apply(diff, recs, strict=True):\n    \"\"\"\n    Transform the records with the patch. May fail if the records do not\n    match those expected in the patch.\n    \"\"\"\n    index_columns = diff['_index']\n    indexed = records.index(copy.deepcopy(list(recs)), index_columns)\n    _add_records(indexed, diff['added'], index_columns, strict=strict)\n    _remove_records(indexed, diff['removed'], index_columns, strict=strict)\n    _update_records(indexed, diff['changed'], strict=strict)\n    return records.sort(indexed.values())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(istream, strict=True):\n    \"Deserialize a patch object.\"\n    try:\n        diff = json.load(istream)\n        if strict:\n            jsonschema.validate(diff, SCHEMA)\n    except ValueError:\n        raise InvalidPatchError('patch is not valid JSON')\n\n    except jsonschema.exceptions.ValidationError as e:\n        raise InvalidPatchError(e.message)\n\n    return diff", "response": "Deserialize a patch object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize a patch object.", "response": "def save(diff, stream=sys.stdout, compact=False):\n    \"Serialize a patch object.\"\n    flags = {'sort_keys': True}\n    if not compact:\n        flags['indent'] = 2\n\n    json.dump(diff, stream, **flags)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new record set from two sets of records.", "response": "def create(from_records, to_records, index_columns, ignore_columns=None):\n    \"\"\"\n    Diff two sets of records, using the index columns as the primary key for\n    both datasets.\n    \"\"\"\n    from_indexed = records.index(from_records, index_columns)\n    to_indexed = records.index(to_records, index_columns)\n\n    if ignore_columns is not None:\n        from_indexed = records.filter_ignored(from_indexed, ignore_columns)\n        to_indexed = records.filter_ignored(to_indexed, ignore_columns)\n\n    return create_indexed(from_indexed, to_indexed, index_columns)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compare_rows(from_recs, to_recs, keys):\n    \"Return the set of keys which have changed.\"\n    return set(\n        k for k in keys\n        if sorted(from_recs[k].items()) != sorted(to_recs[k].items())\n    )", "response": "Return the set of keys which have changed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef record_diff(lhs, rhs):\n    \"Diff an individual row.\"\n    delta = {}\n    for k in set(lhs).union(rhs):\n        from_ = lhs[k]\n        to_ = rhs[k]\n        if from_ != to_:\n            delta[k] = {'from': from_, 'to': to_}\n\n    return delta", "response": "Diff an individual row."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_significance(diff, significance):\n    changed = diff['changed']\n\n    # remove individual field changes that are significant\n    reduced = [{'key': delta['key'],\n                'fields': {k: v\n                           for k, v in delta['fields'].items()\n                           if _is_significant(v, significance)}}\n               for delta in changed]\n\n    # call a key changed only if it still has significant changes\n    filtered = [delta for delta in reduced if delta['fields']]\n\n    diff = diff.copy()\n    diff['changed'] = filtered\n    return diff", "response": "Remove any changes in the patch which are due to numeric changes less than this level of significance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if a change is genuinely significant given our tolerance.", "response": "def _is_significant(change, significance):\n    \"\"\"\n    Return True if a change is genuinely significant given our tolerance.\n    \"\"\"\n    try:\n        a = float(change['from'])\n        b = float(change['to'])\n\n    except ValueError:\n        return True\n\n    return abs(a - b) > 10 ** (-significance)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying the patch to the source CSV file and save the result to the target CSV file.", "response": "def patch_file(patch_stream: TextIO, fromcsv_stream: TextIO, tocsv_stream: TextIO,\n               strict: bool = True, sep: str = ','):\n    \"\"\"\n    Apply the patch to the source CSV file, and save the result to the target\n    file.\n    \"\"\"\n    diff = patch.load(patch_stream)\n\n    from_records = records.load(fromcsv_stream, sep=sep)\n    to_records = patch.apply(diff, from_records, strict=strict)\n\n    # what order should the columns be in?\n    if to_records:\n        # have data, use a nice ordering\n        all_columns = to_records[0].keys()\n        index_columns = diff['_index']\n        fieldnames = _nice_fieldnames(all_columns, index_columns)\n    else:\n        # no data, use the original order\n        fieldnames = from_records.fieldnames\n\n    records.save(to_records, fieldnames, tocsv_stream)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef patch_records(diff, from_records, strict=True):\n    return patch.apply(diff, from_records, strict=strict)", "response": "Apply the patch to the sequence of records returning the transformed sequence of records."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _nice_fieldnames(all_columns, index_columns):\n    \"Indexes on the left, other fields in alphabetical order on the right.\"\n    non_index_columns = set(all_columns).difference(index_columns)\n    return index_columns + sorted(non_index_columns)", "response": "Indexes on the left other fields in alphabetical order on the right."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef csvdiff_cmd(index_columns, from_csv, to_csv, style=None, output=None,\n                sep=',', quiet=False, ignore_columns=None, significance=None):\n    \"\"\"\n    Compare two csv files to see what rows differ between them. The files\n    are each expected to have a header row, and for each row to be uniquely\n    identified by one or more indexing columns.\n    \"\"\"\n\n    if ignore_columns is not None:\n        for i in ignore_columns:\n            if i in index_columns:\n                error.abort(\"You can't ignore an index column\")\n\n    ostream = (open(output, 'w') if output\n               else io.StringIO() if quiet\n               else sys.stdout)\n\n    try:\n        if style == 'summary':\n            _diff_and_summarize(from_csv, to_csv, index_columns, ostream,\n                                sep=sep, ignored_columns=ignore_columns,\n                                significance=significance)\n        else:\n            compact = (style == 'compact')\n            _diff_files_to_stream(from_csv, to_csv, index_columns, ostream,\n                                  compact=compact, sep=sep, ignored_columns=ignore_columns,\n                                  significance=significance)\n\n    except records.InvalidKeyError as e:\n        error.abort(e.args[0])\n\n    finally:\n        ostream.close()", "response": "Compare two csv files to see what rows differ between them."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint a summary of the difference between two files.", "response": "def _diff_and_summarize(from_csv, to_csv, index_columns, stream=sys.stdout,\n                        sep=',', ignored_columns=None, significance=None):\n    \"\"\"\n    Print a summary of the difference between the two files.\n    \"\"\"\n    from_records = list(records.load(from_csv, sep=sep))\n    to_records = records.load(to_csv, sep=sep)\n\n    diff = patch.create(from_records, to_records, index_columns, ignored_columns)\n    if significance is not None:\n        diff = patch.filter_significance(diff, significance)\n\n    _summarize_diff(diff, len(from_records), stream=stream)\n    exit_code = (EXIT_SAME\n                 if patch.is_empty(diff)\n                 else EXIT_DIFFERENT)\n    sys.exit(exit_code)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply the changes from a csvdiff patch to an existing CSV file.", "response": "def csvpatch_cmd(input_csv, input=None, output=None, strict=True):\n    \"\"\"\n    Apply the changes from a csvdiff patch to an existing CSV file.\n    \"\"\"\n    patch_stream = (sys.stdin\n                    if input is None\n                    else open(input))\n    tocsv_stream = (sys.stdout\n                    if output is None\n                    else open(output, 'w'))\n    fromcsv_stream = open(input_csv)\n\n    try:\n        patch_file(patch_stream, fromcsv_stream, tocsv_stream, strict=strict)\n\n    except patch.InvalidPatchError as e:\n        error.abort('reading patch, {0}'.format(e.args[0]))\n\n    finally:\n        patch_stream.close()\n        fromcsv_stream.close()\n        tocsv_stream.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsort records into a canonical order suitable for comparison.", "response": "def sort(records: Sequence[Record]) -> List[Record]:\n    \"Sort records into a canonical order, suitable for comparison.\"\n    return sorted(records, key=_record_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _record_key(record: Record) -> List[Tuple[Column, str]]:\n    \"An orderable representation of this record.\"\n    return sorted(record.items())", "response": "An orderable representation of this record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _outter_split(inpt, delim, openers, closers=None, opener_lookup=None):\n    if closers is None:\n        closers = openers\n    if opener_lookup is None:\n        opener_lookup = {}\n        for i in range(len(openers)):\n            opener_lookup[openers[i]] = i\n    stack = []\n    res = []\n    splt = 0\n    for i in range(len(inpt)):\n        if inpt[i] == delim and len(stack) == 0:\n            res.append(inpt[splt:i].strip())\n            splt = i+1\n        elif inpt[i] in opener_lookup:\n            stack.append(opener_lookup[inpt[i]])\n        elif len(stack) > 0 and inpt[i] == closers[stack[-1]]:\n            stack.pop()\n    res.append(inpt[splt:].strip())\n    return res", "response": "Splits only at delims that are at outter - most level regarding the openers and closers pairs. Returns a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getargspecs(func):\n    if func is None:\n        raise TypeError('None is not a Python function')\n    if hasattr(func, 'ch_func'):\n        return getargspecs(func.ch_func)\n    elif hasattr(func, 'ov_func'):\n        return getargspecs(func.ov_func)\n    if hasattr(inspect, 'getfullargspec'):\n        return inspect.getfullargspec(func) # Python 3\n    else:\n        return inspect.getargspec(func)", "response": "Returns the list of argspecs for a Python function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine whether given argspecs implies required keywords - only args and returns them as a list. Returns empty list if no such args exist.", "response": "def get_required_kwonly_args(argspecs):\n    \"\"\"Determines whether given argspecs implies required keywords-only args\n    and returns them as a list. Returns empty list if no such args exist.\n    \"\"\"\n    try:\n        kwonly = argspecs.kwonlyargs\n        if argspecs.kwonlydefaults is None:\n            return kwonly\n        res = []\n        for name in kwonly:\n            if not name in argspecs.kwonlydefaults:\n                res.append(name)\n        return res\n    except AttributeError:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getargnames(argspecs, with_unbox=False):\n    # todo: We can maybe make use of inspect.formatargspec\n    args = argspecs.args\n    vargs = argspecs.varargs\n    try:\n        kw = argspecs.keywords\n    except AttributeError:\n        kw = argspecs.varkw\n    try:\n        kwonly = argspecs.kwonlyargs\n    except AttributeError:\n        kwonly = None\n    res = []\n    if not args is None:\n        res.extend(args)\n    if not vargs is None:\n        res.append('*'+vargs if with_unbox else vargs)\n    if not kwonly is None:\n        res.extend(kwonly)\n    if not kw is None:\n        res.append('**'+kw if with_unbox else kw)\n    return res", "response": "Returns a list of all the arg - names as would be seen in a function signature including the passed - in argspecs."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nturn a linearized list of args into ( args keywords ) form according to given argspecs.", "response": "def fromargskw(argskw, argspecs, slf_or_clsm = False):\n    \"\"\"Turns a linearized list of args into (args, keywords) form\n    according to given argspecs (like inspect module provides).\n    \"\"\"\n    res_args = argskw\n    try:\n        kwds = argspecs.keywords\n    except AttributeError:\n        kwds = argspecs.varkw\n    if not kwds is None:\n        res_kw = argskw[-1]\n        res_args = argskw[:-1]\n    else:\n        res_kw = None\n    if not argspecs.varargs is None:\n        vargs_pos = (len(argspecs.args)-1) \\\n                if slf_or_clsm else len(argspecs.args)\n        if vargs_pos > 0:\n            res_lst = list(argskw[:vargs_pos])\n            res_lst.extend(argskw[vargs_pos])\n            res_args = tuple(res_lst)\n        else:\n            res_args = argskw[0]\n    try:\n        if len(argspecs.kwonlyargs) > 0:\n            res_kw = {} if res_kw is None else dict(res_kw)\n            ipos = -len(argspecs.kwonlyargs) - (0 if kwds is None else 1)\n            for name in argspecs.kwonlyargs:\n                res_kw[name] = argskw[ipos]\n                ipos += 1\n    except AttributeError:\n        pass\n    if res_kw is None:\n        res_kw = {}\n    return res_args, res_kw"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_staticmethod_qualname(staticmeth):\n    func = _actualfunc(staticmeth)\n    module = sys.modules[func.__module__]\n    nst = _get_class_nesting_list_for_staticmethod(staticmeth, module, [], set())\n    nst = [cl.__name__ for cl in nst]\n    return '.'.join(nst)+'.'+func.__name__", "response": "Determines the fully qualified name of a static method."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines the fully qualified name of a class.", "response": "def get_class_qualname(cls):\n    \"\"\"Determines the fully qualified name of a class.\n    Yields a result similar to what __qualname__ contains, but also works on\n    Python 2.7.\n    \"\"\"\n    if hasattr(cls, '__qualname__'):\n        return cls.__qualname__\n    module = sys.modules[cls.__module__]\n    if cls.__module__ == 'typing' and not hasattr(cls, '__name__'):\n        # Python 3.7\n        return cls._name\n    if hasattr(module, cls.__name__) and getattr(module, cls.__name__) is cls:\n        return cls.__name__\n    else:\n        nst = _get_class_nesting_list(cls, module)\n        nst.append(cls)\n        nst = [cl.__name__ for cl in nst]\n        return '.'.join(nst)\n    return cls.__name__"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch the module that declares cls.", "response": "def search_class_module(cls, deep_search=True):\n    \"\"\"E.g. if cls is a TypeVar, cls.__module__ won't contain the actual module\n    that declares cls. This returns the actual module declaring cls.\n    Can be used with any class (not only TypeVar), though usually cls.__module__\n    is the recommended way.\n    If deep_search is True (default) this even finds the correct module if it\n    declares cls as an inner class of another class.\n    \"\"\"\n    for md_name in sys.modules:\n        module = sys.modules[md_name]\n        if hasattr(module, cls.__name__) and getattr(module, cls.__name__) is cls:\n            return module\n    if deep_search:\n        for md_name in sys.modules:\n            module = sys.modules[md_name]\n            try:\n                nst = _get_class_nesting_list(cls, module)\n                if cls is nst[-1]:\n                    return module\n            except:\n                pass\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_class_that_defined_method(meth):\n    if is_classmethod(meth):\n        return meth.__self__\n    if hasattr(meth, 'im_class'):\n        return meth.im_class\n    elif hasattr(meth, '__qualname__'):\n        # Python 3\n        try:\n            cls_names = meth.__qualname__.split('.<locals>', 1)[0].rsplit('.', 1)[0].split('.')\n            cls = inspect.getmodule(meth)\n            for cls_name in cls_names:\n                cls = getattr(cls, cls_name)\n            if isinstance(cls, type):\n                return cls\n        except AttributeError:\n            # If this was called from a decorator and meth is not a method, this\n            # can result in AttributeError, because at decorator-time meth has not\n            # yet been added to module. If it's really a method, its class would be\n            # already in, so no problem in that case.\n            pass\n    raise ValueError(str(meth)+' is not a method.')", "response": "Determines the class that the given method is in."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_method(func):\n    func0 = _actualfunc(func)\n    argNames = getargnames(getargspecs(func0))\n    if len(argNames) > 0:\n        if argNames[0] == 'self':\n            if inspect.ismethod(func):\n                return True\n            elif sys.version_info.major >= 3:\n                # In Python3 there are no unbound methods, so we count as method,\n                # if first arg is called 'self' \n                return True\n            else:\n                _warn_argname('is_method encountered non-method declaring self',\n                        func0, False, False, None)\n        else:\n            return inspect.ismethod(func)\n    return False", "response": "Detects if the given callable is a method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_classmethod(meth):\n    if inspect.ismethoddescriptor(meth):\n        return isinstance(meth, classmethod)\n    if not inspect.ismethod(meth):\n        return False\n    if not inspect.isclass(meth.__self__):\n        return False\n    if not hasattr(meth.__self__, meth.__name__):\n        return False\n    return meth == getattr(meth.__self__, meth.__name__)", "response": "Detects if the given callable is a classmethod."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines the args of the current function call.", "response": "def get_current_args(caller_level = 0, func = None, argNames = None):\n    \"\"\"Determines the args of current function call.\n    Use caller_level > 0 to get args of even earlier function calls in current stack.\n    \"\"\"\n    if argNames is None:\n        argNames = getargnames(getargspecs(func))\n    if func is None:\n        func = get_current_function(caller_level+1)\n    if isinstance(func, property):\n        func = func.fget if func.fset is None else func.fset\n    stck = inspect.stack()\n    lcs = stck[1+caller_level][0].f_locals\n    return tuple([lcs[t] for t in argNames])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines the fully qualified name of a given code object.", "response": "def get_callable_fq_for_code(code, locals_dict = None):\n    \"\"\"Determines the function belonging to a given code object in a fully qualified fashion.\n    Returns a tuple consisting of\n    - the callable\n    - a list of classes and inner classes, locating the callable (like a fully qualified name)\n    - a boolean indicating whether the callable is a method\n    \"\"\"\n    if code in _code_callable_dict:\n        res = _code_callable_dict[code]\n        if not res[0] is None or locals_dict is None:\n            return res\n    md = getmodule(code)\n    if not md is None:\n        nesting = []\n        res, slf = _get_callable_fq_for_code(code, md, md, False, nesting, set())\n        if res is None and not locals_dict is None:\n            nesting = []\n            res, slf = _get_callable_from_locals(code, locals_dict, md, False, nesting)\n        else:\n            _code_callable_dict[code] = (res, nesting, slf)\n        return res, nesting, slf\n    else:\n        return None, None, None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the limit - parameter to strip away pytypes internals when used with API from traceback module.", "response": "def _calc_traceback_limit(tb):\n    \"\"\"Calculates limit-parameter to strip away pytypes' internals when used\n    with API from traceback module.\n    \"\"\"\n    limit = 1\n    tb2 = tb\n    while not tb2.tb_next is None:\n        try:\n            maybe_pytypes = tb2.tb_next.tb_frame.f_code.co_filename.split(os.sep)[-2]\n        except IndexError:\n            maybe_pytypes = None\n        if maybe_pytypes == 'pytypes' and not \\\n                tb2.tb_next.tb_frame.f_code == pytypes.typechecker._pytypes___import__.__code__:\n            break\n        else:\n            limit += 1\n            tb2 = tb2.tb_next\n    return limit"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _pytypes_excepthook(exctype, value, tb):\n    if pytypes.clean_traceback and issubclass(exctype, TypeError):\n        traceback.print_exception(exctype, value, tb, _calc_traceback_limit(tb))\n    else:\n        if _sys_excepthook is None:\n            sys.__excepthook__(exctype, value, tb)\n        else:\n            _sys_excepthook(exctype, value, tb)", "response": "An excepthook suitable for use as sys. excepthook that strips away\n    the part of the traceback belonging to pytypes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_generator_type(genr):\n    if genr in _checked_generator_types:\n        return _checked_generator_types[genr]\n    if not genr.gi_frame is None and 'gen_type' in genr.gi_frame.f_locals:\n        return genr.gi_frame.f_locals['gen_type']\n    else:\n        cllble, nesting, slf = util.get_callable_fq_for_code(genr.gi_code)\n        if cllble is None:\n            return Generator\n        return _funcsigtypes(cllble, slf, nesting[-1] if slf else None,\n                genr.gi_frame.f_globals if not genr.gi_frame is None else None)[1]", "response": "Obtains PEP 484 style type of a generator object i. e. returns a\nTaxonomy typing. Generator object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_iterable_itemtype(obj):\n    # support further specific iterables on demand\n    try:\n        if isinstance(obj, range):\n            tpl = tuple(deep_type(obj.start), deep_type(obj.stop), deep_type(obj.step))\n            return Union[tpl]\n    except TypeError:\n        # We're running Python 2\n        pass\n    if type(obj) is tuple:\n        tpl = tuple(deep_type(t) for t in obj)\n        return Union[tpl]\n    elif type(obj) is types.GeneratorType:\n        return get_generator_yield_type(obj)\n    else:\n        tp = deep_type(obj)\n        if is_Generic(tp):\n            if issubclass(tp.__origin__, typing.Iterable):\n                if len(tp.__args__) == 1:\n                    return tp.__args__[0]\n                return _select_Generic_superclass_parameters(tp, typing.Iterable)[0]\n    if is_iterable(obj):\n        if type(obj) is str:\n            return str\n        if hasattr(obj, '__iter__'):\n            if has_type_hints(obj.__iter__):\n                itrator = _funcsigtypes(obj.__iter__, True, obj.__class__)[1]\n                if is_Generic(itrator) and itrator.__origin__ is typing.Iterator:\n                    return itrator.__args__[0]\n        if hasattr(obj, '__getitem__'):\n            if has_type_hints(obj.__getitem__):\n                itrator =  _funcsigtypes(obj.__getitem__, True, obj.__class__)[1]\n                if is_Generic(itrator) and itrator.__origin__ is typing.Iterator:\n                    return itrator.__args__[0]\n        return None # means that type is unknown\n    else:\n        raise TypeError('Not an iterable: '+str(type(obj)))", "response": "Attempts to obtain an itemtype of an iterable."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_Generic_itemtype(sq, simplify=True):\n    if is_Tuple(sq):\n        if simplify:\n            itm_tps = [x for x in get_Tuple_params(sq)]\n            simplify_for_Union(itm_tps)\n            return Union[tuple(itm_tps)]\n        else:\n            return Union[get_Tuple_params(sq)]\n    else:\n        try:\n            res = _select_Generic_superclass_parameters(sq, typing.Container)\n        except TypeError:\n            res = None\n        if res is None:\n            try:\n                res = _select_Generic_superclass_parameters(sq, typing.Iterable)\n            except TypeError:\n                pass\n        if res is None:\n            raise TypeError(\"Has no itemtype: \"+type_str(sq))\n        else:\n            return res[0]", "response": "Retrieves the item type from a PEP 484 generic or subclass of such."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_Mapping_key_value(mp):\n    try:\n        res = _select_Generic_superclass_parameters(mp, typing.Mapping)\n    except TypeError:\n        res = None\n    if res is None:\n        raise TypeError(\"Has no key/value types: \"+type_str(mp))\n    else:\n        return tuple(res)", "response": "Retrieves the key and value types from a PEP 484 mapping or subclass of such."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the type values from tp that correspond to parameters defined by generic_supertype.", "response": "def get_Generic_parameters(tp, generic_supertype):\n    \"\"\"tp must be a subclass of generic_supertype.\n    Retrieves the type values from tp that correspond to parameters\n    defined by generic_supertype.\n\n    E.g. get_Generic_parameters(tp, typing.Mapping) is equivalent\n    to get_Mapping_key_value(tp) except for the error message.\n\n    Note that get_Generic_itemtype(tp) is not exactly equal to\n    get_Generic_parameters(tp, typing.Container), as that method\n    additionally contains treatment for typing.Tuple and typing.Iterable.\n    \"\"\"\n    try:\n        res = _select_Generic_superclass_parameters(tp, generic_supertype)\n    except TypeError:\n        res = None\n    if res is None:\n        raise TypeError(\"%s has no proper parameters defined by %s.\"%\n                (type_str(tp), type_str(generic_supertype)))\n    else:\n        return tuple(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_Callable_args_res(clb):\n    try:\n        return clb.__args__, clb.__result__\n    except AttributeError:\n        # Python 3.6\n        return clb.__args__[:-1], clb.__args__[-1]", "response": "Python version independent function to obtain the parameters\n    of a typing. Callable object. Returns as tuple args result."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if an object is a type.", "response": "def is_Type(tp):\n    \"\"\"Python version independent check if an object is a type.\n    For Python 3.7 onwards(?) this is not equivalent to\n    ``isinstance(tp, type)`` any more, as that call would return\n    ``False`` for PEP 484 types.\n    Tested with CPython 2.7, 3.5, 3.6, 3.7 and Jython 2.7.1.\n    \"\"\"\n    if isinstance(tp, type):\n        return True\n    try:\n        typing._type_check(tp, '')\n        return True\n    except TypeError:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntries to construct a type for a given value.", "response": "def deep_type(obj, depth = None, max_sample = None, get_type = None):\n    \"\"\"Tries to construct a type for a given value. In contrast to type(...),\n    deep_type does its best to fit structured types from typing as close as\n    possible to the given value.\n    E.g. deep_type((1, 2, 'a')) will return Tuple[int, int, str] rather than\n    just tuple.\n    Supports various types from typing, but not yet all.\n    Also detects nesting up to given depth (uses pytypes.default_typecheck_depth\n    if no value is given).\n    If a value for max_sample is given, this number of elements is probed\n    from lists, sets and dictionaries to determine the element type. By default,\n    all elements are probed. If there are fewer elements than max_sample, all\n    existing elements are probed.\n    Optionally, a custom get_type function can be provided to further\n    customize how types are resolved. By default it uses type function.\n    \"\"\"\n    return _deep_type(obj, [], 0, depth, max_sample, get_type)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _deep_type(obj, checked, checked_len, depth = None, max_sample = None, get_type = None):\n    if depth is None:\n        depth = pytypes.default_typecheck_depth\n    if max_sample is None:\n        max_sample = pytypes.deep_type_samplesize\n    if -1 != max_sample < 2:\n        max_sample = 2\n    if get_type is None:\n        get_type = type\n    try:\n        res = obj.__orig_class__\n    except AttributeError:\n        res = get_type(obj)\n    if depth == 0 or util._is_in(obj, checked[:checked_len]):\n        return res\n    elif not util._is_in(obj, checked[checked_len:]):\n        checked.append(obj)\n    # We must operate with a consistent checked list for one certain depth level\n    # to avoid issues with a list, tuple, dict, etc containing the same element\n    # multiple times. This could otherwise be misconcepted as a recursion.\n    # Using a fake len checked_len2 ensures this. Each depth level operates with\n    # a common fake length of checked list:\n    checked_len2 = len(checked)\n    if res == tuple:\n        res = Tuple[tuple(_deep_type(t, checked, checked_len2, depth-1, None, get_type) for t in obj)]\n    elif res == list:\n        if len(obj) == 0:\n            return Empty[List]\n        if max_sample == -1 or max_sample >= len(obj)-1 or len(obj) <= 2:\n            tpl = tuple(_deep_type(t, checked, checked_len2, depth-1, None, get_type) for t in obj)\n        else:\n            # In case of lists I somehow feel it's better to ensure that\n            # first and last element are part of the sample\n            sample = [0, len(obj)-1]\n            try:\n                rsmp = random.sample(xrange(1, len(obj)-1), max_sample-2)\n            except NameError:\n                rsmp = random.sample(range(1, len(obj)-1), max_sample-2)\n            sample.extend(rsmp)\n            tpl = tuple(_deep_type(obj[t], checked, checked_len2, depth-1, None, get_type) for t in sample)\n        res = List[Union[tpl]]\n    elif res == dict:\n        if len(obj) == 0:\n            return Empty[Dict]\n        if max_sample == -1 or max_sample >= len(obj)-1 or len(obj) <= 2:\n            try:\n                # We prefer a view (avoid copy)\n                tpl1 = tuple(_deep_type(t, checked, checked_len2, depth-1, None, get_type) \\\n                        for t in obj.viewkeys())\n                tpl2 = tuple(_deep_type(t, checked, checked_len2, depth-1, None, get_type) \\\n                        for t in obj.viewvalues())\n            except AttributeError:\n                # Python 3 gives views like this:\n                tpl1 = tuple(_deep_type(t, checked, checked_len2, depth-1, None, get_type) for t in obj.keys())\n                tpl2 = tuple(_deep_type(t, checked, checked_len2, depth-1, None, get_type) for t in obj.values())\n        else:\n            try:\n                kitr = iter(obj.viewkeys())\n                vitr = iter(obj.viewvalues())\n            except AttributeError:\n                kitr = iter(obj.keys())\n                vitr = iter(obj.values())\n            ksmpl = []\n            vsmpl = []\n            block = (len(obj) // max_sample)-1\n            # I know this method has some bias towards beginning of iteration\n            # sequence, but it's still more random than just taking the\n            # initial sample and better than O(n) random.sample.\n            while len(ksmpl) < max_sample:\n                if block > 0:\n                    j = random.randint(0, block)\n                    k = random.randint(0, block)\n                    while j > 0:\n                        next(vitr) # discard\n                        j -= 1\n                    while k > 0:\n                        next(kitr) # discard\n                        k -= 1\n                ksmpl.append(next(kitr))\n                vsmpl.append(next(vitr))\n            tpl1 = tuple(_deep_type(t, checked, checked_len2, depth-1, None, get_type) for t in ksmpl)\n            tpl2 = tuple(_deep_type(t, checked, checked_len2, depth-1, None, get_type) for t in vsmpl)\n        res = Dict[Union[tpl1], Union[tpl2]]\n    elif res == set or res == frozenset:\n        if res == set:\n            typ = Set\n        else:\n            typ = FrozenSet\n        if len(obj) == 0:\n            return Empty[typ]\n        if max_sample == -1 or max_sample >= len(obj)-1 or len(obj) <= 2:\n            tpl = tuple(_deep_type(t, checked, depth-1, None, None, get_type) for t in obj)\n        else:\n            itr = iter(obj)\n            smpl = []\n            block = (len(obj) // max_sample)-1\n            # I know this method has some bias towards beginning of iteration\n            # sequence, but it's still more random than just taking the\n            # initial sample and better than O(n) random.sample.\n            while len(smpl) < max_sample:\n                if block > 0:\n                    j = random.randint(0, block)\n                    while j > 0:\n                        next(itr) # discard\n                        j -= 1\n                smpl.append(next(itr))\n            tpl = tuple(_deep_type(t, checked, depth-1, None, None, get_type) for t in smpl)\n        res = typ[Union[tpl]]\n    elif res == types.GeneratorType:\n        res = get_generator_type(obj)\n    elif sys.version_info.major == 2 and isinstance(obj, types.InstanceType):\n        # For old-style instances return the actual class:\n        return obj.__class__\n    elif _has_base(res, Container) and len(obj) == 0:\n        return Empty[res]\n    elif hasattr(res, '__origin__') and _has_base(res.__origin__, Container) and len(obj) == 0:\n        return Empty[res.__origin__]\n    return res", "response": "Recursively detects the type of a nested list of objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the given type is a builtin one.", "response": "def is_builtin_type(tp):\n    \"\"\"Checks if the given type is a builtin one.\n    \"\"\"\n    return hasattr(__builtins__, tp.__name__) and tp is getattr(__builtins__, tp.__name__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _tp_relfq_name(tp, tp_name=None, assumed_globals=None, update_assumed_globals=None,\n            implicit_globals=None):\n    # _type: (type, Optional[Union[Set[Union[type, types.ModuleType]], Mapping[Union[type, types.ModuleType], str]]], Optional[bool]) -> str\n    \"\"\"Provides the fully qualified name of a type relative to a set of\n    modules and types that is assumed as globally available.\n    If assumed_globals is None this always returns the fully qualified name.\n    If update_assumed_globals is True, this will return the plain type name,\n    but will add the type to assumed_globals (expected to be a set).\n    This way a caller can query how to generate an appropriate import section.\n    If update_assumed_globals is False, assumed_globals can alternatively be\n    a mapping rather than a set. In that case the mapping is expected to be\n    an alias table, mapping modules or types to their alias names desired for\n    displaying.\n    update_assumed_globals can be None (default). In that case this will return the\n    plain type name if assumed_globals is None as well (default).\n    This mode is there to have a less involved default behavior.\n    \"\"\"\n    if tp_name is None:\n        tp_name = util.get_class_qualname(tp)\n    if implicit_globals is None:\n        implicit_globals = _implicit_globals\n    else:\n        implicit_globals = implicit_globals.copy()\n        implicit_globals.update(_implicit_globals)\n    if assumed_globals is None:\n        if update_assumed_globals is None:\n            return tp_name\n        md = sys.modules[tp.__module__]\n        if md in implicit_globals:\n            return tp_name\n        name = tp.__module__+'.'+tp_name\n        pck = None\n        if not (md.__package__ is None or md.__package__ == ''\n                or name.startswith(md.__package__)):\n            pck = md.__package__\n        return name if pck is None else pck+'.'+name\n    if tp in assumed_globals:\n        try:\n            return assumed_globals[tp]\n        except:\n            return tp_name\n    elif hasattr(tp, '__origin__') and tp.__origin__ in assumed_globals:\n        try:\n            return assumed_globals[tp.__origin__]\n        except:\n            return tp_name\n    # For some reason Callable does not have __origin__, so we special-case\n    # it here. Todo: Find a cleaner solution.\n    elif is_Callable(tp) and typing.Callable in assumed_globals:\n        try:\n            return assumed_globals[typing.Callable]\n        except:\n            return tp_name\n    elif update_assumed_globals == True:\n        if not assumed_globals is None:\n            if hasattr(tp, '__origin__') and not tp.__origin__ is None:\n                toadd = tp.__origin__\n            elif is_Callable(tp):\n                toadd = typing.Callable\n            else:\n                toadd = tp\n            if not sys.modules[toadd.__module__] in implicit_globals:\n                assumed_globals.add(toadd)\n        return tp_name\n    else:\n        md = sys.modules[tp.__module__]\n        if md in implicit_globals:\n            return tp_name\n        md_name = tp.__module__\n        if md in assumed_globals:\n            try:\n                md_name = assumed_globals[md]\n            except:\n                pass\n        else:\n            if not (md.__package__ is None or md.__package__ == ''\n                    or md_name.startswith(md.__package__)):\n                md_name = md.__package__+'.'+tp.__module__\n        return md_name+'.'+tp_name", "response": "Provides the fully qualified name of a type relative to a set of modules and types that are globally available."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef type_str(tp, assumed_globals=None, update_assumed_globals=None,\n            implicit_globals=None, bound_Generic=None, bound_typevars=None):\n    \"\"\"Generates a nicely readable string representation of the given type.\n    The returned representation is workable as a source code string and would\n    reconstruct the given type if handed to eval, provided that globals/locals\n    are configured appropriately (e.g. assumes that various types from typing\n    have been imported).\n    Used as type-formatting backend of ptypes' code generator abilities\n    in modules typelogger and stubfile_2_converter.\n\n    If tp contains unbound TypeVars and bound_Generic is provided, this\n    function attempts to retrieve corresponding values for the unbound TypeVars\n    from bound_Generic.\n\n    For semantics of assumed_globals and update_assumed_globals see\n    _tp_relfq_name. Its doc applies to every argument or result contained in\n    tp (recursively) and to tp itself.\n    \"\"\"\n    if assumed_globals is None and update_assumed_globals is None:\n        if implicit_globals is None:\n            implicit_globals = set()\n        else:\n            implicit_globals = implicit_globals.copy()\n        implicit_globals.add(sys.modules['typing'])\n        implicit_globals.add(sys.modules['__main__'])\n    if isinstance(tp, tuple):\n        return '('+', '.join([type_str(tp0, assumed_globals, update_assumed_globals,\n                implicit_globals, bound_Generic, bound_typevars) for tp0 in tp])+')'\n    try:\n        return type_str(tp.__orig_class__, assumed_globals, update_assumed_globals,\n                implicit_globals, bound_Generic, bound_typevars)\n    except AttributeError:\n        pass\n    tp = _match_stub_type(tp)\n    if isinstance(tp, TypeVar):\n        prm = None\n        if not bound_typevars is None:\n            try:\n                prm = bound_typevars[tp]\n            except:\n                pass\n        if prm is None and not bound_typevars is None and tp in bound_typevars:\n            prm = bound_typevars[tp]\n        if prm is None and not bound_Generic is None:\n            prm = get_arg_for_TypeVar(tp, bound_Generic)\n        if not prm is None:\n            return type_str(prm, assumed_globals, update_assumed_globals,\n                    implicit_globals, bound_Generic, bound_typevars)\n        return tp.__name__\n    elif isinstance(tp, ForwardRef):\n        return \"'%s'\" % tp.__forward_arg__\n    elif isclass(tp) and not is_Generic(tp) \\\n            and not hasattr(typing, tp.__name__):\n        tp_name = _tp_relfq_name(tp, None, assumed_globals, update_assumed_globals,\n                implicit_globals)\n        prm = ''\n        if hasattr(tp, '__args__') and not tp.__args__ is None:\n            params = [type_str(param, assumed_globals, update_assumed_globals,\n                    implicit_globals, bound_Generic, bound_typevars) for param in tp.__args__]\n            prm = '[%s]'%', '.join(params)\n        return tp_name+prm\n    elif is_Union(tp):\n        prms = get_Union_params(tp)\n        params = [type_str(param, assumed_globals, update_assumed_globals,\n                implicit_globals, bound_Generic, bound_typevars) for param in prms]\n        # See: https://github.com/Stewori/pytypes/issues/44\n        if pytypes.canonical_type_str:\n            params = sorted(params)\n        return '%s[%s]'%(_tp_relfq_name(Union, 'Union', assumed_globals,\n                update_assumed_globals, implicit_globals), ', '.join(params))\n    elif is_Tuple(tp):\n        prms = get_Tuple_params(tp)\n        tpl_params = [type_str(param, assumed_globals, update_assumed_globals,\n                implicit_globals, bound_Generic, bound_typevars) for param in prms]\n        return '%s[%s]'%(_tp_relfq_name(Tuple, 'Tuple', assumed_globals,\n                update_assumed_globals, implicit_globals), ', '.join(tpl_params))\n    elif hasattr(tp, '__args__'):\n        tp_name = _tp_relfq_name(tp, None, assumed_globals, update_assumed_globals,\n                implicit_globals)\n        if tp.__args__ is None:\n            if hasattr(tp, '__parameters__') and \\\n                    hasattr(tp, '__origin__') and tp.__origin__ is Generic and \\\n                    not tp.__parameters__ is None and len(tp.__parameters__) > 0:\n                args = tp.__parameters__\n            else:\n                return tp_name\n        else:\n            args = tp.__args__\n        params = [type_str(param, assumed_globals, update_assumed_globals,\n                implicit_globals, bound_Generic, bound_typevars) for param in args]\n        if hasattr(tp, '__result__'):\n            return '%s[[%s], %s]'%(tp_name, ', '.join(params),\n                    type_str(tp.__result__, assumed_globals, update_assumed_globals,\n                    implicit_globals, bound_Generic, bound_typevars))\n        elif is_Callable(tp):\n            return '%s[[%s], %s]'%(tp_name, ', '.join(params[:-1]),\n                    type_str(params[-1], assumed_globals, update_assumed_globals,\n                    implicit_globals, bound_Generic, bound_typevars))\n        else:\n            return '%s[%s]'%(tp_name, ', '.join(params))\n    elif hasattr(tp, '__name__'):\n        result = _tp_relfq_name(tp, None, assumed_globals, update_assumed_globals,\n                implicit_globals)\n    elif tp is Any:\n        # In Python 3.6 Any does not have __name__.\n        result = _tp_relfq_name(tp, 'Any', assumed_globals, update_assumed_globals,\n                implicit_globals)\n    else:\n        # Todo: Care for other special types from typing where necessary.\n        result = str(tp)\n    if not implicit_globals is None:\n        for s in implicit_globals:\n            result = result.replace(s.__name__+'.', '')\n    return result", "response": "Generates a nicely readable string representation of the given type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_types(func):\n    return _get_types(func, util.is_classmethod(func), util.is_method(func))", "response": "Returns types as a sequence rather than a\n    dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_member_types(obj, member_name, prop_getter = False):\n    cls = obj.__class__\n    member = getattr(cls, member_name)\n    slf = not (isinstance(member, staticmethod) or isinstance(member, classmethod))\n    clsm = isinstance(member, classmethod)\n    return _get_types(member, clsm, slf, cls, prop_getter)", "response": "Get types of a member of a node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_types(func, clsm, slf, clss = None, prop_getter = False,\n            unspecified_type = Any, infer_defaults = None):\n    \"\"\"Helper for get_types and get_member_types.\n    \"\"\"\n    func0 = util._actualfunc(func, prop_getter)\n    # check consistency regarding special case with 'self'-keyword\n    if not slf:\n        argNames = util.getargnames(util.getargspecs(func0))\n        if len(argNames) > 0:\n            if clsm:\n                if argNames[0] != 'cls':\n                    util._warn_argname('classmethod using non-idiomatic cls argname',\n                            func0, slf, clsm, clss)\n    if clss is None and (slf or clsm):\n        if slf:\n            assert util.is_method(func) or isinstance(func, property)\n        if clsm:\n            assert util.is_classmethod(func)\n        clss = util.get_class_that_defined_method(func)\n        assert hasattr(clss, func.__name__)\n    args, res = _funcsigtypes(func, slf or clsm, clss, None, prop_getter,\n            unspecified_type = unspecified_type, infer_defaults = infer_defaults)\n    return _match_stub_type(args), _match_stub_type(res)", "response": "Helper for get_types and get_member_types."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresolves forward references in in_type and returns the corresponding object.", "response": "def resolve_fw_decl(in_type, module_name=None, globs=None, level=0,\n        search_stack_depth=2):\n    '''Resolves forward references in ``in_type``, see\n    https://www.python.org/dev/peps/pep-0484/#forward-references.\n\n\n    Note:\n\n    ``globs`` should be a dictionary containing values for the names\n    that must be resolved in ``in_type``. If ``globs`` is not provided, it\n    will be created by ``__globals__`` from the module named ``module_name``,\n    plus ``__locals__`` from the last ``search_stack_depth`` stack frames (Default: 2),\n    beginning at the calling function. This is to resolve cases where ``in_type`` and/or\n    types it fw-references are defined inside a function.\n\n    To prevent walking the stack, set ``search_stack_depth=0``.\n    Ideally provide a proper ``globs`` for best efficiency.\n    See ``util.get_function_perspective_globals`` for obtaining a ``globs`` that can be\n    cached. ``util.get_function_perspective_globals`` works like described above.\n    '''\n    # Also see discussion at https://github.com/Stewori/pytypes/pull/43\n    if in_type in _fw_resolve_cache:\n        return _fw_resolve_cache[in_type], True\n    if globs is None:\n        #if not module_name is None:\n        globs = util.get_function_perspective_globals(module_name, level+1,\n                level+1+search_stack_depth)\n    if isinstance(in_type, _basestring):\n        # For the case that a pure forward ref is given as string\n        out_type = eval(in_type, globs)\n        _fw_resolve_cache[in_type] = out_type\n        return out_type, True\n    elif isinstance(in_type, ForwardRef):\n        # Todo: Mabe somehow get globs from in_type.__forward_code__\n        if not in_type.__forward_evaluated__:\n            in_type.__forward_value__ = eval(in_type.__forward_arg__, globs)\n            in_type.__forward_evaluated__ = True\n            return in_type, True\n    elif is_Tuple(in_type):\n        return in_type, any([resolve_fw_decl(in_tp, None, globs)[1] \\\n                for in_tp in get_Tuple_params(in_type)])\n    elif is_Union(in_type):\n        return in_type, any([resolve_fw_decl(in_tp, None, globs)[1] \\\n                for in_tp in get_Union_params(in_type)])\n    elif is_Callable(in_type):\n        args, res = get_Callable_args_res(in_type)\n        ret = any([resolve_fw_decl(in_tp, None, globs)[1] \\\n                for in_tp in args])\n        ret = resolve_fw_decl(res, None, globs)[1] or ret\n        return in_type, ret\n    elif hasattr(in_type, '__args__') and in_type.__args__ is not None:\n        return in_type, any([resolve_fw_decl(in_tp, None, globs)[1] \\\n                for in_tp in in_type.__args__])\n    return in_type, False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _select_Generic_superclass_parameters(subclass, superclass_origin):\n    subclass = _find_base_with_origin(subclass, superclass_origin)\n    if subclass is None:\n        return None\n    if subclass.__origin__ is superclass_origin:\n        return subclass.__args__\n    prms = _find_Generic_super_origin(subclass, superclass_origin)\n    res = []\n    for prm in prms:\n        sub_search = subclass\n        while not sub_search is None:\n            try:\n                res.append(sub_search.__args__[sub_search.__origin__.__parameters__.index(prm)])\n                break\n            except ValueError:\n                # We search the closest base that actually contains the parameter\n                sub_search = _find_base_with_origin(\n                        sub_search.__origin__, superclass_origin)\n        else:\n            return None\n    return res", "response": "Helper for _issubclass_Generic. _select_Generic_superclass_parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _issubclass_Generic(subclass, superclass, bound_Generic, bound_typevars,\n            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n    \"\"\"Helper for _issubclass, a.k.a pytypes.issubtype.\n    \"\"\"\n    # this function is partly based on code from typing module 3.5.2.2\n    if subclass is None:\n        return False\n    if subclass in _extra_dict:\n        subclass = _extra_dict[subclass]\n    if is_Tuple(subclass):\n        tpl_prms = get_Tuple_params(subclass)\n        if not tpl_prms is None and len(tpl_prms) == 0:\n            # (This section is required because Empty shall not be\n            # used on Tuples.)\n            # an empty Tuple is any Sequence, regardless of type\n            # note that we needn't consider superclass beeing a tuple,\n            # because that should have been checked in _issubclass_Tuple\n            return issubclass(typing.Sequence,\n                    superclass if superclass.__origin__ is None else superclass.__origin__)\n        subclass = Sequence[Union[tpl_prms]]\n    if is_Generic(subclass):\n        # For a class C(Generic[T]) where T is co-variant,\n        # C[X] is a subclass of C[Y] iff X is a subclass of Y.\n        origin = _origin(superclass) #superclass.__origin__\n        if subclass.__origin__ is None:\n            try:\n                orig_bases = subclass.__orig_bases__\n            except AttributeError:\n                # Before typing 3.5.3.0 __bases__ used to contain all info that later\n                # became reserved for __orig_bases__. So we can use it as a fallback:\n                orig_bases = subclass.__bases__\n            for scls in orig_bases:\n                if is_Generic(scls):\n                    if _issubclass_Generic(scls, superclass, bound_Generic, bound_typevars,\n                            bound_typevars_readonly, follow_fwd_refs,\n                            _recursion_check):\n                        return True\n        #Formerly: if origin is not None and origin is subclass.__origin__:\n        elif origin is not None and \\\n                _issubclass(_origin(subclass), origin, bound_Generic, bound_typevars,\n                        # In Python 3.7 this can currently cause infinite recursion.\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n# \t\t\t\t_issubclass(subclass.__origin__, origin, bound_Generic, bound_typevars,\n# \t\t\t\t\t\tbound_typevars_readonly, follow_fwd_refs, _recursion_check):\n            assert len(superclass.__args__) == len(origin.__parameters__)\n            if len(subclass.__args__) == len(origin.__parameters__):\n                sub_args = subclass.__args__\n            else:\n                # We select the relevant subset of args by TypeVar-matching\n                sub_args = _select_Generic_superclass_parameters(subclass, superclass.__origin__)\n                assert len(sub_args) == len(origin.__parameters__)\n            for p_self, p_cls, p_origin in zip(superclass.__args__,\n                                            sub_args,\n                                            origin.__parameters__):\n                if isinstance(p_origin, TypeVar):\n                    if p_origin.__covariant__:\n                        # Covariant -- p_cls must be a subclass of p_self.\n                        if not _issubclass(p_cls, p_self, bound_Generic, bound_typevars,\n                                bound_typevars_readonly, follow_fwd_refs,\n                                _recursion_check):\n                            break\n                    elif p_origin.__contravariant__:\n                        # Contravariant.  I think it's the opposite. :-)\n                        if not _issubclass(p_self, p_cls, bound_Generic, bound_typevars,\n                                bound_typevars_readonly, follow_fwd_refs,\n                                _recursion_check):\n                            break\n                    else:\n                        # Invariant -- p_cls and p_self must equal.\n                        if p_self != p_cls:\n                            if not _issubclass(p_cls, p_self, bound_Generic, bound_typevars,\n                                    bound_typevars_readonly, follow_fwd_refs,\n                                    _recursion_check):\n                                break\n                            if not _issubclass(p_self, p_cls, bound_Generic, bound_typevars,\n                                    bound_typevars_readonly, follow_fwd_refs,\n                                    _recursion_check):\n                                break\n                else:\n                    # If the origin's parameter is not a typevar,\n                    # insist on invariance.\n                    if p_self != p_cls:\n                        if not _issubclass(p_cls, p_self, bound_Generic, bound_typevars,\n                                    bound_typevars_readonly, follow_fwd_refs,\n                                    _recursion_check):\n                            break\n                        if not _issubclass(p_self, p_cls, bound_Generic, bound_typevars,\n                                    bound_typevars_readonly, follow_fwd_refs,\n                                    _recursion_check):\n                            break\n            else:\n                return True\n            # If we break out of the loop, the superclass gets a chance.\n\n        # I.e.: origin is None or not _issubclass(subclass.__origin__, origin)\n        # In this case we must consider origin or subclass.__origin__ to be None\n        # We treat param-values as unknown in the following sense:\n        #   for covariant params: treat unknown more-or-equal specific than Any\n        #   for contravariant param: Any more-or-equal specific than Unknown\n        #   for invariant param: unknown never passes\n        # if both are unknown:\n        #   return False (?) (or NotImplemented? Or let a flag decide behavior?)\n        if origin is None:\n            if not pytypes.check_unbound_types:\n                raise TypeError(\"Attempted to check unbound type(superclass): \"+str(superclass))\n            if not subclass.__origin__ is None:\n                if not type.__subclasscheck__(superclass, subclass.__origin__):\n                    return False\n                prms = _find_Generic_super_origin(subclass.__origin__, superclass)\n                args = _select_Generic_superclass_parameters(subclass, superclass)\n                for i in range(len(prms)):\n                    if prms[i].__covariant__:\n                        if pytypes.strict_unknown_check:\n                            return False\n                    elif prms[i].__contravariant__:\n                        # Subclass-value must be wider than or equal to Any, i.e. must be Any:\n                        if not args[i] is Any:\n                            return False\n                    else:\n                        return False\n                return True\n            #else:\n                # nothing to do here... (?)\n        elif subclass.__origin__ is None:\n            if not pytypes.check_unbound_types:\n                raise TypeError(\"Attempted to check unbound type (subclass): \"+str(subclass))\n            if not type.__subclasscheck__(superclass.__origin__, subclass):\n                return False\n            prms = superclass.__origin__.__parameters__\n            for i in range(len(prms)):\n                if prms[i].__covariant__:\n                    # subclass-arg here is unknown, so in superclass only Any can pass:\n                    if not superclass.__args__[i] is Any:\n                        return False\n                elif prms[i].__contravariant__:\n                    if pytypes.strict_unknown_check:\n                        return False\n                else:\n                    return False\n            return True\n# \tFormerly: if super(GenericMeta, superclass).__subclasscheck__(subclass):\n    try:\n        if type.__subclasscheck__(superclass, subclass):\n            return True\n    except TypeError: pass\n    if _extra(superclass) is None or is_Generic(subclass):\n        return False\n    return _issubclass_2(subclass, _extra(superclass), bound_Generic, bound_typevars,\n            bound_typevars_readonly, follow_fwd_refs, _recursion_check)", "response": "Helper function for _issubclass."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _issubclass_Tuple(subclass, superclass, bound_Generic, bound_typevars,\n            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n    \"\"\"Helper for _issubclass, a.k.a pytypes.issubtype.\n    \"\"\"\n    # this function is partly based on code from typing module 3.5.2.2\n    if subclass in _extra_dict:\n        subclass = _extra_dict[subclass]\n    if not is_Type(subclass):\n        # To TypeError.\n        return False\n    if not is_Tuple(subclass):\n        if is_Generic(subclass):\n            try:\n                return _issubclass_Generic(subclass, superclass,\n                        bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs,\n                        _recursion_check)\n            except:\n                pass\n        elif is_Union(subclass):\n            return all(_issubclass_Tuple(t, superclass, bound_Generic, bound_typevars,\n                    bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n                    for t in get_Union_params(subclass))\n        else:\n            return False\n    super_args = get_Tuple_params(superclass)\n    if super_args is None:\n        return True\n    sub_args = get_Tuple_params(subclass)\n    if sub_args is None:\n        return False  # ???\n    # Covariance.\n    # For now we check ellipsis in most explicit manner.\n    # Todo: Compactify and Pythonify ellipsis branches (tests required before this).\n    if is_Tuple_ellipsis(subclass):\n        if is_Tuple_ellipsis(superclass):\n            # both are ellipsis, so no length check\n            common = min(len(super_args), len(sub_args))\n            for i in range(common):\n                if not _issubclass(sub_args[i], super_args[i], bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                    return False\n            if len(super_args) < len(sub_args):\n                for i in range(len(super_args), len(sub_args)):\n                    # Check remaining super args against the ellipsis type\n                    if not _issubclass(sub_args[i], super_args[-1], bound_Generic, bound_typevars,\n                            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                        return False\n            elif len(super_args) > len(sub_args):\n                for i in range(len(sub_args), len(super_args)):\n                    # Check remaining super args against the ellipsis type\n                    if not _issubclass(sub_args[-1], super_args[i], bound_Generic, bound_typevars,\n                            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                        return False\n            return True\n        else:\n            # only subclass has ellipsis\n            if len(super_args) < len(sub_args)-1:\n                return False\n            for i in range(len(sub_args)-1):\n                if not _issubclass(sub_args[i], super_args[i], bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                    return False\n            for i in range(len(sub_args), len(super_args)):\n                # Check remaining super args against the ellipsis type\n                if not _issubclass(sub_args[-1], super_args[i], bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                    return False\n            return True\n    elif is_Tuple_ellipsis(superclass):\n        # only superclass has ellipsis\n        if len(super_args)-1 > len(sub_args):\n            return False\n        for i in range(len(super_args)-1):\n            if not _issubclass(sub_args[i], super_args[i], bound_Generic, bound_typevars,\n                    bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                return False\n        for i in range(len(super_args), len(sub_args)):\n            # Check remaining sub args against the ellipsis type\n            if not _issubclass(sub_args[i], super_args[-1], bound_Generic, bound_typevars,\n                    bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                return False\n        return True\n    else:\n        # none has ellipsis, so strict length check\n        return (len(super_args) == len(sub_args) and\n                all(_issubclass(x, p, bound_Generic, bound_typevars,\n                bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n                for x, p in zip(sub_args, super_args)))", "response": "Helper for _issubclass, a.k.a pytypes.issubtype."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\naccesses this via ``pytypes.is_subtype``. Works like ``issubclass``, but supports PEP 484 style types from ``typing`` module. subclass : type The type to check for being a subtype of ``superclass``. superclass : type The type to check for being a supertype of ``subclass``. bound_Generic : Optional[Generic] A type object holding values for unbound typevars occurring in ``subclass`` or ``superclass``. Default: None If subclass or superclass contains unbound ``TypeVar``s and ``bound_Generic`` is provided, this function attempts to retrieve corresponding values for the unbound ``TypeVar``s from ``bound_Generic``. In collision case with ``bound_typevars`` the value from ``bound_Generic`` if preferred. bound_typevars : Optional[Dict[typing.TypeVar, type]] A dictionary holding values for unbound typevars occurring in ``subclass`` or ``superclass``. Default: {} Depending on ``bound_typevars_readonly`` pytypes can also bind values to typevars as needed. This is done by inserting according mappings into this dictionary. This can e.g. be useful to infer values for ``TypeVar``s or to consistently check a set of ``TypeVar``s across multiple calls, e.g. when checking all arguments of a function call. In collision case with ``bound_Generic`` the value from ``bound_Generic`` if preferred. bound_typevars_readonly : bool Defines if pytypes is allowed to write into the ``bound_typevars`` dictionary. Default: True If set to False, pytypes cannot assign values to ``TypeVar``s, but only checks regarding values already present in ``bound_typevars`` or ``bound_Generic``. follow_fwd_refs : bool Defines if ``_ForwardRef``s should be explored. Default: True If this is set to ``False`` and a ``_ForwardRef`` is encountered, pytypes aborts the check raising a ForwardRefError. _recursion_check : Optional[Dict[type, Set[type]]] Internally used for recursion checks. Default: None If ``Union``s and ``_ForwardRef``s occur in the same type, recursions can occur. As soon as a ``_ForwardRef`` is encountered, pytypes automatically creates this dictionary and continues in recursion-proof manner.", "response": "def _issubclass(subclass, superclass, bound_Generic=None, bound_typevars=None,\n            bound_typevars_readonly=False, follow_fwd_refs=True, _recursion_check=None):\n    \"\"\"Access this via ``pytypes.is_subtype``.\n    Works like ``issubclass``, but supports PEP 484 style types from ``typing`` module.\n\n    subclass : type\n    The type to check for being a subtype of ``superclass``.\n\n    superclass : type\n    The type to check for being a supertype of ``subclass``.\n\n    bound_Generic : Optional[Generic]\n    A type object holding values for unbound typevars occurring in ``subclass`` or ``superclass``.\n    Default: None\n    If subclass or superclass contains unbound ``TypeVar``s and ``bound_Generic`` is\n    provided, this function attempts to retrieve corresponding values for the\n    unbound ``TypeVar``s from ``bound_Generic``.\n    In collision case with ``bound_typevars`` the value from ``bound_Generic`` if preferred.\n\n    bound_typevars : Optional[Dict[typing.TypeVar, type]]\n    A dictionary holding values for unbound typevars occurring in ``subclass`` or ``superclass``.\n    Default: {}\n    Depending on ``bound_typevars_readonly`` pytypes can also bind values to typevars as needed.\n    This is done by inserting according mappings into this dictionary. This can e.g. be useful to\n    infer values for ``TypeVar``s or to consistently check a set of ``TypeVar``s across multiple\n    calls, e.g. when checking all arguments of a function call.\n    In collision case with ``bound_Generic`` the value from ``bound_Generic`` if preferred.\n\n    bound_typevars_readonly : bool\n    Defines if pytypes is allowed to write into the ``bound_typevars`` dictionary.\n    Default: True\n    If set to False, pytypes cannot assign values to ``TypeVar``s, but only checks regarding\n    values already present in ``bound_typevars`` or ``bound_Generic``.\n\n    follow_fwd_refs : bool\n    Defines if ``_ForwardRef``s should be explored.\n    Default: True\n    If this is set to ``False`` and a ``_ForwardRef`` is encountered, pytypes aborts the check\n    raising a ForwardRefError.\n\n    _recursion_check : Optional[Dict[type, Set[type]]]\n    Internally used for recursion checks.\n    Default: None\n    If ``Union``s and ``_ForwardRef``s occur in the same type, recursions can occur. As soon as\n    a ``_ForwardRef`` is encountered, pytypes automatically creates this dictionary and\n    continues in recursion-proof manner.\n    \"\"\"\n    if bound_typevars is None:\n        bound_typevars = {}\n    if superclass is Any:\n        return True\n    if subclass == superclass:\n        return True\n    if subclass is Any:\n        return superclass is Any\n    if isinstance(subclass, ForwardRef) or isinstance(superclass, ForwardRef):\n        if not follow_fwd_refs:\n            raise pytypes.ForwardRefError(\n                    \"ForwardRef encountered, but follow_fwd_refs is False: '%s'\\n%s\"%\n                    ((subclass if isinstance(subclass, ForwardRef) else superclass)\n                    .__forward_arg__,\n                    \"Retry with follow_fwd_refs=True.\"))\n        # Now that forward refs are in the game, we must continue in recursion-proof manner:\n        if _recursion_check is None:\n            _recursion_check = {superclass: {subclass}}\n        elif superclass in _recursion_check:\n            if subclass in _recursion_check[superclass]:\n                # recursion detected\n                return False\n            else:\n                _recursion_check[superclass].add(subclass)\n        else:\n            _recursion_check[superclass] = {subclass}\n        if isinstance(subclass, ForwardRef):\n            if not subclass.__forward_evaluated__:\n                raise pytypes.ForwardRefError(\"ForwardRef in subclass not evaluated: '%s'\\n%s\"%\n                        (subclass.__forward_arg__, \"Use pytypes.resolve_fw_decl\"))\n            else:\n                return _issubclass(subclass.__forward_value__, superclass,\n                        bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n        else: # isinstance(superclass, ForwardRef)\n            if not superclass.__forward_evaluated__:\n                raise pytypes.ForwardRefError(\"ForwardRef in superclass not evaluated: '%s'\\n%s\"%\n                        (superclass.__forward_arg__, \"Use pytypes.resolve_fw_decl\"))\n            else:\n                return _issubclass(subclass, superclass.__forward_value__,\n                        bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n    if pytypes.apply_numeric_tower:\n        if superclass is float and subclass is int:\n            return True\n        elif superclass is complex and \\\n                (subclass is int or subclass is float):\n            return True\n    if superclass in _extra_dict:\n        superclass = _extra_dict[superclass]\n    try:\n        if _issubclass_2(subclass, Empty, bound_Generic, bound_typevars,\n                    bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n            for empty_target in [Container, Sized, Iterable]:\n                # We cannot simply use Union[Container, Sized, Iterable] as empty_target\n                # because of implementation detail behavior of _issubclass_2.\n                # It would e.g. cause false negative result of\n                # is_subtype(Empty[Dict], Empty[Container])\n                try:\n                    if _issubclass_2(superclass.__origin__, empty_target,\n                            bound_Generic, bound_typevars,\n                            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                        return _issubclass_2(subclass.__args__[0], superclass.__origin__,\n                                bound_Generic, bound_typevars,\n                                bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n                except: pass\n                if _issubclass_2(superclass, empty_target,\n                        bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                    return _issubclass_2(subclass.__args__[0], superclass,\n                            bound_Generic, bound_typevars,\n                            bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n    except: pass\n    try:\n        if _issubclass_2(superclass, Empty, bound_Generic, bound_typevars,\n                    bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n            for empty_target in [Container, Sized, Iterable]:\n                # We cannot simply use Union[Container, Sized, Iterable] as empty_target\n                # because of implementation detail behavior of _issubclass_2.\n                try:\n                    if _issubclass_2(subclass.__origin__, empty_target,\n                            bound_Generic, bound_typevars,\n                            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                        return _issubclass_2(subclass.__origin__, superclass.__args__[0],\n                                bound_Generic, bound_typevars,\n                                bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n                except: pass\n                if _issubclass_2(subclass, empty_target, bound_Generic, bound_typevars,\n                            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                    return _issubclass_2(subclass, superclass.__args__[0],\n                            bound_Generic, bound_typevars,\n                            bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n    except: pass\n    if isinstance(superclass, TypeVar):\n        if not superclass.__bound__ is None:\n            if not _issubclass(subclass, superclass.__bound__, bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                return False\n        if not bound_typevars is None:\n            try:\n                if superclass.__contravariant__:\n                    return _issubclass(bound_typevars[superclass], subclass, bound_Generic,\n                            bound_typevars, bound_typevars_readonly, follow_fwd_refs,\n                            _recursion_check)\n                elif superclass.__covariant__:\n                    return _issubclass(subclass, bound_typevars[superclass], bound_Generic,\n                            bound_typevars, bound_typevars_readonly, follow_fwd_refs,\n                            _recursion_check)\n                else:\n                    return _issubclass(bound_typevars[superclass], subclass, bound_Generic,\n                            bound_typevars, bound_typevars_readonly, follow_fwd_refs,\n                            _recursion_check) and \\\n                            _issubclass(subclass, bound_typevars[superclass], bound_Generic,\n                            bound_typevars, bound_typevars_readonly, follow_fwd_refs,\n                            _recursion_check)\n            except:\n                pass\n        if not bound_Generic is None:\n            superclass = get_arg_for_TypeVar(superclass, bound_Generic)\n            if not superclass is None:\n                return _issubclass(subclass, superclass, bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n        if not bound_typevars is None:\n            if bound_typevars_readonly:\n                return False\n            else:\n                # bind it...\n                bound_typevars[superclass] = subclass\n                return True\n        return False\n    if isinstance(subclass, TypeVar):\n        if not bound_typevars is None:\n            try:\n                return _issubclass(bound_typevars[subclass], superclass, bound_Generic,\n                        bound_typevars, bound_typevars_readonly, follow_fwd_refs,\n                        _recursion_check)\n            except:\n                pass\n        if not bound_Generic is None:\n            subclass = get_arg_for_TypeVar(subclass, bound_Generic)\n            if not subclass is None:\n                return _issubclass(subclass, superclass, bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n        if not subclass.__bound__ is None:\n            return _issubclass(subclass.__bound__, superclass, bound_Generic, bound_typevars,\n                    bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n        return False\n    res = _issubclass_2(subclass, superclass, bound_Generic, bound_typevars,\n            bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _isinstance(obj, cls, bound_Generic=None, bound_typevars=None,\n            bound_typevars_readonly=False, follow_fwd_refs=True, _recursion_check=None):\n    \"\"\"Access this via ``pytypes.is_of_type``.\n    Works like ``isinstance``, but supports PEP 484 style types from ``typing`` module.\n\n    obj : Any\n    The object to check for being an instance of ``cls``.\n\n    cls : type\n    The type to check for ``obj`` being an instance of.\n\n    bound_Generic : Optional[Generic]\n    A type object holding values for unbound typevars occurring in ``cls``.\n    Default: None\n    If ``cls`` contains unbound ``TypeVar``s and ``bound_Generic`` is provided, this function\n    attempts to retrieve corresponding values for the unbound ``TypeVar``s from ``bound_Generic``.\n    In collision case with ``bound_typevars`` the value from ``bound_Generic`` if preferred.\n\n    bound_typevars : Optional[Dict[typing.TypeVar, type]]\n    A dictionary holding values for unbound typevars occurring in ``cls``.\n    Default: {}\n    Depending on ``bound_typevars_readonly`` pytypes can also bind values to typevars as needed.\n    This is done by inserting according mappings into this dictionary. This can e.g. be useful to\n    infer values for ``TypeVar``s or to consistently check a set of ``TypeVar``s across multiple\n    calls, e.g. when checking all arguments of a function call.\n    In collision case with ``bound_Generic`` the value from ``bound_Generic`` if preferred.\n\n    bound_typevars_readonly : bool\n    Defines if pytypes is allowed to write into the ``bound_typevars`` dictionary.\n    Default: True\n    If set to False, pytypes cannot assign values to ``TypeVar``s, but only checks regarding\n    values already present in ``bound_typevars`` or ``bound_Generic``.\n\n    follow_fwd_refs : bool\n    Defines if ``ForwardRef``s should be explored.\n    Default: True\n    If this is set to ``False`` and a ``ForwardRef`` is encountered, pytypes aborts the check\n    raising a ForwardRefError.\n\n    _recursion_check : Optional[Dict[type, Set[type]]]\n    Internally used for recursion checks.\n    Default: None\n    If ``Union``s and ``ForwardRef``s occur in the same type, recursions can occur. As soon as\n    a ``ForwardRef`` is encountered, pytypes automatically creates this dictionary and\n    continues in recursion-proof manner.\n    \"\"\"\n    if bound_typevars is None:\n        bound_typevars = {}\n    # Special treatment if cls is Iterable[...]\n    if is_Generic(cls) and cls.__origin__ is typing.Iterable:\n        if not is_iterable(obj):\n            return False\n        itp = get_iterable_itemtype(obj)\n        if itp is None:\n            return not pytypes.check_iterables\n        else:\n            return _issubclass(itp, cls.__args__[0], bound_Generic, bound_typevars,\n                    bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n    if is_Callable(cls):\n        return _isinstance_Callable(obj, cls, bound_Generic, bound_typevars,\n                bound_typevars_readonly, follow_fwd_refs, _recursion_check)\n    return _issubclass(deep_type(obj), cls, bound_Generic, bound_typevars,\n            bound_typevars_readonly, follow_fwd_refs, _recursion_check)", "response": "Internal function to check if an object is an instance of a class."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a typechecking wrapper around a Python 3 style generator object.", "response": "def generator_checker_py3(gen, gen_type, bound_Generic, bound_typevars,\n            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n    \"\"\"Builds a typechecking wrapper around a Python 3 style generator object.\n    \"\"\"\n    initialized = False\n    sn = None\n    try:\n        while True:\n            a = gen.send(sn)\n            if initialized or not a is None:\n                if not gen_type.__args__[0] is Any and \\\n                        not _isinstance(a, gen_type.__args__[0], bound_Generic, bound_typevars,\n                                bound_typevars_readonly, follow_fwd_refs,\n                                _recursion_check):\n                    tpa = deep_type(a)\n                    msg = _make_generator_error_message(deep_type(a), gen, gen_type.__args__[0],\n                            'has incompatible yield type')\n                    _raise_typecheck_error(msg, True, a, tpa, gen_type.__args__[0])\n# \t\t\t\t\traise pytypes.ReturnTypeError(_make_generator_error_message(deep_type(a), gen,\n# \t\t\t\t\t\t\tgen_type.__args__[0], 'has incompatible yield type'))\n            initialized = True\n            sn = yield a\n            if not gen_type.__args__[1] is Any and \\\n                    not _isinstance(sn, gen_type.__args__[1], bound_Generic, bound_typevars,\n                            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                tpsn = deep_type(sn)\n                msg = _make_generator_error_message(tpsn, gen, gen_type.__args__[1],\n                        'has incompatible send type')\n                _raise_typecheck_error(msg, False, sn, tpsn, gen_type.__args__[1])\n# \t\t\t\traise pytypes.InputTypeError(_make_generator_error_message(deep_type(sn), gen,\n# \t\t\t\t\t\tgen_type.__args__[1], 'has incompatible send type'))\n    except StopIteration as st:\n        # Python 3:\n        # todo: Check if st.value is always defined (i.e. as None if not present)\n        if not gen_type.__args__[2] is Any and \\\n                not _isinstance(st.value, gen_type.__args__[2], bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n            tpst = deep_type(st.value)\n            msg = _make_generator_error_message(tpst, gen, gen_type.__args__[2],\n                    'has incompatible return type')\n            _raise_typecheck_error(msg, True, st.value, tpst, gen_type.__args__[2])\n# \t\t\traise pytypes.ReturnTypeError(_make_generator_error_message(sttp, gen,\n# \t\t\t\t\tgen_type.__args__[2], 'has incompatible return type'))\n        raise st"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generator_checker_py2(gen, gen_type, bound_Generic, bound_typevars,\n            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n    \"\"\"Builds a typechecking wrapper around a Python 2 style generator object.\n    \"\"\"\n    initialized = False\n    sn = None\n    while True:\n        a = gen.send(sn)\n        if initialized or not a is None:\n            if not gen_type.__args__[0] is Any and \\\n                    not _isinstance(a, gen_type.__args__[0], bound_Generic, bound_typevars,\n                            bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n                tpa = deep_type(a)\n                msg = _make_generator_error_message(tpa, gen, gen_type.__args__[0],\n                        'has incompatible yield type')\n                _raise_typecheck_error(msg, True, a, tpa, gen_type.__args__[0])\n# \t\t\t\traise pytypes.ReturnTypeError(_make_generator_error_message(tpa, gen,\n# \t\t\t\t\t\tgen_type.__args__[0], 'has incompatible yield type'))\n        initialized  = True\n        sn = yield a\n        if not gen_type.__args__[1] is Any and \\\n                not _isinstance(sn, gen_type.__args__[1], bound_Generic, bound_typevars,\n                        bound_typevars_readonly, follow_fwd_refs, _recursion_check):\n            tpsn = deep_type(sn)\n            msg = _make_generator_error_message(tpsn, gen, gen_type.__args__[1],\n                    'has incompatible send type')\n            _raise_typecheck_error(msg, False, sn, tpsn, gen_type.__args__[1])", "response": "Creates a typechecking wrapper around a Python 2 style generator object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef annotations_func(func):\n    if not has_type_hints(func):\n        # What about defaults?\n        func.__annotations__ =  {}\n    func.__annotations__ = _get_type_hints(func,\n            infer_defaults = False)\n    return func", "response": "Works like annotations but is only applicable to functions methods and properties."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwork like annotations but is only applicable to classes.", "response": "def annotations_class(cls):\n    \"\"\"Works like annotations, but is only applicable to classes.\n    \"\"\"\n    assert(isclass(cls))\n    # To play it safe we avoid to modify the dict while iterating over it,\n    # so we previously cache keys.\n    # For this we don't use keys() because of Python 3.\n    # Todo: Better use inspect.getmembers here\n    keys = [key for key in cls.__dict__]\n    for key in keys:\n        memb = cls.__dict__[key]\n        if _check_as_func(memb):\n            annotations_func(memb)\n        elif isclass(memb):\n            annotations_class(memb)\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef annotations_module(md):\n    if isinstance(md, str):\n        if md in sys.modules:\n            md = sys.modules[md]\n            if md is None:\n                return md\n        elif md in pytypes.typechecker._pending_modules:\n            # if import is pending, we just store this call for later\n            pytypes.typechecker._pending_modules[md].append(annotations_module)\n            return md\n    assert(ismodule(md))\n    if md.__name__ in pytypes.typechecker._pending_modules:\n            # if import is pending, we just store this call for later\n            pytypes.typechecker._pending_modules[md.__name__].append(annotations_module)\n            # we already process the module now as far as possible for its internal use\n            # todo: Issue warning here that not the whole module might be covered yet\n    if md.__name__ in _annotated_modules and \\\n            _annotated_modules[md.__name__] == len(md.__dict__):\n        return md\n    # To play it safe we avoid to modify the dict while iterating over it,\n    # so we previously cache keys.\n    # For this we don't use keys() because of Python 3.\n    # Todo: Better use inspect.getmembers here\n    keys = [key for key in md.__dict__]\n    for key in keys:\n        memb = md.__dict__[key]\n        if _check_as_func(memb) and memb.__module__ == md.__name__:\n            annotations_func(memb)\n        elif isclass(memb) and memb.__module__ == md.__name__:\n            annotations_class(memb)\n    if not md.__name__ in pytypes.typechecker._pending_modules:\n        _annotated_modules[md.__name__] = len(md.__dict__)\n    return md", "response": "Works like annotations but is only applicable to modules."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simplify_for_Union(type_list):\n    i = 0\n    while i < len(type_list):\n        j = 0\n        while j < i:\n            if _issubclass(type_list[j], type_list[i]):\n                del type_list[j]\n                i -= 1\n            else:\n                j += 1\n        j = i+1\n        while j < len(type_list):\n            if _issubclass(type_list[j], type_list[i]):\n                del type_list[j]\n            else:\n                j += 1\n        i += 1", "response": "Simplifies the given list of types for a single element in the order they appear in the union."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _preprocess_typecheck(argSig, argspecs, slf_or_clsm = False):\n    # todo: Maybe move also slf-logic here\n    vargs = argspecs.varargs\n    try:\n        kw = argspecs.keywords\n    except AttributeError:\n        kw = argspecs.varkw\n    try:\n        kwonly = argspecs.kwonlyargs\n    except AttributeError:\n        kwonly = None\n    if not vargs is None or not kw is None:\n        arg_type_lst = list(get_Tuple_params(argSig))\n        if not vargs is None:\n            vargs_pos = (len(argspecs.args)-1) \\\n                    if slf_or_clsm else len(argspecs.args)\n            # IndexErrors in this section indicate that a child-method was\n            # checked against a parent's type-info with the child featuring\n            # a more wider type on signature level (e.g. adding vargs)\n            try:\n                vargs_type = typing.Sequence[arg_type_lst[vargs_pos]]\n            except IndexError:\n                vargs_type = typing.Sequence[typing.Any]\n            try:\n                arg_type_lst[vargs_pos] = vargs_type\n            except IndexError:\n                arg_type_lst.append(vargs_type)\n        if not kw is None:\n            kw_pos = len(argspecs.args)\n            if slf_or_clsm:\n                kw_pos -= 1\n            if not vargs is None:\n                kw_pos += 1\n            if not kwonly is None:\n                kw_pos += len(kwonly)\n            try:\n                kw_type = typing.Dict[str, arg_type_lst[kw_pos]]\n            except IndexError:\n                kw_type = typing.Dict[str, typing.Any]\n            try:\n                arg_type_lst[kw_pos] = kw_type\n            except IndexError:\n                arg_type_lst.append(kw_type)\n        return typing.Tuple[tuple(arg_type_lst)]\n    else:\n        return argSig", "response": "Preprocess typecheck for a PEP 484 style type - tuple with types for varargs and kwonlyargs and or clsm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restore_profiler():\n    idn = threading.current_thread().ident\n    if not sys.getprofile() is None:\n        warn(\"restore_profiler: Current profile is not None!\")\n    if not idn in _saved_profilers:\n        warn(\"restore_profiler: No saved profiler for calling thread!\")\n    else:\n        sys.setprofile(_saved_profilers[idn])\n        del _saved_profilers[idn]", "response": "Restore the current profile of the current thread."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log_type(args_kw, ret, func, slf=False, prop_getter=False, clss=None, argspecs=None,\n            args_kw_type=None, ret_type = None):\n    \"\"\"Stores information of a function or method call into a cache, so pytypes can\n    create a PEP 484 stubfile from this information later on (see dump_cache).\n    \"\"\"\n    if args_kw_type is None:\n        args_kw_type = deep_type(args_kw)\n    if ret_type is None:\n        ret_type = deep_type(ret)\n    if argspecs is None:\n        argspecs = getargspecs(func)\n    node = _register_logged_func(func, slf, prop_getter, clss, argspecs)\n    node.add_observation(args_kw_type, ret_type)\n    \n    md = util.getmodule_for_member(func, prop_getter)\n    if not md.__name__ in _module_file_map:\n        _module_file_map[md.__name__] = md.__file__\n\n    if clss is None:\n        try:\n            clss = util.get_class_that_defined_method(func)\n        except ValueError:\n            pass\n    if not clss is None and not clss in _member_line_map:\n        _member_line_map[clss] = findsource(clss)[1]", "response": "Stores information of a function or method call into a cache so pytypes can be used later on."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncombining a list of Tuple types into one.", "response": "def combine_argtype(observations):\n    \"\"\"Combines a list of Tuple types into one.\n    Basically these are combined element wise into a Union with some\n    additional unification effort (e.g. can apply PEP 484 style numeric tower).\n    \"\"\"\n    assert len(observations) > 0\n    assert is_Tuple(observations[0])\n    if len(observations) > 1:\n        prms = [get_Tuple_params(observations[0])]\n        ln = len(prms[0])\n        for obs in observations[1:]:\n            assert is_Tuple(obs)\n            prms.append(get_Tuple_params(obs))\n            assert len(prms[-1]) == ln\n        if simplify:\n            prms = map(list, zip(*prms))\n            if not isinstance(prms, list):\n                # special care for Python 3\n                prms = list(prms)\n            for type_list in prms:\n                simplify_for_Union(type_list)\n            prms = map(tuple, prms)\n        else:\n            prms = map(tuple, zip(*prms))\n        prms = map(Union.__getitem__, prms)\n        return Tuple[tuple(prms)]\n    else:\n        return observations[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncombine a list of types into one.", "response": "def combine_type(observations):\n    \"\"\"Combines a list of types into one.\n    Basically these are combined into a Union with some\n    additional unification effort (e.g. can apply PEP 484 style numeric tower).\n    \"\"\"\n    assert len(observations) > 0\n    if len(observations) == 1:\n        return observations[0]\n    else:\n        if simplify:\n            simplify_for_Union(observations)\n        return Union[tuple(observations)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite cached observations by @typelogged into stubfiles.", "response": "def dump_cache(path=None, python2=False, suffix=None):\n    \"\"\"Writes cached observations by @typelogged into stubfiles.\n    Files will be created in the directory provided as 'path'; overwrites\n    existing files without notice.\n    Uses 'pyi2' suffix if 'python2' flag is given else 'pyi'. Resulting\n    files will be Python 2.7 compilant accordingly.\n    \"\"\"\n    typelogging_enabled_tmp = pytypes.typelogging_enabled\n    pytypes.typelogging_enabled = False\n    if suffix is None:\n        suffix = 'pyi2' if python2 else 'pyi'\n    if path is None:\n        path = pytypes.default_typelogger_path\n    modules = {}\n    for key in _member_cache:\n        node = _member_cache[key]\n        mname = node.get_modulename()\n        if not mname in modules:\n            mnode = _module_node(mname)\n            modules[mname] = mnode\n        else:\n            mnode = modules[mname]\n        mnode.append(node)\n    for module in modules:\n        _dump_module(modules[module], path, python2, suffix)\n    pytypes.typelogging_enabled = typelogging_enabled_tmp"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_indentation(func):\n    src_lines = getsourcelines(func)[0]\n    for line in src_lines:\n        if not (line.startswith('@') or line.startswith('def') or line.lstrip().startswith('#')):\n            return line[:len(line) - len(line.lstrip())]\n    return pytypes.default_indent", "response": "Extracts a function s indentation as a string"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwork like typelogged but is only applicable to functions methods and properties.", "response": "def typelogged_func(func):\n    \"\"\"Works like typelogged, but is only applicable to functions,\n    methods and properties.\n    \"\"\"\n    if not pytypes.typelogging_enabled:\n        return func\n    if hasattr(func, 'do_logging'):\n        func.do_logging = True\n        return func\n    elif hasattr(func, 'do_typecheck'):\n        # actually shouldn't happen\n        return _typeinspect_func(func, func.do_typecheck, True)\n    else:\n        return _typeinspect_func(func, False, True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef typelogged_class(cls):\n    if not pytypes.typelogging_enabled:\n        return cls\n    assert(isclass(cls))\n    # To play it safe we avoid to modify the dict while iterating over it,\n    # so we previously cache keys.\n    # For this we don't use keys() because of Python 3.\n    # Todo: Better use inspect.getmembers here\n    keys = [key for key in cls.__dict__]\n    for key in keys:\n        memb = cls.__dict__[key]\n        if _check_as_func(memb):\n            setattr(cls, key, typelogged_func(memb))\n        elif isclass(memb):\n            typelogged_class(memb)\n    return cls", "response": "Works like typelogged but is only applicable to classes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef typelogged_module(md):\n    if not pytypes.typelogging_enabled:\n        return md\n    if isinstance(md, str):\n        if md in sys.modules:\n            md = sys.modules[md]\n            if md is None:\n                return md\n        elif md in pytypes.typechecker._pending_modules:\n            # if import is pending, we just store this call for later\n            pytypes.typechecker._pending_modules[md].append(typelogged_module)\n            return md\n    assert(ismodule(md))\n    if md.__name__ in pytypes.typechecker._pending_modules:\n            # if import is pending, we just store this call for later\n            pytypes.typechecker._pending_modules[md.__name__].append(typelogged_module)\n            # we already process the module now as far as possible for its internal use\n            # todo: Issue warning here that not the whole module might be covered yet\n    assert(ismodule(md))\n    if md.__name__ in _fully_typelogged_modules and \\\n            _fully_typelogged_modules[md.__name__] == len(md.__dict__):\n        return md\n    # To play it safe we avoid to modify the dict while iterating over it,\n    # so we previously cache keys.\n    # For this we don't use keys() because of Python 3.\n    # Todo: Better use inspect.getmembers here\n    keys = [key for key in md.__dict__]\n    for key in keys:\n        memb = md.__dict__[key]\n        if _check_as_func(memb) and memb.__module__ == md.__name__:\n            setattr(md, key, typelogged_func(memb))\n        elif isclass(memb) and memb.__module__ == md.__name__:\n            typelogged_class(memb)\n    if not md.__name__ in pytypes.typechecker._pending_modules:\n        _fully_typelogged_modules[md.__name__] = len(md.__dict__)\n    return md", "response": "Works like typelogged but is only applicable to modules by explicit call."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef enable_global_typechecked_decorator(flag = True, retrospective = True):\n    global global_typechecked_decorator\n    global_typechecked_decorator = flag\n    if import_hook_enabled:\n        _install_import_hook()\n    if global_typechecked_decorator and retrospective:\n        _catch_up_global_typechecked_decorator()\n    return global_typechecked_decorator", "response": "Enables or disables global typechecking mode via decorators."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enable_global_auto_override_decorator(flag = True, retrospective = True):\n    global global_auto_override_decorator\n    global_auto_override_decorator = flag\n    if import_hook_enabled:\n        _install_import_hook()\n    if global_auto_override_decorator and retrospective:\n        _catch_up_global_auto_override_decorator()\n    return global_auto_override_decorator", "response": "Enables or disables global auto_override mode via decorators."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling or disables global annotation mode via decorators.", "response": "def enable_global_annotations_decorator(flag = True, retrospective = True):\n    \"\"\"Enables or disables global annotation mode via decorators.\n    See flag global_annotations_decorator.\n    In contrast to setting the flag directly, this function provides\n    a retrospective option. If retrospective is true, this will also\n    affect already imported modules, not only future imports.\n    \"\"\"\n    global global_annotations_decorator\n    global_annotations_decorator = flag\n    if import_hook_enabled:\n        _install_import_hook()\n    if global_annotations_decorator and retrospective:\n        _catch_up_global_annotations_decorator()\n    return global_annotations_decorator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nenable or disables global typelog mode via decorators.", "response": "def enable_global_typelogged_decorator(flag = True, retrospective = True):\n    \"\"\"Enables or disables global typelog mode via decorators.\n    See flag global_typelogged_decorator.\n    In contrast to setting the flag directly, this function provides\n    a retrospective option. If retrospective is true, this will also\n    affect already imported modules, not only future imports.\n    \"\"\"\n    global global_typelogged_decorator\n    global_typelogged_decorator = flag\n    if import_hook_enabled:\n        _install_import_hook()\n    if global_typelogged_decorator and retrospective:\n        _catch_up_global_typelogged_decorator()\n    return global_typelogged_decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nenables or disables global typechecking mode via a profiler.", "response": "def enable_global_typechecked_profiler(flag = True):\n    \"\"\"Enables or disables global typechecking mode via a profiler.\n    See flag global_typechecked_profiler.\n    Does not work if checking_enabled is false.\n    \"\"\"\n    global global_typechecked_profiler, _global_type_agent, global_typelogged_profiler\n    global_typechecked_profiler = flag\n    if flag and checking_enabled:\n        if _global_type_agent is None:\n            _global_type_agent = TypeAgent()\n            _global_type_agent.start()\n        elif not _global_type_agent.active:\n            _global_type_agent.start()\n    elif not flag and not global_typelogged_profiler and \\\n            not _global_type_agent is None and _global_type_agent.active:\n        _global_type_agent.stop()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenables or disables global typelogging mode via a profiler.", "response": "def enable_global_typelogged_profiler(flag = True):\n    \"\"\"Enables or disables global typelogging mode via a profiler.\n    See flag global_typelogged_profiler.\n    Does not work if typelogging_enabled is false.\n    \"\"\"\n    global global_typelogged_profiler, _global_type_agent, global_typechecked_profiler\n    global_typelogged_profiler = flag\n    if flag and typelogging_enabled:\n        if _global_type_agent is None:\n            _global_type_agent = TypeAgent()\n            _global_type_agent.start()\n        elif not _global_type_agent.active:\n            _global_type_agent.start()\n    elif not flag and not global_typechecked_profiler and \\\n            not _global_type_agent is None and _global_type_agent.active:\n        _global_type_agent.stop()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _detect_issue351():\n    class Tuple(typing.Generic[typing.T]):\n        pass\n\n    res = Tuple[str] == typing.Tuple[str]\n    del Tuple\n    return res", "response": "Detect if github. com / python / typing / issues / 351 applies\n    to the installed typing - version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef typechecked_func(func, force = False, argType = None, resType = None, prop_getter = False):\n    if not pytypes.checking_enabled and not pytypes.do_logging_in_typechecked:\n        return func\n    assert(_check_as_func(func))\n    if not force and is_no_type_check(func):\n        return func\n    if hasattr(func, 'do_typecheck'):\n        func.do_typecheck = True\n        return func\n    elif hasattr(func, 'do_logging'):\n        # actually shouldn't happen\n        return _typeinspect_func(func, True, func.do_logging, argType, resType, prop_getter)\n    else:\n        return _typeinspect_func(func, True, False, argType, resType, prop_getter)", "response": "Works like typechecked but is only applicable to functions methods and properties."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwork like typechecked but is only applicable to classes.", "response": "def typechecked_class(cls, force = False, force_recursive = False):\n    \"\"\"Works like typechecked, but is only applicable to classes.\n    \"\"\"\n    return _typechecked_class(cls, set(), force, force_recursive)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef typechecked_module(md, force_recursive = False):\n    if not pytypes.checking_enabled:\n        return md\n    if isinstance(md, str):\n        if md in sys.modules:\n            md = sys.modules[md]\n            if md is None:\n                return md\n        elif md in _pending_modules:\n            # if import is pending, we just store this call for later\n            _pending_modules[md].append(lambda t: typechecked_module(t, True))\n            return md\n    assert(ismodule(md))\n    if md.__name__ in _pending_modules:\n            # if import is pending, we just store this call for later\n            _pending_modules[md.__name__].append(lambda t: typechecked_module(t, True))\n            # we already process the module now as far as possible for its internal use\n            # todo: Issue warning here that not the whole module might be covered yet\n    if md.__name__ in _fully_typechecked_modules and \\\n            _fully_typechecked_modules[md.__name__] == len(md.__dict__):\n        return md\n    # To play it safe we avoid to modify the dict while iterating over it,\n    # so we previously cache keys.\n    # For this we don't use keys() because of Python 3.\n    # Todo: Better use inspect.getmembers here\n    keys = [key for key in md.__dict__]\n    for key in keys:\n        memb = md.__dict__[key]\n        if force_recursive or not is_no_type_check(memb) and hasattr(memb, '__module__'):\n            if _check_as_func(memb) and memb.__module__ == md.__name__ and \\\n                    has_type_hints(memb):\n                setattr(md, key, typechecked_func(memb, force_recursive))\n            elif isclass(memb) and memb.__module__ == md.__name__:\n                typechecked_class(memb, force_recursive, force_recursive)\n    if not md.__name__ in _pending_modules:\n        _fully_typechecked_modules[md.__name__] = len(md.__dict__)\n    return md", "response": "Works like typechecked but is only applicable to modules."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef auto_override_class(cls, force = False, force_recursive = False):\n    if not pytypes.checking_enabled:\n        return cls\n    assert(isclass(cls))\n    if not force and is_no_type_check(cls):\n        return cls\n    # To play it safe we avoid to modify the dict while iterating over it,\n    # so we previously cache keys.\n    # For this we don't use keys() because of Python 3.\n    # Todo: Better use inspect.getmembers here\n    keys = [key for key in cls.__dict__]\n    for key in keys:\n        memb = cls.__dict__[key]\n        if force_recursive or not is_no_type_check(memb):\n            if isfunction(memb) or ismethod(memb) or ismethoddescriptor(memb):\n                if util._has_base_method(memb, cls):\n                    setattr(cls, key, override(memb))\n            elif isclass(memb):\n                auto_override_class(memb, force_recursive, force_recursive)\n    return cls", "response": "A recursive version of auto_override that will recursively override the class s properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef auto_override_module(md, force_recursive = False):\n    if not pytypes.checking_enabled:\n        return md\n    if isinstance(md, str):\n        if md in sys.modules:\n            md = sys.modules[md]\n            if md is None:\n                return md\n        elif md in _pending_modules:\n            # if import is pending, we just store this call for later\n            _pending_modules[md].append(lambda t: auto_override_module(t, True))\n            return md\n    assert(ismodule(md))\n    if md.__name__ in _pending_modules:\n            # if import is pending, we just store this call for later\n            _pending_modules[md.__name__].append(lambda t: auto_override_module(t, True))\n            # we already process the module now as far as possible for its internal use\n            # todo: Issue warning here that not the whole module might be covered yet\n    if md.__name__ in _auto_override_modules and \\\n            _auto_override_modules[md.__name__] == len(md.__dict__):\n        return md\n    # To play it safe we avoid to modify the dict while iterating over it,\n    # so we previously cache keys.\n    # For this we don't use keys() because of Python 3.\n    # Todo: Better use inspect.getmembers here\n    keys = [key for key in md.__dict__]\n    for key in keys:\n        memb = md.__dict__[key]\n        if force_recursive or not is_no_type_check(memb):\n            if isclass(memb) and memb.__module__ == md.__name__:\n                auto_override_class(memb, force_recursive, force_recursive)\n    if not md.__name__ in _pending_modules:\n        _auto_override_modules[md.__name__] = len(md.__dict__)\n    return md", "response": "Works like auto_override but is only applicable to modules."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef auto_override(memb):\n    if type_util._check_as_func(memb):\n        return override(memb, True)\n    if isclass(memb):\n        return auto_override_class(memb)\n    if ismodule(memb):\n        return auto_override_module(memb, True)\n    if memb in sys.modules or memb in _pending_modules:\n        return auto_override_module(memb, True)\n    return memb", "response": "Decorator applicable to methods classes or modules."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwork like typing. no_type_check but also supports cases where does not have a type check.", "response": "def no_type_check(memb):\n    \"\"\"Works like typing.no_type_check, but also supports cases where\n    typing.no_type_check fails due to AttributeError. This can happen,\n    because typing.no_type_check wants to access __no_type_check__, which\n    might fail if e.g. a class is using slots or an object doesn't support\n    custom attributes.\n    \"\"\"\n    try:\n        return typing.no_type_check(memb)\n    except(AttributeError):\n        _not_type_checked.add(memb)\n        return memb"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_no_type_check(memb):\n    try:\n        return hasattr(memb, '__no_type_check__') and memb.__no_type_check__ or \\\n                memb in _not_type_checked\n    except TypeError:\n        return False", "response": "Checks if an object was annotated with no type check."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that the arguments passed in are of the correct type.", "response": "def check_argument_types(cllable = None, call_args = None, clss = None, caller_level = 0):\n    \"\"\"Can be called from within a function or method to apply typechecking to\n    the arguments that were passed in by the caller. Checking is applied w.r.t.\n    type hints of the function or method hosting the call to check_argument_types.\n    \"\"\"\n    return _check_caller_type(False, cllable, call_args, clss, caller_level+1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the value is of the correct type.", "response": "def check_return_type(value, cllable = None, clss = None, caller_level = 0):\n    \"\"\"Can be called from within a function or method to apply typechecking to\n    the value that is going to be returned. Checking is applied w.r.t.\n    type hints of the function or method hosting the call to check_return_type.\n    \"\"\"\n    return _check_caller_type(True, cllable, value, clss, caller_level+1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_diagram_from_csv(filepath, bpmn_diagram):\n        sequence_flows = bpmn_diagram.sequence_flows\n        process_elements_dict = bpmn_diagram.process_elements\n        diagram_attributes = bpmn_diagram.diagram_attributes\n        plane_attributes = bpmn_diagram.plane_attributes\n\n        process_dict = BpmnDiagramGraphCSVImport.import_csv_file_as_dict(filepath)\n\n        BpmnDiagramGraphCSVImport.populate_diagram_elements_dict(diagram_attributes)\n        BpmnDiagramGraphCSVImport.populate_process_elements_dict(process_elements_dict, process_dict)\n        BpmnDiagramGraphCSVImport.populate_plane_elements_dict(plane_attributes)\n\n        BpmnDiagramGraphCSVImport.import_nodes(process_dict, bpmn_diagram, sequence_flows)\n        BpmnDiagramGraphCSVImport.representation_adjustment(process_dict, bpmn_diagram, sequence_flows)", "response": "Reads an CSV file and maps it into inner representation of BPMN diagram."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_gateway_direction(self, value):\n        if value is None or not isinstance(value, str):\n            raise TypeError(\"GatewayDirection must be set to a String\")\n        elif value not in Gateway.__gateway_directions_list:\n            raise ValueError(\"GatewayDirection must be one of specified values: 'Unspecified', 'Converging', \"\n                             \"'Diverging', 'Mixed'\")\n        else:\n            self.__gateway_direction = value", "response": "Set the value of the gateway_direction field."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_name(self, value):\n        if value is None:\n            self.__name = value\n        elif not isinstance(value, str):\n            raise TypeError(\"Name must be set to a String\")\n        else:\n            self.__name = value", "response": "Setter for name field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading an XML file and maps it into inner representation of BPMN diagram.", "response": "def load_diagram_from_xml(filepath, bpmn_diagram):\n        \"\"\"\n        Reads an XML file from given filepath and maps it into inner representation of BPMN diagram.\n        Returns an instance of BPMNDiagramGraph class.\n\n        :param filepath: string with output filepath,\n        :param bpmn_diagram: an instance of BpmnDiagramGraph class.\n        \"\"\"\n        diagram_graph = bpmn_diagram.diagram_graph\n        sequence_flows = bpmn_diagram.sequence_flows\n        process_elements_dict = bpmn_diagram.process_elements\n        diagram_attributes = bpmn_diagram.diagram_attributes\n        plane_attributes = bpmn_diagram.plane_attributes\n        collaboration = bpmn_diagram.collaboration\n\n        document = BpmnDiagramGraphImport.read_xml_file(filepath)\n        # According to BPMN 2.0 XML Schema, there's only one 'BPMNDiagram' and 'BPMNPlane'\n        diagram_element = document.getElementsByTagNameNS(\"*\", \"BPMNDiagram\")[0]\n        plane_element = diagram_element.getElementsByTagNameNS(\"*\", \"BPMNPlane\")[0]\n        BpmnDiagramGraphImport.import_diagram_and_plane_attributes(diagram_attributes, plane_attributes,\n                                                                   diagram_element, plane_element)\n\n        BpmnDiagramGraphImport.import_process_elements(document, diagram_graph, sequence_flows, process_elements_dict,\n                                                       plane_element)\n\n        collaboration_element_list = document.getElementsByTagNameNS(\"*\", consts.Consts.collaboration)\n        if collaboration_element_list is not None and len(collaboration_element_list) > 0:\n            # Diagram has multiple pools and lanes\n            collaboration_element = collaboration_element_list[0]\n            BpmnDiagramGraphImport.import_collaboration_element(diagram_graph, collaboration_element, collaboration)\n\n        if consts.Consts.message_flows in collaboration:\n            message_flows = collaboration[consts.Consts.message_flows]\n        else:\n            message_flows = {}\n\n        participants = []\n        if consts.Consts.participants in collaboration:\n            participants = collaboration[consts.Consts.participants]\n\n        for element in utils.BpmnImportUtils.iterate_elements(plane_element):\n            if element.nodeType != element.TEXT_NODE:\n                tag_name = utils.BpmnImportUtils.remove_namespace_from_tag_name(element.tagName)\n                if tag_name == consts.Consts.bpmn_shape:\n                    BpmnDiagramGraphImport.import_shape_di(participants, diagram_graph, element)\n                elif tag_name == consts.Consts.bpmn_edge:\n                    BpmnDiagramGraphImport.import_flow_di(diagram_graph, sequence_flows, message_flows, element)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_collaboration_element(diagram_graph, collaboration_element, collaboration_dict):\n        collaboration_dict[consts.Consts.id] = collaboration_element.getAttribute(consts.Consts.id)\n        collaboration_dict[consts.Consts.participants] = {}\n        participants_dict = collaboration_dict[consts.Consts.participants]\n        collaboration_dict[consts.Consts.message_flows] = {}\n        message_flows_dict = collaboration_dict[consts.Consts.message_flows]\n\n        for element in utils.BpmnImportUtils.iterate_elements(collaboration_element):\n            if element.nodeType != element.TEXT_NODE:\n                tag_name = utils.BpmnImportUtils.remove_namespace_from_tag_name(element.tagName)\n                if tag_name == consts.Consts.participant:\n                    BpmnDiagramGraphImport.import_participant_element(diagram_graph, participants_dict, element)\n                elif tag_name == consts.Consts.message_flow:\n                    BpmnDiagramGraphImport.import_message_flow_to_graph(diagram_graph, message_flows_dict, element)", "response": "Method that imports information from collaboration element into collaboration_dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_participant_element(diagram_graph, participants_dictionary, participant_element):\n        participant_id = participant_element.getAttribute(consts.Consts.id)\n        name = participant_element.getAttribute(consts.Consts.name)\n        process_ref = participant_element.getAttribute(consts.Consts.process_ref)\n        if participant_element.getAttribute(consts.Consts.process_ref) == '':\n            diagram_graph.add_node(participant_id)\n            diagram_graph.node[participant_id][consts.Consts.type] = consts.Consts.participant\n            diagram_graph.node[participant_id][consts.Consts.process] = participant_id\n        participants_dictionary[participant_id] = {consts.Consts.name: name, consts.Consts.process_ref: process_ref}", "response": "Adds a participant element to the collaboration dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef import_diagram_and_plane_attributes(diagram_attributes, plane_attributes, diagram_element, plane_element):\n        diagram_attributes[consts.Consts.id] = diagram_element.getAttribute(consts.Consts.id)\n        diagram_attributes[consts.Consts.name] = diagram_element.getAttribute(consts.Consts.name) \\\n            if diagram_element.hasAttribute(consts.Consts.name) else \"\"\n\n        plane_attributes[consts.Consts.id] = plane_element.getAttribute(consts.Consts.id)\n        plane_attributes[consts.Consts.bpmn_element] = plane_element.getAttribute(consts.Consts.bpmn_element)", "response": "Adds attributes of BPMN diagram and plane elements to appropriate forest fields diagram_attributes and plane_attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_process_elements(document, diagram_graph, sequence_flows, process_elements_dict, plane_element):\n        for process_element in document.getElementsByTagNameNS(\"*\", consts.Consts.process):\n            BpmnDiagramGraphImport.import_process_element(process_elements_dict, process_element)\n\n            process_id = process_element.getAttribute(consts.Consts.id)\n            process_attributes = process_elements_dict[process_id]\n\n            lane_set_list = process_element.getElementsByTagNameNS(\"*\", consts.Consts.lane_set)\n            if lane_set_list is not None and len(lane_set_list) > 0:\n                # according to BPMN 2.0 XML Schema, there's at most one 'laneSet' element inside 'process'\n                lane_set = lane_set_list[0]\n                BpmnDiagramGraphImport.import_lane_set_element(process_attributes, lane_set, plane_element)\n\n            for element in utils.BpmnImportUtils.iterate_elements(process_element):\n                if element.nodeType != element.TEXT_NODE:\n                    tag_name = utils.BpmnImportUtils.remove_namespace_from_tag_name(element.tagName)\n                    BpmnDiagramGraphImport.__import_element_by_tag_name(diagram_graph, sequence_flows, process_id,\n                                                                        process_attributes, element, tag_name)\n\n            for flow in utils.BpmnImportUtils.iterate_elements(process_element):\n                if flow.nodeType != flow.TEXT_NODE:\n                    tag_name = utils.BpmnImportUtils.remove_namespace_from_tag_name(flow.tagName)\n                    if tag_name == consts.Consts.sequence_flow:\n                        BpmnDiagramGraphImport.import_sequence_flow_to_graph(diagram_graph, sequence_flows, process_id,\n                                                                             flow)", "response": "Method for importing all process elements in diagram."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_child_lane_set_element(child_lane_set_element, plane_element):\n        lane_set_id = child_lane_set_element.getAttribute(consts.Consts.id)\n        lanes_attr = {}\n        for element in utils.BpmnImportUtils.iterate_elements(child_lane_set_element):\n            if element.nodeType != element.TEXT_NODE:\n                tag_name = utils.BpmnImportUtils.remove_namespace_from_tag_name(element.tagName)\n                if tag_name == consts.Consts.lane:\n                    lane = element\n                    lane_id = lane.getAttribute(consts.Consts.id)\n                    lane_attr = BpmnDiagramGraphImport.import_lane_element(lane, plane_element)\n                    lanes_attr[lane_id] = lane_attr\n\n        child_lane_set_attr = {consts.Consts.id: lane_set_id, consts.Consts.lanes: lanes_attr}\n        return child_lane_set_attr", "response": "Method for importing childLaneSet element from diagram file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd attributes of BPMN process element to appropriate field process_attributes.", "response": "def import_process_element(process_elements_dict, process_element):\n        \"\"\"\n        Adds attributes of BPMN process element to appropriate field process_attributes.\n        Diagram inner representation contains following process attributes:\n\n        - id - assumed to be required in XML file, even thought BPMN 2.0 schema doesn't say so,\n        - isClosed - optional parameter, default value 'false',\n        - isExecutable - optional parameter, default value 'false',\n        - processType - optional parameter, default value 'None',\n        - node_ids - list of flow nodes IDs, associated with given process.\n\n        :param process_elements_dict: dictionary that holds attribute values for imported 'process' element. Key is\n           process ID, value is a dictionary of attributes,\n        :param process_element: object representing a BPMN XML 'process' element.\n        \"\"\"\n        process_id = process_element.getAttribute(consts.Consts.id)\n        process_element_attributes = {consts.Consts.id: process_element.getAttribute(consts.Consts.id),\n                                      consts.Consts.name: process_element.getAttribute(consts.Consts.name)\n                                      if process_element.hasAttribute(consts.Consts.name) else \"\",\n                                      consts.Consts.is_closed: process_element.getAttribute(consts.Consts.is_closed)\n                                      if process_element.hasAttribute(consts.Consts.is_closed) else \"false\",\n                                      consts.Consts.is_executable: process_element.getAttribute(\n                                          consts.Consts.is_executable)\n                                      if process_element.hasAttribute(consts.Consts.is_executable) else \"false\",\n                                      consts.Consts.process_type: process_element.getAttribute(\n                                          consts.Consts.process_type)\n                                      if process_element.hasAttribute(consts.Consts.process_type) else \"None\",\n                                      consts.Consts.node_ids: []}\n        process_elements_dict[process_id] = process_element_attributes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_flow_node_to_graph(bpmn_graph, process_id, process_attributes, flow_node_element):\n        element_id = flow_node_element.getAttribute(consts.Consts.id)\n        bpmn_graph.add_node(element_id)\n        bpmn_graph.node[element_id][consts.Consts.id] = element_id\n        bpmn_graph.node[element_id][consts.Consts.type] = \\\n            utils.BpmnImportUtils.remove_namespace_from_tag_name(flow_node_element.tagName)\n        bpmn_graph.node[element_id][consts.Consts.node_name] = \\\n            flow_node_element.getAttribute(consts.Consts.name) \\\n                if flow_node_element.hasAttribute(consts.Consts.name) \\\n                else \"\"\n        bpmn_graph.node[element_id][consts.Consts.process] = process_id\n        process_attributes[consts.Consts.node_ids].append(element_id)\n\n        # add incoming flow node list\n        incoming_list = []\n        for tmp_element in utils.BpmnImportUtils.iterate_elements(flow_node_element):\n            if tmp_element.nodeType != tmp_element.TEXT_NODE:\n                tag_name = utils.BpmnImportUtils.remove_namespace_from_tag_name(tmp_element.tagName)\n                if tag_name == consts.Consts.incoming_flow:\n                    incoming_value = tmp_element.firstChild.nodeValue\n                    incoming_list.append(incoming_value)\n        bpmn_graph.node[element_id][consts.Consts.incoming_flow] = incoming_list\n\n        # add outgoing flow node list\n        outgoing_list = []\n        for tmp_element in utils.BpmnImportUtils.iterate_elements(flow_node_element):\n            if tmp_element.nodeType != tmp_element.TEXT_NODE:\n                tag_name = utils.BpmnImportUtils.remove_namespace_from_tag_name(tmp_element.tagName)\n                if tag_name == consts.Consts.outgoing_flow:\n                    outgoing_value = tmp_element.firstChild.nodeValue\n                    outgoing_list.append(outgoing_value)\n        bpmn_graph.node[element_id][consts.Consts.outgoing_flow] = outgoing_list", "response": "Method adds a new node to the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new element that represents a BPMN task element to the diagram graph.", "response": "def import_task_to_graph(diagram_graph, process_id, process_attributes, task_element):\n        \"\"\"\n        Adds to graph the new element that represents BPMN task.\n        In our representation tasks have only basic attributes and elements, inherited from Activity type,\n        so this method only needs to call add_flownode_to_graph.\n\n        :param diagram_graph: NetworkX graph representing a BPMN process diagram,\n        :param process_id: string object, representing an ID of process element,\n        :param process_attributes: dictionary that holds attribute values of 'process' element, which is parent of\n            imported flow node,\n        :param task_element: object representing a BPMN XML 'task' element.\n        \"\"\"\n        BpmnDiagramGraphImport.import_activity_to_graph(diagram_graph, process_id, process_attributes, task_element)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_subprocess_to_graph(diagram_graph, sequence_flows, process_id, process_attributes, subprocess_element):\n        BpmnDiagramGraphImport.import_activity_to_graph(diagram_graph, process_id, process_attributes,\n                                                        subprocess_element)\n\n        subprocess_id = subprocess_element.getAttribute(consts.Consts.id)\n        diagram_graph.node[subprocess_id][consts.Consts.triggered_by_event] = \\\n            subprocess_element.getAttribute(consts.Consts.triggered_by_event) \\\n                if subprocess_element.hasAttribute(consts.Consts.triggered_by_event) else \"false\"\n\n        subprocess_attributes = diagram_graph.node[subprocess_id]\n        subprocess_attributes[consts.Consts.node_ids] = []\n        for element in utils.BpmnImportUtils.iterate_elements(subprocess_element):\n            if element.nodeType != element.TEXT_NODE:\n                tag_name = utils.BpmnImportUtils.remove_namespace_from_tag_name(element.tagName)\n                BpmnDiagramGraphImport.__import_element_by_tag_name(diagram_graph, sequence_flows, subprocess_id,\n                                                                    subprocess_attributes, element, tag_name)\n\n        for flow in utils.BpmnImportUtils.iterate_elements(subprocess_element):\n            if flow.nodeType != flow.TEXT_NODE:\n                tag_name = utils.BpmnImportUtils.remove_namespace_from_tag_name(flow.tagName)\n                if tag_name == consts.Consts.sequence_flow:\n                    BpmnDiagramGraphImport.import_sequence_flow_to_graph(diagram_graph, sequence_flows, subprocess_id,\n                                                                         flow)", "response": "Adds a new subprocess element to diagram_graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new element that represents a BPMN data object to the diagram graph.", "response": "def import_data_object_to_graph(diagram_graph, process_id, process_attributes, data_object_element):\n        \"\"\"\n        Adds to graph the new element that represents BPMN data object.\n        Data object inherits attributes from FlowNode. In addition, an attribute 'isCollection' is added to the node.\n\n        :param diagram_graph: NetworkX graph representing a BPMN process diagram,\n        :param process_id: string object, representing an ID of process element,\n        :param process_attributes: dictionary that holds attribute values of 'process' element, which is parent of\n            imported flow node,\n        :param data_object_element: object representing a BPMN XML 'dataObject' element.\n        \"\"\"\n        BpmnDiagramGraphImport.import_flow_node_to_graph(diagram_graph, process_id, process_attributes,\n                                                         data_object_element)\n        data_object_id = data_object_element.getAttribute(consts.Consts.id)\n        diagram_graph.node[data_object_id][consts.Consts.is_collection] = \\\n            data_object_element.getAttribute(consts.Consts.is_collection) \\\n                if data_object_element.hasAttribute(consts.Consts.is_collection) else \"false\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef import_activity_to_graph(diagram_graph, process_id, process_attributes, element):\n        BpmnDiagramGraphImport.import_flow_node_to_graph(diagram_graph, process_id, process_attributes, element)\n\n        element_id = element.getAttribute(consts.Consts.id)\n        diagram_graph.node[element_id][consts.Consts.default] = element.getAttribute(consts.Consts.default) \\\n            if element.hasAttribute(consts.Consts.default) else None", "response": "Method that adds the new element that represents BPMN activity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new element that represents a BPMN gateway.", "response": "def import_gateway_to_graph(diagram_graph, process_id, process_attributes, element):\n        \"\"\"\n        Adds to graph the new element that represents BPMN gateway.\n        In addition to attributes inherited from FlowNode type, Gateway\n        has additional attribute gatewayDirection (simple type, default value - Unspecified).\n\n        :param diagram_graph: NetworkX graph representing a BPMN process diagram,\n        :param process_id: string object, representing an ID of process element,\n        :param process_attributes: dictionary that holds attribute values of 'process' element, which is parent of\n            imported flow node,\n        :param element: object representing a BPMN XML element of Gateway type extension.\n        \"\"\"\n        element_id = element.getAttribute(consts.Consts.id)\n        BpmnDiagramGraphImport.import_flow_node_to_graph(diagram_graph, process_id, process_attributes, element)\n        diagram_graph.node[element_id][consts.Consts.gateway_direction] = \\\n            element.getAttribute(consts.Consts.gateway_direction) \\\n                if element.hasAttribute(consts.Consts.gateway_direction) else \"Unspecified\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_complex_gateway_to_graph(diagram_graph, process_id, process_attributes, element):\n        element_id = element.getAttribute(consts.Consts.id)\n        BpmnDiagramGraphImport.import_gateway_to_graph(diagram_graph, process_id, process_attributes, element)\n        diagram_graph.node[element_id][consts.Consts.default] = element.getAttribute(consts.Consts.default) \\\n            if element.hasAttribute(consts.Consts.default) else None", "response": "Adds a new complex gateway element to diagram_graph"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_event_based_gateway_to_graph(diagram_graph, process_id, process_attributes, element):\n        element_id = element.getAttribute(consts.Consts.id)\n        BpmnDiagramGraphImport.import_gateway_to_graph(diagram_graph, process_id, process_attributes, element)\n        diagram_graph.node[element_id][consts.Consts.instantiate] = element.getAttribute(consts.Consts.instantiate) \\\n            if element.hasAttribute(consts.Consts.instantiate) else \"false\"\n        diagram_graph.node[element_id][consts.Consts.event_gateway_type] = \\\n            element.getAttribute(consts.Consts.event_gateway_type) \\\n                if element.hasAttribute(consts.Consts.event_gateway_type) else \"Exclusive\"", "response": "Adds a new element that represents an event based gateway to the diagram graph."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef import_parallel_gateway_to_graph(diagram_graph, process_id, process_attributes, element):\n        BpmnDiagramGraphImport.import_gateway_to_graph(diagram_graph, process_id, process_attributes, element)", "response": "Adds new element that represents BPMN parallel gateway."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_event_definition_elements(diagram_graph, element, event_definitions):\n        element_id = element.getAttribute(consts.Consts.id)\n        event_def_list = []\n        for definition_type in event_definitions:\n            event_def_xml = element.getElementsByTagNameNS(\"*\", definition_type)\n            for index in range(len(event_def_xml)):\n                # tuple - definition type, definition id\n                event_def_tmp = {consts.Consts.id: event_def_xml[index].getAttribute(consts.Consts.id),\n                                 consts.Consts.definition_type: definition_type}\n                event_def_list.append(event_def_tmp)\n        diagram_graph.node[element_id][consts.Consts.event_definitions] = event_def_list", "response": "Helper function that adds event definitions elements to corresponding events."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_start_event_to_graph(diagram_graph, process_id, process_attributes, element):\n        element_id = element.getAttribute(consts.Consts.id)\n        start_event_definitions = {'messageEventDefinition', 'timerEventDefinition', 'conditionalEventDefinition',\n                                   'escalationEventDefinition', 'signalEventDefinition'}\n        BpmnDiagramGraphImport.import_flow_node_to_graph(diagram_graph, process_id, process_attributes, element)\n        diagram_graph.node[element_id][consts.Consts.parallel_multiple] = \\\n            element.getAttribute(consts.Consts.parallel_multiple) \\\n                if element.hasAttribute(consts.Consts.parallel_multiple) else \"false\"\n        diagram_graph.node[element_id][consts.Consts.is_interrupting] = \\\n            element.getAttribute(consts.Consts.is_interrupting) \\\n                if element.hasAttribute(consts.Consts.is_interrupting) else \"true\"\n        BpmnDiagramGraphImport.import_event_definition_elements(diagram_graph, element, start_event_definitions)", "response": "Adds a new element that represents a BPMN start event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd new element that represents a Intermediate CatchEvent element to diagram_graph.", "response": "def import_intermediate_catch_event_to_graph(diagram_graph, process_id, process_attributes, element):\n        \"\"\"\n        Adds to graph the new element that represents BPMN intermediate catch event.\n        Intermediate catch event inherits attribute parallelMultiple from CatchEvent type\n        and sequence of eventDefinitionRef from Event type.\n        Separate methods for each event type are required since each of them has different variants\n        (Message, Error, Signal etc.).\n\n        :param diagram_graph: NetworkX graph representing a BPMN process diagram,\n        :param process_id: string object, representing an ID of process element,\n        :param process_attributes: dictionary that holds attribute values of 'process' element, which is parent of\n            imported flow node,\n        :param element: object representing a BPMN XML 'intermediateCatchEvent' element.\n        \"\"\"\n        element_id = element.getAttribute(consts.Consts.id)\n        intermediate_catch_event_definitions = {'messageEventDefinition', 'timerEventDefinition',\n                                                'signalEventDefinition', 'conditionalEventDefinition',\n                                                'escalationEventDefinition'}\n        BpmnDiagramGraphImport.import_flow_node_to_graph(diagram_graph, process_id, process_attributes, element)\n        diagram_graph.node[element_id][consts.Consts.parallel_multiple] = \\\n            element.getAttribute(consts.Consts.parallel_multiple) \\\n                if element.hasAttribute(consts.Consts.parallel_multiple) else \"false\"\n        BpmnDiagramGraphImport.import_event_definition_elements(diagram_graph, element,\n                                                                intermediate_catch_event_definitions)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds new elements to diagram graph that represents the end event.", "response": "def import_end_event_to_graph(diagram_graph, process_id, process_attributes, element):\n        \"\"\"\n        Adds to graph the new element that represents BPMN end event.\n        End event inherits sequence of eventDefinitionRef from Event type.\n        Separate methods for each event type are required since each of them has different variants\n        (Message, Error, Signal etc.).\n\n        :param diagram_graph: NetworkX graph representing a BPMN process diagram,\n        :param process_id: string object, representing an ID of process element,\n        :param process_attributes: dictionary that holds attribute values of 'process' element, which is parent of\n            imported flow node,\n        :param element: object representing a BPMN XML 'endEvent' element.\n        \"\"\"\n        end_event_definitions = {'messageEventDefinition', 'signalEventDefinition', 'escalationEventDefinition',\n                                 'errorEventDefinition', 'compensateEventDefinition', 'terminateEventDefinition'}\n        BpmnDiagramGraphImport.import_flow_node_to_graph(diagram_graph, process_id, process_attributes, element)\n        BpmnDiagramGraphImport.import_event_definition_elements(diagram_graph, element, end_event_definitions)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd new intermediate throw event to diagram graph.", "response": "def import_intermediate_throw_event_to_graph(diagram_graph, process_id, process_attributes, element):\n        \"\"\"\n        Adds to graph the new element that represents BPMN intermediate throw event.\n        Intermediate throw event inherits sequence of eventDefinitionRef from Event type.\n        Separate methods for each event type are required since each of them has different variants\n        (Message, Error, Signal etc.).\n\n        :param diagram_graph: NetworkX graph representing a BPMN process diagram,\n        :param process_id: string object, representing an ID of process element,\n        :param process_attributes: dictionary that holds attribute values of 'process' element, which is parent of\n           imported flow node,\n        :param element: object representing a BPMN XML 'intermediateThrowEvent' element.\n        \"\"\"\n        intermediate_throw_event_definitions = {'messageEventDefinition', 'signalEventDefinition',\n                                                'escalationEventDefinition', 'compensateEventDefinition'}\n        BpmnDiagramGraphImport.import_flow_node_to_graph(diagram_graph, process_id, process_attributes, element)\n        BpmnDiagramGraphImport.import_event_definition_elements(diagram_graph, element,\n                                                                intermediate_throw_event_definitions)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_boundary_event_to_graph(diagram_graph, process_id, process_attributes, element):\n        element_id = element.getAttribute(consts.Consts.id)\n        boundary_event_definitions = {'messageEventDefinition', 'timerEventDefinition', 'signalEventDefinition',\n                                      'conditionalEventDefinition', 'escalationEventDefinition', 'errorEventDefinition'}\n        BpmnDiagramGraphImport.import_flow_node_to_graph(diagram_graph, process_id, process_attributes, element)\n\n        diagram_graph.node[element_id][consts.Consts.parallel_multiple] = \\\n            element.getAttribute(consts.Consts.parallel_multiple) \\\n                if element.hasAttribute(consts.Consts.parallel_multiple) else \"false\"\n        diagram_graph.node[element_id][consts.Consts.cancel_activity] = \\\n            element.getAttribute(consts.Consts.cancel_activity) \\\n                if element.hasAttribute(consts.Consts.cancel_activity) else \"true\"\n        diagram_graph.node[element_id][consts.Consts.attached_to_ref] = \\\n            element.getAttribute(consts.Consts.attached_to_ref)\n\n        BpmnDiagramGraphImport.import_event_definition_elements(diagram_graph, element,\n                                                                boundary_event_definitions)", "response": "Adds new element that represents a BPMN boundary event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nimport a sequenceFlow element into the diagram graph.", "response": "def import_sequence_flow_to_graph(diagram_graph, sequence_flows, process_id, flow_element):\n        \"\"\"\n        Adds a new edge to graph and a record to sequence_flows dictionary.\n        Input parameter is object of class xml.dom.Element.\n        Edges are identified by pair of sourceRef and targetRef attributes of BPMNFlow element. We also\n        provide a dictionary, that maps sequenceFlow ID attribute with its sourceRef and targetRef.\n        Method adds basic attributes of sequenceFlow element to edge. Those elements are:\n\n        - id - added as edge attribute, we assume that this is a required value,\n        - name - optional attribute, empty string by default.\n\n        :param diagram_graph: NetworkX graph representing a BPMN process diagram,\n        :param sequence_flows: dictionary (associative list) of sequence flows existing in diagram.\n            Key attribute is sequenceFlow ID, value is a dictionary consisting three key-value pairs: \"name\" (sequence\n            flow name), \"sourceRef\" (ID of node, that is a flow source) and \"targetRef\" (ID of node, that is a flow target),\n        :param process_id: string object, representing an ID of process element,\n        :param flow_element: object representing a BPMN XML 'sequenceFlow' element.\n        \"\"\"\n        flow_id = flow_element.getAttribute(consts.Consts.id)\n        name = flow_element.getAttribute(consts.Consts.name) if flow_element.hasAttribute(consts.Consts.name) else \"\"\n        source_ref = flow_element.getAttribute(consts.Consts.source_ref)\n        target_ref = flow_element.getAttribute(consts.Consts.target_ref)\n        sequence_flows[flow_id] = {consts.Consts.name: name, consts.Consts.source_ref: source_ref,\n                                   consts.Consts.target_ref: target_ref}\n        diagram_graph.add_edge(source_ref, target_ref)\n        diagram_graph[source_ref][target_ref][consts.Consts.id] = flow_id\n        diagram_graph[source_ref][target_ref][consts.Consts.process] = process_id\n        diagram_graph[source_ref][target_ref][consts.Consts.name] = name\n        diagram_graph[source_ref][target_ref][consts.Consts.source_ref] = source_ref\n        diagram_graph[source_ref][target_ref][consts.Consts.target_ref] = target_ref\n        for element in utils.BpmnImportUtils.iterate_elements(flow_element):\n            if element.nodeType != element.TEXT_NODE:\n                tag_name = utils.BpmnImportUtils.remove_namespace_from_tag_name(element.tagName)\n                if tag_name == consts.Consts.condition_expression:\n                    condition_expression = element.firstChild.nodeValue\n                    diagram_graph[source_ref][target_ref][consts.Consts.condition_expression] = {\n                        consts.Consts.id: element.getAttribute(consts.Consts.id),\n                        consts.Consts.condition_expression: condition_expression\n                    }\n\n        '''\n        # Add incoming / outgoing nodes to corresponding elements. May be redundant action since this information is\n        added when processing nodes, but listing incoming / outgoing nodes under node element is optional - this way\n        we can make sure this info will be imported.\n        '''\n        if consts.Consts.outgoing_flow not in diagram_graph.node[source_ref]:\n            diagram_graph.node[source_ref][consts.Consts.outgoing_flow] = []\n        outgoing_list = diagram_graph.node[source_ref][consts.Consts.outgoing_flow]\n        if flow_id not in outgoing_list:\n            outgoing_list.append(flow_id)\n\n        if consts.Consts.incoming_flow not in diagram_graph.node[target_ref]:\n            diagram_graph.node[target_ref][consts.Consts.incoming_flow] = []\n        incoming_list = diagram_graph.node[target_ref][consts.Consts.incoming_flow]\n        if flow_id not in incoming_list:\n            incoming_list.append(flow_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimports a message flow element into a networkX graph.", "response": "def import_message_flow_to_graph(diagram_graph, message_flows, flow_element):\n        \"\"\"\n        Adds a new edge to graph and a record to message flows dictionary.\n        Input parameter is object of class xml.dom.Element.\n        Edges are identified by pair of sourceRef and targetRef attributes of BPMNFlow element. We also\n        provide a dictionary, that maps messageFlow ID attribute with its sourceRef and targetRef.\n        Method adds basic attributes of messageFlow element to edge. Those elements are:\n\n        - id - added as edge attribute, we assume that this is a required value,\n        - name - optional attribute, empty string by default.\n\n        :param diagram_graph: NetworkX graph representing a BPMN process diagram,\n        :param message_flows: dictionary (associative list) of message flows existing in diagram.\n            Key attribute is messageFlow ID, value is a dictionary consisting three key-value pairs: \"name\" (message\n            flow name), \"sourceRef\" (ID of node, that is a flow source) and \"targetRef\" (ID of node, that is a flow target),\n        :param flow_element: object representing a BPMN XML 'messageFlow' element.\n        \"\"\"\n        flow_id = flow_element.getAttribute(consts.Consts.id)\n        name = flow_element.getAttribute(consts.Consts.name) if flow_element.hasAttribute(consts.Consts.name) else \"\"\n        source_ref = flow_element.getAttribute(consts.Consts.source_ref)\n        target_ref = flow_element.getAttribute(consts.Consts.target_ref)\n        message_flows[flow_id] = {consts.Consts.id: flow_id, consts.Consts.name: name,\n                                  consts.Consts.source_ref: source_ref,\n                                  consts.Consts.target_ref: target_ref}\n        diagram_graph.add_edge(source_ref, target_ref)\n        diagram_graph[source_ref][target_ref][consts.Consts.id] = flow_id\n        diagram_graph[source_ref][target_ref][consts.Consts.name] = name\n        diagram_graph[source_ref][target_ref][consts.Consts.source_ref] = source_ref\n        diagram_graph[source_ref][target_ref][consts.Consts.target_ref] = target_ref\n\n        '''\n        # Add incoming / outgoing nodes to corresponding elements. May be redundant action since this information is\n        added when processing nodes, but listing incoming / outgoing nodes under node element is optional - this way\n        we can make sure this info will be imported.\n        '''\n        if consts.Consts.outgoing_flow not in diagram_graph.node[source_ref]:\n            diagram_graph.node[source_ref][consts.Consts.outgoing_flow] = []\n        outgoing_list = diagram_graph.node[source_ref][consts.Consts.outgoing_flow]\n        if flow_id not in outgoing_list:\n            outgoing_list.append(flow_id)\n\n        if consts.Consts.incoming_flow not in diagram_graph.node[target_ref]:\n            diagram_graph.node[target_ref][consts.Consts.incoming_flow] = []\n        incoming_list = diagram_graph.node[target_ref][consts.Consts.incoming_flow]\n        if flow_id not in incoming_list:\n            incoming_list.append(flow_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_shape_di(participants_dict, diagram_graph, shape_element):\n        element_id = shape_element.getAttribute(consts.Consts.bpmn_element)\n        bounds = shape_element.getElementsByTagNameNS(\"*\", \"Bounds\")[0]\n        if diagram_graph.has_node(element_id):\n            node = diagram_graph.node[element_id]\n            node[consts.Consts.width] = bounds.getAttribute(consts.Consts.width)\n            node[consts.Consts.height] = bounds.getAttribute(consts.Consts.height)\n\n            if node[consts.Consts.type] == consts.Consts.subprocess:\n                node[consts.Consts.is_expanded] = \\\n                    shape_element.getAttribute(consts.Consts.is_expanded) \\\n                        if shape_element.hasAttribute(consts.Consts.is_expanded) else \"false\"\n            node[consts.Consts.x] = bounds.getAttribute(consts.Consts.x)\n            node[consts.Consts.y] = bounds.getAttribute(consts.Consts.y)\n        if element_id in participants_dict:\n            # BPMNShape is either connected with FlowNode or Participant\n            participant_attr = participants_dict[element_id]\n            participant_attr[consts.Consts.is_horizontal] = shape_element.getAttribute(consts.Consts.is_horizontal)\n            participant_attr[consts.Consts.width] = bounds.getAttribute(consts.Consts.width)\n            participant_attr[consts.Consts.height] = bounds.getAttribute(consts.Consts.height)\n            participant_attr[consts.Consts.x] = bounds.getAttribute(consts.Consts.x)\n            participant_attr[consts.Consts.y] = bounds.getAttribute(consts.Consts.y)", "response": "Imports the Diagram Interchange information for each BPMN process shape element into the diagram graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting the Diagram Interchange information for a given flow element into the appropriate BPMN sequence flow.", "response": "def import_flow_di(diagram_graph, sequence_flows, message_flows, flow_element):\n        \"\"\"\n        Adds Diagram Interchange information (information about rendering a diagram) to appropriate\n        BPMN sequence flow represented as graph edge.\n        We assume that each BPMNEdge has a list of 'waypoint' elements. BPMN 2.0 XML Schema states,\n        that each BPMNEdge must have at least two waypoints.\n\n        :param diagram_graph: NetworkX graph representing a BPMN process diagram,\n        :param sequence_flows: dictionary (associative list) of sequence flows existing in diagram.\n            Key attribute is sequenceFlow ID, value is a dictionary consisting three key-value pairs: \"name\" (sequence\n            flow name), \"sourceRef\" (ID of node, that is a flow source) and \"targetRef\" (ID of node, that is a flow target),\n        :param message_flows: dictionary (associative list) of message flows existing in diagram.\n            Key attribute is messageFlow ID, value is a dictionary consisting three key-value pairs: \"name\" (message\n            flow name), \"sourceRef\" (ID of node, that is a flow source) and \"targetRef\" (ID of node, that is a flow target),\n        :param flow_element: object representing a BPMN XML 'BPMNEdge' element.\n        \"\"\"\n        flow_id = flow_element.getAttribute(consts.Consts.bpmn_element)\n        waypoints_xml = flow_element.getElementsByTagNameNS(\"*\", consts.Consts.waypoint)\n        length = len(waypoints_xml)\n\n        waypoints = [None] * length\n        for index in range(length):\n            waypoint_tmp = (waypoints_xml[index].getAttribute(consts.Consts.x),\n                            waypoints_xml[index].getAttribute(consts.Consts.y))\n            waypoints[index] = waypoint_tmp\n\n        flow_data = None\n        if flow_id in sequence_flows:\n            flow_data = sequence_flows[flow_id]\n        elif flow_id in message_flows:\n            flow_data = message_flows[flow_id]\n\n        if flow_data is not None:\n            name = flow_data[consts.Consts.name]\n            source_ref = flow_data[consts.Consts.source_ref]\n            target_ref = flow_data[consts.Consts.target_ref]\n            diagram_graph[source_ref][target_ref][consts.Consts.waypoints] = waypoints\n            diagram_graph[source_ref][target_ref][consts.Consts.name] = name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_layout(bpmn_graph):\n    classification = generate_elements_clasification(bpmn_graph)\n    (sorted_nodes_with_classification, backward_flows) = topological_sort(bpmn_graph, classification[0])\n    grid = grid_layout(bpmn_graph, sorted_nodes_with_classification)\n    set_coordinates_for_nodes(bpmn_graph, grid)\n    set_flows_waypoints(bpmn_graph)", "response": "Generate the layout of the BPMN graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef topological_sort(bpmn_graph, nodes_with_classification):\n    node_param_name = \"node\"\n    classification_param_name = \"classification\"\n\n    tmp_nodes_with_classification = copy.deepcopy(nodes_with_classification)\n    sorted_nodes_with_classification = []\n    no_incoming_flow_nodes = []\n    backward_flows = []\n\n    while tmp_nodes_with_classification:\n        for node_with_classification in tmp_nodes_with_classification:\n            incoming_list = node_with_classification[node_param_name][1][consts.Consts.incoming_flow]\n            if len(incoming_list) == 0:\n                no_incoming_flow_nodes.append(node_with_classification)\n        if len(no_incoming_flow_nodes) > 0:\n            while len(no_incoming_flow_nodes) > 0:\n                node_with_classification = no_incoming_flow_nodes.pop()\n                tmp_nodes_with_classification.remove(node_with_classification)\n                sorted_nodes_with_classification \\\n                    .append(next(tmp_node for tmp_node in nodes_with_classification\n                                 if tmp_node[node_param_name][0] == node_with_classification[node_param_name][0]))\n\n                outgoing_list = list(node_with_classification[node_param_name][1][consts.Consts.outgoing_flow])\n                tmp_outgoing_list = list(outgoing_list)\n\n                for flow_id in tmp_outgoing_list:\n                    '''\n                    - Remove the outgoing flow for source flow node (the one without incoming flows)\n                    - Get the target node\n                    - Remove the incoming flow for target flow node\n                    '''\n                    outgoing_list.remove(flow_id)\n                    node_with_classification[node_param_name][1][consts.Consts.outgoing_flow].remove(flow_id)\n\n                    flow = bpmn_graph.get_flow_by_id(flow_id)\n                    target_id = flow[2][consts.Consts.target_ref]\n                    target = next(tmp_node[node_param_name]\n                                  for tmp_node in tmp_nodes_with_classification\n                                  if tmp_node[node_param_name][0] == target_id)\n                    target[1][consts.Consts.incoming_flow].remove(flow_id)\n        else:\n            for node_with_classification in tmp_nodes_with_classification:\n                if \"Join\" in node_with_classification[classification_param_name]:\n                    incoming_list = list(node_with_classification[node_param_name][1][consts.Consts.incoming_flow])\n                    tmp_incoming_list = list(incoming_list)\n                    for flow_id in tmp_incoming_list:\n                        incoming_list.remove(flow_id)\n\n                        flow = bpmn_graph.get_flow_by_id(flow_id)\n\n                        source_id = flow[2][consts.Consts.source_ref]\n                        source = next(tmp_node[node_param_name]\n                                      for tmp_node in tmp_nodes_with_classification\n                                      if tmp_node[node_param_name][0] == source_id)\n                        source[1][consts.Consts.outgoing_flow].remove(flow_id)\n\n                        target_id = flow[2][consts.Consts.target_ref]\n                        target = next(tmp_node[node_param_name]\n                                      for tmp_node in tmp_nodes_with_classification\n                                      if tmp_node[node_param_name][0] == target_id)\n                        target[1][consts.Consts.incoming_flow].remove(flow_id)\n                        backward_flows.append(flow)\n    return sorted_nodes_with_classification, backward_flows", "response": "Topological sort of a list of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_task_info(node_params, output_element):\n        if consts.Consts.default in node_params and node_params[consts.Consts.default] is not None:\n            output_element.set(consts.Consts.default, node_params[consts.Consts.default])", "response": "Adds Task node attributes to exported XML element"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_subprocess_info(bpmn_diagram, subprocess_params, output_element):\n        output_element.set(consts.Consts.triggered_by_event, subprocess_params[consts.Consts.triggered_by_event])\n        if consts.Consts.default in subprocess_params and subprocess_params[consts.Consts.default] is not None:\n            output_element.set(consts.Consts.default, subprocess_params[consts.Consts.default])\n\n        # for each node in graph add correct type of element, its attributes and BPMNShape element\n        subprocess_id = subprocess_params[consts.Consts.id]\n        nodes = bpmn_diagram.get_nodes_list_by_process_id(subprocess_id)\n        for node in nodes:\n            node_id = node[0]\n            params = node[1]\n            BpmnDiagramGraphExport.export_node_data(bpmn_diagram, node_id, params, output_element)\n\n        # for each edge in graph add sequence flow element, its attributes and BPMNEdge element\n        flows = bpmn_diagram.get_flows_list_by_process_id(subprocess_id)\n        for flow in flows:\n            params = flow[2]\n            BpmnDiagramGraphExport.export_flow_process_data(params, output_element)", "response": "Adds Subprocess node attributes and BPMNEdge attributes to exported XML element."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds DataObject node attributes to exported XML element", "response": "def export_data_object_info(bpmn_diagram, data_object_params, output_element):\n        \"\"\"\n        Adds DataObject node attributes to exported XML element\n\n        :param bpmn_diagram: BPMNDiagramGraph class instantion representing a BPMN process diagram,\n        :param data_object_params: dictionary with given subprocess parameters,\n        :param output_element: object representing BPMN XML 'subprocess' element.\n        \"\"\"\n        output_element.set(consts.Consts.is_collection, data_object_params[consts.Consts.is_collection])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_complex_gateway_info(node_params, output_element):\n        output_element.set(consts.Consts.gateway_direction, node_params[consts.Consts.gateway_direction])\n        if consts.Consts.default in node_params and node_params[consts.Consts.default] is not None:\n            output_element.set(consts.Consts.default, node_params[consts.Consts.default])", "response": "Adds ComplexGateway node attributes to exported XML element"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_event_based_gateway_info(node_params, output_element):\n        output_element.set(consts.Consts.gateway_direction, node_params[consts.Consts.gateway_direction])\n        output_element.set(consts.Consts.instantiate, node_params[consts.Consts.instantiate])\n        output_element.set(consts.Consts.event_gateway_type, node_params[consts.Consts.event_gateway_type])", "response": "Adds EventBasedGateway node attributes to exported XML element"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding InclusiveGateway or ExclusiveGateway node attributes to exported XML element", "response": "def export_inclusive_exclusive_gateway_info(node_params, output_element):\n        \"\"\"\n        Adds InclusiveGateway or ExclusiveGateway node attributes to exported XML element\n\n        :param node_params: dictionary with given inclusive or exclusive gateway parameters,\n        :param output_element: object representing BPMN XML 'inclusiveGateway'/'exclusive' element.\n        \"\"\"\n        output_element.set(consts.Consts.gateway_direction, node_params[consts.Consts.gateway_direction])\n        if consts.Consts.default in node_params and node_params[consts.Consts.default] is not None:\n            output_element.set(consts.Consts.default, node_params[consts.Consts.default])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd parallel gateway node attributes to exported XML element", "response": "def export_parallel_gateway_info(node_params, output_element):\n        \"\"\"\n        Adds parallel gateway node attributes to exported XML element\n\n        :param node_params: dictionary with given parallel gateway parameters,\n        :param output_element: object representing BPMN XML 'parallelGateway' element.\n        \"\"\"\n        output_element.set(consts.Consts.gateway_direction, node_params[consts.Consts.gateway_direction])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds StartEvent attributes to exported XML element", "response": "def export_start_event_info(node_params, output_element):\n        \"\"\"\n        Adds StartEvent attributes to exported XML element\n\n        :param node_params: dictionary with given intermediate catch event parameters,\n        :param output_element: object representing BPMN XML 'intermediateCatchEvent' element.\n        \"\"\"\n        output_element.set(consts.Consts.parallel_multiple, node_params.get(consts.Consts.parallel_multiple))\n        output_element.set(consts.Consts.is_interrupting, node_params.get(consts.Consts.is_interrupting))\n        definitions = node_params.get(consts.Consts.event_definitions)\n        for definition in definitions:\n            definition_id = definition[consts.Consts.id]\n            definition_type = definition[consts.Consts.definition_type]\n            output_definition = eTree.SubElement(output_element, definition_type)\n            if definition_id != \"\":\n                output_definition.set(consts.Consts.id, definition_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd EndEvent or IntermediateThrowingEvent attributes to exported XML element.", "response": "def export_throw_event_info(node_params, output_element):\n        \"\"\"\n        Adds EndEvent or IntermediateThrowingEvent attributes to exported XML element\n\n        :param node_params: dictionary with given intermediate throw event parameters,\n        :param output_element: object representing BPMN XML 'intermediateThrowEvent' element.\n        \"\"\"\n        definitions = node_params[consts.Consts.event_definitions]\n        for definition in definitions:\n            definition_id = definition[consts.Consts.id]\n            definition_type = definition[consts.Consts.definition_type]\n            output_definition = eTree.SubElement(output_element, definition_type)\n            if definition_id != \"\":\n                output_definition.set(consts.Consts.id, definition_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds IntermediateCatchEvent attributes to exported XML element", "response": "def export_boundary_event_info(node_params, output_element):\n        \"\"\"\n        Adds IntermediateCatchEvent attributes to exported XML element\n\n        :param node_params: dictionary with given intermediate catch event parameters,\n        :param output_element: object representing BPMN XML 'intermediateCatchEvent' element.\n        \"\"\"\n        output_element.set(consts.Consts.parallel_multiple, node_params[consts.Consts.parallel_multiple])\n        output_element.set(consts.Consts.cancel_activity, node_params[consts.Consts.cancel_activity])\n        output_element.set(consts.Consts.attached_to_ref, node_params[consts.Consts.attached_to_ref])\n        definitions = node_params[consts.Consts.event_definitions]\n        for definition in definitions:\n            definition_id = definition[consts.Consts.id]\n            definition_type = definition[consts.Consts.definition_type]\n            output_definition = eTree.SubElement(output_element, definition_type)\n            if definition_id != \"\":\n                output_definition.set(consts.Consts.id, definition_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates root element ('definitions') for exported BPMN XML file.", "response": "def export_definitions_element():\n        \"\"\"\n        Creates root element ('definitions') for exported BPMN XML file.\n\n        :return: definitions XML element.\n        \"\"\"\n        root = eTree.Element(consts.Consts.definitions)\n        root.set(\"xmlns\", \"http://www.omg.org/spec/BPMN/20100524/MODEL\")\n        root.set(\"xmlns:bpmndi\", \"http://www.omg.org/spec/BPMN/20100524/DI\")\n        root.set(\"xmlns:omgdc\", \"http://www.omg.org/spec/DD/20100524/DC\")\n        root.set(\"xmlns:omgdi\", \"http://www.omg.org/spec/DD/20100524/DI\")\n        root.set(\"xmlns:xsi\", \"http://www.w3.org/2001/XMLSchema-instance\")\n        root.set(\"targetNamespace\", \"http://www.signavio.com/bpmn20\")\n        root.set(\"typeLanguage\", \"http://www.w3.org/2001/XMLSchema\")\n        root.set(\"expressionLanguage\", \"http://www.w3.org/1999/XPath\")\n        root.set(\"xmlns:xsd\", \"http://www.w3.org/2001/XMLSchema\")\n\n        return root"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate process XML element for exported BPMN 2. 0 document.", "response": "def export_process_element(definitions, process_id, process_attributes_dictionary):\n        \"\"\"\n        Creates process element for exported BPMN XML file.\n\n        :param process_id: string object. ID of exported process element,\n        :param definitions: an XML element ('definitions'), root element of BPMN 2.0 document\n        :param process_attributes_dictionary: dictionary that holds attribute values of 'process' element\n        :return: process XML element\n        \"\"\"\n        process = eTree.SubElement(definitions, consts.Consts.process)\n        process.set(consts.Consts.id, process_id)\n        process.set(consts.Consts.is_closed, process_attributes_dictionary[consts.Consts.is_closed])\n        process.set(consts.Consts.is_executable, process_attributes_dictionary[consts.Consts.is_executable])\n        process.set(consts.Consts.process_type, process_attributes_dictionary[consts.Consts.process_type])\n\n        return process"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_lane_set(process, lane_set, plane_element):\n        lane_set_xml = eTree.SubElement(process, consts.Consts.lane_set)\n        for key, value in lane_set[consts.Consts.lanes].items():\n            BpmnDiagramGraphExport.export_lane(lane_set_xml, key, value, plane_element)", "response": "Creates laneSet element for exported BPMN 2. 0 document."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_child_lane_set(parent_xml_element, child_lane_set, plane_element):\n        lane_set_xml = eTree.SubElement(parent_xml_element, consts.Consts.lane_set)\n        for key, value in child_lane_set[consts.Consts.lanes].items():\n            BpmnDiagramGraphExport.export_lane(lane_set_xml, key, value, plane_element)", "response": "Creates childLaneSet element for exported BPMN XML file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_lane(parent_xml_element, lane_id, lane_attr, plane_element):\n        lane_xml = eTree.SubElement(parent_xml_element, consts.Consts.lane)\n        lane_xml.set(consts.Consts.id, lane_id)\n        lane_xml.set(consts.Consts.name, lane_attr[consts.Consts.name])\n        if consts.Consts.child_lane_set in lane_attr and len(lane_attr[consts.Consts.child_lane_set]):\n            child_lane_set = lane_attr[consts.Consts.child_lane_set]\n            BpmnDiagramGraphExport.export_child_lane_set(lane_xml, child_lane_set, plane_element)\n        if consts.Consts.flow_node_refs in lane_attr and len(lane_attr[consts.Consts.flow_node_refs]):\n            for flow_node_ref_id in lane_attr[consts.Consts.flow_node_refs]:\n                flow_node_ref_xml = eTree.SubElement(lane_xml, consts.Consts.flow_node_ref)\n                flow_node_ref_xml.text = flow_node_ref_id\n\n        output_element_di = eTree.SubElement(plane_element, BpmnDiagramGraphExport.bpmndi_namespace +\n                                             consts.Consts.bpmn_shape)\n        output_element_di.set(consts.Consts.id, lane_id + \"_gui\")\n\n        output_element_di.set(consts.Consts.bpmn_element, lane_id)\n        output_element_di.set(consts.Consts.is_horizontal, lane_attr[consts.Consts.is_horizontal])\n        bounds = eTree.SubElement(output_element_di, \"omgdc:Bounds\")\n        bounds.set(consts.Consts.width, lane_attr[consts.Consts.width])\n        bounds.set(consts.Consts.height, lane_attr[consts.Consts.height])\n        bounds.set(consts.Consts.x, lane_attr[consts.Consts.x])\n        bounds.set(consts.Consts.y, lane_attr[consts.Consts.y])", "response": "Creates a lane element for exported BPMN 2. 0 XML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_diagram_plane_elements(root, diagram_attributes, plane_attributes):\n        diagram = eTree.SubElement(root, BpmnDiagramGraphExport.bpmndi_namespace + \"BPMNDiagram\")\n        diagram.set(consts.Consts.id, diagram_attributes[consts.Consts.id])\n        diagram.set(consts.Consts.name, diagram_attributes[consts.Consts.name])\n\n        plane = eTree.SubElement(diagram, BpmnDiagramGraphExport.bpmndi_namespace + \"BPMNPlane\")\n        plane.set(consts.Consts.id, plane_attributes[consts.Consts.id])\n        plane.set(consts.Consts.bpmn_element, plane_attributes[consts.Consts.bpmn_element])\n\n        return diagram, plane", "response": "Creates diagram and plane elements for exported BPMN XML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_node_data(bpmn_diagram, process_id, params, process):\n        node_type = params[consts.Consts.type]\n        output_element = eTree.SubElement(process, node_type)\n        output_element.set(consts.Consts.id, process_id)\n        output_element.set(consts.Consts.name, params[consts.Consts.node_name])\n\n        for incoming in params[consts.Consts.incoming_flow]:\n            incoming_element = eTree.SubElement(output_element, consts.Consts.incoming_flow)\n            incoming_element.text = incoming\n        for outgoing in params[consts.Consts.outgoing_flow]:\n            outgoing_element = eTree.SubElement(output_element, consts.Consts.outgoing_flow)\n            outgoing_element.text = outgoing\n\n        if node_type == consts.Consts.task \\\n                or node_type == consts.Consts.user_task \\\n                or node_type == consts.Consts.service_task \\\n                or node_type == consts.Consts.manual_task:\n            BpmnDiagramGraphExport.export_task_info(params, output_element)\n        elif node_type == consts.Consts.subprocess:\n            BpmnDiagramGraphExport.export_subprocess_info(bpmn_diagram, params, output_element)\n        elif node_type == consts.Consts.data_object:\n            BpmnDiagramGraphExport.export_data_object_info(bpmn_diagram, params, output_element)\n        elif node_type == consts.Consts.complex_gateway:\n            BpmnDiagramGraphExport.export_complex_gateway_info(params, output_element)\n        elif node_type == consts.Consts.event_based_gateway:\n            BpmnDiagramGraphExport.export_event_based_gateway_info(params, output_element)\n        elif node_type == consts.Consts.inclusive_gateway or node_type == consts.Consts.exclusive_gateway:\n            BpmnDiagramGraphExport.export_inclusive_exclusive_gateway_info(params, output_element)\n        elif node_type == consts.Consts.parallel_gateway:\n            BpmnDiagramGraphExport.export_parallel_gateway_info(params, output_element)\n        elif node_type == consts.Consts.start_event:\n            BpmnDiagramGraphExport.export_start_event_info(params, output_element)\n        elif node_type == consts.Consts.intermediate_catch_event:\n            BpmnDiagramGraphExport.export_catch_event_info(params, output_element)\n        elif node_type == consts.Consts.end_event or node_type == consts.Consts.intermediate_throw_event:\n            BpmnDiagramGraphExport.export_throw_event_info(params, output_element)\n        elif node_type == consts.Consts.boundary_event:\n            BpmnDiagramGraphExport.export_boundary_event_info(params, output_element)", "response": "Adds node data to BPMN XML element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new BPMNShape XML element for given node parameters and adds it to plane element.", "response": "def export_node_di_data(node_id, params, plane):\n        \"\"\"\n        Creates a new BPMNShape XML element for given node parameters and adds it to 'plane' element.\n\n        :param node_id: string representing ID of given flow node,\n        :param params: dictionary with node parameters,\n        :param plane: object of Element class, representing BPMN XML 'BPMNPlane' element (root for node DI data).\n        \"\"\"\n        output_element_di = eTree.SubElement(plane, BpmnDiagramGraphExport.bpmndi_namespace + consts.Consts.bpmn_shape)\n        output_element_di.set(consts.Consts.id, node_id + \"_gui\")\n\n        output_element_di.set(consts.Consts.bpmn_element, node_id)\n        bounds = eTree.SubElement(output_element_di, \"omgdc:Bounds\")\n        bounds.set(consts.Consts.width, params[consts.Consts.width])\n        bounds.set(consts.Consts.height, params[consts.Consts.height])\n        bounds.set(consts.Consts.x, params[consts.Consts.x])\n        bounds.set(consts.Consts.y, params[consts.Consts.y])\n        if params[consts.Consts.type] == consts.Consts.subprocess:\n            output_element_di.set(consts.Consts.is_expanded, params[consts.Consts.is_expanded])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new SequenceFlow XML element for given edge parameters and adds it to process element.", "response": "def export_flow_process_data(params, process):\n        \"\"\"\n        Creates a new SequenceFlow XML element for given edge parameters and adds it to 'process' element.\n\n        :param params: dictionary with edge parameters,\n        :param process: object of Element class, representing BPMN XML 'process' element (root for sequence flows)\n        \"\"\"\n        output_flow = eTree.SubElement(process, consts.Consts.sequence_flow)\n        output_flow.set(consts.Consts.id, params[consts.Consts.id])\n        output_flow.set(consts.Consts.name, params[consts.Consts.name])\n        output_flow.set(consts.Consts.source_ref, params[consts.Consts.source_ref])\n        output_flow.set(consts.Consts.target_ref, params[consts.Consts.target_ref])\n        if consts.Consts.condition_expression in params:\n            condition_expression_params = params[consts.Consts.condition_expression]\n            condition_expression = eTree.SubElement(output_flow, consts.Consts.condition_expression)\n            condition_expression.set(consts.Consts.id, condition_expression_params[consts.Consts.id])\n            condition_expression.set(consts.Consts.id, condition_expression_params[consts.Consts.id])\n            condition_expression.text = condition_expression_params[consts.Consts.condition_expression]\n            output_flow.set(consts.Consts.name, condition_expression_params[consts.Consts.condition_expression])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new BPMNEdge XML element for given edge parameters and adds it to plane element.", "response": "def export_flow_di_data(params, plane):\n        \"\"\"\n        Creates a new BPMNEdge XML element for given edge parameters and adds it to 'plane' element.\n\n        :param params: dictionary with edge parameters,\n        :param plane: object of Element class, representing BPMN XML 'BPMNPlane' element (root for edge DI data).\n        \"\"\"\n        output_flow = eTree.SubElement(plane, BpmnDiagramGraphExport.bpmndi_namespace + consts.Consts.bpmn_edge)\n        output_flow.set(consts.Consts.id, params[consts.Consts.id] + \"_gui\")\n        output_flow.set(consts.Consts.bpmn_element, params[consts.Consts.id])\n        waypoints = params[consts.Consts.waypoints]\n        for waypoint in waypoints:\n            waypoint_element = eTree.SubElement(output_flow, \"omgdi:waypoint\")\n            waypoint_element.set(consts.Consts.x, waypoint[0])\n            waypoint_element.set(consts.Consts.y, waypoint[1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports diagram inner graph to BPMN 2. 0 XML file.", "response": "def export_xml_file(directory, filename, bpmn_diagram):\n        \"\"\"\n        Exports diagram inner graph to BPMN 2.0 XML file (with Diagram Interchange data).\n\n        :param directory: string representing output directory,\n        :param filename: string representing output file name,\n        :param bpmn_diagram: BPMNDiagramGraph class instantion representing a BPMN process diagram.\n        \"\"\"\n        diagram_attributes = bpmn_diagram.diagram_attributes\n        plane_attributes = bpmn_diagram.plane_attributes\n        collaboration = bpmn_diagram.collaboration\n        process_elements_dict = bpmn_diagram.process_elements\n        definitions = BpmnDiagramGraphExport.export_definitions_element()\n\n        [_, plane] = BpmnDiagramGraphExport.export_diagram_plane_elements(definitions, diagram_attributes,\n                                                                          plane_attributes)\n\n        if collaboration is not None and len(collaboration) > 0:\n            message_flows = collaboration[consts.Consts.message_flows]\n            participants = collaboration[consts.Consts.participants]\n            collaboration_xml = eTree.SubElement(definitions, consts.Consts.collaboration)\n            collaboration_xml.set(consts.Consts.id, collaboration[consts.Consts.id])\n\n            for message_flow_id, message_flow_attr in message_flows.items():\n                message_flow = eTree.SubElement(collaboration_xml, consts.Consts.message_flow)\n                message_flow.set(consts.Consts.id, message_flow_id)\n                message_flow.set(consts.Consts.name, message_flow_attr[consts.Consts.name])\n                message_flow.set(consts.Consts.source_ref, message_flow_attr[consts.Consts.source_ref])\n                message_flow.set(consts.Consts.target_ref, message_flow_attr[consts.Consts.target_ref])\n\n                message_flow_params = bpmn_diagram.get_flow_by_id(message_flow_id)[2]\n                output_flow = eTree.SubElement(plane, BpmnDiagramGraphExport.bpmndi_namespace + consts.Consts.bpmn_edge)\n                output_flow.set(consts.Consts.id, message_flow_id + \"_gui\")\n                output_flow.set(consts.Consts.bpmn_element, message_flow_id)\n                waypoints = message_flow_params[consts.Consts.waypoints]\n                for waypoint in waypoints:\n                    waypoint_element = eTree.SubElement(output_flow, \"omgdi:waypoint\")\n                    waypoint_element.set(consts.Consts.x, waypoint[0])\n                    waypoint_element.set(consts.Consts.y, waypoint[1])\n\n            for participant_id, participant_attr in participants.items():\n                participant = eTree.SubElement(collaboration_xml, consts.Consts.participant)\n                participant.set(consts.Consts.id, participant_id)\n                participant.set(consts.Consts.name, participant_attr[consts.Consts.name])\n                participant.set(consts.Consts.process_ref, participant_attr[consts.Consts.process_ref])\n\n                output_element_di = eTree.SubElement(plane, BpmnDiagramGraphExport.bpmndi_namespace +\n                                                     consts.Consts.bpmn_shape)\n                output_element_di.set(consts.Consts.id, participant_id + \"_gui\")\n                output_element_di.set(consts.Consts.bpmn_element, participant_id)\n                output_element_di.set(consts.Consts.is_horizontal, participant_attr[consts.Consts.is_horizontal])\n                bounds = eTree.SubElement(output_element_di, \"omgdc:Bounds\")\n                bounds.set(consts.Consts.width, participant_attr[consts.Consts.width])\n                bounds.set(consts.Consts.height, participant_attr[consts.Consts.height])\n                bounds.set(consts.Consts.x, participant_attr[consts.Consts.x])\n                bounds.set(consts.Consts.y, participant_attr[consts.Consts.y])\n\n        for process_id in process_elements_dict:\n            process_element_attr = process_elements_dict[process_id]\n            process = BpmnDiagramGraphExport.export_process_element(definitions, process_id, process_element_attr)\n            if consts.Consts.lane_set in process_element_attr:\n                BpmnDiagramGraphExport.export_lane_set(process, process_element_attr[consts.Consts.lane_set], plane)\n\n            # for each node in graph add correct type of element, its attributes and BPMNShape element\n            nodes = bpmn_diagram.get_nodes_list_by_process_id(process_id)\n            for node in nodes:\n                node_id = node[0]\n                params = node[1]\n                BpmnDiagramGraphExport.export_node_data(bpmn_diagram, node_id, params, process)\n                # BpmnDiagramGraphExport.export_node_di_data(node_id, params, plane)\n\n            # for each edge in graph add sequence flow element, its attributes and BPMNEdge element\n            flows = bpmn_diagram.get_flows_list_by_process_id(process_id)\n            for flow in flows:\n                params = flow[2]\n                BpmnDiagramGraphExport.export_flow_process_data(params, process)\n                # BpmnDiagramGraphExport.export_flow_di_data(params, plane)\n\n        # Export DI data\n        nodes = bpmn_diagram.get_nodes()\n        for node in nodes:\n            node_id = node[0]\n            params = node[1]\n            BpmnDiagramGraphExport.export_node_di_data(node_id, params, plane)\n\n        flows = bpmn_diagram.get_flows()\n        for flow in flows:\n            params = flow[2]\n            BpmnDiagramGraphExport.export_flow_di_data(params, plane)\n\n        BpmnDiagramGraphExport.indent(definitions)\n        tree = eTree.ElementTree(definitions)\n        try:\n            os.makedirs(directory)\n        except OSError as exception:\n            if exception.errno != errno.EEXIST:\n                raise\n        tree.write(directory + filename, encoding='utf-8', xml_declaration=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_xml_file_no_di(directory, filename, bpmn_diagram):\n        diagram_graph = bpmn_diagram.diagram_graph\n        process_elements_dict = bpmn_diagram.process_elements\n        definitions = BpmnDiagramGraphExport.export_definitions_element()\n\n        for process_id in process_elements_dict:\n            process_element_attr = process_elements_dict[process_id]\n            process = BpmnDiagramGraphExport.export_process_element(definitions, process_id, process_element_attr)\n\n            # for each node in graph add correct type of element, its attributes and BPMNShape element\n            nodes = diagram_graph.nodes(data=True)\n            for node in nodes:\n                node_id = node[0]\n                params = node[1]\n                BpmnDiagramGraphExport.export_node_data(bpmn_diagram, node_id, params, process)\n\n            # for each edge in graph add sequence flow element, its attributes and BPMNEdge element\n            flows = diagram_graph.edges(data=True)\n            for flow in flows:\n                params = flow[2]\n                BpmnDiagramGraphExport.export_flow_process_data(params, process)\n\n        BpmnDiagramGraphExport.indent(definitions)\n        tree = eTree.ElementTree(definitions)\n        try:\n            os.makedirs(directory)\n        except OSError as exception:\n            if exception.errno != errno.EEXIST:\n                raise\n        tree.write(directory + filename, encoding='utf-8', xml_declaration=True)", "response": "Exports diagram inner graph to BPMN 2. 0 XML file without Diagram Interchange data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates over child Nodes and Elements of parent Node.", "response": "def iterate_elements(parent):\n        \"\"\"\n        Helper function that iterates over child Nodes/Elements of parent Node/Element.\n\n        :param parent: object of Element class, representing parent element.\n        \"\"\"\n        element = parent.firstChild\n        while element is not None:\n            yield element\n            element = element.nextSibling"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_nodes_clasification(bpmn_diagram):\n        nodes_classification = {}\n\n        classification_element = \"Element\"\n        classification_start_event = \"Start Event\"\n        classification_end_event = \"End Event\"\n\n        task_list = bpmn_diagram.get_nodes(consts.Consts.task)\n        for element in task_list:\n            classification_labels = [classification_element]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        subprocess_list = bpmn_diagram.get_nodes(consts.Consts.subprocess)\n        for element in subprocess_list:\n            classification_labels = [classification_element]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        complex_gateway_list = bpmn_diagram.get_nodes(consts.Consts.complex_gateway)\n        for element in complex_gateway_list:\n            classification_labels = [classification_element]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        event_based_gateway_list = bpmn_diagram.get_nodes(consts.Consts.event_based_gateway)\n        for element in event_based_gateway_list:\n            classification_labels = [classification_element]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        inclusive_gateway_list = bpmn_diagram.get_nodes(consts.Consts.inclusive_gateway)\n        for element in inclusive_gateway_list:\n            classification_labels = [classification_element]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        exclusive_gateway_list = bpmn_diagram.get_nodes(consts.Consts.exclusive_gateway)\n        for element in exclusive_gateway_list:\n            classification_labels = [classification_element]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        parallel_gateway_list = bpmn_diagram.get_nodes(consts.Consts.parallel_gateway)\n        for element in parallel_gateway_list:\n            classification_labels = [classification_element]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        start_event_list = bpmn_diagram.get_nodes(consts.Consts.start_event)\n        for element in start_event_list:\n            classification_labels = [classification_element, classification_start_event]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        intermediate_catch_event_list = bpmn_diagram.get_nodes(consts.Consts.intermediate_catch_event)\n        for element in intermediate_catch_event_list:\n            classification_labels = [classification_element]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        end_event_list = bpmn_diagram.get_nodes(consts.Consts.end_event)\n        for element in end_event_list:\n            classification_labels = [classification_element, classification_end_event]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        intermediate_throw_event_list = bpmn_diagram.get_nodes(consts.Consts.intermediate_throw_event)\n        for element in intermediate_throw_event_list:\n            classification_labels = [classification_element]\n            BpmnImportUtils.split_join_classification(element, classification_labels, nodes_classification)\n\n        return nodes_classification", "response": "This function generates the nodes classification for the process diagram."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the Split Join and Join classification if the element qualifies for.", "response": "def split_join_classification(element, classification_labels, nodes_classification):\n        \"\"\"\n        Add the \"Split\", \"Join\" classification, if the element qualifies for.\n\n        :param element: an element from BPMN diagram,\n        :param classification_labels: list of labels attached to the element,\n        :param nodes_classification: dictionary of classification labels. Key - node id. Value - a list of labels.\n        \"\"\"\n        classification_join = \"Join\"\n        classification_split = \"Split\"\n        if len(element[1][consts.Consts.incoming_flow]) >= 2:\n            classification_labels.append(classification_join)\n        if len(element[1][consts.Consts.outgoing_flow]) >= 2:\n            classification_labels.append(classification_split)\n        nodes_classification[element[0]] = classification_labels"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_all_gateways(bpmn_graph):\n    gateways = filter(lambda node: node[1]['type'] in GATEWAY_TYPES, bpmn_graph.get_nodes())\n\n    return gateways", "response": "Returns a list with all gateways in diagram\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef all_control_flow_elements_count(bpmn_graph):\n\n    gateway_counts = get_gateway_counts(bpmn_graph)\n    events_counts = get_events_counts(bpmn_graph)\n    control_flow_elements_counts = gateway_counts.copy()\n    control_flow_elements_counts.update(events_counts)\n\n    return sum([\n                   count for name, count in control_flow_elements_counts.items()\n                   ])", "response": "Returns the total count of all control flow elements in the BPMN model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef TNE_metric(bpmn_graph):\n\n    events_counts = get_events_counts(bpmn_graph)\n\n    return sum(\n        [count for _, count in events_counts.items()]\n    )", "response": "Returns the value of the TNE metric for the BPMN model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the value of the NOAC metric for the BPMNDiagramGraph instance.", "response": "def NOAC_metric(bpmn_graph):\n    \"\"\"\n    Returns the value of the NOAC metric (Number of Activities and control flow elements)\n    for the BPMNDiagramGraph instance.\n\n    :param bpmn_graph: an instance of BpmnDiagramGraph representing BPMN model.\n    \"\"\"\n\n    activities_count = all_activities_count(bpmn_graph)\n    control_flow_count = all_control_flow_elements_count(bpmn_graph)\n\n    return activities_count + control_flow_count"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the value of the NOAJS metric for the BPMNDiagramGraph instance.", "response": "def NOAJS_metric(bpmn_graph):\n    \"\"\"\n    Returns the value of the NOAJS metric (Number of Activities, joins and splits)\n    for the BPMNDiagramGraph instance.\n\n    :param bpmn_graph: an instance of BpmnDiagramGraph representing BPMN model.\n    \"\"\"\n\n    activities_count = all_activities_count(bpmn_graph)\n    gateways_count = all_gateways_count(bpmn_graph)\n\n    return activities_count + gateways_count"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the value of the Number of Nodes metric", "response": "def NumberOfNodes_metric(bpmn_graph):\n    \"\"\"\n    Returns the value of the Number of Nodes metric\n    (\"Number of activities and routing elements in a model\")\n    for the BPMNDiagramGraph instance.\n\n    :param bpmn_graph: an instance of BpmnDiagramGraph representing BPMN model.\n    \"\"\"\n\n    activities_count = all_activities_count(bpmn_graph)\n    control_flow_count = all_control_flow_elements_count(bpmn_graph)\n\n    return activities_count + control_flow_count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the value of the Gateway Heterogenity metric", "response": "def GatewayHeterogenity_metric(bpmn_graph):\n    \"\"\"\n    Returns the value of the Gateway Heterogenity metric\n    (\"Number of different types of gateways used in a mode\")\n    for the BPMNDiagramGraph instance.\n\n    :param bpmn_graph: an instance of BpmnDiagramGraph representing BPMN model.\n    \"\"\"\n\n    gateway_counts = get_gateway_counts(bpmn_graph)\n    present_gateways = [gateway_name\n                        for gateway_name, count in gateway_counts.items()\n                        if count > 0]\n\n    return len(present_gateways)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the value of the Coefficient of Network Complexity metric", "response": "def CoefficientOfNetworkComplexity_metric(bpmn_graph):\n    \"\"\"\n    Returns the value of the Coefficient of Network Complexity metric\n    (\"Ratio of the total number of arcs in a process model to its total number of nodes.\")\n    for the BPMNDiagramGraph instance.\n\n    :param bpmn_graph: an instance of BpmnDiagramGraph representing BPMN model.\n    \"\"\"\n\n    return float(len(bpmn_graph.get_flows())) / float(len(bpmn_graph.get_nodes()))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef AverageGatewayDegree_metric(bpmn_graph):\n\n    gateways_ids = [gateway[0] for gateway in get_all_gateways(bpmn_graph)]\n    all_nodes_degrees = bpmn_graph.diagram_graph.degree()\n    gateways_degree_values = [all_nodes_degrees[gateway_id] for gateway_id in gateways_ids]\n\n    return float(sum(gateways_degree_values)) / float(len(gateways_degree_values))", "response": "Calculates the average gateway degree metric for the BPMN model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef DurfeeSquare_metric(bpmn_graph):\n\n    all_types_count = Counter([node[1]['type'] for node in bpmn_graph.get_nodes() if node[1]['type']])\n    length = len(all_types_count)\n\n    histogram = [0] * (length + 1)\n    for _, count in all_types_count.items():\n        histogram[min(count, length)] += 1\n\n    sum_ = 0\n    for i, count in reversed(list(enumerate(histogram))):\n        sum_ += count\n        if sum_ >= i:\n            return i\n\n    return 0", "response": "Returns the value of the Durfee Square metric in the BPMN model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the value of the Perfect Square metric", "response": "def PerfectSquare_metric(bpmn_graph):\n    \"\"\"\n    Returns the value of the Perfect Square metric\n    (\"Given a set of element types ranked\n    in decreasing order of the number of their instances,\n    the PSM is the (unique) largest number\n    such that the top p types occur(together)\n    at least p2 times.\")\n    for the BPMNDiagramGraph instance.\n\n    :param bpmn_graph: an instance of BpmnDiagramGraph representing BPMN model.\n    \"\"\"\n\n    all_types_count = Counter([node[1]['type'] for node in bpmn_graph.get_nodes() if node[1]['type']])\n    sorted_counts = [count for _, count in all_types_count.most_common()]\n\n    potential_perfect_square = min(len(sorted_counts), int(sqrt(sum(sorted_counts))))\n\n    for i in range(potential_perfect_square, 0, -1):\n        if sum(sorted_counts[:potential_perfect_square]) >= potential_perfect_square * potential_perfect_square:\n            return potential_perfect_square\n        else:\n            potential_perfect_square -= 1\n\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nroots method of CSV export functionality. :param bpmn_diagram: an instance of BpmnDiagramGraph class, :param directory: a string object, which is a path of output directory, :param filename: a string object, which is a name of output file.", "response": "def export_process_to_csv(bpmn_diagram, directory, filename):\n        \"\"\"\n        Root method of CSV export functionality.\n\n        :param bpmn_diagram: an instance of BpmnDiagramGraph class,\n        :param directory: a string object, which is a path of output directory,\n        :param filename: a string object, which is a name of output file.\n        \"\"\"\n        nodes = copy.deepcopy(bpmn_diagram.get_nodes())\n        start_nodes = []\n        export_elements = []\n\n        for node in nodes:\n            incoming_list = node[1].get(consts.Consts.incoming_flow)\n            if len(incoming_list) == 0:\n                start_nodes.append(node)\n        if len(start_nodes) != 1:\n            raise bpmn_exception.BpmnPythonError(\"Exporting to CSV format accepts only one start event\")\n\n        nodes_classification = utils.BpmnImportUtils.generate_nodes_clasification(bpmn_diagram)\n        start_node = start_nodes.pop()\n        BpmnDiagramGraphCsvExport.export_node(bpmn_diagram, export_elements, start_node, nodes_classification)\n\n        try:\n            os.makedirs(directory)\n        except OSError as exception:\n            if exception.errno != errno.EEXIST:\n                raise\n        file_object = open(directory + filename, \"w\")\n        file_object.write(\"Order,Activity,Condition,Who,Subprocess,Terminated\\n\")\n        BpmnDiagramGraphCsvExport.write_export_node_to_file(file_object, export_elements)\n        file_object.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_element(bpmn_graph, export_elements, node, nodes_classification, order=0, prefix=\"\", condition=\"\",\n                       who=\"\", add_join=False):\n        \"\"\"\n        Export a node with \"Element\" classification (task, subprocess or gateway)\n\n        :param bpmn_graph: an instance of BpmnDiagramGraph class,\n        :param export_elements: a dictionary object. The key is a node ID, value is a dictionary of parameters that\n               will be used in exported CSV document,\n        :param node: networkx.Node object,\n        :param nodes_classification: dictionary of classification labels. Key - node id. Value - a list of labels,\n        :param order: the order param of exported node,\n        :param prefix: the prefix of exported node - if the task appears after some gateway, the prefix will identify\n               the branch\n        :param condition: the condition param of exported node,\n        :param who: the condition param of exported node,\n        :param add_join: boolean flag. Used to indicate if \"Join\" element should be added to CSV.\n        :return: None or the next node object if the exported node was a gateway join.\n        \"\"\"\n        node_type = node[1][consts.Consts.type]\n        node_classification = nodes_classification[node[0]]\n\n        outgoing_flows = node[1].get(consts.Consts.outgoing_flow)\n        if node_type != consts.Consts.parallel_gateway and consts.Consts.default in node[1] \\\n                and node[1][consts.Consts.default] is not None:\n            default_flow_id = node[1][consts.Consts.default]\n        else:\n            default_flow_id = None\n\n        if BpmnDiagramGraphCsvExport.classification_join in node_classification and not add_join:\n            # If the node is a join, then retract the recursion back to the split.\n            # In case of activity - return current node. In case of gateway - return outgoing node\n            # (we are making assumption that join has only one outgoing node)\n            if node_type == consts.Consts.task or node_type == consts.Consts.subprocess:\n                return node\n            else:\n                outgoing_flow_id = outgoing_flows[0]\n                outgoing_flow = bpmn_graph.get_flow_by_id(outgoing_flow_id)\n                outgoing_node = bpmn_graph.get_node_by_id(outgoing_flow[2][consts.Consts.target_ref])\n                return outgoing_node\n        else:\n            if node_type == consts.Consts.task:\n                export_elements.append({\"Order\": prefix + str(order), \"Activity\": node[1][consts.Consts.node_name],\n                                        \"Condition\": condition, \"Who\": who, \"Subprocess\": \"\", \"Terminated\": \"\"})\n            elif node_type == consts.Consts.subprocess:\n                export_elements.append({\"Order\": prefix + str(order), \"Activity\": node[1][consts.Consts.node_name],\n                                        \"Condition\": condition, \"Who\": who, \"Subprocess\": \"yes\", \"Terminated\": \"\"})\n\n        if BpmnDiagramGraphCsvExport.classification_split in node_classification:\n            next_node = None\n            alphabet_suffix_index = 0\n            for outgoing_flow_id in outgoing_flows:\n                outgoing_flow = bpmn_graph.get_flow_by_id(outgoing_flow_id)\n                outgoing_node = bpmn_graph.get_node_by_id(outgoing_flow[2][consts.Consts.target_ref])\n\n                # This will work only up to 26 outgoing flows\n                suffix = string.ascii_lowercase[alphabet_suffix_index]\n                next_prefix = prefix + str(order) + suffix\n                alphabet_suffix_index += 1\n                # parallel gateway does not uses conditions\n                if node_type != consts.Consts.parallel_gateway and consts.Consts.name in outgoing_flow[2] \\\n                        and outgoing_flow[2][consts.Consts.name] is not None:\n                    condition = outgoing_flow[2][consts.Consts.name]\n                else:\n                    condition = \"\"\n\n                if BpmnDiagramGraphCsvExport.classification_join in nodes_classification[outgoing_node[0]]:\n                    export_elements.append(\n                        {\"Order\": next_prefix + str(1), \"Activity\": \"goto \" + prefix + str(order + 1),\n                         \"Condition\": condition, \"Who\": who, \"Subprocess\": \"\", \"Terminated\": \"\"})\n                elif outgoing_flow_id == default_flow_id:\n                    tmp_next_node = BpmnDiagramGraphCsvExport.export_node(bpmn_graph, export_elements, outgoing_node,\n                                                                          nodes_classification, 1, next_prefix, \"else\",\n                                                                          who)\n                    if tmp_next_node is not None:\n                        next_node = tmp_next_node\n                else:\n                    tmp_next_node = BpmnDiagramGraphCsvExport.export_node(bpmn_graph, export_elements, outgoing_node,\n                                                                          nodes_classification, 1, next_prefix,\n                                                                          condition, who)\n                    if tmp_next_node is not None:\n                        next_node = tmp_next_node\n\n            if next_node is not None:\n                return BpmnDiagramGraphCsvExport.export_node(bpmn_graph, export_elements, next_node,\n                                                             nodes_classification, order=(order + 1), prefix=prefix,\n                                                             who=who, add_join=True)\n\n        elif len(outgoing_flows) == 1:\n            outgoing_flow_id = outgoing_flows[0]\n            outgoing_flow = bpmn_graph.get_flow_by_id(outgoing_flow_id)\n            outgoing_node = bpmn_graph.get_node_by_id(outgoing_flow[2][consts.Consts.target_ref])\n            return BpmnDiagramGraphCsvExport.export_node(bpmn_graph, export_elements, outgoing_node,\n                                                         nodes_classification, order=(order + 1), prefix=prefix,\n                                                         who=who)\n        else:\n            return None", "response": "This function returns the exported element of a node in the CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts event export :param bpmn_graph: an instance of BpmnDiagramGraph class, :param export_elements: a dictionary object. The key is a node ID, value is a dictionary of parameters that will be used in exported CSV document, :param node: networkx.Node object, :param order: the order param of exported node, :param nodes_classification: dictionary of classification labels. Key - node id. Value - a list of labels, :param prefix: the prefix of exported node - if the task appears after some gateway, the prefix will identify the branch :param condition: the condition param of exported node, :param who: the condition param of exported node, :return: None or the next node object if the exported node was a gateway join.", "response": "def export_start_event(bpmn_graph, export_elements, node, nodes_classification, order=0, prefix=\"\", condition=\"\",\n                           who=\"\"):\n        \"\"\"\n        Start event export\n\n        :param bpmn_graph: an instance of BpmnDiagramGraph class,\n        :param export_elements: a dictionary object. The key is a node ID, value is a dictionary of parameters that\n               will be used in exported CSV document,\n        :param node: networkx.Node object,\n        :param order: the order param of exported node,\n        :param nodes_classification: dictionary of classification labels. Key - node id. Value - a list of labels,\n        :param prefix: the prefix of exported node - if the task appears after some gateway, the prefix will identify\n               the branch\n        :param condition: the condition param of exported node,\n        :param who: the condition param of exported node,\n        :return: None or the next node object if the exported node was a gateway join.\n        \"\"\"\n\n        # Assuming that there is only one event definition\n        event_definitions = node[1].get(consts.Consts.event_definitions)\n        if event_definitions is not None and len(event_definitions) > 0:\n            event_definition = node[1][consts.Consts.event_definitions][0]\n        else:\n            event_definition = None\n\n        if event_definition is None:\n            activity = node[1][consts.Consts.node_name]\n        elif event_definition[consts.Consts.definition_type] == \"messageEventDefinition\":\n            activity = \"message \" + node[1][consts.Consts.node_name]\n        elif event_definition[consts.Consts.definition_type] == \"timerEventDefinition\":\n            activity = \"timer \" + node[1][consts.Consts.node_name]\n        else:\n            activity = node[1][consts.Consts.node_name]\n\n        export_elements.append({\"Order\": prefix + str(order), \"Activity\": activity, \"Condition\": condition,\n                                \"Who\": who, \"Subprocess\": \"\", \"Terminated\": \"\"})\n\n        outgoing_flow_id = node[1][consts.Consts.outgoing_flow][0]\n        outgoing_flow = bpmn_graph.get_flow_by_id(outgoing_flow_id)\n        outgoing_node = bpmn_graph.get_node_by_id(outgoing_flow[2][consts.Consts.target_ref])\n        return BpmnDiagramGraphCsvExport.export_node(bpmn_graph, export_elements, outgoing_node, nodes_classification,\n                                                     order + 1, prefix, who)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_end_event(export_elements, node, order=0, prefix=\"\", condition=\"\", who=\"\"):\n\n        # Assuming that there is only one event definition\n        event_definitions = node[1].get(consts.Consts.event_definitions)\n        if event_definitions is not None and len(event_definitions) > 0:\n            event_definition = node[1][consts.Consts.event_definitions][0]\n        else:\n            event_definition = None\n\n        if event_definition is None:\n            activity = node[1][consts.Consts.node_name]\n        elif event_definition[consts.Consts.definition_type] == \"messageEventDefinition\":\n            activity = \"message \" + node[1][consts.Consts.node_name]\n        else:\n            activity = node[1][consts.Consts.node_name]\n\n        export_elements.append({\"Order\": prefix + str(order), \"Activity\": activity, \"Condition\": condition, \"Who\": who,\n                                \"Subprocess\": \"\", \"Terminated\": \"yes\"})\n        # No outgoing elements for EndEvent\n        return None", "response": "This function returns the EndEvent object in the CSV document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the export elements to a CSV file.", "response": "def write_export_node_to_file(file_object, export_elements):\n        \"\"\"\n        Exporting process to CSV file\n\n        :param file_object: object of File class,\n        :param export_elements: a dictionary object. The key is a node ID, value is a dictionary of parameters that\n               will be used in exported CSV document.\n        \"\"\"\n        for export_element in export_elements:\n            # Order,Activity,Condition,Who,Subprocess,Terminated\n            file_object.write(\n                export_element[\"Order\"] + \",\" + export_element[\"Activity\"] + \",\" + export_element[\"Condition\"] + \",\" +\n                export_element[\"Who\"] + \",\" + export_element[\"Subprocess\"] + \",\" + export_element[\"Terminated\"] + \"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_parallel_multiple(self, value):\n        if value is None or not isinstance(value, bool):\n            raise TypeError(\"ParallelMultiple must be set to a bool\")\n        else:\n            self.__parallel_multiple = value", "response": "Set the value of the ParallelMultiple field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the value of the triggered_by_event field.", "response": "def set_triggered_by_event(self, value):\n        \"\"\"\n        Setter for 'triggered_by_event' field.\n        :param value - a new value of 'triggered_by_event' field. Must be a boolean type. Does not accept None value.\n        \"\"\"\n        if value is None or not isinstance(value, bool):\n            raise TypeError(\"TriggeredByEvent must be set to a bool\")\n        else:\n            self.__triggered_by_event = value"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the value of the lane_set_list field.", "response": "def set_lane_set_list(self, value):\n        \"\"\"\n        Setter for 'lane_set_list' field.\n        :param value - a new value of 'lane_set_list' field. Must be a list\n        \"\"\"\n        if value is None or not isinstance(value, list):\n            raise TypeError(\"LaneSetList new value must be a list\")\n        else:\n            for element in value:\n                if not isinstance(element, lane_set.LaneSet):\n                    raise TypeError(\"LaneSetList elements in variable must be of LaneSet class\")\n            self.__lane_set_list = value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the value of the flow_element_list field.", "response": "def set_flow_element_list(self, value):\n        \"\"\"\n        Setter for 'flow_element_list' field.\n        :param value - a new value of 'flow_element_list' field. Must be a list\n        \"\"\"\n        if value is None or not isinstance(value, list):\n            raise TypeError(\"FlowElementList new value must be a list\")\n        else:\n            for element in value:\n                if not isinstance(element, flow_element.FlowElement):\n                    raise TypeError(\"FlowElementList elements in variable must be of FlowElement class\")\n            self.__flow_element_list = value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the value of the incoming field.", "response": "def set_incoming(self, value):\n        \"\"\"\n        Setter for 'incoming' field.\n        :param value - a new value of 'incoming' field. List of IDs (String type) of incoming flows.\n        \"\"\"\n        if not isinstance(value, list):\n            raise TypeError(\"IncomingList new value must be a list\")\n        for element in value:\n            if not isinstance(element, str):\n                raise TypeError(\"IncomingList elements in variable must be of String class\")\n        self.__incoming_list = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_outgoing(self, value):\n        if not isinstance(value, list):\n            raise TypeError(\"OutgoingList new value must be a list\")\n        for element in value:\n            if not isinstance(element, str):\n                raise TypeError(\"OutgoingList elements in variable must be of String class\")\n        self.__outgoing_list = value", "response": "Set the value of the outgoing list field."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_process_type(self, value):\n        if value is None or not isinstance(value, str):\n            raise TypeError(\"ProcessType must be set to a String\")\n        elif value not in Process.__process_type_list:\n            raise ValueError(\"ProcessType must be one of specified values: 'None', 'Public', 'Private'\")\n        else:\n            self.__process_type = value", "response": "Setter for process_type field."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the IsClosed field.", "response": "def set_is_closed(self, value):\n        \"\"\"\n        Setter for 'is_closed' field.\n        :param value - a new value of 'is_closed' field. Must be a boolean type. Does not accept None value.\n        \"\"\"\n        if value is None or not isinstance(value, bool):\n            raise TypeError(\"IsClosed must be set to a bool\")\n        else:\n            self.__is_closed = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the IsExecutable field.", "response": "def set_is_executable(self, value):\n        \"\"\"\n        Setter for 'is_executable' field.\n        :param value - a new value of 'is_executable' field. Must be a boolean type. Does not accept None value.\n        \"\"\"\n        if value is None or not isinstance(value, bool):\n            raise TypeError(\"IsExecutable must be set to a bool\")\n        else:\n            self.__is_executable = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisplays a simple visualization of the diagram.", "response": "def visualize_diagram(bpmn_diagram):\n    \"\"\"\n    Shows a simple visualization of diagram\n\n    :param bpmn_diagram: an instance of BPMNDiagramGraph class.\n    \"\"\"\n    g = bpmn_diagram.diagram_graph\n    pos = bpmn_diagram.get_nodes_positions()\n    nx.draw_networkx_nodes(g, pos, node_shape='s', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.task))\n    nx.draw_networkx_nodes(g, pos, node_shape='s', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.subprocess))\n    nx.draw_networkx_nodes(g, pos, node_shape='d', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.complex_gateway))\n    nx.draw_networkx_nodes(g, pos, node_shape='o', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.event_based_gateway))\n    nx.draw_networkx_nodes(g, pos, node_shape='d', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.inclusive_gateway))\n    nx.draw_networkx_nodes(g, pos, node_shape='d', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.exclusive_gateway))\n    nx.draw_networkx_nodes(g, pos, node_shape='d', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.parallel_gateway))\n    nx.draw_networkx_nodes(g, pos, node_shape='o', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.start_event))\n    nx.draw_networkx_nodes(g, pos, node_shape='o', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.intermediate_catch_event))\n    nx.draw_networkx_nodes(g, pos, node_shape='o', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.end_event))\n    nx.draw_networkx_nodes(g, pos, node_shape='o', node_color='white',\n                           nodelist=bpmn_diagram.get_nodes_id_list_by_type(consts.Consts.intermediate_throw_event))\n\n    node_labels = {}\n    for node in g.nodes(data=True):\n        node_labels[node[0]] = node[1].get(consts.Consts.node_name)\n    nx.draw_networkx_labels(g, pos, node_labels)\n\n    nx.draw_networkx_edges(g, pos)\n\n    edge_labels = {}\n    for edge in g.edges(data=True):\n        edge_labels[(edge[0], edge[1])] = edge[2].get(consts.Consts.name)\n    nx.draw_networkx_edge_labels(g, pos, edge_labels)\n\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a png picture for given diagram.", "response": "def bpmn_diagram_to_png(bpmn_diagram, file_name):\n    \"\"\"\n    Create a png picture for given diagram\n\n    :param bpmn_diagram: an instance of BPMNDiagramGraph class,\n    :param file_name: name of generated file.\n    \"\"\"\n    g = bpmn_diagram.diagram_graph\n    graph = pydotplus.Dot()\n\n    for node in g.nodes(data=True):\n\n        if node[1].get(consts.Consts.type) == consts.Consts.task:\n            n = pydotplus.Node(name=node[0], shape=\"box\", style=\"rounded\", label=node[1].get(consts.Consts.node_name))\n        elif node[1].get(consts.Consts.type) == consts.Consts.exclusive_gateway:\n            n = pydotplus.Node(name=node[0], shape=\"diamond\", label=node[1].get(consts.Consts.node_name))\n        else:\n            n = pydotplus.Node(name=node[0], label=node[1].get(consts.Consts.node_name))\n        graph.add_node(n)\n\n    for edge in g.edges(data=True):\n        e = pydotplus.Edge(src=edge[0], dst=edge[1], label=edge[2].get(consts.Consts.name))\n        graph.add_edge(e)\n\n    graph.write(file_name + \".png\", format='png')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_id(self, value):\n        if value is None:\n            self.__id = value\n        if not isinstance(value, str):\n            raise TypeError(\"ID must be set to a String\")\n        else:\n            self.__id = value", "response": "Setter for id field."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the value of the source_ref field. Required field must be a String type.", "response": "def set_source_ref(self, value):\n        \"\"\"\n        Setter for 'source_ref' field.\n        :param value - a new value of 'source_ref' field. Required field. Must be a String type.\n        \"\"\"\n        if value is None or not isinstance(value, str):\n            raise TypeError(\"SourceRef is required and must be set to a String\")\n        else:\n            self.__source_ref = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_is_immediate(self, value):\n        if value is None:\n            self.__is_immediate = value\n        elif not isinstance(value, bool):\n            raise TypeError(\"IsImediate must be set to a bool\")\n        else:\n            self.__is_immediate = value", "response": "Set the IsImediate field."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_condition_expression(self, value):\n        if value is None:\n            self.__condition_expression = value\n        if not isinstance(value, condition_expression.ConditionExpression):\n            raise TypeError(\"ConditionExpression must be set to an instance of class ConditionExpression\")\n        else:\n            self.__condition_expression = value", "response": "Setter for condition_expression field."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_default(self, value):\n        if value is None:\n            self.__default = value\n        elif not isinstance(value, str):\n            raise TypeError(\"Default must be set to a String\")\n        else:\n            self.__default = value", "response": "Set the default value of the national product s log entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the orientation of three points p1 p2 and p3.", "response": "def orientation(p1, p2, p3):\n    \"\"\"\n    Finds orientation of three points p1, p2, p3.\n    The function returns following values\n    0 --> p1, p2 and p3 are collinear\n    1 --> Clockwise\n    2 --> Counterclockwise\n    :param p1: tuple representing two dimensional point\n    :param p2: tuple representing two dimensional point\n    :param p3: tuple representing two dimensional point\n    \"\"\"\n    val = (p2[consts.Consts.y] - p1[consts.Consts.y]) * (p3[consts.Consts.x] - p2[consts.Consts.x]) \\\n        - (p2[consts.Consts.x] - p1[consts.Consts.x]) * (p3[consts.Consts.y] - p2[consts.Consts.y])\n\n    if val == 0:\n        return 0  # collinear\n    elif val > 0:\n        return 1  # clockwise\n    else:\n        return 2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_xml_file(self, directory, filename):\n        bpmn_export.BpmnDiagramGraphExport.export_xml_file(directory, filename, self)", "response": "Exports diagram inner graph to BPMN 2. 0 XML file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexport diagram inner graph to BPMN 2. 0 XML file without Diagram Interchange data.", "response": "def export_xml_file_no_di(self, directory, filename):\n        \"\"\"\n        Exports diagram inner graph to BPMN 2.0 XML file (without Diagram Interchange data).\n\n        :param directory: strings representing output directory,\n        :param filename: string representing output file name.\n        \"\"\"\n        bpmn_export.BpmnDiagramGraphExport.export_xml_file_no_di(directory, filename, self)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport diagram inner graph to BPMN 2. 0 XML file.", "response": "def export_csv_file(self, directory, filename):\n        \"\"\"\n        Exports diagram inner graph to BPMN 2.0 XML file (with Diagram Interchange data).\n\n        :param directory: strings representing output directory,\n        :param filename: string representing output file name.\n        \"\"\"\n        bpmn_csv_export.BpmnDiagramGraphCsvExport.export_process_to_csv(self, directory, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all nodes of requested type in BPMN diagram graph.", "response": "def get_nodes(self, node_type=\"\"):\n        \"\"\"\n        Gets all nodes of requested type. If no type is provided by user, all nodes in BPMN diagram graph are returned.\n        Returns a dictionary, where key is an ID of node, value is a dictionary of all node attributes.\n\n        :param node_type: string with valid BPMN XML tag name (e.g. 'task', 'sequenceFlow').\n        \"\"\"\n        tmp_nodes = self.diagram_graph.nodes(True)\n        if node_type == \"\":\n            return tmp_nodes\n        else:\n            nodes = []\n            for node in tmp_nodes:\n                if node[1][consts.Consts.type] == node_type:\n                    nodes.append(node)\n            return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_nodes_list_by_process_id(self, process_id):\n        tmp_nodes = self.diagram_graph.nodes(True)\n        nodes = []\n        for node in tmp_nodes:\n            if node[1][consts.Consts.process] == process_id:\n                nodes.append(node)\n        return nodes", "response": "Returns a list of all nodes of the requested type in BPMN diagram graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a node with the given ID.", "response": "def get_node_by_id(self, node_id):\n        \"\"\"\n        Gets a node with requested ID.\n        Returns a tuple, where first value is node ID, second - a dictionary of all node attributes.\n\n        :param node_id: string with ID of node.\n        \"\"\"\n        tmp_nodes = self.diagram_graph.nodes(data=True)\n        for node in tmp_nodes:\n            if node[0] == node_id:\n                return node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_nodes_id_list_by_type(self, node_type):\n        tmp_nodes = self.diagram_graph.nodes(data=True)\n        id_list = []\n        for node in tmp_nodes:\n            if node[1][consts.Consts.type] == node_type:\n                id_list.append(node[0])\n        return id_list", "response": "Get a list of node s id by requested type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_flow_by_id(self, flow_id):\n        tmp_flows = self.diagram_graph.edges(data=True)\n        for flow in tmp_flows:\n            if flow[2][consts.Consts.id] == flow_id:\n                return flow", "response": "Gets an edge with requested ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_flows_list_by_process_id(self, process_id):\n        tmp_flows = self.diagram_graph.edges(data=True)\n        flows = []\n        for flow in tmp_flows:\n            if consts.Consts.process in flow[2] and flow[2][consts.Consts.process] == process_id:\n                flows.append(flow)\n        return flows", "response": "Gets an edge with requested ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new BPMN diagram graph.", "response": "def create_new_diagram_graph(self, diagram_name=\"\"):\n        \"\"\"\n        Initializes a new BPMN diagram and sets up a basic diagram attributes.\n        Accepts a user-defined values for following attributes:\n        (Diagram element)\n\n        - name - default value empty string.\n\n        :param diagram_name: string type. Represents a user-defined value of 'BPMNDiagram' element\n            attribute 'name'. Default value - empty string.\n        \"\"\"\n        self.__init__()\n        diagram_id = BpmnDiagramGraph.id_prefix + str(uuid.uuid4())\n\n        self.diagram_attributes[consts.Consts.id] = diagram_id\n        self.diagram_attributes[consts.Consts.name] = diagram_name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_process_to_diagram(self, process_name=\"\", process_is_closed=False, process_is_executable=False,\n                               process_type=\"None\"):\n        \"\"\"\n        Adds a new process to diagram and corresponding participant\n            process, diagram and plane\n\n        Accepts a user-defined values for following attributes:\n        (Process element)\n        - isClosed - default value false,\n        - isExecutable - default value false,\n        - processType - default value None.\n\n        :param process_name: string obejct, process name. Default value - empty string,\n        :param process_is_closed: boolean type. Represents a user-defined value of 'process' element\n            attribute 'isClosed'. Default value false,\n        :param process_is_executable: boolean type. Represents a user-defined value of 'process' element\n            attribute 'isExecutable'. Default value false,\n        :param process_type: string type. Represents a user-defined value of 'process' element\n            attribute 'procesType'. Default value \"None\",\n        \"\"\"\n        plane_id = BpmnDiagramGraph.id_prefix + str(uuid.uuid4())\n        process_id = BpmnDiagramGraph.id_prefix + str(uuid.uuid4())\n\n        self.process_elements[process_id] = {consts.Consts.name: process_name,\n                                             consts.Consts.is_closed: \"true\" if process_is_closed else \"false\",\n                                             consts.Consts.is_executable: \"true\" if process_is_executable else \"false\",\n                                             consts.Consts.process_type: process_type}\n\n        self.plane_attributes[consts.Consts.id] = plane_id\n        self.plane_attributes[consts.Consts.bpmn_element] = process_id\n        return process_id", "response": "Adds a new process to diagram and corresponding participant\n            process element and diagram plane."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_flow_node_to_diagram(self, process_id, node_type, name, node_id=None):\n        if node_id is None:\n            node_id = BpmnDiagramGraph.id_prefix + str(uuid.uuid4())\n        self.diagram_graph.add_node(node_id)\n        self.diagram_graph.node[node_id][consts.Consts.id] = node_id\n        self.diagram_graph.node[node_id][consts.Consts.type] = node_type\n        self.diagram_graph.node[node_id][consts.Consts.node_name] = name\n        self.diagram_graph.node[node_id][consts.Consts.incoming_flow] = []\n        self.diagram_graph.node[node_id][consts.Consts.outgoing_flow] = []\n        self.diagram_graph.node[node_id][consts.Consts.process] = process_id\n\n        # Adding some dummy constant values\n        self.diagram_graph.node[node_id][consts.Consts.width] = \"100\"\n        self.diagram_graph.node[node_id][consts.Consts.height] = \"100\"\n        self.diagram_graph.node[node_id][consts.Consts.x] = \"100\"\n        self.diagram_graph.node[node_id][consts.Consts.y] = \"100\"\n        return node_id, self.diagram_graph.node[node_id]", "response": "This method is used to add a new Flow Node to the diagram. It is used to add a new Flow Node to diagram. It is used to add a new Flow Node to diagram. It is used to add a new Flow Node to diagram. It is used to add a new Flow Node to diagram. It is used to add a basic information inherited from Flow Node type."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a Task element to BPMN diagram.", "response": "def add_task_to_diagram(self, process_id, task_name=\"\", node_id=None):\n        \"\"\"\n        Adds a Task element to BPMN diagram.\n        User-defined attributes:\n\n        - name\n\n\n        :param process_id: string object. ID of parent process,\n        :param task_name: string object. Name of task,\n        :param node_id: string object. ID of node. Default value - None.\n        :return: a tuple, where first value is task ID, second a reference to created object.\n        \"\"\"\n        return self.add_flow_node_to_diagram(process_id, consts.Consts.task, task_name, node_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a SubProcess element to BPMN diagram.", "response": "def add_subprocess_to_diagram(self, process_id, subprocess_name, is_expanded=False, triggered_by_event=False,\n                                  node_id=None):\n        \"\"\"\n        Adds a SubProcess element to BPMN diagram.\n        User-defined attributes:\n\n        - name\n        - triggered_by_event\n\n\n        :param process_id: string object. ID of parent process,\n        :param subprocess_name: string object. Name of subprocess,\n        :param is_expanded: boolean value for attribute \"isExpanded\". Default value false,\n        :param triggered_by_event: boolean value for attribute \"triggeredByEvent\". Default value false,\n        :param node_id: string object. ID of node. Default value - None.\n        :return: a tuple, where first value is subProcess ID, second a reference to created object.\n        \"\"\"\n        subprocess_id, subprocess = self.add_flow_node_to_diagram(process_id, consts.Consts.subprocess, subprocess_name,\n                                                                  node_id)\n        self.diagram_graph.node[subprocess_id][consts.Consts.is_expanded] = \"true\" if is_expanded else \"false\"\n        self.diagram_graph.node[subprocess_id][consts.Consts.triggered_by_event] = \\\n            \"true\" if triggered_by_event else \"false\"\n        return subprocess_id, subprocess"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_start_event_to_diagram(self, process_id, start_event_name=\"\", start_event_definition=None,\n                                   parallel_multiple=False, is_interrupting=True, node_id=None):\n        \"\"\"\n        Adds a StartEvent element to BPMN diagram.\n\n        User-defined attributes:\n\n        - name\n        - parallel_multiple\n        - is_interrupting\n        - event definition (creates a special type of start event). Supported event definitions -\n            * 'message': 'messageEventDefinition', \n            * 'timer': 'timerEventDefinition', \n            * 'signal': 'signalEventDefinition',\n            * 'conditional': 'conditionalEventDefinition', \n            * 'escalation': 'escalationEventDefinition'.\n\n        :param process_id: string object. ID of parent process,\n        :param start_event_name: string object. Name of start event,\n        :param start_event_definition: list of event definitions. By default - empty,\n        :param parallel_multiple: boolean value for attribute \"parallelMultiple\",\n        :param is_interrupting: boolean value for attribute \"isInterrupting,\n        :param node_id: string object. ID of node. Default value - None.\n\n        :return: a tuple, where first value is startEvent ID, second a reference to created object.\n        \"\"\"\n        start_event_id, start_event = self.add_flow_node_to_diagram(process_id, consts.Consts.start_event,\n                                                                    start_event_name, node_id)\n        self.diagram_graph.node[start_event_id][consts.Consts.parallel_multiple] = \\\n            \"true\" if parallel_multiple else \"false\"\n        self.diagram_graph.node[start_event_id][consts.Consts.is_interrupting] = \"true\" if is_interrupting else \"false\"\n        start_event_definitions = {\"message\": \"messageEventDefinition\", \"timer\": \"timerEventDefinition\",\n                                   \"conditional\": \"conditionalEventDefinition\", \"signal\": \"signalEventDefinition\",\n                                   \"escalation\": \"escalationEventDefinition\"}\n        event_def_list = []\n        if start_event_definition == \"message\":\n            event_def_list.append(BpmnDiagramGraph.add_event_definition_element(\"message\", start_event_definitions))\n        elif start_event_definition == \"timer\":\n            event_def_list.append(BpmnDiagramGraph.add_event_definition_element(\"timer\", start_event_definitions))\n        elif start_event_definition == \"conditional\":\n            event_def_list.append(BpmnDiagramGraph.add_event_definition_element(\"conditional\", start_event_definitions))\n        elif start_event_definition == \"signal\":\n            event_def_list.append(BpmnDiagramGraph.add_event_definition_element(\"signal\", start_event_definitions))\n        elif start_event_definition == \"escalation\":\n            event_def_list.append(BpmnDiagramGraph.add_event_definition_element(\"escalation\", start_event_definitions))\n\n        self.diagram_graph.node[start_event_id][consts.Consts.event_definitions] = event_def_list\n        return start_event_id, start_event", "response": "Adds a StartEvent element to BPMN diagram."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_end_event_to_diagram(self, process_id, end_event_name=\"\", end_event_definition=None, node_id=None):\n        end_event_id, end_event = self.add_flow_node_to_diagram(process_id, consts.Consts.end_event, end_event_name,\n                                                                node_id)\n        end_event_definitions = {\"terminate\": \"terminateEventDefinition\", \"escalation\": \"escalationEventDefinition\",\n                                 \"message\": \"messageEventDefinition\", \"compensate\": \"compensateEventDefinition\",\n                                 \"signal\": \"signalEventDefinition\", \"error\": \"errorEventDefinition\"}\n        event_def_list = []\n        if end_event_definition == \"terminate\":\n            event_def_list.append(self.add_event_definition_element(\"terminate\", end_event_definitions))\n        elif end_event_definition == \"escalation\":\n            event_def_list.append(self.add_event_definition_element(\"escalation\", end_event_definitions))\n        elif end_event_definition == \"message\":\n            event_def_list.append(self.add_event_definition_element(\"message\", end_event_definitions))\n        elif end_event_definition == \"compensate\":\n            event_def_list.append(self.add_event_definition_element(\"compensate\", end_event_definitions))\n        elif end_event_definition == \"signal\":\n            event_def_list.append(self.add_event_definition_element(\"signal\", end_event_definitions))\n        elif end_event_definition == \"error\":\n            event_def_list.append(self.add_event_definition_element(\"error\", end_event_definitions))\n\n        self.diagram_graph.node[end_event_id][consts.Consts.event_definitions] = event_def_list\n        return end_event_id, end_event", "response": "Adds an EndEvent element to BPMN diagram."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd an exclusiveGateway element to the diagram.", "response": "def add_gateway_to_diagram(self, process_id, gateway_type, gateway_name=\"\", gateway_direction=\"Unspecified\",\n                               node_id=None):\n        \"\"\"\n        Adds an exclusiveGateway element to BPMN diagram.\n\n        :param process_id: string object. ID of parent process,\n        :param gateway_type: string object. Type of gateway to be added.\n        :param gateway_name: string object. Name of exclusive gateway,\n        :param gateway_direction: string object. Accepted values - \"Unspecified\", \"Converging\", \"Diverging\", \"Mixed\".\n            Default value - \"Unspecified\",\n        :param node_id: string object. ID of node. Default value - None.\n\n        :return: a tuple, where first value is gateway ID, second a reference to created object.\n        \"\"\"\n        gateway_id, gateway = self.add_flow_node_to_diagram(process_id, gateway_type, gateway_name, node_id)\n        if not (gateway_direction in (\"Unspecified\", \"Converging\", \"Diverging\", \"Mixed\")):\n            raise bpmn_exception.BpmnPythonError(\"Invalid value passed as gatewayDirection parameter. Value passed: \"\n                                                 + gateway_direction)\n        self.diagram_graph.node[gateway_id][consts.Consts.gateway_direction] = gateway_direction\n        return gateway_id, gateway"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_exclusive_gateway_to_diagram(self, process_id, gateway_name=\"\", gateway_direction=\"Unspecified\",\n                                         default=None, node_id=None):\n        \"\"\"\n        Adds an exclusiveGateway element to BPMN diagram.\n\n        :param process_id: string object. ID of parent process,\n        :param gateway_name: string object. Name of exclusive gateway,\n        :param gateway_direction: string object. Accepted values - \"Unspecified\", \"Converging\", \"Diverging\", \"Mixed\".\n            Default value - \"Unspecified\".\n        :param default: string object. ID of flow node, target of gateway default path. Default value - None,\n        :param node_id: string object. ID of node. Default value - None.\n        \n        :return: a tuple, where first value is exculusiveGateway ID, second a reference to created object.\n        \"\"\"\n        exclusive_gateway_id, exclusive_gateway = self.add_gateway_to_diagram(process_id,\n                                                                              consts.Consts.exclusive_gateway,\n                                                                              gateway_name=gateway_name,\n                                                                              gateway_direction=gateway_direction,\n                                                                              node_id=node_id)\n        self.diagram_graph.node[exclusive_gateway_id][consts.Consts.default] = default\n        return exclusive_gateway_id, exclusive_gateway", "response": "Adds an exclusiveGateway element to BPMN diagram."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an inclusiveGateway element to BPMN diagram.", "response": "def add_inclusive_gateway_to_diagram(self, process_id, gateway_name=\"\", gateway_direction=\"Unspecified\",\n                                         default=None, node_id=None):\n        \"\"\"\n        Adds an inclusiveGateway element to BPMN diagram.\n\n        :param process_id: string object. ID of parent process,\n        :param gateway_name: string object. Name of inclusive gateway,\n        :param gateway_direction: string object. Accepted values - \"Unspecified\", \"Converging\", \"Diverging\", \"Mixed\".\n           Default value - \"Unspecified\",\n        :param default: string object. ID of flow node, target of gateway default path. Default value - None,\n        :param node_id: string object. ID of node. Default value - None.\n\n        :return: a tuple, where first value is inclusiveGateway ID, second a reference to created object.\n        \"\"\"\n        inclusive_gateway_id, inclusive_gateway = self.add_gateway_to_diagram(process_id,\n                                                                              consts.Consts.inclusive_gateway,\n                                                                              gateway_name=gateway_name,\n                                                                              gateway_direction=gateway_direction,\n                                                                              node_id=node_id)\n        self.diagram_graph.node[inclusive_gateway_id][consts.Consts.default] = default\n        return inclusive_gateway_id, inclusive_gateway"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an parallelGateway element to BPMN diagram.", "response": "def add_parallel_gateway_to_diagram(self, process_id, gateway_name=\"\", gateway_direction=\"Unspecified\",\n                                        node_id=None):\n        \"\"\"\n        Adds an parallelGateway element to BPMN diagram.\n\n        :param process_id: string object. ID of parent process,\n        :param gateway_name: string object. Name of inclusive gateway,\n        :param gateway_direction: string object. Accepted values - \"Unspecified\", \"Converging\", \"Diverging\", \"Mixed\".\n            Default value - \"Unspecified\",\n        :param node_id: string object. ID of node. Default value - None.\n\n        :return: a tuple, where first value is parallelGateway ID, second a reference to created object.\n        \"\"\"\n        parallel_gateway_id, parallel_gateway = self.add_gateway_to_diagram(process_id,\n                                                                            consts.Consts.parallel_gateway,\n                                                                            gateway_name=gateway_name,\n                                                                            gateway_direction=gateway_direction,\n                                                                            node_id=node_id)\n        return parallel_gateway_id, parallel_gateway"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_sequence_flow_to_diagram(self, process_id, source_ref_id, target_ref_id, sequence_flow_name=\"\"):\n        sequence_flow_id = BpmnDiagramGraph.id_prefix + str(uuid.uuid4())\n        self.sequence_flows[sequence_flow_id] = {consts.Consts.name: sequence_flow_name,\n                                                 consts.Consts.source_ref: source_ref_id,\n                                                 consts.Consts.target_ref: target_ref_id}\n        self.diagram_graph.add_edge(source_ref_id, target_ref_id)\n        flow = self.diagram_graph[source_ref_id][target_ref_id]\n        flow[consts.Consts.id] = sequence_flow_id\n        flow[consts.Consts.name] = sequence_flow_name\n        flow[consts.Consts.process] = process_id\n        flow[consts.Consts.source_ref] = source_ref_id\n        flow[consts.Consts.target_ref] = target_ref_id\n        source_node = self.diagram_graph.node[source_ref_id]\n        target_node = self.diagram_graph.node[target_ref_id]\n        flow[consts.Consts.waypoints] = \\\n            [(source_node[consts.Consts.x], source_node[consts.Consts.y]),\n             (target_node[consts.Consts.x], target_node[consts.Consts.y])]\n\n        # add target node (target_ref_id) as outgoing node from source node (source_ref_id)\n        source_node[consts.Consts.outgoing_flow].append(sequence_flow_id)\n\n        # add source node (source_ref_id) as incoming node to target node (target_ref_id)\n        target_node[consts.Consts.incoming_flow].append(sequence_flow_id)\n        return sequence_flow_id, flow", "response": "Adds a SequenceFlow element to the diagram."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_nodes_positions(self):\n        nodes = self.get_nodes()\n        output = {}\n        for node in nodes:\n            output[node[0]] = (float(node[1][consts.Consts.x]), float(node[1][consts.Consts.y]))\n        return output", "response": "Getter method for nodes positions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_tree(path, depth=DEPTH):\n    os.mkdir(path)\n    for i in range(NUM_FILES):\n        filename = os.path.join(path, 'file{0:03}.txt'.format(i))\n        with open(filename, 'wb') as f:\n            f.write(b'foo')\n    if depth <= 1:\n        return\n    for i in range(NUM_DIRS):\n        dirname = os.path.join(path, 'dir{0:03}'.format(i))\n        create_tree(dirname, depth - 1)", "response": "Create a directory tree at path with given depth and NUM_DIRS and NUM_FILES at each level."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning total size of all files in directory tree at path.", "response": "def get_tree_size(path):\n    \"\"\"Return total size of all files in directory tree at path.\"\"\"\n    size = 0\n    try:\n        for entry in scandir.scandir(path):\n            if entry.is_symlink():\n                pass\n            elif entry.is_dir():\n                size += get_tree_size(os.path.join(path, entry.name))\n            else:\n                size += entry.stat().st_size\n    except OSError:\n        pass\n    return size"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unfold(tensor, mode):\n    return np.moveaxis(tensor, mode, 0).reshape((tensor.shape[mode], -1))", "response": "Returns the mode - mode unfolding of tensor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef soft_cluster_factor(factor):\n\n    # copy factor of interest\n    f = np.copy(factor)\n\n    # cluster based on score of maximum absolute value\n    cluster_ids = np.argmax(np.abs(f), axis=1)\n    scores = f[range(f.shape[0]), cluster_ids]\n\n    # resort within each cluster\n    perm = []\n    for cluster in np.unique(cluster_ids):\n        idx = np.where(cluster_ids == cluster)[0]\n        perm += list(idx[np.argsort(scores[idx])][::-1])\n\n    return cluster_ids, perm", "response": "Returns soft - clustering of data based on CP decomposition results."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tsp_linearize(data, niter=1000, metric='euclidean', **kwargs):\n\n    # Compute pairwise distances between all datapoints\n    N = data.shape[0]\n    D = scipy.spatial.distance.pdist(data, metric=metric, **kwargs)\n\n    # To solve the travelling salesperson problem with no return to the\n    # original node we add a dummy node that has distance zero connections\n    # to all other nodes. The dummy node is then removed after we've converged\n    # to a solution.\n    dist = np.zeros((N+1, N+1))\n    dist[:N, :N] = scipy.spatial.distance.squareform(D)\n\n    # solve TSP\n    perm, cost_hist = _solve_tsp(dist)\n\n    # remove dummy node at position i\n    i = np.argwhere(perm == N).ravel()[0]\n    perm = np.hstack((perm[(i+1):], perm[:i]))\n\n    return perm", "response": "Sorts a matrix dataset to linearize the salesperson problem."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hclust_linearize(U):\n\n    from scipy.cluster import hierarchy\n    Z = hierarchy.ward(U)\n    return hierarchy.leaves_list(hierarchy.optimal_leaf_ordering(Z, U))", "response": "Sorts the rows of a matrix by hierarchical clustering."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reverse_segment(path, n1, n2):\n    q = path.copy()\n    if n2 > n1:\n        q[n1:(n2+1)] = path[n1:(n2+1)][::-1]\n        return q\n    else:\n        seg = np.hstack((path[n1:], path[:(n2+1)]))[::-1]\n        brk = len(q) - n1\n        q[n1:] = seg[:brk]\n        q[:(n2+1)] = seg[brk:]\n        return q", "response": "Reverse the nodes between n1 and n2."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsolving travelling salesperson problem by two - opt swapping.", "response": "def _solve_tsp(dist, niter):\n    \"\"\"Solve travelling salesperson problem (TSP) by two-opt swapping.\n\n    Params\n    ------\n    dist (ndarray) : distance matrix\n\n    Returns\n    -------\n    path (ndarray) : permutation of nodes in graph (rows of dist matrix)\n    \"\"\"\n\n    # number of nodes\n    N = dist.shape[0]\n\n    # tsp path for quick calculation of cost\n    ii = np.arange(N)\n    jj = np.hstack((np.arange(1, N), 0))\n\n    # for each node, cache a sorted list of all other nodes in order of\n    # increasing distance.\n    dsort = [np.argsort(d) for d in dist]\n    dsort = [d[d != i] for i, d in enumerate(dsort)]\n\n    # randomly initialize path through graph\n    path = np.random.permutation(N)\n    idx = np.argsort(path)\n    cost = np.sum(dist[path[ii], path[jj]])\n\n    # keep track of objective function over time\n    cost_hist = [cost]\n\n    # optimization loop\n    node = 0\n    itercount = 0\n    n = 0\n\n    while n < N and itercount < niter:\n\n        # count iterations\n        itercount += 1\n\n        # we'll try breaking the connection i -> j\n        i = path[node]\n        j = path[(node+1) % N]\n\n        # We are breaking i -> j so we can remove the cost of that connection.\n        c = cost - dist[i, j]\n\n        # Search over nodes k that are closer to j than i.\n        for k in dsort[j]:\n\n            # Can safely continue if dist[i,j] < dist[k,j] for the remaining k.\n            if k == i:\n                n += 1\n                break\n\n            # Break connection k -> p.\n            # Add connection j -> p.\n            # Add connection i -> k.\n            p = path[(idx[k]+1) % N]\n            new_cost = c - dist[k, p] + dist[j, p] + dist[i, k]\n\n            # If this swap improves the cost, implement it and move to next i.\n            if new_cost < cost:\n                path = reverse_segment(path, idx[j], idx[k])\n                idx = np.argsort(path)\n                cost = new_cost\n                # Restart from the begining of the graph.\n                cost_hist.append(cost)\n                n = 0\n                break\n\n        # move to next node\n        node = (node + 1) % N\n\n    return path, cost_hist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a KTensor to a dense ndarray.", "response": "def full(self):\n        \"\"\"Converts KTensor to a dense ndarray.\"\"\"\n\n        # Compute tensor unfolding along first mode\n        unf = sci.dot(self.factors[0], khatri_rao(self.factors[1:]).T)\n\n        # Inverse unfolding along first mode\n        return sci.reshape(unf, self.shape)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rebalance(self):\n\n        # Compute norms along columns for each factor matrix\n        norms = [sci.linalg.norm(f, axis=0) for f in self.factors]\n\n        # Multiply norms across all modes\n        lam = sci.multiply.reduce(norms) ** (1/self.ndim)\n\n        # Update factors\n        self.factors = [f * (lam / fn) for f, fn in zip(self.factors, norms)]\n        return self", "response": "Rescales factors across all modes so that all norms match."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef permute(self, idx):\n\n        # Check that input is a true permutation\n        if set(idx) != set(range(self.rank)):\n            raise ValueError('Invalid permutation specified.')\n\n        # Update factors\n        self.factors = [f[:, idx] for f in self.factors]\n        return self.factors", "response": "Permutes the columns of the factor matrices inplace\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nalign two KTensors and returns a similarity score between zero and one.", "response": "def kruskal_align(U, V, permute_U=False, permute_V=False):\n    \"\"\"Aligns two KTensors and returns a similarity score.\n\n    Parameters\n    ----------\n    U : KTensor\n        First kruskal tensor to align.\n    V : KTensor\n        Second kruskal tensor to align.\n    permute_U : bool\n        If True, modifies 'U' to align the KTensors (default is False).\n    permute_V : bool\n        If True, modifies 'V' to align the KTensors (default is False).\n\n    Notes\n    -----\n    If both `permute_U` and `permute_V` are both set to True, then the\n    factors are ordered from most to least similar. If only one is\n    True then the factors on the modified KTensor are re-ordered to\n    match the factors in the un-aligned KTensor.\n\n    Returns\n    -------\n    similarity : float\n        Similarity score between zero and one.\n    \"\"\"\n\n    # Compute similarity matrices.\n    unrm = [f / np.linalg.norm(f, axis=0) for f in U.factors]\n    vnrm = [f / np.linalg.norm(f, axis=0) for f in V.factors]\n    sim_matrices = [np.dot(u.T, v) for u, v in zip(unrm, vnrm)]\n    cost = 1 - np.mean(np.abs(sim_matrices), axis=0)\n\n    # Solve matching problem via Hungarian algorithm.\n    indices = Munkres().compute(cost.copy())\n    prmU, prmV = zip(*indices)\n\n    # Compute mean factor similarity given the optimal matching.\n    similarity = np.mean(1 - cost[prmU, prmV])\n\n    # If U and V are of different ranks, identify unmatched factors.\n    unmatched_U = list(set(range(U.rank)) - set(prmU))\n    unmatched_V = list(set(range(V.rank)) - set(prmV))\n\n    # If permuting both U and V, order factors from most to least similar.\n    if permute_U and permute_V:\n        idx = np.argsort(cost[prmU, prmV])\n\n    # If permute_U is False, then order the factors such that the ordering\n    # for U is unchanged.\n    elif permute_V:\n        idx = np.argsort(prmU)\n\n    # If permute_V is False, then order the factors such that the ordering\n    # for V is unchanged.\n    elif permute_U:\n        idx = np.argsort(prmV)\n\n    # If permute_U and permute_V are both False, then we are done and can\n    # simply return the similarity.\n    else:\n        return similarity\n\n    # Re-order the factor permutations.\n    prmU = [prmU[i] for i in idx]\n    prmV = [prmV[i] for i in idx]\n\n    # Permute the factors.\n    if permute_U:\n        U.permute(prmU)\n    if permute_V:\n        V.permute(prmV)\n\n    # Flip the signs of factors.\n    flips = np.sign([F[prmU, prmV] for F in sim_matrices])\n    flips[0] *= np.prod(flips, axis=0)  # always flip an even number of factors\n\n    if permute_U:\n        for i, f in enumerate(flips):\n            U.factors[i] *= f\n\n    elif permute_V:\n        for i, f in enumerate(flips):\n            V.factors[i] *= f\n\n    # Return the similarity score\n    return similarity"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_objective(ensemble, partition='train', ax=None, jitter=0.1,\n                   scatter_kw=dict(), line_kw=dict()):\n    \"\"\"Plots objective function as a function of model rank.\n\n    Parameters\n    ----------\n    ensemble : Ensemble object\n        holds optimization results across a range of model ranks\n    partition : string, one of: {'train', 'test'}\n        specifies whether to plot the objective function on the training\n        data or the held-out test set.\n    ax : matplotlib axis (optional)\n        axis to plot on (defaults to current axis object)\n    jitter : float (optional)\n        amount of horizontal jitter added to scatterpoints (default=0.1)\n    scatter_kw : dict (optional)\n        keyword arguments for styling the scatterpoints\n    line_kw : dict (optional)\n        keyword arguments for styling the line\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if partition == 'train':\n        pass\n    elif partition == 'test':\n        raise NotImplementedError('Cross-validation is on the TODO list.')\n    else:\n        raise ValueError(\"partition must be 'train' or 'test'.\")\n\n    # compile statistics for plotting\n    x, obj, min_obj = [], [], []\n    for rank in sorted(ensemble.results):\n        # reconstruction errors for rank-r models\n        o = ensemble.objectives(rank)\n        obj.extend(o)\n        x.extend(np.full(len(o), rank))\n        min_obj.append(min(o))\n\n    # add horizontal jitter\n    ux = np.unique(x)\n    x = np.array(x) + (np.random.rand(len(x))-0.5)*jitter\n\n    # make plot\n    ax.scatter(x, obj, **scatter_kw)\n    ax.plot(ux, min_obj, **line_kw)\n    ax.set_xlabel('model rank')\n    ax.set_ylabel('objective')\n\n    return ax", "response": "Plots objective function as a function of model rank."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_similarity(ensemble, ax=None, jitter=0.1,\n                    scatter_kw=dict(), line_kw=dict()):\n    \"\"\"Plots similarity across optimization runs as a function of model rank.\n\n    Parameters\n    ----------\n    ensemble : Ensemble object\n        holds optimization results across a range of model ranks\n    ax : matplotlib axis (optional)\n        axis to plot on (defaults to current axis object)\n    jitter : float (optional)\n        amount of horizontal jitter added to scatterpoints (default=0.1)\n    scatter_kw : dict (optional)\n        keyword arguments for styling the scatterpoints\n    line_kw : dict (optional)\n        keyword arguments for styling the line\n\n    References\n    ----------\n    Ulrike von Luxburg (2010). Clustering Stability: An Overview.\n    Foundations and Trends in Machine Learning.\n    https://arxiv.org/abs/1007.1075\n\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    # compile statistics for plotting\n    x, sim, mean_sim = [], [], []\n    for rank in sorted(ensemble.results):\n        # reconstruction errors for rank-r models\n        s = ensemble.similarities(rank)[1:]\n        sim.extend(s)\n        x.extend(np.full(len(s), rank))\n        mean_sim.append(np.mean(s))\n\n    # add horizontal jitter\n    ux = np.unique(x)\n    x = np.array(x) + (np.random.rand(len(x))-0.5)*jitter\n\n    # make plot\n    ax.scatter(x, sim, **scatter_kw)\n    ax.plot(ux, mean_sim, **line_kw)\n\n    ax.set_xlabel('model rank')\n    ax.set_ylabel('model similarity')\n    ax.set_ylim([0, 1.1])\n\n    return ax", "response": "Plots similarity across optimization runs as a function of model rank."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_factors(U, plots='line', fig=None, axes=None, scatter_kw=dict(),\n                 line_kw=dict(), bar_kw=dict(), **kwargs):\n    \"\"\"Plots a KTensor.\n\n    Note: Each keyword option is broadcast to all modes of the KTensor. For\n    example, if `U` is a 3rd-order tensor (i.e. `U.ndim == 3`) then\n    `plot_factors(U, plots=['line','bar','scatter'])` plots all factors for the\n    first mode as a line plot, the second as a bar plot, and the third mode as\n    a scatterplot. But, thanks to broadcasting semantics,\n    `plot_factors(U, color='line')` produces line plots for each mode.\n\n    Parameters\n    ----------\n    U : KTensor\n        Kruskal tensor to be plotted.\n\n    plots : str or list\n        One of {'bar','line','scatter'} to specify the type of plot for each\n        factor. The default is 'line'.\n    fig : matplotlib Figure object\n        If provided, add plots to the specified figure. The figure must have a\n        sufficient number of axes objects.\n    axes : 2d numpy array of matplotlib Axes objects\n        If provided, add plots to the specified figure.\n    scatter_kw : dict or sequence of dicts\n        Keyword arguments provided to scatterplots. If a single dict is\n        provided, these options are broadcasted to all modes.\n    line_kw : dict or sequence of dicts\n        Keyword arguments provided to line plots. If a single dict is provided,\n        these options are broadcasted to all modes.\n    bar_kw : dict or sequence of dicts\n        Keyword arguments provided to bar plots. If a single dict is provided,\n        these options are broadcasted to all modes.\n    **kwargs : dict\n        Additional keyword parameters are passed to the `subplots(...)`\n        function to specify options such as `figsize` and `gridspec_kw`. See\n        `matplotlib.pyplot.subplots(...)` documentation for more info.\n    \"\"\"\n\n    # ~~~~~~~~~~~~~\n    # PARSE OPTIONS\n    # ~~~~~~~~~~~~~\n    kwargs.setdefault('figsize', (8, U.rank))\n\n    # parse optional inputs\n    plots = _broadcast_arg(U, plots, str, 'plots')\n    bar_kw = _broadcast_arg(U, bar_kw, dict, 'bar_kw')\n    line_kw = _broadcast_arg(U, line_kw, dict, 'line_kw')\n    scatter_kw = _broadcast_arg(U, scatter_kw, dict, 'scatter_kw')\n\n    # default scatterplot options\n    for sckw in scatter_kw:\n        sckw.setdefault('edgecolor', 'none')\n        sckw.setdefault('s', 10)\n\n    # ~~~~~~~~~~~~~~\n    # SETUP SUBPLOTS\n    # ~~~~~~~~~~~~~~\n    if fig is None and axes is None:\n        fig, axes = plt.subplots(U.rank, U.ndim, **kwargs)\n        # make sure axes is a 2d-array\n        if U.rank == 1:\n            axes = axes[None, :]\n\n    # if axes are passed in, identify figure\n    elif fig is None:\n        fig = axes[0, 0].get_figure()\n\n    # if figure is passed, identify axes\n    else:\n        axes = np.array(fig.get_axes(), dtype=object).reshape(U.rank, U.ndim)\n\n    # main loop, plot each factor\n    plot_obj = np.empty((U.rank, U.ndim), dtype=object)\n    for r in range(U.rank):\n        for i, f in enumerate(U):\n            # start plots at 1 instead of zero\n            x = np.arange(1, f.shape[0]+1)\n\n            # determine type of plot\n            if plots[i] == 'bar':\n                plot_obj[r, i] = axes[r, i].bar(x, f[:, r], **bar_kw[i])\n                axes[r, i].set_xlim(0, f.shape[0]+1)\n            elif plots[i] == 'scatter':\n                plot_obj[r, i] = axes[r, i].scatter(x, f[:, r], **scatter_kw[i])\n                axes[r, i].set_xlim(0, f.shape[0])\n            elif plots[i] == 'line':\n                plot_obj[r, i] = axes[r, i].plot(f[:, r], '-', **line_kw[i])\n                axes[r, i].set_xlim(0, f.shape[0])\n            else:\n                raise ValueError('invalid plot type')\n\n            # format axes\n            axes[r, i].locator_params(nbins=4)\n            axes[r, i].spines['top'].set_visible(False)\n            axes[r, i].spines['right'].set_visible(False)\n            axes[r, i].xaxis.set_tick_params(direction='out')\n            axes[r, i].yaxis.set_tick_params(direction='out')\n            axes[r, i].yaxis.set_ticks_position('left')\n            axes[r, i].xaxis.set_ticks_position('bottom')\n\n            # remove xticks on all but bottom row\n            if r != U.rank-1:\n                plt.setp(axes[r, i].get_xticklabels(), visible=False)\n\n    # link y-axes within columns\n    for i in range(U.ndim):\n        yl = [a.get_ylim() for a in axes[:, i]]\n        y0, y1 = min([y[0] for y in yl]), max([y[1] for y in yl])\n        [a.set_ylim((y0, y1)) for a in axes[:, i]]\n\n    # format y-ticks\n    for r in range(U.rank):\n        for i in range(U.ndim):\n            # only two labels\n            ymin, ymax = np.round(axes[r, i].get_ylim(), 2)\n            axes[r, i].set_ylim((ymin, ymax))\n\n            # remove decimals from labels\n            if ymin.is_integer():\n                ymin = int(ymin)\n            if ymax.is_integer():\n                ymax = int(ymax)\n\n            # update plot\n            axes[r, i].set_yticks([ymin, ymax])\n\n    plt.tight_layout()\n\n    return fig, axes, plot_obj", "response": "Plots all factors for each mode of a KTensor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _broadcast_arg(U, arg, argtype, name):\n\n    # if input is not iterable, broadcast it all dimensions of the tensor\n    if arg is None or isinstance(arg, argtype):\n        return [arg for _ in range(U.ndim)]\n\n    # check if iterable input is valid\n    elif np.iterable(arg):\n        if len(arg) != U.ndim:\n            raise ValueError('Parameter {} was specified as a sequence of '\n                             'incorrect length. The length must match the '\n                             'number of tensor dimensions '\n                             '(U.ndim={})'.format(name, U.ndim))\n        elif not all([isinstance(a, argtype) for a in arg]):\n            raise TypeError('Parameter {} specified as a sequence of '\n                            'incorrect type. '\n                            'Expected {}.'.format(name, argtype))\n        else:\n            return arg\n\n    # input is not iterable and is not the corrent type.\n    else:\n        raise TypeError('Parameter {} specified as a {}.'\n                        ' Expected {}.'.format(name, type(arg), argtype))", "response": "Broadcasts plotting option arg to all factors of the user s KTensor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that inputs to optimization function are appropriate.", "response": "def _check_cpd_inputs(X, rank):\n    \"\"\"Checks that inputs to optimization function are appropriate.\n\n    Parameters\n    ----------\n    X : ndarray\n        Tensor used for fitting CP decomposition.\n    rank : int\n        Rank of low rank decomposition.\n\n    Raises\n    ------\n    ValueError: If inputs are not suited for CP decomposition.\n    \"\"\"\n    if X.ndim < 3:\n        raise ValueError(\"Array with X.ndim > 2 expected.\")\n    if rank <= 0 or not isinstance(rank, int):\n        raise ValueError(\"Rank is invalid.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the initial factor matrices used optimization.", "response": "def _get_initial_ktensor(init, X, rank, random_state, scale_norm=True):\n    \"\"\"\n    Parameters\n    ----------\n    init : str\n        Specifies type of initializations ('randn', 'rand')\n    X : ndarray\n        Tensor that the decomposition is fit to.\n    rank : int\n        Rank of decomposition\n    random_state : RandomState or int\n        Specifies seed for random number generator\n    scale_norm : bool\n        If True, norm is scaled to match X (default: True)\n\n    Returns\n    -------\n    U : KTensor\n        Initial factor matrices used optimization.\n    normX : float\n        Frobenious norm of tensor data.\n    \"\"\"\n    normX = linalg.norm(X) if scale_norm else None\n\n    if init == 'randn':\n        # TODO - match the norm of the initialization to the norm of X.\n        U = randn_ktensor(X.shape, rank, norm=normX, random_state=random_state)\n\n    elif init == 'rand':\n        # TODO - match the norm of the initialization to the norm of X.\n        U = rand_ktensor(X.shape, rank, norm=normX, random_state=random_state)\n\n    elif isinstance(init, KTensor):\n        U = init.copy()\n\n    else:\n        raise ValueError(\"Expected 'init' to either be a KTensor or a string \"\n                         \"specifying how to initialize optimization. Valid \"\n                         \"strings are ('randn', 'rand').\")\n\n    return U, normX"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntrue unless converged or maximum iterations or time exceeded.", "response": "def still_optimizing(self):\n        \"\"\"True unless converged or maximum iterations/time exceeded.\"\"\"\n\n        # Check if we need to give up on optimizing.\n        if (self.iterations > self.max_iter) or (self.time_elapsed() > self.max_time):\n            return False\n\n        # Always optimize for at least 'min_iter' iterations.\n        elif not hasattr(self, 'improvement') or (self.iterations < self.min_iter):\n            return True\n\n        # Check convergence.\n        else:\n            self.converged = self.improvement < self.tol\n            return False if self.converged else True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_random_state(random_state):\n    if random_state is None or isinstance(random_state, int):\n        return sci.random.RandomState(random_state)\n    elif isinstance(random_state, sci.random.RandomState):\n        return random_state\n    else:\n        raise TypeError('Seed should be None, int or np.random.RandomState')", "response": "Checks and processes user input for seeding random numbers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a random N - way tensor with rank R.", "response": "def randn_ktensor(shape, rank, norm=None, random_state=None):\n    \"\"\"\n    Generates a random N-way tensor with rank R, where the entries are\n    drawn from the standard normal distribution.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the tensor\n\n    rank : integer\n        rank of the tensor\n\n    norm : float or None, optional (defaults: None)\n        If not None, the factor matrices are rescaled so that the Frobenius\n        norm of the returned tensor is equal to ``norm``.\n\n    random_state : integer, RandomState instance or None, optional (default ``None``)\n        If integer, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used by np.random.\n\n\n    Returns\n    -------\n    X : (I_1, ..., I_N) array_like\n        N-way tensor with rank R.\n\n    Example\n    -------\n    >>> # Create a rank-2 tensor of dimension 5x5x5:\n    >>> import tensortools as tt\n    >>> X = tt.randn_tensor((5,5,5), rank=2)\n\n    \"\"\"\n\n    # Check input.\n    rns = _check_random_state(random_state)\n\n    # Draw low-rank factor matrices with i.i.d. Gaussian elements.\n    factors = KTensor([rns.standard_normal((i, rank)) for i in shape])\n    return _rescale_tensor(factors, norm)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rand_ktensor(shape, rank, norm=None, random_state=None):\n\n    # Check input.\n    rns = _check_random_state(random_state)\n\n    # Randomize low-rank factor matrices i.i.d. uniform random elements.\n    factors = KTensor([rns.uniform(0.0, 1.0, size=(i, rank)) for i in shape])\n    return _rescale_tensor(factors, norm)", "response": "Generates a random N - way tensor with rank R."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mcp_als(X, rank, mask, random_state=None, init='randn', **options):\n\n    # Check inputs.\n    optim_utils._check_cpd_inputs(X, rank)\n\n    # Initialize problem.\n    U, _ = optim_utils._get_initial_ktensor(init, X, rank, random_state, scale_norm=False)\n    result = FitResult(U, 'MCP_ALS', **options)\n    normX = np.linalg.norm((X * mask))\n\n    # Main optimization loop.\n    while result.still_optimizing:\n\n        # Iterate over each tensor mode.\n        for n in range(X.ndim):\n\n            # i) Normalize factors to prevent singularities.\n            U.rebalance()\n\n            # ii) Unfold data and mask along the nth mode.\n            unf = unfold(X, n)  # i_n x N\n            m = unfold(mask, n)  # i_n x N\n\n            # iii) Form Khatri-Rao product of factors matrices.\n            components = [U[j] for j in range(X.ndim) if j != n]\n            krt = khatri_rao(components).T  # N x r\n\n            # iv) Broadcasted solve of linear systems.\n            # Left hand side of equations, R x R x X.shape[n]\n            # Right hand side of equations, X.shape[n] x R x 1\n            lhs_stack = np.matmul(m[:, None, :] * krt[None, :, :], krt.T[None, :, :])\n            rhs_stack = np.dot(unf * m, krt.T)[:, :, None]\n\n            # vi) Update factor.\n            U[n] = np.linalg.solve(lhs_stack, rhs_stack).reshape(X.shape[n], rank)\n\n        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        # Update the optimization result, checks for convergence.\n        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        # Compute objective function\n        # grams *= U[-1].T.dot(U[-1])\n        # obj = np.sqrt(np.sum(grams) - 2*sci.sum(p*U[-1]) + normX**2) / normX\n        obj = linalg.norm(mask * (U.full() - X)) / normX\n\n        # Update result\n        result.update(obj)\n\n    # Finalize and return the optimization result.\n    return result.finalize()", "response": "Fits CP Decomposition with missing data using Alternating Least Squares."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ncp_bcd(X, rank, random_state=None, init='rand', **options):\n\n    # Check inputs.\n    optim_utils._check_cpd_inputs(X, rank)\n\n    # Store norm of X for computing objective function.\n    N = X.ndim\n\n    # Initialize problem.\n    U, normX = optim_utils._get_initial_ktensor(init, X, rank, random_state)\n    result = FitResult(U, 'NCP_BCD', **options)\n\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    # Block coordinate descent\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Um = U.copy()  # Extrapolations of compoenents\n    extraw = 1  # Used for extrapolation weight update\n    weights_U = np.ones(N)  # Extrapolation weights\n    L = np.ones(N)  # Lipschitz constants\n    obj_bcd = 0.5 * normX**2  # Initial objective value\n\n    # Main optimization loop.\n    while result.still_optimizing:\n        obj_bcd_old = obj_bcd  # Old objective value\n        U_old = U.copy()\n        extraw_old = extraw\n\n        for n in range(N):\n\n            # Select all components, but U_n\n            components = [U[j] for j in range(N) if j != n]\n\n            # i) compute the N-1 gram matrices\n            grams = sci.multiply.reduce([arr.T.dot(arr) for arr in components])\n\n            # Update gradient Lipschnitz constant\n            L0 = L  # Lipschitz constants\n            L[n] = linalg.norm(grams, 2)\n\n            # ii)  Compute Khatri-Rao product\n            kr = khatri_rao(components)\n            p = unfold(X, n).dot( kr )\n\n            # Compute Gradient.\n            grad = Um[n] .dot(grams) - p\n\n            # Enforce nonnegativity (project onto nonnegative orthant).\n            U[n] = sci.maximum(0.0, Um[n] - grad / L[n])\n\n        # Compute objective function and update optimization result.\n        # grams *= U[X.ndim - 1].T.dot(U[X.ndim - 1])\n        # obj = np.sqrt(sci.sum(grams) - 2 * sci.sum(U[X.ndim - 1] * p) + normX**2) / normX\n        obj = linalg.norm(X - U.full()) / normX\n        result.update(obj)\n\n        # Correction and extrapolation.\n        grams *= U[N - 1].T.dot(U[N - 1])\n        obj_bcd = 0.5 * (sci.sum(grams) - 2 * sci.sum(U[N-1] * p) + normX**2 )\n\n        extraw = (1 + sci.sqrt(1 + 4 * extraw_old**2)) / 2.0\n\n        if obj_bcd >= obj_bcd_old:\n            # restore previous A to make the objective nonincreasing\n            Um = sci.copy(U_old)\n\n        else:\n            # apply extrapolation\n            w = (extraw_old - 1.0) / extraw # Extrapolation weight\n            for n in range(N):\n                weights_U[n] = min(w, 1.0 * sci.sqrt( L0[n] / L[n] )) # choose smaller weights for convergence\n                Um[n] = U[n] + weights_U[n] * (U[n] - U_old[n]) # extrapolation\n\n    # Finalize and return the optimization result.\n    return result.finalize()", "response": "Computes nonnegative CP decomposition of a real array with nonnegative entries and nonnegative CP decomposition."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes nonnegtaive CP decomposition of a real array with nonnegative entries and nonnegative entries.", "response": "def ncp_hals(X, rank, random_state=None, init='rand', **options):\n    \"\"\"\n    Fits nonnegtaive CP Decomposition using the Hierarcial Alternating Least\n    Squares (HALS) Method.\n\n    Parameters\n    ----------\n    X : (I_1, ..., I_N) array_like\n        A real array with nonnegative entries and ``X.ndim >= 3``.\n\n    rank : integer\n        The `rank` sets the number of components to be computed.\n\n    random_state : integer, RandomState instance or None, optional (default ``None``)\n        If integer, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used by np.random.\n\n    init : str, or KTensor, optional (default ``'rand'``).\n        Specifies initial guess for KTensor factor matrices.\n        If ``'randn'``, Gaussian random numbers are used to initialize.\n        If ``'rand'``, uniform random numbers are used to initialize.\n        If KTensor instance, a copy is made to initialize the optimization.\n\n    options : dict, specifying fitting options.\n\n        tol : float, optional (default ``tol=1E-5``)\n            Stopping tolerance for reconstruction error.\n\n        max_iter : integer, optional (default ``max_iter = 500``)\n            Maximum number of iterations to perform before exiting.\n\n        min_iter : integer, optional (default ``min_iter = 1``)\n            Minimum number of iterations to perform before exiting.\n\n        max_time : integer, optional (default ``max_time = np.inf``)\n            Maximum computational time before exiting.\n\n        verbose : bool ``{'True', 'False'}``, optional (default ``verbose=True``)\n            Display progress.\n\n\n    Returns\n    -------\n    result : FitResult instance\n        Object which holds the fitted results. It provides the factor matrices\n        in form of a KTensor, ``result.factors``.\n\n\n    Notes\n    -----\n    This implemenation is using the Hierarcial Alternating Least Squares Method.\n\n\n    References\n    ----------\n    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\n    large scale nonnegative matrix and tensor factorizations.\"\n    IEICE transactions on fundamentals of electronics, communications and\n    computer sciences 92.3: 708-721, 2009.\n\n    Examples\n    --------\n\n\n    \"\"\"\n\n    # Check inputs.\n    optim_utils._check_cpd_inputs(X, rank)\n\n    # Initialize problem.\n    U, normX = optim_utils._get_initial_ktensor(init, X, rank, random_state)\n    result = FitResult(U, 'NCP_HALS', **options)\n\n    # Store problem dimensions.\n    normX = linalg.norm(X)\n\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    # Iterate the HALS algorithm until convergence or maxiter is reached\n    # i)   compute the N gram matrices and multiply\n    # ii)  Compute Khatri-Rao product\n    # iii) Update component U_1, U_2, ... U_N\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    while result.still_optimizing:\n\n        violation = 0.0\n\n        for n in range(X.ndim):\n\n            # Select all components, but U_n\n            components = [U[j] for j in range(X.ndim) if j != n]\n\n            # i) compute the N-1 gram matrices\n            grams = sci.multiply.reduce([arr.T.dot(arr) for arr in components])\n\n            # ii)  Compute Khatri-Rao product\n            kr = khatri_rao(components)\n            p = unfold(X, n).dot(kr)\n\n            # iii) Update component U_n\n            violation += _hals_update(U[n], grams, p)\n\n        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        # Update the optimization result, checks for convergence.\n        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        # Compute objective function\n        # grams *= U[X.ndim - 1].T.dot(U[X.ndim - 1])\n        # obj = np.sqrt( (sci.sum(grams) - 2 * sci.sum(U[X.ndim - 1] * p) + normX**2)) / normX\n        result.update(linalg.norm(X - U.full()) / normX)\n\n    # end optimization loop, return result.\n    return result.finalize()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cp_als(X, rank, random_state=None, init='randn', **options):\n\n    # Check inputs.\n    optim_utils._check_cpd_inputs(X, rank)\n\n    # Initialize problem.\n    U, normX = optim_utils._get_initial_ktensor(init, X, rank, random_state)\n    result = FitResult(U, 'CP_ALS', **options)\n\n    # Main optimization loop.\n    while result.still_optimizing:\n\n        # Iterate over each tensor mode.\n        for n in range(X.ndim):\n\n            # i) Normalize factors to prevent singularities.\n            U.rebalance()\n\n            # ii) Compute the N-1 gram matrices.\n            components = [U[j] for j in range(X.ndim) if j != n]\n            grams = sci.multiply.reduce([sci.dot(u.T, u) for u in components])\n\n            # iii)  Compute Khatri-Rao product.\n            kr = khatri_rao(components)\n\n            # iv) Form normal equations and solve via Cholesky\n            c = linalg.cho_factor(grams, overwrite_a=False)\n            p = unfold(X, n).dot(kr)\n            U[n] = linalg.cho_solve(c, p.T, overwrite_b=False).T\n            # U[n] = linalg.solve(grams, unfold(X, n).dot(kr).T).T\n\n        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        # Update the optimization result, checks for convergence.\n        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        # Compute objective function\n        # grams *= U[-1].T.dot(U[-1])\n        # obj = np.sqrt(np.sum(grams) - 2*sci.sum(p*U[-1]) + normX**2) / normX\n        obj = linalg.norm(U.full() - X) / normX\n\n        # Update result\n        result.update(obj)\n\n    # Finalize and return the optimization result.\n    return result.finalize()", "response": "Fits CP Decomposition using Alternating Least Squares ( ALS )."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfit CP tensor decompositions for different choices of CP tensor.", "response": "def fit(self, X, ranks, replicates=1, verbose=True):\n        \"\"\"\n        Fits CP tensor decompositions for different choices of rank.\n\n        Parameters\n        ----------\n        X : array_like\n            Real tensor\n        ranks : int, or iterable\n            iterable specifying number of components in each model\n        replicates: int\n            number of models to fit at each rank\n        verbose : bool\n            If True, prints summaries and optimization progress.\n        \"\"\"\n\n        # Make ranks iterable if necessary.\n        if not isinstance(ranks, collections.Iterable):\n            ranks = (ranks,)\n\n        # Iterate over model ranks, optimize multiple replicates at each rank.\n        for r in ranks:\n\n            # Initialize storage\n            if r not in self.results:\n                self.results[r] = []\n\n            # Display fitting progress.\n            if verbose:\n                itr = trange(replicates,\n                             desc='Fitting rank-{} models'.format(r),\n                             leave=False)\n            else:\n                itr = range(replicates)\n\n            # Fit replicates.\n            for i in itr:\n                model_fit = self._fit_method(X, r, **self._fit_options)\n                self.results[r].append(model_fit)\n\n            # Print summary of results.\n            if verbose:\n                min_obj = min([res.obj for res in self.results[r]])\n                max_obj = max([res.obj for res in self.results[r]])\n                elapsed = sum([res.total_time for res in self.results[r]])\n                print('Rank-{} models:  min obj, {:.2f};  '\n                      'max obj, {:.2f};  time to fit, '\n                      '{:.1f}s'.format(r, min_obj, max_obj, elapsed))\n\n        # Sort results from lowest to largest loss.\n        for r in ranks:\n            idx = np.argsort([result.obj for result in self.results[r]])\n            self.results[r] = [self.results[r][i] for i in idx]\n\n        # Align best model within each rank to best model of next larger rank.\n        # Here r0 is the rank of the lower-dimensional model and r1 is the rank\n        # of the high-dimensional model.\n        for i in reversed(range(1, len(ranks))):\n            r0, r1 = ranks[i-1], ranks[i]\n            U = self.results[r0][0].factors\n            V = self.results[r1][0].factors\n            kruskal_align(U, V, permute_U=True)\n\n        # For each rank, align everything to the best model\n        for r in ranks:\n            # store best factors\n            U = self.results[r][0].factors       # best model factors\n            self.results[r][0].similarity = 1.0  # similarity to itself\n\n            # align lesser fit models to best models\n            for res in self.results[r][1:]:\n                res.similarity = kruskal_align(U, res.factors, permute_V=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef objectives(self, rank):\n        self._check_rank(rank)\n        return [result.obj for result in self.results[rank]]", "response": "Returns objective values of models with specified rank."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of similarity scores for models with specified rank.", "response": "def similarities(self, rank):\n        \"\"\"Returns similarity scores for models with specified rank.\n        \"\"\"\n        self._check_rank(rank)\n        return [result.similarity for result in self.results[rank]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef factors(self, rank):\n        self._check_rank(rank)\n        return [result.factors for result in self.results[rank]]", "response": "Returns a list of KTensor factors for models with specified rank."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncommitting dirty records to the server.", "response": "def commit(self):\n        \"\"\"Commit dirty records to the server. This method is automatically\n        called when the `auto_commit` option is set to `True` (default).\n        It can be useful to set the former option to `False` to get better\n        performance by reducing the number of RPC requests generated.\n\n        With `auto_commit` set to `True` (default behaviour), each time a value\n        is set on a record field a RPC request is sent to the server to update\n        the record:\n\n        .. doctest::\n\n            >>> user = odoo.env.user\n            >>> user.name = \"Joe\"               # write({'name': \"Joe\"})\n            >>> user.email = \"joe@odoo.net\"     # write({'email': \"joe@odoo.net\"})\n\n        With `auto_commit` set to `False`, changes on a record are sent all at\n        once when calling the :func:`commit` method:\n\n        .. doctest::\n\n            >>> odoo.config['auto_commit'] = False\n            >>> user = odoo.env.user\n            >>> user.name = \"Joe\"\n            >>> user.email = \"joe@odoo.net\"\n            >>> user in odoo.env.dirty\n            True\n            >>> odoo.env.commit()   # write({'name': \"Joe\", 'email': \"joe@odoo.net\"})\n            >>> user in odoo.env.dirty\n            False\n\n        Only one RPC request is generated in the last case.\n        \"\"\"\n        # Iterate on a new set, as we remove record during iteration from the\n        # original one\n        for record in set(self.dirty):\n            values = {}\n            for field in record._values_to_write:\n                if record.id in record._values_to_write[field]:\n                    value = record._values_to_write[field].pop(record.id)\n                    values[field] = value\n                    # Store the value in the '_values' dictionary. This\n                    # operation is delegated to each field descriptor as some\n                    # values can not be stored \"as is\" (e.g. magic tuples of\n                    # 2many fields need to be converted)\n                    record.__class__.__dict__[field].store(record, value)\n            record.write(values)\n            self.dirty.remove(record)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the record corresponding to the given xml_id. Raise an exception if no record is found.", "response": "def ref(self, xml_id):\n        \"\"\"Return the record corresponding to the given `xml_id` (also called\n        external ID).\n        Raise an :class:`RPCError <odoorpc.error.RPCError>` if no record\n        is found.\n\n        .. doctest::\n\n            >>> odoo.env.ref('base.lang_en')\n            Recordset('res.lang', [1])\n\n        :return: a :class:`odoorpc.models.Model` instance (recordset)\n        :raise: :class:`odoorpc.error.RPCError`\n        \"\"\"\n        model, id_ = self._odoo.execute(\n            'ir.model.data', 'xmlid_to_res_model_res_id', xml_id, True)\n        return self[model].browse(id_)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_model_class(self, model):\n        cls_name = model.replace('.', '_')\n        # Hack for Python 2 (no need to do this for Python 3)\n        if sys.version_info[0] < 3:\n            if isinstance(cls_name, unicode):\n                cls_name = cls_name.encode('utf-8')\n        # Retrieve server fields info and generate corresponding local fields\n        attrs = {\n            '_env': self,\n            '_odoo': self._odoo,\n            '_name': model,\n            '_columns': {},\n        }\n        fields_get = self._odoo.execute(model, 'fields_get')\n        for field_name, field_data in fields_get.items():\n            if field_name not in FIELDS_RESERVED:\n                Field = fields.generate_field(field_name, field_data)\n                attrs['_columns'][field_name] = Field\n                attrs[field_name] = Field\n        # Case where no field 'name' exists, we generate one (which will be\n        # in readonly mode) in purpose to be filled with the 'name_get' method\n        if 'name' not in attrs['_columns']:\n            field_data = {'type': 'text', 'string': 'Name', 'readonly': True}\n            Field = fields.generate_field('name', field_data)\n            attrs['_columns']['name'] = Field\n            attrs['name'] = Field\n        return type(cls_name, (Model,), attrs)", "response": "Generate the model proxy class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all session configurations from the rc_file file.", "response": "def get_all(rc_file='~/.odoorpcrc'):\n    \"\"\"Return all session configurations from the `rc_file` file.\n\n    >>> import odoorpc\n    >>> from pprint import pprint as pp\n    >>> pp(odoorpc.session.get_all())     # doctest: +SKIP\n    {'foo': {'database': 'db_name',\n             'host': 'localhost',\n             'passwd': 'password',\n             'port': 8069,\n             'protocol': 'jsonrpc',\n             'timeout': 120,\n             'type': 'ODOO',\n             'user': 'admin'},\n     ...}\n\n    .. doctest::\n        :hide:\n\n        >>> import odoorpc\n        >>> session = '%s_session' % DB\n        >>> odoo.save(session)\n        >>> data = odoorpc.session.get_all()\n        >>> data[session]['host'] == HOST\n        True\n        >>> data[session]['protocol'] == PROTOCOL\n        True\n        >>> data[session]['port'] == int(PORT)\n        True\n        >>> data[session]['database'] == DB\n        True\n        >>> data[session]['user'] == USER\n        True\n        >>> data[session]['passwd'] == PWD\n        True\n        >>> data[session]['type'] == 'ODOO'\n        True\n    \"\"\"\n    conf = ConfigParser()\n    conf.read([os.path.expanduser(rc_file)])\n    sessions = {}\n    for name in conf.sections():\n        sessions[name] = {\n            'type': conf.get(name, 'type'),\n            'host': conf.get(name, 'host'),\n            'protocol': conf.get(name, 'protocol'),\n            'port': conf.getint(name, 'port'),\n            'timeout': conf.getfloat(name, 'timeout'),\n            'user': conf.get(name, 'user'),\n            'passwd': conf.get(name, 'passwd'),\n            'database': conf.get(name, 'database'),\n        }\n    return sessions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the session configuration identified by name from the rc_file.", "response": "def get(name, rc_file='~/.odoorpcrc'):\n    \"\"\"Return the session configuration identified by `name`\n    from the `rc_file` file.\n\n    >>> import odoorpc\n    >>> from pprint import pprint as pp\n    >>> pp(odoorpc.session.get('foo'))    # doctest: +SKIP\n    {'database': 'db_name',\n     'host': 'localhost',\n     'passwd': 'password',\n     'port': 8069,\n     'protocol': 'jsonrpc',\n     'timeout': 120,\n     'type': 'ODOO',\n     'user': 'admin'}\n\n    .. doctest::\n        :hide:\n\n        >>> import odoorpc\n        >>> session = '%s_session' % DB\n        >>> odoo.save(session)\n        >>> data = odoorpc.session.get(session)\n        >>> data['host'] == HOST\n        True\n        >>> data['protocol'] == PROTOCOL\n        True\n        >>> data['port'] == int(PORT)\n        True\n        >>> data['database'] == DB\n        True\n        >>> data['user'] == USER\n        True\n        >>> data['passwd'] == PWD\n        True\n        >>> data['type'] == 'ODOO'\n        True\n\n    :raise: `ValueError` (wrong session name)\n    \"\"\"\n    conf = ConfigParser()\n    conf.read([os.path.expanduser(rc_file)])\n    if not conf.has_section(name):\n        raise ValueError(\n            \"'%s' session does not exist in %s\" % (name, rc_file))\n    return {\n        'type': conf.get(name, 'type'),\n        'host': conf.get(name, 'host'),\n        'protocol': conf.get(name, 'protocol'),\n        'port': conf.getint(name, 'port'),\n        'timeout': conf.getfloat(name, 'timeout'),\n        'user': conf.get(name, 'user'),\n        'passwd': conf.get(name, 'passwd'),\n        'database': conf.get(name, 'database'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(name, data, rc_file='~/.odoorpcrc'):\n    conf = ConfigParser()\n    conf.read([os.path.expanduser(rc_file)])\n    if not conf.has_section(name):\n        conf.add_section(name)\n    for key in data:\n        value = data[key]\n        conf.set(name, key, str(value))\n    with open(os.path.expanduser(rc_file), 'w') as file_:\n        os.chmod(os.path.expanduser(rc_file), stat.S_IREAD | stat.S_IWRITE)\n        conf.write(file_)", "response": "Save the data dictionary under the name in the rc_file file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the session configuration identified by name from the rc_file.", "response": "def remove(name, rc_file='~/.odoorpcrc'):\n    \"\"\"Remove the session configuration identified by `name`\n    from the `rc_file` file.\n\n    >>> import odoorpc\n    >>> odoorpc.session.remove('foo')     # doctest: +SKIP\n\n    .. doctest::\n        :hide:\n\n        >>> import odoorpc\n        >>> session = '%s_session' % DB\n        >>> odoorpc.session.remove(session)\n\n    :raise: `ValueError` (wrong session name)\n    \"\"\"\n    conf = ConfigParser()\n    conf.read([os.path.expanduser(rc_file)])\n    if not conf.has_section(name):\n        raise ValueError(\n            \"'%s' session does not exist in %s\" % (name, rc_file))\n    conf.remove_section(name)\n    with open(os.path.expanduser(rc_file), 'wb') as file_:\n        conf.write(file_)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_encodings(hint_encoding='utf-8'):\n    fallbacks = {\n        'latin1': 'latin9',\n        'iso-8859-1': 'iso8859-15',\n        'cp1252': '1252',\n    }\n    if hint_encoding:\n        yield hint_encoding\n        if hint_encoding.lower() in fallbacks:\n            yield fallbacks[hint_encoding.lower()]\n\n    # some defaults (also taking care of pure ASCII)\n    for charset in ['utf8', 'latin1', 'ascii']:\n        if not hint_encoding or (charset.lower() != hint_encoding.lower()):\n            yield charset\n\n    from locale import getpreferredencoding\n    prefenc = getpreferredencoding()\n    if prefenc and prefenc.lower() != 'utf-8':\n        yield prefenc\n        prefenc = fallbacks.get(prefenc.lower())\n        if prefenc:\n            yield prefenc", "response": "Yields the encodings of the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new data dictionary with hidden params for log purpose.", "response": "def get_json_log_data(data):\n    \"\"\"Returns a new `data` dictionary with hidden params\n    for log purpose.\n    \"\"\"\n    log_data = data\n    for param in LOG_HIDDEN_JSON_PARAMS:\n        if param in data['params']:\n            if log_data is data:\n                log_data = copy.deepcopy(data)\n            log_data['params'][param] = \"**********\"\n    return log_data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef json(self, url, params):\n        data = self._connector.proxy_json(url, params)\n        if data.get('error'):\n            raise error.RPCError(\n                data['error']['data']['message'],\n                data['error'])\n        return data", "response": "This method is used to execute a JSON query on a single object. It is basically a simple method that returns a dictionary of the result of the call to the read method of the user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a HTTP request to get the company logo on Odoo 12. 0.", "response": "def http(self, url, data=None, headers=None):\n        \"\"\"Low level method to execute raw HTTP queries.\n\n        .. note::\n\n            For low level JSON-RPC queries, see the more convenient\n            :func:`odoorpc.ODOO.json` method instead.\n\n        You have to know the names of each POST parameter required by the\n        URL, and set them in the `data` string/buffer.\n        The `data` argument must be built by yourself, following the expected\n        URL parameters (with :func:`urllib.urlencode` function for simple\n        parameters, or multipart/form-data structure to handle file upload).\n\n        E.g., the HTTP raw query to get the company logo on `Odoo 12.0`:\n\n        .. doctest::\n\n            >>> response = odoo.http('web/binary/company_logo')\n            >>> binary_data = response.read()\n\n        *Python 2:*\n\n        :return: `urllib.addinfourl`\n        :raise: `urllib2.HTTPError`\n        :raise: `urllib2.URLError` (connection error)\n\n        *Python 3:*\n\n        :return: `http.client.HTTPResponse`\n        :raise: `urllib.error.HTTPError`\n        :raise: `urllib.error.URLError` (connection error)\n        \"\"\"\n        return self._connector.proxy_http(url, data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if a user is logged.", "response": "def _check_logged_user(self):\n        \"\"\"Check if a user is logged. Otherwise, an error is raised.\"\"\"\n        if not self._env or not self._password or not self._login:\n            raise error.InternalError(\"Login required\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlogging in as the given user with the given password on the given database.", "response": "def login(self, db, login='admin', password='admin'):\n        \"\"\"Log in as the given `user` with the password `passwd` on the\n        database `db`.\n\n        .. doctest::\n            :options: +SKIP\n\n            >>> odoo.login('db_name', 'admin', 'admin')\n            >>> odoo.env.user.name\n            'Administrator'\n\n        *Python 2:*\n\n        :raise: :class:`odoorpc.error.RPCError`\n        :raise: `urllib2.URLError` (connection error)\n\n        *Python 3:*\n\n        :raise: :class:`odoorpc.error.RPCError`\n        :raise: `urllib.error.URLError` (connection error)\n        \"\"\"\n        # Get the user's ID and generate the corresponding user record\n        data = self.json(\n            '/web/session/authenticate',\n            {'db': db, 'login': login, 'password': password})\n        uid = data['result']['uid']\n        if uid:\n            context = data['result']['user_context']\n            self._env = Environment(self, db, uid, context=context)\n            self._login = login\n            self._password = password\n        else:\n            raise error.RPCError(\"Wrong login ID or password\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef logout(self):\n        if not self._env:\n            return False\n        self.json('/web/session/destroy', {})\n        self._env = None\n        self._login = None\n        self._password = None\n        return True", "response": "Log out the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self, model, method, *args):\n        self._check_logged_user()\n        # Execute the query\n        args_to_send = [self.env.db, self.env.uid, self._password,\n                        model, method]\n        args_to_send.extend(args)\n        data = self.json(\n            '/jsonrpc',\n            {'service': 'object',\n             'method': 'execute',\n             'args': args_to_send})\n        return data.get('result')", "response": "Execute the method of model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute the workflow signal on the record with the ID record_id of model.", "response": "def exec_workflow(self, model, record_id, signal):\n        \"\"\"Execute the workflow `signal` on\n        the instance having the ID `record_id` of `model`.\n\n        *Python 2:*\n\n        :raise: :class:`odoorpc.error.RPCError`\n        :raise: :class:`odoorpc.error.InternalError` (if not logged)\n        :raise: `urllib2.URLError` (connection error)\n\n        *Python 3:*\n\n        :raise: :class:`odoorpc.error.RPCError`\n        :raise: :class:`odoorpc.error.InternalError` (if not logged)\n        :raise: `urllib.error.URLError` (connection error)\n        \"\"\"\n        if tools.v(self.version)[0] >= 11:\n            raise DeprecationWarning(\n                u\"Workflows have been removed in Odoo >= 11.0\")\n        self._check_logged_user()\n        # Execute the workflow query\n        args_to_send = [self.env.db, self.env.uid, self._password,\n                        model, signal, record_id]\n        data = self.json(\n            '/jsonrpc',\n            {'service': 'object',\n             'method': 'exec_workflow',\n             'args': args_to_send})\n        return data.get('result')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the current session to rc_file.", "response": "def save(self, name, rc_file='~/.odoorpcrc'):\n        \"\"\"Save the current :class:`ODOO <odoorpc.ODOO>` instance (a `session`)\n        inside `rc_file` (``~/.odoorpcrc`` by default). This session will be\n        identified by `name`::\n\n            >>> import odoorpc\n            >>> odoo = odoorpc.ODOO('localhost', port=8069)\n            >>> odoo.login('db_name', 'admin', 'admin')\n            >>> odoo.save('foo')\n\n        Use the :func:`list <odoorpc.ODOO.list>` class method to list all\n        stored sessions, and the :func:`load <odoorpc.ODOO.load>` class method\n        to retrieve an already-connected :class:`ODOO <odoorpc.ODOO>` instance.\n\n        *Python 2:*\n\n        :raise: :class:`odoorpc.error.InternalError` (if not logged)\n        :raise: `IOError`\n\n        *Python 3:*\n\n        :raise: :class:`odoorpc.error.InternalError` (if not logged)\n        :raise: `PermissionError`\n        :raise: `FileNotFoundError`\n        \"\"\"\n        self._check_logged_user()\n        data = {\n            'type': self.__class__.__name__,\n            'host': self.host,\n            'protocol': self.protocol,\n            'port': self.port,\n            'timeout': self.config['timeout'],\n            'user': self._login,\n            'passwd': self._password,\n            'database': self.env.db,\n        }\n        session.save(name, data, rc_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(cls, name, rc_file='~/.odoorpcrc'):\n        data = session.get(name, rc_file)\n        if data.get('type') != cls.__name__:\n            raise error.InternalError(\n                \"'{0}' session is not of type '{1}'\".format(\n                    name, cls.__name__))\n        odoo = cls(\n            host=data['host'],\n            protocol=data['protocol'],\n            port=data['port'],\n            timeout=data['timeout'],\n        )\n        odoo.login(\n            db=data['database'], login=data['user'], password=data['passwd'])\n        return odoo", "response": "Load a connected odoorpc. odoo. odoorpc. odoo. o. o"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list(cls, rc_file='~/.odoorpcrc'):\n        sessions = session.get_all(rc_file)\n        return [name for name in sessions\n                if sessions[name].get('type') == cls.__name__]", "response": "Return a list of all stored sessions available in the odoorpc. odoorpc."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove(cls, name, rc_file='~/.odoorpcrc'):\n        data = session.get(name, rc_file)\n        if data.get('type') != cls.__name__:\n            raise error.InternalError(\n                \"'{0}' session is not of type '{1}'\".format(\n                    name, cls.__name__))\n        return session.remove(name, rc_file)", "response": "Remove the session identified by name from the rc_file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new database named db.", "response": "def create(self, password, db, demo=False, lang='en_US', admin_password='admin'):\n        \"\"\"Request the server to create a new database named `db`\n        which will have `admin_password` as administrator password and\n        localized with the `lang` parameter.\n        You have to set the flag `demo` to `True` in order to insert\n        demonstration data.\n\n        >>> odoo.db.create('super_admin_passwd', 'prod', False, 'fr_FR', 'my_admin_passwd') # doctest: +SKIP\n\n        If you get a timeout error, increase this one before performing the\n        request:\n\n        >>> timeout_backup = odoo.config['timeout']\n        >>> odoo.config['timeout'] = 600    # Timeout set to 10 minutes\n        >>> odoo.db.create('super_admin_passwd', 'prod', False, 'fr_FR', 'my_admin_passwd') # doctest: +SKIP\n        >>> odoo.config['timeout'] = timeout_backup\n\n        The super administrator password is required to perform this method.\n\n        *Python 2:*\n\n        :raise: :class:`odoorpc.error.RPCError` (access denied)\n        :raise: `urllib2.URLError` (connection error)\n\n        *Python 3:*\n\n        :raise: :class:`odoorpc.error.RPCError` (access denied)\n        :raise: `urllib.error.URLError` (connection error)\n        \"\"\"\n        self._odoo.json(\n            '/jsonrpc',\n            {'service': 'db',\n             'method': 'create_database',\n             'args': [password, db, demo, lang, admin_password]})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef drop(self, password, db):\n        if self._odoo._env and self._odoo._env.db == db:\n            # Remove the existing session to avoid HTTP session error\n            self._odoo.logout()\n        data = self._odoo.json(\n            '/jsonrpc',\n            {'service': 'db',\n             'method': 'drop',\n             'args': [password, db]})\n        return data['result']", "response": "Drop the db database. Returns True if the database was removed False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef duplicate(self, password, db, new_db):\n        self._odoo.json(\n            '/jsonrpc',\n            {'service': 'db',\n             'method': 'duplicate_database',\n             'args': [password, db, new_db]})", "response": "Duplicate database as new_db."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrestores the dump file into the new database.", "response": "def restore(self, password, db, dump, copy=False):\n        \"\"\"Restore the `dump` database into the new `db` database.\n        The `dump` file object can be obtained with the\n        :func:`dump <DB.dump>` method.\n        If `copy` is set to `True`, the restored database will have a new UUID.\n\n        >>> odoo.db.restore('super_admin_passwd', 'test', dump_file) # doctest: +SKIP\n\n        If you get a timeout error, increase this one before performing the\n        request:\n\n        >>> timeout_backup = odoo.config['timeout']\n        >>> odoo.config['timeout'] = 7200   # Timeout set to 2 hours\n        >>> odoo.db.restore('super_admin_passwd', 'test', dump_file) # doctest: +SKIP\n        >>> odoo.config['timeout'] = timeout_backup\n\n        The super administrator password is required to perform this method.\n\n        *Python 2:*\n\n        :raise: :class:`odoorpc.error.RPCError`\n                (access denied / database already exists)\n        :raise: :class:`odoorpc.error.InternalError` (dump file closed)\n        :raise: `urllib2.URLError` (connection error)\n\n        *Python 3:*\n\n        :raise: :class:`odoorpc.error.RPCError`\n                (access denied / database already exists)\n        :raise: :class:`odoorpc.error.InternalError` (dump file closed)\n        :raise: `urllib.error.URLError` (connection error)\n        \"\"\"\n        if dump.closed:\n            raise error.InternalError(\"Dump file closed\")\n        b64_data = base64.standard_b64encode(dump.read()).decode()\n        self._odoo.json(\n            '/jsonrpc',\n            {'service': 'db',\n             'method': 'restore',\n             'args': [password, db, b64_data, copy]})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_proxies(self):\n        proxy_json = jsonrpclib.ProxyJSON(\n            self.host, self.port, self._timeout,\n            ssl=self.ssl, deserialize=self.deserialize, opener=self._opener)\n        proxy_http = jsonrpclib.ProxyHTTP(\n            self.host, self.port, self._timeout,\n            ssl=self.ssl, opener=self._opener)\n        # Detect the server version\n        if self.version is None:\n            result = proxy_json('/web/webclient/version_info')['result']\n            if 'server_version' in result:\n                self.version = result['server_version']\n        return proxy_json, proxy_http", "response": "Returns the : class : ProxyJSON and : class : ProxyHTTP instances for the server version used."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_int(value):\n    if isinstance(value, bool):\n        return False\n    try:\n        int(value)\n        return True\n    except (ValueError, TypeError):\n        return False", "response": "Return True if value is an integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef odoo_tuple_in(iterable):\n    if not iterable:\n        return False\n    def is_odoo_tuple(elt):\n        \"\"\"Return `True` if `elt` is a Odoo special tuple.\"\"\"\n        try:\n            return elt[:1][0] in [1, 2, 3, 4, 5] \\\n                    or elt[:2] in [(6, 0), [6, 0], (0, 0), [0, 0]]\n        except (TypeError, IndexError):\n            return False\n    return any(is_odoo_tuple(elt) for elt in iterable)", "response": "Return True if iterable contains an expected tuple like\n    6 0 1 2 1 and 42."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tuples2ids(tuples, ids):\n    for value in tuples:\n        if value[0] == 6 and value[2]:\n            ids = value[2]\n        elif value[0] == 5:\n            ids[:] = []\n        elif value[0] == 4 and value[1] and value[1] not in ids:\n            ids.append(value[1])\n        elif value[0] == 3 and value[1] and value[1] in ids:\n            ids.remove(value[1])\n    return ids", "response": "Update ids according to tuples e. g. 3 0 X X"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreplace records contained in iterable with their corresponding IDs.", "response": "def records2ids(iterable):\n    \"\"\"Replace records contained in `iterable` with their corresponding IDs:\n\n        >>> groups = list(odoo.env.user.groups_id)\n        >>> records2ids(groups)\n        [1, 2, 3, 14, 17, 18, 19, 7, 8, 9, 5, 20, 21, 22, 23]\n    \"\"\"\n    def record2id(elt):\n        \"\"\"If `elt` is a record, return its ID.\"\"\"\n        if isinstance(elt, Model):\n            return elt.id\n        return elt\n    return [record2id(elt) for elt in iterable]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a well - typed field according to the data dictionary supplied.", "response": "def generate_field(name, data):\n    \"\"\"Generate a well-typed field according to the data dictionary supplied\n    (obtained via the `fields_get' method of any models).\n    \"\"\"\n    assert 'type' in data\n    field = TYPES_TO_FIELDS.get(data['type'], Unknown)(name, data)\n    return field"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks the validity of a value for the field.", "response": "def check_value(self, value):\n        \"\"\"Check the validity of a value for the field.\"\"\"\n        #if self.readonly:\n        #    raise error.Error(\n        #        \"'{field_name}' field is readonly\".format(\n        #            field_name=self.name))\n        if value and self.size:\n            if not is_string(value):\n                raise ValueError(\"Value supplied has to be a string\")\n            if len(value) > self.size:\n                raise ValueError(\n                    \"Lenght of the '{0}' is limited to {1}\".format(\n                        self.name, self.size))\n        if not value and self.required:\n            raise ValueError(\"'{0}' field is required\".format(self.name))\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef store(self, record, value):\n        record._values[self.name][record.id] = value", "response": "Store the value in the record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef store(self, record, value):\n        if record._values[self.name].get(record.id):\n            tuples2ids(value, record._values[self.name][record.id])\n        else:\n            record._values[self.name][record.id] = tuples2ids(value, [])", "response": "Store the value in the record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_relation(self, relation):\n        selection = [val[0] for val in self.selection]\n        if relation not in selection:\n            raise ValueError(\n                (\"The value '{value}' supplied doesn't match with the possible\"\n                 \" values '{selection}' for the '{field_name}' field\").format(\n                     value=relation,\n                     selection=selection,\n                     field_name=self.name,\n                 ))\n        return relation", "response": "Raise a ValueError if the relation is not allowed among the possible values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads a report from the server and return it as a remote file.", "response": "def download(self, name, ids, datas=None, context=None):\n        \"\"\"Download a report from the server and return it as a remote file.\n        For instance, to download the \"Quotation / Order\" report of sale orders\n        identified by the IDs ``[2, 3]``:\n\n        .. doctest::\n            :options: +SKIP\n\n            >>> report = odoo.report.download('sale.report_saleorder', [2, 3])\n\n        .. doctest::\n            :hide:\n\n            >>> report = odoo.report.download('sale.report_saleorder', [2])\n\n        Write it on the file system:\n\n        .. doctest::\n            :options: +SKIP\n\n            >>> with open('sale_orders.pdf', 'wb') as report_file:\n            ...     report_file.write(report.read())\n            ...\n\n        .. doctest::\n            :hide:\n\n            >>> with open('sale_orders.pdf', 'wb') as report_file:\n            ...     fileno = report_file.write(report.read())   # Python 3\n            ...\n\n        *Python 2:*\n\n        :return: `io.BytesIO`\n        :raise: :class:`odoorpc.error.RPCError` (wrong parameters)\n        :raise: `ValueError`  (received invalid data)\n        :raise: `urllib2.URLError`  (connection error)\n\n        *Python 3:*\n\n        :return: `io.BytesIO`\n        :raise: :class:`odoorpc.error.RPCError` (wrong parameters)\n        :raise: `ValueError`  (received invalid data)\n        :raise: `urllib.error.URLError` (connection error)\n        \"\"\"\n        if context is None:\n            context = self._odoo.env.context\n\n        def check_report(name):\n            report_model = 'ir.actions.report'\n            if v(self._odoo.version)[0] < 11:\n                report_model = 'ir.actions.report.xml'\n            IrReport = self._odoo.env[report_model]\n            report_ids = IrReport.search([('report_name', '=', name)])\n            report_id = report_ids and report_ids[0] or False\n            if not report_id:\n                raise ValueError(\"The report '%s' does not exist.\" % name)\n            return report_id\n\n        report_id = check_report(name)\n\n        # Odoo >= 11.0\n        if v(self._odoo.version)[0] >= 11:\n            IrReport = self._odoo.env['ir.actions.report']\n            report = IrReport.browse(report_id)\n            response = report.with_context(context).render(ids, data=datas)\n            content = response[0]\n            # On the server the result is a bytes string,\n            # but the RPC layer of Odoo returns it as a unicode string,\n            # so we encode it again as bytes\n            result = content.encode('latin1')\n            return io.BytesIO(result)\n        # Odoo < 11.0\n        else:\n            args_to_send = [self._odoo.env.db,\n                            self._odoo.env.uid, self._odoo._password,\n                            name, ids, datas, context]\n            data = self._odoo.json(\n                '/jsonrpc',\n                {'service': 'report',\n                 'method': 'render_report',\n                 'args': args_to_send})\n            if 'result' not in data and not data['result'].get('result'):\n                raise ValueError(\"Received invalid data.\")\n            # Encode to bytes forced to be compatible with Python 3.2\n            # (its 'base64.standard_b64decode()' function only accepts bytes)\n            result = encode2bytes(data['result']['result'])\n            content = base64.standard_b64decode(result)\n            return io.BytesIO(content)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist available reports from the server by returning a dictionary with the model and reports classified by data model.", "response": "def list(self):\n        \"\"\"List available reports from the server by returning a dictionary\n        with reports classified by data model:\n\n        .. doctest::\n            :options: +SKIP\n\n            >>> odoo.report.list()['account.invoice']\n            [{'name': u'Duplicates',\n              'report_name': u'account.account_invoice_report_duplicate_main',\n              'report_type': u'qweb-pdf'},\n             {'name': 'Invoices',\n              'report_type': 'qweb-pdf',\n              'report_name': 'account.report_invoice'}]\n\n        .. doctest::\n            :hide:\n\n            >>> from pprint import pprint as pp\n            >>> any(data['report_name'] == 'account.report_invoice'\n            ...     for data in odoo.report.list()['account.invoice'])\n            True\n\n        *Python 2:*\n\n        :return: `list` of dictionaries\n        :raise: `urllib2.URLError` (connection error)\n\n        *Python 3:*\n\n        :return: `list` of dictionaries\n        :raise: `urllib.error.URLError` (connection error)\n        \"\"\"\n        report_model = 'ir.actions.report'\n        if v(self._odoo.version)[0] < 11:\n            report_model = 'ir.actions.report.xml'\n        IrReport = self._odoo.env[report_model]\n        report_ids = IrReport.search([])\n        reports = IrReport.read(\n            report_ids, ['name', 'model', 'report_name', 'report_type'])\n        result = {}\n        for report in reports:\n            model = report.pop('model')\n            report.pop('id')\n            if model not in result:\n                result[model] = []\n            result[model].append(report)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _browse(cls, env, ids, from_record=None, iterated=None):\n        records = cls()\n        records._env_local = env\n        records._ids = _normalize_ids(ids)\n        if iterated:\n            records._values = iterated._values\n            records._values_to_write = iterated._values_to_write\n        else:\n            records._from_record = from_record\n            records._values = {}\n            records._values_to_write = {}\n            for field in cls._columns:\n                records._values[field] = {}\n                records._values_to_write[field] = {}\n            records._init_values()\n        return records", "response": "Create an instance of the recordset corresponding to ids and a\n        attached to env."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef with_context(cls, *args, **kwargs):\n        context = dict(args[0] if args else cls.env.context, **kwargs)\n        return cls.with_env(cls.env(context=context))", "response": "Return a new instance of the current model with the given context."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new model instance equivalent to the current model attached to env.", "response": "def with_env(cls, env):\n        \"\"\"Return a model (or recordset) equivalent to the current model\n        (or recordset) attached to `env`.\n        \"\"\"\n        new_cls = type(cls.__name__, cls.__bases__, dict(cls.__dict__))\n        new_cls._env = env\n        return new_cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _with_env(self, env):\n        res = self._browse(env, self._ids)\n        return res", "response": "As the with_env class method but for recordset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the values from the server.", "response": "def _init_values(self, context=None):\n        \"\"\"Retrieve field values from the server.\n        May be used to restore the original values in the purpose to cancel\n        all changes made.\n        \"\"\"\n        if context is None:\n            context = self.env.context\n        # Get basic fields (no relational ones)\n        basic_fields = []\n        for field_name in self._columns:\n            field = self._columns[field_name]\n            if not getattr(field, 'relation', False):\n                basic_fields.append(field_name)\n        # Fetch values from the server\n        if self.ids:\n            rows = self.__class__.read(\n                self.ids, basic_fields, context=context, load='_classic_write')\n            ids_fetched = set()\n            for row in rows:\n                ids_fetched.add(row['id'])\n                for field_name in row:\n                    if field_name == 'id':\n                        continue\n                    self._values[field_name][row['id']] = row[field_name]\n            ids_in_error = set(self.ids) - ids_fetched\n            if ids_in_error:\n                raise ValueError(\n                    \"There is no '{model}' record with IDs {ids}.\".format(\n                        model=self._name, ids=list(ids_in_error)))\n        # No ID: fields filled with default values\n        else:\n            default_get = self.__class__.default_get(\n                list(self._columns), context=context)\n            for field_name in self._columns:\n                self._values[field_name][None] = default_get.get(\n                    field_name, False)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a number of wei to any other ether unit.", "response": "def from_wei(number: int, unit: str) -> Union[int, decimal.Decimal]:\n    \"\"\"\n    Takes a number of wei and converts it to any other ether unit.\n    \"\"\"\n    if unit.lower() not in units:\n        raise ValueError(\n            \"Unknown unit.  Must be one of {0}\".format(\"/\".join(units.keys()))\n        )\n\n    if number == 0:\n        return 0\n\n    if number < MIN_WEI or number > MAX_WEI:\n        raise ValueError(\"value must be between 1 and 2**256 - 1\")\n\n    unit_value = units[unit.lower()]\n\n    with localcontext() as ctx:\n        ctx.prec = 999\n        d_number = decimal.Decimal(value=number, context=ctx)\n        result_value = d_number / unit_value\n\n    return result_value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_wei(number: int, unit: str) -> int:\n    if unit.lower() not in units:\n        raise ValueError(\n            \"Unknown unit.  Must be one of {0}\".format(\"/\".join(units.keys()))\n        )\n\n    if is_integer(number) or is_string(number):\n        d_number = decimal.Decimal(value=number)\n    elif isinstance(number, float):\n        d_number = decimal.Decimal(value=str(number))\n    elif isinstance(number, decimal.Decimal):\n        d_number = number\n    else:\n        raise TypeError(\"Unsupported type.  Must be one of integer, float, or string\")\n\n    s_number = str(number)\n    unit_value = units[unit.lower()]\n\n    if d_number == 0:\n        return 0\n\n    if d_number < 1 and \".\" in s_number:\n        with localcontext() as ctx:\n            multiplier = len(s_number) - s_number.index(\".\") - 1\n            ctx.prec = multiplier\n            d_number = decimal.Decimal(value=number, context=ctx) * 10 ** multiplier\n        unit_value /= 10 ** multiplier\n\n    with localcontext() as ctx:\n        ctx.prec = 999\n        result_value = decimal.Decimal(value=d_number, context=ctx) * unit_value\n\n    if result_value < MIN_WEI or result_value > MAX_WEI:\n        raise ValueError(\"Resulting wei value must be between 1 and 2**256 - 1\")\n\n    return int(result_value)", "response": "Takes a number of a unit and converts it to wei."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_int(\n    primitive: Primitives = None, hexstr: HexStr = None, text: str = None\n) -> int:\n    \"\"\"\n    Converts value to its integer representation.\n    Values are converted this way:\n\n     * primitive:\n\n       * bytes, bytearrays: big-endian integer\n       * bool: True => 1, False => 0\n     * hexstr: interpret hex as integer\n     * text: interpret as string of digits, like '12' => 12\n    \"\"\"\n    if hexstr is not None:\n        return int(hexstr, 16)\n    elif text is not None:\n        return int(text)\n    elif isinstance(primitive, (bytes, bytearray)):\n        return big_endian_to_int(primitive)\n    elif isinstance(primitive, str):\n        raise TypeError(\"Pass in strings with keyword hexstr or text\")\n    elif isinstance(primitive, (int, bool)):\n        return int(primitive)\n    else:\n        raise TypeError(\n            \"Invalid type.  Expected one of int/bool/str/bytes/bytearray.  Got \"\n            \"{0}\".format(type(primitive))\n        )", "response": "Converts value to its integer representation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a string to a text object if it is a unicode string.", "response": "def text_if_str(\n    to_type: Callable[..., T], text_or_primitive: Union[bytes, int, str]\n) -> T:\n    \"\"\"\n    Convert to a type, assuming that strings can be only unicode text (not a hexstr)\n\n    :param to_type function: takes the arguments (primitive, hexstr=hexstr, text=text),\n        eg~ to_bytes, to_text, to_hex, to_int, etc\n    :param text_or_primitive bytes, str, int: value to convert\n    \"\"\"\n    if isinstance(text_or_primitive, str):\n        return to_type(text=text_or_primitive)\n    else:\n        return to_type(text_or_primitive)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a string to a type assuming that it is a hex string.", "response": "def hexstr_if_str(\n    to_type: Callable[..., T], hexstr_or_primitive: Union[bytes, int, str]\n) -> T:\n    \"\"\"\n    Convert to a type, assuming that strings can be only hexstr (not unicode text)\n\n    :param to_type function: takes the arguments (primitive, hexstr=hexstr, text=text),\n        eg~ to_bytes, to_text, to_hex, to_int, etc\n    :param hexstr_or_primitive bytes, str, int: value to convert\n    \"\"\"\n    if isinstance(hexstr_or_primitive, str):\n        if remove_0x_prefix(hexstr_or_primitive) and not is_hex(hexstr_or_primitive):\n            raise ValueError(\n                \"when sending a str, it must be a hex string. Got: {0!r}\".format(\n                    hexstr_or_primitive\n                )\n            )\n        return to_type(hexstr=hexstr_or_primitive)\n    else:\n        return to_type(hexstr_or_primitive)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating arguments for conversion functions.", "response": "def validate_conversion_arguments(to_wrap):\n    \"\"\"\n    Validates arguments for conversion functions.\n    - Only a single argument is present\n    - Kwarg must be 'primitive' 'hexstr' or 'text'\n    - If it is 'hexstr' or 'text' that it is a text type\n    \"\"\"\n\n    @functools.wraps(to_wrap)\n    def wrapper(*args, **kwargs):\n        _assert_one_val(*args, **kwargs)\n        if kwargs:\n            _validate_supported_kwarg(kwargs)\n\n        if len(args) == 0 and \"primitive\" not in kwargs:\n            _assert_hexstr_or_text_kwarg_is_text_type(**kwargs)\n        return to_wrap(*args, **kwargs)\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef return_arg_type(at_position):\n\n    def decorator(to_wrap):\n        @functools.wraps(to_wrap)\n        def wrapper(*args, **kwargs):\n            result = to_wrap(*args, **kwargs)\n            ReturnType = type(args[at_position])\n            return ReturnType(result)\n\n        return wrapper\n\n    return decorator", "response": "Decorator to wrap the return value with the result of type at_position"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replace_exceptions(\n    old_to_new_exceptions: Dict[Type[BaseException], Type[BaseException]]\n) -> Callable[..., Any]:\n    \"\"\"\n    Replaces old exceptions with new exceptions to be raised in their place.\n    \"\"\"\n    old_exceptions = tuple(old_to_new_exceptions.keys())\n\n    def decorator(to_wrap: Callable[..., Any]) -> Callable[..., Any]:\n        @functools.wraps(to_wrap)\n        # String type b/c pypy3 throws SegmentationFault with Iterable as arg on nested fn\n        # Ignore so we don't have to import `Iterable`\n        def wrapper(\n            *args: Iterable[Any], **kwargs: Dict[str, Any]\n        ) -> Callable[..., Any]:\n            try:\n                return to_wrap(*args, **kwargs)\n            except old_exceptions as err:\n                try:\n                    raise old_to_new_exceptions[type(err)] from err\n                except KeyError:\n                    raise TypeError(\n                        \"could not look up new exception to use for %r\" % err\n                    ) from err\n\n        return wrapper\n\n    return decorator", "response": "Decorator that replaces old exceptions with new exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a tuple from a dict to a parenthesized list of its types.", "response": "def collapse_if_tuple(abi):\n    \"\"\"Converts a tuple from a dict to a parenthesized list of its types.\n\n    >>> from eth_utils.abi import collapse_if_tuple\n    >>> collapse_if_tuple(\n    ...     {\n    ...         'components': [\n    ...             {'name': 'anAddress', 'type': 'address'},\n    ...             {'name': 'anInt', 'type': 'uint256'},\n    ...             {'name': 'someBytes', 'type': 'bytes'},\n    ...         ],\n    ...         'type': 'tuple',\n    ...     }\n    ... )\n    '(address,uint256,bytes)'\n    \"\"\"\n    typ = abi[\"type\"]\n    if not typ.startswith(\"tuple\"):\n        return typ\n\n    delimited = \",\".join(collapse_if_tuple(c) for c in abi[\"components\"])\n    # Whatever comes after \"tuple\" is the array dims.  The ABI spec states that\n    # this will have the form \"\", \"[]\", or \"[k]\".\n    array_dim = typ[5:]\n    collapsed = \"({}){}\".format(delimited, array_dim)\n\n    return collapsed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the given string of text type is an address in hexadecimal encoded form.", "response": "def is_hex_address(value: Any) -> bool:\n    \"\"\"\n    Checks if the given string of text type is an address in hexadecimal encoded form.\n    \"\"\"\n    if not is_text(value):\n        return False\n    elif not is_hex(value):\n        return False\n    else:\n        unprefixed = remove_0x_prefix(value)\n        return len(unprefixed) == 40"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_binary_address(value: Any) -> bool:\n    if not is_bytes(value):\n        return False\n    elif len(value) != 20:\n        return False\n    else:\n        return True", "response": "Checks if the given string is a raw bytes form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_address(value: Any) -> bool:\n    if is_checksum_formatted_address(value):\n        return is_checksum_address(value)\n    elif is_hex_address(value):\n        return True\n    elif is_binary_address(value):\n        return True\n    else:\n        return False", "response": "Checks if the given string is an address in any of the known formats."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_normalized_address(value: AnyStr) -> HexAddress:\n    try:\n        hex_address = hexstr_if_str(to_hex, value).lower()\n    except AttributeError:\n        raise TypeError(\n            \"Value must be any string, instead got type {}\".format(type(value))\n        )\n    if is_address(hex_address):\n        return HexAddress(hex_address)\n    else:\n        raise ValueError(\n            \"Unknown format {}, attempted to normalize to {}\".format(value, hex_address)\n        )", "response": "Converts an address to its normalized hexadecimal representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_normalized_address(value: Any) -> bool:\n    if not is_address(value):\n        return False\n    else:\n        return value == to_normalized_address(value)", "response": "Returns whether the provided value is an address in its normalized form."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the value is a canonical form.", "response": "def is_canonical_address(address: Any) -> bool:\n    \"\"\"\n    Returns `True` if the `value` is an address in its canonical form.\n    \"\"\"\n    if not is_bytes(address) or len(address) != 20:\n        return False\n    return address == to_canonical_address(address)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_same_address(left: AnyAddress, right: AnyAddress) -> bool:\n    if not is_address(left) or not is_address(right):\n        raise ValueError(\"Both values must be valid addresses\")\n    else:\n        return to_normalized_address(left) == to_normalized_address(right)", "response": "Checks if both addresses are same or not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_checksum_address(value: AnyStr) -> ChecksumAddress:\n    norm_address = to_normalized_address(value)\n    address_hash = encode_hex(keccak(text=remove_0x_prefix(norm_address)))\n\n    checksum_address = add_0x_prefix(\n        \"\".join(\n            (\n                norm_address[i].upper()\n                if int(address_hash[i], 16) > 7\n                else norm_address[i]\n            )\n            for i in range(2, 42)\n        )\n    )\n    return ChecksumAddress(HexAddress(checksum_address))", "response": "Makes a checksum address given a supported format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget MSI token from the MSI_ENDPOINT.", "response": "def get_msi_token(resource, port=50342, msi_conf=None):\n    \"\"\"Get MSI token if MSI_ENDPOINT is set.\n\n    IF MSI_ENDPOINT is not set, will try legacy access through 'http://localhost:{}/oauth2/token'.format(port).\n\n    If msi_conf is used, must be a dict of one key in [\"client_id\", \"object_id\", \"msi_res_id\"]\n\n    :param str resource: The resource where the token would be use.\n    :param int port: The port if not the default 50342 is used. Ignored if MSI_ENDPOINT is set.\n    :param dict[str,str] msi_conf: msi_conf if to request a token through a User Assigned Identity (if not specified, assume System Assigned)\n    \"\"\"\n    request_uri = os.environ.get(\"MSI_ENDPOINT\", 'http://localhost:{}/oauth2/token'.format(port))\n    payload = {\n        'resource': resource\n    }\n    if msi_conf:\n        if len(msi_conf) > 1:\n            raise ValueError(\"{} are mutually exclusive\".format(list(msi_conf.keys())))\n        payload.update(msi_conf)\n\n    try:\n        result = requests.post(request_uri, data=payload, headers={'Metadata': 'true'})\n        _LOGGER.debug(\"MSI: Retrieving a token from %s, with payload %s\", request_uri, payload)\n        result.raise_for_status()\n    except Exception as ex:  # pylint: disable=broad-except\n        _LOGGER.warning(\"MSI: Failed to retrieve a token from '%s' with an error of '%s'. This could be caused \"\n                        \"by the MSI extension not yet fully provisioned.\",\n                        request_uri, ex)\n        raise\n    token_entry = result.json()\n    return token_entry['token_type'], token_entry['access_token'], token_entry"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a MSI token from inside a webapp or functions.", "response": "def get_msi_token_webapp(resource):\n    \"\"\"Get a MSI token from inside a webapp or functions.\n\n    Env variable will look like:\n\n    - MSI_ENDPOINT = http://127.0.0.1:41741/MSI/token/\n    - MSI_SECRET = 69418689F1E342DD946CB82994CDA3CB\n    \"\"\"\n    try:\n        msi_endpoint = os.environ['MSI_ENDPOINT']\n        msi_secret = os.environ['MSI_SECRET']\n    except KeyError as err:\n        err_msg = \"{} required env variable was not found. You might need to restart your app/function.\".format(err)\n        _LOGGER.critical(err_msg)\n        raise RuntimeError(err_msg)\n    request_uri = '{}/?resource={}&api-version=2017-09-01'.format(msi_endpoint, resource)\n    headers = {\n        'secret': msi_secret\n    }\n\n    err = None\n    try:\n        result = requests.get(request_uri, headers=headers)\n        _LOGGER.debug(\"MSI: Retrieving a token from %s\", request_uri)\n        if result.status_code != 200:\n            err = result.text\n        # Workaround since not all failures are != 200\n        if 'ExceptionMessage' in result.text:\n            err = result.text\n    except Exception as ex:  # pylint: disable=broad-except\n        err = str(ex)\n\n    if err:\n        err_msg = \"MSI: Failed to retrieve a token from '{}' with an error of '{}'.\".format(\n            request_uri, err\n        )\n        _LOGGER.critical(err_msg)\n        raise RuntimeError(err_msg)\n    _LOGGER.debug('MSI: token retrieved')\n    token_entry = result.json()\n    return token_entry['token_type'], token_entry['access_token'], token_entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconfigures the internal state of the object.", "response": "def _configure(self, **kwargs):\n        \"\"\"Configure authentication endpoint.\n\n        Optional kwargs may include:\n\n            - cloud_environment (msrestazure.azure_cloud.Cloud): A targeted cloud environment\n            - china (bool): Configure auth for China-based service,\n              default is 'False'.\n            - tenant (str): Alternative tenant, default is 'common'.\n            - resource (str): Alternative authentication resource, default\n              is 'https://management.core.windows.net/'.\n            - verify (bool): Verify secure connection, default is 'True'.\n            - timeout (int): Timeout of the request in seconds.\n            - proxies (dict): Dictionary mapping protocol or protocol and\n              hostname to the URL of the proxy.\n            - cache (adal.TokenCache): A adal.TokenCache, see ADAL configuration\n              for details. This parameter is not used here and directly passed to ADAL.\n        \"\"\"\n        if kwargs.get('china'):\n            err_msg = (\"china parameter is deprecated, \"\n                       \"please use \"\n                       \"cloud_environment=msrestazure.azure_cloud.AZURE_CHINA_CLOUD\")\n            warnings.warn(err_msg, DeprecationWarning)\n            self._cloud_environment = AZURE_CHINA_CLOUD\n        else:\n            self._cloud_environment = AZURE_PUBLIC_CLOUD\n        self._cloud_environment = kwargs.get('cloud_environment', self._cloud_environment)\n\n        auth_endpoint = self._cloud_environment.endpoints.active_directory\n        resource = self._cloud_environment.endpoints.active_directory_resource_id\n\n        self._tenant = kwargs.get('tenant', \"common\")\n        self._verify = kwargs.get('verify')  # 'None' will honor ADAL_PYTHON_SSL_NO_VERIFY\n        self.resource = kwargs.get('resource', resource)\n        self._proxies = kwargs.get('proxies')\n        self._timeout = kwargs.get('timeout')\n        self._cache = kwargs.get('cache')\n        self.store_key = \"{}_{}\".format(\n            auth_endpoint.strip('/'), self.store_key)\n        self.secret = None\n        self._context = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting token fields from camel case to vacation.", "response": "def _convert_token(self, token):\n        \"\"\"Convert token fields from camel case.\n\n        :param dict token: An authentication token.\n        :rtype: dict\n        \"\"\"\n        # Beware that ADAL returns a pointer to its own dict, do\n        # NOT change it in place\n        token = token.copy()\n\n        # If it's from ADAL, expiresOn will be in ISO form.\n        # Bring it back to float, using expiresIn\n        if \"expiresOn\" in token and \"expiresIn\" in token:\n            token[\"expiresOn\"] = token['expiresIn'] + time.time()\n        return {self._case.sub(r'\\1_\\2', k).lower(): v\n                for k, v in token.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate token - friendly Requests session using auto - refresh.", "response": "def signed_session(self, session=None):\n        \"\"\"Create token-friendly Requests session, using auto-refresh.\n        Used internally when a request is made.\n\n        If a session object is provided, configure it directly. Otherwise,\n        create a new session and return it.\n\n        :param session: The session to configure for authentication\n        :type session: requests.Session\n        \"\"\"\n        self.set_token() # Adal does the caching.\n        self._parse_token()\n        return super(AADMixin, self).signed_session(session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nattempts to refresh the session using a newly acquired token. If no session object is provided create a new session and return it.", "response": "def refresh_session(self, session=None):\n        \"\"\"Return updated session if token has expired, attempts to\n        refresh using newly acquired token.\n\n        If a session object is provided, configure it directly. Otherwise,\n        create a new session and return it.\n\n        :param session: The session to configure for authentication\n        :type session: requests.Session\n        :rtype: requests.Session.\n        \"\"\"\n        if 'refresh_token' in self.token:\n            try:\n                token = self._context.acquire_token_with_refresh_token(\n                    self.token['refresh_token'],\n                    self.id,\n                    self.resource,\n                    self.secret # This is needed when using Confidential Client\n                )\n                self.token = self._convert_token(token)\n            except adal.AdalError as err:\n                raise_with_traceback(AuthenticationError, \"\", err)\n        return self.signed_session(session)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting token using Username and Password credentials.", "response": "def set_token(self):\n        \"\"\"Get token using Username/Password credentials.\n\n        :raises: AuthenticationError if credentials invalid, or call fails.\n        \"\"\"\n        super(UserPassCredentials, self).set_token()\n        try:\n            token = self._context.acquire_token_with_username_password(\n                self.resource,\n                self.username,\n                self.password,\n                self.id\n            )\n            self.token = self._convert_token(token)\n        except adal.AdalError as err:\n            raise_with_traceback(AuthenticationError, \"\", err)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_token(self):\n        super(ServicePrincipalCredentials, self).set_token()\n        try:\n            token = self._context.acquire_token_with_client_credentials(\n                self.resource,\n                self.id,\n                self.secret\n            )\n            self.token = self._convert_token(token)\n        except adal.AdalError as err:\n            raise_with_traceback(AuthenticationError, \"\", err)", "response": "Get token using Client ID and Secret credentials."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a requests session with any required auth headers applied.", "response": "def signed_session(self, session=None):\n        \"\"\"Create requests session with any required auth headers applied.\n\n        If a session object is provided, configure it directly. Otherwise,\n        create a new session and return it.\n\n        :param session: The session to configure for authentication\n        :type session: requests.Session\n        :rtype: requests.Session\n        \"\"\"\n        session = super(AdalAuthentication, self).signed_session(session)\n\n        try:\n            raw_token = self._adal_method(*self._args, **self._kwargs)\n        except adal.AdalError as err:\n            # pylint: disable=no-member\n            if 'AADSTS70008:' in ((getattr(err, 'error_response', None) or {}).get('error_description') or ''):\n                raise Expired(\"Credentials have expired due to inactivity.\")\n            else:\n                raise AuthenticationError(err)\n        except ConnectionError as err:\n            raise AuthenticationError('Please ensure you have network connection. Error detail: ' + str(err))\n\n        scheme, token = raw_token['tokenType'], raw_token['accessToken']\n        header = \"{} {}\".format(scheme, token)\n        session.headers['Authorization'] = header\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef signed_session(self, session=None):\n        # Token cache is handled by the VM extension, call each time to avoid expiration\n        self.set_token()\n        return super(MSIAuthentication, self).signed_session(session)", "response": "Create requests session with any required auth headers applied."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate(url):\n    if url is None:\n        return\n    parsed = urlparse(url)\n    if not parsed.scheme or not parsed.netloc:\n        raise ValueError(\"Invalid URL header\")", "response": "Validate a url.\n\n    :param str url: Polling URL extracted from response header.\n    :raises: ValueError if URL has no scheme or host."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a URL from a header requests. Response.", "response": "def _get_header_url(response, header_name):\n    \"\"\"Get a URL from a header requests.\n\n    :param requests.Response response: REST call response.\n    :param str header_name: Header name.\n    :returns: URL if not None AND valid, None otherwise\n    \"\"\"\n    url = response.headers.get(header_name)\n    try:\n        _validate(url)\n    except ValueError:\n        return None\n    else:\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _raise_if_bad_http_status_and_method(self, response):\n        code = response.status_code\n        if code in {200, 202} or \\\n           (code == 201 and self.method in {'PUT', 'PATCH'}) or \\\n           (code == 204 and self.method in {'DELETE', 'POST'}):\n            return\n        raise BadStatus(\n            \"Invalid return status for {!r} operation\".format(self.method))", "response": "Check if the response status code is valid for a Put or Patch\n        request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the response body contains meaningful content.", "response": "def _is_empty(self, response):\n        \"\"\"Check if response body contains meaningful content.\n\n        :rtype: bool\n        :raises: DeserializationError if response body contains invalid\n         json data.\n        \"\"\"\n        if not response.content:\n            return True\n        try:\n            body = response.json()\n            return not body\n        except ValueError:\n            raise DeserializationError(\n                \"Error occurred in deserializing the response body.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattempting to deserialize resource from response.", "response": "def _deserialize(self, response):\n        \"\"\"Attempt to deserialize resource from response.\n\n        :param requests.Response response: latest REST call response.\n        \"\"\"\n        # Hacking response with initial status_code\n        previous_status = response.status_code\n        response.status_code = self.initial_status_code\n        resource = self.get_outputs(response)\n        response.status_code = previous_status\n\n        # Hack for Storage or SQL, to workaround the bug in the Python generator\n        if resource is None:\n            previous_status = response.status_code\n            for status_code_to_test in [200, 201]:\n                try:\n                    response.status_code = status_code_to_test\n                    resource = self.get_outputs(response)\n                except ClientException:\n                    pass\n                else:\n                    return resource\n                finally:\n                    response.status_code = previous_status\n        return resource"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_async_status(self, response):\n        if self._is_empty(response):\n            return None\n        body = response.json()\n        return body.get('status')", "response": "Attempt to find status info in response body."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the provisioning state from the latest REST call response.", "response": "def _get_provisioning_state(self, response):\n        \"\"\"\n        Attempt to get provisioning state from resource.\n        :param requests.Response response: latest REST call response.\n        :returns: Status if found, else 'None'.\n        \"\"\"\n        if self._is_empty(response):\n            return None\n        body = response.json()\n        return body.get(\"properties\", {}).get(\"provisioningState\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_status_from_location(self, response):\n        self._raise_if_bad_http_status_and_method(response)\n        code = response.status_code\n        if code == 202:\n            self.status = \"InProgress\"\n        else:\n            self.status = 'Succeeded'\n            if self._is_empty(response):\n                self.resource = None\n            else:\n                self.resource = self._deserialize(response)", "response": "Process the latest status update retrieved from a location HTTP response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_status_from_resource(self, response):\n        self._raise_if_bad_http_status_and_method(response)\n        if self._is_empty(response):\n            raise BadResponse('The response from long running operation '\n                              'does not contain a body.')\n\n        status = self._get_provisioning_state(response)\n        self.status = status or 'Succeeded'\n\n        self.resource = self._deserialize(response)", "response": "Process the latest status update retrieved from the same URL as\n othewise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _start(self, update_cmd):\n        try:\n            self._poll(update_cmd)\n\n        except BadStatus:\n            self._operation.status = 'Failed'\n            self._exception = CloudError(self._response)\n\n        except BadResponse as err:\n            self._operation.status = 'Failed'\n            self._exception = CloudError(self._response, str(err))\n\n        except OperationFailed:\n            self._exception = CloudError(self._response)\n\n        except Exception as err:\n            self._exception = err\n\n        finally:\n            self._done.set()\n\n        callbacks, self._callbacks = self._callbacks, []\n        while callbacks:\n            for call in callbacks:\n                call(self._operation)\n            callbacks, self._callbacks = self._callbacks, []", "response": "Start the long running operation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _polling_cookie(self):\n        parsed_url = urlparse(self._response.request.url)\n        host = parsed_url.hostname.strip('.')\n        if host == 'localhost':\n            return {'cookie': self._response.headers.get('set-cookie', '')}\n        return {}", "response": "Collect retry cookie - we only want to do this for the test server\n        at this point unless we implement a proper cookie policy."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _poll(self, update_cmd):\n        initial_url = self._response.request.url\n\n        while not finished(self.status()):\n            self._delay()\n            headers = self._polling_cookie()\n\n            if self._operation.async_url:\n                self._response = update_cmd(\n                    self._operation.async_url, headers)\n                self._operation.set_async_url_if_present(self._response)\n                self._operation.get_status_from_async(\n                    self._response)\n            elif self._operation.location_url:\n                self._response = update_cmd(\n                    self._operation.location_url, headers)\n                self._operation.set_async_url_if_present(self._response)\n                self._operation.get_status_from_location(\n                    self._response)\n            elif self._operation.method == \"PUT\":\n                self._response = update_cmd(initial_url, headers)\n                self._operation.set_async_url_if_present(self._response)\n                self._operation.get_status_from_resource(\n                    self._response)\n            else:\n                raise BadResponse(\n                    'Location header is missing from long running operation.')\n\n        if failed(self._operation.status):\n            raise OperationFailed(\"Operation failed or cancelled\")\n        elif self._operation.should_do_final_get():\n            self._response = update_cmd(initial_url)\n            self._operation.get_status_from_resource(\n                self._response)", "response": "Poll status of the long running operation so long as operation is incomplete and update_cmd will be called to retrieve the latest status of the long running operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a callback function to be run once the long running operation has completed.", "response": "def add_done_callback(self, func):\n        \"\"\"Add callback function to be run once the long running operation\n        has completed - regardless of the status of the operation.\n\n        :param callable func: Callback function that takes at least one\n         argument, a completed LongRunningOperation.\n        :raises: ValueError if the long running operation has already\n         completed.\n        \"\"\"\n        if self._done is None or self._done.is_set():\n            raise ValueError(\"Process is complete.\")\n        self._callbacks.append(func)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_done_callback(self, func):\n        if self._done is None or self._done.is_set():\n            raise ValueError(\"Process is complete.\")\n        self._callbacks = [c for c in self._callbacks if c != func]", "response": "Removes a callback from the long running operation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _extract_subscription_url(url):\n    match = re.match(r\".*/subscriptions/[a-f0-9-]+/\", url, re.IGNORECASE)\n    if not match:\n        raise ValueError(\"Unable to extract subscription ID from URL\")\n    return match.group(0)", "response": "Extract the subscription ID from the URL"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _register_rp(session, url_prefix, rp_name):\n    post_url = \"{}providers/{}/register?api-version=2016-02-01\".format(url_prefix, rp_name)\n    get_url = \"{}providers/{}?api-version=2016-02-01\".format(url_prefix, rp_name)\n    _LOGGER.warning(\"Resource provider '%s' used by this operation is not \"\n                    \"registered. We are registering for you.\", rp_name)\n    post_response = session.post(post_url)\n    if post_response.status_code != 200:\n        _LOGGER.warning(\"Registration failed. Please register manually.\")\n        return False\n\n    while True:\n        time.sleep(10)\n        rp_info = session.get(get_url).json()\n        if rp_info['registrationState'] == 'Registered':\n            _LOGGER.warning(\"Registration succeeded.\")\n            return True", "response": "Synchronously register the RP is paremeter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_resource_id(rid):\n    if not rid:\n        return {}\n    match = _ARMID_RE.match(rid)\n    if match:\n        result = match.groupdict()\n        children = _CHILDREN_RE.finditer(result['children'] or '')\n        count = None\n        for count, child in enumerate(children):\n            result.update({\n                key + '_%d' % (count + 1): group for key, group in child.groupdict().items()})\n        result['last_child_num'] = count + 1 if isinstance(count, int) else None\n        result = _populate_alternate_kwargs(result)\n    else:\n        result = dict(name=rid)\n    return {key: value for key, value in result.items() if value is not None}", "response": "Parses a resource id into its various parts."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes a dictionary of parsed arguments and populates the kwargs with the values that are used by generic ARM commands.", "response": "def _populate_alternate_kwargs(kwargs):\n    \"\"\" Translates the parsed arguments into a format used by generic ARM commands\n    such as the resource and lock commands.\n    \"\"\"\n\n    resource_namespace = kwargs['namespace']\n    resource_type = kwargs.get('child_type_{}'.format(kwargs['last_child_num'])) or kwargs['type']\n    resource_name = kwargs.get('child_name_{}'.format(kwargs['last_child_num'])) or kwargs['name']\n\n    _get_parents_from_parts(kwargs)\n    kwargs['resource_namespace'] = resource_namespace\n    kwargs['resource_type'] = resource_type\n    kwargs['resource_name'] = resource_name\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_parents_from_parts(kwargs):\n    parent_builder = []\n    if kwargs['last_child_num'] is not None:\n        parent_builder.append('{type}/{name}/'.format(**kwargs))\n        for index in range(1, kwargs['last_child_num']):\n            child_namespace = kwargs.get('child_namespace_{}'.format(index))\n            if child_namespace is not None:\n                parent_builder.append('providers/{}/'.format(child_namespace))\n            kwargs['child_parent_{}'.format(index)] = ''.join(parent_builder)\n            parent_builder.append(\n                '{{child_type_{0}}}/{{child_name_{0}}}/'\n                .format(index).format(**kwargs))\n        child_namespace = kwargs.get('child_namespace_{}'.format(kwargs['last_child_num']))\n        if child_namespace is not None:\n            parent_builder.append('providers/{}/'.format(child_namespace))\n        kwargs['child_parent_{}'.format(kwargs['last_child_num'])] = ''.join(parent_builder)\n    kwargs['resource_parent'] = ''.join(parent_builder) if kwargs['name'] else None\n    return kwargs", "response": "Get the parents given all the children parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resource_id(**kwargs):\n    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n    rid_builder = ['/subscriptions/{subscription}'.format(**kwargs)]\n    try:\n        try:\n            rid_builder.append('resourceGroups/{resource_group}'.format(**kwargs))\n        except KeyError:\n            pass\n        rid_builder.append('providers/{namespace}'.format(**kwargs))\n        rid_builder.append('{type}/{name}'.format(**kwargs))\n        count = 1\n        while True:\n            try:\n                rid_builder.append('providers/{{child_namespace_{}}}'\n                                   .format(count).format(**kwargs))\n            except KeyError:\n                pass\n            rid_builder.append('{{child_type_{0}}}/{{child_name_{0}}}'\n                               .format(count).format(**kwargs))\n            count += 1\n    except KeyError:\n        pass\n    return '/'.join(rid_builder)", "response": "Creates a valid resource id string from the given parts."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_valid_resource_id(rid, exception_type=None):\n    is_valid = False\n    try:\n        is_valid = rid and resource_id(**parse_resource_id(rid)).lower() == rid.lower()\n    except KeyError:\n        pass\n    if not is_valid and exception_type:\n        raise exception_type()\n    return is_valid", "response": "Validates the given resource id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates the given resource name to ARM guidelines.", "response": "def is_valid_resource_name(rname, exception_type=None):\n    \"\"\"Validates the given resource name to ARM guidelines, individual services may be more restrictive.\n\n    :param rname: The resource name being validated.\n    :type rname: str\n    :param exception_type: Raises this Exception if invalid.\n    :type exception_type: :class:`Exception`\n    :returns: A boolean describing whether the name is valid.\n    :rtype: bool\n    \"\"\"\n\n    match = _ARMNAME_RE.match(rname)\n\n    if match:\n        return True\n    if exception_type:\n        raise exception_type()\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave current configuration to file.", "response": "def save(self, filepath):\n        \"\"\"Save current configuration to file.\n\n        :param str filepath: Path to save file to.\n        :raises: ValueError if supplied filepath cannot be written to.\n        \"\"\"\n        self._config.add_section(\"Azure\")\n        self._config.set(\"Azure\",\n                         \"long_running_operation_timeout\",\n                         self.long_running_operation_timeout)\n        return super(AzureConfiguration, self).save(filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading configuration from existing file.", "response": "def load(self, filepath):\n        \"\"\"Load configuration from existing file.\n\n        :param str filepath: Path to existing config file.\n        :raises: ValueError if supplied config file is invalid.\n        \"\"\"\n        try:\n            self._config.read(filepath)\n            self.long_running_operation_timeout = self._config.getint(\n                \"Azure\", \"long_running_operation_timeout\")\n        except (ValueError, EnvironmentError, NoOptionError):\n            msg = \"Supplied config file incompatible\"\n            raise_with_traceback(ValueError, msg)\n        finally:\n            self._clear_config()\n        return super(AzureConfiguration, self).load(filepath)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _poll(self):\n\n        while not self.finished():\n            await self._delay()\n            await self.update_status()\n\n        if failed(self._operation.status):\n            raise OperationFailed(\"Operation failed or cancelled\")\n\n        elif self._operation.should_do_final_get():\n            if self._operation.method == 'POST' and self._operation.location_url:\n                final_get_url = self._operation.location_url\n            else:\n                final_get_url = self._operation.initial_response.request.url\n            self._response = await self.request_status(final_get_url)\n            self._operation.get_status_from_resource(self._response)", "response": "Poll status of the current operation so long as operation is incomplete and\n         is not incomplete."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndelaying the HTTP request.", "response": "async def _delay(self):\n        \"\"\"Check for a 'retry-after' header to set timeout,\n        otherwise use configured timeout.\n        \"\"\"\n        if self._response is None:\n            await asyncio.sleep(0)\n        if self._response.headers.get('retry-after'):\n            await asyncio.sleep(int(self._response.headers['retry-after']))\n        else:\n            await asyncio.sleep(self._timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def update_status(self):\n        if self._operation.async_url:\n            self._response = await self.request_status(self._operation.async_url)\n            self._operation.set_async_url_if_present(self._response)\n            self._operation.get_status_from_async(self._response)\n        elif self._operation.location_url:\n            self._response = await self.request_status(self._operation.location_url)\n            self._operation.set_async_url_if_present(self._response)\n            self._operation.get_status_from_location(self._response)\n        elif self._operation.method == \"PUT\":\n            initial_url = self._operation.initial_response.request.url\n            self._response = await self.request_status(initial_url)\n            self._operation.set_async_url_if_present(self._response)\n            self._operation.get_status_from_resource(self._response)\n        else:\n            raise BadResponse(\"Unable to find status link for polling.\")", "response": "Update the current status of the LRO."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def request_status(self, status_link):\n        # ARM requires to re-inject 'x-ms-client-request-id' while polling\n        header_parameters = {\n            'x-ms-client-request-id': self._operation.initial_response.request.headers['x-ms-client-request-id']\n        }\n        request = self._client.get(status_link, headers=header_parameters)\n        return await self._client.async_send(request, stream=False, **self._operation_config)", "response": "Do a simple GET to this status link."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattempting to deconstruct error message to retrieve further error data.", "response": "def message(self, value):\n        \"\"\"Attempt to deconstruct error message to retrieve further\n        error data.\n        \"\"\"\n        try:\n            import ast\n            value = ast.literal_eval(value)\n        except (SyntaxError, TypeError, ValueError):\n            pass\n        try:\n            value = value.get('value', value)\n            msg_data = value.split('\\n')\n            self._message = msg_data[0]\n        except AttributeError:\n            self._message = value\n            return\n        try:\n            self.request_id = msg_data[1].partition(':')[2]\n            time_str = msg_data[2].partition(':')\n            self.error_time = Deserializer.deserialize_iso(\n                \"\".join(time_str[2:]))\n        except (IndexError, DeserializationError):\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_cloud_from_metadata_endpoint(arm_endpoint, name=None, session=None):\n    cloud = Cloud(name or arm_endpoint)\n    cloud.endpoints.management = arm_endpoint\n    cloud.endpoints.resource_manager = arm_endpoint\n    _populate_from_metadata_endpoint(cloud, arm_endpoint, session)\n    return cloud", "response": "Get a Cloud object from an ARM endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_header_url(response, header_name):\n    url = response.headers.get(header_name)\n    try:\n        _validate(url)\n    except ValueError:\n        return None\n    else:\n        return url", "response": "Get a URL from a header requests. Response object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_empty(self, response):\n        # Assume ClientResponse has \"body\", and otherwise it's a requests.Response\n        content = response.text() if hasattr(response, \"body\") else response.text\n        if not content:\n            return True\n        try:\n            return not json.loads(content)\n        except ValueError:\n            raise DeserializationError(\n                \"Error occurred in deserializing the response body.\")", "response": "Check if the response body contains meaningful content."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _as_json(self, response):\n        # Assume ClientResponse has \"body\", and otherwise it's a requests.Response\n        content = response.text() if hasattr(response, \"body\") else response.text\n        try:\n            return json.loads(content)\n        except ValueError:\n            raise DeserializationError(\n                \"Error occurred in deserializing the response body.\")", "response": "Assuming this is not empty return the content as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nattempting to find status info in response body.", "response": "def _get_async_status(self, response):\n        \"\"\"Attempt to find status info in response body.\n\n        :param requests.Response response: latest REST call response.\n        :rtype: str\n        :returns: Status if found, else 'None'.\n        \"\"\"\n        if self._is_empty(response):\n            return None\n        body = self._as_json(response)\n        return body.get('status')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the provisioning state from the latest REST call response.", "response": "def _get_provisioning_state(self, response):\n        \"\"\"\n        Attempt to get provisioning state from resource.\n        :param requests.Response response: latest REST call response.\n        :returns: Status if found, else 'None'.\n        \"\"\"\n        if self._is_empty(response):\n            return None\n        body = self._as_json(response)\n        return body.get(\"properties\", {}).get(\"provisioningState\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking whether the polling should end doing a final GET.", "response": "def should_do_final_get(self):\n        \"\"\"Check whether the polling should end doing a final GET.\n\n        :param requests.Response response: latest REST call response.\n        :rtype: bool\n        \"\"\"\n        return ((self.async_url or not self.resource) and self.method in {'PUT', 'PATCH'}) \\\n                or (self.lro_options['final-state-via'] == _LOCATION_FINAL_STATE and self.location_url and self.async_url and self.method == 'POST')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing first response after initiating long running operation and set self. status attribute.", "response": "def set_initial_status(self, response):\n        \"\"\"Process first response after initiating long running\n        operation and set self.status attribute.\n\n        :param requests.Response response: initial REST call response.\n        \"\"\"\n        self._raise_if_bad_http_status_and_method(response)\n\n        if self._is_empty(response):\n            self.resource = None\n        else:\n            try:\n                self.resource = self._deserialize(response)\n            except DeserializationError:\n                self.resource = None\n\n        self.set_async_url_if_present(response)\n\n        if response.status_code in {200, 201, 202, 204}:\n            if self.async_url or self.location_url or response.status_code == 202:\n                self.status = 'InProgress'\n            elif response.status_code == 201:\n                status = self._get_provisioning_state(response)\n                self.status = status or 'InProgress'\n            elif response.status_code == 200:\n                status = self._get_provisioning_state(response)\n                self.status = status or 'Succeeded'\n            elif response.status_code == 204:\n                self.status = 'Succeeded'\n                self.resource = None\n            else:\n                raise OperationFailed(\"Invalid status found\")\n            return\n        raise OperationFailed(\"Operation failed or cancelled\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess the latest status update retrieved from the same URL as othewise.", "response": "def get_status_from_resource(self, response):\n        \"\"\"Process the latest status update retrieved from the same URL as\n        the previous request.\n\n        :param requests.Response response: latest REST call response.\n        :raises: BadResponse if status not 200 or 204.\n        \"\"\"\n        self._raise_if_bad_http_status_and_method(response)\n        if self._is_empty(response):\n            raise BadResponse('The response from long running operation '\n                              'does not contain a body.')\n\n        status = self._get_provisioning_state(response)\n        self.status = status or 'Succeeded'\n\n        self.parse_resource(response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_resource(self, response):\n        self._raise_if_bad_http_status_and_method(response)\n        if not self._is_empty(response):\n            self.resource = self._deserialize(response)\n        else:\n            self.resource = None", "response": "Parse the response body into a resource object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the latest status update retrieved from the async REST call header.", "response": "def get_status_from_async(self, response):\n        \"\"\"Process the latest status update retrieved from a\n        'azure-asyncoperation' header.\n\n        :param requests.Response response: latest REST call response.\n        :raises: BadResponse if response has no body, or body does not\n         contain status.\n        \"\"\"\n        self._raise_if_bad_http_status_and_method(response)\n        if self._is_empty(response):\n            raise BadResponse('The response from long running operation '\n                              'does not contain a body.')\n\n        self.status = self._get_async_status(response)\n        if not self.status:\n            raise BadResponse(\"No status found in body\")\n\n        # Status can contains information, see ARM spec:\n        # https://github.com/Azure/azure-resource-manager-rpc/blob/master/v1.0/Addendum.md#operation-resource-format\n        # \"properties\": {\n        # /\\* The resource provider can choose the values here, but it should only be\n        #   returned on a successful operation (status being \"Succeeded\"). \\*/\n        #},\n        # So try to parse it\n        try:\n            self.resource = self._deserialize(response)\n        except Exception:\n            self.resource = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize(self, client, initial_response, deserialization_callback):\n        self._client = client\n        self._response = initial_response\n        self._operation = LongRunningOperation(initial_response, deserialization_callback, self._lro_options)\n        try:\n            self._operation.set_initial_status(initial_response)\n        except BadStatus:\n            self._operation.status = 'Failed'\n            raise CloudError(initial_response)\n        except BadResponse as err:\n            self._operation.status = 'Failed'\n            raise CloudError(initial_response, str(err))\n        except OperationFailed:\n            raise CloudError(initial_response)", "response": "Initialize the LRO with the initial status of the LRO."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef worker():\n\n  import torch\n  import torch.distributed as dist\n  from torch.multiprocessing import Process\n  import numpy as np\n\n  print(\"Initializing distributed pytorch\")\n  os.environ['MASTER_ADDR'] = str(args.master_addr)\n  os.environ['MASTER_PORT'] = str(args.master_port)\n  # Use TCP backend. Gloo needs nightly, where it currently fails with\n  #     dist.init_process_group('gloo', rank=args.rank,\n  #   AttributeError: module 'torch.distributed' has no attribute 'init_process_group'\n  dist.init_process_group('tcp', rank=args.rank,\n                          world_size=args.size)\n\n  tensor = torch.ones(args.size_mb*250*1000)*(args.rank+1)\n  time_list = []\n  outfile = 'out' if args.rank == 0 else '/dev/null'\n  log = util.FileLogger(outfile)\n  for i in range(args.iters):\n    # print('before: rank ', args.rank, ' has data ', tensor[0])\n\n    start_time = time.perf_counter()\n    if args.rank == 0:\n      dist.send(tensor=tensor, dst=1)\n    else:\n      dist.recv(tensor=tensor, src=0)\n      \n    elapsed_time_ms = (time.perf_counter() - start_time)*1000\n    time_list.append(elapsed_time_ms)\n    # print('after: rank ', args.rank, ' has data ', tensor[0])\n    rate = args.size_mb/(elapsed_time_ms/1000)\n\n    log('%03d/%d added %d MBs in %.1f ms: %.2f MB/second' % (i, args.iters, args.size_mb, elapsed_time_ms, rate))\n\n  min = np.min(time_list)\n  median = np.median(time_list)\n  log(f\"min: {min:8.2f}, median: {median:8.2f}, mean: {np.mean(time_list):8.2f}\")", "response": "Initialize the distributed environment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_backend(backend_name: str):\n  global _backend, _backend_name\n  _backend_name = backend_name\n\n  assert not ncluster_globals.task_launched, \"Not allowed to change backend after launching a task (this pattern is error-prone)\"\n  if backend_name == 'aws':\n    _backend = aws_backend\n  elif backend_name == 'local':\n    _backend = local_backend\n  else:\n    assert False, f\"Unknown backend {backend_name}\"\n  ncluster_globals.LOGDIR_ROOT = _backend.LOGDIR_ROOT", "response": "Sets backend to be used by the task."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_job(name: str = '',\n             run_name: str = '',\n             num_tasks: int = 0,\n             install_script: str = '',\n             **kwargs\n             ) -> backend.Job:\n  \"\"\"\n  Create a job using current backend. Blocks until all tasks are up and initialized.\n\n  Args:\n    name: name of the job\n    run_name: name of the run (auto-assigned if empty)\n    num_tasks: number of tasks\n    install_script: bash-runnable script\n    **kwargs:\n\n  Returns:\n    backend.Job\n  \"\"\"\n  return _backend.make_job(name=name, run_name=run_name, num_tasks=num_tasks,\n                           install_script=install_script, **kwargs)", "response": "Creates a new job using current backend. Blocks until all tasks are up and initialized."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates task also create dummy run if not specified.", "response": "def make_task(name='',\n              run_name='',\n              **kwargs) -> Task:\n  \"\"\"Create task, also create dummy run if not specified.\"\"\"\n  ncluster_globals.task_launched = True\n\n  name = ncluster_globals.auto_assign_task_name_if_needed(name)\n\n  # tmux can't use . for session names\n  tmux_session = name.replace('.', '=')\n  tmux_window_id = 0\n  util.log(f'killing session {tmux_session}')\n\n  if not util.is_set(\"NCLUSTER_NOKILL_TMUX\"):\n    os.system(f'tmux kill-session -t {tmux_session}')\n  os.system(f'tmux new-session -s {tmux_session} -n {tmux_window_id} -d')\n\n  task = Task(name,\n              tmux_session=tmux_session,  # propagate optional args\n              run_name=run_name,\n              **kwargs)\n  ncluster_globals.register_task(task, run_name)\n  return task"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nswitch currently active tmux window for given task. 0 is the default window. 1 is the current window 2 is the current window", "response": "def switch_window(self, window_id: int):\n    \"\"\"\n    Switches currently active tmux window for given task. 0 is the default window\n    Args:\n      window_id: integer id of tmux window to use\n    \"\"\"\n\n    # windows are numbered sequentially 0, 1, 2, ...\n    # create any missing windows and make them point to the same directory\n    if window_id not in self.tmux_available_window_ids:\n      for i in range(max(self.tmux_available_window_ids)+1, window_id+1):\n        self._run_raw(f'tmux new-window -t {self.tmux_session} -d')\n\n        tmux_window = self.tmux_session + ':' + str(i)\n        cmd = shlex.quote(f'cd {self.taskdir}')\n        tmux_cmd = f'tmux send-keys -t {tmux_window} {cmd} Enter'\n        self._run_raw(tmux_cmd)\n        self.tmux_available_window_ids.append(i)\n\n    self.tmux_window_id = window_id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _run_raw(self, cmd, ignore_errors=False):\n    # TODO: capture stdout/stderr for feature parity with aws_backend\n    result = os.system(cmd)\n    if result != 0:\n      if ignore_errors:\n        self.log(f\"command ({cmd}) failed.\")\n        assert False, \"_run_raw failed\"", "response": "Runs command directly skipping tmux interface"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupload a file to the remote instance. If location not specified dumps it into default directory. Creates missing directories in path name.", "response": "def upload(self, local_fn, remote_fn=None, dont_overwrite=False):\n    \"\"\"Uploads file to remote instance. If location not specified, dumps it\n    into default directory. Creates missing directories in path name.\"\"\"\n\n    # support wildcard through glob\n    if '*' in local_fn:\n      for local_subfn in glob.glob(local_fn):\n        self.upload(local_subfn)\n      return\n\n    if remote_fn is None:\n      remote_fn = os.path.basename(local_fn)\n\n    if dont_overwrite and self.exists(remote_fn):\n      self.log(\"Remote file %s exists, skipping\" % (remote_fn,))\n      return\n\n    if not remote_fn.startswith('/'):\n      remote_fn = self.taskdir + '/' + remote_fn\n\n    remote_fn = remote_fn.replace('~', self.homedir)\n    self.log('uploading ' + local_fn + ' to ' + remote_fn)\n\n    local_fn = os.path.abspath(local_fn)\n    self._run_raw(\"cp -R %s %s\" % (local_fn, remote_fn))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn logging directory creating one if necessary.", "response": "def logdir(self):\n    \"\"\"Returns logging directory, creating one if necessary. See \"Logdir\" section  of design doc on naming convention.\"\"\"\n\n    run_name = ncluster_globals.get_run_for_task(self)\n    logdir = ncluster_globals.get_logdir(run_name)\n    if logdir:\n      return logdir\n\n    # create logdir. Only single task in a group creates the logdir\n    if ncluster_globals.is_chief(self, run_name):\n      chief = self\n    else:\n      chief = ncluster_globals.get_chief(run_name)\n\n    chief.setup_logdir()\n    return ncluster_globals.get_logdir(run_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_logdir(self):\n    # todo: locking on logdir creation\n\n    \"\"\"Create logdir for task/job/run. No-op if the task is not chief (0'th task of 0'th job of run)\n    \"\"\"\n    run_name = ncluster_globals.get_run_for_task(self)\n    self.log(\"Creating logdir for run \"+run_name)\n    logdir_root = ncluster_globals.LOGDIR_ROOT\n    assert logdir_root\n\n    self.run(f'mkdir -p {logdir_root}')\n    find_command = f'find {logdir_root} -maxdepth 1 -type d'\n\n    stdout, stderr = self.run_with_output(find_command)\n    logdir = f\"{logdir_root}/{run_name}\"\n\n    counter = 0\n    while logdir in stdout:\n      counter += 1\n      new_logdir = f'{logdir_root}/{run_name}.{counter:02d}'\n      self.log(f'Warning, logdir {logdir} exists, deduping to {new_logdir}')\n      logdir = new_logdir\n    self.run(f'mkdir -p {logdir}')\n\n    ncluster_globals.set_logdir(run_name, logdir)\n    return logdir", "response": "Create logdir for the task and job."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, *args, **kwargs):\n\n    for job in self.jobs:\n      job.run(*args, **kwargs)", "response": "Runs command on every job in the run."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_with_output(self, *args, **kwargs):\n    for job in self.jobs:\n      job.run_with_output(*args, **kwargs)", "response": "Runs command on every first job in the run returns stdout."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef upload(self, *args, **kwargs):\n    for job in self.jobs:\n      job.upload(*args, **kwargs)", "response": "Runs command on every job in the run."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef network_setup():\n\n  # from https://gist.github.com/nguyendv/8cfd92fc8ed32ebb78e366f44c2daea6\n\n  ec2 = u.get_ec2_resource()\n  client = u.get_ec2_client()\n  existing_vpcs = u.get_vpc_dict()\n  zones = u.get_zones()\n\n  # create VPC from scratch. Remove this if default VPC works well enough.\n  vpc_name = u.get_vpc_name()\n  if u.get_vpc_name() in existing_vpcs:\n    print(\"Reusing VPC \" + vpc_name)\n    vpc = existing_vpcs[vpc_name]\n    subnets = list(vpc.subnets.all())\n    assert len(subnets) == len(\n      zones), \"Has %s subnets, but %s zones, something went wrong during resource creation, try delete_resources.py/create_resources.py\" % (\n      len(subnets), len(zones))\n\n  else:\n    print(\"Creating VPC \" + vpc_name)\n    vpc = ec2.create_vpc(CidrBlock='192.168.0.0/16')\n\n    # enable DNS on the VPC\n    response = vpc.modify_attribute(EnableDnsHostnames={\"Value\": True})\n    assert u.is_good_response(response)\n    response = vpc.modify_attribute(EnableDnsSupport={\"Value\": True})\n    assert u.is_good_response(response)\n\n    vpc.create_tags(Tags=u.create_name_tags(vpc_name))\n    vpc.wait_until_available()\n\n  gateways = u.get_gateway_dict(vpc)\n  gateway_name = u.get_gateway_name()\n  if gateway_name in gateways:\n    print(\"Reusing gateways \" + gateway_name)\n  else:\n    print(\"Creating internet gateway \" + gateway_name)\n    ig = ec2.create_internet_gateway()\n    ig.attach_to_vpc(VpcId=vpc.id)\n    ig.create_tags(Tags=u.create_name_tags(gateway_name))\n\n    # check that attachment succeeded\n    attach_state = u.extract_attr_for_match(ig.attachments, State=-1,\n                                            VpcId=vpc.id)\n    assert attach_state == 'available', \"vpc %s is in state %s\" % (vpc.id,\n                                                                   attach_state)\n\n    route_table = vpc.create_route_table()\n    route_table_name = u.get_route_table_name()\n    route_table.create_tags(Tags=u.create_name_tags(route_table_name))\n\n    dest_cidr = '0.0.0.0/0'\n    route_table.create_route(\n      DestinationCidrBlock=dest_cidr,\n      GatewayId=ig.id\n    )\n    # check success\n    for route in route_table.routes:\n      # result looks like this\n      # ec2.Route(route_table_id='rtb-a8b438cf',\n      #    destination_cidr_block='0.0.0.0/0')\n      if route.destination_cidr_block == dest_cidr:\n        break\n    else:\n      # sometimes get\n      #      AssertionError: Route for 0.0.0.0/0 not found in [ec2.Route(route_table_id='rtb-cd9153b0', destination_cidr_block='192.168.0.0/16')]\n      # TODO: add a wait/retry?\n      assert False, \"Route for %s not found in %s\" % (dest_cidr,\n                                                      route_table.routes)\n\n    assert len(zones) <= 16  # for cidr/20 to fit into cidr/16\n    ip = 0\n    for zone in zones:\n      cidr_block = '192.168.%d.0/20' % (ip,)\n      ip += 16\n      print(\"Creating subnet %s in zone %s\" % (cidr_block, zone))\n      subnet = vpc.create_subnet(CidrBlock=cidr_block,\n                                 AvailabilityZone=zone)\n      subnet.create_tags(Tags=[{'Key': 'Name', 'Value': f'{vpc_name}-subnet'},\n                               {'Key': 'Region', 'Value': zone}])\n      response = client.modify_subnet_attribute(\n        MapPublicIpOnLaunch={'Value': True},\n        SubnetId=subnet.id\n      )\n      assert u.is_good_response(response)\n      u.wait_until_available(subnet)\n      assert subnet.map_public_ip_on_launch, \"Subnet doesn't enable public IP by default, why?\"\n\n      route_table.associate_with_subnet(SubnetId=subnet.id)\n\n  # Use default VPC from now on\n  vpc = u.get_default_vpc()\n  if not vpc:\n    util.log(f\"Creating default VPC for region {u.get_region()}\")\n    client.create_default_vpc()\n  vpc = u.get_default_vpc()\n  assert vpc, \"Could not create default VPC?\"\n\n  existing_security_groups = u.get_security_group_dict()\n  security_group_name = u.get_security_group_name()\n  if security_group_name in existing_security_groups:\n    print(\"Reusing security group \" + security_group_name)\n    security_group = existing_security_groups[security_group_name]\n    assert security_group.vpc_id == vpc.id, f\"Found security group {security_group} \" \\\n                                            f\"attached to {security_group.vpc_id} but expected {vpc.id}\"\n  else:\n    print(\"Creating security group \" + security_group_name)\n    security_group = ec2.create_security_group(\n      GroupName=security_group_name, Description=security_group_name,\n      VpcId=vpc.id)\n\n    security_group.create_tags(Tags=u.create_name_tags(security_group_name))\n\n    # allow ICMP access for public ping\n    security_group.authorize_ingress(\n      CidrIp='0.0.0.0/0',\n      IpProtocol='icmp',\n      FromPort=-1,\n      ToPort=-1\n    )\n\n    # open public ports\n    # always include SSH port which is required for basic functionality\n    assert 22 in PUBLIC_TCP_RANGES, \"Must enable SSH access\"\n    for port in PUBLIC_TCP_RANGES:\n      if util.is_iterable(port):\n        assert len(port) == 2\n        from_port, to_port = port\n      else:\n        from_port, to_port = port, port\n\n      response = security_group.authorize_ingress(IpProtocol=\"tcp\",\n                                                  CidrIp=\"0.0.0.0/0\",\n                                                  FromPort=from_port,\n                                                  ToPort=to_port)\n      assert u.is_good_response(response)\n\n    for port in PUBLIC_UDP_RANGES:\n      if util.is_iterable(port):\n        assert len(port) == 2\n        from_port, to_port = port\n      else:\n        from_port, to_port = port, port\n\n      response = security_group.authorize_ingress(IpProtocol=\"udp\",\n                                                  CidrIp=\"0.0.0.0/0\",\n                                                  FromPort=from_port,\n                                                  ToPort=to_port)\n      assert u.is_good_response(response)\n\n    # allow ingress within security group\n    # Authorizing ingress doesn't work with names in a non-default VPC,\n    # so must use more complicated syntax\n    # https://github.com/boto/boto3/issues/158\n    response = {}\n    for protocol in ['icmp']:\n      try:\n        rule = {'FromPort': -1,\n                'IpProtocol': protocol,\n                'IpRanges': [],\n                'PrefixListIds': [],\n                'ToPort': -1,\n                'UserIdGroupPairs': [{'GroupId': security_group.id}]}\n        response = security_group.authorize_ingress(IpPermissions=[rule])\n      except Exception as e:\n        if response['Error']['Code'] == 'InvalidPermission.Duplicate':\n          print(\"Warning, got \" + str(e))\n        else:\n          assert False, \"Failed while authorizing ingress with \" + str(e)\n\n    for protocol in ['tcp', 'udp']:\n      try:\n        rule = {'FromPort': 0,\n                'IpProtocol': protocol,\n                'IpRanges': [],\n                'PrefixListIds': [],\n                'ToPort': 65535,\n                'UserIdGroupPairs': [{'GroupId': security_group.id}]}\n        response = security_group.authorize_ingress(IpPermissions=[rule])\n      except Exception as e:\n        if response['Error']['Code'] == 'InvalidPermission.Duplicate':\n          print(\"Warning, got \" + str(e))\n        else:\n          assert False, \"Failed while authorizing ingress with \" + str(e)\n\n  return vpc, security_group", "response": "Creates VPC if it doesn t already exists configures it for internet access returns vpc subnet security_group"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates keypair if necessary saves private key locally returns contents of private key file.", "response": "def keypair_setup():\n  \"\"\"Creates keypair if necessary, saves private key locally, returns contents\n  of private key file.\"\"\"\n\n  os.system('mkdir -p ' + u.PRIVATE_KEY_LOCATION)\n\n  keypair_name = u.get_keypair_name()\n  keypair = u.get_keypair_dict().get(keypair_name, None)\n  keypair_fn = u.get_keypair_fn()\n  if keypair:\n    print(\"Reusing keypair \" + keypair_name)\n    # check that local pem file exists and is readable\n    assert os.path.exists(\n      keypair_fn), \"Keypair %s exists, but corresponding .pem file %s is not found, delete keypair %s through console and run again to recreate keypair/.pem together\" % (\n      keypair_name, keypair_fn, keypair_name)\n    keypair_contents = open(keypair_fn).read()\n    assert len(keypair_contents) > 0\n  else:\n    print(\"Creating keypair \" + keypair_name)\n    ec2 = u.get_ec2_resource()\n    assert not os.path.exists(\n      keypair_fn), \"previous keypair exists, delete it with 'sudo rm %s' and also delete corresponding keypair through console\" % (\n      keypair_fn)\n    keypair = ec2.create_key_pair(KeyName=keypair_name)\n\n    open(keypair_fn, 'w').write(keypair.key_material)\n    os.system('chmod 400 ' + keypair_fn)\n\n  return keypair"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates placement_group group if necessary. Returns True if new placement_group group was created False otherwise.", "response": "def placement_group_setup(group_name):\n  \"\"\"Creates placement_group group if necessary. Returns True if new placement_group\n  group was created, False otherwise.\"\"\"\n\n  existing_placement_groups = u.get_placement_group_dict()\n\n  group = existing_placement_groups.get(group_name, None)\n  if group:\n    assert group.state == 'available'\n    assert group.strategy == 'cluster'\n    print(\"Reusing group \", group.name)\n    return group\n\n  print(\"Creating group \" + group_name)\n  ec2 = u.get_ec2_resource()\n  group = ec2.create_placement_group(GroupName=group_name, Strategy='cluster')\n  return group"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwait for a file to be found in the file system. Returns True if the file was detected within specified max_wait_sec False otherwise.", "response": "def wait_for_file(self, fn: str, max_wait_sec: int = 3600 * 24 * 365,\n                    check_interval: float = 0.02) -> bool:\n    \"\"\"\n    Waits for file maximum of max_wait_sec. Returns True if file was detected within specified max_wait_sec\n    Args:\n      fn: filename on task machine\n      max_wait_sec: how long to wait in seconds\n      check_interval: how often to check in seconds\n    Returns:\n      False if waiting was was cut short by max_wait_sec limit, True otherwise\n    \"\"\"\n    print(\"Waiting for file\", fn)\n    start_time = time.time()\n    while True:\n      if time.time() - start_time > max_wait_sec:\n        util.log(f\"Timeout exceeded ({max_wait_sec} sec) for {fn}\")\n        return False\n      if not self.exists(fn):\n        time.sleep(check_interval)\n        continue\n      else:\n        break\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upload(self, local_fn: str, remote_fn: str = '',\n             dont_overwrite: bool = False):\n    \"\"\"Uploads given file to the task. If remote_fn is not specified, dumps it\n    into task current directory with the same name.\n\n    Args:\n      local_fn: location of file locally\n      remote_fn: location of file on task\n      dont_overwrite: if True, will be no-op if target file exists\n      \"\"\"\n    raise NotImplementedError()", "response": "Uploads given file to the task."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning given method on every task in the job. Blocks until all tasks finish. Propagates exception from first failed task.", "response": "def _non_blocking_wrapper(self, method, *args, **kwargs):\n    \"\"\"Runs given method on every task in the job. Blocks until all tasks finish. Propagates exception from first\n    failed task.\"\"\"\n\n    exceptions = []\n\n    def task_run(task):\n      try:\n        getattr(task, method)(*args, **kwargs)\n      except Exception as e:\n        exceptions.append(e)\n\n    threads = [threading.Thread(name=f'task_{method}_{i}',\n                                target=task_run, args=[t])\n               for i, t in enumerate(self.tasks)]\n    for thread in threads:\n      thread.start()\n    for thread in threads:\n      thread.join()\n    if exceptions:\n      raise exceptions[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_vpc_dict():\n\n  client = get_ec2_client()\n  response = client.describe_vpcs()\n  assert is_good_response(response)\n\n  result = OrderedDict()\n  ec2 = get_ec2_resource()\n  for vpc_response in response['Vpcs']:\n    key = get_name(vpc_response.get('Tags', []))\n    if not key or key == EMPTY_NAME:  # skip VPC's that don't have a name assigned\n      continue\n\n    if key in result:\n      util.log(f\"Warning: Duplicate VPC group {key} in {response}\")\n      if DUPLICATE_CHECKING:\n        assert False\n    result[key] = ec2.Vpc(vpc_response['VpcId'])\n\n  return result", "response": "Returns a dictionary of named VPCs {name : vpc }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_vpc():\n  ec2 = get_ec2_resource()\n  for vpc in ec2.vpcs.all():\n    if vpc.is_default:\n      return vpc", "response": "Return default VPC if not present"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns dictionary of availability zone -> subnet for current VPC.", "response": "def get_subnet_dict():\n  \"\"\"Returns dictionary of \"availability zone\" -> subnet for current VPC.\"\"\"\n  subnet_dict = {}\n  vpc = get_vpc()\n  for subnet in vpc.subnets.all():\n    zone = subnet.availability_zone\n    assert zone not in subnet_dict, \"More than one subnet in %s, why?\" % (zone,)\n    subnet_dict[zone] = subnet\n  return subnet_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_efs_dict():\n  # there's no EC2 resource for EFS objects, so return EFS_ID instead\n  # https://stackoverflow.com/questions/47870342/no-ec2-resource-for-efs-objects\n\n  efs_client = get_efs_client()\n  response = call_with_retries(efs_client.describe_file_systems,\n                               'efs_client.describe_file_systems')\n  assert is_good_response(response)\n  result = OrderedDict()\n  for efs_response in response['FileSystems']:\n    fs_id = efs_response['FileSystemId']\n\n    tag_response = call_with_retries(efs_client.describe_tags,\n                                     \"efs_client.describe_tags\",\n                                     FileSystemId=fs_id, retry_interval_sec=2)\n    assert is_good_response(tag_response)\n    key = get_name(tag_response['Tags'])\n    if not key or key == EMPTY_NAME:  # skip EFS's without a name\n      continue\n    assert key not in result\n    result[key] = fs_id\n\n  return result", "response": "Returns a dictionary of EFS names and EFS IDs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary of placement group names to state and strategy.", "response": "def get_placement_group_dict():\n  \"\"\"Returns dictionary of {placement_group_name: (state, strategy)}\"\"\"\n\n  client = get_ec2_client()\n  response = client.describe_placement_groups()\n  assert is_good_response(response)\n\n  result = OrderedDict()\n  ec2 = get_ec2_resource()\n  for placement_group_response in response['PlacementGroups']:\n    key = placement_group_response['GroupName']\n    if key in result:\n      util.log(f\"Warning: Duplicate placement_group group {key}\")\n      if DUPLICATE_CHECKING:\n        assert False\n    result[key] = ec2.PlacementGroup(key)\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns dictionary of named security groups.", "response": "def get_security_group_dict():\n  \"\"\"Returns dictionary of named security groups {name: securitygroup}.\"\"\"\n\n  client = get_ec2_client()\n  response = client.describe_security_groups()\n  assert is_good_response(response)\n\n  result = OrderedDict()\n  ec2 = get_ec2_resource()\n  for security_group_response in response['SecurityGroups']:\n    key = get_name(security_group_response.get('Tags', []))\n    if not key or key == EMPTY_NAME:\n      continue  # ignore unnamed security groups\n    #    key = security_group_response['GroupName']\n    if key in result:\n      util.log(f\"Warning: Duplicate security group {key}\")\n      if DUPLICATE_CHECKING:\n        assert key not in result, (\"Duplicate security group \" + key)\n    result[key] = ec2.SecurityGroup(security_group_response['GroupId'])\n\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning dictionary of keypair name to keypair", "response": "def get_keypair_dict():\n  \"\"\"Returns dictionary of {keypairname: keypair}\"\"\"\n\n  client = get_ec2_client()\n  response = client.describe_key_pairs()\n  assert is_good_response(response)\n\n  result = {}\n  ec2 = get_ec2_resource()\n  for keypair in response['KeyPairs']:\n    keypair_name = keypair.get('KeyName', '')\n    if keypair_name in result:\n      util.log(f\"Warning: Duplicate key {keypair_name}\")\n    if DUPLICATE_CHECKING:\n      assert keypair_name not in result, \"Duplicate key \" + keypair_name\n    result[keypair_name] = ec2.KeyPair(keypair_name)\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn current keypair name.", "response": "def get_keypair_name():\n  \"\"\"Returns current keypair name.\"\"\"\n\n  username = get_username()\n  assert '-' not in username, \"username must not contain -, change $USER\"\n  validate_aws_name(username)\n  assert len(username) < 30  # to avoid exceeding AWS 127 char limit\n  return get_prefix() + '-' + username"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_keypair_fn():\n\n  keypair_name = get_keypair_name()\n  account = get_account_number()\n  region = get_region()\n  fn = f'{PRIVATE_KEY_LOCATION}/{keypair_name}-{account}-{region}.pem'\n  return fn", "response": "Returns the path to the. pem file for the keypair"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lookup_image(wildcard):\n\n  ec2 = get_ec2_resource()\n  filter_ = {'Name': 'name', 'Values': [wildcard]}\n\n  images = list(ec2.images.filter(Filters=[filter_]))\n\n  # Note, can add filtering by Owners as follows\n  #  images = list(ec2.images.filter_(Filters = [filter_], Owners=['self', 'amazon']))\n\n  assert len(images) <= 1, \"Multiple images match \" + str(wildcard)\n  assert len(images) > 0, \"No images match \" + str(wildcard)\n  return images[0]", "response": "Returns unique ec2. Image whose name matches wildcard\n \u2013 Returns None if no matching image is found"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lookup_instance(name: str, instance_type: str = '', image_name: str = '',\n                    states: tuple = ('running', 'stopped', 'initializing')):\n  \"\"\"Looks up AWS instance for given instance name, like\n   simple.worker. If no instance found in current AWS environment, returns None. \"\"\"\n\n  ec2 = get_ec2_resource()\n\n  instances = ec2.instances.filter(\n    Filters=[{'Name': 'instance-state-name', 'Values': states}])\n\n  prefix = get_prefix()\n  username = get_username()\n\n  # look for an existing instance matching job, ignore instances launched\n  # by different user or under different resource name\n  result = []\n  for i in instances.all():\n    instance_name = get_name(i)\n    if instance_name != name:\n      continue\n\n    seen_prefix, seen_username = parse_key_name(i.key_name)\n    if prefix != seen_prefix:\n      print(f\"Found {name} launched under {seen_prefix}, ignoring\")\n      continue\n    if username != seen_username:\n      print(f\"Found {name} launched by {seen_username}, ignoring\")\n      continue\n\n    if instance_type:\n      assert i.instance_type == instance_type, f\"Found existing instance for job {name} but different instance type ({i.instance_type}) than requested ({instance_type}), terminate {name} first or use new task name.\"\n\n    if image_name:\n      assert i.image.name == image_name, f\"Found existing instance for job {name} but launched with different image ({i.image.name}) than requested ({image_name}), terminate {name} first or use new task name.\"\n    result.append(i)\n\n    assert len(result) < 2, f\"Found two instances with name {name}\"\n    if not result:\n      return None\n    else:\n      return result[0]", "response": "Look up an existing instance for given instance name and instance type and image name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ssh_to_task(task) -> paramiko.SSHClient:\n\n  username = task.ssh_username\n  hostname = task.public_ip\n  ssh_key_fn = get_keypair_fn()\n  print(f\"ssh -i {ssh_key_fn} {username}@{hostname}\")\n  pkey = paramiko.RSAKey.from_private_key_file(ssh_key_fn)\n\n  ssh_client = paramiko.SSHClient()\n  ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n  assert ssh_client\n\n  counter = 1\n  while True:\n    try:\n      ssh_client.connect(hostname=hostname, username=username, pkey=pkey)\n      if counter % 11 == 0:  # occasionally re-obtain public ip, machine could've gotten restarted\n        hostname = task.public_ip\n      break\n    except Exception as e:\n      print(\n        f'{task.name}: Exception connecting to {hostname} via ssh (could be a timeout): {e}')\n      time.sleep(RETRY_INTERVAL_SEC)\n\n  return ssh_client", "response": "Create ssh connection to task s machine\n\n eturn paramiko. SSHClient connected to host."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_aws_name(name):\n  assert len(name) <= 127\n  # disallow unicode characters to avoid pain\n  assert name == name.encode('ascii').decode('ascii')\n  assert aws_name_regexp.match(name)", "response": "Validate resource name using AWS name restrictions from # http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete an EFS by its ID.", "response": "def delete_efs_by_id(efs_id):\n  \"\"\"Deletion sometimes fails, try several times.\"\"\"\n  start_time = time.time()\n  efs_client = get_efs_client()\n  sys.stdout.write(\"deleting %s ... \" % (efs_id,))\n  while True:\n    try:\n      response = efs_client.delete_file_system(FileSystemId=efs_id)\n      if is_good_response(response):\n        print(\"succeeded\")\n        break\n      time.sleep(RETRY_INTERVAL_SEC)\n    except Exception as e:\n      print(\"Failed with %s\" % (e,))\n      if time.time() - start_time - RETRY_INTERVAL_SEC < RETRY_TIMEOUT_SEC:\n        print(\"Retrying in %s sec\" % (RETRY_INTERVAL_SEC,))\n        time.sleep(RETRY_INTERVAL_SEC)\n      else:\n        print(\"Giving up\")\n        break"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a property of an instance keeps retrying until it gets a non - None value", "response": "def get_instance_property(instance, property_name):\n  \"\"\"Retrieves property of an instance, keeps retrying until getting a non-None\"\"\"\n\n  name = get_name(instance)\n  while True:\n    try:\n      value = getattr(instance, property_name)\n      if value is not None:\n        break\n      print(f\"retrieving {property_name} on {name} produced None, retrying\")\n      time.sleep(RETRY_INTERVAL_SEC)\n      instance.reload()\n      continue\n    except Exception as e:\n      print(f\"retrieving {property_name} on {name} failed with {e}, retrying\")\n      time.sleep(RETRY_INTERVAL_SEC)\n      try:\n        instance.reload()\n      except Exception:\n        pass\n      continue\n\n  return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_name(tags_or_instance_or_id):\n\n  ec2 = get_ec2_resource()\n  if hasattr(tags_or_instance_or_id, 'tags'):\n    tags = tags_or_instance_or_id.tags\n  elif isinstance(tags_or_instance_or_id, str):\n    tags = ec2.Instance(tags_or_instance_or_id).tags\n  elif tags_or_instance_or_id is None:\n    return EMPTY_NAME\n  else:\n    assert isinstance(tags_or_instance_or_id,\n                      Iterable), \"expected iterable of tags\"\n    tags = tags_or_instance_or_id\n\n  if not tags:\n    return EMPTY_NAME\n  names = [entry['Value'] for entry in tags if entry['Key'] == 'Name']\n  if not names:\n    return ''\n  if len(names) > 1:\n    assert False, \"have more than one name: \" + str(names)\n  return names[0]", "response": "Helper utility to extract name out of tags dictionary or instance id."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwaits until the resource is available", "response": "def wait_until_available(resource):\n  \"\"\"Waits until interval state becomes 'available'\"\"\"\n  while True:\n    resource.load()\n    if resource.state == 'available':\n      break\n    time.sleep(RETRY_INTERVAL_SEC)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef maybe_create_placement_group(name='', max_retries=10):\n\n  if not name:\n    return\n\n  client = get_ec2_client()\n  while True:\n    try:\n      client.describe_placement_groups(GroupNames=[name])\n      print(\"Reusing placement_group group: \" + name)\n      break  # no Exception means group name was found\n    except Exception:\n      print(\"Creating placement_group group: \" + name)\n      try:\n        _response = client.create_placement_group(GroupName=name,\n                                                  Strategy='cluster')\n      except Exception:\n        # because of race can get InvalidPlacementGroup.Duplicate\n        pass\n\n  counter = 0\n  while True:\n    try:\n      res = client.describe_placement_groups(GroupNames=[name])\n      res_entry = res['PlacementGroups'][0]\n      if res_entry['State'] == 'available':\n        assert res_entry['Strategy'] == 'cluster'\n        break\n    except Exception as e:\n      print(\"Got exception: %s\" % (e,))\n    counter += 1\n    if counter >= max_retries:\n      assert False, f'Failed to create placement_group group {name} in {max_retries} attempts'\n    time.sleep(RETRY_INTERVAL_SEC)", "response": "Create placement_group group or reuses existing one."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of ec2. Instance objects whose name contains fragment.", "response": "def lookup_instances(fragment, verbose=True, filter_by_key=True):\n  \"\"\"Returns ec2.Instance object whose name contains fragment, in reverse order of launching (ie,\n  most recent intance first). Optionally filters by key, only including instances launched with\n  key_name matching current username.\n\n  args:\n    verbose: print information about all matching instances found\n\n    filter_by_key  if True, ignore instances that are not launched with current\n        user's default key\n  \"\"\"\n\n  def vprint(*args):\n    if verbose:\n      print(*args)\n\n  region = get_region()\n  client = get_ec2_client()\n  ec2 = get_ec2_resource()\n  response = client.describe_instances()\n  assert is_good_response(response)\n\n  instance_list = []\n  for instance in ec2.instances.all():\n    if instance.state['Name'] != 'running':\n      continue\n\n    name = get_name(instance)\n    if (fragment in name or fragment in str(instance.public_ip_address) or\n            fragment in str(instance.id) or fragment in str(instance.private_ip_address)):\n      instance_list.append((util.toseconds(instance.launch_time), instance))\n\n  sorted_instance_list = reversed(sorted(instance_list, key=itemgetter(0)))\n  filtered_instance_list = []  # filter by key\n  vprint(\"Using region \", region)\n  for (ts, instance) in sorted_instance_list:\n    if filter_by_key and instance.key_name != get_keypair_name():\n      vprint(f\"Got key {instance.key_name}, expected {get_keypair_name()}\")\n      continue\n    filtered_instance_list.append(instance)\n  return filtered_instance_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates spot instances from the EC2 instance service.", "response": "def create_spot_instances(launch_specs, spot_price=26, expiration_mins=15):\n    \"\"\"\n    args:\n      spot_price: default is $26 which is right above p3.16xlarge on demand price\n      expiration_mins: this request only valid for this many mins from now\n    \"\"\"\n    ec2c = get_ec2_client()\n\n    num_tasks = launch_specs['MinCount'] or 1\n    if 'MinCount' in launch_specs: del launch_specs['MinCount']\n    if 'MaxCount' in launch_specs: del launch_specs['MaxCount']\n    if 'TagSpecifications' in launch_specs: \n      try: tags = launch_specs['TagSpecifications'][0]['Tags']\n      except: pass\n      del launch_specs['TagSpecifications']\n\n    import pytz      # datetime is not timezone aware, use pytz to fix\n    import datetime as dt\n    now = dt.datetime.utcnow().replace(tzinfo=pytz.utc)\n\n    spot_args = {}\n    spot_args['LaunchSpecification'] = launch_specs\n    spot_args['SpotPrice'] = str(spot_price)\n    spot_args['InstanceCount'] = num_tasks\n    spot_args['ValidUntil'] = now + dt.timedelta(minutes=expiration_mins)\n    \n    try:\n      spot_requests = ec2c.request_spot_instances(**spot_args)\n    except Exception as e:\n      assert False, f\"Spot instance request failed (out of capacity?), error was {e}\"\n      \n    spot_requests = spot_requests['SpotInstanceRequests']\n    instance_ids = wait_on_fulfillment(ec2c, spot_requests)\n    \n    print('Instances fullfilled...')\n    ec2 = get_ec2_resource()\n    instances = list(ec2.instances.filter(Filters=[{'Name': 'instance-id', 'Values': list(filter(None, instance_ids))}]))\n\n    if not all(instance_ids):\n      for i in instances: \n        i.terminate()\n      raise RuntimeError('Failed to create spot instances:', instance_ids)\n\n    if tags:\n      for i in instances:\n          i.create_tags(Tags=tags)\n\n    return instances"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if task is chief task in the corresponding run", "response": "def is_chief(task: backend.Task, run_name: str):\n  \"\"\"Returns True if task is chief task in the corresponding run\"\"\"\n  global run_task_dict\n  if run_name not in run_task_dict:\n    return True\n  task_list = run_task_dict[run_name]\n  assert task in task_list, f\"Task {task.name} doesn't belong to run {run_name}\"\n  return task_list[0] == task"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ossystem(cmd):\n  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,\n                       stderr=subprocess.STDOUT)\n  (stdout, stderr) = p.communicate()\n  return stdout.decode('ascii')", "response": "Like os. system but returns output of command as string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new Task object on AWS.", "response": "def make_task(\n        name: str = '',\n        run_name: str = '',\n        install_script: str = '',\n        instance_type: str = '',\n        image_name: str = '',\n        disk_size: int = 0,\n        preemptible=None,\n        logging_task: backend.Task = None,\n        create_resources=True,\n        spot=False\n) -> Task:\n  \"\"\"\n  Create task on AWS.\n\n  Automatically places it in singleton Run/singleton Job objects, see Run/Job/Task hierarchy for details\n  https://docs.google.com/document/d/1Gg4T243cYrDUW1YDCikmqp7fzSQDU3rZxOkJr9ohhs8/edit#heading=h.j4td4oixogib\n\n\n  Args:\n    disk_size: default size of root disk, in GBs\n    create_resources: whether this task will handle resource creation\n    name: see ncluster.make_task\n    run_name: see ncluster.make_task\n    install_script: see ncluster.make_task\n    instance_type: instance type to use, defaults to $NCLUSTER_INSTANCE or t3.micro if unset\n    image_name: name of image, ie, \"Deep Learning AMI (Ubuntu) Version 12.0\", defaults to $NCLUSTER_IMAGE or amzn2-ami-hvm-2.0.20180622.1-x86_64-gp2 if unset\n    preemptible: use cheaper preemptible/spot instances\n    logging_task: partially initialized Task object, use it for logging\n\n  Returns:\n\n  \"\"\"\n\n  ncluster_globals.task_launched = True\n\n  def log(*_args):\n    if logging_task:\n      logging_task.log(*_args)\n    else:\n      util.log(*_args)\n\n  # if name not specified, use name which is the same across script invocations for given image/instance-type\n  name = ncluster_globals.auto_assign_task_name_if_needed(name, instance_type,\n                                                          image_name)\n\n  if not instance_type:\n    instance_type = os.environ.get('NCLUSTER_INSTANCE', 't3.micro')\n    log(\"Using instance \" + instance_type)\n\n  _set_aws_environment()\n  if create_resources:\n    _maybe_create_resources(logging_task=logging_task)\n  else:\n    pass\n\n  run: Run = ncluster_globals.get_run_object(run_name)\n  placement_group = ''\n  if u.instance_supports_placement_groups(instance_type) and run:\n    placement_group = run.placement_group\n    log(f\"Launching into placement_group group {placement_group}\")\n    u.maybe_create_placement_group(run.placement_group)\n\n  if not image_name:\n    image_name = os.environ.get('NCLUSTER_IMAGE',\n                                GENERIC_SMALL_IMAGE)\n  log(\"Using image \" + image_name)\n\n  if preemptible is None:\n    preemptible = os.environ.get('NCLUSTER_PREEMPTIBLE', False)\n    preemptible = bool(preemptible)\n    if preemptible:\n      log(\"Using preemptible instances\")\n\n  image = u.lookup_image(image_name)\n  keypair = u.get_keypair()\n  security_group = u.get_security_group()\n  ec2 = u.get_ec2_resource()\n\n  instance = u.lookup_instance(name, instance_type,\n                               image_name)\n  _maybe_start_instance(instance)\n  _maybe_wait_for_initializing_instance(instance)\n\n  # create the instance if not present\n  if instance:\n    log(f\"Reusing {instance}\")\n  else:\n    log(f\"Allocating {instance_type} for task {name}\")\n    args = {'ImageId': image.id,\n            'InstanceType': instance_type,\n            'MinCount': 1,\n            'MaxCount': 1,\n            'SecurityGroupIds': [security_group.id],\n            'KeyName': keypair.name}\n\n    args['TagSpecifications'] = [{\n      'ResourceType': 'instance',\n      'Tags': [{\n        'Key': 'Name',\n        'Value': name\n      }]\n    }]\n\n    #    subnet = u.get_subnet()\n    #    args['NetworkInterfaces'] = [{'SubnetId': subnet.id,\n    #                                  'DeviceIndex': 0,\n    #                                  'AssociatePublicIpAddress': True,\n    #                                  'Groups': [security_group.id]}]\n    #    placement_specs = {'AvailabilityZone': u.get_zone()}\n\n    placement_specs = {}\n    if placement_group:\n      placement_specs['GroupName'] = placement_group\n\n    args['Placement'] = placement_specs\n    args['Monitoring'] = {'Enabled': True}\n\n    if disk_size:\n      assert disk_size > 0\n      ebs = {\n        'VolumeSize': disk_size,\n        'VolumeType': 'gp2',\n      }\n\n      args['BlockDeviceMappings'] = [{\n        'DeviceName': '/dev/sda1',\n        'Ebs': ebs\n      }]\n\n    # Use high throughput disk (0.065/iops-month = about $1/hour)\n    if 'NCLUSTER_AWS_FAST_ROOTDISK' in os.environ:\n      assert not disk_size, f\"Specified both disk_size {disk_size} and $NCLUSTER_AWS_FAST_ROOTDISK, they are incompatible as $NCLUSTER_AWS_FAST_ROOTDISK hardwired disk size\"\n\n      ebs = {\n        'VolumeSize': 500,\n        'VolumeType': 'io1',\n        'Iops': 11500\n      }\n\n      args['BlockDeviceMappings'] = [{\n        'DeviceName': '/dev/sda1',\n        'Ebs': ebs\n      }]\n\n    instances = []\n    try:\n      if spot:\n        instances = u.create_spot_instances(args)\n      else:\n        instances = ec2.create_instances(**args)\n    except Exception as e:\n      log(f\"Instance creation for {name} failed with ({e})\")\n      log(\n        \"You can change availability zone using export NCLUSTER_ZONE=...\")\n      log(\"Terminating\")\n      os.kill(os.getpid(),\n              signal.SIGINT)  # sys.exit() doesn't work inside thread\n\n    assert instances, f\"ec2.create_instances returned {instances}\"\n    log(f\"Allocated {len(instances)} instances\")\n    instance = instances[0]\n\n  task = Task(name, instance=instance,\n              install_script=install_script,\n              image_name=image_name,\n              instance_type=instance_type)\n\n  ncluster_globals.register_task(task, run_name)\n  return task"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_job(\n        name: str = '',\n        run_name: str = '',\n        num_tasks: int = 1,\n        install_script: str = '',\n        instance_type: str = '',\n        image_name: str = '',\n        create_resources=True,\n        **kwargs) -> Job:\n  \"\"\"\n  Args:\n    create_resources: if True, will create resources if necessary\n    name: see backend.make_task\n    run_name: see backend.make_task\n    num_tasks: number of tasks to launch\n    install_script: see make_task\n    instance_type: see make_task\n    image_name: see make_task\n\n  Returns:\n\n  \"\"\"\n  assert num_tasks > 0, f\"Can't create job with {num_tasks} tasks\"\n  assert name.count(\n    '.') <= 1, \"Job name has too many .'s (see ncluster design: Run/Job/Task hierarchy for  convention)\"\n\n  # dummy tasks for logging\n  tasks = [backend.Task(f\"{i}.{name}\") for i in range(num_tasks)]\n\n  _set_aws_environment(tasks[0])\n  if create_resources:\n    _maybe_create_resources(tasks[0])\n\n  name = ncluster_globals.auto_assign_job_name_if_needed(name)\n  run_name = ncluster_globals.auto_assign_run_name_if_needed(run_name)\n  _run = ncluster_globals.create_run_if_needed(run_name, make_run)\n\n  job = Job(name=name, tasks=tasks, run_name=run_name, **kwargs)\n\n  exceptions = []\n\n  # make tasks in parallel\n  def make_task_fn(i: int):\n    try:\n      tasks[i] = make_task(f\"{i}.{name}\", run_name=run_name,\n                           install_script=install_script,\n                           instance_type=instance_type, image_name=image_name,\n                           logging_task=tasks[i],\n                           create_resources=False,\n                           # handle resources in job already\n                           **kwargs)\n    except Exception as e:\n      exceptions.append(e)\n\n  util.log(\"Creating threads\")\n  threads = [threading.Thread(name=f'make_task_{i}',\n                              target=make_task_fn, args=[i])\n             for i in range(num_tasks)]\n  for thread in threads:\n    thread.start()\n  for thread in threads:\n    thread.join()\n  print(\"Exception are \", exceptions)\n  if exceptions:\n    raise exceptions[0]\n\n  job.tasks = tasks\n\n  # double check that all instances are in the same placement_group group\n  # this can happen if some instances from previous smaller run are getting reused\n  placement_dict = {task.instance.placement_group: task.name for task in\n                    job.tasks}\n  # TODO: make placement_group group name derived from run, to make it deterministic\n  # on individual instance restarts\n  if len(placement_dict) > 1:\n    util.log(\"Job tasks are spread over multiple placement_group groups\")\n    pprint.pprint(placement_dict)\n    raise RuntimeError(\n      f\"Got instance spread over multiple placement_group groups: {placement_dict}. Must terminate all instances in run {run_name} and try again.\")\n  return job", "response": "Create a new job with the given name run_name num_tasks number of tasks in parallel."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _maybe_start_instance(instance):\n\n  if not instance:\n    return\n\n  if instance.state['Name'] == 'stopped':\n    instance.start()\n    while True:\n      print(f\"Waiting  for {instance} to start.\")\n      instance.reload()\n      if instance.state['Name'] == 'running':\n        break\n      time.sleep(10)", "response": "Starts instance if it s stopped no - op otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting instance if it s stopped no - op otherwise.", "response": "def _maybe_wait_for_initializing_instance(instance):\n  \"\"\"Starts instance if it's stopped, no-op otherwise.\"\"\"\n\n  if not instance:\n    return\n\n  if instance.state['Name'] == 'initializing':\n    while True:\n      print(f\"Waiting  for {instance} to leave state 'initializing'.\")\n      instance.reload()\n      if instance.state['Name'] == 'running':\n        break\n      time.sleep(10)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if resources exist and create them if not.", "response": "def _maybe_create_resources(logging_task: Task = None):\n  \"\"\"Use heuristics to decide to possibly create resources\"\"\"\n\n  def log(*args):\n    if logging_task:\n      logging_task.log(*args)\n    else:\n      util.log(*args)\n\n  def should_create_resources():\n    \"\"\"Check if gateway, keypair, vpc exist.\"\"\"\n    prefix = u.get_prefix()\n    if u.get_keypair_name() not in u.get_keypair_dict():\n      log(f\"Missing {u.get_keypair_name()} keypair, creating resources\")\n      return True\n    vpcs = u.get_vpc_dict()\n    if prefix not in vpcs:\n      log(f\"Missing {prefix} vpc, creating resources\")\n      return True\n    vpc = vpcs[prefix]\n    gateways = u.get_gateway_dict(vpc)\n    if prefix not in gateways:\n      log(f\"Missing {prefix} gateway, creating resources\")\n      return True\n    return False\n\n  try:\n    # this locking is approximate, still possible for threads to slip through\n    if os.path.exists(AWS_LOCK_FN):\n      pid, ts, lock_taskname = open(AWS_LOCK_FN).read().split('-')\n      ts = int(ts)\n      log(f\"waiting for aws resource creation, another resource initiation was \"\n          f\"initiated {int(time.time()-ts)} seconds ago by \"\n          f\"{lock_taskname}, delete lock file \"\n          f\"{AWS_LOCK_FN} if this is an error\")\n      while True:\n        if os.path.exists(AWS_LOCK_FN):\n          log(f\"waiting for lock file {AWS_LOCK_FN} to get deleted \"\n              f\"initiated {int(time.time()-ts)} seconds ago by \")\n          time.sleep(2)\n          continue\n        else:\n          break\n      return\n\n    with open(AWS_LOCK_FN, 'w') as f:\n      f.write(\n        f'{os.getpid()}-{int(time.time())}-{logging_task.name if logging_task else \"\"}')\n\n    if not should_create_resources():\n      util.log(\"Resources already created, no-op\")\n      os.remove(AWS_LOCK_FN)\n      return\n\n    create_lib.create_resources()\n  finally:\n    if os.path.exists(AWS_LOCK_FN):\n      os.remove(AWS_LOCK_FN)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_aws_environment(task: Task = None):\n  current_zone = os.environ.get('NCLUSTER_ZONE', '')\n  current_region = os.environ.get('AWS_DEFAULT_REGION', '')\n\n  def log(*args):\n    if task:\n      task.log(*args)\n    else:\n      util.log(*args)\n\n  if current_region and current_zone:\n    assert current_zone.startswith(\n      current_region), f'Current zone \"{current_zone}\" ($NCLUSTER_ZONE) is not ' \\\n                       f'in current region \"{current_region} ($AWS_DEFAULT_REGION)'\n    assert u.get_session().region_name == current_region  # setting from ~/.aws\n\n  # zone is set, set region from zone\n  if current_zone and not current_region:\n    current_region = current_zone[:-1]\n    os.environ['AWS_DEFAULT_REGION'] = current_region\n\n  # neither zone nor region not set, use default setting for region\n  # if default is not set, use NCLUSTER_DEFAULT_REGION\n  if not current_region:\n    current_region = u.get_session().region_name\n    if not current_region:\n      log(f\"No default region available, using {NCLUSTER_DEFAULT_REGION}\")\n      current_region = NCLUSTER_DEFAULT_REGION\n    os.environ['AWS_DEFAULT_REGION'] = current_region\n\n  # zone not set, use first zone of the region\n  #  if not current_zone:\n  #    current_zone = current_region + 'a'\n  #    os.environ['NCLUSTER_ZONE'] = current_zone\n\n  log(f\"Using account {u.get_account_number()}, region {current_region}, \"\n      f\"zone {current_zone}\")", "response": "Sets up AWS environment from NCLUSTER environment variables"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef join(self, ignore_errors=False):\n    assert self._status_fn, \"Asked to join a task which hasn't had any commands executed on it\"\n    check_interval = 0.2\n    status_fn = self._status_fn\n    if not self.wait_for_file(status_fn, max_wait_sec=30):\n      self.log(f\"Retrying waiting for {status_fn}\")\n    while not self.exists(status_fn):\n      self.log(f\"Still waiting for {self._cmd}\")\n      self.wait_for_file(status_fn, max_wait_sec=30)\n    contents = self.read(status_fn)\n\n    # if empty wait a bit to allow for race condition\n    if len(contents) == 0:\n      time.sleep(check_interval)\n      contents = self.read(status_fn)\n    status = int(contents.strip())\n    self.last_status = status\n\n    if status != 0:\n      extra_msg = '(ignoring error)' if ignore_errors else '(failing)'\n      if util.is_set('NCLUSTER_RUN_WITH_OUTPUT_ON_FAILURE') or True:\n        self.log(\n          f\"Start failing output {extra_msg}: \\n{'*'*80}\\n\\n '{self.read(self._out_fn)}'\")\n        self.log(f\"\\n{'*'*80}\\nEnd failing output\")\n      if not ignore_errors:\n        raise RuntimeError(f\"Command {self._cmd} returned status {status}\")\n      else:\n        self.log(f\"Warning: command {self._cmd} returned status {status}\")\n\n    return status", "response": "Waits until the last executed command completed on the task."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _run_with_output_on_failure(self, cmd, non_blocking=False,\n                                  ignore_errors=False,\n                                  max_wait_sec=365 * 24 * 3600,\n                                  check_interval=0.2) -> str:\n    \"\"\"Experimental version of run propagates error messages to client. This command will be default \"run\" eventually\"\"\"\n\n    if not self._can_run:\n      assert False, \"Using .run before initialization finished\"\n\n    if '\\n' in cmd:\n      assert False, \"Don't support multi-line for run2\"\n\n    cmd = cmd.strip()\n    if cmd.startswith('#'):  # ignore empty/commented out lines\n      return ''\n    self.run_counter += 1\n    self.log(\"tmux> %s\", cmd)\n\n    self._cmd = cmd\n    self._cmd_fn = f'{self.remote_scratch}/{self.run_counter}.cmd'\n    self._status_fn = f'{self.remote_scratch}/{self.run_counter}.status'\n    self._out_fn = f'{self.remote_scratch}/{self.run_counter}.out'\n\n    cmd = util.shell_strip_comment(cmd)\n    assert '&' not in cmd, f\"cmd {cmd} contains &, that breaks things\"\n\n    # modify command to dump shell success status into file\n    self.file_write(self._cmd_fn, cmd + '\\n')\n\n    #    modified_cmd = f'{cmd} > {out_fn} 2>&1; echo $? > {status_fn}'\n    # https://stackoverflow.com/a/692407/419116\n    # $cmd > >(tee -a fn) 2> >(tee -a fn >&2)\n\n    modified_cmd = f'{cmd} > >(tee -a {self._out_fn}) 2> >(tee -a {self._out_fn} >&2); echo $? > {self._status_fn}'\n    modified_cmd = shlex.quote(modified_cmd)\n\n    start_time = time.time()\n    tmux_window = self.tmux_session + ':' + str(self.tmux_window_id)\n    tmux_cmd = f\"tmux send-keys -t {tmux_window} {modified_cmd} Enter\"\n    self._run_raw(tmux_cmd, ignore_errors=ignore_errors)\n    if non_blocking:\n      return 0\n\n    if not self.wait_for_file(self._status_fn, max_wait_sec=60):\n      self.log(f\"Retrying waiting for {self._status_fn}\")\n    elapsed_time = time.time() - start_time\n    while not self.exists(self._status_fn) and elapsed_time < max_wait_sec:\n      self.log(f\"Still waiting for {cmd}\")\n      self.wait_for_file(self._status_fn, max_wait_sec=60)\n      elapsed_time = time.time() - start_time\n    contents = self.read(self._status_fn)\n\n    # if empty wait a bit to allow for race condition\n    if len(contents) == 0:\n      time.sleep(check_interval)\n      contents = self.read(self._status_fn)\n    status = int(contents.strip())\n    self.last_status = status\n\n    if status != 0:\n      extra_msg = '(ignoring error)' if ignore_errors else '(failing)'\n      self.log(\n        f\"Start failing output {extra_msg}: \\n{'*'*80}\\n\\n '{self.read(self._out_fn)}'\")\n      self.log(f\"\\n{'*'*80}\\nEnd failing output\")\n      if not ignore_errors:\n        raise RuntimeError(f\"Command {cmd} returned status {status}\")\n      else:\n        self.log(f\"Warning: command {cmd} returned status {status}\")\n\n    return self.read(self._out_fn)", "response": "Runs the given command and returns the output of the command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run_raw(self, cmd: str, ignore_errors=False) -> Tuple[str, str]:\n    #    self._log(\"run_ssh: %s\"%(cmd,))\n\n    stdin, stdout, stderr = u.call_with_retries(self.ssh_client.exec_command,\n                                                command=cmd, get_pty=True)\n    stdout_str = stdout.read().decode()\n    stderr_str = stderr.read().decode()\n    if stdout.channel.recv_exit_status() != 0:\n      if not ignore_errors:\n        self.log(f\"command ({cmd}) failed with --->\")\n        self.log(\"failing stdout: \" + stdout_str)\n        self.log(\"failing stderr: \" + stderr_str)\n        assert False, \"_run_raw failed (see logs for error)\"\n\n    return stdout_str, stderr_str", "response": "Runs given cmd in the task using current SSH session returns stdout and stderr as strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef upload(self, local_fn: str, remote_fn: str = '',\n             dont_overwrite: bool = False) -> None:\n    \"\"\"Uploads file to remote instance. If location not specified, dumps it\n    into default directory. If remote location has files or directories with the\n     same name, behavior is undefined.\"\"\"\n\n    # support wildcard through glob\n    if '*' in local_fn:\n      for local_subfn in glob.glob(local_fn):\n        self.upload(local_subfn)\n      return\n\n    if '#' in local_fn:  # hashes also give problems from shell commands\n      self.log(\"skipping backup file {local_fn}\")\n      return\n\n    if not self.sftp:\n      self.sftp = u.call_with_retries(self.ssh_client.open_sftp,\n                                      'self.ssh_client.open_sftp')\n\n    def maybe_fix_mode(local_fn_, remote_fn_):\n      \"\"\"Makes remote file execute for locally executable files\"\"\"\n      mode = oct(os.stat(local_fn_)[stat.ST_MODE])[-3:]\n      if '7' in mode:\n        self.log(f\"Making {remote_fn_} executable with mode {mode}\")\n        # use raw run, in case tmux is unavailable\n        self._run_raw(f\"chmod {mode} {remote_fn_}\")\n\n    # augmented SFTP client that can transfer directories, from\n    # https://stackoverflow.com/a/19974994/419116\n    def _put_dir(source, target):\n      \"\"\" Uploads the contents of the source directory to the target path.\"\"\"\n\n      def _safe_mkdir(path, mode=511, ignore_existing=True):\n        \"\"\" Augments mkdir by adding an option to not fail if the folder exists  asdf asdf asdf as\"\"\"\n        try:\n          self.sftp.mkdir(path, mode)\n        except IOError:\n          if ignore_existing:\n            pass\n          else:\n            raise\n\n      assert os.path.isdir(source)\n      _safe_mkdir(target)\n\n      for item in os.listdir(source):\n        if os.path.isfile(os.path.join(source, item)):\n          self.sftp.put(os.path.join(source, item), os.path.join(target, item))\n          maybe_fix_mode(os.path.join(source, item), os.path.join(target, item))\n        else:\n          _safe_mkdir(f'{target}/{item}')\n          _put_dir(f'{source}/{item}', f'{target}/{item}')\n\n    if not remote_fn:\n      remote_fn = os.path.basename(local_fn)\n\n    self.log('uploading ' + local_fn + ' to ' + remote_fn)\n    remote_fn = remote_fn.replace('~', self.homedir)\n\n    if '/' in remote_fn:\n      remote_dir = os.path.dirname(remote_fn)\n      assert self.exists(\n        remote_dir), f\"Remote dir {remote_dir} doesn't exist\"\n    if dont_overwrite and self.exists(remote_fn):\n      self.log(\"Remote file %s exists, skipping\" % (remote_fn,))\n      return\n\n    assert os.path.exists(local_fn), f\"{local_fn} not found\"\n    if os.path.isdir(local_fn):\n      _put_dir(local_fn, remote_fn)\n    else:\n      assert os.path.isfile(local_fn), \"%s is not a file\" % (local_fn,)\n      # this crashes with IOError when upload failed\n      if self.exists(remote_fn) and self.isdir(remote_fn):\n        remote_fn = remote_fn + '/' + os.path.basename(local_fn)\n      self.sftp.put(localpath=local_fn, remotepath=remote_fn)\n      maybe_fix_mode(local_fn, remote_fn)", "response": "Uploads a file to the remote instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nswitch currently active tmux window for given task. 0 is the default window. 1 is the current tmux window 2 is the current tmux window", "response": "def switch_window(self, window_id: int):\n    \"\"\"\n    Switches currently active tmux window for given task. 0 is the default window\n    Args:\n      window_id: integer id of tmux window to use\n    \"\"\"\n\n    # windows are numbered sequentially 0, 1, 2, ...\n    # create any missing windows and make them point to the same directory\n    if window_id not in self.tmux_available_window_ids:\n      for i in range(max(self.tmux_available_window_ids) + 1, window_id + 1):\n        self._run_raw(f'tmux new-window -t {self.tmux_session} -d')\n        self.tmux_available_window_ids.append(i)\n\n    self.tmux_window_id = window_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplacing lines starting with starts_with in fn with new_line.", "response": "def _replace_lines(fn, startswith, new_line):\n  \"\"\"Replace lines starting with starts_with in fn with new_line.\"\"\"\n  new_lines = []\n  for line in open(fn):\n    if line.startswith(startswith):\n      new_lines.append(new_line)\n    else:\n      new_lines.append(line)\n  with open(fn, 'w') as f:\n    f.write('\\n'.join(new_lines))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning current micros since epoch as integer.", "response": "def now_micros(absolute=False) -> int:\n  \"\"\"Return current micros since epoch as integer.\"\"\"\n  micros = int(time.time() * 1e6)\n  if absolute:\n    return micros\n  return micros - EPOCH_MICROS"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef now_millis(absolute=False) -> int:\n  millis = int(time.time() * 1e3)\n  if absolute:\n    return millis\n  return millis - EPOCH_MICROS // 1000", "response": "Return current millis since epoch as integer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef install_pdb_handler():\n\n  import signal\n  import pdb\n\n  def handler(_signum, _frame):\n    pdb.set_trace()\n\n  signal.signal(signal.SIGQUIT, handler)", "response": "Installs a handler for CTRL + break into gdb."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd echo cmd in front of each cmd.", "response": "def shell_add_echo(script):\n  \"\"\"Goes over each line script, adds \"echo cmd\" in front of each cmd.\n\n  ls a\n\n  becomes\n\n  echo * ls a\n  ls a\n  \"\"\"\n  new_script = \"\"\n  for cmd in script.split('\\n'):\n    cmd = cmd.strip()\n    if not cmd:\n      continue\n    new_script += \"echo \\\\* \" + shlex.quote(cmd) + \"\\n\"\n    new_script += cmd + \"\\n\"\n  return new_script"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef alphanumeric_hash(s: str, size=5):\n  import hashlib\n  import base64\n  hash_object = hashlib.md5(s.encode('ascii'))\n  s = base64.b32encode(hash_object.digest())\n  result = s[:size].decode('ascii').lower()\n  return result", "response": "Short alphanumeric string derived from hash of given string"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreverses the name of a task in the alphabetical order.", "response": "def reverse_taskname(name: str) -> str:\n  \"\"\"\n  Reverses components in the name of task. Reversed convention is used for filenames since\n  it groups log/scratch files of related tasks together\n\n  0.somejob.somerun -> somerun.somejob.0\n  0.somejob -> somejob.0\n  somename -> somename\n\n  Args:\n    name: name of task\n\n  \"\"\"\n  components = name.split('.')\n  assert len(components) <= 3\n  return '.'.join(components[::-1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_bash_builtin(cmd):\n  # from compgen -b\n  bash_builtins = ['alias', 'bg', 'bind', 'alias', 'bg', 'bind', 'break',\n                   'builtin', 'caller', 'cd', 'command', 'compgen', 'complete',\n                   'compopt', 'continue', 'declare', 'dirs', 'disown', 'echo',\n                   'enable', 'eval', 'exec', 'exit', 'export', 'false', 'fc',\n                   'fg', 'getopts', 'hash', 'help', 'history', 'jobs', 'kill',\n                   'let', 'local', 'logout', 'mapfile', 'popd', 'printf',\n                   'pushd', 'pwd', 'read', 'readarray', 'readonly', 'return',\n                   'set', 'shift', 'shopt', 'source', 'suspend', 'test',\n                   'times', 'trap', 'true', 'type', 'typeset', 'ulimit',\n                   'umask', 'unalias', 'unset', 'wait']\n  toks = cmd.split()\n  if toks and toks[0] in bash_builtins:\n    return True\n  return False", "response": "Return true if command is invoking bash built - in - in - alphabet"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nasserting fail if current directory of the script is different from location of the script and run again.", "response": "def assert_script_in_current_directory():\n  \"\"\"Assert fail if current directory is different from location of the script\"\"\"\n\n  script = sys.argv[0]\n  assert os.path.abspath(os.path.dirname(script)) == os.path.abspath(\n    '.'), f\"Change into directory of script {script} and run again.\""}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef push_ctx(app=None):\n    if app is not None:\n        ctx = app.test_request_context()\n        ctx.fixtures_request_context = True\n        ctx.push()\n        if _app_ctx_stack is not None:\n            _app_ctx_stack.top.fixtures_app_context = True\n\n    # Make sure that we have an application in the current context\n    if (_app_ctx_stack is None or _app_ctx_stack.top is None) and _request_ctx_stack.top is None:\n        raise AssertionError('A Flask application must be specified for Fixtures to work.')", "response": "Pushes a new context on the top of the stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pop_ctx():\n    if getattr(_request_ctx_stack.top, 'fixtures_request_context', False):\n        _request_ctx_stack.pop()\n    if _app_ctx_stack is not None and getattr(_app_ctx_stack.top, 'fixtures_app_context', False):\n        _app_ctx_stack.pop()", "response": "Removes the test context from the current stack"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_fixtures(db, fixtures):\n    conn = db.engine.connect()\n    metadata = db.metadata\n\n    for fixture in fixtures:\n        if 'model' in fixture:\n            module_name, class_name = fixture['model'].rsplit('.', 1)\n            module = importlib.import_module(module_name)\n            model = getattr(module, class_name)\n            for fields in fixture['records']:\n                obj = model(**fields)\n                db.session.add(obj)\n            db.session.commit()\n        elif 'table' in fixture:\n            table = Table(fixture['table'], metadata)\n            conn.execute(table.insert(), fixture['records'])\n        else:\n            raise ValueError(\"Fixture missing a 'model' or 'table' field: {0}\".format(json.dumps(fixture)))", "response": "Loads the given fixtures into the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a function that adds fixtures handling to the setup method.", "response": "def setup_handler(setup_fixtures_fn, setup_fn):\n        \"\"\"Returns a function that adds fixtures handling to the setup method.\n\n        Makes sure that fixtures are setup before calling the given setup method.\n        \"\"\"\n        def handler(obj):\n            setup_fixtures_fn(obj)\n            setup_fn(obj)\n        return handler"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a function that adds fixtures handling to the given teardown method.", "response": "def teardown_handler(teardown_fixtures_fn, teardown_fn):\n        \"\"\"Returns a function that adds fixtures handling to the teardown method.\n\n        Calls the given teardown method first before calling the fixtures teardown.\n        \"\"\"\n        def handler(obj):\n            teardown_fn(obj)\n            teardown_fixtures_fn(obj)\n        return handler"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_child_fn(attrs, names, bases):\n        def call_method(obj, method):\n            \"\"\"Calls a method as either a class method or an instance method.\n            \"\"\"\n            # The __get__ method takes an instance and an owner which changes\n            # depending on the calling object. If the calling object is a class,\n            # the instance is None and the owner will be the object itself. If the\n            # calling object is an instance, the instance will be the calling object\n            # and the owner will be its class. For more info on the __get__ method,\n            # see http://docs.python.org/2/reference/datamodel.html#object.__get__.\n            if isinstance(obj, type):\n                instance = None\n                owner = obj\n            else:\n                instance = obj\n                owner = obj.__class__\n            method.__get__(instance, owner)()\n\n        # Create a default function that calls the default method on each parent\n        default_name = names[0]\n        def default_fn(obj):\n            for cls in bases:\n                if hasattr(cls, default_name):\n                    call_method(obj, getattr(cls, default_name))\n        default_fn.__name__ = default_name\n\n        # Get all of the functions in the child class that match the list of names\n        fns = [(name, attrs[name]) for name in names if name in attrs]\n\n        # Raise an error if more than one setup/teardown method is found\n        if len(fns) > 1:\n            raise RuntimeError(\"Cannot have more than one setup or teardown method per context (class or test).\")\n        # If one setup/teardown function was found, return it\n        elif len(fns) == 1:\n            name, fn = fns[0]\n            def child_fn(obj):\n                call_method(obj, fn)\n            child_fn.__name__ = name\n            return child_fn\n        # Otherwise, return the default function\n        else:\n            return default_fn", "response": "Returns a function that returns the child class that matches one of the names."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting a message to the screen.", "response": "def print_msg(msg, header, file=sys.stdout):\n    \"\"\"Prints a boardered message to the screen\"\"\"\n    DEFAULT_MSG_BLOCK_WIDTH = 60\n\n    # Calculate the length of the boarder on each side of the header and the\n    # total length of the bottom boarder\n    side_boarder_length = (DEFAULT_MSG_BLOCK_WIDTH - (len(header) + 2)) // 2\n    msg_block_width = side_boarder_length * 2 + (len(header) + 2)\n\n    # Create the top and bottom boarders\n    side_boarder = '#' * side_boarder_length\n    top_boarder = '{0} {1} {2}'.format(side_boarder, header, side_boarder)\n    bottom_boarder = '#' * msg_block_width\n\n    def pad(line, length):\n        \"\"\"Returns a string padded and centered by the given length\"\"\"\n        padding_length = length - len(line)\n        left_padding = ' ' * (padding_length//2)\n        right_padding = ' ' * (padding_length - len(left_padding))\n        return '{0} {1} {2}'.format(left_padding, line, right_padding)\n\n    words = msg.split(' ')\n    lines = []\n    line = ''\n    for word in words:\n        if len(line + ' ' + word) <= msg_block_width - 4:\n            line = (line + ' ' + word).strip()\n        else:\n            lines.append('#{0}#'.format(pad(line, msg_block_width - 4)))\n            line = word\n    lines.append('#{0}#'.format(pad(line, msg_block_width - 4)))\n\n    # Print the full message\n    print(file=file)\n    print(top_boarder, file=file)\n    print('#{0}#'.format(pad('', msg_block_width - 4)), file=file)\n    for line in lines:\n        print(line, file=file)\n    print('#{0}#'.format(pad('', msg_block_width - 4)), file=file)\n    print(bottom_boarder, file=file)\n    print(file=file)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef can_persist_fixtures():\n    # If we're running python 2.7 or greater, we're fine\n    if sys.hexversion >= 0x02070000:\n        return True\n\n    # Otherwise, nose and py.test support the setUpClass and tearDownClass\n    # methods, so if we're using either of those, go ahead and run the tests\n    filename = inspect.stack()[-1][1]\n    executable = os.path.split(filename)[1]\n    return executable in ('py.test', 'nosetests')", "response": "Returns True if we can persist fixtures across tests."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all photos from the user or members of the list.", "response": "def get(self, count=None, since_id=None, silent=False):\n        \"\"\"\n        Get all photos from the user or members of the list\n        :param count: Number of tweets to try and retrieve. If None, return\n            all photos since `since_id`\n        :param since_id: An integer specifying the oldest tweet id\n        \"\"\"\n        if not silent:\n            print('Retrieving photos from Twitter API...')\n        self.auth_user = self.verify_credentials().screen_name\n        self.since_ids = read_since_ids(self.users)\n        for user in self.users:\n            if self.increment:\n                since_id = self.since_ids.get(user)\n            photos = self.load(user=user,\n                               count=count,\n                               since_id=since_id,\n                               num=self.num)\n            self.photos[user] = photos[:self.num]\n            self._total += len(self.photos[user])\n            if not photos and user in self.max_ids:\n                del self.max_ids[user]\n        return self.photos"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads max ids of the last downloads", "response": "def read_since_ids(users):\n    \"\"\"\n    Read max ids of the last downloads\n\n    :param users: A list of users\n\n    Return a dictionary mapping users to ids\n    \"\"\"\n    since_ids = {}\n    for user in users:\n        if config.has_option(SECTIONS['INCREMENTS'], user):\n            since_ids[user] = config.getint(SECTIONS['INCREMENTS'], user) + 1\n    return since_ids"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the max ids of the current downloads", "response": "def set_max_ids(max_ids):\n    \"\"\"\n    Set max ids of the current downloads\n\n    :param max_ids: A dictionary mapping users to ids\n    \"\"\"\n    config.read(CONFIG)\n    for user, max_id in max_ids.items():\n        config.set(SECTIONS['INCREMENTS'], user, str(max_id))\n    with open(CONFIG, 'w') as f:\n        config.write(f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhashes a byte array using the same method the registry uses.", "response": "def hash_bytes(buf):\n    \"\"\"\n    Hash bytes using the same method the registry uses (currently SHA-256).\n\n    :param buf: Bytes to hash\n    :type buf: binary str\n\n    :rtype: str\n    :returns: Hex-encoded hash of file's content (prefixed by ``sha256:``)\n    \"\"\"\n    sha256 = hashlib.sha256()\n    sha256.update(buf)\n    return 'sha256:' + sha256.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hash_file(filename):\n    sha256 = hashlib.sha256()\n    with open(filename, 'rb') as f:\n        for chunk in iter(lambda: f.read(8192), b''):\n            sha256.update(chunk)\n    return 'sha256:' + sha256.hexdigest()", "response": "Hash a file using the same method the registry uses."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef authenticate(self,\n                     username=None, password=None,\n                     actions=None, response=None,\n                     authorization=None):\n        # pylint: disable=too-many-arguments,too-many-locals\n        \"\"\"\n        Authenticate to the registry using a username and password,\n        an authorization header or otherwise as the anonymous user.\n\n        :param username: User name to authenticate as.\n        :type username: str\n\n        :param password: User's password.\n        :type password: str\n\n        :param actions: If you know which types of operation you need to make on the registry, specify them here. Valid actions are ``pull``, ``push`` and ``*``.\n        :type actions: list\n\n        :param response: When the ``auth`` function you passed to :class:`DXFBase`'s constructor is called, it is passed a HTTP response object. Pass it back to :meth:`authenticate` to have it automatically detect which actions are required.\n        :type response: requests.Response\n\n        :param authorization: ``Authorization`` header value.\n        :type authorization: str\n\n        :rtype: str\n        :returns: Authentication token, if the registry supports bearer tokens. Otherwise ``None``, and HTTP Basic auth is used (if the registry requires authentication).\n        \"\"\"\n        if response is None:\n            with warnings.catch_warnings():\n                _ignore_warnings(self)\n                response = self._sessions[0].get(self._base_url, verify=self._tlsverify)\n\n        if response.ok:\n            return None\n\n        # pylint: disable=no-member\n        if response.status_code != requests.codes.unauthorized:\n            raise exceptions.DXFUnexpectedStatusCodeError(response.status_code,\n                                                          requests.codes.unauthorized)\n\n        if self._insecure:\n            raise exceptions.DXFAuthInsecureError()\n\n        parsed = www_authenticate.parse(response.headers['www-authenticate'])\n\n        if username is not None and password is not None:\n            headers = {\n                'Authorization': 'Basic ' + base64.b64encode(_to_bytes_2and3(username + ':' + password)).decode('utf-8')\n            }\n        elif authorization is not None:\n            headers = {\n                'Authorization': authorization\n            }\n        else:\n            headers = {}\n\n        if 'bearer' in parsed:\n            info = parsed['bearer']\n            if actions and self._repo:\n                scope = 'repository:' + self._repo + ':' + ','.join(actions)\n            elif 'scope' in info:\n                scope = info['scope']\n            else:\n                scope = ''\n            url_parts = list(urlparse.urlparse(info['realm']))\n            query = urlparse.parse_qs(url_parts[4])\n            query.update({\n                'service': info['service'],\n                'scope': scope\n            })\n            url_parts[4] = urlencode(query, True)\n            url_parts[0] = 'https'\n            if self._auth_host:\n                url_parts[1] = self._auth_host\n            auth_url = urlparse.urlunparse(url_parts)\n            with warnings.catch_warnings():\n                _ignore_warnings(self)\n                r = self._sessions[0].get(auth_url, headers=headers, verify=self._tlsverify)\n            _raise_for_status(r)\n            rjson = r.json()\n            self.token = rjson['access_token'] if 'access_token' in rjson else rjson['token']\n            return self._token\n\n        self._headers = headers\n        return None", "response": "Authenticate to the registry using a username and password and an authorization header."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_repos(self, batch_size=None, iterate=False):\n        it = PaginatingResponse(self, '_base_request',\n                                '_catalog', 'repositories',\n                                params={'n': batch_size})\n        return it if iterate else list(it)", "response": "List all repositories in the registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads a file to the registry and returns its hash.", "response": "def push_blob(self,\n                  filename=None,\n                  progress=None,\n                  data=None, digest=None,\n                  check_exists=True):\n        # pylint: disable=too-many-arguments\n        \"\"\"\n        Upload a file to the registry and return its (SHA-256) hash.\n\n        The registry is content-addressable so the file's content (aka blob)\n        can be retrieved later by passing the hash to :meth:`pull_blob`.\n\n        :param filename: File to upload.\n        :type filename: str\n\n        :param data: Data to upload if ``filename`` isn't given. The data is uploaded in chunks and you must also pass ``digest``.\n        :type data: Generator or iterator\n\n        :param digest: Hash of the data to be uploaded in ``data``, if specified.\n        :type digest: str (hex-encoded SHA-256, prefixed by ``sha256:``)\n\n        :param progress: Optional function to call as the upload progresses. The function will be called with the hash of the file's content (or ``digest``), the blob just read from the file (or chunk from ``data``) and if ``filename`` is specified the total size of the file.\n        :type progress: function(dgst, chunk, size)\n\n        :param check_exists: Whether to check if a blob with the same hash already exists in the registry. If so, it won't be uploaded again.\n        :type check_exists: bool\n\n        :rtype: str\n        :returns: Hash of file's content.\n        \"\"\"\n        if filename is None:\n            dgst = digest\n        else:\n            dgst = hash_file(filename)\n        if check_exists:\n            try:\n                self._request('head', 'blobs/' + dgst)\n                return dgst\n            except requests.exceptions.HTTPError as ex:\n                # pylint: disable=no-member\n                if ex.response.status_code != requests.codes.not_found:\n                    raise\n        r = self._request('post', 'blobs/uploads/')\n        upload_url = r.headers['Location']\n        url_parts = list(urlparse.urlparse(upload_url))\n        query = urlparse.parse_qs(url_parts[4])\n        query.update({'digest': dgst})\n        url_parts[4] = urlencode(query, True)\n        url_parts[0] = 'http' if self._insecure else 'https'\n        upload_url = urlparse.urlunparse(url_parts)\n        if filename is None:\n            data = _ReportingChunks(dgst, data, progress) if progress else data\n            self._base_request('put', upload_url, data=data)\n        else:\n            with open(filename, 'rb') as f:\n                data = _ReportingFile(dgst, f, progress) if progress else f\n                self._base_request('put', upload_url, data=data)\n        return dgst"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads a blob from the registry given the hash of its content.", "response": "def pull_blob(self, digest, size=False, chunk_size=None):\n        \"\"\"\n        Download a blob from the registry given the hash of its content.\n\n        :param digest: Hash of the blob's content (prefixed by ``sha256:``).\n        :type digest: str\n\n        :param size: Whether to return the size of the blob too.\n        :type size: bool\n\n        :param chunk_size: Number of bytes to download at a time. Defaults to 8192.\n        :type chunk_size: int\n\n        :rtype: iterator\n        :returns: If ``size`` is falsey, a byte string iterator over the blob's content. If ``size`` is truthy, a tuple containing the iterator and the blob's size.\n        \"\"\"\n        if chunk_size is None:\n            chunk_size = 8192\n        r = self._request('get', 'blobs/' + digest, stream=True)\n        class Chunks(object):\n            # pylint: disable=too-few-public-methods\n            def __iter__(self):\n                sha256 = hashlib.sha256()\n                for chunk in r.iter_content(chunk_size):\n                    sha256.update(chunk)\n                    yield chunk\n                dgst = 'sha256:' + sha256.hexdigest()\n                if dgst != digest:\n                    raise exceptions.DXFDigestMismatchError(dgst, digest)\n        return (Chunks(), long(r.headers['content-length'])) if size else Chunks()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef blob_size(self, digest):\n        r = self._request('head', 'blobs/' + digest)\n        return long(r.headers['content-length'])", "response": "Return the size of a blob in the registry given the hash of its content."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset a manifest for a specific name.", "response": "def set_manifest(self, alias, manifest_json):\n        \"\"\"\n        Give a name (alias) to a manifest.\n\n        :param alias: Alias name\n        :type alias: str\n\n        :param manifest_json: A V2 Schema 2 manifest JSON string\n        :type digests: list\n        \"\"\"\n        self._request('put',\n                      'manifests/' + alias,\n                      data=manifest_json,\n                      headers={'Content-Type': _schema2_mimetype})"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_alias(self, alias, *digests):\n        # pylint: disable=too-many-locals\n        \"\"\"\n        Give a name (alias) to a set of blobs. Each blob is specified by\n        the hash of its content.\n\n        :param alias: Alias name\n        :type alias: str\n\n        :param digests: List of blob hashes (prefixed by ``sha256:``).\n        :type digests: list of strings\n\n        :rtype: str\n        :returns: The registry manifest used to define the alias. You almost definitely won't need this.\n        \"\"\"\n        try:\n            manifest_json = self.make_manifest(*digests)\n            self.set_manifest(alias, manifest_json)\n            return manifest_json\n        except requests.exceptions.HTTPError as ex:\n            # pylint: disable=no-member\n            if ex.response.status_code != requests.codes.bad_request:\n                raise\n            manifest_json = self.make_unsigned_manifest(alias, *digests)\n            signed_json = _sign_manifest(manifest_json)\n            self._request('put', 'manifests/' + alias, data=signed_json)\n            return signed_json", "response": "Set an alias for a set of blobs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrequesting the manifest for an alias and return the manifest and the requests. Response", "response": "def get_manifest_and_response(self, alias):\n        \"\"\"\n        Request the manifest for an alias and return the manifest and the\n        response.\n\n        :param alias: Alias name.\n        :type alias: str\n\n        :rtype: tuple\n        :returns: Tuple containing the manifest as a string (JSON) and the `requests.Response <http://docs.python-requests.org/en/master/api/#requests.Response>`_\n        \"\"\"\n        r = self._request('get',\n                          'manifests/' + alias,\n                          headers={'Accept': _schema2_mimetype + ', ' +\n                                             _schema1_mimetype})\n        return r.content.decode('utf-8'), r"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_alias(self,\n                  alias=None,\n                  manifest=None,\n                  verify=True,\n                  sizes=False,\n                  dcd=None):\n        # pylint: disable=too-many-arguments\n        \"\"\"\n        Get the blob hashes assigned to an alias.\n\n        :param alias: Alias name. You almost definitely will only need to pass this argument.\n        :type alias: str\n\n        :param manifest: If you previously obtained a manifest, specify it here instead of ``alias``. You almost definitely won't need to do this.\n        :type manifest: str\n\n        :param verify: (v1 schema only) Whether to verify the integrity of the alias definition in the registry itself. You almost definitely won't need to change this from the default (``True``).\n        :type verify: bool\n\n        :param sizes: Whether to return sizes of the blobs along with their hashes\n        :type sizes: bool\n\n        :param dcd: (if ``manifest`` is specified) The Docker-Content-Digest header returned when getting the manifest. If present, this is checked against the manifest.\n        :type dcd: str\n\n        :rtype: list\n        :returns: If ``sizes`` is falsey, a list of blob hashes (strings) which are assigned to the alias. If ``sizes`` is truthy, a list of (hash,size) tuples for each blob.\n        \"\"\"\n        return self._get_alias(alias, manifest, verify, sizes, dcd, False)", "response": "Get the blob hashes assigned to an alias."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_digest(self,\n                   alias=None,\n                   manifest=None,\n                   verify=True,\n                   dcd=None):\n        \"\"\"\n        (v2 schema only) Get the hash of an alias's configuration blob.\n\n        For an alias created using ``dxf``, this is the hash of the first blob\n        assigned to the alias.\n\n        For a Docker image tag, this is the same as\n        ``docker inspect alias --format='{{.Id}}'``.\n\n        :param alias: Alias name. You almost definitely will only need to pass this argument.\n        :type alias: str\n\n        :param manifest: If you previously obtained a manifest, specify it here instead of ``alias``. You almost definitely won't need to do this.\n        :type manifest: str\n\n        :param verify: (v1 schema only) Whether to verify the integrity of the alias definition in the registry itself. You almost definitely won't need to change this from the default (``True``).\n        :type verify: bool\n\n        :param dcd: (if ``manifest`` is specified) The Docker-Content-Digest header returned when getting the manifest. If present, this is checked against the manifest.\n        :type dcd: str\n\n        :rtype: str\n        :returns: Hash of the alias's configuration blob.\n        \"\"\"\n        return self._get_alias(alias, manifest, verify, False, dcd, True)", "response": "Get the digest of an alias s configuration blob."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the Docker - Content - Digest header for an alias.", "response": "def _get_dcd(self, alias):\n        \"\"\"\n        Get the Docker-Content-Digest header for an alias.\n\n        :param alias: Alias name.\n        :type alias: str\n\n        :rtype: str\n        :returns: DCD header for the alias.\n        \"\"\"\n        # https://docs.docker.com/registry/spec/api/#deleting-an-image\n        # Note When deleting a manifest from a registry version 2.3 or later,\n        # the following header must be used when HEAD or GET-ing the manifest\n        # to obtain the correct digest to delete:\n        # Accept: application/vnd.docker.distribution.manifest.v2+json\n        return self._request(\n            'head',\n            'manifests/{}'.format(alias),\n            headers={'Accept': _schema2_mimetype},\n        ).headers.get('Docker-Content-Digest')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef del_alias(self, alias):\n        dcd = self._get_dcd(alias)\n        dgsts = self.get_alias(alias)\n        self._request('delete', 'manifests/{}'.format(dcd))\n        return dgsts", "response": "Delete an alias from the registry."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_base(cls, base, repo):\n        # pylint: disable=protected-access\n        r = cls(base._host, repo, base._auth, base._insecure, base._auth_host, base._tlsverify)\n        r._token = base._token\n        r._headers = base._headers\n        r._sessions = [base._sessions[0]]\n        return r", "response": "Create a new object which shares the same host settings and session with the given base."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets Name converted case", "response": "def get_name(self, name_case=DdlParseBase.NAME_CASE.original):\n        \"\"\"\n        Get Name converted case\n\n        :param name_case: name case type\n            * DdlParse.NAME_CASE.original : Return to no convert\n            * DdlParse.NAME_CASE.lower : Return to lower\n            * DdlParse.NAME_CASE.upper : Return to upper\n\n        :return: name\n        \"\"\"\n        if name_case == self.NAME_CASE.lower:\n            return self._name.lower()\n        elif name_case == self.NAME_CASE.upper:\n            return self._name.upper()\n        else:\n            return self._name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the string representation of the table s constraint.", "response": "def constraint(self):\n        \"\"\"Constraint string\"\"\"\n        constraint_arr = []\n        if self._not_null:\n            constraint_arr.append(\"PRIMARY KEY\" if self._pk else \"NOT NULL\")\n        if self._unique:\n            constraint_arr.append(\"UNIQUE\")\n\n        return \" \".join(constraint_arr)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bigquery_data_type(self):\n\n        # BigQuery data type = {source_database: [data type, ...], ...}\n        BQ_DATA_TYPE_DIC = OrderedDict()\n        BQ_DATA_TYPE_DIC[\"STRING\"] = {None: [re.compile(r\"(CHAR|TEXT|CLOB|JSON|UUID)\")]}\n        BQ_DATA_TYPE_DIC[\"INTEGER\"] = {None: [re.compile(r\"INT|SERIAL|YEAR\")]}\n        BQ_DATA_TYPE_DIC[\"FLOAT\"] = {None: [re.compile(r\"(FLOAT|DOUBLE)\"), \"REAL\", \"MONEY\"]}\n        BQ_DATA_TYPE_DIC[\"DATETIME\"] = {\n            None: [\"DATETIME\", \"TIMESTAMP\", \"TIMESTAMP WITHOUT TIME ZONE\"],\n            self.DATABASE.oracle: [\"DATE\"]\n        }\n        BQ_DATA_TYPE_DIC[\"TIMESTAMP\"] = {None: [\"TIMESTAMPTZ\", \"TIMESTAMP WITH TIME ZONE\"]}\n        BQ_DATA_TYPE_DIC[\"DATE\"] = {None: [\"DATE\"]}\n        BQ_DATA_TYPE_DIC[\"TIME\"] = {None: [\"TIME\"]}\n        BQ_DATA_TYPE_DIC[\"BOOLEAN\"] = {None: [re.compile(r\"BOOL\")]}\n\n        for bq_type, conditions in BQ_DATA_TYPE_DIC.items():\n            for source_db, source_datatypes in conditions.items():\n                for source_datatype in source_datatypes:\n\n                    if isinstance(source_datatype, str):\n                        if self._data_type == source_datatype \\\n                            and (  self._source_database == source_db\n                                or (self._source_database is not None and source_db is None)):\n                            return bq_type\n\n                    elif re.search(source_datatype, self._data_type) \\\n                        and (  self._source_database == source_db\n                            or (self._source_database is not None and source_db is None)):\n                        return bq_type\n\n        if self._data_type in [\"NUMERIC\", \"NUMBER\", \"DECIMAL\"]:\n            if self._scale is not None:\n                return \"FLOAT\"\n\n            if self._data_type == \"NUMBER\" \\\n                and self._source_database == self.DATABASE.oracle \\\n                and self._length is None:\n                return \"FLOAT\"\n\n            return \"INTEGER\"\n\n        raise ValueError(\"Unknown data type : '{}'\".format(self._data_type))", "response": "Get BigQuery Legacy SQL data type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate BigQuery JSON field define", "response": "def to_bigquery_field(self, name_case=DdlParseBase.NAME_CASE.original):\n        \"\"\"Generate BigQuery JSON field define\"\"\"\n\n        col_name = self.get_name(name_case)\n        mode = self.bigquery_mode\n\n        if self.array_dimensional <= 1:\n            # no or one dimensional array data type\n            type = self.bigquery_legacy_data_type\n\n        else:\n            # multiple dimensional array data type\n            type = \"RECORD\"\n\n            fields = OrderedDict()\n            fields_cur = fields\n\n            for i in range(1, self.array_dimensional):\n                is_last = True if i == self.array_dimensional - 1 else False\n\n                fields_cur['fields'] = [OrderedDict()]\n                fields_cur = fields_cur['fields'][0]\n\n                fields_cur['name'] = \"dimension_{}\".format(i)\n                fields_cur['type'] = self.bigquery_legacy_data_type if is_last else \"RECORD\"\n                fields_cur['mode'] = self.bigquery_mode if is_last else \"REPEATED\"\n\n        col = OrderedDict()\n        col['name'] = col_name\n        col['type'] = type\n        col['mode'] = mode\n        if self.array_dimensional > 1:\n            col['fields'] = fields['fields']\n\n        return json.dumps(col)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate BigQuery JSON fields define", "response": "def to_bigquery_fields(self, name_case=DdlParseBase.NAME_CASE.original):\n        \"\"\"\n        Generate BigQuery JSON fields define\n\n        :param name_case: name case type\n            * DdlParse.NAME_CASE.original : Return to no convert\n            * DdlParse.NAME_CASE.lower : Return to lower\n            * DdlParse.NAME_CASE.upper : Return to upper\n\n        :return: BigQuery JSON fields define\n        \"\"\"\n\n        bq_fields = []\n\n        for col in self.values():\n            bq_fields.append(col.to_bigquery_field(name_case))\n\n        return \"[{}]\".format(\",\".join(bq_fields))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_bigquery_fields(self, name_case=DdlParseBase.NAME_CASE.original):\n\n        return self._columns.to_bigquery_fields(name_case)", "response": "Generate BigQuery JSON fields define\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a BigQuery CREATE TABLE statement for the current object.", "response": "def to_bigquery_ddl(self, name_case=DdlParseBase.NAME_CASE.original):\n        \"\"\"\n        Generate BigQuery CREATE TABLE statements\n\n        :param name_case: name case type\n            * DdlParse.NAME_CASE.original : Return to no convert\n            * DdlParse.NAME_CASE.lower : Return to lower\n            * DdlParse.NAME_CASE.upper : Return to upper\n\n        :return: BigQuery CREATE TABLE statements\n        \"\"\"\n\n        if self.schema is None:\n            dataset = \"dataset\"\n        elif name_case == self.NAME_CASE.lower:\n            dataset = self.schema.lower()\n        elif name_case == self.NAME_CASE.upper:\n            dataset = self.schema.upper()\n        else:\n            dataset = self.schema\n\n        cols_defs = []\n        for col in self.columns.values():\n            col_name = col.get_name(name_case)\n\n            if col.array_dimensional < 1:\n                # no array data type\n                type = col.bigquery_standard_data_type\n                not_null = \" NOT NULL\" if col.not_null else \"\"\n\n            else:\n                # one or multiple dimensional array data type\n                type_front = \"ARRAY<\"\n                type_back = \">\"\n                for i in range(1, col.array_dimensional):\n                    type_front += \"STRUCT<dimension_{} ARRAY<\".format(i)\n                    type_back += \">>\"\n\n                type = \"{}{}{}\".format(type_front, col.bigquery_standard_data_type, type_back)\n                not_null = \"\"\n\n            cols_defs.append(\"{name} {type}{not_null}\".format(\n                name=col_name,\n                type=type,\n                not_null=not_null,\n            ))\n\n        return textwrap.dedent(\n            \"\"\"\\\n            #standardSQL\n            CREATE TABLE `project.{dataset}.{table}`\n            (\n              {colmns_define}\n            )\"\"\").format(\n            dataset=dataset,\n            table=self.get_name(name_case),\n            colmns_define=\",\\n  \".join(cols_defs),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing DDL script and return parsed table define info.", "response": "def parse(self, ddl=None, source_database=None):\n        \"\"\"\n        Parse DDL script.\n\n        :param ddl: DDL script\n        :return: DdlParseTable, Parsed table define info.\n        \"\"\"\n\n        if ddl is not None:\n            self._ddl = ddl\n\n        if source_database is not None:\n            self.source_database = source_database\n\n        if self._ddl is None:\n            raise ValueError(\"DDL is not specified\")\n\n        ret = self._DDL_PARSE_EXPR.parseString(self._ddl)\n        # print(ret.dump())\n\n        if \"schema\" in ret:\n            self._table.schema = ret[\"schema\"]\n\n        self._table.name = ret[\"table\"]\n        self._table.is_temp = True if \"temp\" in ret else False\n\n        for ret_col in ret[\"columns\"]:\n\n            if ret_col.getName() == \"column\":\n                # add column\n                col = self._table.columns.append(\n                    column_name=ret_col[\"name\"],\n                    data_type_array=ret_col[\"type\"],\n                    array_brackets=ret_col['array_brackets'] if \"array_brackets\" in ret_col else None)\n\n                if \"constraint\" in ret_col:\n                    col.constraint = ret_col[\"constraint\"]\n\n            elif ret_col.getName() == \"constraint\":\n                # set column constraint\n                for col_name in ret_col[\"constraint_columns\"]:\n                    col = self._table.columns[col_name]\n\n                    if ret_col[\"type\"] == \"PRIMARY KEY\":\n                        col.not_null = True\n                        col.primary_key = True\n                    elif ret_col[\"type\"] in [\"UNIQUE\", \"UNIQUE KEY\"]:\n                        col.unique = True\n                    elif ret_col[\"type\"] == \"NOT NULL\":\n                        col.not_null = True\n\n        return self._table"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef launch(program, sock, stderr=True, cwd=None, env=None):\n        if stderr is True:\n            err = sock # redirect to socket\n        elif stderr is False:\n            err = open(os.devnull, 'wb') # hide\n        elif stderr is None:\n            err = None # redirect to console\n\n        p = subprocess.Popen(program,\n                shell=type(program) not in (list, tuple),\n                stdin=sock, stdout=sock, stderr=err,\n                cwd=cwd, env=env,\n                close_fds=True)\n\n        sock.close()\n        return p", "response": "A static method for launching a process that is connected to a given socket."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef respond(self, packet, peer, flags=0):\n        self.sock.sendto(packet, flags, peer)", "response": "Send a message back to a peer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the user - supplied target and return a tuple that is its arguments but the original target.", "response": "def _parse_target(target, listen, udp, ipv6):\n        \"\"\"\n        Takes the basic version of the user args and extract as much data as\n        possible from target. Returns a tuple that is its arguments but\n        sanitized.\n        \"\"\"\n        if isinstance(target, str):\n            if target.startswith('nc '):\n                out_host = None\n                out_port = None\n\n                try:\n                    opts, pieces = getopt.getopt(target.split()[1:], 'u46lp:',\n                                                 [])\n                except getopt.GetoptError as exc:\n                    raise ValueError(exc)\n\n                for opt, arg in opts:\n                    if opt == '-u':\n                        udp = True\n                    elif opt == '-4':\n                        ipv6 = False\n                    elif opt == '-6':\n                        ipv6 = True\n                    elif opt == '-l':\n                        listen = True\n                    elif opt == '-p':\n                        out_port = int(arg)\n                    else:\n                        assert False, \"unhandled option\"\n\n                if not pieces:\n                    pass\n                elif len(pieces) == 1:\n                    if listen and pieces[0].isdigit():\n                        out_port = int(pieces[0])\n                    else:\n                        out_host = pieces[0]\n                elif len(pieces) == 2 and pieces[1].isdigit():\n                    out_host = pieces[0]\n                    out_port = int(pieces[1])\n                else:\n                    raise ValueError(\"Bad cmdline: %s\" % target)\n\n                if out_host is None:\n                    if listen:\n                        out_host = '::' if ipv6 else '0.0.0.0'\n                    else:\n                        raise ValueError(\"Missing address: %s\" % target)\n                if out_port is None:\n                    raise ValueError(\"Missing port: %s\" % target)\n\n                if _is_ipv6_addr(out_host):\n                    ipv6 = True\n\n                return (out_host, out_port), listen, udp, ipv6\n\n            elif PROTOCAL_RE.match(target) is not None:\n                parsed = urlparse(target)\n                port = None\n\n                try:\n                    scheme_udp, scheme_ipv6, scheme_port = KNOWN_SCHEMES[parsed.scheme]\n                except KeyError:\n                    raise ValueError(\"Unknown scheme: %s\" % parsed.scheme)\n\n                if scheme_udp is not None:\n                    udp = scheme_udp\n                if scheme_ipv6 is not None:\n                    ipv6 = scheme_ipv6\n                if scheme_port is not None:\n                    port = scheme_port\n\n                if parsed.netloc.startswith('['):\n                    addr, extra = parsed.netloc[1:].split(']', 1)\n                    if extra.startswith(':'):\n                        port = int(extra[1:])\n                else:\n                    if ':' in parsed.netloc:\n                        addr, port = parsed.netloc.split(':', 1)\n                        port = int(port)\n                    else:\n                        addr = parsed.netloc\n\n                if addr is None or port is None:\n                    raise ValueError(\"Can't parse addr/port from %s\" % target)\n\n                if _is_ipv6_addr(addr):\n                    ipv6 = True\n\n                return (addr, port), listen, udp, ipv6\n\n            else:\n                if target.startswith('['):\n                    addr, extra = target[1:].split(']', 1)\n                    if extra.startswith(':'):\n                        port = int(extra[1:])\n                    else:\n                        port = None\n                else:\n                    if ':' in target:\n                        addr, port = target.split(':', 1)\n                        port = int(port)\n                    else:\n                        addr = target\n                        port = None\n\n                if port is None:\n                    raise ValueError(\"No port given: %s\" % target)\n\n                if _is_ipv6_addr(addr):\n                    ipv6 = True\n\n                return (addr, port), listen, udp, ipv6\n\n        elif isinstance(target, (int, long)):\n            if listen:\n                out_port = target\n            else:\n                raise ValueError(\"Can't deal with number as connection address\")\n\n            return ('::' if ipv6 else '0.0.0.0', out_port), listen, udp, ipv6\n\n        elif isinstance(target, tuple):\n            if len(target) >= 1 and isinstance(target[0], str) and _is_ipv6_addr(target[0]):\n                ipv6 = True\n            return target, listen, udp, ipv6\n\n        else:\n            raise ValueError(\"Can't parse target: %r\" % target)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect to the specified target and sets self. sock and self. peer .", "response": "def _connect(self, target, listen, udp, ipv6, retry):\n        \"\"\"\n        Takes target/listen/udp/ipv6 and sets self.sock and self.peer\n        \"\"\"\n        ty = socket.SOCK_DGRAM if udp else socket.SOCK_STREAM\n        fam = socket.AF_INET6 if ipv6 else socket.AF_INET\n        self.sock = socket.socket(fam, ty)\n        if listen:\n            self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            self.sock.bind(target)\n            if not udp:\n                self.sock.listen(1)\n                conn, addr = self.sock.accept()\n                self.sock.close()\n                self.sock = conn\n                self.peer = addr\n            else:\n                self.buf, self.peer = self.sock.recvfrom(1024)\n                self.sock.connect(self.peer)\n                self._log_recv(self.buf, False)\n            if self.verbose:\n                self._print_verbose('Connection from %s accepted' % str(self.peer))\n        else:\n            while True:\n                try:\n                    self.sock.connect(target)\n                except (socket.gaierror, socket.herror) as exc:\n                    raise NetcatError('Could not connect to %r: %r' \\\n                            % (target, exc))\n                except socket.error as exc:\n                    if retry:\n                        time.sleep(0.2)\n                    else:\n                        raise NetcatError('Could not connect to %r: %r' \\\n                                % (target, exc))\n                else:\n                    break\n            self.peer = target"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shutdown(self, how=socket.SHUT_RDWR):\n        if self._sock_send is not None:\n            self._sock_send.shutdown(how)\n        return self.sock.shutdown(how)", "response": "Send a shutdown signal for both reading and writing."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shutdown_rd(self):\n        if self._sock_send is not None:\n            self.sock.close()\n        else:\n            return self.shutdown(socket.SHUT_RD)", "response": "Send a shutdown signal for reading - you may no longer read from this ArcGIS socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a shutdown signal for writing - you may no longer write to this ArcGIS socket.", "response": "def shutdown_wr(self):\n        \"\"\"\n        Send a shutdown signal for writing - you may no longer write to this\n        socket.\n        \"\"\"\n        if self._sock_send is not None:\n            self._sock_send.close()\n        else:\n            return self.shutdown(socket.SHUT_WR)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreceive until predicate returns a positive integer.", "response": "def _recv_predicate(self, predicate, timeout='default', raise_eof=True):\n        \"\"\"\n        Receive until predicate returns a positive integer.\n        The returned number is the size to return.\n        \"\"\"\n\n        if timeout == 'default':\n            timeout = self._timeout\n\n        self.timed_out = False\n\n        start = time.time()\n        try:\n            while True:\n                cut_at = predicate(self.buf)\n                if cut_at > 0:\n                    break\n                if timeout is not None:\n                    time_elapsed = time.time() - start\n                    if time_elapsed > timeout:\n                        raise socket.timeout\n                    self._settimeout(timeout - time_elapsed)\n\n                data = self._recv(4096)\n                self._log_recv(data, False)\n                self.buf += data\n\n                if not data:\n                    if raise_eof:\n                        raise NetcatError(\"Connection dropped!\")\n                    cut_at = len(self.buf)\n                    break\n\n        except KeyboardInterrupt:\n            self._print_header('\\n======== Connection interrupted! ========')\n            raise\n        except socket.timeout:\n            self.timed_out = True\n            if self._raise_timeout:\n                raise NetcatTimeout()\n            return b''\n        except socket.error as exc:\n            raise NetcatError('Socket error: %r' % exc)\n\n        self._settimeout(self._timeout)\n\n        ret = self.buf[:cut_at]\n        self.buf = self.buf[cut_at:]\n        self._log_recv(ret, True)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recv(self, n=4096, timeout='default'):\n\n        self._print_recv_header(\n            '======== Receiving {0}B{timeout_text} ========', timeout, n)\n\n        return self._recv_predicate(lambda s: min(n, len(s)), timeout)", "response": "Receive at most n bytes from the socket."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recv_until(self, s, max_size=None, timeout='default'):\n\n        self._print_recv_header(\n            '======== Receiving until {0}{timeout_text} ========', timeout, repr(s))\n\n        if max_size is None:\n            max_size = 2 ** 62\n\n        def _predicate(buf):\n            try:\n                return min(buf.index(s) + len(s), max_size)\n            except ValueError:\n                return 0 if len(buf) < max_size else max_size\n        return self._recv_predicate(_predicate, timeout)", "response": "Recieve data from the socket until the given substring is observed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef recv_all(self, timeout='default'):\n\n        self._print_recv_header('======== Receiving until close{timeout_text} ========', timeout)\n\n        return self._recv_predicate(lambda s: 0, timeout, raise_eof=False)", "response": "Receive all data until connection closes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recv_exactly(self, n, timeout='default'):\n\n        self._print_recv_header(\n            '======== Receiving until exactly {0}B{timeout_text} ========', timeout, n)\n\n        return self._recv_predicate(lambda s: n if len(s) >= n else 0, timeout)", "response": "Recieve exactly n bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends all the given data to the socket.", "response": "def send(self, s):\n        \"\"\"\n        Sends all the given data to the socket.\n\n        Aliases: write, put, sendall, send_all\n        \"\"\"\n        self._print_header('======== Sending ({0}) ========'.format(len(s)))\n\n        self._log_send(s)\n        out = len(s)\n\n        while s:\n            s = s[self._send(s):]\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconnects the socket to the terminal and interacts with the user.", "response": "def interact(self, insock=sys.stdin, outsock=sys.stdout):\n        \"\"\"\n        Connects the socket to the terminal for user interaction.\n        Alternate input and output files may be specified.\n\n        This method cannot be used with a timeout.\n\n        Aliases: interactive, interaction\n        \"\"\"\n        self._print_header('======== Beginning interactive session ========')\n\n        if hasattr(outsock, 'buffer'):\n            outsock = outsock.buffer    # pylint: disable=no-member\n\n        self.timed_out = False\n\n        save_verbose = self.verbose\n        self.verbose = 0\n        try:\n            if self.buf:\n                outsock.write(self.buf)\n                outsock.flush()\n                self.buf = b''\n\n            while True:\n                readable_socks = select(self.sock, insock)\n                for readable in readable_socks:\n                    if readable is insock:\n                        data = os.read(insock.fileno(), 4096)\n                        self.send(data)\n                        if not data:\n                            raise NetcatError\n                    else:\n                        data = self.recv(timeout=None)\n                        outsock.write(data)\n                        outsock.flush()\n                        if not data:\n                            raise NetcatError\n        except KeyboardInterrupt:\n            self.verbose = save_verbose\n            self._print_header('\\n======== Connection interrupted! ========')\n            raise\n        except (socket.error, NetcatError):\n            self.verbose = save_verbose\n            self._print_header('\\n======== Connection dropped! ========')\n        finally:\n            self.verbose = save_verbose"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef recv_line(self, max_size=None, timeout='default', ending=None):\n        if ending is None:\n            ending = self.LINE_ENDING\n        return self.recv_until(ending, max_size, timeout)", "response": "Recieve until the next newline and return the next line."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_line(self, line, ending=None):\n        if ending is None:\n            ending = self.LINE_ENDING\n        return self.send(line + ending)", "response": "Write the string to the wire followed by a newline."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the modulation is active", "response": "def is_active(self, timperiods):\n        \"\"\"\n        Know if this result modulation is active now\n\n        :return: True is we are in the period, otherwise False\n        :rtype: bool\n        \"\"\"\n        now = int(time.time())\n        timperiod = timperiods[self.modulation_period]\n        if not timperiod or timperiod.is_time_valid(now):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget an object from the scheduler.", "response": "def object(self, o_type, o_name=None):\n        \"\"\"Get an object from the scheduler.\n\n        The result is a serialized object which is a Json structure containing:\n        - content: the serialized object content\n        - __sys_python_module__: the python class of the returned object\n\n        The Alignak unserialize function of the alignak.misc.serialization package allows\n        to restore the initial object.\n\n        .. code-block:: python\n\n            from alignak.misc.serialization import unserialize\n            from alignak.objects.hostgroup import Hostgroup\n            raw_data = req.get(\"http://127.0.0.1:7768/object/hostgroup/allhosts\")\n            print(\"Got: %s / %s\" % (raw_data.status_code, raw_data.content))\n            assert raw_data.status_code == 200\n            object = raw_data.json()\n            group = unserialize(object, True)\n            assert group.__class__ == Hostgroup\n            assert group.get_name() == 'allhosts'\n\n        As an example:\n        {\n            \"__sys_python_module__\": \"alignak.objects.hostgroup.Hostgroup\",\n            \"content\": {\n                \"uuid\": \"32248642-97dd-4f39-aaa2-5120112a765d\",\n                \"name\": \"\",\n                \"hostgroup_name\": \"allhosts\",\n                \"use\": [],\n                \"tags\": [],\n                \"alias\": \"All Hosts\",\n                \"notes\": \"\",\n                \"definition_order\": 100,\n                \"register\": true,\n                \"unknown_members\": [],\n                \"notes_url\": \"\",\n                \"action_url\": \"\",\n\n                \"imported_from\": \"unknown\",\n                \"conf_is_correct\": true,\n                \"configuration_errors\": [],\n                \"configuration_warnings\": [],\n                \"realm\": \"\",\n                \"downtimes\": {},\n                \"hostgroup_members\": [],\n                \"members\": [\n                    \"553d47bc-27aa-426c-a664-49c4c0c4a249\",\n                    \"f88093ca-e61b-43ff-a41e-613f7ad2cea2\",\n                    \"df1e2e13-552d-43de-ad2a-fe80ad4ba979\",\n                    \"d3d667dd-f583-4668-9f44-22ef3dcb53ad\"\n                ]\n            }\n        }\n\n        :param o_type: searched object type\n        :type o_type: str\n        :param o_name: searched object name (or uuid)\n        :type o_name: str\n        :return: serialized object information\n        :rtype: str\n        \"\"\"\n        o_found = self._get_object(o_type=o_type, o_name=o_name)\n        if not o_found:\n            return {'_status': u'ERR', '_message': u'Required %s not found.' % o_type}\n        return o_found"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndumps an internal host from the scheduler.", "response": "def dump(self, o_name=None, details=False, raw=False):\n        # pylint: disable=too-many-locals, too-many-branches\n        \"\"\"Dump an host (all hosts) from the scheduler.\n\n        This gets the main host information from the scheduler. If details is set, then some\n        more information are provided. This will not get all the host known attributes but only\n        a reduced set that will inform about the host and its services status\n\n        If raw is set the information are provided in two string lists formated as CSV strings.\n        The first list element contains the hosts information and the second one contains the\n        services information.\n\n        If an host name is provided, this function will get only this host information, else\n        all the scheduler hosts are returned.\n\n        As an example (raw format):\n        [\n            [   # Host information\n                \"type;host;name;last_check;state_id;state;state_type;is_problem;is_impact;output\",\n                \"BR_host;host;BR_host;1532451511;0;UP;HARD;False;False;Host assumed to be UP\"\n            ],\n            [   # Services information\n                \"type;host;name;last_check;state_id;state;state_type;is_problem;is_impact;output\",\n                \"BR_host;service;dummy_critical;1532451490;2;CRITICAL;SOFT;False;False;\n                BR_host-dummy_critical-2\",\n                \"BR_host;service;BR_Simple_And;0;0;OK;HARD;False;False;\",\n                \"BR_host;service;dummy_unreachable;1532451501;4;UNREACHABLE;SOFT;False;False;\n                BR_host-dummy_unreachable-4\",\n                \"BR_host;service;dummy_no_output;1532451495;0;OK;HARD;False;False;\n                Service internal check result: 0\",\n                \"BR_host;service;dummy_unknown;1532451475;3;UNKNOWN;SOFT;False;False;\n                BR_host-dummy_unknown-3\",\n                \"BR_host;service;dummy_echo;1532451501;0;OK;HARD;False;False;\",\n                \"BR_host;service;dummy_warning;1532451492;1;WARNING;SOFT;False;False;\n                BR_host-dummy_warning-1\",\n                \"BR_host;service;dummy_random;1532451496;2;CRITICAL;SOFT;False;False;\n                Service internal check result: 2\",\n                \"BR_host;service;dummy_ok;1532451492;0;OK;HARD;False;False;BR_host\"\n            ]\n        ]\n\n        As an example (json format):\n        {\n            is_impact: false,\n            name: \"BR_host\",\n            state: \"UP\",\n            last_check: 1532451811,\n            state_type: \"HARD\",\n            host: \"BR_host\",\n            output: \"Host assumed to be UP\",\n            services: [\n                {\n                    is_impact: false,\n                    name: \"dummy_critical\",\n                    state: \"CRITICAL\",\n                    last_check: 1532451790,\n                    state_type: \"HARD\",\n                    host: \"BR_host\",\n                    output: \"BR_host-dummy_critical-2\",\n                    state_id: 2,\n                    type: \"service\",\n                    is_problem: true\n                },\n                {\n                    is_impact: true,\n                    name: \"BR_Simple_And\",\n                    state: \"WARNING\",\n                    last_check: 1532451775,\n                    state_type: \"SOFT\",\n                    host: \"BR_host\",\n                    output: \"\",\n                    state_id: 1,\n                    type: \"service\",\n                    is_problem: false\n                },\n                ....\n                ....\n            },\n            state_id: 0,\n            type: \"host\",\n            is_problem: false\n        }\n\n        :param o_name: searched host name (or uuid)\n        :type o_name: str\n        :param details: less or more details\n        :type details: bool\n        :param raw: json or raw text format\n        :type raw: bool\n        :return: list of host and services information\n        :rtype: list\n        \"\"\"\n\n        def get_host_info(host, services, details=False, raw=False):\n            # pylint: disable=too-many-branches\n            \"\"\"Get the host information\n\n            :return: None\n            \"\"\"\n            __props__ = [\n                'last_check', 'state_id', 'state', 'state_type', 'is_problem', 'is_impact', 'output'\n            ]\n            if details:\n                __props__ = __props__ + [\n                    'uuid', 'address', 'alias', 'business_impact', 'tags', 'customs', 'parents',\n                    'long_output', 'perf_data',\n                    'check_period', 'active_checks_enabled', 'passive_checks_enabled',\n                    'check_freshness', 'freshness_threshold', 'freshness_state',\n                    'get_overall_state', 'overall_state_id', 'state_id', 'state', 'state_type',\n                    'passive_check', 'acknowledged', 'downtimed', 'next_check',\n                    'last_time_up', 'last_time_down',\n                    'last_time_ok', 'last_time_warning', 'last_time_critical',\n                    'last_time_unknown', 'last_time_unreachable'\n                ]\n\n            host_data = OrderedDict({'type': 'host',\n                                     'host': host.get_name(),\n                                     'name': host.get_name()})\n            __header__ = ['type', 'host', 'name']\n            for key in __props__:\n                if hasattr(host, key):\n                    __header__.append(key)\n                    if isinstance(getattr(host, key), Callable):\n                        host_data[key] = getattr(host, key)(services)\n                    elif isinstance(getattr(host, key), set):\n                        host_data[key] = list(getattr(host, key))\n                    else:\n                        host_data[key] = getattr(host, key)\n            if raw:\n                host_data['_header_host'] = __header__\n\n            host_data['services'] = []\n            __header__ = ['type', 'host', 'name']\n            for service in host.services:\n                service = services[service]\n                service_data = OrderedDict({'type': 'service',\n                                            'host': host.get_name(),\n                                            'name': service.get_name()})\n                for key in __props__:\n                    if hasattr(service, key):\n                        if key not in __header__:\n                            __header__.append(key)\n                        if isinstance(getattr(service, key), Callable):\n                            service_data[key] = getattr(services, key)()\n                        elif isinstance(getattr(service, key), set):\n                            service_data[key] = list(getattr(service, key))\n                        else:\n                            service_data[key] = getattr(service, key)\n                host_data['services'].append(service_data)\n            if raw:\n                host_data['_header_service'] = __header__\n\n            return host_data\n\n        if details is not False:\n            details = bool(details)\n        if raw is not False:\n            raw = bool(raw)\n\n        ls = []\n        try:\n            hosts = self._get_objects('host')\n            services = self._get_objects('service')\n            if o_name is None:\n                for host in hosts:\n                    ls.append(get_host_info(host, services, details=details, raw=raw))\n            else:\n                # Perhaps we got an host uuid...\n                host = hosts.find_by_name(o_name)\n                if o_name in hosts:\n                    host = hosts[o_name]\n\n                if host:\n                    ls.append(get_host_info(host, services, details=False, raw=raw))\n        except Exception as exp:  # pylint: disable=broad-except\n            return str(exp) + \" / \" + traceback.print_exc()\n\n        if o_name and not host:\n            return {'_status': u'ERR', '_message': u'Required host (%s) not found.' % o_name}\n\n        if raw and ls:\n            raw_ls_hosts = []\n            _header_host = ['type', 'host', 'name']\n            raw_ls_services = []\n            _header_service = ['type', 'host', 'name']\n\n            for item in ls:\n                if len(item['_header_host']) > len(_header_host):\n                    _header_host = item['_header_host']\n                if len(item['_header_service']) > len(_header_service):\n                    _header_service = item['_header_service']\n                item.pop('_header_host')\n                item.pop('_header_service')\n\n                services = []\n                if 'services' in item:\n                    services = item.pop('services')\n                    # Write host line\n                    raw_ls_hosts.append(';'.join(\"%s\" % val for val in list(item.values())))\n                    for service in services:\n                        raw_ls_services.append(\n                            ';'.join(\"%s\" % val for val in list(service.values())))\n            raw_ls_hosts.insert(0, ';'.join(_header_host))\n            raw_ls_services.insert(0, ';'.join(_header_service))\n\n            return [raw_ls_hosts, raw_ls_services]\n\n        return ls"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef monitoring_problems(self):\n        if self.app.type != 'scheduler':\n            return {'_status': u'ERR',\n                    '_message': u\"This service is only available for a scheduler daemon\"}\n\n        res = self.identity()\n        res.update(self.app.get_monitoring_problems())\n        return res", "response": "Get Alignak scheduler monitoring status"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _wait_new_conf(self):\n        # Stop the scheduling loop\n        self.app.sched.stop_scheduling()\n        super(SchedulerInterface, self)._wait_new_conf()", "response": "Ask the scheduler to drop its configuration and wait for a new one."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _initial_broks(self, broker_name):\n        with self.app.conf_lock:\n            logger.info(\"A new broker just connected : %s\", broker_name)\n            return self.app.sched.fill_initial_broks(broker_name)", "response": "Get initial broks from the scheduler"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _broks(self, broker_name):\n        logger.debug(\"Getting broks for %s from the scheduler\", broker_name)\n        for broker_link in list(self.app.brokers.values()):\n            if broker_name == broker_link.name:\n                break\n        else:\n            logger.warning(\"Requesting broks for an unknown broker: %s\", broker_name)\n            return {}\n\n        # Now get the broks for this specific broker\n        with self.app.broks_lock:\n            res = self.app.get_broks(broker_name)\n\n        return serialize(res, True)", "response": "Get the broks list for a specific broker"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _checks(self, do_checks=False, do_actions=False, poller_tags=None,\n                reactionner_tags=None, worker_name='none', module_types=None):\n        \"\"\"Get checks from scheduler, used by poller or reactionner when they are\n        in active mode (passive = False)\n\n        This function is not intended for external use. Let the poller and reactionner\n        manage all this stuff by themselves ;)\n\n        :param do_checks: used for poller to get checks\n        :type do_checks: bool\n        :param do_actions: used for reactionner to get actions\n        :type do_actions: bool\n        :param poller_tags: poller tags to filter on this poller\n        :type poller_tags: list\n        :param reactionner_tags: reactionner tags to filter on this reactionner\n        :type reactionner_tags: list\n        :param worker_name: Worker name asking (so that the scheduler add it to actions objects)\n        :type worker_name: str\n        :param module_types: Module type to filter actions/checks\n        :type module_types: list\n        :return: serialized check/action list\n        :rtype: str\n        \"\"\"\n        if poller_tags is None:\n            poller_tags = ['None']\n        if reactionner_tags is None:\n            reactionner_tags = ['None']\n        if module_types is None:\n            module_types = ['fork']\n        do_checks = (do_checks == 'True')\n        do_actions = (do_actions == 'True')\n        res = self.app.sched.get_to_run_checks(do_checks, do_actions, poller_tags, reactionner_tags,\n                                               worker_name, module_types)\n\n        return serialize(res, True)", "response": "Get the checks from the scheduler"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put_results(self):\n        res = cherrypy.request.json\n        who_sent = res['from']\n        results = res['results']\n\n        results = unserialize(results, no_load=True)\n        if results:\n            logger.debug(\"Got some results: %d results from %s\", len(results), who_sent)\n        else:\n            logger.debug(\"-> no results\")\n\n        for result in results:\n            logger.debug(\"-> result: %s\", result)\n\n            # Append to the scheduler result queue\n            self.app.sched.waiting_results.put(result)\n\n        return True", "response": "Put results to scheduler"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npost external commands to scheduler", "response": "def _run_external_commands(self):\n        \"\"\"Post external_commands to scheduler (from arbiter)\n        Wrapper to to app.sched.run_external_commands method\n\n        :return: None\n        \"\"\"\n        commands = cherrypy.request.json\n        with self.app.lock:\n            self.app.sched.run_external_commands(commands['cmds'])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_objects(self, o_type):\n        if o_type not in [t for t in self.app.sched.pushed_conf.types_creations]:\n            return None\n\n        try:\n            _, _, strclss, _, _ = self.app.sched.pushed_conf.types_creations[o_type]\n            o_list = getattr(self.app.sched, strclss)\n        except Exception:  # pylint: disable=broad-except\n            return None\n\n        return o_list", "response": "Get an object list from the scheduler"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget an object from the scheduler", "response": "def _get_object(self, o_type, o_name=None):\n        \"\"\"Get an object from the scheduler\n\n        Returns None if the required object type (`o_type`) is not known.\n        Else returns the serialized object if found. The object is searched first with\n        o_name as its name and then with o_name as its uuid.\n\n        :param o_type: searched object type\n        :type o_type: str\n        :param name: searched object name\n        :type name: str\n        :return: serialized object\n        :rtype: str\n        \"\"\"\n        try:\n            o_found = None\n            o_list = self._get_objects(o_type)\n            if o_list:\n                if o_name is None:\n                    return serialize(o_list, True) if o_list else None\n                # We expected a name...\n                o_found = o_list.find_by_name(o_name)\n                if not o_found:\n                    # ... but perharps we got an object uuid\n                    o_found = o_list[o_name]\n        except Exception:  # pylint: disable=broad-except\n            return None\n        return serialize(o_found, True) if o_found else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_a_module(self, module_type):\n        if hasattr(self, 'type'):\n            return module_type in self.type\n        return module_type in self.module_types", "response": "Check if the module of the required type?"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize this object into a dictionary.", "response": "def serialize(self):\n        \"\"\"A module may have some properties that are not defined in the class properties list.\n        Serializing a module is the same as serializing an Item but we also also include all the\n        existing properties that are not defined in the properties or running_properties\n        class list.\n\n        We must also exclude the reference to the daemon that loaded the module!\n        \"\"\"\n        res = super(Module, self).serialize()\n\n        cls = self.__class__\n        for prop in self.__dict__:\n            if prop in cls.properties or prop in cls.running_properties or prop in ['properties',\n                                                                                    'my_daemon']:\n                continue\n            res[prop] = getattr(self, prop)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlink a module to some other modules", "response": "def linkify_s_by_plug(self):\n        \"\"\"Link a module to some other modules\n\n        :return: None\n        \"\"\"\n        for module in self:\n            new_modules = []\n            for related in getattr(module, 'modules', []):\n                related = related.strip()\n                if not related:\n                    continue\n                o_related = self.find_by_name(related)\n                if o_related is not None:\n                    new_modules.append(o_related.uuid)\n                else:\n                    self.add_error(\"the module '%s' for the module '%s' is unknown!\"\n                                   % (related, module.get_name()))\n            module.modules = new_modules"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_start_of_day(year, month, day):\n    # DST is not known in the provided date\n    try:\n        timestamp = time.mktime((year, month, day, 00, 00, 00, 0, 0, -1))\n    except (OverflowError, ValueError):\n        # Windows mktime sometimes crashes on (1970, 1, 1, ...)\n        timestamp = 0.0\n\n    return int(timestamp)", "response": "Get the timestamp associated to the first second of a specific day of the current date."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the timestamp associated to the last second of a specific day of the date", "response": "def get_end_of_day(year, month, day):\n    \"\"\"Get the timestamp associated to the last second of a specific day\n\n    :param year: date year\n    :type year: int\n    :param month: date month (int)\n    :type month: int\n    :param day: date day\n    :type day: int\n    :return: timestamp\n    :rtype: int\n    \"\"\"\n    # DST is not known in the provided date\n    timestamp = time.mktime((year, month, day, 23, 59, 59, 0, 0, -1))\n    return int(timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sec_from_morning(timestamp):\n    t_lt = time.localtime(timestamp)\n    return t_lt.tm_hour * 3600 + t_lt.tm_min * 60 + t_lt.tm_sec", "response": "Get the number of seconds elapsed since the beginning of the\n    day deducted from the provided timestamp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the day number of a given weekday and offset.", "response": "def find_day_by_weekday_offset(year, month, weekday, offset):\n    \"\"\"Get the day number based on a date and offset\n\n    :param year: date year\n    :type year: int\n    :param month: date month\n    :type month: int\n    :param weekday: date week day\n    :type weekday: int\n    :param offset: offset (-1 is last, 1 is first etc)\n    :type offset: int\n    :return: day number in the month\n    :rtype: int\n\n    >>> find_day_by_weekday_offset(2010, 7, 1, -1)\n    27\n    \"\"\"\n    # thanks calendar :)\n    cal = calendar.monthcalendar(year, month)\n\n    # If we ask for a -1 day, just reverse cal\n    if offset < 0:\n        offset = abs(offset)\n        cal.reverse()\n\n    # ok go for it\n    nb_found = 0\n    try:\n        for i in range(0, offset + 1):\n            # in cal 0 mean \"there are no day here :)\"\n            if cal[i][weekday] != 0:\n                nb_found += 1\n            if nb_found == offset:\n                return cal[i][weekday]\n        return None\n    except KeyError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_day_by_offset(year, month, offset):\n    (_, days_in_month) = calendar.monthrange(year, month)\n    if offset >= 0:\n        return min(offset, days_in_month)\n\n    return max(1, days_in_month + offset + 1)", "response": "Return the day number in the month based on date and offset in day"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_first_sec_out_from_morning(self):\n        # If start at 0:0, the min out is the end\n        if self.hstart == 0 and self.mstart == 0:\n            return self.hend * 3600 + self.mend * 60\n        return 0", "response": "Get the first second from midnight where we are out of the timerange"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_time_valid(self, timestamp):\n        sec_from_morning = get_sec_from_morning(timestamp)\n        return (self.is_valid and\n                self.hstart * 3600 + self.mstart * 60 <=\n                sec_from_morning <=\n                self.hend * 3600 + self.mend * 60)", "response": "Check if time is valid for this Timerange\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if time is valid for one of the timerange.", "response": "def is_time_valid(self, timestamp):\n        \"\"\"Check if time is valid for one of the timerange.\n\n        :param timestamp: time to check\n        :type timestamp: int\n        :return: True if one of the timerange is valid for t, False otherwise\n        :rtype: bool\n        \"\"\"\n        if self.is_time_day_valid(timestamp):\n            for timerange in self.timeranges:\n                if timerange.is_time_valid(timestamp):\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the first second from midnight where a timerange is effective", "response": "def get_min_sec_from_morning(self):\n        \"\"\"Get the first second from midnight where a timerange is effective\n\n        :return: smallest amount of second from midnight of all timerange\n        :rtype: int\n        \"\"\"\n        mins = []\n        for timerange in self.timeranges:\n            mins.append(timerange.get_sec_from_morning())\n        return min(mins)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_min_sec_out_from_morning(self):\n        mins = []\n        for timerange in self.timeranges:\n            mins.append(timerange.get_first_sec_out_from_morning())\n        return min(mins)", "response": "Get the first second from midnight of all timeranges where we are not effective"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the next time from t where a timerange is valid", "response": "def get_min_from_t(self, timestamp):\n        \"\"\"Get next time from t where a timerange is valid (withing range)\n\n        :param timestamp: base time to look for the next one\n        :return: time where a timerange is valid\n        :rtype: int\n        \"\"\"\n        if self.is_time_valid(timestamp):\n            return timestamp\n        t_day_epoch = get_day(timestamp)\n        tr_mins = self.get_min_sec_from_morning()\n        return t_day_epoch + tr_mins"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the time day t is within start time and end time of the DateRange", "response": "def is_time_day_valid(self, timestamp):\n        \"\"\"Check if it is within start time and end time of the DateRange\n\n        :param timestamp: time to check\n        :type timestamp: int\n        :return: True if t in range, False otherwise\n        :rtype: bool\n        \"\"\"\n        (start_time, end_time) = self.get_start_and_end_time(timestamp)\n        return start_time <= timestamp <= end_time"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_next_future_timerange_valid(self, timestamp):\n        sec_from_morning = get_sec_from_morning(timestamp)\n        starts = []\n        for timerange in self.timeranges:\n            tr_start = timerange.hstart * 3600 + timerange.mstart * 60\n            if tr_start >= sec_from_morning:\n                starts.append(tr_start)\n        if starts != []:\n            return min(starts)\n\n        return None", "response": "Get the next valid timerange in the timeranges attribute"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget next invalid time for timeranges", "response": "def get_next_future_timerange_invalid(self, timestamp):\n        \"\"\"Get next invalid time for timeranges\n\n        :param timestamp: time to check\n        :type timestamp: int\n        :return: next time when a timerange is not valid\n        :rtype: None | int\n        \"\"\"\n        sec_from_morning = get_sec_from_morning(timestamp)\n        ends = []\n        for timerange in self.timeranges:\n            tr_end = timerange.hend * 3600 + timerange.mend * 60\n            if tr_end >= sec_from_morning:\n                # Remove the last second of the day for 00->24h\"\n                if tr_end == 86400:\n                    tr_end = 86399\n                ends.append(tr_end)\n        if ends != []:\n            return min(ends)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets next valid day for timerange", "response": "def get_next_valid_day(self, timestamp):\n        \"\"\"Get next valid day for timerange\n\n        :param timestamp: time we compute from\n        :type timestamp: int\n        :return: timestamp of the next valid day (midnight) in LOCAL time.\n        :rtype: int | None\n        \"\"\"\n        if self.get_next_future_timerange_valid(timestamp) is None:\n            # this day is finish, we check for next period\n            (start_time, _) = self.get_start_and_end_time(get_day(timestamp) + 86400)\n        else:\n            (start_time, _) = self.get_start_and_end_time(timestamp)\n\n        if timestamp <= start_time:\n            return get_day(start_time)\n\n        if self.is_time_day_valid(timestamp):\n            return get_day(timestamp)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the next valid time for time range from a given timestamp.", "response": "def get_next_valid_time_from_t(self, timestamp):\n        \"\"\"Get next valid time for time range\n\n        :param timestamp: time we compute from\n        :type timestamp: int\n        :return: timestamp of the next valid time (LOCAL TIME)\n        :rtype: int | None\n        \"\"\"\n        if self.is_time_valid(timestamp):\n            return timestamp\n\n        # First we search for the day of t\n        t_day = self.get_next_valid_day(timestamp)\n        if t_day is None:\n            return t_day\n\n        # We search for the min of all tr.start > sec_from_morning\n        # if it's the next day, use a start of the day search for timerange\n        if timestamp < t_day:\n            sec_from_morning = self.get_next_future_timerange_valid(t_day)\n        else:  # it is in this day, so look from t (can be in the evening or so)\n            sec_from_morning = self.get_next_future_timerange_valid(timestamp)\n\n        if sec_from_morning is not None:\n            if t_day is not None and sec_from_morning is not None:\n                return t_day + sec_from_morning\n\n        # Then we search for the next day of t\n        # The sec will be the min of the day\n        timestamp = get_day(timestamp) + 86400\n        t_day2 = self.get_next_valid_day(timestamp)\n        sec_from_morning = self.get_next_future_timerange_valid(t_day2)\n        if t_day2 is not None and sec_from_morning is not None:\n            return t_day2 + sec_from_morning\n\n        # I did not found any valid time\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_next_invalid_day(self, timestamp):\n        # pylint: disable=no-else-return\n        \"\"\"Get next day where timerange is not active\n\n        :param timestamp: time we compute from\n        :type timestamp: int\n        :return: timestamp of the next invalid day (midnight) in LOCAL time.\n        :rtype: int | None\n        \"\"\"\n        if self.is_time_day_invalid(timestamp):\n            return timestamp\n\n        next_future_timerange_invalid = self.get_next_future_timerange_invalid(timestamp)\n\n        # If today there is no more unavailable timerange, search the next day\n        if next_future_timerange_invalid is None:\n            # this day is finish, we check for next period\n            (start_time, end_time) = self.get_start_and_end_time(get_day(timestamp))\n        else:\n            (start_time, end_time) = self.get_start_and_end_time(timestamp)\n\n        # (start_time, end_time) = self.get_start_and_end_time(t)\n\n        # The next invalid day can be t day if there a possible\n        # invalid time range (timerange is not 00->24\n        if next_future_timerange_invalid is not None:\n            if start_time <= timestamp <= end_time:\n                return get_day(timestamp)\n            if start_time >= timestamp:\n                return get_day(start_time)\n        else:\n            # Else, there is no possibility than in our start_time<->end_time we got\n            # any invalid time (full period out). So it's end_time+1 sec (tomorrow of end_time)\n            return get_day(end_time + 1)\n        return None", "response": "Get next invalid day in LOCAL time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_next_invalid_time_from_t(self, timestamp):\n        if not self.is_time_valid(timestamp):\n            return timestamp\n\n        # First we search for the day of time range\n        t_day = self.get_next_invalid_day(timestamp)\n\n        # We search for the min of all tr.start > sec_from_morning\n        # if it's the next day, use a start of the day search for timerange\n        if timestamp < t_day:\n            sec_from_morning = self.get_next_future_timerange_invalid(t_day)\n        else:  # it is in this day, so look from t (can be in the evening or so)\n            sec_from_morning = self.get_next_future_timerange_invalid(timestamp)\n\n        # tr can't be valid, or it will be return at the beginning\n        # sec_from_morning = self.get_next_future_timerange_invalid(t)\n\n        # Ok we've got a next invalid day and a invalid possibility in\n        # timerange, so the next invalid is this day+sec_from_morning\n        if t_day is not None and sec_from_morning is not None:\n            return t_day + sec_from_morning + 1\n\n        # We've got a day but no sec_from_morning: the timerange is full (0->24h)\n        # so the next invalid is this day at the day_start\n        if t_day is not None and sec_from_morning is None:\n            return t_day\n\n        # Then we search for the next day of t\n        # The sec will be the min of the day\n        timestamp = get_day(timestamp) + 86400\n        t_day2 = self.get_next_invalid_day(timestamp)\n        sec_from_morning = self.get_next_future_timerange_invalid(t_day2)\n        if t_day2 is not None and sec_from_morning is not None:\n            return t_day2 + sec_from_morning + 1\n\n        if t_day2 is not None and sec_from_morning is None:\n            return t_day2\n\n        # I did not found any valid time\n        return None", "response": "Get next invalid time for time range and time from a given timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_start_and_end_time(self, ref=None):\n        return (get_start_of_day(self.syear, int(self.smon), self.smday),\n                get_end_of_day(self.eyear, int(self.emon), self.emday))", "response": "Specific function to get start and end time for CalendarDaterange\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize(self):\n        return {'day': self.day, 'other': self.other,\n                'timeranges': [t.serialize() for t in self.timeranges]}", "response": "This function serialize into a simple dict object. It is used when transferring data to other daemons."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_correct(self):\n        valid = self.day in Daterange.weekdays\n        if not valid:\n            logger.error(\"Error: %s is not a valid day\", self.day)\n        # Check also if Daterange is correct.\n        valid &= super(StandardDaterange, self).is_correct()\n        return valid", "response": "Check if the Daterange is correct."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_start_and_end_time(self, ref=None):\n        now = time.localtime(ref)\n        self.syear = now.tm_year\n        self.month = now.tm_mon\n        self.wday = now.tm_wday\n        day_id = Daterange.get_weekday_id(self.day)\n        today_morning = get_start_of_day(now.tm_year, now.tm_mon, now.tm_mday)\n        tonight = get_end_of_day(now.tm_year, now.tm_mon, now.tm_mday)\n        day_diff = (day_id - now.tm_wday) % 7\n        morning = datetime.fromtimestamp(today_morning) + timedelta(days=day_diff)\n        night = datetime.fromtimestamp(tonight) + timedelta(days=day_diff)\n        return (int(morning.strftime(\"%s\")), int(night.strftime(\"%s\")))", "response": "Specific function to get start and end time for StandardDaterange\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_correct(self):\n        valid = True\n        valid &= self.swday in range(7)\n        if not valid:\n            logger.error(\"Error: %s is not a valid day\", self.swday)\n\n        valid &= self.ewday in range(7)\n        if not valid:\n            logger.error(\"Error: %s is not a valid day\", self.ewday)\n\n        return valid", "response": "Check if the Daterange is correct."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_start_and_end_time(self, ref=None):\n        now = time.localtime(ref)\n        if self.syear == 0:\n            self.syear = now.tm_year\n        day_start = find_day_by_offset(self.syear, self.smon, self.smday)\n        start_time = get_start_of_day(self.syear, self.smon, day_start)\n\n        if self.eyear == 0:\n            self.eyear = now.tm_year\n        day_end = find_day_by_offset(self.eyear, self.emon, self.emday)\n        end_time = get_end_of_day(self.eyear, self.emon, day_end)\n\n        now_epoch = time.mktime(now)\n        if start_time > end_time:  # the period is between years\n            if now_epoch > end_time:\n                # check for next year\n                day_end = find_day_by_offset(self.eyear + 1, self.emon, self.emday)\n                end_time = get_end_of_day(self.eyear + 1, self.emon, day_end)\n            else:\n                # it s just that start was the last year\n                day_start = find_day_by_offset(self.syear - 1, self.smon, self.emday)\n                start_time = get_start_of_day(self.syear - 1, self.smon, day_start)\n        else:\n            if now_epoch > end_time:\n                # just have to check for next year if necessary\n                day_start = find_day_by_offset(self.syear + 1, self.smon, self.smday)\n                start_time = get_start_of_day(self.syear + 1, self.smon, day_start)\n                day_end = find_day_by_offset(self.eyear + 1, self.emon, self.emday)\n                end_time = get_end_of_day(self.eyear + 1, self.emon, day_end)\n\n        return (start_time, end_time)", "response": "Specific function to get start and end time for MonthDateDaterange\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_start_and_end_time(self, ref=None):\n        now = time.localtime(ref)\n        if self.syear == 0:\n            self.syear = now.tm_year\n        month_start_id = now.tm_mon\n        day_start = find_day_by_offset(self.syear, month_start_id, self.smday)\n        start_time = get_start_of_day(self.syear, month_start_id, day_start)\n\n        if self.eyear == 0:\n            self.eyear = now.tm_year\n        month_end_id = now.tm_mon\n        day_end = find_day_by_offset(self.eyear, month_end_id, self.emday)\n        end_time = get_end_of_day(self.eyear, month_end_id, day_end)\n\n        now_epoch = time.mktime(now)\n\n        if start_time > end_time:\n            month_start_id -= 1\n            if month_start_id < 1:\n                month_start_id = 12\n                self.syear -= 1\n        day_start = find_day_by_offset(self.syear, month_start_id, self.smday)\n        start_time = get_start_of_day(self.syear, month_start_id, day_start)\n\n        if end_time < now_epoch:\n            month_end_id += 1\n            month_start_id += 1\n            if month_end_id > 12:\n                month_end_id = 1\n                self.eyear += 1\n            if month_start_id > 12:\n                month_start_id = 1\n                self.syear += 1\n\n            # For the start\n            day_start = find_day_by_offset(self.syear, month_start_id, self.smday)\n            start_time = get_start_of_day(self.syear, month_start_id, day_start)\n\n            # For the end\n            day_end = find_day_by_offset(self.eyear, month_end_id, self.emday)\n            end_time = get_end_of_day(self.eyear, month_end_id, day_end)\n\n        return (start_time, end_time)", "response": "Specific function to get start and end time for MonthDayDaterange\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending an element to our daemon", "response": "def send_an_element(self, element):\n        \"\"\"Send an element (Brok, Comment,...) to our daemon\n\n        Use the daemon `add` function if it exists, else raise an error log\n\n        :param element: elementto be sent\n        :type: alignak.Brok, or Comment, or Downtime, ...\n        :return:\n        \"\"\"\n        # Comment this log because it raises an encoding exception on Travis CI with python 2.7!\n        # logger.debug(\"Sending to %s for %s\", self.daemon, element)\n        if hasattr(self.daemon, \"add\"):\n            func = getattr(self.daemon, \"add\")\n            if isinstance(func, collections.Callable):\n                try:\n                    func(element)\n                except Exception as exp:  # pylint: disable=broad-except\n                    logger.critical(\"Daemon report exception: %s\", exp)\n                return\n\n        logger.critical(\"External command or Brok could not be sent to any daemon!\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse and dispatch the command and return the result.", "response": "def resolve_command(self, excmd):\n        \"\"\"Parse command and dispatch it (to schedulers for example) if necessary\n        If the command is not global it will be executed.\n\n        :param excmd: external command to handle\n        :type excmd: alignak.external_command.ExternalCommand\n        :return: result of command parsing. None for an invalid command.\n        \"\"\"\n        # Maybe the command is invalid. Bailout\n        try:\n            command = excmd.cmd_line\n        except AttributeError as exp:  # pragma: no cover, simple protection\n            logger.warning(\"resolve_command, error with command %s\", excmd)\n            logger.exception(\"Exception: %s\", exp)\n            return None\n\n        # Parse command\n        command = command.strip()\n        cmd = self.get_command_and_args(command, excmd)\n        if cmd is None:\n            return cmd\n\n        # If we are a receiver, bail out here... do not try to execute the command\n        if self.mode == 'receiver' and not cmd.get('internal', False):\n            return cmd\n\n        if self.mode == 'applyer' and self.log_external_commands:\n            make_a_log = True\n            # #912: only log an external command if it is not a passive check\n            if self.my_conf.log_passive_checks and cmd['c_name'] \\\n                    in ['process_host_check_result', 'process_service_check_result']:\n                # Do not log the command\n                make_a_log = False\n\n            if make_a_log:\n                # I am a command dispatcher, notifies to my arbiter\n                self.send_an_element(make_monitoring_log('info', 'EXTERNAL COMMAND: ' + command))\n\n        if not cmd['global']:\n            # Execute the command\n            c_name = cmd['c_name']\n            args = cmd['args']\n            logger.debug(\"Execute command: %s %s\", c_name, str(args))\n            logger.debug(\"Command time measurement: %s (%d s)\",\n                         excmd.creation_timestamp, time.time() - excmd.creation_timestamp)\n            statsmgr.timer('external-commands.latency', time.time() - excmd.creation_timestamp)\n            getattr(self, c_name)(*args)\n        else:\n            # Send command to all our schedulers\n            for scheduler_link in self.my_conf.schedulers:\n                logger.debug(\"Preparing an external command '%s' for the scheduler %s\",\n                             excmd, scheduler_link.name)\n                scheduler_link.pushed_commands.append(excmd.cmd_line)\n\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch the hosts list and dispatch a command to the scheduler", "response": "def search_host_and_dispatch(self, host_name, command, extcmd):\n        # pylint: disable=too-many-branches\n        \"\"\"Try to dispatch a command for a specific host (so specific scheduler)\n        because this command is related to a host (change notification interval for example)\n\n        :param host_name: host name to search\n        :type host_name: str\n        :param command: command line\n        :type command: str\n        :param extcmd:  external command object (the object will be added to sched commands list)\n        :type extcmd: alignak.external_command.ExternalCommand\n        :return: None\n        \"\"\"\n        logger.debug(\"Calling search_host_and_dispatch for %s\", host_name)\n        host_found = False\n\n        # If we are a receiver, just look in the receiver\n        if self.mode == 'receiver':\n            logger.debug(\"Receiver is searching a scheduler for the external command %s %s\",\n                         host_name, command)\n            scheduler_link = self.daemon.get_scheduler_from_hostname(host_name)\n            if scheduler_link:\n                host_found = True\n                logger.debug(\"Receiver pushing external command to scheduler %s\",\n                             scheduler_link.name)\n                scheduler_link.pushed_commands.append(extcmd)\n            else:\n                logger.warning(\"I did not found a scheduler for the host: %s\", host_name)\n        else:\n            for cfg_part in list(self.cfg_parts.values()):\n                if cfg_part.hosts.find_by_name(host_name) is not None:\n                    logger.debug(\"Host %s found in a configuration\", host_name)\n                    if cfg_part.is_assigned:\n                        host_found = True\n                        scheduler_link = cfg_part.scheduler_link\n                        logger.debug(\"Sending command to the scheduler %s\", scheduler_link.name)\n                        scheduler_link.push_external_commands([command])\n                        # scheduler_link.my_daemon.external_commands.append(command)\n                        break\n                    else:\n                        logger.warning(\"Problem: the host %s was found in a configuration, \"\n                                       \"but this configuration is not assigned to any scheduler!\",\n                                       host_name)\n        if not host_found:\n            if self.accept_passive_unknown_check_results:\n                brok = self.get_unknown_check_result_brok(command)\n                if brok:\n                    self.send_an_element(brok)\n                else:\n                    logger.warning(\"External command was received for the host '%s', \"\n                                   \"but the host could not be found! Command is: %s\",\n                                   host_name, command)\n            else:\n                logger.warning(\"External command was received for host '%s', \"\n                               \"but the host could not be found!\", host_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_unknown_check_result_brok(cmd_line):\n        match = re.match(\n            r'^\\[([0-9]{10})] PROCESS_(SERVICE)_CHECK_RESULT;'\n            r'([^\\;]*);([^\\;]*);([^\\;]*);([^\\|]*)(?:\\|(.*))?', cmd_line)\n        if not match:\n            match = re.match(\n                r'^\\[([0-9]{10})] PROCESS_(HOST)_CHECK_RESULT;'\n                r'([^\\;]*);([^\\;]*);([^\\|]*)(?:\\|(.*))?', cmd_line)\n\n        if not match:\n            return None\n\n        data = {\n            'time_stamp': int(match.group(1)),\n            'host_name': match.group(3),\n        }\n\n        if match.group(2) == 'SERVICE':\n            data['service_description'] = match.group(4)\n            data['return_code'] = match.group(5)\n            data['output'] = match.group(6)\n            data['perf_data'] = match.group(7)\n        else:\n            data['return_code'] = match.group(4)\n            data['output'] = match.group(5)\n            data['perf_data'] = match.group(6)\n\n        return Brok({'type': 'unknown_%s_check_result' % match.group(2).lower(), 'data': data})", "response": "Create an unknown check result brok and fill it with command data\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse command and return command and args.", "response": "def get_command_and_args(self, command, extcmd=None):\n        # pylint: disable=too-many-return-statements, too-many-nested-blocks\n        # pylint: disable=too-many-locals,too-many-branches,too-many-statements\n        \"\"\"Parse command and get args\n\n        :param command: command line to parse\n        :type command: str\n        :param extcmd: external command object (used to dispatch)\n        :type extcmd: None | object\n        :return: Dict containing command and arg ::\n\n        {'global': False, 'c_name': c_name, 'args': args}\n\n        :rtype: dict | None\n        \"\"\"\n        # danger!!! passive check results with perfdata\n        elts = split_semicolon(command)\n\n        try:\n            timestamp, c_name = elts[0].split()\n        except ValueError as exp:\n            splitted_command = elts[0].split()\n            if len(splitted_command) == 1:\n                # Assume no timestamp and only a command\n                timestamp = \"[%s]\" % int(time.time())\n                logger.warning(\"Missing timestamp in command '%s', using %s as a timestamp.\",\n                               elts[0], timestamp)\n                c_name = elts[0].split()[0]\n            else:\n                logger.warning(\"Malformed command '%s'\", command)\n                # logger.exception(\"Malformed command exception: %s\", exp)\n\n                if self.log_external_commands:\n                    # The command failed, make a monitoring log to inform\n                    self.send_an_element(make_monitoring_log(\n                        'error', \"Malformed command: '%s'\" % command))\n                return None\n\n        c_name = c_name.lower()\n\n        # Is timestamp already an integer value?\n        try:\n            timestamp = int(timestamp)\n        except ValueError as exp:\n            # Else, remove enclosing characters: [], (), {}, ...\n            timestamp = timestamp[1:-1]\n\n        # Finally, check that the timestamp is really a timestamp\n        try:\n            self.current_timestamp = int(timestamp)\n        except ValueError as exp:\n            logger.warning(\"Malformed command '%s'\", command)\n            # logger.exception(\"Malformed command exception: %s\", exp)\n\n            if self.log_external_commands:\n                # The command failed, make a monitoring log to inform\n                self.send_an_element(make_monitoring_log(\n                    'error', \"Malformed command: '%s'\" % command))\n            return None\n\n        if c_name not in ExternalCommandManager.commands:\n            logger.warning(\"External command '%s' is not recognized, sorry\", c_name)\n\n            if self.log_external_commands:\n                # The command failed, make a monitoring log to inform\n                self.send_an_element(make_monitoring_log(\n                    'error', \"Command '%s' is not recognized, sorry\" % command))\n            return None\n\n        # Split again based on the number of args we expect. We cannot split\n        # on every ; because this character may appear in the perfdata of\n        # passive check results.\n        entry = ExternalCommandManager.commands[c_name]\n\n        # Look if the command is purely internal (Alignak) or not\n        internal = False\n        if 'internal' in entry and entry['internal']:\n            internal = True\n\n        numargs = len(entry['args'])\n        if numargs and 'service' in entry['args']:\n            numargs += 1\n        elts = split_semicolon(command, numargs)\n\n        logger.debug(\"mode= %s, global= %s\", self.mode, str(entry['global']))\n        if self.mode in ['dispatcher', 'receiver'] and entry['global']:\n            if not internal:\n                logger.debug(\"Command '%s' is a global one, we resent it to all schedulers\", c_name)\n                return {'global': True, 'cmd': command}\n\n        args = []\n        i = 1\n        in_service = False\n        tmp_host = ''\n        obsolete_arg = 0\n        try:\n            for elt in elts[1:]:\n                try:\n                    elt = elt.decode('utf8', 'ignore')\n                except AttributeError:\n                    # Python 3 will raise an error...\n                    pass\n                except UnicodeEncodeError:\n                    pass\n                logger.debug(\"Searching for a new arg: %s (%d)\", elt, i)\n                val = elt.strip()\n                if val.endswith('\\n'):\n                    val = val[:-1]\n\n                logger.debug(\"For command arg: %s\", val)\n\n                if not in_service:\n                    type_searched = entry['args'][i - 1]\n                    logger.debug(\"Type searched: %s\", type_searched)\n\n                    if type_searched == 'host':\n                        if self.mode == 'dispatcher' or self.mode == 'receiver':\n                            self.search_host_and_dispatch(val, command, extcmd)\n                            return None\n                        host = self.hosts.find_by_name(val)\n                        if host is None:\n                            if self.accept_passive_unknown_check_results:\n                                brok = self.get_unknown_check_result_brok(command)\n                                if brok:\n                                    self.daemon.add_brok(brok)\n                            else:\n                                logger.warning(\"A command was received for the host '%s', \"\n                                               \"but the host could not be found!\", val)\n                            return None\n\n                        args.append(host)\n\n                    elif type_searched == 'contact':\n                        contact = self.contacts.find_by_name(val)\n                        if contact is not None:\n                            args.append(contact)\n\n                    elif type_searched == 'time_period':\n                        timeperiod = self.timeperiods.find_by_name(val)\n                        if timeperiod is not None:\n                            args.append(timeperiod)\n\n                    elif type_searched == 'obsolete':\n                        obsolete_arg += 1\n\n                    elif type_searched == 'to_bool':\n                        args.append(to_bool(val))\n\n                    elif type_searched == 'to_int':\n                        args.append(to_int(val))\n\n                    elif type_searched in ('author', None):\n                        args.append(val)\n\n                    elif type_searched == 'command':\n                        command = self.commands.find_by_name(val)\n                        if command is not None:\n                            # the find will be redone by\n                            # the commandCall creation, but != None\n                            # is useful so a bad command will be caught\n                            args.append(val)\n\n                    elif type_searched == 'host_group':\n                        hostgroup = self.hostgroups.find_by_name(val)\n                        if hostgroup is not None:\n                            args.append(hostgroup)\n\n                    elif type_searched == 'service_group':\n                        servicegroup = self.servicegroups.find_by_name(val)\n                        if servicegroup is not None:\n                            args.append(servicegroup)\n\n                    elif type_searched == 'contact_group':\n                        contactgroup = self.contactgroups.find_by_name(val)\n                        if contactgroup is not None:\n                            args.append(contactgroup)\n\n                    # special case: service are TWO args host;service, so one more loop\n                    # to get the two parts\n                    elif type_searched == 'service':\n                        in_service = True\n                        tmp_host = elt.strip()\n                        if tmp_host[-1] == '\\n':\n                            tmp_host = tmp_host[:-1]\n                        if self.mode == 'dispatcher':\n                            self.search_host_and_dispatch(tmp_host, command, extcmd)\n                            return None\n\n                    i += 1\n                else:\n                    in_service = False\n                    srv_name = elt\n                    if srv_name[-1] == '\\n':\n                        srv_name = srv_name[:-1]\n                    # If we are in a receiver, bailout now.\n                    if self.mode == 'receiver':\n                        self.search_host_and_dispatch(tmp_host, command, extcmd)\n                        return None\n\n                    serv = self.services.find_srv_by_name_and_hostname(tmp_host, srv_name)\n                    if serv is None:\n                        if self.accept_passive_unknown_check_results:\n                            brok = self.get_unknown_check_result_brok(command)\n                            self.send_an_element(brok)\n                        else:\n                            logger.warning(\"A command was received for the service '%s' on \"\n                                           \"host '%s', but the service could not be found!\",\n                                           srv_name, tmp_host)\n                        return None\n\n                    args.append(serv)\n            logger.debug(\"Got args: %s\", args)\n\n        except IndexError as exp:\n            logger.warning(\"Sorry, the arguments for the command '%s' are not correct\")\n            logger.exception(\"Arguments parsing exception: %s\", exp)\n\n            if self.log_external_commands:\n                # The command failed, make a monitoring log to inform\n                self.send_an_element(make_monitoring_log(\n                    'error', \"Arguments are not correct for the command: '%s'\" % command))\n        else:\n            if len(args) == (len(entry['args']) - obsolete_arg):\n                return {\n                    'global': False, 'internal': internal,\n                    'c_name': c_name, 'args': args\n                }\n\n            logger.warning(\"Sorry, the arguments for the command '%s' are not correct (%s)\",\n                           command, (args))\n\n            if self.log_external_commands:\n                # The command failed, make a monitoring log to inform\n                self.send_an_element(make_monitoring_log(\n                    'error', \"Arguments are not correct for the command: '%s'\" % command))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef change_contact_host_notification_timeperiod(self, contact, notification_timeperiod):\n        # todo: deprecate this\n        contact.modified_host_attributes |= DICT_MODATTR[\"MODATTR_NOTIFICATION_TIMEPERIOD\"].value\n        contact.host_notification_period = notification_timeperiod\n        self.send_an_element(contact.get_update_status_brok())", "response": "Change the contact host notification timeperiod value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_svc_comment(self, service, author, comment):\n        data = {\n            'author': author, 'comment': comment, 'comment_type': 2, 'entry_type': 1, 'source': 1,\n            'expires': False, 'ref': service.uuid\n        }\n        comm = Comment(data)\n        service.add_comment(comm)\n        self.send_an_element(service.get_update_status_brok())\n\n        try:\n            brok = make_monitoring_log('info', \"SERVICE COMMENT: %s;%s;%s;%s\"\n                                       % (self.hosts[service.host].get_name(),\n                                          service.get_name(),\n                                          str(author, 'utf-8'), str(comment, 'utf-8')))\n        except TypeError:\n            brok = make_monitoring_log('info', \"SERVICE COMMENT: %s;%s;%s;%s\"\n                                       % (self.hosts[service.host].get_name(),\n                                          service.get_name(), author, comment))\n\n        self.send_an_element(brok)\n        self.send_an_element(comm.get_comment_brok(\n            self.hosts[service.host].get_name(), service.get_name()))", "response": "This function adds a comment to the service object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a host comment Format of the line that triggers function call:: ADD_HOST_COMMENT;<host_name>;<persistent:obsolete>;<author>;<comment> :param host: host to add the comment :type host: alignak.objects.host.Host :param author: author name :type author: str :param comment: text comment :type comment: str :return: None", "response": "def add_host_comment(self, host, author, comment):\n        \"\"\"Add a host comment\n        Format of the line that triggers function call::\n\n        ADD_HOST_COMMENT;<host_name>;<persistent:obsolete>;<author>;<comment>\n\n        :param host: host to add the comment\n        :type host: alignak.objects.host.Host\n        :param author: author name\n        :type author: str\n        :param comment: text comment\n        :type comment: str\n        :return: None\n        \"\"\"\n        data = {\n            'author': author, 'comment': comment, 'comment_type': 1, 'entry_type': 1, 'source': 1,\n            'expires': False, 'ref': host.uuid\n        }\n        comm = Comment(data)\n        host.add_comment(comm)\n        self.send_an_element(host.get_update_status_brok())\n\n        try:\n            brok = make_monitoring_log('info', \"HOST COMMENT: %s;%s;%s\"\n                                       % (host.get_name(),\n                                          str(author, 'utf-8'), str(comment, 'utf-8')))\n        except TypeError:\n            brok = make_monitoring_log('info', \"HOST COMMENT: %s;%s;%s\"\n                                       % (host.get_name(), author, comment))\n\n        self.send_an_element(brok)\n        self.send_an_element(comm.get_comment_brok(self.hosts[host].get_name()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nacknowledges a service problem Format of the line that triggers function call:: ACKNOWLEDGE_SVC_PROBLEM;<host_name>;<service_description>;<sticky>;<notify>; <persistent:obsolete>;<author>;<comment> :param service: service to acknowledge the problem :type service: alignak.objects.service.Service :param sticky: if sticky == 2, the acknowledge will remain until the service returns to an OK state else the acknowledge will be removed as soon as the service state changes :param notify: if to 1, send a notification :type notify: integer :param author: name of the author or the acknowledge :type author: str :param comment: comment (description) of the acknowledge :type comment: str :return: None", "response": "def acknowledge_svc_problem(self, service, sticky, notify, author, comment):\n        \"\"\"Acknowledge a service problem\n        Format of the line that triggers function call::\n\n        ACKNOWLEDGE_SVC_PROBLEM;<host_name>;<service_description>;<sticky>;<notify>;\n        <persistent:obsolete>;<author>;<comment>\n\n        :param service: service to acknowledge the problem\n        :type service: alignak.objects.service.Service\n        :param sticky: if sticky == 2, the acknowledge will remain until the service returns to an\n        OK state else the acknowledge will be removed as soon as the service state changes\n        :param notify: if to 1, send a notification\n        :type notify: integer\n        :param author: name of the author or the acknowledge\n        :type author: str\n        :param comment: comment (description) of the acknowledge\n        :type comment: str\n        :return: None\n        \"\"\"\n        notification_period = None\n        if getattr(service, 'notification_period', None) is not None:\n            notification_period = self.daemon.timeperiods[service.notification_period]\n        service.acknowledge_problem(notification_period, self.hosts, self.services, sticky,\n                                    notify, author, comment)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef acknowledge_host_problem(self, host, sticky, notify, author, comment):\n        notification_period = None\n        if getattr(host, 'notification_period', None) is not None:\n            notification_period = self.daemon.timeperiods[host.notification_period]\n        host.acknowledge_problem(notification_period, self.hosts, self.services, sticky,\n                                 notify, author, comment)", "response": "This function acknowledge a host problem"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges the contact service notification timeperiod value", "response": "def change_contact_svc_notification_timeperiod(self, contact, notification_timeperiod):\n        \"\"\"Change contact service notification timeperiod value\n        Format of the line that triggers function call::\n\n        CHANGE_CONTACT_SVC_NOTIFICATION_TIMEPERIOD;<contact_name>;<notification_timeperiod>\n\n        :param contact: contact to edit\n        :type contact: alignak.objects.contact.Contact\n        :param notification_timeperiod: timeperiod to set\n        :type notification_timeperiod: alignak.objects.timeperiod.Timeperiod\n        :return: None\n        \"\"\"\n        contact.modified_service_attributes |= \\\n            DICT_MODATTR[\"MODATTR_NOTIFICATION_TIMEPERIOD\"].value\n        contact.service_notification_period = notification_timeperiod\n        self.send_an_element(contact.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges custom contact variable Format of the line that triggers function call:: CHANGE_CUSTOM_CONTACT_VAR;<contact_name>;<varname>;<varvalue> :param contact: contact to edit :type contact: alignak.objects.contact.Contact :param varname: variable name to change :type varname: str :param varvalue: variable new value :type varvalue: str :return: None", "response": "def change_custom_contact_var(self, contact, varname, varvalue):\n        \"\"\"Change custom contact variable\n        Format of the line that triggers function call::\n\n        CHANGE_CUSTOM_CONTACT_VAR;<contact_name>;<varname>;<varvalue>\n\n        :param contact: contact to edit\n        :type contact: alignak.objects.contact.Contact\n        :param varname: variable name to change\n        :type varname: str\n        :param varvalue: variable new value\n        :type varvalue: str\n        :return: None\n        \"\"\"\n        if varname.upper() in contact.customs:\n            contact.modified_attributes |= DICT_MODATTR[\"MODATTR_CUSTOM_VARIABLE\"].value\n            contact.customs[varname.upper()] = varvalue\n            self.send_an_element(contact.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_custom_host_var(self, host, varname, varvalue):\n        if varname.upper() in host.customs:\n            host.modified_attributes |= DICT_MODATTR[\"MODATTR_CUSTOM_VARIABLE\"].value\n            host.customs[varname.upper()] = varvalue\n\n            self.send_an_element(host.get_update_status_brok())", "response": "This function is used to change the value of a custom host variable in a specific host"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef change_custom_svc_var(self, service, varname, varvalue):\n        if varname.upper() in service.customs:\n            service.modified_attributes |= DICT_MODATTR[\"MODATTR_CUSTOM_VARIABLE\"].value\n            service.customs[varname.upper()] = varvalue\n            self.send_an_element(service.get_update_status_brok())", "response": "This function is used to change the value of a specific variable in a specific service"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmodifies the check command for the specified host", "response": "def change_host_check_command(self, host, check_command):\n        \"\"\"Modify host check command\n        Format of the line that triggers function call::\n\n        CHANGE_HOST_CHECK_COMMAND;<host_name>;<check_command>\n\n        :param host: host to modify check command\n        :type host: alignak.objects.host.Host\n        :param check_command: command line\n        :type check_command:\n        :return: None\n        \"\"\"\n        host.modified_attributes |= DICT_MODATTR[\"MODATTR_CHECK_COMMAND\"].value\n        data = {\"commands\": self.commands, \"call\": check_command, \"poller_tag\": host.poller_tag}\n        host.change_check_command(data)\n        self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef change_host_check_timeperiod(self, host, timeperiod):\n        host.modified_attributes |= DICT_MODATTR[\"MODATTR_CHECK_TIMEPERIOD\"].value\n        host.check_period = timeperiod\n        self.send_an_element(host.get_update_status_brok())", "response": "Modify the check_timeperiod of a host"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmodifying the event handler of a host", "response": "def change_host_event_handler(self, host, event_handler_command):\n        \"\"\"Modify host event handler\n        Format of the line that triggers function call::\n\n        CHANGE_HOST_EVENT_HANDLER;<host_name>;<event_handler_command>\n\n        :param host: host to modify event handler\n        :type host: alignak.objects.host.Host\n        :param event_handler_command: event handler command line\n        :type event_handler_command:\n        :return: None\n        \"\"\"\n        host.modified_attributes |= DICT_MODATTR[\"MODATTR_EVENT_HANDLER_COMMAND\"].value\n        data = {\"commands\": self.commands, \"call\": event_handler_command}\n        host.change_event_handler(data)\n        self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef change_host_snapshot_command(self, host, snapshot_command):\n        host.modified_attributes |= DICT_MODATTR[\"MODATTR_EVENT_HANDLER_COMMAND\"].value\n        data = {\"commands\": self.commands, \"call\": snapshot_command}\n        host.change_snapshot_command(data)\n        self.send_an_element(host.get_update_status_brok())", "response": "Modify the state of the host s snapshot command"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef change_host_modattr(self, host, value):\n        # todo: deprecate this\n        # We need to change each of the needed attributes.\n        previous_value = host.modified_attributes\n        changes = int(value)\n\n        # For all boolean and non boolean attributes\n        for modattr in [\"MODATTR_NOTIFICATIONS_ENABLED\", \"MODATTR_ACTIVE_CHECKS_ENABLED\",\n                        \"MODATTR_PASSIVE_CHECKS_ENABLED\", \"MODATTR_EVENT_HANDLER_ENABLED\",\n                        \"MODATTR_FLAP_DETECTION_ENABLED\", \"MODATTR_PERFORMANCE_DATA_ENABLED\",\n                        \"MODATTR_FRESHNESS_CHECKS_ENABLED\",\n                        \"MODATTR_EVENT_HANDLER_COMMAND\", \"MODATTR_CHECK_COMMAND\",\n                        \"MODATTR_NORMAL_CHECK_INTERVAL\", \"MODATTR_RETRY_CHECK_INTERVAL\",\n                        \"MODATTR_MAX_CHECK_ATTEMPTS\", \"MODATTR_FRESHNESS_CHECKS_ENABLED\",\n                        \"MODATTR_CHECK_TIMEPERIOD\", \"MODATTR_CUSTOM_VARIABLE\",\n                        \"MODATTR_NOTIFICATION_TIMEPERIOD\"]:\n            if changes & DICT_MODATTR[modattr].value:\n                # Toggle the concerned service attribute\n                setattr(host, DICT_MODATTR[modattr].attribute, not\n                        getattr(host, DICT_MODATTR[modattr].attribute))\n\n        host.modified_attributes = previous_value ^ changes\n\n        # And we need to push the information to the scheduler.\n        self.send_an_element(host.get_update_status_brok())", "response": "Change the host modified attributes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmodifies max host check attempt Format of the line that triggers function call:: CHANGE_MAX_HOST_CHECK_ATTEMPTS;<host_name>;<check_attempts> :param host: host to edit :type host: alignak.objects.host.Host :param check_attempts: new value to set :type check_attempts: int :return: None", "response": "def change_max_host_check_attempts(self, host, check_attempts):\n        \"\"\"Modify max host check attempt\n        Format of the line that triggers function call::\n\n        CHANGE_MAX_HOST_CHECK_ATTEMPTS;<host_name>;<check_attempts>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :param check_attempts: new value to set\n        :type check_attempts: int\n        :return: None\n        \"\"\"\n        host.modified_attributes |= DICT_MODATTR[\"MODATTR_MAX_CHECK_ATTEMPTS\"].value\n        host.max_check_attempts = check_attempts\n        if host.state_type == u'HARD' and host.state == u'UP' and host.attempt > 1:\n            host.attempt = host.max_check_attempts\n        self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_max_svc_check_attempts(self, service, check_attempts):\n        service.modified_attributes |= DICT_MODATTR[\"MODATTR_MAX_CHECK_ATTEMPTS\"].value\n        service.max_check_attempts = check_attempts\n        if service.state_type == u'HARD' and service.state == u'OK' and service.attempt > 1:\n            service.attempt = service.max_check_attempts\n        self.send_an_element(service.get_update_status_brok())", "response": "This function is called when the service is up and the max_check_attempts attribute of the service is changed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmodifies host check interval Format of the line that triggers function call:: CHANGE_NORMAL_HOST_CHECK_INTERVAL;<host_name>;<check_interval> :param host: host to edit :type host: alignak.objects.host.Host :param check_interval: new value to set :type check_interval: :return: None", "response": "def change_normal_host_check_interval(self, host, check_interval):\n        \"\"\"Modify host check interval\n        Format of the line that triggers function call::\n\n        CHANGE_NORMAL_HOST_CHECK_INTERVAL;<host_name>;<check_interval>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :param check_interval: new value to set\n        :type check_interval:\n        :return: None\n        \"\"\"\n        host.modified_attributes |= DICT_MODATTR[\"MODATTR_NORMAL_CHECK_INTERVAL\"].value\n        old_interval = host.check_interval\n        host.check_interval = check_interval\n        # If there were no regular checks (interval=0), then schedule\n        # a check immediately.\n        if old_interval == 0 and host.checks_enabled:\n            host.schedule(self.daemon.hosts, self.daemon.services,\n                          self.daemon.timeperiods, self.daemon.macromodulations,\n                          self.daemon.checkmodulations, self.daemon.checks,\n                          force=False, force_time=int(time.time()))\n        self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_retry_host_check_interval(self, host, check_interval):\n        host.modified_attributes |= DICT_MODATTR[\"MODATTR_RETRY_CHECK_INTERVAL\"].value\n        host.retry_interval = check_interval\n        self.send_an_element(host.get_update_status_brok())", "response": "Modify the retry interval of a host"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmodifies the retry interval of a service", "response": "def change_retry_svc_check_interval(self, service, check_interval):\n        \"\"\"Modify service retry interval\n        Format of the line that triggers function call::\n\n        CHANGE_RETRY_SVC_CHECK_INTERVAL;<host_name>;<service_description>;<check_interval>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :param check_interval: new value to set\n        :type check_interval:\n        :return: None\n        \"\"\"\n        service.modified_attributes |= DICT_MODATTR[\"MODATTR_RETRY_CHECK_INTERVAL\"].value\n        service.retry_interval = check_interval\n        self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmodifies the service check command", "response": "def change_svc_check_command(self, service, check_command):\n        \"\"\"Modify service check command\n        Format of the line that triggers function call::\n\n        CHANGE_SVC_CHECK_COMMAND;<host_name>;<service_description>;<check_command>\n\n        :param service: service to modify check command\n        :type service: alignak.objects.service.Service\n        :param check_command: command line\n        :type check_command:\n        :return: None\n        \"\"\"\n        service.modified_attributes |= DICT_MODATTR[\"MODATTR_CHECK_COMMAND\"].value\n        data = {\"commands\": self.commands, \"call\": check_command, \"poller_tag\": service.poller_tag}\n        service.change_check_command(data)\n        self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef change_svc_check_timeperiod(self, service, check_timeperiod):\n        service.modified_attributes |= DICT_MODATTR[\"MODATTR_CHECK_TIMEPERIOD\"].value\n        service.check_period = check_timeperiod\n        self.send_an_element(service.get_update_status_brok())", "response": "Modify the check_timeperiod of a service"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef change_svc_event_handler(self, service, event_handler_command):\n        service.modified_attributes |= DICT_MODATTR[\"MODATTR_EVENT_HANDLER_COMMAND\"].value\n        data = {\"commands\": self.commands, \"call\": event_handler_command}\n        service.change_event_handler(data)\n        self.send_an_element(service.get_update_status_brok())", "response": "Modify the event handler for a service"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_svc_snapshot_command(self, service, snapshot_command):\n        service.modified_attributes |= DICT_MODATTR[\"MODATTR_EVENT_HANDLER_COMMAND\"].value\n        data = {\"commands\": self.commands, \"call\": snapshot_command}\n        service.change_snapshot_command(data)\n        self.send_an_element(service.get_update_status_brok())", "response": "Modify host snapshot command"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef change_svc_modattr(self, service, value):\n        # todo: deprecate this\n        # We need to change each of the needed attributes.\n        previous_value = service.modified_attributes\n        changes = int(value)\n\n        # For all boolean and non boolean attributes\n        for modattr in [\"MODATTR_NOTIFICATIONS_ENABLED\", \"MODATTR_ACTIVE_CHECKS_ENABLED\",\n                        \"MODATTR_PASSIVE_CHECKS_ENABLED\", \"MODATTR_EVENT_HANDLER_ENABLED\",\n                        \"MODATTR_FLAP_DETECTION_ENABLED\", \"MODATTR_PERFORMANCE_DATA_ENABLED\",\n                        \"MODATTR_FRESHNESS_CHECKS_ENABLED\",\n                        \"MODATTR_EVENT_HANDLER_COMMAND\", \"MODATTR_CHECK_COMMAND\",\n                        \"MODATTR_NORMAL_CHECK_INTERVAL\", \"MODATTR_RETRY_CHECK_INTERVAL\",\n                        \"MODATTR_MAX_CHECK_ATTEMPTS\", \"MODATTR_FRESHNESS_CHECKS_ENABLED\",\n                        \"MODATTR_CHECK_TIMEPERIOD\", \"MODATTR_CUSTOM_VARIABLE\",\n                        \"MODATTR_NOTIFICATION_TIMEPERIOD\"]:\n            if changes & DICT_MODATTR[modattr].value:\n                # Toggle the concerned service attribute\n                setattr(service, DICT_MODATTR[modattr].attribute, not\n                        getattr(service, DICT_MODATTR[modattr].attribute))\n\n        service.modified_attributes = previous_value ^ changes\n\n        # And we need to push the information to the scheduler.\n        self.send_an_element(service.get_update_status_brok())", "response": "Change the service modified attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_svc_notification_timeperiod(self, service, notification_timeperiod):\n        service.modified_attributes |= DICT_MODATTR[\"MODATTR_NOTIFICATION_TIMEPERIOD\"].value\n        service.notification_period = notification_timeperiod\n        self.send_an_element(service.get_update_status_brok())", "response": "Change the service notification timeperiod"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmodifies the first notification of a host", "response": "def delay_host_notification(self, host, notification_time):\n        \"\"\"Modify host first notification delay\n        Format of the line that triggers function call::\n\n        DELAY_HOST_NOTIFICATION;<host_name>;<notification_time>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :param notification_time: new value to set\n        :type notification_time:\n        :return: None\n        \"\"\"\n        host.first_notification_delay = notification_time\n        self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmodifies service first notification delay", "response": "def delay_svc_notification(self, service, notification_time):\n        \"\"\"Modify service first notification delay\n        Format of the line that triggers function call::\n\n        DELAY_SVC_NOTIFICATION;<host_name>;<service_description>;<notification_time>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :param notification_time: new value to set\n        :type notification_time:\n        :return: None\n        \"\"\"\n        service.first_notification_delay = notification_time\n        self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete all comments from a host", "response": "def del_all_host_comments(self, host):\n        \"\"\"Delete all host comments\n        Format of the line that triggers function call::\n\n        DEL_ALL_HOST_COMMENTS;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        comments = list(host.comments.keys())\n        for uuid in comments:\n            host.del_comment(uuid)\n        self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete all host downtimes", "response": "def del_all_host_downtimes(self, host):\n        \"\"\"Delete all host downtimes\n        Format of the line that triggers function call::\n\n        DEL_ALL_HOST_DOWNTIMES;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        for downtime in host.downtimes:\n            self.del_host_downtime(downtime)\n        self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes all service comments", "response": "def del_all_svc_comments(self, service):\n        \"\"\"Delete all service comments\n        Format of the line that triggers function call::\n\n        DEL_ALL_SVC_COMMENTS;<host_name>;<service_description>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        comments = list(service.comments.keys())\n        for uuid in comments:\n            service.del_comment(uuid)\n        self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef del_all_svc_downtimes(self, service):\n        for downtime in service.downtimes:\n            self.del_svc_downtime(downtime)\n        self.send_an_element(service.get_update_status_brok())", "response": "Delete all service downtimes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef del_contact_downtime(self, downtime_id):\n        for item in self.daemon.contacts:\n            if downtime_id in item.downtimes:\n                item.downtimes[downtime_id].cancel(self.daemon.contacts)\n                break\n        else:\n            self.send_an_element(make_monitoring_log(\n                'warning', 'DEL_CONTACT_DOWNTIME: downtime id: %s does not exist '\n                           'and cannot be deleted.' % downtime_id))", "response": "Delete a contact downtime"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef del_host_comment(self, comment_id):\n        for item in self.daemon.hosts:\n            if comment_id in item.comments:\n                item.del_comment(comment_id)\n                self.send_an_element(item.get_update_status_brok())\n                break\n        else:\n            self.send_an_element(make_monitoring_log(\n                'warning', 'DEL_HOST_COMMENT: comment id: %s does not exist '\n                           'and cannot be deleted.' % comment_id))", "response": "Delete a comment from a specific host"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a host downtime", "response": "def del_host_downtime(self, downtime_id):\n        \"\"\"Delete a host downtime\n        Format of the line that triggers function call::\n\n        DEL_HOST_DOWNTIME;<downtime_id>\n\n        :param downtime_id: downtime id to delete\n        :type downtime_id: int\n        :return: None\n        \"\"\"\n        broks = []\n        for item in self.daemon.hosts:\n            if downtime_id in item.downtimes:\n                broks.extend(item.downtimes[downtime_id].cancel(self.daemon.timeperiods,\n                                                                self.daemon.hosts,\n                                                                self.daemon.services))\n                break\n        else:\n            self.send_an_element(make_monitoring_log(\n                'warning', 'DEL_HOST_DOWNTIME: downtime id: %s does not exist '\n                           'and cannot be deleted.' % downtime_id))\n        for brok in broks:\n            self.send_an_element(brok)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a comment from a service", "response": "def del_svc_comment(self, comment_id):\n        \"\"\"Delete a service comment\n        Format of the line that triggers function call::\n\n        DEL_SVC_COMMENT;<comment_id>\n\n        :param comment_id: comment id to delete\n        :type comment_id: int\n        :return: None\n        \"\"\"\n        for svc in self.daemon.services:\n            if comment_id in svc.comments:\n                svc.del_comment(comment_id)\n                self.send_an_element(svc.get_update_status_brok())\n                break\n        else:\n            self.send_an_element(make_monitoring_log(\n                'warning', 'DEL_SVC_COMMENT: comment id: %s does not exist '\n                           'and cannot be deleted.' % comment_id))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef del_svc_downtime(self, downtime_id):\n        broks = []\n        for svc in self.daemon.services:\n            if downtime_id in svc.downtimes:\n                broks.extend(svc.downtimes[downtime_id].cancel(self.daemon.timeperiods,\n                                                               self.daemon.hosts,\n                                                               self.daemon.services))\n                break\n        else:\n            self.send_an_element(make_monitoring_log(\n                'warning', 'DEL_SVC_DOWNTIME: downtime id: %s does not exist '\n                           'and cannot be deleted.' % downtime_id))\n        for brok in broks:\n            self.send_an_element(brok)", "response": "Delete a service downtime"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disable_contactgroup_host_notifications(self, contactgroup):\n        for contact_id in contactgroup.get_contacts():\n            self.disable_contact_host_notifications(self.daemon.contacts[contact_id])", "response": "Disable host notifications for a contactgroup"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisables service notifications for a given contactgroup", "response": "def disable_contactgroup_svc_notifications(self, contactgroup):\n        \"\"\"Disable service notifications for a contactgroup\n        Format of the line that triggers function call::\n\n        DISABLE_CONTACTGROUP_SVC_NOTIFICATIONS;<contactgroup_name>\n\n        :param contactgroup: contactgroup to disable\n        :type contactgroup: alignak.objects.contactgroup.Contactgroup\n        :return: None\n        \"\"\"\n        for contact_id in contactgroup.get_contacts():\n            self.disable_contact_svc_notifications(self.daemon.contacts[contact_id])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisable host notifications for a given contact", "response": "def disable_contact_host_notifications(self, contact):\n        \"\"\"Disable host notifications for a contact\n        Format of the line that triggers function call::\n\n        DISABLE_CONTACT_HOST_NOTIFICATIONS;<contact_name>\n\n        :param contact: contact to disable\n        :type contact: alignak.objects.contact.Contact\n        :return: None\n        \"\"\"\n        if contact.host_notifications_enabled:\n            contact.modified_attributes |= DICT_MODATTR[\"MODATTR_NOTIFICATIONS_ENABLED\"].value\n            contact.host_notifications_enabled = False\n            self.send_an_element(contact.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef disable_contact_svc_notifications(self, contact):\n        if contact.service_notifications_enabled:\n            contact.modified_attributes |= DICT_MODATTR[\"MODATTR_NOTIFICATIONS_ENABLED\"].value\n            contact.service_notifications_enabled = False\n            self.send_an_element(contact.get_update_status_brok())", "response": "Disable service notifications for a specific locale"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisables event handlers for the current locale", "response": "def disable_event_handlers(self):\n        \"\"\"Disable event handlers (globally)\n        Format of the line that triggers function call::\n\n        DISABLE_EVENT_HANDLERS\n\n        :return: None\n        \"\"\"\n        # todo: #783 create a dedicated brok for global parameters\n        if self.my_conf.enable_event_handlers:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_EVENT_HANDLER_ENABLED\"].value\n            self.my_conf.enable_event_handlers = False\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisable flap detection for a specific locale", "response": "def disable_flap_detection(self):\n        \"\"\"Disable flap detection (globally)\n        Format of the line that triggers function call::\n\n        DISABLE_FLAP_DETECTION\n\n        :return: None\n        \"\"\"\n        # todo: #783 create a dedicated brok for global parameters\n        if self.my_conf.enable_flap_detection:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_FLAP_DETECTION_ENABLED\"].value\n            self.my_conf.enable_flap_detection = False\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()\n            # Is need, disable flap state for hosts and services\n            for service in self.my_conf.services:\n                if service.is_flapping:\n                    service.is_flapping = False\n                    service.flapping_changes = []\n                    self.send_an_element(service.get_update_status_brok())\n            for host in self.my_conf.hosts:\n                if host.is_flapping:\n                    host.is_flapping = False\n                    host.flapping_changes = []\n                    self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisabling host checks for a hostgroup", "response": "def disable_hostgroup_host_checks(self, hostgroup):\n        \"\"\"Disable host checks for a hostgroup\n        Format of the line that triggers function call::\n\n        DISABLE_HOSTGROUP_HOST_CHECKS;<hostgroup_name>\n\n        :param hostgroup: hostgroup to disable\n        :type hostgroup: alignak.objects.hostgroup.Hostgroup\n        :return: None\n        \"\"\"\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                self.disable_host_check(self.daemon.hosts[host_id])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisables all host notifications for a hostgroup", "response": "def disable_hostgroup_host_notifications(self, hostgroup):\n        \"\"\"Disable host notifications for a hostgroup\n        Format of the line that triggers function call::\n\n        DISABLE_HOSTGROUP_HOST_NOTIFICATIONS;<hostgroup_name>\n\n        :param hostgroup: hostgroup to disable\n        :type hostgroup: alignak.objects.hostgroup.Hostgroup\n        :return: None\n        \"\"\"\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                self.disable_host_notifications(self.daemon.hosts[host_id])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisabling passive host checks for a hostgroup", "response": "def disable_hostgroup_passive_host_checks(self, hostgroup):\n        \"\"\"Disable host passive checks for a hostgroup\n        Format of the line that triggers function call::\n\n        DISABLE_HOSTGROUP_PASSIVE_HOST_CHECKS;<hostgroup_name>\n\n        :param hostgroup: hostgroup to disable\n        :type hostgroup: alignak.objects.hostgroup.Hostgroup\n        :return: None\n        \"\"\"\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                self.disable_passive_host_checks(self.daemon.hosts[host_id])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisables passive checks for a hostgroup", "response": "def disable_hostgroup_passive_svc_checks(self, hostgroup):\n        \"\"\"Disable service passive checks for a hostgroup\n        Format of the line that triggers function call::\n\n        DISABLE_HOSTGROUP_PASSIVE_SVC_CHECKS;<hostgroup_name>\n\n        :param hostgroup: hostgroup to disable\n        :type hostgroup: alignak.objects.hostgroup.Hostgroup\n        :return: None\n        \"\"\"\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                for service_id in self.daemon.hosts[host_id].services:\n                    if service_id in self.daemon.services:\n                        self.disable_passive_svc_checks(self.daemon.services[service_id])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisable service checks for a hostgroup", "response": "def disable_hostgroup_svc_checks(self, hostgroup):\n        \"\"\"Disable service checks for a hostgroup\n        Format of the line that triggers function call::\n\n        DISABLE_HOSTGROUP_SVC_CHECKS;<hostgroup_name>\n\n        :param hostgroup: hostgroup to disable\n        :type hostgroup: alignak.objects.hostgroup.Hostgroup\n        :return: None\n        \"\"\"\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                for service_id in self.daemon.hosts[host_id].services:\n                    if service_id in self.daemon.services:\n                        self.disable_svc_check(self.daemon.services[service_id])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef disable_hostgroup_svc_notifications(self, hostgroup):\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                for service_id in self.daemon.hosts[host_id].services:\n                    if service_id in self.daemon.services:\n                        self.disable_svc_notifications(self.daemon.services[service_id])", "response": "Disable service notifications for a hostgroup"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisabling active checks for a host", "response": "def disable_host_check(self, host):\n        \"\"\"Disable checks for a host\n        Format of the line that triggers function call::\n\n        DISABLE_HOST_CHECK;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        if host.active_checks_enabled:\n            host.modified_attributes |= DICT_MODATTR[\"MODATTR_ACTIVE_CHECKS_ENABLED\"].value\n            host.disable_active_checks(self.daemon.checks)\n            self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disable_host_event_handler(self, host):\n        if host.event_handler_enabled:\n            host.modified_attributes |= DICT_MODATTR[\"MODATTR_EVENT_HANDLER_ENABLED\"].value\n            host.event_handler_enabled = False\n            self.send_an_element(host.get_update_status_brok())", "response": "Disable event handlers for a host"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef disable_host_flap_detection(self, host):\n        if host.flap_detection_enabled:\n            host.modified_attributes |= DICT_MODATTR[\"MODATTR_FLAP_DETECTION_ENABLED\"].value\n            host.flap_detection_enabled = False\n            # Maybe the host was flapping, if so, stop flapping\n            if host.is_flapping:\n                host.is_flapping = False\n                host.flapping_changes = []\n            self.send_an_element(host.get_update_status_brok())", "response": "Disable flap detection for a host"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisables freshness check for a host", "response": "def disable_host_freshness_check(self, host):\n        \"\"\"Disable freshness check for a host\n        Format of the line that triggers function call::\n\n        DISABLE_HOST_FRESHNESS_CHECK;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        if host.check_freshness:\n            host.modified_attributes |= DICT_MODATTR[\"MODATTR_FRESHNESS_CHECKS_ENABLED\"].value\n            host.check_freshness = False\n            self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisabling freshness checks for the host.", "response": "def disable_host_freshness_checks(self):\n        \"\"\"Disable freshness checks (globally)\n        Format of the line that triggers function call::\n\n        DISABLE_HOST_FRESHNESS_CHECKS\n\n        :return: None\n        \"\"\"\n        if self.my_conf.check_host_freshness:\n            self.my_conf.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_FRESHNESS_CHECKS_ENABLED\"].value\n            self.my_conf.check_host_freshness = False\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disable_host_notifications(self, host):\n        if host.notifications_enabled:\n            host.modified_attributes |= DICT_MODATTR[\"MODATTR_NOTIFICATIONS_ENABLED\"].value\n            host.notifications_enabled = False\n            self.send_an_element(host.get_update_status_brok())", "response": "Disable notifications for a host"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disable_host_svc_checks(self, host):\n        for service_id in host.services:\n            if service_id in self.daemon.services:\n                service = self.daemon.services[service_id]\n                self.disable_svc_check(service)\n                self.send_an_element(service.get_update_status_brok())", "response": "Disable service checks for a host"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisable services notifications for a host", "response": "def disable_host_svc_notifications(self, host):\n        \"\"\"Disable services notifications for a host\n        Format of the line that triggers function call::\n\n        DISABLE_HOST_SVC_NOTIFICATIONS;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        for service_id in host.services:\n            if service_id in self.daemon.services:\n                service = self.daemon.services[service_id]\n                self.disable_svc_notifications(service)\n                self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disable_notifications(self):\n        # todo: #783 create a dedicated brok for global parameters\n        if self.my_conf.enable_notifications:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_NOTIFICATIONS_ENABLED\"].value\n            self.my_conf.enable_notifications = False\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Disable notifications for a specific locale"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisable passive checks for a host", "response": "def disable_passive_host_checks(self, host):\n        \"\"\"Disable passive checks for a host\n        Format of the line that triggers function call::\n\n        DISABLE_PASSIVE_HOST_CHECKS;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        if host.passive_checks_enabled:\n            host.modified_attributes |= DICT_MODATTR[\"MODATTR_PASSIVE_CHECKS_ENABLED\"].value\n            host.passive_checks_enabled = False\n            self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disable_passive_svc_checks(self, service):\n        if service.passive_checks_enabled:\n            service.modified_attributes |= DICT_MODATTR[\"MODATTR_PASSIVE_CHECKS_ENABLED\"].value\n            service.passive_checks_enabled = False\n            self.send_an_element(service.get_update_status_brok())", "response": "Disable passive checks for a service"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisable performance data processing ( globally )", "response": "def disable_performance_data(self):\n        \"\"\"Disable performance data processing (globally)\n        Format of the line that triggers function call::\n\n        DISABLE_PERFORMANCE_DATA\n\n        :return: None\n        \"\"\"\n        # todo: #783 create a dedicated brok for global parameters\n        if self.my_conf.process_performance_data:\n            self.my_conf.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_PERFORMANCE_DATA_ENABLED\"].value\n            self.my_conf.process_performance_data = False\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisables host checks for a servicegroup", "response": "def disable_servicegroup_host_checks(self, servicegroup):\n        \"\"\"Disable host checks for a servicegroup\n        Format of the line that triggers function call::\n\n        DISABLE_SERVICEGROUP_HOST_CHECKS;<servicegroup_name>\n\n        :param servicegroup: servicegroup to disable\n        :type servicegroup: alignak.objects.servicegroup.Servicegroup\n        :return: None\n        \"\"\"\n        for service_id in servicegroup.get_services():\n            if service_id in self.daemon.services:\n                host_id = self.daemon.services[service_id].host\n                self.disable_host_check(self.daemon.hosts[host_id])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disable_servicegroup_host_notifications(self, servicegroup):\n        for service_id in servicegroup.get_services():\n            if service_id in self.daemon.services:\n                host_id = self.daemon.services[service_id].host\n                self.disable_host_notifications(self.daemon.hosts[host_id])", "response": "Disable host notifications for a servicegroup"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisables passive host checks for a servicegroup", "response": "def disable_servicegroup_passive_host_checks(self, servicegroup):\n        \"\"\"Disable passive host checks for a servicegroup\n        Format of the line that triggers function call::\n\n        DISABLE_SERVICEGROUP_PASSIVE_HOST_CHECKS;<servicegroup_name>\n\n        :param servicegroup: servicegroup to disable\n        :type servicegroup: alignak.objects.servicegroup.Servicegroup\n        :return: None\n        \"\"\"\n        for service_id in servicegroup.get_services():\n            if service_id in self.daemon.services:\n                host_id = self.daemon.services[service_id].host\n                self.disable_passive_host_checks(self.daemon.hosts[host_id])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisables passive service checks for a servicegroup", "response": "def disable_servicegroup_passive_svc_checks(self, servicegroup):\n        \"\"\"Disable passive service checks for a servicegroup\n        Format of the line that triggers function call::\n\n        DISABLE_SERVICEGROUP_PASSIVE_SVC_CHECKS;<servicegroup_name>\n\n        :param servicegroup: servicegroup to disable\n        :type servicegroup: alignak.objects.servicegroup.Servicegroup\n        :return: None\n        \"\"\"\n        for service_id in servicegroup.get_services():\n            self.disable_passive_svc_checks(self.daemon.services[service_id])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisable service checks for a servicegroup", "response": "def disable_servicegroup_svc_checks(self, servicegroup):\n        \"\"\"Disable service checks for a servicegroup\n        Format of the line that triggers function call::\n\n        DISABLE_SERVICEGROUP_SVC_CHECKS;<servicegroup_name>\n\n        :param servicegroup: servicegroup to disable\n        :type servicegroup: alignak.objects.servicegroup.Servicegroup\n        :return: None\n        \"\"\"\n        for service_id in servicegroup.get_services():\n            self.disable_svc_check(self.daemon.services[service_id])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disable_servicegroup_svc_notifications(self, servicegroup):\n        for service_id in servicegroup.get_services():\n            self.disable_svc_notifications(self.daemon.services[service_id])", "response": "Disable service notifications for a servicegroup"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisables flap detection for a service", "response": "def disable_service_flap_detection(self, service):\n        \"\"\"Disable flap detection for a service\n        Format of the line that triggers function call::\n\n        DISABLE_SERVICE_FLAP_DETECTION;<host_name>;<service_description>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        if service.flap_detection_enabled:\n            service.modified_attributes |= DICT_MODATTR[\"MODATTR_FLAP_DETECTION_ENABLED\"].value\n            service.flap_detection_enabled = False\n            # Maybe the service was flapping, if so, stop flapping\n            if service.is_flapping:\n                service.is_flapping = False\n                service.flapping_changes = []\n            self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisabling freshness check for a service", "response": "def disable_svc_freshness_check(self, service):\n        \"\"\"Disable freshness check for a service\n        Format of the line that triggers function call::\n\n        DISABLE_SERVICE_FRESHNESS_CHECK;<host_name>;<service_description>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        if service.check_freshness:\n            service.modified_attributes |= DICT_MODATTR[\"MODATTR_FRESHNESS_CHECKS_ENABLED\"].value\n            service.check_freshness = False\n            self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disable_service_freshness_checks(self):\n        if self.my_conf.check_service_freshness:\n            self.my_conf.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_FRESHNESS_CHECKS_ENABLED\"].value\n            self.my_conf.check_service_freshness = False\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Disable service freshness checks (globally )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disable_svc_check(self, service):\n        if service.active_checks_enabled:\n            service.disable_active_checks(self.daemon.checks)\n            service.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_ACTIVE_CHECKS_ENABLED\"].value\n            self.send_an_element(service.get_update_status_brok())", "response": "Disable active checks for a service"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisabling event handlers for a service", "response": "def disable_svc_event_handler(self, service):\n        \"\"\"Disable event handlers for a service\n        Format of the line that triggers function call::\n\n        DISABLE_SVC_EVENT_HANDLER;<host_name>;<service_description>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        if service.event_handler_enabled:\n            service.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_EVENT_HANDLER_ENABLED\"].value\n            service.event_handler_enabled = False\n            self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisables notifications for a service", "response": "def disable_svc_notifications(self, service):\n        \"\"\"Disable notifications for a service\n        Format of the line that triggers function call::\n\n        DISABLE_SVC_NOTIFICATIONS;<host_name>;<service_description>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        if service.notifications_enabled:\n            service.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_NOTIFICATIONS_ENABLED\"].value\n            service.notifications_enabled = False\n            self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enable_contactgroup_host_notifications(self, contactgroup):\n        for contact_id in contactgroup.get_contacts():\n            self.enable_contact_host_notifications(self.daemon.contacts[contact_id])", "response": "Enable host notifications for a contactgroup"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enable_contactgroup_svc_notifications(self, contactgroup):\n        for contact_id in contactgroup.get_contacts():\n            self.enable_contact_svc_notifications(self.daemon.contacts[contact_id])", "response": "Enable service notifications for a given contactgroup"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enable_contact_host_notifications(self, contact):\n        if not contact.host_notifications_enabled:\n            contact.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_NOTIFICATIONS_ENABLED\"].value\n            contact.host_notifications_enabled = True\n            self.send_an_element(contact.get_update_status_brok())", "response": "Enable host notifications for a given contact"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enable_contact_svc_notifications(self, contact):\n        if not contact.service_notifications_enabled:\n            contact.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_NOTIFICATIONS_ENABLED\"].value\n            contact.service_notifications_enabled = True\n            self.send_an_element(contact.get_update_status_brok())", "response": "Enable service notifications for a specific entry in the database"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenable host checks for a hostgroup", "response": "def enable_hostgroup_host_checks(self, hostgroup):\n        \"\"\"Enable host checks for a hostgroup\n        Format of the line that triggers function call::\n\n        ENABLE_HOSTGROUP_HOST_CHECKS;<hostgroup_name>\n\n        :param hostgroup: hostgroup to enable\n        :type hostgroup: alignak.objects.hostgroup.Hostgroup\n        :return: None\n        \"\"\"\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                self.enable_host_check(self.daemon.hosts[host_id])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enable_hostgroup_host_notifications(self, hostgroup):\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                self.enable_host_notifications(self.daemon.hosts[host_id])", "response": "Enable host notifications for a hostgroup"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef enable_hostgroup_passive_host_checks(self, hostgroup):\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                self.enable_passive_host_checks(self.daemon.hosts[host_id])", "response": "Enable host passive checks for a hostgroup"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enable_hostgroup_passive_svc_checks(self, hostgroup):\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                for service_id in self.daemon.hosts[host_id].services:\n                    if service_id in self.daemon.services:\n                        self.enable_passive_svc_checks(self.daemon.services[service_id])", "response": "Enable passive service checks for a hostgroup"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enable_hostgroup_svc_checks(self, hostgroup):\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                for service_id in self.daemon.hosts[host_id].services:\n                    if service_id in self.daemon.services:\n                        self.enable_svc_check(self.daemon.services[service_id])", "response": "Enable service checks for a hostgroup"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enable_hostgroup_svc_notifications(self, hostgroup):\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                for service_id in self.daemon.hosts[host_id].services:\n                    if service_id in self.daemon.services:\n                        self.enable_svc_notifications(self.daemon.services[service_id])", "response": "Enable service notifications for a hostgroup"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nenables checks for a host", "response": "def enable_host_check(self, host):\n        \"\"\"Enable checks for a host\n        Format of the line that triggers function call::\n\n        ENABLE_HOST_CHECK;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        if not host.active_checks_enabled:\n            host.active_checks_enabled = True\n            host.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_ACTIVE_CHECKS_ENABLED\"].value\n            self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enable_host_event_handler(self, host):\n        if not host.event_handler_enabled:\n            host.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_EVENT_HANDLER_ENABLED\"].value\n            host.event_handler_enabled = True\n            self.send_an_element(host.get_update_status_brok())", "response": "Enable event handlers for a host"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nenabling flap detection for a host", "response": "def enable_host_flap_detection(self, host):\n        \"\"\"Enable flap detection for a host\n        Format of the line that triggers function call::\n\n        ENABLE_HOST_FLAP_DETECTION;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        if not host.flap_detection_enabled:\n            host.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_FLAP_DETECTION_ENABLED\"].value\n            host.flap_detection_enabled = True\n            self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling freshness check for a host", "response": "def enable_host_freshness_check(self, host):\n        \"\"\"Enable freshness check for a host\n        Format of the line that triggers function call::\n\n        ENABLE_HOST_FRESHNESS_CHECK;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        if not host.check_freshness:\n            host.modified_attributes |= DICT_MODATTR[\"MODATTR_FRESHNESS_CHECKS_ENABLED\"].value\n            host.check_freshness = True\n            self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enable_host_freshness_checks(self):\n        if not self.my_conf.check_host_freshness:\n            self.my_conf.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_FRESHNESS_CHECKS_ENABLED\"].value\n            self.my_conf.check_host_freshness = True\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Enable freshness checks for the host."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enable_host_notifications(self, host):\n        if not host.notifications_enabled:\n            host.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_NOTIFICATIONS_ENABLED\"].value\n            host.notifications_enabled = True\n            self.send_an_element(host.get_update_status_brok())", "response": "Enable notifications for a host"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenables service checks for a host", "response": "def enable_host_svc_checks(self, host):\n        \"\"\"Enable service checks for a host\n        Format of the line that triggers function call::\n\n        ENABLE_HOST_SVC_CHECKS;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        for service_id in host.services:\n            if service_id in self.daemon.services:\n                service = self.daemon.services[service_id]\n                self.enable_svc_check(service)\n                self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable_host_svc_notifications(self, host):\n        for service_id in host.services:\n            if service_id in self.daemon.services:\n                service = self.daemon.services[service_id]\n                self.enable_svc_notifications(service)\n                self.send_an_element(service.get_update_status_brok())", "response": "Enable services notifications for a host"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable_notifications(self):\n        # todo: #783 create a dedicated brok for global parameters\n        if not self.my_conf.enable_notifications:\n            self.my_conf.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_NOTIFICATIONS_ENABLED\"].value\n            self.my_conf.enable_notifications = True\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Enable notifications for the current locale"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enable_passive_host_checks(self, host):\n        if not host.passive_checks_enabled:\n            host.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_PASSIVE_CHECKS_ENABLED\"].value\n            host.passive_checks_enabled = True\n            self.send_an_element(host.get_update_status_brok())", "response": "Enable passive checks for a host"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nenables passive checks for a service", "response": "def enable_passive_svc_checks(self, service):\n        \"\"\"Enable passive checks for a service\n        Format of the line that triggers function call::\n\n        ENABLE_PASSIVE_SVC_CHECKS;<host_name>;<service_description>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        if not service.passive_checks_enabled:\n            service.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_PASSIVE_CHECKS_ENABLED\"].value\n            service.passive_checks_enabled = True\n            self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enable_performance_data(self):\n        if not self.my_conf.process_performance_data:\n            self.my_conf.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_PERFORMANCE_DATA_ENABLED\"].value\n            self.my_conf.process_performance_data = True\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Enable performance data processing ( globally )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable_servicegroup_host_checks(self, servicegroup):\n        for service_id in servicegroup.get_services():\n            if service_id in self.daemon.services:\n                host_id = self.daemon.services[service_id].host\n                self.enable_host_check(self.daemon.hosts[host_id])", "response": "Enable host checks for a servicegroup"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enable_servicegroup_host_notifications(self, servicegroup):\n        for service_id in servicegroup.get_services():\n            if service_id in self.daemon.services:\n                host_id = self.daemon.services[service_id].host\n                self.enable_host_notifications(self.daemon.hosts[host_id])", "response": "Enable host notifications for a servicegroup"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nenables passive host checks for a servicegroup", "response": "def enable_servicegroup_passive_host_checks(self, servicegroup):\n        \"\"\"Enable passive host checks for a servicegroup\n        Format of the line that triggers function call::\n\n        ENABLE_SERVICEGROUP_PASSIVE_HOST_CHECKS;<servicegroup_name>\n\n        :param servicegroup: servicegroup to enable\n        :type servicegroup: alignak.objects.servicegroup.Servicegroup\n        :return: None\n        \"\"\"\n        for service_id in servicegroup.get_services():\n            if service_id in self.daemon.services:\n                host_id = self.daemon.services[service_id].host\n                self.enable_passive_host_checks(self.daemon.hosts[host_id])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenables passive service checks for a servicegroup", "response": "def enable_servicegroup_passive_svc_checks(self, servicegroup):\n        \"\"\"Enable passive service checks for a servicegroup\n        Format of the line that triggers function call::\n\n        ENABLE_SERVICEGROUP_PASSIVE_SVC_CHECKS;<servicegroup_name>\n\n        :param servicegroup: servicegroup to enable\n        :type servicegroup: alignak.objects.servicegroup.Servicegroup\n        :return: None\n        \"\"\"\n        for service_id in servicegroup.get_services():\n            self.enable_passive_svc_checks(self.daemon.services[service_id])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enable_servicegroup_svc_checks(self, servicegroup):\n        for service_id in servicegroup.get_services():\n            self.enable_svc_check(self.daemon.services[service_id])", "response": "Enable service checks for a servicegroup"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nenable service notifications for a servicegroup", "response": "def enable_servicegroup_svc_notifications(self, servicegroup):\n        \"\"\"Enable service notifications for a servicegroup\n        Format of the line that triggers function call::\n\n        ENABLE_SERVICEGROUP_SVC_NOTIFICATIONS;<servicegroup_name>\n\n        :param servicegroup: servicegroup to enable\n        :type servicegroup: alignak.objects.servicegroup.Servicegroup\n        :return: None\n        \"\"\"\n        for service_id in servicegroup.get_services():\n            self.enable_svc_notifications(self.daemon.services[service_id])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enable_service_freshness_checks(self):\n        if not self.my_conf.check_service_freshness:\n            self.my_conf.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_FRESHNESS_CHECKS_ENABLED\"].value\n            self.my_conf.check_service_freshness = True\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Enable service freshness checks (globally )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling checks for a service", "response": "def enable_svc_check(self, service):\n        \"\"\"Enable checks for a service\n        Format of the line that triggers function call::\n\n        ENABLE_SVC_CHECK;<host_name>;<service_description>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        if not service.active_checks_enabled:\n            service.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_ACTIVE_CHECKS_ENABLED\"].value\n            service.active_checks_enabled = True\n            self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nenabling event handlers for a service", "response": "def enable_svc_event_handler(self, service):\n        \"\"\"Enable event handlers for a service\n        Format of the line that triggers function call::\n\n        ENABLE_SVC_EVENT_HANDLER;<host_name>;<service_description>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        if not service.event_handler_enabled:\n            service.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_EVENT_HANDLER_ENABLED\"].value\n            service.event_handler_enabled = True\n            self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enable_svc_freshness_check(self, service):\n        if not service.check_freshness:\n            service.modified_attributes |= DICT_MODATTR[\"MODATTR_FRESHNESS_CHECKS_ENABLED\"].value\n            service.check_freshness = True\n            self.send_an_element(service.get_update_status_brok())", "response": "Enable freshness check for a service"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nenable flap detection for a service", "response": "def enable_svc_flap_detection(self, service):\n        \"\"\"Enable flap detection for a service\n        Format of the line that triggers function call::\n\n        ENABLE_SVC_FLAP_DETECTION;<host_name>;<service_description>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        if not service.flap_detection_enabled:\n            service.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_FLAP_DETECTION_ENABLED\"].value\n            service.flap_detection_enabled = True\n            self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling notifications for a service", "response": "def enable_svc_notifications(self, service):\n        \"\"\"Enable notifications for a service\n        Format of the line that triggers function call::\n\n        ENABLE_SVC_NOTIFICATIONS;<host_name>;<service_description>\n\n        :param service: service to edit\n        :type service: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        if not service.notifications_enabled:\n            service.modified_attributes |= \\\n                DICT_MODATTR[\"MODATTR_NOTIFICATIONS_ENABLED\"].value\n            service.notifications_enabled = True\n            self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_host_check_result(self, host, status_code, plugin_output):\n        now = time.time()\n        cls = host.__class__\n\n        # If globally disabled OR host disabled, do not launch..\n        if not cls.accept_passive_checks or not host.passive_checks_enabled:\n            return\n\n        try:\n            plugin_output = plugin_output.decode('utf8', 'ignore')\n            logger.debug('%s > Passive host check plugin output: %s',\n                         host.get_full_name(), plugin_output)\n        except AttributeError:\n            # Python 3 will raise an exception\n            pass\n        except UnicodeError:\n            pass\n\n        # Maybe the check is just too old, if so, bail out!\n        if self.current_timestamp < host.last_chk:\n            logger.debug('%s > Passive host check is too old (%.2f seconds). '\n                         'Ignoring, check output: %s',\n                         host.get_full_name(), self.current_timestamp < host.last_chk,\n                         plugin_output)\n            return\n\n        chk = host.launch_check(now, self.hosts, self.services, self.timeperiods,\n                                self.daemon.macromodulations, self.daemon.checkmodulations,\n                                self.daemon.checks, force=True)\n        # We will not have a check if an host/service is checked but it has no defined check_command\n        if not chk:\n            return\n\n        # Now we 'transform the check into a result'\n        # So exit_status, output and status is eaten by the host\n        chk.exit_status = status_code\n        chk.get_outputs(plugin_output, host.max_plugins_output_length)\n        chk.status = ACT_STATUS_WAIT_CONSUME\n        chk.check_time = self.current_timestamp  # we are using the external command timestamps\n        # Set the corresponding host's check type to passive\n        chk.set_type_passive()\n        # self.daemon.nb_check_received += 1\n        self.send_an_element(chk)\n        # Ok now this result will be read by the scheduler the next loop\n\n        # raise a passive check log only if needed\n        if self.my_conf.log_passive_checks:\n            log_level = 'info'\n            if status_code == 1:  # DOWN\n                log_level = 'error'\n            if status_code == 2:  # UNREACHABLE\n                log_level = 'warning'\n            self.send_an_element(make_monitoring_log(\n                log_level, 'PASSIVE HOST CHECK: %s;%d;%s;%s;%s' % (\n                    host.get_name(), status_code, chk.output, chk.long_output, chk.perf_data)))", "response": "Process the host check result and return the unique identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses the output of a host check", "response": "def process_host_output(self, host, plugin_output):\n        \"\"\"Process host output\n        Format of the line that triggers function call::\n\n        PROCESS_HOST_OUTPUT;<host_name>;<plugin_output>\n\n        :param host: host to process check to\n        :type host: alignak.objects.host.Host\n        :param plugin_output: plugin output\n        :type plugin_output: str\n        :return: None\n        \"\"\"\n        self.process_host_check_result(host, host.state_id, plugin_output)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_service_check_result(self, service, return_code, plugin_output):\n        now = time.time()\n        cls = service.__class__\n\n        # If globally disabled OR service disabled, do not launch..\n        if not cls.accept_passive_checks or not service.passive_checks_enabled:\n            return\n\n        try:\n            plugin_output = plugin_output.decode('utf8', 'ignore')\n            logger.debug('%s > Passive service check plugin output: %s',\n                         service.get_full_name(), plugin_output)\n        except AttributeError:\n            # Python 3 will raise an exception\n            pass\n        except UnicodeError:\n            pass\n\n        # Maybe the check is just too old, if so, bail out!\n        if self.current_timestamp < service.last_chk:\n            logger.debug('%s > Passive service check is too old (%d seconds). '\n                         'Ignoring, check output: %s',\n                         service.get_full_name(), self.current_timestamp < service.last_chk,\n                         plugin_output)\n            return\n\n        # Create a check object from the external command\n        chk = service.launch_check(now, self.hosts, self.services, self.timeperiods,\n                                   self.daemon.macromodulations, self.daemon.checkmodulations,\n                                   self.daemon.checks, force=True)\n        # Should not be possible to not find the check, but if so, don't crash\n        if not chk:\n            logger.error('%s > Passive service check failed. None check launched !?',\n                         service.get_full_name())\n            return\n\n        # Now we 'transform the check into a result'\n        # So exit_status, output and status is eaten by the service\n        chk.exit_status = return_code\n        chk.get_outputs(plugin_output, service.max_plugins_output_length)\n\n        logger.debug('%s > Passive service check output: %s',\n                     service.get_full_name(), chk.output)\n\n        chk.status = ACT_STATUS_WAIT_CONSUME\n        chk.check_time = self.current_timestamp  # we are using the external command timestamps\n        # Set the corresponding service's check type to passive\n        chk.set_type_passive()\n        # self.daemon.nb_check_received += 1\n        self.send_an_element(chk)\n        # Ok now this result will be read by the scheduler the next loop\n\n        # raise a passive check log only if needed\n        if self.my_conf.log_passive_checks:\n            log_level = 'info'\n            if return_code == 1:  # WARNING\n                log_level = 'warning'\n            if return_code == 2:  # CRITICAL\n                log_level = 'error'\n            self.send_an_element(make_monitoring_log(\n                log_level, 'PASSIVE SERVICE CHECK: %s;%s;%d;%s;%s;%s' % (\n                    self.hosts[service.host].get_name(), service.get_name(),\n                    return_code, chk.output, chk.long_output, chk.perf_data)))", "response": "Process the service check result and return the unique identifier."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_service_output(self, service, plugin_output):\n        self.process_service_check_result(service, service.state_id, plugin_output)", "response": "Process the output of a service check"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restart_program(self):\n        restart_cmd = self.commands.find_by_name('restart-alignak')\n        if not restart_cmd:\n            logger.error(\"Cannot restart Alignak : missing command named\"\n                         \" 'restart-alignak'. Please add one\")\n            return\n        restart_cmd_line = restart_cmd.command_line\n        logger.warning(\"RESTART command : %s\", restart_cmd_line)\n\n        # Ok get an event handler command that will run in 15min max\n        e_handler = EventHandler({'command': restart_cmd_line, 'timeout': 900})\n        # Ok now run it\n        e_handler.execute()\n        # And wait for the command to finish\n        while e_handler.status not in [ACT_STATUS_DONE, ACT_STATUS_TIMEOUT]:\n            e_handler.check_finished(64000)\n\n        log_level = 'info'\n        if e_handler.status == ACT_STATUS_TIMEOUT or e_handler.exit_status != 0:\n            logger.error(\"Cannot restart Alignak : the 'restart-alignak' command failed with\"\n                         \" the error code '%d' and the text '%s'.\",\n                         e_handler.exit_status, e_handler.output)\n            log_level = 'error'\n        # Ok here the command succeed, we can now wait our death\n        self.send_an_element(make_monitoring_log(log_level, \"RESTART: %s\" % (e_handler.output)))", "response": "Restart Alignak\n        Format of the line that triggers function call::\n\n        RESTART_PROGRAM\n\n        :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreloads Alignak configuration Format of the line that triggers function call:: RELOAD_CONFIG :return: None", "response": "def reload_config(self):\n        \"\"\"Reload Alignak configuration\n        Format of the line that triggers function call::\n\n        RELOAD_CONFIG\n\n        :return: None\n        \"\"\"\n        reload_cmd = self.commands.find_by_name('reload-alignak')\n        if not reload_cmd:\n            logger.error(\"Cannot restart Alignak : missing command\"\n                         \" named 'reload-alignak'. Please add one\")\n            return\n        logger.warning(\"RELOAD command : %s\", reload_cmd)\n        reload_cmd_line = reload_cmd.command_line\n        logger.warning(\"RELOAD command : %s\", reload_cmd_line)\n\n        # Ok get an event handler command that will run in 15min max\n        e_handler = EventHandler({'command': reload_cmd_line, 'timeout': 900})\n        # Ok now run it\n        e_handler.execute()\n        # And wait for the command to finish\n        while e_handler.status not in [ACT_STATUS_DONE, ACT_STATUS_TIMEOUT]:\n            e_handler.check_finished(64000)\n\n        log_level = 'info'\n        if e_handler.status == ACT_STATUS_TIMEOUT or e_handler.exit_status != 0:\n            logger.error(\"Cannot reload Alignak configuration: the 'reload-alignak' command failed\"\n                         \" with the error code '%d' and the text '%s'.\",\n                         e_handler.exit_status, e_handler.output)\n            log_level = 'error'\n        # Ok here the command succeed, we can now wait our death\n        self.send_an_element(make_monitoring_log(log_level, \"RELOAD: %s\" % (e_handler.output)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef schedule_and_propagate_host_downtime(self, host, start_time, end_time,\n                                             fixed, trigger_id, duration, author, comment):\n        \"\"\"DOES NOTHING (Should create host downtime and start it?)\n        Format of the line that triggers function call::\n\n        SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME;<host_name>;<start_time>;<end_time>;\n        <fixed>;<trigger_id>;<duration>;<author>;<comment>\n\n        :return: None\n        \"\"\"\n        logger.warning(\"The external command 'SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME' \"\n                       \"is not currently implemented in Alignak. If you really need it, \"\n                       \"request for its implementation in the project repository: \"\n                       \"https://github.com/Alignak-monitoring/alignak\")\n        self.send_an_element(make_monitoring_log(\n            'warning', 'SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME: this command is not implemented!'))", "response": "This function is used to create host downtime and propagate it to Alignak"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef schedule_contact_downtime(self, contact, start_time, end_time, author, comment):\n        data = {'ref': contact.uuid, 'start_time': start_time,\n                'end_time': end_time, 'author': author, 'comment': comment}\n        cdt = ContactDowntime(data)\n        contact.add_downtime(cdt)\n        self.send_an_element(contact.get_update_status_brok())", "response": "This function is used to schedule a contact downtime"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nschedule a forced check on a host Format of the line that triggers function call:: SCHEDULE_FORCED_HOST_CHECK;<host_name>;<check_time> :param host: host to check :type host: alignak.object.host.Host :param check_time: time to check :type check_time: int :return: None", "response": "def schedule_forced_host_check(self, host, check_time):\n        \"\"\"Schedule a forced check on a host\n        Format of the line that triggers function call::\n\n        SCHEDULE_FORCED_HOST_CHECK;<host_name>;<check_time>\n\n        :param host: host to check\n        :type host: alignak.object.host.Host\n        :param check_time: time to check\n        :type check_time: int\n        :return: None\n        \"\"\"\n        host.schedule(self.daemon.hosts, self.daemon.services,\n                      self.daemon.timeperiods, self.daemon.macromodulations,\n                      self.daemon.checkmodulations, self.daemon.checks,\n                      force=True, force_time=check_time)\n        self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef schedule_forced_host_svc_checks(self, host, check_time):\n        for service_id in host.services:\n            service = self.daemon.services[service_id]\n            self.schedule_forced_svc_check(service, check_time)\n            self.send_an_element(service.get_update_status_brok())", "response": "This function schedules a forced check on all services of a host."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef schedule_forced_svc_check(self, service, check_time):\n        service.schedule(self.daemon.hosts, self.daemon.services,\n                         self.daemon.timeperiods, self.daemon.macromodulations,\n                         self.daemon.checkmodulations, self.daemon.checks,\n                         force=True, force_time=check_time)\n        self.send_an_element(service.get_update_status_brok())", "response": "This method schedules a forced check on a service."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nschedules a downtime for each host of a hostgroup Format of the line that triggers function call:: SCHEDULE_HOSTGROUP_HOST_DOWNTIME;<hostgroup_name>;<start_time>;<end_time>; <fixed>;<trigger_id>;<duration>;<author>;<comment> :param hostgroup: hostgroup to schedule :type hostgroup: alignak.objects.hostgroup.Hostgroup :param start_time: downtime start time :type start_time: :param end_time: downtime end time :type end_time: :param fixed: is downtime fixed :type fixed: :param trigger_id: downtime id that triggered this one :type trigger_id: str :param duration: downtime duration :type duration: int :param author: downtime author :type author: str :param comment: downtime comment :type comment: str :return: None", "response": "def schedule_hostgroup_host_downtime(self, hostgroup, start_time, end_time, fixed,\n                                         trigger_id, duration, author, comment):\n        \"\"\"Schedule a downtime for each host of a hostgroup\n        Format of the line that triggers function call::\n\n        SCHEDULE_HOSTGROUP_HOST_DOWNTIME;<hostgroup_name>;<start_time>;<end_time>;\n        <fixed>;<trigger_id>;<duration>;<author>;<comment>\n\n        :param hostgroup: hostgroup to schedule\n        :type hostgroup: alignak.objects.hostgroup.Hostgroup\n        :param start_time: downtime start time\n        :type start_time:\n        :param end_time: downtime end time\n        :type end_time:\n        :param fixed: is downtime fixed\n        :type fixed:\n        :param trigger_id: downtime id that triggered this one\n        :type trigger_id: str\n        :param duration: downtime duration\n        :type duration: int\n        :param author: downtime author\n        :type author: str\n        :param comment: downtime comment\n        :type comment: str\n        :return: None\n        \"\"\"\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                host = self.daemon.hosts[host_id]\n                self.schedule_host_downtime(host, start_time, end_time, fixed,\n                                            trigger_id, duration, author, comment)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef schedule_hostgroup_svc_downtime(self, hostgroup, start_time, end_time, fixed,\n                                        trigger_id, duration, author, comment):\n        \"\"\"Schedule a downtime for each service of each host of a hostgroup\n        Format of the line that triggers function call::\n\n        SCHEDULE_HOSTGROUP_SVC_DOWNTIME;;<hostgroup_name>;<start_time>;<end_time>;<fixed>;\n        <trigger_id>;<duration>;<author>;<comment>\n\n        :param hostgroup: hostgroup to schedule\n        :type hostgroup: alignak.objects.hostgroup.Hostgroup\n        :param start_time: downtime start time\n        :type start_time:\n        :param end_time: downtime end time\n        :type end_time:\n        :param fixed: is downtime fixed\n        :type fixed:\n        :param trigger_id: downtime id that triggered this one\n        :type trigger_id: str\n        :param duration: downtime duration\n        :type duration: int\n        :param author: downtime author\n        :type author: str\n        :param comment: downtime comment\n        :type comment: str\n        :return: None\n        \"\"\"\n        for host_id in hostgroup.get_hosts():\n            if host_id in self.daemon.hosts:\n                host = self.daemon.hosts[host_id]\n                for service_id in host.services:\n                    service = self.daemon.services[service_id]\n                    self.schedule_svc_downtime(service, start_time, end_time, fixed,\n                                               trigger_id, duration, author, comment)", "response": "This function schedules a downtime for each service of each host in a hostgroup"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nschedules a check on a host Format of the line that triggers function call:: SCHEDULE_HOST_CHECK;<host_name>;<check_time> :param host: host to check :type host: alignak.object.host.Host :param check_time: time to check :type check_time: :return: None", "response": "def schedule_host_check(self, host, check_time):\n        \"\"\"Schedule a check on a host\n        Format of the line that triggers function call::\n\n        SCHEDULE_HOST_CHECK;<host_name>;<check_time>\n\n        :param host: host to check\n        :type host: alignak.object.host.Host\n        :param check_time: time to check\n        :type check_time:\n        :return: None\n        \"\"\"\n        host.schedule(self.daemon.hosts, self.daemon.services,\n                      self.daemon.timeperiods, self.daemon.macromodulations,\n                      self.daemon.checkmodulations, self.daemon.checks,\n                      force=False, force_time=check_time)\n        self.send_an_element(host.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef schedule_host_downtime(self, host, start_time, end_time, fixed,\n                               trigger_id, duration, author, comment):\n        \"\"\"Schedule a host downtime\n        Format of the line that triggers function call::\n\n        SCHEDULE_HOST_DOWNTIME;<host_name>;<start_time>;<end_time>;<fixed>;\n        <trigger_id>;<duration>;<author>;<comment>\n\n        :param host: host to schedule downtime\n        :type host: alignak.object.host.Host\n        :param start_time: downtime start time\n        :type start_time:\n        :param end_time: downtime end time\n        :type end_time:\n        :param fixed: is downtime fixed\n        :type fixed: bool\n        :param trigger_id: downtime id that triggered this one\n        :type trigger_id: str\n        :param duration: downtime duration\n        :type duration: int\n        :param author: downtime author\n        :type author: str\n        :param comment: downtime comment\n        :type comment: str\n        :return: None\n        \"\"\"\n        data = {'ref': host.uuid, 'ref_type': host.my_type, 'start_time': start_time,\n                'end_time': end_time, 'fixed': fixed, 'trigger_id': trigger_id,\n                'duration': duration, 'author': author, 'comment': comment}\n        downtime = Downtime(data)\n        downtime.add_automatic_comment(host)\n        host.add_downtime(downtime)\n\n        self.send_an_element(host.get_update_status_brok())\n        if trigger_id not in ('', 0):\n            for item in self.daemon.hosts:\n                if trigger_id in item.downtimes:\n                    host.downtimes[trigger_id].trigger_me(downtime.uuid)", "response": "This function is used to schedule a host downtime"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef schedule_host_svc_checks(self, host, check_time):\n        for service_id in host.services:\n            service = self.daemon.services[service_id]\n            self.schedule_svc_check(service, check_time)\n            self.send_an_element(service.get_update_status_brok())", "response": "This function schedules a check on all services of a host."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef schedule_host_svc_downtime(self, host, start_time, end_time, fixed,\n                                   trigger_id, duration, author, comment):\n        \"\"\"Schedule a service downtime for each service of an host\n        Format of the line that triggers function call::\n\n        SCHEDULE_HOST_SVC_DOWNTIME;<host_name>;<start_time>;<end_time>;\n        <fixed>;<trigger_id>;<duration>;<author>;<comment>\n\n        :param host: host to schedule downtime\n        :type host: alignak.object.host.Host\n        :param start_time: downtime start time\n        :type start_time:\n        :param end_time: downtime end time\n        :type end_time:\n        :param fixed: is downtime fixed\n        :type fixed: bool\n        :param trigger_id: downtime id that triggered this one\n        :type trigger_id: str\n        :param duration: downtime duration\n        :type duration: int\n        :param author: downtime author\n        :type author: str\n        :param comment: downtime comment\n        :type comment: str\n        :return: None\n        \"\"\"\n        for serv in host.services:\n            self.schedule_svc_downtime(serv, start_time, end_time, fixed,\n                                       trigger_id, duration, author, comment)", "response": "This function schedules a downtime for each service of a host"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef schedule_servicegroup_host_downtime(self, servicegroup, start_time, end_time,\n                                            fixed, trigger_id, duration, author, comment):\n        \"\"\"Schedule a host downtime for each host of services in a servicegroup\n        Format of the line that triggers function call::\n\n        SCHEDULE_SERVICEGROUP_HOST_DOWNTIME;<servicegroup_name>;<start_time>;<end_time>;<fixed>;\n        <trigger_id>;<duration>;<author>;<comment>\n\n        :param servicegroup: servicegroup to schedule downtime\n        :type servicegroup: alignak.object.servicegroup.Servicegroup\n        :param start_time: downtime start time\n        :type start_time:\n        :param end_time: downtime end time\n        :type end_time:\n        :param fixed: is downtime fixed\n        :type fixed: bool\n        :param trigger_id: downtime id that triggered this one\n        :type trigger_id: str\n        :param duration: downtime duration\n        :type duration: int\n        :param author: downtime author\n        :type author: str\n        :param comment: downtime comment\n        :type comment: str\n        :return: None\n        \"\"\"\n        for host in [s.host for s in servicegroup.get_services()]:\n            self.schedule_host_downtime(host, start_time, end_time, fixed,\n                                        trigger_id, duration, author, comment)", "response": "This function schedules a host downtime for each host of services in a servicegroup"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef schedule_servicegroup_svc_downtime(self, servicegroup, start_time, end_time,\n                                           fixed, trigger_id, duration, author, comment):\n        \"\"\"Schedule a service downtime for each service of a servicegroup\n        Format of the line that triggers function call::\n\n        SCHEDULE_SERVICEGROUP_SVC_DOWNTIME;<servicegroup_name>;<start_time>;<end_time>;\n        <fixed>;<trigger_id>;<duration>;<author>;<comment>\n\n        :param servicegroup: servicegroup to schedule downtime\n        :type servicegroup: alignak.object.servicegroup.Servicegroup\n        :param start_time: downtime start time\n        :type start_time:\n        :param end_time: downtime end time\n        :type end_time:\n        :param fixed: is downtime fixed\n        :type fixed: bool\n        :param trigger_id: downtime id that triggered this one\n        :type trigger_id: str\n        :param duration: downtime duration\n        :type duration: int\n        :param author: downtime author\n        :type author: str\n        :param comment: downtime comment\n        :type comment: str\n        :return: None\n        \"\"\"\n        for serv in servicegroup.get_services():\n            self.schedule_svc_downtime(serv, start_time, end_time, fixed,\n                                       trigger_id, duration, author, comment)", "response": "This function schedules a downtime for each service in a servicegroup"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscheduling a check on a service Format of the line that triggers function call:: SCHEDULE_SVC_CHECK;<host_name>;<service_description>;<check_time> :param service: service to check :type service: alignak.object.service.Service :param check_time: time to check :type check_time: :return: None", "response": "def schedule_svc_check(self, service, check_time):\n        \"\"\"Schedule a check on a service\n        Format of the line that triggers function call::\n\n        SCHEDULE_SVC_CHECK;<host_name>;<service_description>;<check_time>\n\n        :param service: service to check\n        :type service: alignak.object.service.Service\n        :param check_time: time to check\n        :type check_time:\n        :return: None\n        \"\"\"\n        service.schedule(self.daemon.hosts, self.daemon.services,\n                         self.daemon.timeperiods, self.daemon.macromodulations,\n                         self.daemon.checkmodulations, self.daemon.checks,\n                         force=False, force_time=check_time)\n        self.send_an_element(service.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nschedules a service downtime Format of the line that triggers function call:: SCHEDULE_SVC_DOWNTIME;<host_name>;<service_description><start_time>;<end_time>; <fixed>;<trigger_id>;<duration>;<author>;<comment> :param service: service to check :type service: alignak.object.service.Service :param start_time: downtime start time :type start_time: :param end_time: downtime end time :type end_time: :param fixed: is downtime fixed :type fixed: bool :param trigger_id: downtime id that triggered this one :type trigger_id: int :param duration: downtime duration :type duration: int :param author: downtime author :type author: str :param comment: downtime comment :type comment: str :return: None", "response": "def schedule_svc_downtime(self, service, start_time, end_time, fixed,\n                              trigger_id, duration, author, comment):\n        \"\"\"Schedule a service downtime\n        Format of the line that triggers function call::\n\n        SCHEDULE_SVC_DOWNTIME;<host_name>;<service_description><start_time>;<end_time>;\n        <fixed>;<trigger_id>;<duration>;<author>;<comment>\n\n        :param service: service to check\n        :type service: alignak.object.service.Service\n        :param start_time: downtime start time\n        :type start_time:\n        :param end_time: downtime end time\n        :type end_time:\n        :param fixed: is downtime fixed\n        :type fixed: bool\n        :param trigger_id: downtime id that triggered this one\n        :type trigger_id: int\n        :param duration: downtime duration\n        :type duration: int\n        :param author: downtime author\n        :type author: str\n        :param comment: downtime comment\n        :type comment: str\n        :return: None\n        \"\"\"\n        data = {'ref': service.uuid, 'ref_type': service.my_type, 'start_time': start_time,\n                'end_time': end_time, 'fixed': fixed, 'trigger_id': trigger_id,\n                'duration': duration, 'author': author, 'comment': comment}\n        downtime = Downtime(data)\n        downtime.add_automatic_comment(service)\n        service.add_downtime(downtime)\n        self.send_an_element(service.get_update_status_brok())\n        if trigger_id not in ('', 0):\n            for item in self.daemon.services:\n                if trigger_id in item.downtimes:\n                    service.downtimes[trigger_id].trigger_me(downtime.uuid)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a custom host notification", "response": "def send_custom_host_notification(self, host, options, author, comment):\n        \"\"\"DOES NOTHING (Should send a custom notification)\n        Format of the line that triggers function call::\n\n        SEND_CUSTOM_HOST_NOTIFICATION;<host_name>;<options>;<author>;<comment>\n\n        :param host: host to send notif for\n        :type host: alignak.object.host.Host\n        :param options: notification options\n        :type options:\n        :param author: notification author\n        :type author: str\n        :param comment: notification text\n        :type comment: str\n        :return: None\n        \"\"\"\n        logger.warning(\"The external command 'SEND_CUSTOM_HOST_NOTIFICATION' \"\n                       \"is not currently implemented in Alignak. If you really need it, \"\n                       \"request for its implementation in the project repository: \"\n                       \"https://github.com/Alignak-monitoring/alignak\")\n        self.send_an_element(make_monitoring_log(\n            'warning', 'SEND_CUSTOM_HOST_NOTIFICATION: this command is not implemented!'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a custom notification for a specific service", "response": "def send_custom_svc_notification(self, service, options, author, comment):\n        \"\"\"DOES NOTHING (Should send a custom notification)\n        Format of the line that triggers function call::\n\n        SEND_CUSTOM_SVC_NOTIFICATION;<host_name>;<service_description>;<options>;<author>;<comment>>\n\n        :param service: service to send notif for\n        :type service: alignak.object.service.Service\n        :param options: notification options\n        :type options:\n        :param author: notification author\n        :type author: str\n        :param comment: notification text\n        :type comment: str\n        :return: None\n        \"\"\"\n        logger.warning(\"The external command 'SEND_CUSTOM_SVC_NOTIFICATION' \"\n                       \"is not currently implemented in Alignak. If you really need it, \"\n                       \"request for its implementation in the project repository: \"\n                       \"https://github.com/Alignak-monitoring/alignak\")\n        self.send_an_element(make_monitoring_log(\n            'warning', 'SEND_CUSTOM_SVC_NOTIFICATION: this command is not implemented!'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_accepting_passive_host_checks(self):\n        # todo: #783 create a dedicated brok for global parameters\n        if not self.my_conf.accept_passive_host_checks:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_PASSIVE_CHECKS_ENABLED\"].value\n            self.my_conf.accept_passive_host_checks = True\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Enable passive host check submission ( globally"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling passive service check submission ( globally )", "response": "def start_accepting_passive_svc_checks(self):\n        \"\"\"Enable passive service check submission (globally)\n        Format of the line that triggers function call::\n\n        START_ACCEPTING_PASSIVE_SVC_CHECKS\n\n        :return: None\n        \"\"\"\n        # todo: #783 create a dedicated brok for global parameters\n        if not self.my_conf.accept_passive_service_checks:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_PASSIVE_CHECKS_ENABLED\"].value\n            self.my_conf.accept_passive_service_checks = True\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_executing_host_checks(self):\n        # todo: #783 create a dedicated brok for global parameters\n        if not self.my_conf.execute_host_checks:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_ACTIVE_CHECKS_ENABLED\"].value\n            self.my_conf.execute_host_checks = True\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Enable host check execution ( globally )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nenables service check execution ( globally", "response": "def start_executing_svc_checks(self):\n        \"\"\"Enable service check execution (globally)\n        Format of the line that triggers function call::\n\n        START_EXECUTING_SVC_CHECKS\n\n        :return: None\n        \"\"\"\n        # todo: #783 create a dedicated brok for global parameters\n        if not self.my_conf.execute_service_checks:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_ACTIVE_CHECKS_ENABLED\"].value\n            self.my_conf.execute_service_checks = True\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop_accepting_passive_host_checks(self):\n        if self.my_conf.accept_passive_host_checks:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_PASSIVE_CHECKS_ENABLED\"].value\n            self.my_conf.accept_passive_host_checks = False\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Disable passive host check submission ( globally"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stop_accepting_passive_svc_checks(self):\n        if self.my_conf.accept_passive_service_checks:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_PASSIVE_CHECKS_ENABLED\"].value\n            self.my_conf.accept_passive_service_checks = False\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Disable passive service check submission ( globally )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisabling host check execution ( globally )", "response": "def stop_executing_host_checks(self):\n        \"\"\"Disable host check execution (globally)\n        Format of the line that triggers function call::\n\n        STOP_EXECUTING_HOST_CHECKS\n\n        :return: None\n        \"\"\"\n        if self.my_conf.execute_host_checks:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_ACTIVE_CHECKS_ENABLED\"].value\n            self.my_conf.execute_host_checks = False\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop_executing_svc_checks(self):\n        if self.my_conf.execute_service_checks:\n            self.my_conf.modified_attributes |= DICT_MODATTR[\"MODATTR_ACTIVE_CHECKS_ENABLED\"].value\n            self.my_conf.execute_service_checks = False\n            self.my_conf.explode_global_conf()\n            self.daemon.update_program_status()", "response": "Disable service check execution ( globally )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef launch_svc_event_handler(self, service):\n        service.get_event_handlers(self.hosts, self.daemon.macromodulations,\n                                   self.daemon.timeperiods, ext_cmd=True)", "response": "Launch the event handler for a service"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlaunches event handler for a service", "response": "def launch_host_event_handler(self, host):\n        \"\"\"Launch event handler for a service\n        Format of the line that triggers function call::\n\n        LAUNCH_HOST_EVENT_HANDLER;<host_name>\n\n        :param host: host to execute the event handler\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n        host.get_event_handlers(self.hosts, self.daemon.macromodulations, self.daemon.timeperiods,\n                                ext_cmd=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget name based on dependent_host_name and host_name attributes", "response": "def get_name(self):\n        \"\"\"Get name based on dependent_host_name and host_name attributes\n        Each attribute is substituted by 'unknown' if attribute does not exist\n\n        :return: dependent_host_name/host_name\n        :rtype: str\n        \"\"\"\n        dependent_host_name = 'unknown'\n        if getattr(self, 'dependent_host_name', None):\n            dependent_host_name = getattr(\n                getattr(self, 'dependent_host_name'), 'host_name', 'unknown'\n            )\n        host_name = 'unknown'\n        if getattr(self, 'host_name', None):\n            host_name = getattr(getattr(self, 'host_name'), 'host_name', 'unknown')\n        return dependent_host_name + '/' + host_name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexplode all host dependency for each member of hostgroup or hostgroup in dependency", "response": "def explode(self, hostgroups):\n        # pylint: disable=too-many-locals\n        \"\"\"Explode all host dependency for each member of hostgroups\n        Each member of dependent hostgroup or hostgroup in dependency have to get a copy of\n        host dependencies (quite complex to parse)\n\n\n        :param hostgroups: used to look for hostgroup\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :return: None\n        \"\"\"\n        # The \"old\" dependencies will be removed. All dependencies with\n        # more than one host or a host group will be in it\n        hstdep_to_remove = []\n\n        # Then for every host create a copy of the dependency with just the host\n        # because we are adding services, we can't just loop in it\n        hostdeps = list(self.items.keys())\n        for h_id in hostdeps:\n            hostdep = self.items[h_id]\n            # We explode first the dependent (son) part\n            dephnames = []\n            if hasattr(hostdep, 'dependent_hostgroup_name'):\n                dephg_names = [n.strip() for n in hostdep.dependent_hostgroup_name.split(',')]\n                for dephg_name in dephg_names:\n                    dephg = hostgroups.find_by_name(dephg_name)\n                    if dephg is None:\n                        err = \"ERROR: the hostdependency got \" \\\n                              \"an unknown dependent_hostgroup_name '%s'\" % dephg_name\n                        hostdep.add_error(err)\n                        continue\n                    dephnames.extend([m.strip() for m in dephg.get_hosts()])\n\n            if hasattr(hostdep, 'dependent_host_name'):\n                dephnames.extend([n.strip() for n in hostdep.dependent_host_name.split(',')])\n\n            # Ok, and now the father part :)\n            hnames = []\n            if hasattr(hostdep, 'hostgroup_name'):\n                hg_names = [n.strip() for n in hostdep.hostgroup_name.split(',')]\n                for hg_name in hg_names:\n                    hostgroup = hostgroups.find_by_name(hg_name)\n                    if hostgroup is None:\n                        err = \"ERROR: the hostdependency got\" \\\n                              \" an unknown hostgroup_name '%s'\" % hg_name\n                        hostdep.add_error(err)\n                        continue\n                    hnames.extend([m.strip() for m in hostgroup.get_hosts()])\n\n            if hasattr(hostdep, 'host_name'):\n                hnames.extend([n.strip() for n in hostdep.host_name.split(',')])\n\n            # Loop over all sons and fathers to get S*F host deps\n            for dephname in dephnames:\n                dephname = dephname.strip()\n                for hname in hnames:\n                    new_hd = hostdep.copy()\n                    new_hd.dependent_host_name = dephname\n                    new_hd.host_name = hname\n                    self.add_item(new_hd)\n            hstdep_to_remove.append(h_id)\n\n        self.delete_hostsdep_by_id(hstdep_to_remove)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate link between objects. hosts and timeperiods.", "response": "def linkify(self, hosts, timeperiods):\n        \"\"\"Create link between objects::\n\n         * hostdependency -> host\n         * hostdependency -> timeperiods\n\n        :param hosts: hosts to link\n        :type hosts: alignak.objects.host.Hosts\n        :param timeperiods: timeperiods to link\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :return: None\n        \"\"\"\n        self.linkify_hd_by_h(hosts)\n        self.linkify_hd_by_tp(timeperiods)\n        self.linkify_h_by_hd(hosts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces dependent_host_name and host_name in host dependency by real object", "response": "def linkify_hd_by_h(self, hosts):\n        \"\"\"Replace dependent_host_name and host_name\n        in host dependency by the real object\n\n        :param hosts: host list, used to look for a specific one\n        :type hosts: alignak.objects.host.Hosts\n        :return: None\n        \"\"\"\n        for hostdep in self:\n            try:\n                h_name = hostdep.host_name\n                dh_name = hostdep.dependent_host_name\n                host = hosts.find_by_name(h_name)\n                if host is None:\n                    err = \"Error: the host dependency got a bad host_name definition '%s'\" % h_name\n                    hostdep.add_error(err)\n                dephost = hosts.find_by_name(dh_name)\n                if dephost is None:\n                    err = \"Error: the host dependency got \" \\\n                          \"a bad dependent_host_name definition '%s'\" % dh_name\n                    hostdep.add_error(err)\n                if host:\n                    hostdep.host_name = host.uuid\n                if dephost:\n                    hostdep.dependent_host_name = dephost.uuid\n            except AttributeError as exp:\n                err = \"Error: the host dependency miss a property '%s'\" % exp\n                hostdep.add_error(err)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreplacing dependency_period by a real object in host dependency_object", "response": "def linkify_hd_by_tp(self, timeperiods):\n        \"\"\"Replace dependency_period by a real object in host dependency\n\n        :param timeperiods: list of timeperiod, used to look for a specific one\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :return: None\n        \"\"\"\n        for hostdep in self:\n            try:\n                tp_name = hostdep.dependency_period\n                timeperiod = timeperiods.find_by_name(tp_name)\n                if timeperiod:\n                    hostdep.dependency_period = timeperiod.uuid\n                else:\n                    hostdep.dependency_period = ''\n            except AttributeError as exp:  # pragma: no cover, simple protectionn\n                logger.error(\"[hostdependency] fail to linkify by timeperiod: %s\", exp)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlinking the dependency in host objects by HD.", "response": "def linkify_h_by_hd(self, hosts):\n        \"\"\"Add dependency in host objects\n        :param hosts: hosts list\n        :type hosts: alignak.objects.host.Hosts\n\n        :return: None\n        \"\"\"\n        for hostdep in self:\n            # Only used for debugging purpose when loops are detected\n            setattr(hostdep, \"host_name_string\", \"undefined\")\n            setattr(hostdep, \"dependent_host_name_string\", \"undefined\")\n\n            # if the host dep conf is bad, pass this one\n            if getattr(hostdep, 'host_name', None) is None or\\\n                    getattr(hostdep, 'dependent_host_name', None) is None:\n                continue\n\n            if hostdep.host_name not in hosts or hostdep.dependent_host_name not in hosts:\n                continue\n\n            hosts.add_act_dependency(hostdep.dependent_host_name, hostdep.host_name,\n                                     hostdep.notification_failure_criteria,\n                                     getattr(hostdep, 'dependency_period', ''),\n                                     hostdep.inherits_parent)\n\n            hosts.add_chk_dependency(hostdep.dependent_host_name, hostdep.host_name,\n                                     hostdep.execution_failure_criteria,\n                                     getattr(hostdep, 'dependency_period', ''),\n                                     hostdep.inherits_parent)\n\n            # Only used for debugging purpose when loops are detected\n            setattr(hostdep, \"host_name_string\", hosts[hostdep.host_name].get_name())\n            setattr(hostdep, \"dependent_host_name_string\",\n                    hosts[hostdep.dependent_host_name].get_name())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_correct(self):\n        state = True\n\n        # Internal checks before executing inherited function...\n        loop = self.no_loop_in_parents(\"host_name\", \"dependent_host_name\")\n        if loop:\n            msg = \"Loop detected while checking host dependencies\"\n            self.add_error(msg)\n            state = False\n            for item in self:\n                for elem in loop:\n                    if elem == item.host_name:\n                        msg = \"Host %s is parent host_name in dependency defined in %s\" % (\n                            item.host_name_string, item.imported_from\n                        )\n                        self.add_error(msg)\n                    elif elem == item.dependent_host_name:\n                        msg = \"Host %s is child host_name in dependency defined in %s\" % (\n                            item.dependent_host_name_string, item.imported_from\n                        )\n                        self.add_error(msg)\n\n        return super(Hostdependencies, self).is_correct() and state", "response": "Check if this object configuration is correct."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge(self, hosts):\n        for extinfo in self:\n            host_name = extinfo.get_name()\n            host = hosts.find_by_name(host_name)\n            if host is not None:\n                # Fusion\n                self.merge_extinfo(host, extinfo)", "response": "Merge extended host information into services\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging extended host information into a host object.", "response": "def merge_extinfo(host, extinfo):\n        \"\"\"Merge extended host information into a host\n\n        :param host: the host to edit\n        :type host: alignak.objects.host.Host\n        :param extinfo: the external info we get data from\n        :type extinfo: alignak.objects.hostextinfo.HostExtInfo\n        :return: None\n        \"\"\"\n        # Note that 2d_coords and 3d_coords are never merged, so not usable !\n        properties = ['notes', 'notes_url', 'icon_image', 'icon_image_alt',\n                      'vrml_image', 'statusmap_image']\n        # host properties have precedence over hostextinfo properties\n        for prop in properties:\n            if getattr(host, prop) == '' and getattr(extinfo, prop) != '':\n                setattr(host, prop, getattr(extinfo, prop))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets HTTP proxy url", "response": "def set_proxy(self, proxy):  # pragma: no cover, not with unit tests\n        \"\"\"Set HTTP proxy\n\n        :param proxy: proxy url\n        :type proxy: str\n        :return: None\n        \"\"\"\n        if proxy:\n            logger.debug('PROXY SETTING PROXY %s', proxy)\n            self._requests_con.proxies = {\n                'http': proxy,\n                'https': proxy,\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef post(self, path, args, wait=False):\n        uri = self.make_uri(path)\n        timeout = self.make_timeout(wait)\n        for (key, value) in list(args.items()):\n            args[key] = serialize(value, True)\n        try:\n            logger.debug(\"post: %s, timeout: %s, params: %s\", uri, timeout, args)\n            rsp = self._requests_con.post(uri, json=args, timeout=timeout, verify=self.strong_ssl)\n            logger.debug(\"got: %d - %s\", rsp.status_code, rsp.text)\n            if rsp.status_code != 200:\n                raise HTTPClientDataException(rsp.status_code, rsp.text, uri)\n            return rsp.content\n        except (requests.Timeout, requests.ConnectTimeout):\n            raise HTTPClientTimeoutException(timeout, uri)\n        except requests.ConnectionError as exp:\n            raise HTTPClientConnectionException(uri, exp.args[0])\n        except Exception as exp:\n            raise HTTPClientException('Request error to %s: %s' % (uri, exp))", "response": "POST an HTTP request to a daemon s cache file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nput request to a daemon s cache entry", "response": "def put(self, path, args, wait=False):  # pragma: no cover, looks never used!\n        # todo: remove this because it looks never used anywhere...\n        \"\"\"PUT and HTTP request to a daemon\n\n        :param path: path to do the request\n        :type path: str\n        :param args: data to send in the request\n        :type args:\n        :return: Content of the HTTP response if server returned 200\n        :rtype: str\n        \"\"\"\n        uri = self.make_uri(path)\n        timeout = self.make_timeout(wait)\n        try:\n            logger.debug(\"put: %s, timeout: %s, params: %s\", uri, timeout, args)\n            rsp = self._requests_con.put(uri, args, timeout=timeout, verify=self.strong_ssl)\n            logger.debug(\"got: %d - %s\", rsp.status_code, rsp.text)\n            if rsp.status_code != 200:\n                raise HTTPClientDataException(rsp.status_code, rsp.text, uri)\n            return rsp.content\n        except (requests.Timeout, requests.ConnectTimeout):\n            raise HTTPClientTimeoutException(timeout, uri)\n        except requests.ConnectionError as exp:\n            raise HTTPClientConnectionException(uri, exp.args[0])\n        except Exception as exp:\n            raise HTTPClientException('Request error to %s: %s' % (uri, exp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an Escalation object for each HostEscalation object in the list of escalations.", "response": "def explode(self, escalations):\n        \"\"\"Create instance of Escalation for each HostEscalation object\n\n        :param escalations: list of escalation, used to add new ones\n        :type escalations: alignak.objects.escalation.Escalations\n        :return: None\n        \"\"\"\n        # Now we explode all escalations (host_name, hostgroup_name) to escalations\n        for escalation in self:\n            properties = escalation.__class__.properties\n            name = getattr(escalation, 'host_name', getattr(escalation, 'hostgroup_name', ''))\n            creation_dict = {\n                'escalation_name':\n                    'Generated-HE-%s-%s' % (name, escalation.uuid)\n            }\n            for prop in properties:\n                if hasattr(escalation, prop):\n                    creation_dict[prop] = getattr(escalation, prop)\n\n            escalations.add_escalation(Escalation(creation_dict))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize(self):\n        return {'uuid': self.uuid, 'ref': self.ref, 'sticky': self.sticky, 'notify': self.notify,\n                'end_time': self.end_time, 'author': self.author, 'comment': self.comment}", "response": "This function serialize into a simple dict object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register(self, name, _type, statsd_host='localhost', statsd_port=8125,\n                 statsd_prefix='alignak', statsd_enabled=False, broks_enabled=False):\n        \"\"\"Init instance with real values\n\n        :param name: daemon name\n        :type name: str\n        :param _type: daemon type\n        :type _type:\n        :param statsd_host: host to post data\n        :type statsd_host: str\n        :param statsd_port: port to post data\n        :type statsd_port: int\n        :param statsd_prefix: prefix to add to metric\n        :type statsd_prefix: str\n        :param statsd_enabled: bool to enable statsd\n        :type statsd_enabled: bool\n        :param broks_enabled: bool to enable broks sending\n        :type broks_enabled: bool\n        :return: None\n        \"\"\"\n        self.name = name\n        # This attribute is not used, but I keep ascending compatibility with former interface!\n        self._type = _type\n\n        # local statsd part\n        self.statsd_host = statsd_host\n        self.statsd_port = int(statsd_port)\n        self.statsd_prefix = statsd_prefix\n        self.statsd_enabled = statsd_enabled\n\n        # local broks part\n        self.broks_enabled = broks_enabled\n\n        logger.debug(\"StatsD configuration for %s - %s:%s, prefix: %s, \"\n                     \"enabled: %s, broks: %s, file: %s\",\n                     self.name, self.statsd_host, self.statsd_port,\n                     self.statsd_prefix, self.statsd_enabled, self.broks_enabled,\n                     self.stats_file)\n\n        if self.statsd_enabled and self.statsd_host is not None and self.statsd_host != 'None':\n            logger.info(\"Sending %s statistics to: %s:%s, prefix: %s\",\n                        self.name, self.statsd_host, self.statsd_port, self.statsd_prefix)\n            if self.load_statsd():\n                logger.info('Alignak internal statistics are sent to StatsD.')\n            else:\n                logger.info('StatsD server is not available.')\n\n        if self.stats_file:\n            try:\n                self.file_d = open(self.stats_file, 'a')\n                logger.info(\"Alignak internal statistics are written in the file %s\",\n                            self.stats_file)\n            except OSError as exp:  # pragma: no cover, should never happen...\n                logger.exception(\"Error when opening the file '%s' : %s\", self.stats_file, exp)\n                self.file_d = None\n\n        return self.statsd_enabled", "response": "This method is used to register a new local statsd instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a statsd socket connection to statsd server and add the data to the carbon list", "response": "def load_statsd(self):\n        \"\"\"Create socket connection to statsd host\n\n        Note that because of the UDP protocol used by StatsD, if no server is listening the\n        socket connection will be accepted anyway :)\n\n        :return: True if socket got created else False and an exception log is raised\n        \"\"\"\n        if not self.statsd_enabled:\n            logger.info('Stats reporting is not enabled, connection is not allowed')\n            return False\n\n        if self.statsd_enabled and self.carbon:\n            self.my_metrics.append(('.'.join([self.statsd_prefix, self.name, 'connection-test']),\n                                    (int(time.time()), int(time.time()))))\n            self.carbon.add_data_list(self.my_metrics)\n            self.flush(log=True)\n        else:\n            try:\n                logger.info('Trying to contact StatsD server...')\n                self.statsd_addr = (socket.gethostbyname(self.statsd_host.encode('utf-8')),\n                                    self.statsd_port)\n                self.statsd_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n            except (socket.error, socket.gaierror) as exp:\n                logger.warning('Cannot create StatsD socket: %s', exp)\n                return False\n            except Exception as exp:  # pylint: disable=broad-except\n                logger.exception('Cannot create StatsD socket (other): %s', exp)\n                return False\n\n            logger.info('StatsD server contacted')\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect to a daemon and return a new instance of this class", "response": "def connect(self, name, _type, host='localhost', port=2004,\n                prefix='alignak', enabled=False, broks_enabled=False):\n        \"\"\"Init instance with real values for a graphite/carbon connection\n\n        :param name: daemon name\n        :type name: str\n        :param _type: daemon type\n        :type _type:\n        :param host: host to post data\n        :type host: str\n        :param port: port to post data\n        :type port: int\n        :param prefix: prefix to add to metric\n        :type prefix: str\n        :param enabled: bool to enable statsd\n        :type enabled: bool\n        :param broks_enabled: bool to enable broks sending\n        :type broks_enabled: bool\n        :return: None\n        \"\"\"\n        self.name = name\n        # This attribute is not used, but I keep ascending compatibility with former interface!\n        self._type = _type\n\n        # local graphite/carbon part\n        self.statsd_host = host\n        try:\n            self.statsd_port = int(port)\n        except ValueError:\n            self.statsd_port = 2004\n        self.statsd_prefix = prefix\n        self.statsd_enabled = enabled\n\n        # local broks part\n        self.broks_enabled = broks_enabled\n\n        logger.debug(\"Graphite/carbon configuration for %s - %s:%s, prefix: %s, \"\n                     \"enabled: %s, broks: %s, file: %s\",\n                     self.name, self.statsd_host, self.statsd_port,\n                     self.statsd_prefix, self.statsd_enabled, self.broks_enabled,\n                     self.stats_file)\n\n        if self.statsd_enabled and self.statsd_host is not None and self.statsd_host != 'None':\n            logger.info(\"Sending %s statistics to: %s:%s, prefix: %s\",\n                        self.name, self.statsd_host, self.statsd_port, self.statsd_prefix)\n\n            self.carbon = CarbonIface(self.statsd_host, self.statsd_port)\n            logger.info('Alignak internal statistics will be sent to Graphite.')\n\n        return self.statsd_enabled"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flush(self, log=False):\n        if not self.my_metrics:\n            logger.debug(\"Flushing - no metrics to send\")\n            return True\n\n        now = int(time.time())\n        if self.last_failure and self.last_failure + self.metrics_flush_pause > now:\n            if not self.log_metrics_flush_pause:\n                date = datetime.datetime.fromtimestamp(\n                    self.last_failure).strftime(self.date_fmt)\n                logger.warning(\"Metrics flush paused on connection error \"\n                               \"(last failed: %s). \"\n                               \"Inner stored metric: %d. Trying to send...\",\n                               date, self.metrics_count)\n                self.log_metrics_flush_pause = True\n            return True\n\n        try:\n            logger.debug(\"Flushing %d metrics to Graphite/carbon\", self.metrics_count)\n            if self.carbon.send_data():\n                self.my_metrics = []\n            else:\n                logger.warning(\"Failed sending metrics to Graphite/carbon. \"\n                               \"Inner stored metric: %d\", self.metrics_count)\n                if log:\n                    logger.warning(\"Failed sending metrics to Graphite/carbon. \"\n                                   \"Inner stored metric: %d\", self.metrics_count)\n                return False\n            if self.log_metrics_flush_pause:\n                logger.warning(\"Metrics flush restored. \"\n                               \"Remaining stored metric: %d\", self.metrics_count)\n            self.last_failure = 0\n            self.log_metrics_flush_pause = False\n        except Exception as exp:  # pylint: disable=broad-except\n            if not self.log_metrics_flush_pause:\n                logger.warning(\"Failed sending metrics to Graphite/carbon. \"\n                               \"Inner stored metric: %d\", self.metrics_count)\n            else:\n                date = datetime.datetime.fromtimestamp(\n                    self.last_failure).strftime(self.date_fmt)\n                logger.warning(\"Metrics flush paused on connection error \"\n                               \"(last failed: %s). \"\n                               \"Inner stored metric: %d. Trying to send...\",\n                               date, self.metrics_count)\n\n            logger.warning(\"Exception: %s\", str(exp))\n            self.last_failure = now\n            return False\n        return True", "response": "Send the inner stored metrics to the Graphite server. Returns False if the sending failed with a warning log."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting a counter value for the inner key.", "response": "def counter(self, key, value, timestamp=None):\n        \"\"\"Set a counter value\n\n        If the inner key does not exist is is created\n\n        :param key: counter to update\n        :type key: str\n        :param value: counter value\n        :type value: float\n        :return: An alignak_stat brok if broks are enabled else None\n        \"\"\"\n        _min, _max, count, _sum = self.stats.get(key, (None, None, 0, 0))\n        count += 1\n        _sum += value\n        if _min is None or value < _min:\n            _min = value\n        if _max is None or value > _max:\n            _max = value\n        self.stats[key] = (_min, _max, count, _sum)\n\n        # Manage local statsd part\n        if self.statsd_enabled and self.statsd_sock:\n            # beware, we are sending ms here, timer is in seconds\n            packet = '%s.%s.%s:%d|c' % (self.statsd_prefix, self.name, key, value)\n            packet = packet.encode('utf-8')\n            try:\n                self.statsd_sock.sendto(packet, self.statsd_addr)\n            except (socket.error, socket.gaierror):\n                pass\n                # cannot send? ok not a huge problem here and we cannot\n                # log because it will be far too verbose :p\n\n        # Manage Graphite part\n        if self.statsd_enabled and self.carbon:\n            self.send_to_graphite(key, value, timestamp=timestamp)\n\n        # Manage file part\n        if self.statsd_enabled and self.file_d:\n            if timestamp is None:\n                timestamp = int(time.time())\n\n            packet = self.line_fmt\n            if not self.date_fmt:\n                date = \"%s\" % timestamp\n            else:\n                date = datetime.datetime.fromtimestamp(timestamp).strftime(self.date_fmt)\n            packet = packet.replace(\"#date#\", date)\n            packet = packet.replace(\"#counter#\", '%s.%s.%s' % (self.statsd_prefix, self.name, key))\n            packet = packet.replace(\"#value#\", '%d' % value)\n            packet = packet.replace(\"#uom#\", 'c')\n            try:\n                self.file_d.write(packet)\n            except IOError:\n                logger.warning(\"Could not write to the file: %s\", packet)\n\n        if self.broks_enabled:\n            logger.debug(\"alignak stat brok: %s = %s\", key, value)\n            if timestamp is None:\n                timestamp = int(time.time())\n\n            return Brok({'type': 'alignak_stat',\n                         'data': {\n                             'ts': timestamp,\n                             'type': 'counter',\n                             'metric': '%s.%s.%s' % (self.statsd_prefix, self.name, key),\n                             'value': value,\n                             'uom': 'c'\n                         }})\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the configurations managed by this satellite", "response": "def get_managed_configurations(self):\n        \"\"\"Get the configurations managed by this satellite\n\n        The configurations managed by a satellite is a list of the configuration attached to\n        the schedulers related to the satellites. A broker linked to several schedulers\n        will return the list of the configuration parts of its scheduler links.\n\n        :return: a dict of scheduler links with instance_id as key and\n        hash, push_flavor and configuration identifier as values\n        :rtype: dict\n        \"\"\"\n        res = {}\n        for scheduler_link in list(self.schedulers.values()):\n            res[scheduler_link.instance_id] = {\n                'hash': scheduler_link.hash,\n                'push_flavor': scheduler_link.push_flavor,\n                'managed_conf_id': scheduler_link.managed_conf_id\n            }\n        logger.debug(\"Get managed configuration: %s\", res)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_scheduler_from_hostname(self, host_name):\n        scheduler_uuid = self.hosts_schedulers.get(host_name, None)\n        return self.schedulers.get(scheduler_uuid, None)", "response": "Get the scheduler with id corresponding to the given host_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_external_commands(self):\n        res = self.external_commands\n        logger.debug(\"Get and clear external commands list: %s\", res)\n        self.external_commands = []\n        return res", "response": "Get the external commands list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget executed actions results from a passive satellite for a specific scheduler_instance_id.", "response": "def get_results_from_passive(self, scheduler_instance_id):\n        \"\"\"Get executed actions results from a passive satellite for a specific scheduler\n\n        :param scheduler_instance_id: scheduler id\n        :type scheduler_instance_id: int\n        :return: Results list\n        :rtype: list\n        \"\"\"\n        # Do I know this scheduler?\n        # logger.info(\"My schedulers: %s %s\", self.schedulers, type(self.schedulers))\n        if not self.schedulers:\n            # Probably not yet configured ...\n            logger.debug(\"I do not have any scheduler: %s\", self.schedulers)\n            return []\n\n        scheduler_link = None\n        for link in list(self.schedulers.values()):\n            if scheduler_instance_id == link.instance_id:\n                scheduler_link = link\n                break\n        else:\n            logger.warning(\"I do not know this scheduler: %s\", scheduler_instance_id)\n            return []\n\n        logger.debug(\"Get results for the scheduler: %s\", scheduler_instance_id)\n        ret, scheduler_link.wait_homerun = scheduler_link.wait_homerun, {}\n        logger.debug(\"Results: %s\" % (list(ret.values())) if ret else \"No results available\")\n\n        return list(ret.values())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclean variables from previous configuration and broks and external commands.", "response": "def clean_previous_run(self):\n        \"\"\"Clean variables from previous configuration,\n        such as schedulers, broks and external commands\n\n        :return: None\n        \"\"\"\n        # Clean all lists\n        self.arbiters.clear()\n        self.schedulers.clear()\n        with self.external_commands_lock:\n            self.external_commands = self.external_commands[:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the event list from satellite", "response": "def get_events(self):\n        \"\"\"Get event list from satellite\n\n        :return: A copy of the events list\n        :rtype: list\n        \"\"\"\n        res = copy.copy(self.events)\n        del self.events[:]\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nincreases the stats provided by the Daemon base class", "response": "def get_daemon_stats(self, details=False):\n        \"\"\"Increase the stats provided by the Daemon base class\n\n        :return: stats dictionary\n        :rtype: dict\n        \"\"\"\n        # call the daemon one\n        res = super(BaseSatellite, self).get_daemon_stats(details=details)\n\n        counters = res['counters']\n        counters['external-commands'] = len(self.external_commands)\n        counters['satellites.arbiters'] = len(self.arbiters)\n        counters['satellites.schedulers'] = len(self.schedulers)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef manage_action_return(self, action):\n        # Maybe our workers send us something else than an action\n        # if so, just add this in other queues and return\n        # todo: test a class instance\n        if action.__class__.my_type not in ['check', 'notification', 'eventhandler']:\n            self.add(action)\n            return\n\n        # Ok, it's a result. Get the concerned scheduler uuid\n        scheduler_uuid = action.my_scheduler\n        logger.debug(\"Got action return: %s / %s\", scheduler_uuid, action.uuid)\n\n        try:\n            # Now that we know where to put the action result, we do not need any reference to\n            # the scheduler nor the worker\n            del action.my_scheduler\n            del action.my_worker\n        except AttributeError:  # pragma: no cover, simple protection\n            logger.error(\"AttributeError Got action return: %s / %s\", scheduler_uuid, action)\n\n        # And we remove it from the actions queue of the scheduler too\n        try:\n            del self.schedulers[scheduler_uuid].actions[action.uuid]\n        except KeyError as exp:\n            logger.error(\"KeyError del scheduler action: %s / %s - %s\",\n                         scheduler_uuid, action.uuid, str(exp))\n\n        # We tag it as \"return wanted\", and move it in the wait return queue\n        try:\n            self.schedulers[scheduler_uuid].wait_homerun[action.uuid] = action\n        except KeyError:  # pragma: no cover, simple protection\n            logger.error(\"KeyError Add home run action: %s / %s - %s\",\n                         scheduler_uuid, action.uuid, str(exp))", "response": "Manage action return from Workers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npushing the checks and actions results to our schedulers .", "response": "def push_results(self):\n        \"\"\"Push the checks/actions results to our schedulers\n\n        :return: None\n        \"\"\"\n        # For all schedulers, we check for wait_homerun\n        # and we send back results\n        for scheduler_link_uuid in self.schedulers:\n            scheduler_link = self.schedulers[scheduler_link_uuid]\n            if not scheduler_link.active:\n                logger.warning(\"My scheduler '%s' is not active currently\", scheduler_link.name)\n                continue\n\n            if not scheduler_link.wait_homerun:\n                # Nothing to push back...\n                continue\n\n            # NB: it's **mostly** safe for us to not use some lock around\n            # this 'results' / sched['wait_homerun'].\n            # Because it can only be modified (for adding new values) by the\n            # same thread running this function (that is the main satellite\n            # thread), and this occurs exactly in self.manage_action_return().\n            # Another possibility is for the sched['wait_homerun'] to be\n            # cleared within/by :\n            # ISchedulers.get_results() -> Satelitte.get_return_for_passive()\n            # This can so happen in an (http) client thread.\n            results = scheduler_link.wait_homerun\n            logger.debug(\"Pushing %d results to '%s'\", len(results), scheduler_link.name)\n\n            # So, at worst, some results would be received twice on the\n            # scheduler level, which shouldn't be a problem given they are\n            # indexed by their \"action_id\".\n\n            scheduler_link.push_results(list(results.values()), self.name)\n            results.clear()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_and_launch_worker(self, module_name='fork'):\n        logger.info(\"Allocating new '%s' worker...\", module_name)\n\n        # If we are in the fork module, we do not specify a target\n        target = None\n        __warned = []\n        if module_name == 'fork':\n            target = None\n        else:\n            for module in self.modules_manager.instances:\n                # First, see if the module name matches...\n                if module.get_name() == module_name:\n                    # ... and then if is a 'worker' module one or not\n                    if not module.properties.get('worker_capable', False):\n                        raise NotWorkerMod\n                    target = module.work\n            if target is None:\n                if module_name not in __warned:\n                    logger.warning(\"No target found for %s, NOT creating a worker for it...\",\n                                   module_name)\n                    __warned.append(module_name)\n                return\n        # We give to the Worker the instance name of the daemon (eg. poller-master)\n        # and not the daemon type (poller)\n        queue = Queue()\n        worker = Worker(module_name, queue, self.returns_queue, self.processes_by_worker,\n                        max_plugins_output_length=self.max_plugins_output_length,\n                        target=target, loaded_into=self.name)\n        # worker.module_name = module_name\n        # save this worker\n        self.workers[worker.get_id()] = worker\n\n        # And save the Queue of this worker, with key = worker id\n        # self.q_by_mod[module_name][worker.uuid] = queue\n        self.q_by_mod[module_name][worker.get_id()] = queue\n\n        # Ok, all is good. Start it!\n        worker.start()\n\n        logger.info(\"Started '%s' worker: %s (pid=%d)\",\n                    module_name, worker.get_id(), worker.get_pid())", "response": "Create and launch a new worker and put it into self. workers\n        "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops all workers and join them", "response": "def do_stop_workers(self):\n        \"\"\"Stop all workers\n\n        :return: None\n        \"\"\"\n        logger.info(\"Stopping all workers (%d)\", len(self.workers))\n        for worker in list(self.workers.values()):\n            try:\n                logger.info(\" - stopping '%s'\", worker.get_id())\n                worker.terminate()\n                worker.join(timeout=1)\n                logger.info(\" - stopped\")\n            # A already dead worker or in a worker\n            except (AttributeError, AssertionError):\n                pass\n            except Exception as exp:  # pylint: disable=broad-except\n                logger.error(\"exception: %s\", str(exp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_broks(self):\n        res = copy.copy(self.broks)\n        del self.broks[:]\n        return res", "response": "Get the brok list from satellite\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if workers are fine and kill them if not.", "response": "def check_and_del_zombie_workers(self):  # pragma: no cover, not with unit tests...\n        # pylint: disable= not-callable\n        \"\"\"Check if worker are fine and kill them if not.\n        Dispatch the actions in the worker to another one\n\n        TODO: see if unit tests would allow to check this code?\n\n        :return: None\n        \"\"\"\n        # Active children make a join with everyone, useful :)\n        # active_children()\n        for p in active_children():\n            logger.debug(\"got child: %s\", p)\n\n        w_to_del = []\n        for worker in list(self.workers.values()):\n            # If a worker goes down and we did not ask him, it's not\n            # good: we can think that we have a worker and it's not True\n            # So we del it\n            logger.debug(\"checking if worker %s (pid=%d) is alive\",\n                         worker.get_id(), worker.get_pid())\n            if not self.interrupted and not worker.is_alive():\n                logger.warning(\"The worker %s (pid=%d) went down unexpectedly!\",\n                               worker.get_id(), worker.get_pid())\n                # Terminate immediately\n                worker.terminate()\n                worker.join(timeout=1)\n                w_to_del.append(worker.get_id())\n\n        # OK, now really del workers from queues\n        # And requeue the actions it was managed\n        for worker_id in w_to_del:\n            worker = self.workers[worker_id]\n\n            # Del the queue of the module queue\n            del self.q_by_mod[worker.module_name][worker.get_id()]\n\n            for scheduler_uuid in self.schedulers:\n                sched = self.schedulers[scheduler_uuid]\n                for act in list(sched.actions.values()):\n                    if act.status == ACT_STATUS_QUEUED and act.my_worker == worker_id:\n                        # Got a check that will NEVER return if we do not restart it\n                        self.assign_to_a_queue(act)\n\n            # So now we can really forgot it\n            del self.workers[worker_id]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry to create the minimum workers specified in the configuration.", "response": "def adjust_worker_number_by_load(self):\n        \"\"\"Try to create the minimum workers specified in the configuration\n\n        :return: None\n        \"\"\"\n        if self.interrupted:\n            logger.debug(\"Trying to adjust worker number. Ignoring because we are stopping.\")\n            return\n\n        to_del = []\n        logger.debug(\"checking worker count.\"\n                     \" Currently: %d workers, min per module : %d, max per module : %d\",\n                     len(self.workers), self.min_workers, self.max_workers)\n\n        # I want at least min_workers by module then if I can, I add worker for load balancing\n        for mod in self.q_by_mod:\n            # At least min_workers\n            todo = max(0, self.min_workers - len(self.q_by_mod[mod]))\n            for _ in range(todo):\n                try:\n                    self.create_and_launch_worker(module_name=mod)\n                # Maybe this modules is not a true worker one.\n                # if so, just delete if from q_by_mod\n                except NotWorkerMod:\n                    to_del.append(mod)\n                    break\n\n        for mod in to_del:\n            logger.warning(\"The module %s is not a worker one, I remove it from the worker list.\",\n                           mod)\n            del self.q_by_mod[mod]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_queue_for_the_action(self, action):\n        # get the module name, if not, take fork\n        mod = getattr(action, 'module_type', 'fork')\n        queues = list(self.q_by_mod[mod].items())\n\n        # Maybe there is no more queue, it's very bad!\n        if not queues:\n            return (0, None)\n\n        # if not get action round robin index to get action queue based\n        # on the action id\n        self.rr_qid = (self.rr_qid + 1) % len(queues)\n        (worker_id, queue) = queues[self.rr_qid]\n\n        # return the id of the worker (i), and its queue\n        return (worker_id, queue)", "response": "Find the action queue for the action based on the action id."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a list of actions to the satellite queues.", "response": "def add_actions(self, actions_list, scheduler_instance_id):\n        \"\"\"Add a list of actions to the satellite queues\n\n        :param actions_list: Actions list to add\n        :type actions_list: list\n        :param scheduler_instance_id: sheduler link to assign the actions to\n        :type scheduler_instance_id: SchedulerLink\n        :return: None\n        \"\"\"\n        # We check for new check in each schedulers and put the result in new_checks\n        scheduler_link = None\n        for scheduler_id in self.schedulers:\n            logger.debug(\"Trying to add an action, scheduler: %s\", self.schedulers[scheduler_id])\n            if scheduler_instance_id == self.schedulers[scheduler_id].instance_id:\n                scheduler_link = self.schedulers[scheduler_id]\n                break\n        else:\n            logger.error(\"Trying to add actions from an unknwown scheduler: %s\",\n                         scheduler_instance_id)\n            return\n        if not scheduler_link:\n            logger.error(\"Trying to add actions, but scheduler link is not found for: %s, \"\n                         \"actions: %s\", scheduler_instance_id, actions_list)\n            return\n        logger.debug(\"Found scheduler link: %s\", scheduler_link)\n\n        for action in actions_list:\n            # First we look if the action is identified\n            uuid = getattr(action, 'uuid', None)\n            if uuid is None:\n                try:\n                    action = unserialize(action, no_load=True)\n                    uuid = action.uuid\n                except AlignakClassLookupException:\n                    logger.error('Cannot un-serialize action: %s', action)\n                    continue\n\n            # If we already have this action, we are already working for it!\n            if uuid in scheduler_link.actions:\n                continue\n            # Action is attached to a scheduler\n            action.my_scheduler = scheduler_link.uuid\n            scheduler_link.actions[action.uuid] = action\n            self.assign_to_a_queue(action)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assign_to_a_queue(self, action):\n        (worker_id, queue) = self._get_queue_for_the_action(action)\n        if not worker_id:\n            return\n\n        # Tag the action as \"in the worker i\"\n        action.my_worker = worker_id\n        action.status = ACT_STATUS_QUEUED\n\n        msg = Message(_type='Do', data=action, source=self.name)\n        logger.debug(\"Queuing message: %s\", msg)\n        queue.put_nowait(msg)\n        logger.debug(\"Queued\")", "response": "Take an action and put it to a worker actions queue\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps function for do_get_new_actions For stats purpose is used to get new actions for a specific locale", "response": "def get_new_actions(self):\n        \"\"\" Wrapper function for do_get_new_actions\n        For stats purpose\n\n        :return: None\n        TODO: Use a decorator for timing this function\n        \"\"\"\n        try:\n            _t0 = time.time()\n            self.do_get_new_actions()\n            statsmgr.timer('actions.got.time', time.time() - _t0)\n        except RuntimeError:\n            logger.error(\"Exception like issue #1007\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget new actions from schedulers Create a Message and put into the module queue REF : doc / alignak - action - queues. png ( 1 )", "response": "def do_get_new_actions(self):\n        \"\"\"Get new actions from schedulers\n        Create a Message and put into the module queue\n        REF: doc/alignak-action-queues.png (1)\n\n        :return: None\n        \"\"\"\n        # Here are the differences between a poller and a reactionner:\n        # Poller will only do checks,\n        # Reactionner will do actions (notifications and event handlers)\n        do_checks = self.__class__.do_checks\n        do_actions = self.__class__.do_actions\n\n        # We check and get the new actions to execute in each of our schedulers\n        for scheduler_link_uuid in self.schedulers:\n            scheduler_link = self.schedulers[scheduler_link_uuid]\n\n            if not scheduler_link.active:\n                logger.warning(\"My scheduler '%s' is not active currently\", scheduler_link.name)\n                continue\n\n            logger.debug(\"get new actions, scheduler: %s\", scheduler_link.name)\n\n            # OK, go for it :)\n            _t0 = time.time()\n            actions = scheduler_link.get_actions({'do_checks': do_checks, 'do_actions': do_actions,\n                                                  'poller_tags': self.poller_tags,\n                                                  'reactionner_tags': self.reactionner_tags,\n                                                  'worker_name': self.name,\n                                                  'module_types': list(self.q_by_mod.keys())})\n            if actions:\n                logger.debug(\"Got %d actions from %s\", len(actions), scheduler_link.name)\n                # We 'tag' them with my_scheduler and put into queue for workers\n                self.add_actions(actions, scheduler_link.instance_id)\n                logger.debug(\"Got %d actions from %s in %s\",\n                             len(actions), scheduler_link.name, time.time() - _t0)\n            statsmgr.gauge('actions.added.count.%s' % (scheduler_link.name), len(actions))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncleaning variables from previous configuration broks and external commands.", "response": "def clean_previous_run(self):\n        \"\"\"Clean variables from previous configuration,\n        such as schedulers, broks and external commands\n\n        :return: None\n        \"\"\"\n        # Execute the base class treatment...\n        super(Satellite, self).clean_previous_run()\n\n        # Clean my lists\n        del self.broks[:]\n        del self.events[:]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_loop_turn(self):  # pylint: disable=too-many-branches\n        # Try to see if one of my module is dead, and restart previously dead modules\n        self.check_and_del_zombie_modules()\n\n        # Also if some zombie workers exist...\n        self.check_and_del_zombie_workers()\n\n        # Call modules that manage a starting tick pass\n        self.hook_point('tick')\n\n        # Print stats for debug\n        for _, sched in self.schedulers.items():\n            for mod in self.q_by_mod:\n                # In workers we've got actions sent to queue - queue size\n                for (worker_id, queue) in list(self.q_by_mod[mod].items()):\n                    try:\n                        actions_count = queue.qsize()\n                        results_count = self.returns_queue.qsize()\n                        logger.debug(\"[%s][%s][%s] actions queued: %d, results queued: %d\",\n                                     sched.name, mod, worker_id, actions_count, results_count)\n                        # Update the statistics\n                        statsmgr.gauge('worker.%s.actions-queue-size' % worker_id,\n                                       actions_count)\n                        statsmgr.gauge('worker.%s.results-queue-size' % worker_id,\n                                       results_count)\n                    except (IOError, EOFError):\n                        pass\n\n        # todo temporaray deactivate all this stuff!\n        # Before return or get new actions, see how we managed\n        # the former ones: are they still in queue(s)? If so, we\n        # must wait more or at least have more workers\n        # wait_ratio = self.wait_ratio.get_load()\n        # total_q = 0\n        # try:\n        #     for mod in self.q_by_mod:\n        #         for queue in list(self.q_by_mod[mod].values()):\n        #             total_q += queue.qsize()\n        # except (IOError, EOFError):\n        #     pass\n        # if total_q != 0 and wait_ratio < 2 * self.worker_polling_interval:\n        #     logger.debug(\"I decide to increase the wait ratio\")\n        #     self.wait_ratio.update_load(wait_ratio * 2)\n        #     # self.wait_ratio.update_load(self.worker_polling_interval)\n        # else:\n        #     # Go to self.worker_polling_interval on normal run, if wait_ratio\n        #     # was >2*self.worker_polling_interval,\n        #     # it make it come near 2 because if < 2, go up :)\n        #     self.wait_ratio.update_load(self.worker_polling_interval)\n        # wait_ratio = self.wait_ratio.get_load()\n        # statsmgr.timer('core.wait-ratio', wait_ratio)\n        # if self.log_loop:\n        #     logger.debug(\"[%s] wait ratio: %f\", self.name, wait_ratio)\n\n        # Maybe we do not have enough workers, we check for it\n        # and launch the new ones if needed\n        self.adjust_worker_number_by_load()\n\n        # Manage all messages we've got in the last timeout\n        # for queue in self.return_messages:\n        try:\n            logger.debug(\"[%s] manage action results: %d results\",\n                         self.name, self.returns_queue.qsize())\n            while self.returns_queue.qsize():\n                msg = self.returns_queue.get_nowait()\n                if msg is None:\n                    continue\n                if not isinstance(msg, Message):\n                    logger.warning(\"Should have received a Message, got a %s!\", type(msg))\n                    continue\n                logger.debug(\"Got a message: %s\", msg)\n                if msg.get_type() == 'Done':\n                    logger.debug(\"Got (from %s) an action result: %s\",\n                                 msg.get_source(), msg.get_data())\n                    self.manage_action_return(msg.get_data())\n                elif msg.get_type() == 'Stats':\n                    logger.debug(\"Got (from %s) stats: %s\",\n                                 msg.get_source(), msg.get_data())\n                    if msg.get_source() in self.workers:\n                        self.workers[msg.get_source()].stats = msg.get_data()\n                else:\n                    logger.warning(\"Ignoring message of type: %s\", msg.get_type())\n        except Full:\n            logger.warning(\"Returns queue is full\")\n        except Empty:\n            logger.debug(\"Returns queue is empty\")\n        except (IOError, EOFError) as exp:\n            logger.warning(\"My returns queue is no more available: %s\", str(exp))\n        except Exception as exp:  # pylint: disable=broad-except\n            logger.error(\"Failed getting messages in returns queue: %s\", str(exp))\n            logger.error(traceback.format_exc())\n\n        for _, sched in self.schedulers.items():\n            if sched.wait_homerun:\n                logger.debug(\"scheduler home run: %d results\", len(sched.wait_homerun))\n\n        if not self.passive:\n            # If we are an active satellite, we do not initiate the check getting\n            # and return\n            try:\n                # We send to our schedulers the results of all finished checks\n                logger.debug(\"pushing results...\")\n                self.push_results()\n            except LinkError as exp:\n                logger.warning(\"Scheduler connection failed, I could not send my results!\")\n\n            try:\n                # And we get the new actions from our schedulers\n                logger.debug(\"getting new actions...\")\n                self.get_new_actions()\n            except LinkError as exp:\n                logger.warning(\"Scheduler connection failed, I could not get new actions!\")\n\n        # Get objects from our modules that are not Worker based\n        if self.log_loop:\n            logger.debug(\"[%s] get objects from queues\", self.name)\n        self.get_objects_from_from_queues()\n        statsmgr.gauge('external-commands.count', len(self.external_commands))\n        statsmgr.gauge('broks.count', len(self.broks))\n        statsmgr.gauge('events.count', len(self.events))", "response": "This function is called by the main loop when the main loop is started."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_new_conf(self):\n        # pylint: disable=too-many-branches\n        \"\"\"Setup the new configuration received from Arbiter\n\n        This function calls the base satellite treatment and manages the configuration needed\n        for a simple satellite daemon that executes some actions (eg. poller or reactionner):\n        - configure the passive mode\n        - configure the workers\n        - configure the tags\n        - configure the modules\n\n        :return: None\n        \"\"\"\n        # Execute the base class treatment...\n        super(Satellite, self).setup_new_conf()\n\n        # ...then our own specific treatment!\n        with self.conf_lock:\n            logger.info(\"Received a new configuration\")\n\n            # self_conf is our own configuration from the alignak environment\n            # self_conf = self.cur_conf['self_conf']\n\n            # Now manage modules\n            if not self.have_modules:\n                try:\n                    self.modules = unserialize(self.cur_conf['modules'], no_load=True)\n                except AlignakClassLookupException as exp:  # pragma: no cover, simple protection\n                    logger.error('Cannot un-serialize modules configuration '\n                                 'received from arbiter: %s', exp)\n                if self.modules:\n                    logger.info(\"I received some modules configuration: %s\", self.modules)\n                    self.have_modules = True\n\n                    for module in self.modules:\n                        if module.name not in self.q_by_mod:\n                            self.q_by_mod[module.name] = {}\n\n                    self.do_load_modules(self.modules)\n                    # and start external modules too\n                    self.modules_manager.start_external_instances()\n                else:\n                    logger.info(\"I do not have modules\")\n\n            # Initialize connection with all our satellites\n            logger.info(\"Initializing connection with my satellites:\")\n            my_satellites = self.get_links_of_type(s_type='')\n            for satellite in list(my_satellites.values()):\n                logger.info(\"- : %s/%s\", satellite.type, satellite.name)\n                if not self.daemon_connection_init(satellite):\n                    logger.error(\"Satellite connection failed: %s\", satellite)\n\n        # Now I have a configuration!\n        self.have_conf = True", "response": "Setup the new configuration received from Arbiter and manage the modules and the daemons."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_daemon_stats(self, details=False):\n        # call the daemon one\n        res = super(Satellite, self).get_daemon_stats(details=details)\n\n        counters = res['counters']\n        counters['broks'] = len(self.broks)\n        counters['events'] = len(self.events)\n        counters['workers'] = len(self.workers)\n\n        if self.workers:\n            res['workers'] = {}\n            for worker in list(self.workers.values()):\n                stats = getattr(self.workers[worker.get_id()], 'stats', None)\n                if stats:\n                    res['workers'][worker.get_id()] = stats\n\n        return res", "response": "Increase the stats provided by the Daemon base class\n        and return the stats dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_activation(self, contacts):\n        now = time.time()\n        was_is_in_effect = self.is_in_effect\n        self.is_in_effect = (self.start_time <= now <= self.end_time)\n\n        # Raise a log entry when we get in the downtime\n        if not was_is_in_effect and self.is_in_effect:\n            self.enter(contacts)\n\n        # Same for exit purpose\n        if was_is_in_effect and not self.is_in_effect:\n            self.exit(contacts)", "response": "Enter or exit downtime if necessary\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping to call raise_exit_downtime_log_entry for ref ( host or service", "response": "def exit(self, contacts):\n        \"\"\"Wrapper to call raise_exit_downtime_log_entry for ref (host/service)\n        set can_be_deleted to True\n\n        :return: None\n        \"\"\"\n        contact = contacts[self.ref]\n        contact.raise_exit_downtime_log_entry()\n        self.can_be_deleted = True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cancel(self, contacts):\n        self.is_in_effect = False\n        contact = contacts[self.ref]\n        contact.raise_cancel_downtime_log_entry()\n        self.can_be_deleted = True", "response": "Wrapper to call raise_cancel_downtime_log_entry for ref ( host or service"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_semicolon(line, maxsplit=None):\n    # Split on ';' character\n    split_line = line.split(';')\n\n    split_line_size = len(split_line)\n\n    # if maxsplit is not specified, we set it to the number of part\n    if maxsplit is None or maxsplit < 0:\n        maxsplit = split_line_size\n\n    # Join parts  to the next one, if ends with a '\\'\n    # because we mustn't split if the semicolon is escaped\n    i = 0\n    while i < split_line_size - 1:\n\n        # for each part, check if its ends with a '\\'\n        ends = split_line[i].endswith('\\\\')\n\n        if ends:\n            # remove the last character '\\'\n            split_line[i] = split_line[i][:-1]\n\n        # append the next part to the current if it is not the last and the current\n        # ends with '\\' or if there is more than maxsplit parts\n        if (ends or i >= maxsplit) and i < split_line_size - 1:\n\n            split_line[i] = \";\".join([split_line[i], split_line[i + 1]])\n\n            # delete the next part\n            del split_line[i + 1]\n            split_line_size -= 1\n\n        # increase i only if we don't have append because after append the new\n        # string can end with '\\'\n        else:\n            i += 1\n\n    return split_line", "response": "r Split a line on semicolons characters but not on the escaped semicolons characters and not on the escaped semicolons characters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef jsonify_r(obj):  # pragma: no cover, not for unit tests...\n    # pylint: disable=too-many-branches\n    \"\"\"Convert an object into json (recursively on attribute)\n\n    :param obj: obj to jsonify\n    :type obj: object\n    :return: json representation of obj\n    :rtype: dict\n    \"\"\"\n    res = {}\n    cls = obj.__class__\n    if not hasattr(cls, 'properties'):\n        try:\n            json.dumps(obj)\n            return obj\n        except TypeError:\n            return None\n    properties = list(cls.properties.keys())\n    if hasattr(cls, 'running_properties'):\n        properties += list(cls.running_properties.keys())\n    for prop in properties:\n        if not hasattr(obj, prop):\n            continue\n        val = getattr(obj, prop)\n        # Maybe the property is not jsonable\n        try:\n            if isinstance(val, set):\n                val = list(val)\n            if isinstance(val, list):\n                val = sorted(val)\n            json.dumps(val)\n            res[prop] = val\n        except TypeError:\n            if isinstance(val, list):\n                lst = []\n                for subval in val:\n                    o_type = getattr(subval.__class__, 'my_type', '')\n                    if o_type == 'CommandCall':\n                        try:\n                            lst.append(subval.call)\n                        except AttributeError:  # pragma: no cover, should not happen...\n                            pass\n                        continue\n                    if o_type and hasattr(subval, o_type + '_name'):\n                        lst.append(getattr(subval, o_type + '_name'))\n                    else:\n                        pass\n                res[prop] = lst\n            else:\n                o_type = getattr(val.__class__, 'my_type', '')\n                if o_type == 'CommandCall':\n                    try:\n                        res[prop] = val.call\n                    except AttributeError:  # pragma: no cover, should not happen...\n                        pass\n                    continue\n                if o_type and hasattr(val, o_type + '_name'):\n                    res[prop] = getattr(val, o_type + '_name')\n    return res", "response": "Convert an object into json representation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert an amount of second into day hour min and sec", "response": "def format_t_into_dhms_format(timestamp):\n    \"\"\" Convert an amount of second into day, hour, min and sec\n\n    :param timestamp: seconds\n    :type timestamp: int\n    :return: 'Ad Bh Cm Ds'\n    :rtype: str\n\n    >>> format_t_into_dhms_format(456189)\n    '5d 6h 43m 9s'\n\n    >>> format_t_into_dhms_format(3600)\n    '0d 1h 0m 0s'\n\n    \"\"\"\n    mins, timestamp = divmod(timestamp, 60)\n    hour, mins = divmod(mins, 60)\n    day, hour = divmod(hour, 24)\n    return '%sd %sh %sm %ss' % (day, hour, mins, timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_periods(data):\n    # sort by start date\n    newdata = sorted(data, key=lambda drange: drange[0])\n    end = 0\n    for period in newdata:\n        if period[0] != end and period[0] != (end - 1):\n            end = period[1]\n\n    # dat = np.array(newdata)\n    dat = newdata\n    new_intervals = []\n    cur_start = None\n    cur_end = None\n    for (dt_start, dt_end) in dat:\n        if cur_end is None:\n            cur_start = dt_start\n            cur_end = dt_end\n            continue\n        else:\n            if cur_end >= dt_start:\n                # merge, keep existing cur_start, extend cur_end\n                cur_end = dt_end\n            else:\n                # new interval, save previous and reset current to this\n                new_intervals.append((cur_start, cur_end))\n                cur_start = dt_start\n                cur_end = dt_end\n    # make sure final interval is saved\n    new_intervals.append((cur_start, cur_end))\n    return new_intervals", "response": "Merge periods to have better continous periods."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_split(val, split_on_comma=True):\n    if isinstance(val, list):\n        return val\n    if not split_on_comma:\n        return [val]\n    val = val.split(',')\n    if val == ['']:\n        val = []\n    return val", "response": "Try to split a string with comma separator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry to split each member of a list with comma separator.", "response": "def list_split(val, split_on_comma=True):\n    \"\"\"Try to split each member of a list with comma separator.\n    If we don't have to split just return val\n\n    :param val: value to split\n    :type val:\n    :param split_on_comma:\n    :type split_on_comma: bool\n    :return: list with members split on comma\n    :rtype: list\n\n    >>> list_split(['a,b,c'], False)\n    ['a,b,c']\n\n    >>> list_split(['a,b,c'])\n    ['a', 'b', 'c']\n\n    >>> list_split('')\n    []\n\n    \"\"\"\n    if not split_on_comma:\n        return val\n    new_val = []\n    for subval in val:\n        # This may happen when re-serializing\n        if isinstance(subval, list):\n            continue\n        new_val.extend(subval.split(','))\n    return new_val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting best type for value between int and float", "response": "def to_best_int_float(val):\n    \"\"\"Get best type for value between int and float\n\n    :param val: value\n    :type val:\n    :return: int(float(val)) if int(float(val)) == float(val), else float(val)\n    :rtype: int | float\n\n    >>> to_best_int_float(\"20.1\")\n    20.1\n\n    >>> to_best_int_float(\"20.0\")\n    20\n\n    >>> to_best_int_float(\"20\")\n    20\n    \"\"\"\n    integer = int(float(val))\n    flt = float(val)\n    # If the f is a .0 value,\n    # best match is int\n    if integer == flt:\n        return integer\n    return flt"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nserializes the list of elements to a dictionary", "response": "def dict_to_serialized_dict(ref, the_dict):\n    \"\"\"Serialize the list of elements to a dictionary\n\n    Used for the retention store\n\n    :param ref: Not used\n    :type ref:\n    :param the_dict: dictionary to convert\n    :type the_dict: dict\n    :return: dict of serialized\n    :rtype: dict\n    \"\"\"\n    result = {}\n    for elt in list(the_dict.values()):\n        if not getattr(elt, 'serialize', None):\n            continue\n        result[elt.uuid] = elt.serialize()\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_to_serialized(ref, the_list):\n    result = []\n    for elt in the_list:\n        if not getattr(elt, 'serialize', None):\n            continue\n        result.append(elt.serialize())\n    return result", "response": "Serialize the list of elements\nTaxonomy Used for the retention store\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_hostnames_list(ref, tab):  # pragma: no cover, to be deprecated?\n    res = []\n    for host in tab:\n        if hasattr(host, 'host_name'):\n            res.append(host.host_name)\n    return res", "response": "Convert a list of host_name objects into a list of host_name objects"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_svc_hst_distinct_lists(ref, tab):  # pragma: no cover, to be deprecated?\n    res = {'hosts': [], 'services': []}\n    for elem in tab:\n        cls = elem.__class__\n        name = elem.get_full_name()\n        if cls.my_type == 'service':\n            res['services'].append(name)\n        else:\n            res['hosts'].append(name)\n    return res", "response": "create a dict with 2 lists ::\n    services and hosts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the provided satellites list sorted as master then spare.", "response": "def master_then_spare(data):\n    \"\"\"Return the provided satellites list sorted as:\n        - alive first,\n        - then spare\n        - then dead\n        satellites.\n\n    :param data: the SatelliteLink list\n    :type data: list\n    :return: sorted list\n    :rtype: list\n    \"\"\"\n    master = []\n    spare = []\n    for sdata in data:\n        if sdata.spare:\n            spare.append(sdata)\n        else:\n            master.append(sdata)\n    rdata = []\n    rdata.extend(master)\n    rdata.extend(spare)\n    return rdata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sort_by_number_values(x00, y00):  # pragma: no cover, looks like not used!\n    if len(x00) < len(y00):\n        return 1\n    if len(x00) > len(y00):\n        return -1\n    # So is equal\n    return 0", "response": "Compare x00 y00 base on number of values in\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef average_percentile(values):\n    if not values:\n        return None, None, None\n\n    value_avg = round(float(sum(values)) / len(values), 2)\n    value_max = round(percentile(values, 95), 2)\n    value_min = round(percentile(values, 5), 2)\n    return value_avg, value_min, value_max", "response": "Get the average min and max percentile of a list of values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef strip_and_uniq(tab):\n    _list = []\n    for elt in tab:\n        val = elt.strip()\n        if val and val not in _list:\n            _list.append(val)\n    return _list", "response": "Strip every element of a list and keep a list of ordered unique values"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef expand_ranges(value):\n    match_dict = RANGE_REGEX.match(value).groupdict()  # the regex is supposed to always match..\n    before = match_dict['before']\n    after = match_dict['after']\n    from_value = match_dict['from']\n    if from_value is None:\n        yield value\n    else:\n        # we have a [x-y] range\n        from_value = int(from_value)\n        to_value = int(match_dict['to']) + 1  # y is inclusive\n        step = int(match_dict['step'] or 1)\n        for idx in range(from_value, to_value, step):\n            # yield \"%s%s%s\" % (before, idx, after)\n            for sub_val in expand_ranges(\"%s%s%s\" % (before, idx, after)):\n                yield sub_val", "response": "A generator that yields the values from expanding\n             the eventual ranges present in the input value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_key_value_sequences(entry, default_value):\n    no_one_yielded = True\n    for value in entry.split(','):\n        value = value.strip()\n        if not value:\n            continue\n        full_match = KEY_VALUES_REGEX.match(value)\n        if full_match is None:\n            raise KeyValueSyntaxError(\"%r is an invalid key(-values) pattern\" % value)\n        key = full_match.group(1)\n        tmp = {'KEY': key}\n        values = full_match.group(2)\n        if values:  # there is, at least, one value provided\n            for idx, value_match in enumerate(VALUE_REGEX.finditer(values), 1):\n                tmp['VALUE%s' % idx] = value_match.group(1)\n        else:  # no value provided for this key, use the default provided:\n            tmp['VALUE1'] = default_value\n        tmp['VALUE'] = tmp['VALUE1']  # alias from VALUE -> VALUE1\n        for subkey in expand_ranges(key):\n            current = tmp.copy()\n            current['KEY'] = subkey\n            yield current\n            no_one_yielded = False\n    if no_one_yielded:\n        raise KeyValueSyntaxError('At least one key must be present')", "response": "Parse a key value config line and yield a list of dicts with KEY & VALUE & VALUE1 keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_host_by_name(name):\n\n    def inner_filter(items):\n        \"\"\"Inner filter for host. Accept if host_name == name\"\"\"\n        host = items[\"host\"]\n        if host is None:\n            return False\n        return host.host_name == name\n\n    return inner_filter", "response": "Filter for host by name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering for host by regex", "response": "def filter_host_by_regex(regex):\n    \"\"\"Filter for host\n    Filter on regex\n\n    :param regex: regex to filter\n    :type regex: str\n    :return: Filter\n    :rtype: bool\n    \"\"\"\n    host_re = re.compile(regex)\n\n    def inner_filter(items):\n        \"\"\"Inner filter for host. Accept if regex match host_name\"\"\"\n        host = items[\"host\"]\n        if host is None:\n            return False\n        return host_re.match(host.host_name) is not None\n\n    return inner_filter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_host_by_group(group):\n\n    def inner_filter(items):\n        \"\"\"Inner filter for host. Accept if group in host.hostgroups\"\"\"\n        host = items[\"host\"]\n        if host is None:\n            return False\n        return group in [items[\"hostgroups\"][g].hostgroup_name for g in host.hostgroups]\n\n    return inner_filter", "response": "Filter for host by group"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfilter for host by tag", "response": "def filter_host_by_tag(tpl):\n    \"\"\"Filter for host\n    Filter on tag\n\n    :param tpl: tag to filter\n    :type tpl: str\n    :return: Filter\n    :rtype: bool\n    \"\"\"\n\n    def inner_filter(items):\n        \"\"\"Inner filter for host. Accept if tag in host.tags\"\"\"\n        host = items[\"host\"]\n        if host is None:\n            return False\n        return tpl in [t.strip() for t in host.tags]\n\n    return inner_filter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_service_by_name(name):\n\n    def inner_filter(items):\n        \"\"\"Inner filter for service. Accept if service_description == name\"\"\"\n        service = items[\"service\"]\n        if service is None:\n            return False\n        return service.service_description == name\n\n    return inner_filter", "response": "Filter for service by name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter for service by regex name", "response": "def filter_service_by_regex_name(regex):\n    \"\"\"Filter for service\n    Filter on regex\n\n    :param regex: regex to filter\n    :type regex: str\n    :return: Filter\n    :rtype: bool\n    \"\"\"\n    host_re = re.compile(regex)\n\n    def inner_filter(items):\n        \"\"\"Inner filter for service. Accept if regex match service_description\"\"\"\n        service = items[\"service\"]\n        if service is None:\n            return False\n        return host_re.match(service.service_description) is not None\n\n    return inner_filter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering for service by host_name", "response": "def filter_service_by_host_name(host_name):\n    \"\"\"Filter for service\n    Filter on host_name\n\n    :param host_name: host_name to filter\n    :type host_name: str\n    :return: Filter\n    :rtype: bool\n    \"\"\"\n\n    def inner_filter(items):\n        \"\"\"Inner filter for service. Accept if service.host.host_name == host_name\"\"\"\n        service = items[\"service\"]\n        host = items[\"hosts\"][service.host]\n        if service is None or host is None:\n            return False\n        return host.host_name == host_name\n\n    return inner_filter"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_service_by_regex_host_name(regex):\n    host_re = re.compile(regex)\n\n    def inner_filter(items):\n        \"\"\"Inner filter for service. Accept if regex match service.host.host_name\"\"\"\n        service = items[\"service\"]\n        host = items[\"hosts\"][service.host]\n        if service is None or host is None:\n            return False\n        return host_re.match(host.host_name) is not None\n\n    return inner_filter", "response": "Filter for service by regex host_name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_service_by_hostgroup_name(group):\n\n    def inner_filter(items):\n        \"\"\"Inner filter for service. Accept if hostgroup in service.host.hostgroups\"\"\"\n        service = items[\"service\"]\n        host = items[\"hosts\"][service.host]\n        if service is None or host is None:\n            return False\n        return group in [items[\"hostgroups\"][g].hostgroup_name for g in host.hostgroups]\n\n    return inner_filter", "response": "Filter for service by hostgroup name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter_service_by_host_tag_name(tpl):\n\n    def inner_filter(items):\n        \"\"\"Inner filter for service. Accept if tpl in service.host.tags\"\"\"\n        service = items[\"service\"]\n        host = items[\"hosts\"][service.host]\n        if service is None or host is None:\n            return False\n        return tpl in [t.strip() for t in host.tags]\n\n    return inner_filter", "response": "Filter for service by tag name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter for service by servicegroup name", "response": "def filter_service_by_servicegroup_name(group):\n    \"\"\"Filter for service\n    Filter on group\n\n    :param group: group to filter\n    :type group: str\n    :return: Filter\n    :rtype: bool\n    \"\"\"\n\n    def inner_filter(items):\n        \"\"\"Inner filter for service. Accept if group in service.servicegroups\"\"\"\n        service = items[\"service\"]\n        if service is None:\n            return False\n        return group in [items[\"servicegroups\"][g].servicegroup_name for g in service.servicegroups]\n\n    return inner_filter"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter_host_by_bp_rule_label(label):\n\n    def inner_filter(items):\n        \"\"\"Inner filter for host. Accept if label in host.labels\"\"\"\n        host = items[\"host\"]\n        if host is None:\n            return False\n        return label in host.labels\n\n    return inner_filter", "response": "Filter for host by bp rule label"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter_service_by_host_bp_rule_label(label):\n\n    def inner_filter(items):\n        \"\"\"Inner filter for service. Accept if label in service.host.labels\"\"\"\n        service = items[\"service\"]\n        host = items[\"hosts\"][service.host]\n        if service is None or host is None:\n            return False\n        return label in host.labels\n\n    return inner_filter", "response": "Filter for service by label"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_service_by_bp_rule_label(label):\n    def inner_filter(items):\n        \"\"\"Inner filter for service. Accept if label in service.labels\"\"\"\n        service = items[\"service\"]\n        if service is None:\n            return False\n        return label in service.labels\n\n    return inner_filter", "response": "Filter for service by bp rule label"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef manage_signal(self, sig, frame):  # pylint: disable=unused-argument\n        logger.info(\"worker '%s' (pid=%d) received a signal: %s\",\n                    self._id, os.getpid(), SIGNALS_TO_NAMES_DICT[sig])\n        # Do not do anything... our master daemon is managing our termination.\n        self.interrupted = True", "response": "Manage signals caught by the process but I do not do anything..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the signal handler to manage_signal", "response": "def set_exit_handler(self):\n        \"\"\"Set the signal handler to manage_signal (defined in this class)\n        Only set handlers for signal.SIGTERM, signal.SIGINT, signal.SIGUSR1, signal.SIGUSR2\n\n        :return: None\n        \"\"\"\n        signal.signal(signal.SIGINT, self.manage_signal)\n        signal.signal(signal.SIGTERM, self.manage_signal)\n        signal.signal(signal.SIGHUP, self.manage_signal)\n        signal.signal(signal.SIGQUIT, self.manage_signal)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_new_checks(self, queue, return_queue):\n        try:\n            logger.debug(\"get_new_checks: %s / %s\", len(self.checks), self.processes_by_worker)\n            while len(self.checks) < self.processes_by_worker:\n                msg = queue.get_nowait()\n                if msg is None:\n                    time.sleep(0.01)\n                    continue\n                logger.debug(\"Got a message: %s\", msg)\n                if msg.get_type() == 'Do':\n                    logger.debug(\"Got an action: %s\", msg.get_data())\n                    self.checks.append(msg.get_data())\n                    self.actions_got += 1\n                elif msg.get_type() == 'ping':\n                    msg = Message(_type='pong', data='pong!', source=self._id)\n                    logger.debug(\"Queuing message: %s\", msg)\n                    return_queue.put_nowait(msg)\n                    logger.debug(\"Queued\")\n                else:\n                    logger.warning(\"Ignoring message of type: %s\", msg.get_type())\n        except Full:\n            logger.warning(\"Actions queue is full\")\n        except Empty:\n            logger.debug(\"Actions queue is empty\")\n            if not self.checks:\n                self._idletime += 1\n        # Maybe the Queue() has been deleted by our master ?\n        except (IOError, EOFError) as exp:\n            logger.warning(\"My actions queue is no more available: %s\", str(exp))\n            self.interrupted = True\n        except Exception as exp:  # pylint: disable=broad-except\n            logger.error(\"Failed getting messages in actions queue: %s\", str(exp))\n\n        logger.debug(\"get_new_checks exit\")", "response": "Get new checks from the queue and return them if they are not available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef launch_new_checks(self):\n        # queue\n        for chk in self.checks:\n            if chk.status not in [ACT_STATUS_QUEUED]:\n                continue\n            logger.debug(\"Launch check: %s\", chk.uuid)\n            self._idletime = 0\n            self.actions_launched += 1\n            process = chk.execute()\n            # Maybe we got a true big problem in the action launching\n            if process == 'toomanyopenfiles':\n                # We should die as soon as we return all checks\n                logger.error(\"I am dying because of too many open files: %s\", chk)\n                self.i_am_dying = True\n            else:\n                if not isinstance(process, string_types):\n                    logger.debug(\"Launched check: %s, pid=%d\", chk.uuid, process.pid)", "response": "Launch new checks that are in status\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef manage_finished_checks(self, queue):\n        to_del = []\n        wait_time = 1.0\n        now = time.time()\n        logger.debug(\"--- manage finished checks\")\n        for action in self.checks:\n            logger.debug(\"--- checking: last poll: %s, now: %s, wait_time: %s, action: %s\",\n                         action.last_poll, now, action.wait_time, action)\n            if action.status == ACT_STATUS_LAUNCHED and action.last_poll < now - action.wait_time:\n                action.check_finished(self.max_plugins_output_length)\n                wait_time = min(wait_time, action.wait_time)\n            # If action done, we can launch a new one\n            if action.status in [ACT_STATUS_DONE, ACT_STATUS_TIMEOUT]:\n                logger.debug(\"--- check done/timeout: %s\", action.uuid)\n                self.actions_finished += 1\n                to_del.append(action)\n                # We answer to our master\n                try:\n                    msg = Message(_type='Done', data=action, source=self._id)\n                    logger.debug(\"Queuing message: %s\", msg)\n                    queue.put_nowait(msg)\n                except Exception as exp:  # pylint: disable=broad-except\n                    logger.error(\"Failed putting messages in returns queue: %s\", str(exp))\n\n        for chk in to_del:\n            logger.debug(\"--- delete check: %s\", chk.uuid)\n            self.checks.remove(chk)\n\n        # Little sleep\n        logger.debug(\"--- manage finished checks terminated, I will wait: %s\", wait_time)\n        time.sleep(wait_time)", "response": "Manage the status of checks and return messages done."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if our system time change. If so change ourCOOKIE.", "response": "def check_for_system_time_change(self):  # pragma: no cover, hardly testable with unit tests...\n        \"\"\"Check if our system time change. If so, change our\n\n        :return: 0 if the difference < 900, difference else\n        :rtype: int\n        \"\"\"\n        now = time.time()\n        difference = now - self.t_each_loop\n\n        # Now set the new value for the tick loop\n        self.t_each_loop = now\n\n        # If we have more than 15 min time change, we need to compensate it\n        # todo: confirm that 15 minutes is a good choice...\n        if abs(difference) > 900:  # pragma: no cover, not with unit tests...\n            return difference\n\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps function for do_work in order to catch the exception to see the real work and exit anyway", "response": "def work(self, actions_queue, returns_queue, control_queue=None):  # pragma: no cover\n        \"\"\"Wrapper function for do_work in order to catch the exception\n        to see the real work, look at do_work\n\n        :param actions_queue: Global Queue Master->Slave\n        :type actions_queue: Queue.Queue\n        :param returns_queue: queue managed by manager\n        :type returns_queue: Queue.Queue\n        :return: None\n        \"\"\"\n        try:\n            logger.info(\"[%s] (pid=%d) starting my job...\", self._id, os.getpid())\n            self.do_work(actions_queue, returns_queue, control_queue)\n            logger.info(\"[%s] (pid=%d) stopped\", self._id, os.getpid())\n        except ActionError as exp:\n            logger.error(\"[%s] exited with an ActionError exception : %s\", self._id, str(exp))\n            logger.exception(exp)\n            raise\n        # Catch any exception, log the exception and exit anyway\n        except Exception as exp:  # pragma: no cover, this should never happen indeed ;)\n            logger.error(\"[%s] exited with an unmanaged exception : %s\", self._id, str(exp))\n            logger.exception(exp)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the list of strings from a requirements. txt file. Uses the requirements. txt by default.", "response": "def read_requirements(filename='requirements.txt'):\n    \"\"\"Reads the list of requirements from given file.\n\n    :param filename: Filename to read the requirements from.\n                     Uses ``'requirements.txt'`` by default.\n\n    :return: Requirments as list of strings.\n    \"\"\"\n    # allow for some leeway with the argument\n    if not filename.startswith('requirements'):\n        filename = 'requirements-' + filename\n    if not os.path.splitext(filename)[1]:\n        filename += '.txt'  # no extension, add default\n\n    def valid_line(line):\n        line = line.strip()\n        return line and not any(line.startswith(p) for p in ('#', '-'))\n\n    def extract_requirement(line):\n        egg_eq = '#egg='\n        if egg_eq in line:\n            _, requirement = line.split(egg_eq, 1)\n            return requirement\n        return line\n\n    with open(filename) as f:\n        lines = f.readlines()\n        return list(map(extract_requirement, filter(valid_line, lines)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the running_properties. Each instance has own property.", "response": "def init_running_properties(self):\n        \"\"\"\n        Initialize the running_properties.\n        Each instance have own property.\n\n        :return: None\n        \"\"\"\n        for prop, entry in list(self.__class__.running_properties.items()):\n            val = entry.default\n            # Make a copy of the value for complex iterable types\n            # As such, each instance has its own copy and not a simple reference\n            setattr(self, prop, copy(val) if isinstance(val, (set, list, dict)) else val)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a copy of this object but with a new id", "response": "def copy(self):\n        \"\"\"\n        Get a copy of this item but with a new id\n\n        :return: copy of this object with a new id\n        :rtype: object\n        \"\"\"\n        # New dummy item with it's own running properties\n        copied_item = self.__class__({})\n        # Now, copy the properties\n        for prop in self.__class__.properties:\n            if prop in ['uuid']:\n                continue\n            val = getattr(self, prop, None)\n            if val is not None:\n                setattr(copied_item, prop, val)\n\n        # Also copy some running properties\n        # The custom variables\n        if hasattr(self, \"customs\"):\n            copied_item.customs = copy(self.customs)\n        # And tags/templates\n        if hasattr(self, \"tags\"):\n            copied_item.tags = copy(self.tags)\n        if hasattr(self, \"templates\"):\n            copied_item.templates = copy(self.templates)\n\n        return copied_item"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans properties only needed for initialization and configuration", "response": "def clean(self):\n        \"\"\"\n        Clean properties only needed for initialization and configuration\n\n        :return: None\n        \"\"\"\n        for prop in ('imported_from', 'use', 'plus', 'templates', 'register'):\n            try:\n                delattr(self, prop)\n            except AttributeError:\n                pass\n        for prop in ('configuration_warnings', 'configuration_errors'):\n            try:\n                if getattr(self, prop, None) is not None and not getattr(self, prop):\n                    delattr(self, prop)\n            except AttributeError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_global_conf(cls, global_configuration):\n        logger.debug(\"Propagate global parameter for %s:\", cls)\n        for prop, entry in global_configuration.properties.items():\n            # If some global managed configuration properties have a class_inherit clause,\n            if not entry.managed or not getattr(entry, 'class_inherit'):\n                continue\n            for (cls_dest, change_name) in entry.class_inherit:\n                if cls_dest == cls:  # ok, we've got something to get\n                    value = getattr(global_configuration, prop)\n                    logger.debug(\"- global parameter %s=%s -> %s=%s\",\n                                 prop, getattr(global_configuration, prop),\n                                 change_name, value)\n                    if change_name is None:\n                        setattr(cls, prop, value)\n                    else:\n                        setattr(cls, change_name, value)", "response": "Apply global Alignak configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_templates(self):\n        use = getattr(self, 'use', '')\n        if isinstance(use, list):\n            return [n.strip() for n in use if n.strip()]\n\n        return [n.strip() for n in use.split(',') if n.strip()]", "response": "Get list of templates this object use\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all self. plus items of list. We copy it delete the original and return the copy", "response": "def get_all_plus_and_delete(self):\n        \"\"\"\n        Get all self.plus items of list. We copy it, delete the original and return the copy list\n\n        :return: list of self.plus\n        :rtype: list\n        \"\"\"\n        res = {}\n        props = list(self.plus.keys())  # we delete entries, so no for ... in ...\n        for prop in props:\n            res[prop] = self.get_plus_and_delete(prop)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a copy of the property in self. plus and delete the original and return the value of the property in self. plus", "response": "def get_plus_and_delete(self, prop):\n        \"\"\"\n        get a copy of the property (parameter) in self.plus, delete the original and return the\n        value of copy\n\n        :param prop: a property\n        :type prop: str\n        :return: return the value of the property\n        :rtype: str\n        \"\"\"\n        val = self.plus[prop]\n        del self.plus[prop]\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a message to the list of errors that can be printed to the console", "response": "def add_error(self, txt):\n        \"\"\"Add a message in the configuration errors list so we can print them\n         all in one place\n\n         Set the object configuration as not correct\n\n        :param txt: error message\n        :type txt: str\n        :return: None\n        \"\"\"\n        self.configuration_errors.append(txt)\n        self.conf_is_correct = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_correct(self):\n        state = self.conf_is_correct\n        properties = self.__class__.properties\n\n        for prop, entry in list(properties.items()):\n            if hasattr(self, 'special_properties') and prop in getattr(self, 'special_properties'):\n                continue\n            if not hasattr(self, prop) and entry.required:\n                msg = \"[%s::%s] %s property is missing\" % (self.my_type, self.get_name(), prop)\n                self.add_error(msg)\n\n        state = state & self.conf_is_correct\n        return state", "response": "Check if this object is correct."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_raw_import_values(self):  # pragma: no cover, never used\n        res = {}\n        properties = list(self.__class__.properties.keys())\n        # Register is not by default in the properties\n        if 'register' not in properties:\n            properties.append('register')\n\n        for prop in properties:\n            if hasattr(self, prop):\n                val = getattr(self, prop)\n                res[prop] = val\n        return res", "response": "Get properties => values of this object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef del_downtime(self, downtime_id):\n        if downtime_id in self.downtimes:\n            self.downtimes[downtime_id].can_be_deleted = True\n            del self.downtimes[downtime_id]", "response": "Delete a downtime in this object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the value of a property of an object and brok_transformation if needed and return the value", "response": "def get_property_value_for_brok(self, prop, tab):\n        \"\"\"\n        Get the property of an object and brok_transformation if needed and return the value\n\n        :param prop: property name\n        :type prop: str\n        :param tab: object with all properties of an object\n        :type tab: object\n        :return: value of the property original or brok converted\n        :rtype: str\n        \"\"\"\n        entry = tab[prop]\n        # Get the current value, or the default if need\n        value = getattr(self, prop, entry.default)\n\n        # Apply brok_transformation if need\n        # Look if we must preprocess the value first\n        pre_op = entry.brok_transformation\n        if pre_op is not None:\n            value = pre_op(self, value)\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding properties to data parameter with properties of this object when brok_type is defined in fill_brok of these properties", "response": "def fill_data_brok_from(self, data, brok_type):\n        \"\"\"\n        Add properties to 'data' parameter with properties of this object when 'brok_type'\n        parameter is defined in fill_brok of these properties\n\n        :param data: object to fill\n        :type data: object\n        :param brok_type: name of brok_type\n        :type brok_type: var\n        :return: None\n        \"\"\"\n        cls = self.__class__\n        # Configuration properties\n        for prop, entry in list(cls.properties.items()):\n            # Is this property intended for broking?\n            if brok_type in entry.fill_brok:\n                data[prop] = self.get_property_value_for_brok(prop, cls.properties)\n\n        # And the running properties\n        if hasattr(cls, 'running_properties'):\n            # We've got prop in running_properties too\n            for prop, entry in list(cls.running_properties.items()):\n                # if 'fill_brok' in cls.running_properties[prop]:\n                if brok_type in entry.fill_brok:\n                    data[prop] = self.get_property_value_for_brok(prop, cls.running_properties)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an initial status brok", "response": "def get_initial_status_brok(self, extra=None):\n        \"\"\"\n        Create an initial status brok\n\n        :param extra: some extra information to be added in the brok data\n        :type extra: dict\n        :return: Brok object\n        :rtype: alignak.Brok\n        \"\"\"\n        data = {'uuid': self.uuid}\n        self.fill_data_brok_from(data, 'full_status')\n        if extra:\n            data.update(extra)\n        return Brok({'type': 'initial_' + self.my_type + '_status', 'data': data})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an update item brok", "response": "def get_update_status_brok(self):\n        \"\"\"\n        Create an update item brok\n\n        :return: Brok object\n        :rtype: alignak.Brok\n        \"\"\"\n        data = {'uuid': self.uuid}\n        self.fill_data_brok_from(data, 'full_status')\n        return Brok({'type': 'update_' + self.my_type + '_status', 'data': data})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_check_result_brok(self):\n        data = {'uuid': self.uuid}\n        self.fill_data_brok_from(data, 'check_result')\n        return Brok({'type': self.my_type + '_check_result', 'data': data})", "response": "Create check_result brok\n\n        :return: Brok object\n        :rtype: alignak.Brok"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_next_schedule_brok(self):\n        data = {'uuid': self.uuid}\n        self.fill_data_brok_from(data, 'next_schedule')\n        return Brok({'type': self.my_type + '_next_schedule', 'data': data})", "response": "Create next_schedule (next check) brok\n\n        :return: Brok object\n        :rtype: alignak.Brok"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_snapshot_brok(self, snap_output, exit_status):\n        data = {\n            'uuid': self.uuid,\n            'snapshot_output': snap_output,\n            'snapshot_time': int(time.time()),\n            'snapshot_exit_status': exit_status,\n        }\n        self.fill_data_brok_from(data, 'check_result')\n        return Brok({'type': self.my_type + '_snapshot', 'data': data})", "response": "Create snapshot (check_result type) brok\n\n        :param snap_output: value of output\n        :type snap_output: str\n        :param exit_status: status of exit\n        :type exit_status: integer\n        :return: Brok object\n        :rtype: alignak.Brok"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndump the object properties of the current object to a dictionary.", "response": "def dump(self, dump_file_name=None):  # pragma: no cover, never called\n        # pylint: disable=unused-argument\n        \"\"\"\n        Dump Item object properties\n\n        :return: dictionary with properties\n        :rtype: dict\n        \"\"\"\n        dump = {}\n        for prop in self.properties:\n            if not hasattr(self, prop):\n                continue\n            attr = getattr(self, prop)\n            if isinstance(attr, list) and attr and isinstance(attr[0], Item):\n                dump[prop] = [i.dump() for i in attr]\n            elif isinstance(attr, Item):\n                dump[prop] = attr.dump()\n            elif attr:\n                dump[prop] = getattr(self, prop)\n        return dump"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding items to template if is template else add in item list", "response": "def add_items(self, items, index_items):\n        \"\"\"\n        Add items to template if is template, else add in item list\n\n        :param items: items list to add\n        :type items: alignak.objects.item.Items\n        :param index_items: Flag indicating if the items should be indexed on the fly.\n        :type index_items: bool\n        :return: None\n        \"\"\"\n        count_templates = 0\n        count_items = 0\n        generated_items = []\n        for item in items:\n            if item.is_tpl():\n                self.add_template(item)\n                count_templates = count_templates + 1\n            else:\n                new_items = self.add_item(item, index_items)\n                count_items = count_items + max(1, len(new_items))\n                if new_items:\n                    generated_items.extend(new_items)\n        if count_templates:\n            logger.info('    indexed %d template(s)', count_templates)\n        if count_items:\n            logger.info('    created %d %s(s).', count_items, self.inner_class.my_type)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if an object holding the same name already exists in the index and if so updates the item s definition_order attribute with the new item s definition_order. If so returns the object that should be replaced after the existing item. If not returns the item that should be replaced.", "response": "def manage_conflict(self, item, name):\n        \"\"\"\n        Checks if an object holding the same name already exists in the index.\n\n        If so, it compares their definition order: the lowest definition order\n        is kept. If definition order equal, an error is risen.Item\n\n        The method returns the item that should be added after it has decided\n        which one should be kept.\n\n        If the new item has precedence over the New existing one, the\n        existing is removed for the new to replace it.\n\n        :param item: object to check for conflict\n        :type item: alignak.objects.item.Item\n        :param name: name of the object\n        :type name: str\n        :return: 'item' parameter modified\n        :rtype: object\n        \"\"\"\n        if item.is_tpl():\n            existing = self.name_to_template[name]\n        else:\n            existing = self.name_to_item[name]\n        if existing == item:\n            return item\n\n        existing_prio = getattr(\n            existing,\n            \"definition_order\",\n            existing.properties[\"definition_order\"].default)\n        item_prio = getattr(\n            item,\n            \"definition_order\",\n            item.properties[\"definition_order\"].default)\n        if existing_prio < item_prio:\n            # Existing item has lower priority, so it has precedence.\n            return existing\n        if existing_prio > item_prio:\n            # New item has lower priority, so it has precedence.\n            # Existing item will be deleted below\n            pass\n        else:\n            # Don't know which one to keep, lastly defined has precedence\n            objcls = getattr(self.inner_class, \"my_type\", \"[unknown]\")\n            mesg = \"duplicate %s '%s', from: '%s' and '%s', using lastly defined. \" \\\n                   \"You may manually set the definition_order parameter to avoid this message.\" \\\n                   % (objcls, name, item.imported_from, existing.imported_from)\n            item.configuration_warnings.append(mesg)\n        if item.is_tpl():\n            self.remove_template(existing)\n        else:\n            self.remove_item(existing)\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_template(self, tpl):\n        tpl = self.index_template(tpl)\n        self.templates[tpl.uuid] = tpl", "response": "Add a template into the templates container."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef index_template(self, tpl):\n        objcls = self.inner_class.my_type\n        name = getattr(tpl, 'name', '')\n        if not name:\n            mesg = \"a %s template has been defined without name, from: %s\" % \\\n                   (objcls, tpl.imported_from)\n            tpl.add_error(mesg)\n        elif name in self.name_to_template:\n            tpl = self.manage_conflict(tpl, name)\n        self.name_to_template[name] = tpl\n        logger.debug(\"Indexed a %s template: %s, uses: %s\",\n                     tpl.my_type, name, getattr(tpl, 'use', 'Nothing'))\n        return tpl", "response": "Indexes a template by name into the name_to_template dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_template(self, tpl):\n        try:\n            del self.templates[tpl.uuid]\n        except KeyError:  # pragma: no cover, simple protection\n            pass\n        self.unindex_template(tpl)", "response": "Removes and un - index a template from the templates container."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_item(self, item, index=True):\n        # pylint: disable=too-many-branches, too-many-locals, too-many-nested-blocks\n        \"\"\"\n        Add an item into our containers, and index it depending on the `index` flag.\n\n        :param item: object to add\n        :type item: alignak.objects.item.Item\n        :param index: Flag indicating if the item should be indexed\n        :type index: bool\n        :return: the new items created\n        :rtype list\n        \"\"\"\n        name_property = getattr(self.__class__, \"name_property\", None)\n\n        # Check if some hosts are to be self-generated...\n        generated_hosts = []\n        if name_property:\n            name = getattr(item, name_property, None)\n            if name and '[' in name and ']' in name:\n                # We can create several objects from the same configuration!\n                pattern = name[name.find(\"[\")+1:name.find(\"]\")]\n                if '-' in pattern:\n                    logger.debug(\"Found an host with a patterned name: %s\", pattern)\n                    # pattern is format-min-max\n                    # format is optional\n                    limits = pattern.split('-')\n                    fmt = \"%d\"\n                    min_v = 1\n                    max_v = 1\n                    if len(limits) == 3:\n                        fmt = limits[2]\n                        new_name = name.replace('[%s-%s-%s]' % (limits[0], limits[1], fmt), '***')\n                    else:\n                        new_name = name.replace('[%s-%s]' % (limits[0], limits[1]), '***')\n                    try:\n                        min_v = int(limits[0])\n                    except ValueError:\n                        pass\n                    try:\n                        max_v = int(limits[1])\n                    except ValueError:\n                        pass\n\n                    for idx in range(min_v, max_v + 1):\n                        logger.debug(\"- cloning host: %s\", new_name.replace('***', fmt % idx))\n                        new_host = deepcopy(item)\n                        new_host.uuid = get_a_new_object_id()\n                        new_host.host_name = new_name.replace('***', fmt % idx)\n\n                        # Update some fields with the newly generated host name\n                        for prop in ['display_name', 'alias', 'notes', 'notes_url', 'action_url']:\n                            if getattr(new_host, prop, None) is None:\n                                continue\n                            value = getattr(new_host, prop)\n                            if '$HOSTNAME$' in value:\n                                setattr(new_host, prop, value.replace('$HOSTNAME$',\n                                                                      new_host.host_name))\n\n                        generated_hosts.append(new_host)\n\n        if generated_hosts:\n            for new_host in generated_hosts:\n                if index is True:\n                    new_host = self.index_item(new_host)\n                self.items[new_host.uuid] = new_host\n            logger.info(\"    cloned %d hosts from %s\", len(generated_hosts), item.get_name())\n        else:\n            if index is True and name_property:\n                item = self.index_item(item)\n            self.items[item.uuid] = item\n\n        return generated_hosts", "response": "Add an item into our containers and index it depending on the index flag."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove an object from the set of items and un - indexes it", "response": "def remove_item(self, item):\n        \"\"\"\n        Remove (and un-index) an object\n\n        :param item: object to remove\n        :type item: alignak.objects.item.Item\n        :return: None\n        \"\"\"\n        self.unindex_item(item)\n        self.items.pop(item.uuid, None)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index_item(self, item):\n        name_property = getattr(self.__class__, \"name_property\", None)\n        if name_property is None:\n            return None\n        name = getattr(item, name_property, None)\n        if name is None:\n            item.add_error(\"a %s item has been defined without %s, from: %s\"\n                           % (self.inner_class.my_type, name_property,\n                              getattr(item, 'imported_from', 'Unknown importation source!')))\n        elif name in self.name_to_item:\n            item = self.manage_conflict(item, name)\n        self.name_to_item[name] = item\n        return item", "response": "Index an item into our name_to_item dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef old_properties_names_to_new(self):  # pragma: no cover, never called\n        for i in itertools.chain(iter(list(self.items.values())),\n                                 iter(list(self.templates.values()))):\n            i.old_properties_names_to_new()", "response": "Convert old Nagios2 names to Nagios3 new names\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all_tags(self, item):\n        all_tags = item.get_templates()\n\n        for template_id in item.templates:\n            template = self.templates[template_id]\n            all_tags.append(template.name)\n            all_tags.extend(self.get_all_tags(template))\n        return list(set(all_tags))", "response": "Get all tags of an item"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef linkify_item_templates(self, item):\n        tpls = []\n        tpl_names = item.get_templates()\n\n        for name in tpl_names:\n            template = self.find_tpl_by_name(name)\n            if not template:\n                # TODO: Check if this should not be better to report as an error ?\n                self.add_warning(\"%s %s use/inherit from an unknown template: %s ! from: %s\"\n                                 % (type(item).__name__, item.get_name(), name, item.imported_from))\n            else:\n                if template is item:\n                    self.add_error(\"%s %s use/inherits from itself ! from: %s\"\n                                   % (type(item).__name__, item._get_name(), item.imported_from))\n                else:\n                    tpls.append(template.uuid)\n        item.templates = tpls", "response": "Link templates to item."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef linkify_templates(self):\n        # First we create a list of all templates\n        for i in itertools.chain(iter(list(self.items.values())),\n                                 iter(list(self.templates.values()))):\n            self.linkify_item_templates(i)\n        for i in self:\n            i.tags = self.get_all_tags(i)", "response": "Link all templates and create the template graph too\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_correct(self):\n        # we are ok at the beginning. Hope we are still ok at the end...\n        valid = True\n\n        # Better check individual items before displaying the global items list errors and warnings\n        for i in self:\n            # Alias and display_name hook hook\n            # prop_name = getattr(self.__class__, 'name_property', None)\n            # if prop_name and not getattr(i, 'alias', '') and hasattr(i, prop_name):\n            #     setattr(i, 'alias', getattr(i, prop_name))\n            # if prop_name and getattr(i, 'display_name', '') and hasattr(i, prop_name):\n            #     setattr(i, 'display_name', getattr(i, prop_name))\n\n            # Now other checks\n            if not i.is_correct():\n                valid = False\n                i.add_error(\"Configuration in %s::%s is incorrect; from: %s\"\n                            % (i.my_type, i.get_name(), i.imported_from))\n\n            if i.configuration_errors:\n                self.configuration_errors += i.configuration_errors\n            if i.configuration_warnings:\n                self.configuration_warnings += i.configuration_warnings\n\n        # Raise all previous warnings\n        if self.configuration_warnings:\n            for msg in self.configuration_warnings:\n                logger.warning(\"[items] %s\", msg)\n\n        # Raise all previous errors\n        if self.configuration_errors:\n            valid = False\n            for msg in self.configuration_errors:\n                logger.error(\"[items] %s\", msg)\n\n        return valid", "response": "Check if the items list configuration is correct."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_partial_inheritance(self, prop):\n        for i in itertools.chain(iter(list(self.items.values())),\n                                 iter(list(self.templates.values()))):\n            self.get_property_by_inheritance(i, prop)\n            # If a \"null\" attribute was inherited, delete it\n            try:\n                if getattr(i, prop) == 'null':\n                    delattr(i, prop)\n            except AttributeError:  # pragma: no cover, simple protection\n                pass", "response": "Applies the property with inheritance value of the property\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef linkify_with_contacts(self, contacts):\n        for i in self:\n            if not hasattr(i, 'contacts'):\n                continue\n\n            links_list = strip_and_uniq(i.contacts)\n            new = []\n            for name in [e for e in links_list if e]:\n                contact = contacts.find_by_name(name)\n                if contact is not None and contact.uuid not in new:\n                    new.append(contact.uuid)\n                else:\n                    i.add_error(\"the contact '%s' defined for '%s' is unknown\"\n                                % (name, i.get_name()))\n\n            i.contacts = new", "response": "Link items with contacts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef linkify_with_escalations(self, escalations):\n        for i in self:\n            if not hasattr(i, 'escalations'):\n                continue\n\n            links_list = strip_and_uniq(i.escalations)\n            new = []\n            for name in [e for e in links_list if e]:\n                escalation = escalations.find_by_name(name)\n                if escalation is not None and escalation.uuid not in new:\n                    new.append(escalation.uuid)\n                else:\n                    i.add_error(\"the escalation '%s' defined for '%s' is unknown\"\n                                % (name, i.get_name()))\n\n            i.escalations = new", "response": "Link with escalations\n\n        :param escalations: all escalations object\n        :type escalations: alignak.objects.escalation.Escalations\n        :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef explode_contact_groups_into_contacts(item, contactgroups):\n        if not hasattr(item, 'contact_groups'):\n            return\n\n        # TODO : See if we can remove this if\n        cgnames = ''\n        if item.contact_groups:\n            if isinstance(item.contact_groups, list):\n                cgnames = item.contact_groups\n            else:\n                cgnames = item.contact_groups.split(',')\n        cgnames = strip_and_uniq(cgnames)\n        for cgname in cgnames:\n            contactgroup = contactgroups.find_by_name(cgname)\n            if not contactgroup:\n                item.add_error(\"The contact group '%s' defined on the %s '%s' do not exist\"\n                               % (cgname, item.__class__.my_type, item.get_name()))\n                continue\n            cnames = contactgroups.get_members_of_group(cgname)\n            # We add contacts into our contacts\n            if cnames:\n                if hasattr(item, 'contacts'):\n                    # Fix #1054 - bad contact explosion\n                    # item.contacts.extend(cnames)\n                    item.contacts = item.contacts + cnames\n                else:\n                    item.contacts = cnames", "response": "Get all contacts of contact_groups and put them into contacts container\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlinking items with timeperiods", "response": "def linkify_with_timeperiods(self, timeperiods, prop):\n        \"\"\"\n        Link items with timeperiods items\n\n        :param timeperiods: all timeperiods object\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param prop: property name\n        :type prop: str\n        :return: None\n        \"\"\"\n        for i in self:\n            if not hasattr(i, prop):\n                continue\n\n            tpname = getattr(i, prop).strip()\n            # some default values are '', so set None\n            if not tpname:\n                setattr(i, prop, '')\n                continue\n\n            # Ok, get a real name, search for it\n            timeperiod = timeperiods.find_by_name(tpname)\n            if timeperiod is None:\n                i.add_error(\"The %s of the %s '%s' named '%s' is unknown!\"\n                            % (prop, i.__class__.my_type, i.get_name(), tpname))\n                continue\n\n            setattr(i, prop, timeperiod.uuid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef linkify_with_checkmodulations(self, checkmodulations):\n        for i in self:\n            if not hasattr(i, 'checkmodulations'):\n                continue\n\n            links_list = strip_and_uniq(i.checkmodulations)\n            new = []\n            for name in [e for e in links_list if e]:\n                modulation = checkmodulations.find_by_name(name)\n                if modulation is not None and modulation.uuid not in new:\n                    new.append(modulation.uuid)\n                else:\n                    i.add_error(\"The checkmodulations of the %s '%s' named \"\n                                \"'%s' is unknown!\" % (i.__class__.my_type, i.get_name(), name))\n\n            i.checkmodulations = new", "response": "Link checkmodulations to the checkmodulations of the items in self."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef linkify_s_by_module(self, modules):\n        for i in self:\n\n            links_list = strip_and_uniq(i.modules)\n            new = []\n            for name in [e for e in links_list if e]:\n                module = modules.find_by_name(name)\n                if module is not None and module.uuid not in new:\n                    new.append(module)\n                else:\n                    i.add_error(\"Error: the module %s is unknown for %s\" % (name, i.get_name()))\n\n            i.modules = new", "response": "Link modules to items in modules object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate an expression that evaluates the hosts and hostgroups and returns a list of hosts and hostgroups entries.", "response": "def evaluate_hostgroup_expression(expr, hosts, hostgroups, look_in='hostgroups'):\n        \"\"\"\n        Evaluate hostgroup expression\n\n        :param expr: an expression\n        :type expr: str\n        :param hosts: hosts object (all hosts)\n        :type hosts: alignak.objects.host.Hosts\n        :param hostgroups: hostgroups object (all hostgroups)\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :param look_in: item name where search\n        :type look_in: str\n        :return: return list of hostgroups\n        :rtype: list\n        \"\"\"\n        # Maybe exp is a list, like numerous hostgroups entries in a service, link them\n        if isinstance(expr, list):\n            expr = '|'.join(expr)\n        if look_in == 'hostgroups':\n            node = ComplexExpressionFactory(look_in, hostgroups, hosts)\n        else:  # templates\n            node = ComplexExpressionFactory(look_in, hosts, hosts)\n        expr_tree = node.eval_cor_pattern(expr)\n\n        set_res = expr_tree.resolve_elements()\n\n        # HOOK DBG\n        return list(set_res)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting hosts of hostgroups from hostgroups object", "response": "def get_hosts_from_hostgroups(hgname, hostgroups):\n        \"\"\"\n        Get hosts of hostgroups\n\n        :param hgname: hostgroup name\n        :type hgname: str\n        :param hostgroups: hostgroups object (all hostgroups)\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :return: list of hosts\n        :rtype: list\n        \"\"\"\n        if not isinstance(hgname, list):\n            hgname = [e.strip() for e in hgname.split(',') if e.strip()]\n\n        host_names = []\n\n        for name in hgname:\n            hostgroup = hostgroups.find_by_name(name)\n            if hostgroup is None:\n                raise ValueError(\"the hostgroup '%s' is unknown\" % hgname)\n            mbrs = [h.strip() for h in hostgroup.get_hosts() if h.strip()]\n            host_names.extend(mbrs)\n        return host_names"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexpand hostgroups into hosts", "response": "def explode_host_groups_into_hosts(self, item, hosts, hostgroups):\n        \"\"\"\n        Get all hosts of hostgroups and add all in host_name container\n\n        :param item: the item object\n        :type item: alignak.objects.item.Item\n        :param hosts: hosts object\n        :type hosts: alignak.objects.host.Hosts\n        :param hostgroups: hostgroups object\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :return: None\n        \"\"\"\n        hnames_list = []\n        # Gets item's hostgroup_name\n        hgnames = getattr(item, \"hostgroup_name\", '') or ''\n\n        # Defines if hostgroup is a complex expression\n        # Expands hostgroups\n        if is_complex_expr(hgnames):\n            hnames_list.extend(self.evaluate_hostgroup_expression(\n                item.hostgroup_name, hosts, hostgroups))\n        elif hgnames:\n            try:\n                hnames_list.extend(\n                    self.get_hosts_from_hostgroups(hgnames, hostgroups))\n            except ValueError as err:  # pragma: no cover, simple protection\n                item.add_error(str(err))\n\n        # Expands host names\n        hname = getattr(item, \"host_name\", '')\n        hnames_list.extend([n.strip() for n in hname.split(',') if n.strip()])\n        hnames = set()\n\n        for host in hnames_list:\n            # If the host start with a !, it's to be removed from\n            # the hostgroup get list\n            if host.startswith('!'):\n                hst_to_remove = host[1:].strip()\n                try:\n                    hnames.remove(hst_to_remove)\n                except KeyError:\n                    pass\n            elif host == '*':\n                hnames.update([host.host_name for host\n                               in hosts.items.values() if getattr(host, 'host_name', '')])\n            # Else it's a host to add, but maybe it's ALL\n            else:\n                hnames.add(host)\n\n        item.host_name = ','.join(hnames)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef no_loop_in_parents(self, attr1, attr2):\n        # pylint: disable=too-many-branches\n        \"\"\"\n        Find loop in dependencies.\n        For now, used with the following attributes :\n        :(self, parents):\n            host dependencies from host object\n        :(host_name, dependent_host_name):\\\n            host dependencies from hostdependencies object\n        :(service_description, dependent_service_description):\n            service dependencies from servicedependencies object\n\n        :param attr1: attribute name\n        :type attr1: str\n        :param attr2: attribute name\n        :type attr2: str\n        :return: list\n        :rtype: list\n        \"\"\"\n        # Ok, we say \"from now, no loop :) \"\n        # in_loop = []\n\n        # Create parent graph\n        parents = Graph()\n\n        # Start with all items as nodes\n        for item in self:\n            # Hack to get self here. Used when looping on host and host parent's\n            if attr1 == \"self\":\n                obj = item.uuid          # obj is a host/service [list]\n            else:\n                obj = getattr(item, attr1, None)\n            if obj is not None:\n                if isinstance(obj, list):\n                    for sobj in obj:\n                        parents.add_node(sobj)\n                else:\n                    parents.add_node(obj)\n\n        # And now fill edges\n        # pylint: disable=too-many-nested-blocks\n        for item in self:\n            if attr1 == \"self\":\n                obj1 = item.uuid\n            else:\n                obj1 = getattr(item, attr1, None)\n            obj2 = getattr(item, attr2, None)\n            if obj2 is not None:\n                if isinstance(obj2, list):\n                    for sobj2 in obj2:\n                        if isinstance(obj1, list):\n                            for sobj1 in obj1:\n                                parents.add_edge(sobj1, sobj2)\n                        else:\n                            parents.add_edge(obj1, sobj2)\n                else:\n                    if isinstance(obj1, list):\n                        for sobj1 in obj1:\n                            parents.add_edge(sobj1, obj2)\n                    else:\n                        parents.add_edge(obj1, obj2)\n\n        return parents.loop_check()", "response": "Find loop in dependencies."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_property_by_inheritance(self, obj, prop):\n        # pylint: disable=too-many-branches, too-many-nested-blocks\n        \"\"\"\n        Get the property asked in parameter to this object or from defined templates of this\n        object\n\n        todo: rewrite this function which is really too complex!\n\n        :param obj: the object to search the property\n        :type obj: alignak.objects.item.Item\n        :param prop: name of property\n        :type prop: str\n        :return: Value of property of this object or of a template\n        :rtype: str or None\n        \"\"\"\n        if prop == 'register':\n            # We do not inherit the register property\n            return None\n\n        # If I have the property, I take mine but I check if I must add a plus property\n        if hasattr(obj, prop):\n            value = getattr(obj, prop)\n            # Manage the additive inheritance for the property,\n            # if property is in plus, add or replace it\n            # Template should keep the '+' at the beginning of the chain\n            if obj.has_plus(prop):\n                if not isinstance(value, list):\n                    value = [value]\n                value.insert(0, obj.get_plus_and_delete(prop))\n                value = list(set(value))\n                if obj.is_tpl():\n                    value.insert(0, '+')\n\n            # Clean the returned value\n            if isinstance(value, list):\n                # Get unique ordered list\n                new_list = []\n                for elt in value:\n                    if elt not in new_list:\n                        new_list.append(elt)\n                value = new_list\n                if not obj.is_tpl():\n                    while '+' in value:\n                        value.remove('+')\n            return value\n\n        # Ok, I do not have prop, Maybe my templates do?\n        # Same story for plus\n        # We reverse list, so that when looking for properties by inheritance,\n        # the least defined template wins (if property is set).\n        for t_id in obj.templates:\n            template = self.templates[t_id]\n            value = self.get_property_by_inheritance(template, prop)\n\n            if value is None or (isinstance(value, list) and not value):\n                continue\n\n            # If our template give us a '+' value, we continue the loop\n            still_loop = False\n            if isinstance(value, list) and value[0] == '+':\n                # Templates should keep their + inherited from their parents\n                if not obj.is_tpl():\n                    value = list(value)\n                    value = value[1:]\n                still_loop = True\n\n            # Maybe in the previous loop, we set a value, use it too\n            if hasattr(obj, prop):\n                # If the current value is a string, it will simplify the problem\n                if isinstance(value, (list, string_types)) and value and value[0] == '+':\n                    # In this case we can remove the + from our current\n                    # tpl because our value will be final\n                    new_val = list(getattr(obj, prop))\n                    new_val.extend(value[1:])\n                    value = new_val\n                else:  # If not, we should keep the + sign of need\n                    new_val = list(getattr(obj, prop))\n                    new_val.extend(value)\n                    value = new_val\n\n            # Ok, we can set it and uniquify a list if needed\n            if isinstance(value, list):\n                # Get unique ordered list\n                new_list = []\n                for elt in value:\n                    if elt not in new_list:\n                        new_list.append(elt)\n                value = new_list\n                if not obj.is_tpl():\n                    while '+' in value:\n                        value.remove('+')\n\n            setattr(obj, prop, value)\n\n            # If we only got some '+' values, we must still loop\n            # for an end value without it\n            if not still_loop:\n                # And set my own value in the end if need\n                if obj.has_plus(prop):\n                    # value = list(getattr(obj, prop, []))\n                    value = list(value)\n                    value.extend(obj.get_plus_and_delete(prop))\n                    # Template should keep their '+'\n                    if obj.is_tpl() and value[0] != '+':\n                        value.insert(0, '+')\n\n                    # Clean the returned value\n                    if isinstance(value, list):\n                        # Get unique ordered list\n                        new_list = []\n                        for elt in value:\n                            if elt not in new_list:\n                                new_list.append(elt)\n                        value = new_list\n                        if not obj.is_tpl():\n                            while '+' in value:\n                                value.remove('+')\n\n                    setattr(obj, prop, value)\n                return value\n\n        # Maybe templates only give us + values, so we didn't quit, but we already got a\n        # self.prop value after all\n        template_with_only_plus = hasattr(obj, prop)\n\n        # I do not have endingprop, my templates too... Maybe a plus?\n        # warning: if all my templates gave me '+' values, do not forgot to\n        # add the already set self.prop value\n        if obj.has_plus(prop):\n            if template_with_only_plus:\n                value = list(getattr(obj, prop))\n                value.extend(obj.get_plus_and_delete(prop))\n            else:\n                value = obj.get_plus_and_delete(prop)\n            # Template should keep their '+' chain\n            # We must say it's a '+' value, so our son will know that it must continue looping\n            if obj.is_tpl() and value != [] and value[0] != '+':\n                value.insert(0, '+')\n\n            # Clean the returned value\n            if isinstance(value, list):\n                # Get unique ordered list\n                new_list = []\n                for elt in value:\n                    if elt not in new_list:\n                        new_list.append(elt)\n                value = new_list\n                if not obj.is_tpl():\n                    while '+' in value:\n                        value.remove('+')\n\n            setattr(obj, prop, value)\n            return value\n\n        # Ok so in the end, we give the value we got if we have one, or None\n        # Not even a plus... so None :)\n        return getattr(obj, prop, None)", "response": "Get the value of a property in the object or from defined templates of this object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_customs_properties_by_inheritance(self, obj):\n        for t_id in obj.templates:\n            template = self.templates[t_id]\n            tpl_cv = self.get_customs_properties_by_inheritance(template)\n            if tpl_cv:\n                for prop in tpl_cv:\n                    if prop not in obj.customs:\n                        value = tpl_cv[prop]\n                    else:\n                        value = obj.customs[prop]\n                    if obj.has_plus(prop):\n                        value.insert(0, obj.get_plus_and_delete(prop))\n                        # value = self.get_plus_and_delete(prop) + ',' + value\n                    obj.customs[prop] = value\n        for prop in obj.customs:\n            value = obj.customs[prop]\n            if obj.has_plus(prop):\n                value.insert(0, obj.get_plus_and_delete(prop))\n                obj.customs[prop] = value\n        # We can get custom properties in plus, we need to get all\n        # entires and put\n        # them into customs\n        cust_in_plus = obj.get_all_plus_and_delete()\n        for prop in cust_in_plus:\n            obj.customs[prop] = cust_in_plus[prop]\n        return obj.customs", "response": "Get custom properties from the templates defined in this object and put them into the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_edge(self, from_node, to_node):\n        # Maybe to_node is unknown\n        if to_node not in self.nodes:\n            self.add_node(to_node)\n\n        try:\n            self.nodes[from_node][\"sons\"].append(to_node)\n        # If from_node does not exist, add it with its son\n        except KeyError:\n            self.nodes[from_node] = {\"dfs_loop_status\": \"\", \"sons\": [to_node]}", "response": "Add edge between two node objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if we have a loop in the graph.", "response": "def loop_check(self):\n        \"\"\"Check if we have a loop in the graph\n\n        :return: Nodes in loop\n        :rtype: list\n        \"\"\"\n        in_loop = []\n        # Add the tag for dfs check\n        for node in list(self.nodes.values()):\n            node['dfs_loop_status'] = 'DFS_UNCHECKED'\n\n        # Now do the job\n        for node_id, node in self.nodes.items():\n            # Run the dfs only if the node has not been already done */\n            if node['dfs_loop_status'] == 'DFS_UNCHECKED':\n                self.dfs_loop_search(node_id)\n            # If LOOP_INSIDE, must be returned\n            if node['dfs_loop_status'] == 'DFS_LOOP_INSIDE':\n                in_loop.append(node_id)\n\n        # Remove the tag\n        for node in list(self.nodes.values()):\n            del node['dfs_loop_status']\n\n        return in_loop"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dfs_loop_search(self, root):\n        # Make the root temporary checked\n        self.nodes[root]['dfs_loop_status'] = 'DFS_TEMPORARY_CHECKED'\n\n        # We are scanning the sons\n        for child in self.nodes[root][\"sons\"]:\n            child_status = self.nodes[child]['dfs_loop_status']\n            # If a child is not checked, check it\n            if child_status == 'DFS_UNCHECKED':\n                self.dfs_loop_search(child)\n                child_status = self.nodes[child]['dfs_loop_status']\n\n            # If a child has already been temporary checked, it's a problem,\n            # loop inside, and its a checked status\n            if child_status == 'DFS_TEMPORARY_CHECKED':\n                self.nodes[child]['dfs_loop_status'] = 'DFS_LOOP_INSIDE'\n                self.nodes[root]['dfs_loop_status'] = 'DFS_LOOP_INSIDE'\n\n            # If a child has already been temporary checked, it's a problem, loop inside\n            if child_status in ('DFS_NEAR_LOOP', 'DFS_LOOP_INSIDE'):\n                # if a node is known to be part of a loop, do not let it be less\n                if self.nodes[root]['dfs_loop_status'] != 'DFS_LOOP_INSIDE':\n                    self.nodes[root]['dfs_loop_status'] = 'DFS_NEAR_LOOP'\n                # We've already seen this child, it's a problem\n                self.nodes[child]['dfs_loop_status'] = 'DFS_LOOP_INSIDE'\n\n        # If root have been modified, do not set it OK\n        # A node is OK if and only if all of its children are OK\n        # if it does not have a child, goes ok\n        if self.nodes[root]['dfs_loop_status'] == 'DFS_TEMPORARY_CHECKED':\n            self.nodes[root]['dfs_loop_status'] = 'DFS_OK'", "response": "This method searches the dependency tree for loop. A loop is found in the loop. The loop is found in the dependency tree and the loop is found in the dependency tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the list of accessibility packs of the graph in one pack element.", "response": "def get_accessibility_packs(self):\n        \"\"\"Get accessibility packs of the graph:\n        in one pack element are related in a way. Between packs, there is no relation at all.\n        TODO: Make it work for directional graph too\n        Because for now, edge must be father->son AND son->father\n\n        :return: packs of nodes\n        :rtype: list\n        \"\"\"\n        packs = []\n        # Add the tag for dfs check\n        for node in list(self.nodes.values()):\n            node['dfs_loop_status'] = 'DFS_UNCHECKED'\n\n        for node_id, node in self.nodes.items():\n            # Run the dfs only if the node is not already done */\n            if node['dfs_loop_status'] == 'DFS_UNCHECKED':\n                packs.append(self.dfs_get_all_childs(node_id))\n\n        # Remove the tag\n        for node in list(self.nodes.values()):\n            del node['dfs_loop_status']\n\n        return packs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dfs_get_all_childs(self, root):\n        self.nodes[root]['dfs_loop_status'] = 'DFS_CHECKED'\n\n        ret = set()\n        # Me\n        ret.add(root)\n        # And my sons\n        ret.update(self.nodes[root]['sons'])\n\n        for child in self.nodes[root]['sons']:\n            # I just don't care about already checked children\n            if self.nodes[child]['dfs_loop_status'] == 'DFS_UNCHECKED':\n                ret.update(self.dfs_get_all_childs(child))\n\n        return list(ret)", "response": "Recursively get all sons of this node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef identity(self):\n        res = self.app.get_id()\n        res.update({\"start_time\": self.start_time})\n        res.update({\"running_id\": self.running_id})\n        return res", "response": "Get the daemon identity"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting the methods available on the daemon Web service interface and their parameters.", "response": "def api(self):\n        \"\"\"List the methods available on the daemon Web service interface\n\n        :return: a list of methods and parameters\n        :rtype: dict\n        \"\"\"\n        functions = [x[0]for x in inspect.getmembers(self, predicate=inspect.ismethod)\n                     if not x[0].startswith('_')]\n\n        full_api = {\n            'doc': u\"When posting data you have to use the JSON format.\",\n            'api': []\n        }\n        my_daemon_type = \"%s\" % getattr(self.app, 'type', 'unknown')\n        my_address = getattr(self.app, 'host_name', getattr(self.app, 'name', 'unknown'))\n        if getattr(self.app, 'address', '127.0.0.1') not in ['127.0.0.1']:\n            # If an address is explicitely specified, I must use it!\n            my_address = self.app.address\n        for fun in functions:\n            endpoint = {\n                'daemon': my_daemon_type,\n                'name': fun,\n                'doc': getattr(self, fun).__doc__,\n                'uri': '%s://%s:%s/%s' % (getattr(self.app, 'scheme', 'http'),\n                                          my_address,\n                                          self.app.port, fun),\n                'args': {}\n            }\n\n            try:\n                spec = inspect.getfullargspec(getattr(self, fun))\n            except Exception:  # pylint: disable=broad-except\n                # pylint: disable=deprecated-method\n                spec = inspect.getargspec(getattr(self, fun))\n            args = [a for a in spec.args if a not in ('self', 'cls')]\n            if spec.defaults:\n                a_dict = dict(list(zip(args, spec.defaults)))\n            else:\n                a_dict = dict(list(zip(args, (\"No default value\",) * len(args))))\n\n            endpoint[\"args\"] = a_dict\n            full_api['api'].append(endpoint)\n\n        return full_api"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrequest the daemon to stop now.", "response": "def stop_request(self, stop_now='0'):\n        \"\"\"Request the daemon to stop\n\n        If `stop_now` is set to '1' the daemon will stop now. Else, the daemon\n        will enter the stop wait mode. In this mode the daemon stops its activity and\n        waits until it receives a new `stop_now` request to stop really.\n\n        :param stop_now: stop now or go to stop wait mode\n        :type stop_now: bool\n        :return: None\n        \"\"\"\n        self.app.interrupted = (stop_now == '1')\n        self.app.will_stop = True\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_log_level(self):\n        level_names = {\n            logging.DEBUG: 'DEBUG', logging.INFO: 'INFO', logging.WARNING: 'WARNING',\n            logging.ERROR: 'ERROR', logging.CRITICAL: 'CRITICAL'\n        }\n        alignak_logger = logging.getLogger(ALIGNAK_LOGGER_NAME)\n\n        res = self.identity()\n        res.update({\"log_level\": alignak_logger.getEffectiveLevel(),\n                    \"log_level_name\": level_names[alignak_logger.getEffectiveLevel()]})\n        return res", "response": "Get the current daemon log level"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_log_level(self, log_level=None):\n        if log_level is None:\n            log_level = cherrypy.request.json['log_level']\n\n        if log_level not in ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']:\n            return {'_status': u'ERR',\n                    '_message': u\"Required log level is not allowed: %s\" % log_level}\n\n        alignak_logger = logging.getLogger(ALIGNAK_LOGGER_NAME)\n        alignak_logger.setLevel(log_level)\n        return self.get_log_level()", "response": "Set the current log level for the daemon"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets statistics and information from the daemons", "response": "def stats(self, details=False):\n        \"\"\"Get statistics and information from the daemon\n\n        Returns an object with the daemon identity, the daemon start_time\n        and some extra properties depending upon the daemon type.\n\n        All daemons provide these ones:\n        - program_start: the Alignak start timestamp\n        - spare: to indicate if the daemon is a spare one\n        - load: the daemon load\n        - modules: the daemon modules information\n        - counters: the specific daemon counters\n\n        :param details: Details are required (different from 0)\n        :type details str\n\n        :return: daemon stats\n        :rtype: dict\n        \"\"\"\n        if details is not False:\n            details = bool(details)\n        res = self.identity()\n        res.update(self.app.get_daemon_stats(details=details))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nask the daemon to drop its configuration and wait for a new one.", "response": "def _wait_new_conf(self):\n        \"\"\"Ask the daemon to drop its configuration and wait for a new one\n\n        :return: None\n        \"\"\"\n        with self.app.conf_lock:\n            logger.debug(\"My Arbiter wants me to wait for a new configuration.\")\n            # Clear can occur while setting up a new conf and lead to error.\n            self.app.schedulers.clear()\n            self.app.cur_conf = {}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _push_configuration(self, pushed_configuration=None):\n        if pushed_configuration is None:\n            confs = cherrypy.request.json\n            pushed_configuration = confs['conf']\n        # It is safer to lock this part\n        with self.app.conf_lock:\n            self.app.new_conf = pushed_configuration\n            return True", "response": "Send a new configuration to the daemon"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _have_conf(self, magic_hash=None):\n        self.app.have_conf = getattr(self.app, 'cur_conf', None) not in [None, {}]\n        if magic_hash is not None:\n            # Beware, we got an str in entry, not an int\n            magic_hash = int(magic_hash)\n            # I've got a conf and a good one\n            return self.app.have_conf and self.app.cur_conf.magic_hash == magic_hash\n\n        return self.app.have_conf", "response": "Return True if the daemon has a configuration from its arbiter."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _push_actions(self):\n        data = cherrypy.request.json\n        with self.app.lock:\n            self.app.add_actions(data['actions'], data['scheduler_instance_id'])", "response": "Push the actions to the poller"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _results(self, scheduler_instance_id):\n        with self.app.lock:\n            res = self.app.get_results_from_passive(scheduler_instance_id)\n        return serialize(res, True)", "response": "Get the results of the executed actions for the scheduler which instance id is provided."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _broks(self, broker_name):  # pylint: disable=unused-argument\n        with self.app.broks_lock:\n            res = self.app.get_broks()\n        return serialize(res, True)", "response": "Get the broks list from the daemon"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the monitoring events from the daemon", "response": "def _events(self):\n        \"\"\"Get the monitoring events from the daemon\n\n        This is used by the arbiter to get the monitoring events from all its satellites\n\n        :return: Events list serialized\n        :rtype: list\n        \"\"\"\n        with self.app.events_lock:\n            res = self.app.get_events()\n        return serialize(res, True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_state(self, hosts, services):\n        # If we are a host or a service, we just got the host/service\n        # hard state\n        if self.operand == 'host':\n            host = hosts[self.sons[0]]\n            return self.get_host_node_state(host.last_hard_state_id,\n                                            host.problem_has_been_acknowledged,\n                                            host.in_scheduled_downtime)\n        if self.operand == 'service':\n            service = services[self.sons[0]]\n            return self.get_service_node_state(service.last_hard_state_id,\n                                               service.problem_has_been_acknowledged,\n                                               service.in_scheduled_downtime)\n        if self.operand == '|':\n            return self.get_complex_or_node_state(hosts, services)\n\n        if self.operand == '&':\n            return self.get_complex_and_node_state(hosts, services)\n\n        #  It's an Xof rule\n        if self.operand == 'of:':\n            return self.get_complex_xof_node_state(hosts, services)\n\n        # We have an unknown node. Code is not reachable because we validate operands\n        return 4", "response": "Get the state of the node by looking recursively over sons and applying operand\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the state of the host node.", "response": "def get_host_node_state(self, state, problem_has_been_acknowledged, in_scheduled_downtime):\n        \"\"\"Get host node state, simplest case ::\n\n        * Handle not value (revert) for host and consider 1 as 2\n\n        :return: 0, 1 or 2\n        :rtype: int\n        \"\"\"\n        # Make DOWN look as CRITICAL (2 instead of 1)\n        if state == 1:\n            state = 2\n\n        # If our node is acknowledged or in downtime, state is ok/up\n        if problem_has_been_acknowledged or in_scheduled_downtime:\n            state = 0\n\n        # Maybe we are a NOT node, so manage this\n        if self.not_value:\n            return 0 if state else 2  # Keep the logic of return Down on NOT rules\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_service_node_state(self, state, problem_has_been_acknowledged, in_scheduled_downtime):\n        # If our node is acknowledged or in downtime, state is ok/up\n        if problem_has_been_acknowledged or in_scheduled_downtime:\n            state = 0\n\n        # Maybe we are a NOT node, so manage this\n        if self.not_value:\n            # Critical -> OK\n            if state == 2:\n                return 0\n            # OK -> CRITICAL (warning is untouched)\n            if state == 0:\n                return 2\n        return state", "response": "Get the state of the service node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_complex_or_node_state(self, hosts, services):\n        # First we get the state of all our sons\n        states = [s.get_state(hosts, services) for s in self.sons]\n        # Next we calculate the best state\n        best_state = min(states)\n        # Then we handle eventual not value\n        if self.not_value:\n            return self.get_reverse_state(best_state)\n        return best_state", "response": "Get the state of the node object in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_complex_and_node_state(self, hosts, services):\n        # First we get the state of all our sons\n        states = [s.get_state(hosts, services) for s in self.sons]\n        # Next we calculate the worst state\n        if 2 in states:\n            worst_state = 2\n        else:\n            worst_state = max(states)\n        # Then we handle eventual not value\n        if self.not_value:\n            return self.get_reverse_state(worst_state)\n        return worst_state", "response": "Get the state of the node and its sons."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the state of a complex X of aggregation.", "response": "def get_complex_xof_node_state(self, hosts, services):\n        # pylint: disable=too-many-locals, too-many-return-statements, too-many-branches\n        \"\"\"Get state , handle X of aggregation ::\n\n           * Count the number of OK, WARNING, CRITICAL\n           * Try too apply, in this order, Critical, Warning, OK rule\n           * Return the code for first match (2, 1, 0)\n           * If no rule apply, return OK for simple X of and worst state for multiple X of\n\n        :param hosts: host objects\n        :param services: service objects\n        :return: 0, 1 or 2\n        :rtype: int\n        TODO: Looks like the last if does the opposite of what the comment says\n        \"\"\"\n        # First we get the state of all our sons\n        states = [s.get_state(hosts, services) for s in self.sons]\n\n        # We search for OK, WARNING or CRITICAL applications\n        # And we will choice between them\n        nb_search_ok = self.of_values[0]\n        nb_search_warn = self.of_values[1]\n        nb_search_crit = self.of_values[2]\n\n        # We look for each application\n        nb_sons = len(states)\n        nb_ok = nb_warn = nb_crit = 0\n        for state in states:\n            if state == 0:\n                nb_ok += 1\n            elif state == 1:\n                nb_warn += 1\n            elif state == 2:\n                nb_crit += 1\n\n        def get_state_for(nb_tot, nb_real, nb_search):\n            \"\"\"Check if there is enough value to apply this rule\n\n            :param nb_tot: total number of value\n            :type nb_tot: int\n            :param nb_real: number of value that apply for this rule\n            :type nb_real: int\n            :param nb_search: max value to apply this rule (can be a percent)\n            :type nb_search: int\n            :return: True if the rule is effective (roughly nb_real > nb_search), False otherwise\n            :rtype: bool\n            \"\"\"\n            if nb_search.endswith('%'):\n                nb_search = int(nb_search[:-1])\n                if nb_search < 0:\n                    # nb_search is negative, so +\n                    nb_search = max(100 + nb_search, 0)\n                apply_for = float(nb_real) / nb_tot * 100 >= nb_search\n            else:\n                nb_search = int(nb_search)\n                if nb_search < 0:\n                    # nb_search is negative, so +\n                    nb_search = max(nb_tot + nb_search, 0)\n                apply_for = nb_real >= nb_search\n            return apply_for\n\n        ok_apply = get_state_for(nb_sons, nb_ok, nb_search_ok)\n        warn_apply = get_state_for(nb_sons, nb_warn + nb_crit, nb_search_warn)\n        crit_apply = get_state_for(nb_sons, nb_crit, nb_search_crit)\n\n        # return the worst state that apply\n        if crit_apply:\n            if self.not_value:\n                return self.get_reverse_state(2)\n            return 2\n\n        if warn_apply:\n            if self.not_value:\n                return self.get_reverse_state(1)\n            return 1\n\n        if ok_apply:\n            if self.not_value:\n                return self.get_reverse_state(0)\n            return 0\n\n        # Maybe even OK is not possible, if so, it depends if the admin\n        # ask a simple form Xof: or a multiple one A,B,Cof:\n        # the simple should give OK, the mult should give the worst state\n        if self.is_of_mul:\n            if self.not_value:\n                return self.get_reverse_state(0)\n            return 0\n\n        if 2 in states:\n            worst_state = 2\n        else:\n            worst_state = max(states)\n        if self.not_value:\n            return self.get_reverse_state(worst_state)\n        return worst_state"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all host and service uuid in our node and below", "response": "def list_all_elements(self):\n        \"\"\"Get all host/service uuid in our node and below\n\n        :return: list of hosts/services uuids\n        :rtype: list\n        \"\"\"\n        res = []\n\n        # We are a host/service\n        if self.operand in ['host', 'service']:\n            return [self.sons[0]]\n\n        for son in self.sons:\n            res.extend(son.list_all_elements())\n\n        # and returns a list of unique uuids\n        return list(set(res))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nswitching the values of the of_values attribute to NB sons instead.", "response": "def switch_zeros_of_values(self):\n        \"\"\"If we are a of: rule, we can get some 0 in of_values,\n           if so, change them with NB sons instead\n\n        :return: None\n        \"\"\"\n        nb_sons = len(self.sons)\n        # Need a list for assignment\n        new_values = list(self.of_values)\n        for i in [0, 1, 2]:\n            if new_values[i] == '0':\n                new_values[i] = str(nb_sons)\n        self.of_values = tuple(new_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if all leaves are correct", "response": "def is_valid(self):\n        \"\"\"Check if all leaves are correct (no error)\n\n        :return: True if correct, otherwise False\n        :rtype: bool\n        \"\"\"\n\n        valid = True\n        if not self.sons:\n            valid = False\n        else:\n            for son in self.sons:\n                if isinstance(son, DependencyNode) and not son.is_valid():\n                    self.configuration_errors.extend(son.configuration_errors)\n                    valid = False\n        return valid"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses and build a tree of DependencyNode from a pattern.", "response": "def eval_cor_pattern(self, pattern, hosts, services, hostgroups, servicegroups, running=False):\n        \"\"\"Parse and build recursively a tree of DependencyNode from pattern\n\n        :param pattern: pattern to parse\n        :type pattern: str\n        :param hosts: hosts list, used to find a specific host\n        :type hosts: alignak.objects.host.Host\n        :param services: services list, used to find a specific service\n        :type services: alignak.objects.service.Service\n        :param running: rules are evaluated at run time and parsing. True means runtime\n        :type running: bool\n        :return: root node of parsed tree\n        :rtype: alignak.dependencynode.DependencyNode\n        \"\"\"\n        pattern = pattern.strip()\n        complex_node = False\n\n        # Look if it's a complex pattern (with rule) or\n        # if it's a leaf of it, like a host/service\n        for char in '()&|':\n            if char in pattern:\n                complex_node = True\n\n        # If it's a simple node, evaluate it directly\n        if complex_node is False:\n            return self.eval_simple_cor_pattern(pattern, hosts, services,\n                                                hostgroups, servicegroups, running)\n        return self.eval_complex_cor_pattern(pattern, hosts, services,\n                                             hostgroups, servicegroups, running)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eval_xof_pattern(node, pattern):\n        xof_pattern = r\"^(-?\\d+%?),*(-?\\d*%?),*(-?\\d*%?) *of: *(.+)\"\n        regex = re.compile(xof_pattern)\n        matches = regex.search(pattern)\n        if matches is not None:\n            node.operand = 'of:'\n            groups = matches.groups()\n            # We can have a Aof: rule, or a multiple A,B,Cof: rule.\n            mul_of = (groups[1] != '' and groups[2] != '')\n            # If multi got (A,B,C)\n            if mul_of:\n                node.is_of_mul = True\n                node.of_values = (groups[0], groups[1], groups[2])\n            else:  # if not, use A,0,0, we will change 0 after to put MAX\n                node.of_values = (groups[0], '0', '0')\n            pattern = matches.groups()[3]\n        return pattern", "response": "Parse a X of pattern and return the pattern"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eval_complex_cor_pattern(self, pattern, hosts, services,\n                                 hostgroups, servicegroups, running=False):\n        # pylint: disable=too-many-branches\n        \"\"\"Parse and build recursively a tree of DependencyNode from a complex pattern\n\n        :param pattern: pattern to parse\n        :type pattern: str\n        :param hosts: hosts list, used to find a specific host\n        :type hosts: alignak.objects.host.Host\n        :param services: services list, used to find a specific service\n        :type services: alignak.objects.service.Service\n        :param running: rules are evaluated at run time and parsing. True means runtime\n        :type running: bool\n        :return: root node of parsed tree\n        :rtype: alignak.dependencynode.DependencyNode\n        \"\"\"\n        node = DependencyNode()\n        pattern = self.eval_xof_pattern(node, pattern)\n\n        in_par = False\n        tmp = ''\n        son_is_not = False  # We keep is the next son will be not or not\n        stacked_parenthesis = 0\n        for char in pattern:\n            if char == '(':\n                stacked_parenthesis += 1\n\n                in_par = True\n                tmp = tmp.strip()\n                # Maybe we just start a par, but we got some things in tmp\n                # that should not be good in fact !\n                if stacked_parenthesis == 1 and tmp != '':\n                    # TODO : real error\n                    print(\"ERROR : bad expression near\", tmp)\n                    continue\n\n                # If we are already in a par, add this (\n                # but not if it's the first one so\n                if stacked_parenthesis > 1:\n                    tmp += char\n\n            elif char == ')':\n                stacked_parenthesis -= 1\n\n                if stacked_parenthesis < 0:\n                    # TODO : real error\n                    print(\"Error : bad expression near\", tmp, \"too much ')'\")\n                    continue\n\n                if stacked_parenthesis == 0:\n                    tmp = tmp.strip()\n                    son = self.eval_cor_pattern(tmp, hosts, services,\n                                                hostgroups, servicegroups, running)\n                    # Maybe our son was notted\n                    if son_is_not:\n                        son.not_value = True\n                        son_is_not = False\n                    node.sons.append(son)\n                    in_par = False\n                    # OK now clean the tmp so we start clean\n                    tmp = ''\n                    continue\n\n                # ok here we are still in a huge par, we just close one sub one\n                tmp += char\n\n            # Expressions in par will be parsed in a sub node after. So just\n            # stack pattern\n            elif in_par:\n                tmp += char\n\n            # Until here, we're not in par\n\n            # Manage the NOT for an expression. Only allow ! at the beginning\n            # of a host or a host,service expression.\n            elif char == '!':\n                tmp = tmp.strip()\n                if tmp and tmp[0] != '!':\n                    print(\"Error : bad expression near\", tmp, \"wrong position for '!'\")\n                    continue\n                # Flags next node not state\n                son_is_not = True\n                # DO NOT keep the c in tmp, we consumed it\n\n            elif char in ['&', '|']:\n                # Oh we got a real cut in an expression, if so, cut it\n                tmp = tmp.strip()\n                # Look at the rule viability\n                if node.operand is not None and node.operand != 'of:' and char != node.operand:\n                    # Should be logged as a warning / info? :)\n                    return None\n\n                if node.operand != 'of:':\n                    node.operand = char\n                if tmp != '':\n                    son = self.eval_cor_pattern(tmp, hosts, services,\n                                                hostgroups, servicegroups, running)\n                    # Maybe our son was notted\n                    if son_is_not:\n                        son.not_value = True\n                        son_is_not = False\n                    node.sons.append(son)\n                tmp = ''\n\n            # Maybe it's a classic character or we're in par, if so, continue\n            else:\n                tmp += char\n\n        # Be sure to manage the trainling part when the line is done\n        tmp = tmp.strip()\n        if tmp != '':\n            son = self.eval_cor_pattern(tmp, hosts, services,\n                                        hostgroups, servicegroups, running)\n            # Maybe our son was notted\n            if son_is_not:\n                son.not_value = True\n                son_is_not = False\n            node.sons.append(son)\n\n        # We got our nodes, so we can update 0 values of of_values\n        # with the number of sons\n        node.switch_zeros_of_values()\n\n        return node", "response": "Parse and build a tree of DependencyNode from a complex pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eval_simple_cor_pattern(self, pattern, hosts, services,\n                                hostgroups, servicegroups, running=False):\n        \"\"\"Parse and build recursively a tree of DependencyNode from a simple pattern\n\n        :param pattern: pattern to parse\n        :type pattern: str\n        :param hosts: hosts list, used to find a specific host\n        :type hosts: alignak.objects.host.Host\n        :param services: services list, used to find a specific service\n        :type services: alignak.objects.service.Service\n        :param running: rules are evaluated at run time and parsing. True means runtime\n        :type running: bool\n        :return: root node of parsed tree\n        :rtype: alignak.dependencynode.DependencyNode\n        \"\"\"\n        node = DependencyNode()\n        pattern = self.eval_xof_pattern(node, pattern)\n\n        # If it's a not value, tag the node and find\n        # the name without this ! operator\n        if pattern.startswith('!'):\n            node.not_value = True\n            pattern = pattern[1:]\n        # Is the pattern an expression to be expanded?\n        if re.search(r\"^([%s]+|\\*):\" % self.host_flags, pattern) or \\\n                re.search(r\",\\s*([%s]+:.*|\\*)$\" % self.service_flags, pattern):\n            # o is just extracted its attributes, then trashed.\n            son = self.expand_expression(pattern, hosts, services,\n                                         hostgroups, servicegroups, running)\n            if node.operand != 'of:':\n                node.operand = '&'\n            node.sons.extend(son.sons)\n            node.configuration_errors.extend(son.configuration_errors)\n            node.switch_zeros_of_values()\n        else:\n            node.operand = 'object'\n            obj, error = self.find_object(pattern, hosts, services)\n            # here we have Alignak SchedulingItem object (Host/Service)\n            if obj is not None:\n                # Set host or service\n                # pylint: disable=E1101\n                node.operand = obj.__class__.my_type\n                node.sons.append(obj.uuid)  # Only store the uuid, not the full object.\n            else:\n                if running is False:\n                    node.configuration_errors.append(error)\n                else:\n                    # As business rules are re-evaluated at run time on\n                    # each scheduling loop, if the rule becomes invalid\n                    # because of a badly written macro modulation, it\n                    # should be notified upper for the error to be\n                    # displayed in the check output.\n                    raise Exception(error)\n        return node", "response": "Parse and build a tree of DependencyNode from a simple pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind object from pattern in hosts and services list.", "response": "def find_object(self, pattern, hosts, services):\n        \"\"\"Find object from pattern\n\n        :param pattern: text to search (host1,service1)\n        :type pattern: str\n        :param hosts: hosts list, used to find a specific host\n        :type hosts: alignak.objects.host.Host\n        :param services: services list, used to find a specific service\n        :type services: alignak.objects.service.Service\n        :return: tuple with Host or Service object and error\n        :rtype: tuple\n        \"\"\"\n        obj = None\n        error = None\n        is_service = False\n        # h_name, service_desc are , separated\n        elts = pattern.split(',')\n        host_name = elts[0].strip()\n        # If host_name is empty, use the host_name the business rule is bound to\n        if not host_name:\n            host_name = self.bound_item.host_name\n        # Look if we have a service\n        if len(elts) > 1:\n            is_service = True\n            service_description = elts[1].strip()\n        if is_service:\n            obj = services.find_srv_by_name_and_hostname(host_name, service_description)\n            if not obj:\n                error = \"Business rule uses unknown service %s/%s\"\\\n                        % (host_name, service_description)\n        else:\n            obj = hosts.find_by_name(host_name)\n            if not obj:\n                error = \"Business rule uses unknown host %s\" % (host_name,)\n        return obj, error"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpand a pattern into a dependency node tree using regex or labels as item selector.", "response": "def expand_expression(self, pattern, hosts, services, hostgroups, servicegroups, running=False):\n        # pylint: disable=too-many-locals\n        \"\"\"Expand a host or service expression into a dependency node tree\n        using (host|service)group membership, regex, or labels as item selector.\n\n        :param pattern: pattern to parse\n        :type pattern: str\n        :param hosts: hosts list, used to find a specific host\n        :type hosts: alignak.objects.host.Host\n        :param services: services list, used to find a specific service\n        :type services: alignak.objects.service.Service\n        :param running: rules are evaluated at run time and parsing. True means runtime\n        :type running: bool\n        :return: root node of parsed tree\n        :rtype: alignak.dependencynode.DependencyNode\n        \"\"\"\n        error = None\n        node = DependencyNode()\n        node.operand = '&'\n        elts = [e.strip() for e in pattern.split(',')]\n        # If host_name is empty, use the host_name the business rule is bound to\n        if not elts[0]:\n            elts[0] = self.bound_item.host_name\n        filters = []\n        # Looks for hosts/services using appropriate filters\n        try:\n            all_items = {\n                \"hosts\": hosts,\n                \"hostgroups\": hostgroups,\n                \"servicegroups\": servicegroups\n            }\n            if len(elts) > 1:\n                # We got a service expression\n                host_expr, service_expr = elts\n                filters.extend(self.get_srv_host_filters(host_expr))\n                filters.extend(self.get_srv_service_filters(service_expr))\n                items = services.find_by_filter(filters, all_items)\n            else:\n                # We got a host expression\n                host_expr = elts[0]\n                filters.extend(self.get_host_filters(host_expr))\n                items = hosts.find_by_filter(filters, all_items)\n        except re.error as regerr:\n            error = \"Business rule uses invalid regex %s: %s\" % (pattern, regerr)\n        else:\n            if not items:\n                error = \"Business rule got an empty result for pattern %s\" % pattern\n\n        # Checks if we got result\n        if error:\n            if running is False:\n                node.configuration_errors.append(error)\n            else:\n                # As business rules are re-evaluated at run time on\n                # each scheduling loop, if the rule becomes invalid\n                # because of a badly written macro modulation, it\n                # should be notified upper for the error to be\n                # displayed in the check output.\n                raise Exception(error)\n            return node\n\n        # Creates dependency node subtree\n        # here we have Alignak SchedulingItem object (Host/Service)\n        for item in items:\n            # Creates a host/service node\n            son = DependencyNode()\n            son.operand = item.__class__.my_type\n            son.sons.append(item.uuid)  # Only store the uuid, not the full object.\n            # Appends it to wrapping node\n            node.sons.append(son)\n\n        node.switch_zeros_of_values()\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_host_filters(self, expr):\n        # pylint: disable=too-many-return-statements\n        \"\"\"Generates host filter list corresponding to the expression ::\n\n        * '*' => any\n        * 'g' => group filter\n        * 'r' => regex name filter\n        * 'l' => bp rule label filter\n        * 't' => tag  filter\n        * '' => none filter\n        * No flag match => host name filter\n\n        :param expr: expression to parse\n        :type expr: str\n        :return: filter list\n        :rtype: list\n        \"\"\"\n        if expr == \"*\":\n            return [filter_any]\n\n        match = re.search(r\"^([%s]+):(.*)\" % self.host_flags, expr)\n        if match is None:\n            return [filter_host_by_name(expr)]\n\n        flags, expr = match.groups()\n        if \"g\" in flags:\n            return [filter_host_by_group(expr)]\n        if \"r\" in flags:\n            return [filter_host_by_regex(expr)]\n        if \"l\" in flags:\n            return [filter_host_by_bp_rule_label(expr)]\n        if \"t\" in flags:\n            return [filter_host_by_tag(expr)]\n\n        return [filter_none]", "response": "Generates a list of host filters corresponding to the expression expr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_srv_host_filters(self, expr):\n        # pylint: disable=too-many-return-statements\n        \"\"\"Generates service filter list corresponding to the expression ::\n\n        * '*' => any\n        * 'g' => hostgroup filter\n        * 'r' => host regex name filter\n        * 'l' => host bp rule label filter\n        * 't' => tag  filter\n        * '' => none filter\n        * No flag match => host name filter\n\n        :param expr: expression to parse\n        :type expr: str\n        :return: filter list\n        :rtype: list\n        \"\"\"\n        if expr == \"*\":\n            return [filter_any]\n\n        match = re.search(r\"^([%s]+):(.*)\" % self.host_flags, expr)\n        if match is None:\n            return [filter_service_by_host_name(expr)]\n\n        flags, expr = match.groups()\n        if \"g\" in flags:\n            return [filter_service_by_hostgroup_name(expr)]\n        if \"r\" in flags:\n            return [filter_service_by_regex_host_name(expr)]\n        if \"l\" in flags:\n            return [filter_service_by_host_bp_rule_label(expr)]\n        if \"t\" in flags:\n            return [filter_service_by_host_tag_name(expr)]\n\n        return [filter_none]", "response": "Generates a list of service filters corresponding to the expression expr"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a list of service filters corresponding to the expression expr", "response": "def get_srv_service_filters(self, expr):\n        \"\"\"Generates service filter list corresponding to the expression ::\n\n        * '*' => any\n        * 'g' => servicegroup filter\n        * 'r' => service regex name filter\n        * 'l' => service bp rule label filter\n        * 't' => tag  filter\n        * '' => none filter\n        * No flag match => service name filter\n\n        :param expr: expression to parse\n        :type expr: str\n        :return: filter list\n        :rtype: list\n        \"\"\"\n        if expr == \"*\":\n            return [filter_any]\n\n        match = re.search(r\"^([%s]+):(.*)\" % self.service_flags, expr)\n        if match is None:\n            return [filter_service_by_name(expr)]\n\n        flags, expr = match.groups()\n        if \"g\" in flags:\n            return [filter_service_by_servicegroup_name(expr)]\n        if \"r\" in flags:\n            return [filter_service_by_regex_name(expr)]\n        if \"l\" in flags:\n            return [filter_service_by_bp_rule_label(expr)]\n\n        return [filter_none]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_raw_import_values(self):  # pragma: no cover, deprecation\n        properties = ['timeperiod_name', 'alias', 'use', 'register']\n        res = {}\n        for prop in properties:\n            if hasattr(self, prop):\n                val = getattr(self, prop)\n                res[prop] = val\n        # Now the unresolved one. The only way to get ride of same key things is to put\n        # directly the full value as the key\n        for other in self.unresolved:\n            res[other] = ''\n        return res", "response": "Get some properties of timeperiod and other properties of classic item"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_time_valid(self, timestamp):\n        if hasattr(self, 'exclude'):\n            for daterange in self.exclude:\n                if daterange.is_time_valid(timestamp):\n                    return False\n        for daterange in self.dateranges:\n            if daterange.is_time_valid(timestamp):\n                return True\n        return False", "response": "Check if a time is valid or not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the first time > timestamp which is valid", "response": "def get_min_from_t(self, timestamp):\n        \"\"\"\n        Get the first time > timestamp which is valid\n\n        :param timestamp: number of seconds\n        :type timestamp: int\n        :return: number of seconds\n        :rtype: int\n        TODO: not used, so delete it\n        \"\"\"\n        mins_incl = []\n        for daterange in self.dateranges:\n            mins_incl.append(daterange.get_min_from_t(timestamp))\n        return min(mins_incl)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the timeperiod is active and log it if it is not.", "response": "def check_and_log_activation_change(self):\n        \"\"\"\n        Will look for active/un-active change of timeperiod.\n        In case it change, we log it like:\n        [1327392000] TIMEPERIOD TRANSITION: <name>;<from>;<to>\n\n        States of is_active:\n        -1: default value when start\n        0: when timeperiod end\n        1: when timeperiod start\n\n        :return: None or a brok if TP changed\n        \"\"\"\n        now = int(time.time())\n\n        was_active = self.is_active\n        self.is_active = self.is_time_valid(now)\n\n        # If we got a change, log it!\n        if self.is_active != was_active:\n            _from = 0\n            _to = 0\n            # If it's the start, get a special value for was\n            if not self.activated_once:\n                _from = -1\n                self.activated_once = True\n            if was_active:\n                _from = 1\n            if self.is_active:\n                _to = 1\n\n            # Now raise the log\n            brok = make_monitoring_log(\n                'info', 'TIMEPERIOD TRANSITION: %s;%d;%d' % (self.get_name(), _from, _to)\n            )\n            return brok\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clean_cache(self):\n        now = int(time.time())\n        t_to_del = []\n        for timestamp in self.cache:\n            if timestamp < now:\n                t_to_del.append(timestamp)\n        for timestamp in t_to_del:\n            del self.cache[timestamp]\n\n        # same for the invalid cache\n        t_to_del = []\n        for timestamp in self.invalid_cache:\n            if timestamp < now:\n                t_to_del.append(timestamp)\n        for timestamp in t_to_del:\n            del self.invalid_cache[timestamp]", "response": "Clean cache with entries older than now because not used in future ; )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_next_valid_time_from_t(self, timestamp):\n        # pylint: disable=too-many-branches\n        \"\"\"\n        Get next valid time. If it's in cache, get it, otherwise define it.\n        The limit to find it is 1 year.\n\n        :param timestamp: number of seconds\n        :type timestamp: int or float\n        :return: Nothing or time in seconds\n        :rtype: None or int\n        \"\"\"\n        timestamp = int(timestamp)\n        original_t = timestamp\n\n        res_from_cache = self.find_next_valid_time_from_cache(timestamp)\n        if res_from_cache is not None:\n            return res_from_cache\n\n        still_loop = True\n\n        # Loop for all minutes...\n        while still_loop:\n            local_min = None\n\n            # Ok, not in cache...\n            dr_mins = []\n\n            for daterange in self.dateranges:\n                dr_mins.append(daterange.get_next_valid_time_from_t(timestamp))\n\n            s_dr_mins = sorted([d for d in dr_mins if d is not None])\n\n            for t01 in s_dr_mins:\n                if not self.exclude and still_loop:\n                    # No Exclude so we are good\n                    local_min = t01\n                    still_loop = False\n                else:\n                    for timeperiod in self.exclude:\n                        if not timeperiod.is_time_valid(t01) and still_loop:\n                            # OK we found a date that is not valid in any exclude timeperiod\n                            local_min = t01\n                            still_loop = False\n\n            if local_min is None:\n                # Looking for next invalid date\n                exc_mins = []\n                if s_dr_mins != []:\n                    for timeperiod in self.exclude:\n                        exc_mins.append(timeperiod.get_next_invalid_time_from_t(s_dr_mins[0]))\n\n                s_exc_mins = sorted([d for d in exc_mins if d is not None])\n\n                if s_exc_mins != []:\n                    local_min = s_exc_mins[0]\n\n            if local_min is None:\n                still_loop = False\n            else:\n                timestamp = local_min\n                # No loop more than one year\n                if timestamp > original_t + 3600 * 24 * 366 + 1:\n                    still_loop = False\n                    local_min = None\n\n        # Ok, we update the cache...\n        self.cache[original_t] = local_min\n        return local_min", "response": "Get next valid time from a given timestamp. If it s in cache return it otherwise define it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_next_invalid_time_from_t(self, timestamp):\n        # pylint: disable=too-many-branches\n        \"\"\"\n        Get the next invalid time\n\n        :param timestamp: timestamp in seconds (of course)\n        :type timestamp: int or float\n        :return: timestamp of next invalid time\n        :rtype: int or float\n        \"\"\"\n        timestamp = int(timestamp)\n        original_t = timestamp\n\n        dr_mins = []\n        for daterange in self.dateranges:\n            timestamp = original_t\n            cont = True\n            while cont:\n                start = daterange.get_next_valid_time_from_t(timestamp)\n                if start is not None:\n                    end = daterange.get_next_invalid_time_from_t(start)\n                    dr_mins.append((start, end))\n                    timestamp = end\n                else:\n                    cont = False\n                if timestamp > original_t + (3600 * 24 * 365):\n                    cont = False\n        periods = merge_periods(dr_mins)\n\n        # manage exclude periods\n        dr_mins = []\n        for exclude in self.exclude:\n            for daterange in exclude.dateranges:\n                timestamp = original_t\n                cont = True\n                while cont:\n                    start = daterange.get_next_valid_time_from_t(timestamp)\n                    if start is not None:\n                        end = daterange.get_next_invalid_time_from_t(start)\n                        dr_mins.append((start, end))\n                        timestamp = end\n                    else:\n                        cont = False\n                    if timestamp > original_t + (3600 * 24 * 365):\n                        cont = False\n        if not dr_mins:\n            periods_exclude = []\n        else:\n            periods_exclude = merge_periods(dr_mins)\n\n        if len(periods) >= 1:\n            # if first valid period is after original timestamp, the first invalid time\n            # is the original timestamp\n            if periods[0][0] > original_t:\n                return original_t\n            # check the first period + first period of exclude\n            if len(periods_exclude) >= 1:\n                if periods_exclude[0][0] < periods[0][1]:\n                    return periods_exclude[0][0]\n            return periods[0][1]\n        return original_t", "response": "Get the next invalid time from a given timestamp in seconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_correct(self):\n        state = True\n        for daterange in self.dateranges:\n            good = daterange.is_correct()\n            if not good:\n                self.add_error(\"[timeperiod::%s] invalid daterange '%s'\"\n                               % (self.get_name(), daterange))\n            state &= good\n\n        # Warn about non correct entries\n        for entry in self.invalid_entries:\n            self.add_error(\"[timeperiod::%s] invalid entry '%s'\" % (self.get_name(), entry))\n\n        return super(Timeperiod, self).is_correct() and state", "response": "Check if this object configuration is correct."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to find a daterange in a timeperiod entry.", "response": "def resolve_daterange(self, dateranges, entry):\n        # pylint: disable=too-many-return-statements,too-many-statements,\n        # pylint: disable=too-many-branches,too-many-locals\n        \"\"\"\n        Try to solve dateranges (special cases)\n\n        :param dateranges: dateranges\n        :type dateranges: list\n        :param entry: property of timeperiod\n        :type entry: string\n        :return: None\n        \"\"\"\n        res = re.search(\n            r'(\\d{4})-(\\d{2})-(\\d{2}) - (\\d{4})-(\\d{2})-(\\d{2}) / (\\d+)[\\s\\t]*([0-9:, -]+)', entry\n        )\n        if res is not None:\n            (syear, smon, smday, eyear, emon, emday, skip_interval, other) = res.groups()\n            data = {'syear': syear, 'smon': smon, 'smday': smday, 'swday': 0,\n                    'swday_offset': 0, 'eyear': eyear, 'emon': emon, 'emday': emday,\n                    'ewday': 0, 'ewday_offset': 0, 'skip_interval': skip_interval,\n                    'other': other}\n            dateranges.append(CalendarDaterange(data))\n            return\n\n        res = re.search(r'(\\d{4})-(\\d{2})-(\\d{2}) / (\\d+)[\\s\\t]*([0-9:, -]+)', entry)\n        if res is not None:\n            (syear, smon, smday, skip_interval, other) = res.groups()\n            eyear = syear\n            emon = smon\n            emday = smday\n            data = {'syear': syear, 'smon': smon, 'smday': smday, 'swday': 0,\n                    'swday_offset': 0, 'eyear': eyear, 'emon': emon, 'emday': emday,\n                    'ewday': 0, 'ewday_offset': 0, 'skip_interval': skip_interval,\n                    'other': other}\n            dateranges.append(CalendarDaterange(data))\n            return\n\n        res = re.search(\n            r'(\\d{4})-(\\d{2})-(\\d{2}) - (\\d{4})-(\\d{2})-(\\d{2})[\\s\\t]*([0-9:, -]+)', entry\n        )\n        if res is not None:\n            (syear, smon, smday, eyear, emon, emday, other) = res.groups()\n            data = {'syear': syear, 'smon': smon, 'smday': smday, 'swday': 0,\n                    'swday_offset': 0, 'eyear': eyear, 'emon': emon, 'emday': emday,\n                    'ewday': 0, 'ewday_offset': 0, 'skip_interval': 0,\n                    'other': other}\n            dateranges.append(CalendarDaterange(data))\n            return\n\n        res = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})[\\s\\t]*([0-9:, -]+)', entry)\n        if res is not None:\n            (syear, smon, smday, other) = res.groups()\n            eyear = syear\n            emon = smon\n            emday = smday\n            data = {'syear': syear, 'smon': smon, 'smday': smday, 'swday': 0,\n                    'swday_offset': 0, 'eyear': eyear, 'emon': emon, 'emday': emday,\n                    'ewday': 0, 'ewday_offset': 0, 'skip_interval': 0,\n                    'other': other}\n            dateranges.append(CalendarDaterange(data))\n            return\n\n        res = re.search(\n            r'([a-z]*) ([\\d-]+) ([a-z]*) - ([a-z]*) ([\\d-]+) ([a-z]*) / (\\d+)[\\s\\t]*([0-9:, -]+)',\n            entry\n        )\n        if res is not None:\n            (swday, swday_offset, smon, ewday,\n             ewday_offset, emon, skip_interval, other) = res.groups()\n            smon_id = Daterange.get_month_id(smon)\n            emon_id = Daterange.get_month_id(emon)\n            swday_id = Daterange.get_weekday_id(swday)\n            ewday_id = Daterange.get_weekday_id(ewday)\n            data = {'syear': 0, 'smon': smon_id, 'smday': 0, 'swday': swday_id,\n                    'swday_offset': swday_offset, 'eyear': 0, 'emon': emon_id, 'emday': 0,\n                    'ewday': ewday_id, 'ewday_offset': ewday_offset, 'skip_interval': skip_interval,\n                    'other': other}\n            dateranges.append(MonthWeekDayDaterange(data))\n            return\n\n        res = re.search(r'([a-z]*) ([\\d-]+) - ([a-z]*) ([\\d-]+) / (\\d+)[\\s\\t]*([0-9:, -]+)', entry)\n        if res is not None:\n            (t00, smday, t01, emday, skip_interval, other) = res.groups()\n            if t00 in Daterange.weekdays and t01 in Daterange.weekdays:\n                swday = Daterange.get_weekday_id(t00)\n                ewday = Daterange.get_weekday_id(t01)\n                swday_offset = smday\n                ewday_offset = emday\n                data = {'syear': 0, 'smon': 0, 'smday': 0, 'swday': swday,\n                        'swday_offset': swday_offset, 'eyear': 0, 'emon': 0, 'emday': 0,\n                        'ewday': ewday, 'ewday_offset': ewday_offset,\n                        'skip_interval': skip_interval, 'other': other}\n                dateranges.append(WeekDayDaterange(data))\n                return\n\n            if t00 in Daterange.months and t01 in Daterange.months:\n                smon = Daterange.get_month_id(t00)\n                emon = Daterange.get_month_id(t01)\n                data = {'syear': 0, 'smon': smon, 'smday': smday, 'swday': 0, 'swday_offset': 0,\n                        'eyear': 0, 'emon': emon, 'emday': emday, 'ewday': 0, 'ewday_offset': 0,\n                        'skip_interval': skip_interval, 'other': other}\n                dateranges.append(MonthDateDaterange(data))\n                return\n\n            if t00 == 'day' and t01 == 'day':\n                data = {'syear': 0, 'smon': 0, 'smday': smday, 'swday': 0, 'swday_offset': 0,\n                        'eyear': 0, 'emon': 0, 'emday': emday, 'ewday': 0, 'ewday_offset': 0,\n                        'skip_interval': skip_interval, 'other': other}\n                dateranges.append(MonthDayDaterange(data))\n                return\n\n        res = re.search(r'([a-z]*) ([\\d-]+) - ([\\d-]+) / (\\d+)[\\s\\t]*([0-9:, -]+)', entry)\n        if res is not None:\n            (t00, smday, emday, skip_interval, other) = res.groups()\n            if t00 in Daterange.weekdays:\n                swday = Daterange.get_weekday_id(t00)\n                swday_offset = smday\n                ewday = swday\n                ewday_offset = emday\n                data = {'syear': 0, 'smon': 0, 'smday': 0, 'swday': swday,\n                        'swday_offset': swday_offset, 'eyear': 0, 'emon': 0, 'emday': 0,\n                        'ewday': ewday, 'ewday_offset': ewday_offset,\n                        'skip_interval': skip_interval, 'other': other}\n                dateranges.append(WeekDayDaterange(data))\n                return\n\n            if t00 in Daterange.months:\n                smon = Daterange.get_month_id(t00)\n                emon = smon\n                data = {'syear': 0, 'smon': smon, 'smday': smday, 'swday': 0, 'swday_offset': 0,\n                        'eyear': 0, 'emon': emon, 'emday': emday, 'ewday': 0, 'ewday_offset': 0,\n                        'skip_interval': skip_interval, 'other': other}\n                dateranges.append(MonthDateDaterange(data))\n                return\n\n            if t00 == 'day':\n                data = {'syear': 0, 'smon': 0, 'smday': smday, 'swday': 0, 'swday_offset': 0,\n                        'eyear': 0, 'emon': 0, 'emday': emday, 'ewday': 0, 'ewday_offset': 0,\n                        'skip_interval': skip_interval, 'other': other}\n                dateranges.append(MonthDayDaterange(data))\n                return\n\n        res = re.search(\n            r'([a-z]*) ([\\d-]+) ([a-z]*) - ([a-z]*) ([\\d-]+) ([a-z]*) [\\s\\t]*([0-9:, -]+)', entry\n        )\n        if res is not None:\n            (swday, swday_offset, smon, ewday, ewday_offset, emon, other) = res.groups()\n            smon_id = Daterange.get_month_id(smon)\n            emon_id = Daterange.get_month_id(emon)\n            swday_id = Daterange.get_weekday_id(swday)\n            ewday_id = Daterange.get_weekday_id(ewday)\n            data = {'syear': 0, 'smon': smon_id, 'smday': 0, 'swday': swday_id,\n                    'swday_offset': swday_offset, 'eyear': 0, 'emon': emon_id, 'emday': 0,\n                    'ewday': ewday_id, 'ewday_offset': ewday_offset, 'skip_interval': 0,\n                    'other': other}\n            dateranges.append(MonthWeekDayDaterange(data))\n            return\n\n        res = re.search(r'([a-z]*) ([\\d-]+) - ([\\d-]+)[\\s\\t]*([0-9:, -]+)', entry)\n        if res is not None:\n            (t00, smday, emday, other) = res.groups()\n            if t00 in Daterange.weekdays:\n                swday = Daterange.get_weekday_id(t00)\n                swday_offset = smday\n                ewday = swday\n                ewday_offset = emday\n                data = {'syear': 0, 'smon': 0, 'smday': 0, 'swday': swday,\n                        'swday_offset': swday_offset, 'eyear': 0, 'emon': 0, 'emday': 0,\n                        'ewday': ewday, 'ewday_offset': ewday_offset, 'skip_interval': 0,\n                        'other': other}\n                dateranges.append(WeekDayDaterange(data))\n                return\n\n            if t00 in Daterange.months:\n                smon = Daterange.get_month_id(t00)\n                emon = smon\n                data = {'syear': 0, 'smon': smon, 'smday': smday, 'swday': 0,\n                        'swday_offset': 0, 'eyear': 0, 'emon': emon, 'emday': emday,\n                        'ewday': 0, 'ewday_offset': 0, 'skip_interval': 0,\n                        'other': other}\n                dateranges.append(MonthDateDaterange(data))\n                return\n\n            if t00 == 'day':\n                data = {'syear': 0, 'smon': 0, 'smday': smday, 'swday': 0,\n                        'swday_offset': 0, 'eyear': 0, 'emon': 0, 'emday': emday,\n                        'ewday': 0, 'ewday_offset': 0, 'skip_interval': 0,\n                        'other': other}\n                dateranges.append(MonthDayDaterange(data))\n                return\n\n        res = re.search(r'([a-z]*) ([\\d-]+) - ([a-z]*) ([\\d-]+)[\\s\\t]*([0-9:, -]+)', entry)\n        if res is not None:\n            (t00, smday, t01, emday, other) = res.groups()\n            if t00 in Daterange.weekdays and t01 in Daterange.weekdays:\n                swday = Daterange.get_weekday_id(t00)\n                ewday = Daterange.get_weekday_id(t01)\n                swday_offset = smday\n                ewday_offset = emday\n                data = {'syear': 0, 'smon': 0, 'smday': 0, 'swday': swday,\n                        'swday_offset': swday_offset, 'eyear': 0, 'emon': 0, 'emday': 0,\n                        'ewday': ewday, 'ewday_offset': ewday_offset, 'skip_interval': 0,\n                        'other': other}\n                dateranges.append(WeekDayDaterange(data))\n                return\n\n            if t00 in Daterange.months and t01 in Daterange.months:\n                smon = Daterange.get_month_id(t00)\n                emon = Daterange.get_month_id(t01)\n                data = {'syear': 0, 'smon': smon, 'smday': smday, 'swday': 0,\n                        'swday_offset': 0, 'eyear': 0, 'emon': emon, 'emday': emday,\n                        'ewday': 0, 'ewday_offset': 0, 'skip_interval': 0,\n                        'other': other}\n                dateranges.append(MonthDateDaterange(data))\n                return\n\n            if t00 == 'day' and t01 == 'day':\n                data = {'syear': 0, 'smon': 0, 'smday': smday, 'swday': 0,\n                        'swday_offset': 0, 'eyear': 0, 'emon': 0, 'emday': emday,\n                        'ewday': 0, 'ewday_offset': 0, 'skip_interval': 0,\n                        'other': other}\n                dateranges.append(MonthDayDaterange(data))\n                return\n\n        res = re.search(r'([a-z]*) ([\\d-]+) ([a-z]*)[\\s\\t]*([0-9:, -]+)', entry)\n        if res is not None:\n            (t00, t02, t01, other) = res.groups()\n            if t00 in Daterange.weekdays and t01 in Daterange.months:\n                swday = Daterange.get_weekday_id(t00)\n                smon = Daterange.get_month_id(t01)\n                emon = smon\n                ewday = swday\n                ewday_offset = t02\n                data = {'syear': 0, 'smon': smon, 'smday': 0, 'swday': swday,\n                        'swday_offset': t02, 'eyear': 0, 'emon': emon, 'emday': 0,\n                        'ewday': ewday, 'ewday_offset': ewday_offset, 'skip_interval': 0,\n                        'other': other}\n                dateranges.append(MonthWeekDayDaterange(data))\n                return\n            if not t01:\n                if t00 in Daterange.weekdays:\n                    swday = Daterange.get_weekday_id(t00)\n                    swday_offset = t02\n                    ewday = swday\n                    ewday_offset = swday_offset\n                    data = {'syear': 0, 'smon': 0, 'smday': 0, 'swday': swday,\n                            'swday_offset': swday_offset, 'eyear': 0, 'emon': 0, 'emday': 0,\n                            'ewday': ewday, 'ewday_offset': ewday_offset, 'skip_interval': 0,\n                            'other': other}\n                    dateranges.append(WeekDayDaterange(data))\n                    return\n                if t00 in Daterange.months:\n                    smon = Daterange.get_month_id(t00)\n                    emon = smon\n                    emday = t02\n                    data = {'syear': 0, 'smon': smon, 'smday': t02, 'swday': 0,\n                            'swday_offset': 0, 'eyear': 0, 'emon': emon, 'emday': emday,\n                            'ewday': 0, 'ewday_offset': 0, 'skip_interval': 0,\n                            'other': other}\n                    dateranges.append(MonthDateDaterange(data))\n                    return\n                if t00 == 'day':\n                    emday = t02\n                    data = {'syear': 0, 'smon': 0, 'smday': t02, 'swday': 0,\n                            'swday_offset': 0, 'eyear': 0, 'emon': 0, 'emday': emday,\n                            'ewday': 0, 'ewday_offset': 0, 'skip_interval': 0,\n                            'other': other}\n                    dateranges.append(MonthDayDaterange(data))\n                    return\n\n        res = re.search(r'([a-z]*)[\\s\\t]+([0-9:, -]+)', entry)\n        if res is not None:\n            (t00, other) = res.groups()\n            if t00 in Daterange.weekdays:\n                day = t00\n                data = {'day': day, 'other': other}\n                dateranges.append(StandardDaterange(data))\n                return\n        logger.info(\"[timeentry::%s] no match for %s\", self.get_name(), entry)\n        self.invalid_entries.append(entry)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to resolve all unresolved elements", "response": "def explode(self):\n        \"\"\"\n        Try to resolve all unresolved elements\n\n        :return: None\n        \"\"\"\n        for entry in self.unresolved:\n            self.resolve_daterange(self.dateranges, entry)\n        self.unresolved = []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef linkify(self, timeperiods):\n        new_exclude = []\n        if hasattr(self, 'exclude') and self.exclude != []:\n            logger.debug(\"[timeentry::%s] have excluded %s\", self.get_name(), self.exclude)\n            excluded_tps = self.exclude\n            for tp_name in excluded_tps:\n                timepriod = timeperiods.find_by_name(tp_name.strip())\n                if timepriod is not None:\n                    new_exclude.append(timepriod.uuid)\n                else:\n                    msg = \"[timeentry::%s] unknown %s timeperiod\" % (self.get_name(), tp_name)\n                    self.add_error(msg)\n        self.exclude = new_exclude", "response": "Link the timeperiods to the timeperiods that are not in the exclude list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if this timeperiod is tagged and if not return False", "response": "def check_exclude_rec(self):\n        # pylint: disable=access-member-before-definition\n        \"\"\"\n        Check if this timeperiod is tagged\n\n        :return: if tagged return false, if not true\n        :rtype: bool\n        \"\"\"\n        if self.rec_tag:\n            msg = \"[timeentry::%s] is in a loop in exclude parameter\" % (self.get_name())\n            self.add_error(msg)\n            return False\n        self.rec_tag = True\n        for timeperiod in self.exclude:\n            timeperiod.check_exclude_rec()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef explode(self):\n        for t_id in self.items:\n            timeperiod = self.items[t_id]\n            timeperiod.explode()", "response": "Try to resolve each timeperiod\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlink the items in this object to the timeperiods in this object.", "response": "def linkify(self):\n        \"\"\"\n        Check exclusion for each timeperiod\n\n        :return: None\n        \"\"\"\n        for t_id in self.items:\n            timeperiod = self.items[t_id]\n            timeperiod.linkify(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfills full properties with template if needed for the unresolved values", "response": "def get_unresolved_properties_by_inheritance(self, timeperiod):\n        \"\"\"\n        Fill full properties with template if needed for the\n        unresolved values (example: sunday ETCETC)\n        :return: None\n        \"\"\"\n        # Ok, I do not have prop, Maybe my templates do?\n        # Same story for plus\n        for i in timeperiod.templates:\n            template = self.templates[i]\n            timeperiod.unresolved.extend(template.unresolved)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies inheritance to all custom properties and unresolved properties.", "response": "def apply_inheritance(self):\n        \"\"\"\n        The only interesting property to inherit is exclude\n\n        :return: None\n        \"\"\"\n        self.apply_partial_inheritance('exclude')\n        for i in self:\n            self.get_customs_properties_by_inheritance(i)\n\n        # And now apply inheritance for unresolved properties\n        # like the dateranges in fact\n        for timeperiod in self:\n            self.get_unresolved_properties_by_inheritance(timeperiod)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if each properties of timeperiods are valid", "response": "def is_correct(self):\n        \"\"\"\n        check if each properties of timeperiods are valid\n\n        :return: True if is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        valid = True\n        # We do not want a same hg to be explode again and again\n        # so we tag it\n        for timeperiod in list(self.items.values()):\n            timeperiod.rec_tag = False\n\n        for timeperiod in list(self.items.values()):\n            for tmp_tp in list(self.items.values()):\n                tmp_tp.rec_tag = False\n            valid = timeperiod.check_exclude_rec() and valid\n\n        # We clean the tags and collect the warning/erro messages\n        for timeperiod in list(self.items.values()):\n            del timeperiod.rec_tag\n\n            # Now other checks\n            if not timeperiod.is_correct():\n                valid = False\n                source = getattr(timeperiod, 'imported_from', \"unknown source\")\n                msg = \"Configuration in %s::%s is incorrect; from: %s\" % (\n                    timeperiod.my_type, timeperiod.get_name(), source\n                )\n                self.add_error(msg)\n\n            self.configuration_errors += timeperiod.configuration_errors\n            self.configuration_warnings += timeperiod.configuration_warnings\n\n        # And check all timeperiods for correct (sunday is false)\n        for timeperiod in self:\n            valid = timeperiod.is_correct() and valid\n\n        return valid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_override_configuration(self):\n        res = {}\n        properties = self.__class__.properties\n        for prop, entry in list(properties.items()):\n            if entry.override:\n                res[prop] = getattr(self, prop)\n        return res", "response": "Returns a dictionary of properties that can be overridden by the scheduler"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking all daemons state and get the global result.", "response": "def check_reachable(self, forced=False, test=False):\n        # pylint: disable=too-many-branches\n        \"\"\"Check all daemons state (reachable or not)\n\n        If test parameter is True, do not really send but simulate only for testing purpose...\n\n        The update_infos function returns None when no ping has been executed\n        (too early...), or True / False according to the real ping and get managed\n        configuration result. So, if the result is None, consider as not valid,\n        else compute the global result...\n\n        :return: True if all daemons are reachable\n        \"\"\"\n        all_ok = True\n        self.not_configured = []\n        for daemon_link in self.all_daemons_links:\n            if daemon_link == self.arbiter_link:\n                # I exclude myself from the polling, sure I am reachable ;)\n                continue\n\n            if not daemon_link.active:\n                # I exclude the daemons that are not active\n                continue\n\n            # ----------\n            if test:\n                # print(\"*** unit tests - setting %s as alive\" % daemon_link.name)\n                # Set the satellite as alive\n                daemon_link.set_alive()\n                daemon_link.running_id = time.time()\n                # daemon_link.cfg_managed = {}\n                # continue\n            # ----------\n            # Force the daemon communication only if a configuration is prepared\n            result = False\n            try:\n                result = daemon_link.update_infos(forced=(forced or self.new_to_dispatch),\n                                                  test=test)\n            except LinkError:\n                logger.warning(\"Daemon connection failed, I could not get fresh information.\")\n\n            if result is not False:\n                if result is None:\n                    # Come back later ... too recent daemon connection!\n                    continue\n\n                if result:\n                    # Got a managed configuration\n                    logger.debug(\"The %s '%s' manages %s\",\n                                 daemon_link.type, daemon_link.name, daemon_link.cfg_managed)\n                    if not self.first_dispatch_done:\n                        # I just (re)started the arbiter\n                        self.not_configured.append(daemon_link)\n                else:\n                    # No managed configuration - a new dispatching is necessary but only\n                    # if we already dispatched a configuration\n                    # Probably a freshly restarted daemon ;)\n                    logger.debug(\"The %s %s do not have a configuration\",\n                                 daemon_link.type, daemon_link.name)\n                    # the daemon is not yet configured\n                    self.not_configured.append(daemon_link)\n                    daemon_link.configuration_sent = False\n            else:\n                # Got a timeout !\n                self.not_configured.append(daemon_link)\n\n        if self.not_configured and self.new_to_dispatch and not self.first_dispatch_done:\n            logger.info(\"Dispatcher, these daemons are not configured: %s, \"\n                        \"and a configuration is ready to dispatch, run the dispatching...\",\n                        ','.join(d.name for d in self.not_configured))\n            self.dispatch_ok = False\n            self.dispatch(test=test)\n\n        elif self.not_configured and self.first_dispatch_done:\n            logger.info(\"Dispatcher, these daemons are not configured: %s, \"\n                        \"and a configuration has yet been dispatched dispatch, \"\n                        \"a new dispatch is required...\",\n                        ','.join(d.name for d in self.not_configured))\n            self.dispatch_ok = False\n            # Avoid exception because dispatch is not accepted!\n            self.new_to_dispatch = True\n            self.first_dispatch_done = False\n            self.dispatch(test=test)\n\n        return all_ok"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets all the daemons status and events.", "response": "def check_status_and_get_events(self):\n        # pylint: disable=too-many-branches\n        \"\"\"Get all the daemons status\n\n\n        :return: Dictionary with all the daemons returned information\n        :rtype: dict\n        \"\"\"\n        statistics = {}\n        events = []\n        for daemon_link in self.all_daemons_links:\n            if daemon_link == self.arbiter_link:\n                # I exclude myself from the polling, sure I am reachable ;)\n                continue\n\n            if not daemon_link.active:\n                # I exclude the daemons that are not active\n                continue\n\n            try:\n                # Do not get the details to avoid overloading the communication\n                daemon_link.statistics = daemon_link.get_daemon_stats(details=False)\n                if daemon_link.statistics:\n                    daemon_link.statistics['_freshness'] = int(time.time())\n                    statistics[daemon_link.name] = daemon_link.statistics\n                    logger.debug(\"Daemon %s statistics: %s\",\n                                 daemon_link.name, daemon_link.statistics)\n            except LinkError:\n                logger.warning(\"Daemon connection failed, I could not get statistics.\")\n\n            try:\n                got = daemon_link.get_events()\n                if got:\n                    events.extend(got)\n                    logger.debug(\"Daemon %s has %d events: %s\", daemon_link.name, len(got), got)\n            except LinkError:\n                logger.warning(\"Daemon connection failed, I could not get events.\")\n\n        return events"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_dispatch(self):  # pylint: disable=too-many-branches\n        if not self.arbiter_link:\n            raise DispatcherError(\"Dispatcher configuration problem: no valid arbiter link!\")\n\n        if not self.first_dispatch_done:\n            raise DispatcherError(\"Dispatcher cannot check the dispatching, \"\n                                  \"because no configuration is dispatched!\")\n\n        # We check for configuration parts to be dispatched on alive schedulers.\n        # If not dispatched, we need a dispatch :) and if dispatched on a failed node,\n        # remove the association, and need a new dispatch\n        self.dispatch_ok = True\n        some_satellites_are_missing = False\n\n        # Get fresh information about the satellites\n        logger.info(\"Getting fresh information\")\n        self.check_reachable(forced=True)\n\n        logger.info(\"Checking realms dispatch:\")\n        for realm in self.alignak_conf.realms:\n            logger.info(\"- realm %s:\", realm.name)\n            for cfg_part in list(realm.parts.values()):\n                logger.info(\"  .configuration %s\", cfg_part)\n\n                # This should never happen, logically!\n                if not cfg_part.scheduler_link:\n                    self.dispatch_ok = False\n                    logger.error(\"- realm %s:\", realm.name)\n                    logger.error(\"  .configuration %s\", cfg_part)\n                    logger.error(\"    not managed by any scheduler!\")\n                    continue\n\n                logger.debug(\"    checking scheduler %s configuration: %s\",\n                             cfg_part.scheduler_link.name, cfg_part.instance_id)\n\n                # Maybe the scheduler restarts, so it is alive but without\n                # the expected configuration; set the configuration part as unmanaged\n                # and ask for a new configuration dispatch\n                if not cfg_part.scheduler_link.manages(cfg_part):\n                    # We ask for a new dispatching\n                    self.dispatch_ok = False\n                    if cfg_part.scheduler_link.cfg_managed is None:\n                        logger.warning(\"    %s not yet !.\",\n                                       cfg_part.scheduler_link.name)\n                    else:\n                        logger.warning(\"    the assigned scheduler %s does not manage the \"\n                                       \"configuration; asking for a new configuration dispatch.\",\n                                       cfg_part.scheduler_link.name)\n                    cfg_part.scheduler_link.cfg_to_manage = None\n                    cfg_part.scheduler_link.push_flavor = ''\n                    cfg_part.scheduler_link.hash = ''\n                    cfg_part.scheduler_link.need_conf = True\n\n                for sat_type in ('reactionner', 'poller', 'broker', 'receiver'):\n                    logger.debug(\"    checking %ss configuration\", sat_type)\n                    # We must have the correct number of satellites or we are not happy\n                    # So we are sure to raise a dispatch every loop a satellite is missing\n                    if (len(realm.to_satellites_managed_by[sat_type][cfg_part.instance_id]) <\n                            realm.get_nb_of_must_have_satellites(sat_type)):\n                        some_satellites_are_missing = True\n\n                        logger.warning(\"    missing %s satellites: %s / %s!\", sat_type,\n                                       realm.to_satellites_managed_by[sat_type][\n                                           cfg_part.instance_id],\n                                       realm.get_nb_of_must_have_satellites(sat_type))\n\n                        # TODO: less violent! Must only resend to the one needing?\n                        # must be caught by satellite who sees that\n                        # it already has the conf and do nothing\n                        self.dispatch_ok = False  # so we will redispatch all\n                        realm.to_satellites_need_dispatch[sat_type][cfg_part.instance_id] = True\n                        realm.to_satellites_managed_by[sat_type][cfg_part.instance_id] = []\n\n                    for satellite in realm.to_satellites_managed_by[sat_type][cfg_part.instance_id]:\n                        # Maybe the sat was marked as not alive, but still in\n                        # to_satellites_managed_by. That means that a new dispatch\n                        # is needed\n                        # Or maybe it is alive but I thought that this satellite\n                        # managed the conf and it doesn't.\n                        # I ask a full redispatch of these cfg for both cases\n\n                        if not satellite.reachable:\n                            logger.info(\"    the %s %s is not reachable; \"\n                                        \"assuming a correct configuration dispatch.\",\n                                        sat_type, satellite.name)\n                            continue\n                        # if not cfg_part.push_flavor:\n                        #     logger.warning(\"    the %s %s manages an unmanaged configuration; \"\n                        #                    \"asking for a new configuration dispatch.\",\n                        #                    sat_type, satellite.name)\n                        if not satellite.manages(cfg_part):\n                            logger.warning(\"    the %s %s does not manage \"\n                                           \"the correct configuration; \"\n                                           \"asking for a new configuration dispatch.\",\n                                           sat_type, satellite.name)\n                            self.dispatch_ok = False\n                            realm.to_satellites_need_dispatch[sat_type][cfg_part.instance_id] = True\n                            realm.to_satellites_managed_by[sat_type][cfg_part.instance_id] = []\n\n        if some_satellites_are_missing:\n            logger.warning(\"Some satellites are not available for the current configuration\")\n\n        return self.dispatch_ok", "response": "Check that all active satellites have a configuration dispatched."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_satellites_list(self, sat_type):\n        satellites_list = []\n        if sat_type in ['arbiters', 'schedulers', 'reactionners',\n                        'brokers', 'receivers', 'pollers']:\n            for satellite in getattr(self, sat_type):\n                satellites_list.append(satellite)\n            satellites_list = master_then_spare(satellites_list)\n\n        return satellites_list", "response": "Get a sorted satellite list for the given type of satellites"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_scheduler_ordered_list(self, realm):\n        # Get the schedulers for the required realm\n        scheduler_links = []\n        for scheduler_link_uuid in realm.schedulers:\n            scheduler_links.append(self.schedulers[scheduler_link_uuid])\n\n        # Now we sort the schedulers so we take alive, then spare, then dead,\n        alive = []\n        spare = []\n        deads = []\n        for sdata in scheduler_links:\n            if sdata.alive and not sdata.spare:\n                alive.append(sdata)\n            elif sdata.alive and sdata.spare:\n                spare.append(sdata)\n            else:\n                deads.append(sdata)\n        scheduler_links = []\n        scheduler_links.extend(alive)\n        scheduler_links.extend(spare)\n        scheduler_links.extend(deads)\n\n        scheduler_links.reverse()  # I need to pop the list, so reverse the list...\n        return scheduler_links", "response": "Get the ordered scheduler list for a specific realm."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_dispatch(self):\n        # pylint:disable=too-many-branches, too-many-statements, too-many-locals\n        \"\"\"\n        Prepare dispatch, so prepare for each daemon (schedulers, brokers, receivers, reactionners,\n        pollers)\n\n        This function will only prepare something if self.new_to_dispatch is False\n        It will reset the first_dispatch_done flag\n\n        A DispatcherError exception is raised if a configuration is already prepared! Unset the\n        new_to_dispatch flag before calling!\n\n        :return: None\n        \"\"\"\n        if self.new_to_dispatch:\n            raise DispatcherError(\"A configuration is already prepared!\")\n\n        # So we are preparing a new dispatching...\n        self.new_to_dispatch = True\n        self.first_dispatch_done = False\n\n        # Update Alignak name for all the satellites\n        for daemon_link in self.all_daemons_links:\n            daemon_link.cfg.update({'alignak_name': self.alignak_conf.alignak_name})\n\n        logger.info(\"Preparing realms dispatch:\")\n\n        # Prepare the arbiters configuration\n        master_arbiter_cfg = arbiters_cfg = {}\n        for arbiter_link in self.get_satellites_list('arbiters'):\n            # # If not me and not a spare arbiter...\n            # if arbiter_link == self.arbiter_link:\n            #     # I exclude myself from the dispatching, I have my configuration ;)\n            #     continue\n\n            if not arbiter_link.active:\n                # I exclude the daemons that are not active\n                continue\n\n            arbiter_cfg = arbiter_link.cfg\n            arbiter_cfg.update({\n                'managed_hosts_names': [h.get_name() for h in self.alignak_conf.hosts],\n                'modules': serialize(arbiter_link.modules, True),\n\n                'managed_conf_id': self.alignak_conf.instance_id,\n                'push_flavor': ''\n            })\n\n            # Hash the configuration\n            cfg_string = json.dumps(arbiter_cfg, sort_keys=True).encode('utf-8')\n            arbiter_cfg['hash'] = hashlib.sha1(cfg_string).hexdigest()\n\n            # Update the arbiters list, but do not include the whole conf\n            arbiters_cfg[arbiter_link.uuid] = arbiter_cfg['self_conf']\n\n            # Not for the master arbiter...\n            if arbiter_link != self.arbiter_link:\n                arbiter_cfg.update({\n                    'arbiters': master_arbiter_cfg,\n                    'whole_conf': self.alignak_conf.spare_arbiter_conf,\n                })\n\n                # Hash the whole configuration\n                try:\n                    s_conf_part = json.dumps(arbiter_cfg, sort_keys=True).encode('utf-8')\n                except UnicodeDecodeError:\n                    pass\n                arbiter_cfg['hash'] = hashlib.sha1(s_conf_part).hexdigest()\n\n            # Dump the configuration part size\n            pickled_conf = pickle.dumps(arbiter_cfg)\n            logger.info('   arbiter configuration size: %d bytes', sys.getsizeof(pickled_conf))\n\n            # The configuration is assigned to the arbiter\n            # todo: perhaps this should be done in the realms (like schedulers and satellites)?\n            arbiter_link.cfg = arbiter_cfg\n            arbiter_link.cfg_to_manage = self.alignak_conf\n            arbiter_link.push_flavor = arbiter_cfg['push_flavor']\n            arbiter_link.hash = arbiter_cfg['hash']\n            arbiter_link.need_conf = False\n            arbiter_link.configuration_sent = False\n\n            # If not me and not a spare arbiter...\n            if arbiter_link == self.arbiter_link:\n                # The master arbiter configuration for the other satellites\n                master_arbiter_cfg = {self.arbiter_link.uuid: arbiter_cfg['self_conf']}\n\n            logger.info('   arbiter configuration prepared for %s', arbiter_link.name)\n\n        # main_realm = self.alignak_conf.realms.find_by_name('All')\n        # all_realms = main_realm.all_sub_members\n        # for realm_uuid in all_realms:\n        #     realm = self.alignak_conf.realms[realm_uuid]\n        #     logger.info(\"- realm %s: %s\", realm_uuid, realm)\n\n        for realm in self.alignak_conf.realms:\n            logger.info(\"- realm %s: %d configuration part(s)\", realm.name, len(realm.parts))\n\n            # parts_to_dispatch is a list of configuration parts built when\n            # the configuration is split into parts for the realms and their schedulers\n            # Only get the parts that are not yet assigned to a scheduler\n            parts_to_dispatch = [cfg for cfg in list(realm.parts.values()) if not cfg.is_assigned]\n            if not parts_to_dispatch:\n                logger.info('  no configuration to dispatch for this realm!')\n                continue\n\n            logger.info(\" preparing the dispatch for schedulers:\")\n\n            # Now we get all the schedulers of this realm and upper\n            # schedulers = self.get_scheduler_ordered_list(realm)\n            schedulers = realm.get_potential_satellites_by_type(\n                self.get_satellites_list('schedulers'), 'scheduler')\n            if not schedulers:\n                logger.error('  no available schedulers in this realm (%s)!', realm)\n                continue\n            logger.info(\"  realm schedulers: %s\",\n                        ','.join([s.get_name() for s in schedulers]))\n\n            for cfg_part in parts_to_dispatch:\n                logger.info(\"  .assigning configuration part %s (%s), name:%s\",\n                            cfg_part.instance_id, cfg_part.uuid, cfg_part.config_name)\n\n                # we need to loop until the configuration part is assigned to a scheduler\n                # or no more scheduler is available\n                while True:\n                    try:\n                        scheduler_link = schedulers.pop()\n                    except IndexError:  # No more schedulers.. not good, no loop\n                        # The configuration part do not need to be dispatched anymore\n                        # todo: should be managed inside the Realm class!\n                        logger.error(\"No more scheduler link: %s\", realm)\n                        for sat_type in ('reactionner', 'poller', 'broker', 'receiver'):\n                            realm.to_satellites[sat_type][cfg_part.instance_id] = None\n                            realm.to_satellites_need_dispatch[sat_type][cfg_part.instance_id] = \\\n                                False\n                            realm.to_satellites_managed_by[sat_type][cfg_part.instance_id] = []\n                        break\n\n                    # if scheduler_link.manage_sub_realms:\n                    #     logger.warning('[%s] The scheduler %s is configured to manage sub realms.'\n                    #                    ' This is not yet possible, sorry!',\n                    #                    realm.name, scheduler_link.name)\n                    #     scheduler_link.manage_sub_realms = False\n                    #     continue\n\n                    if not scheduler_link.need_conf:\n                        logger.info('[%s] The scheduler %s do not need any configuration, sorry',\n                                    realm.name, scheduler_link.name)\n                        continue\n\n                    logger.debug(\"   preparing configuration part '%s' for the scheduler '%s'\",\n                                 cfg_part.instance_id, scheduler_link.name)\n                    logger.debug(\"   - %d hosts, %d services\",\n                                 len(cfg_part.hosts), len(cfg_part.services))\n\n                    # Serialization and hashing\n                    s_conf_part = serialize(realm.parts[cfg_part.instance_id])\n                    try:\n                        s_conf_part = s_conf_part.encode('utf-8')\n                    except UnicodeDecodeError:\n                        pass\n                    cfg_part.push_flavor = hashlib.sha1(s_conf_part).hexdigest()\n\n                    # We generate the scheduler configuration for the satellites:\n                    # ---\n                    sat_scheduler_cfg = scheduler_link.give_satellite_cfg()\n                    sat_scheduler_cfg.update({\n                        'managed_hosts_names': [h.get_name() for h in cfg_part.hosts],\n\n                        'managed_conf_id': cfg_part.instance_id,\n                        'push_flavor': cfg_part.push_flavor\n                    })\n                    # Generate a configuration hash\n                    cfg_string = json.dumps(sat_scheduler_cfg, sort_keys=True).encode('utf-8')\n                    sat_scheduler_cfg['hash'] = hashlib.sha1(cfg_string).hexdigest()\n\n                    logger.debug(' satellite scheduler configuration: %s', sat_scheduler_cfg)\n                    for sat_type in ('reactionner', 'poller', 'broker', 'receiver'):\n                        realm.to_satellites[sat_type][cfg_part.instance_id] = sat_scheduler_cfg\n                        realm.to_satellites_need_dispatch[sat_type][cfg_part.instance_id] = True\n                        realm.to_satellites_managed_by[sat_type][cfg_part.instance_id] = []\n                    # ---\n\n                    scheduler_link.cfg.update({\n                        # Global instance configuration\n                        'instance_id': scheduler_link.instance_id,\n                        'instance_name': scheduler_link.name,\n\n                        'schedulers': {scheduler_link.uuid: sat_scheduler_cfg},\n                        'arbiters': arbiters_cfg if scheduler_link.manage_arbiters else {},\n                        'satellites': realm.get_links_for_a_scheduler(self.pollers,\n                                                                      self.reactionners,\n                                                                      self.brokers),\n\n                        'modules': serialize(scheduler_link.modules, True),\n\n                        'conf_part': serialize(realm.parts[cfg_part.instance_id]),\n                        'managed_conf_id': cfg_part.instance_id,\n                        'push_flavor': cfg_part.push_flavor,\n\n                        'override_conf': scheduler_link.get_override_configuration()\n                    })\n\n                    # Hash the whole configuration\n                    cfg_string = json.dumps(scheduler_link.cfg, sort_keys=True).encode('utf-8')\n                    scheduler_link.cfg['hash'] = hashlib.sha1(cfg_string).hexdigest()\n\n                    # Dump the configuration part size\n                    pickled_conf = pickle.dumps(scheduler_link.cfg)\n                    logger.info(\"   scheduler configuration size: %d bytes\",\n                                sys.getsizeof(pickled_conf))\n                    logger.info(\"   scheduler satellites:\")\n                    satellites = realm.get_links_for_a_scheduler(self.pollers,\n                                                                 self.reactionners,\n                                                                 self.brokers)\n                    for sat_type in satellites:\n                        logger.info(\"   - %s\", sat_type)\n                        for sat_link_uuid in satellites[sat_type]:\n                            satellite = satellites[sat_type][sat_link_uuid]\n                            logger.info(\"   %s\", satellite['name'])\n\n                    # The configuration part is assigned to a scheduler\n                    cfg_part.is_assigned = True\n                    cfg_part.scheduler_link = scheduler_link\n                    scheduler_link.cfg_to_manage = cfg_part\n                    scheduler_link.push_flavor = cfg_part.push_flavor\n                    scheduler_link.hash = scheduler_link.cfg['hash']\n                    scheduler_link.need_conf = False\n                    scheduler_link.configuration_sent = False\n\n                    logger.info('   configuration %s (%s) assigned to %s',\n                                cfg_part.instance_id, cfg_part.push_flavor, scheduler_link.name)\n\n                    # The configuration part is assigned to a scheduler, no need to go further ;)\n                    break\n\n            logger.info(\" preparing the dispatch for satellites:\")\n            for cfg_part in list(realm.parts.values()):\n                logger.info(\"  .configuration part %s (%s), name:%s\",\n                            cfg_part.instance_id, cfg_part.uuid, cfg_part.config_name)\n                for sat_type in ('reactionner', 'poller', 'broker', 'receiver'):\n                    if cfg_part.instance_id not in realm.to_satellites_need_dispatch[sat_type]:\n                        logger.warning(\"   nothing to dispatch for %ss\", sat_type)\n                        return\n\n                    if not realm.to_satellites_need_dispatch[sat_type][cfg_part.instance_id]:\n                        logger.warning(\"   no need to dispatch to %ss\", sat_type)\n                        return\n\n                    # Get the list of the concerned satellites\n                    satellites = realm.get_potential_satellites_by_type(self.satellites, sat_type)\n                    if satellites:\n                        logger.info(\"  realm %ss: %s\",\n                                    sat_type, ','.join([s.get_name() for s in satellites]))\n                    else:\n                        logger.info(\"   no %s satellites\", sat_type)\n\n                    # Now we dispatch cfg to every one ask for it\n                    nb_cfg_prepared = 0\n                    for sat_link in satellites:\n                        if not sat_link.active:\n                            # I exclude the daemons that are not active\n                            continue\n\n                        if nb_cfg_prepared > realm.get_nb_of_must_have_satellites(sat_type):\n                            logger.warning(\"Too much configuration parts prepared \"\n                                           \"for the expected satellites count. \"\n                                           \"Realm: %s, satellite: %s - prepared: %d out of %d\",\n                                           realm.name, sat_link.name, nb_cfg_prepared,\n                                           realm.get_nb_of_must_have_satellites(sat_type))\n                            # Fred - 2018-07-20 - temporary disable this error raising!\n                            # raise DispatcherError(\"Too much configuration parts prepared \"\n                            #                       \"for the expected satellites count. \"\n                            #                       \"This should never happen!\")\n\n                        logger.info(\"   preparing configuration part '%s' for the %s '%s'\",\n                                    cfg_part.instance_id, sat_type, sat_link.name)\n\n                        sat_link.cfg.update({\n                            # Global instance configuration\n                            'arbiters': arbiters_cfg if sat_link.manage_arbiters else {},\n                            'modules': serialize(sat_link.modules, True),\n                            'managed_conf_id': 'see_my_schedulers',\n                            'global_conf': self.global_conf\n                        })\n                        sat_link.cfg['schedulers'].update({\n                            cfg_part.uuid: realm.to_satellites[sat_type][cfg_part.instance_id]})\n\n                        # Brokers should have pollers and reactionners links too\n                        if sat_type == \"broker\":\n                            sat_link.cfg.update({'satellites': realm.get_links_for_a_broker(\n                                self.pollers, self.reactionners, self.receivers,\n                                self.alignak_conf.realms, sat_link.manage_sub_realms)})\n\n                        # Hash the whole configuration\n                        cfg_string = json.dumps(sat_link.cfg, sort_keys=True).encode('utf-8')\n                        sat_link.cfg['hash'] = hashlib.sha1(cfg_string).hexdigest()\n\n                        # Dump the configuration part size\n                        pickled_conf = pickle.dumps(sat_link.cfg)\n                        logger.info('   %s configuration size: %d bytes',\n                                    sat_type, sys.getsizeof(pickled_conf))\n\n                        # The configuration part is assigned to a satellite\n                        sat_link.cfg_to_manage = cfg_part\n                        sat_link.push_flavor = cfg_part.push_flavor\n                        sat_link.hash = sat_link.cfg['hash']\n                        sat_link.need_conf = False\n                        sat_link.configuration_sent = False\n\n                        logger.info('   configuration %s (%s) assigned to %s',\n                                    cfg_part.instance_id, cfg_part.push_flavor, sat_link.name)\n\n                        nb_cfg_prepared += 1\n                        realm.to_satellites_managed_by[sat_type][\n                            cfg_part.instance_id].append(sat_link)\n\n                        # I've got enough satellite, the next ones are considered unuseful!\n                        if nb_cfg_prepared == realm.get_nb_of_must_have_satellites(sat_type):\n                            logger.info(\"   no more %s needed in this realm.\", sat_type)\n                            realm.to_satellites_need_dispatch[sat_type][\n                                cfg_part.instance_id] = False\n\n        nb_missed = len([cfg for cfg in list(\n            self.alignak_conf.parts.values()) if not cfg.is_assigned])\n        if nb_missed > 0:\n            logger.warning(\"Some configuration parts are not dispatched, %d are missing\", nb_missed)\n        else:\n            logger.info(\"All configuration parts are assigned \"\n                        \"to schedulers and their satellites :)\")\n\n        # Schedulers without a configuration in a dispatch ok do not need a configuration\n        # so they do not raise dispatching errors if they are not used\n        for scheduler_link in self.schedulers:\n            if not scheduler_link.cfg_to_manage:\n                # \"so it do not ask anymore for conf\"\n                logger.warning('The scheduler %s do not need a configuration!', scheduler_link.name)\n                scheduler_link.need_conf = False", "response": "Prepare the dispatching configuration for each daemon."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndispatching the configuration to satellites.", "response": "def dispatch(self, test=False):  # pylint: disable=too-many-branches\n        \"\"\"\n        Send configuration to satellites\n\n        :return: None\n        \"\"\"\n        if not self.new_to_dispatch:\n            raise DispatcherError(\"Dispatcher cannot dispatch, \"\n                                  \"because no configuration is prepared!\")\n\n        if self.first_dispatch_done:\n            raise DispatcherError(\"Dispatcher cannot dispatch, \"\n                                  \"because the configuration is still dispatched!\")\n\n        if self.dispatch_ok:\n            logger.info(\"Dispatching is already done and ok...\")\n            return\n\n        logger.info(\"Trying to send configuration to the satellites...\")\n\n        self.dispatch_ok = True\n\n        # todo: the 3 loops hereunder may be factorized\n        for link in self.arbiters:\n            # If not me and a spare arbiter...\n            if link == self.arbiter_link:\n                # I exclude myself from the dispatching, I have my configuration ;)\n                continue\n\n            if not link.active:\n                # I exclude the daemons that are not active\n                continue\n\n            if not link.spare:\n                # Do not dispatch to a master arbiter!\n                continue\n\n            if link.configuration_sent:\n                logger.debug(\"Arbiter %s already sent!\", link.name)\n                continue\n\n            if not link.reachable:\n                logger.debug(\"Arbiter %s is not reachable to receive its configuration\",\n                             link.name)\n                continue\n\n            logger.info(\"Sending configuration to the arbiter %s\", link.name)\n            logger.debug(\"- %s\", link.cfg)\n\n            link.put_conf(link.cfg, test=test)\n            link.configuration_sent = True\n\n            logger.info(\"- sent\")\n\n            # Now that the spare arbiter has a configuration, tell him it must not run,\n            # because I'm not dead ;)\n            link.do_not_run()\n\n        for link in self.schedulers:\n            if link.configuration_sent:\n                logger.debug(\"Scheduler %s already sent!\", link.name)\n                continue\n\n            if not link.active:\n                # I exclude the daemons that are not active\n                continue\n\n            if not link.reachable:\n                logger.debug(\"Scheduler %s is not reachable to receive its configuration\",\n                             link.name)\n                continue\n\n            logger.info(\"Sending configuration to the scheduler %s\", link.name)\n            logger.debug(\"- %s\", link.cfg)\n\n            link.put_conf(link.cfg, test=test)\n            link.configuration_sent = True\n\n            logger.info(\"- sent\")\n\n        for link in self.satellites:\n            if link.configuration_sent:\n                logger.debug(\"%s %s already sent!\", link.type, link.name)\n                continue\n\n            if not link.active:\n                # I exclude the daemons that are not active\n                continue\n\n            if not link.reachable:\n                logger.warning(\"%s %s is not reachable to receive its configuration\",\n                               link.type, link.name)\n                continue\n\n            logger.info(\"Sending configuration to the %s %s\", link.type, link.name)\n            logger.debug(\"- %s\", link.cfg)\n\n            link.put_conf(link.cfg, test=test)\n            link.configuration_sent = True\n\n            logger.info(\"- sent\")\n\n        if self.dispatch_ok:\n            # Newly prepared configuration got dispatched correctly\n            self.new_to_dispatch = False\n            self.first_dispatch_done = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stop_request(self, stop_now=False):\n        all_ok = True\n        for daemon_link in self.all_daemons_links:\n            logger.debug(\"Stopping: %s (%s)\", daemon_link, stop_now)\n            if daemon_link == self.arbiter_link:\n                # I exclude myself from the process, I know we are going to stop ;)\n                continue\n\n            if not daemon_link.active:\n                # I exclude the daemons that are not active\n                continue\n\n            # Send a stop request to the daemon\n            try:\n                stop_ok = daemon_link.stop_request(stop_now=stop_now)\n            except LinkError:\n                stop_ok = True\n                logger.warning(\"Daemon stop request failed, %s probably stopped!\", daemon_link)\n\n            all_ok = all_ok and stop_ok\n\n            daemon_link.stopping = True\n\n        self.stop_request_sent = all_ok\n        return self.stop_request_sent", "response": "Send a stop request to all the daemons that are reachable from the arbiter."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pythonize(self, val):\n        __boolean_states__ = {'1': True, 'yes': True, 'true': True, 'on': True,\n                              '0': False, 'no': False, 'false': False, 'off': False}\n\n        if isinstance(val, bool):\n            return val\n        val = unique_value(val).lower()\n        if val in list(__boolean_states__.keys()):\n            return __boolean_states__[val]\n\n        raise PythonizeError(\"Cannot convert '%s' to a boolean value\" % val)", "response": "Convert value into a boolean value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts value into a list of strings on coma char", "response": "def pythonize(self, val):\n        \"\"\"Convert value into a list::\n\n        * split value (or each element if value is a list) on coma char\n        * strip split values\n\n        :param val: value to convert\n        :type val: str\n        :return: list corresponding to value\n        :rtype: list\n        \"\"\"\n        if isinstance(val, list):\n            return [s.strip() if hasattr(s, \"strip\") else s\n                    for s in list_split(val, self.split_on_comma)\n                    if hasattr(s, \"strip\") and s.strip() != '' or self.keep_empty]\n\n        return [s.strip() if hasattr(s, \"strip\") else s\n                for s in to_split(val, self.split_on_comma)\n                if hasattr(s, \"strip\") and s.strip() != '' or self.keep_empty]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pythonize(self, val):\n        val = unique_value(val)\n\n        def split(keyval):\n            \"\"\"Split key-value string into (key,value)\n\n            :param keyval: key value string\n            :return: key, value\n            :rtype: tuple\n            \"\"\"\n            matches = re.match(r\"^\\s*([^\\s]+)\\s*=\\s*([^\\s]+)\\s*$\", keyval)\n            if matches is None:\n                raise ValueError\n\n            return (\n                matches.group(1),\n                # >2.4 only. we keep it for later. m.group(2) if self.elts_prop is None\n                # else self.elts_prop.pythonize(m.group(2))\n                (self.elts_prop.pythonize(matches.group(2)),\n                 matches.group(2))[self.elts_prop is None]\n            )\n\n        if val is None:\n            return dict()\n\n        if self.elts_prop is None:\n            return val\n\n        # val is in the form \"key1=addr:[port],key2=addr:[port],...\"\n        return dict([split(kv) for kv in to_split(val)])", "response": "Convert value into a dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts value into a dict of address and port", "response": "def pythonize(self, val):\n        \"\"\"Convert value into a address ip format::\n\n        * If value is a list, try to take the last element\n        * match ip address and port (if available)\n\n        :param val: value to convert\n        :type val:\n        :return: address/port corresponding to value\n        :rtype: dict\n        \"\"\"\n        val = unique_value(val)\n        matches = re.match(r\"^([^:]*)(?::(\\d+))?$\", val)\n        if matches is None:\n            raise ValueError\n\n        addr = {'address': matches.group(1)}\n        if matches.group(2) is not None:\n            addr['port'] = int(matches.group(2))\n\n        return addr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pythonize(self, val):\n        if isinstance(val, list) and len(set(val)) == 1:\n            # If we have a list with a unique value just use it\n            return val[0]\n\n        # Well, can't choose to remove something.\n        return val", "response": "Convert the value to a list element."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts value into a list of integer values", "response": "def pythonize(self, val):\n        \"\"\"Convert value into a integer list::\n\n        * Try to convert into a list\n        * Convert each element into a int\n\n        :param val: value to convert\n        :type val:\n        :return: integer list corresponding to value\n        :rtype: list[int]\n        \"\"\"\n        val = super(IntListProp, self).pythonize(val)\n        try:\n            return [int(e) for e in val]\n        except ValueError as value_except:\n            raise PythonizeError(str(value_except))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_response(self, method, endpoint, headers=None, json=None, params=None, data=None):\n        # pylint: disable=too-many-arguments\n        \"\"\"\n        Returns the response from the requested endpoint with the requested method\n        :param method: str. one of the methods accepted by Requests ('POST', 'GET', ...)\n        :param endpoint: str. the relative endpoint to access\n        :param params: (optional) Dictionary or bytes to be sent in the query string\n        for the :class:`Request`.\n        :param data: (optional) Dictionary, bytes, or file-like object to send in the body\n        of the :class:`Request`.\n        :param json: (optional) json to send in the body of the :class:`Request`.\n        :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.\n        :return: Requests.response\n        \"\"\"\n        logger.debug(\"Parameters for get_response:\")\n        logger.debug(\"\\t - endpoint: %s\", endpoint)\n        logger.debug(\"\\t - method: %s\", method)\n        logger.debug(\"\\t - headers: %s\", headers)\n        logger.debug(\"\\t - json: %s\", json)\n        logger.debug(\"\\t - params: %s\", params)\n        logger.debug(\"\\t - data: %s\", data)\n\n        url = self.get_url(endpoint)\n\n        # First stage. Errors are connection errors (timeout, no session, ...)\n        try:\n            response = self.session.request(method=method, url=url, headers=headers, json=json,\n                                            params=params, data=data, timeout=self.timeout)\n            logger.debug(\"response headers: %s\", response.headers)\n            logger.debug(\"response content: %s\", response.content)\n        except RequestException as exp:\n            response = {\"_status\": \"ERR\",\n                        \"_error\": {\"message\": exp},\n                        \"_issues\": {\"message\": exp}}\n\n        return response", "response": "Get the response from the requested endpoint with the requested method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode(response):\n\n        # Second stage. Errors are backend errors (bad login, bad url, ...)\n        try:\n            response.raise_for_status()\n        except requests.HTTPError as exp:\n            response = {\"_status\": \"ERR\",\n                        \"_error\": {\"message\": exp, \"code\": response.status_code},\n                        \"_issues\": {\"message\": exp, \"code\": response.status_code}}\n            return response\n        else:\n            return response.json()", "response": "Decodes the response object into a dict or raises BackendException\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef login(self, username, password):\n        logger.debug(\"login for: %s\", username)\n\n        # Configured as not authenticated WS\n        if not username and not password:\n            self.set_token(token=None)\n            return False\n\n        if not username or not password:\n            logger.error(\"Username or password cannot be None!\")\n            self.set_token(token=None)\n            return False\n\n        endpoint = 'login'\n        json = {'username': username, 'password': password}\n        response = self.get_response(method='POST', endpoint=endpoint, json=json)\n        if response.status_code == 401:\n            logger.error(\"Access denied to %s\", self.url_endpoint_root)\n            self.set_token(token=None)\n            return False\n\n        resp = self.decode(response=response)\n\n        if 'token' in resp:\n            self.set_token(token=resp['token'])\n            return True\n\n        return False", "response": "Log into the WS interface and get the authentication token"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef logout(self):\n        logger.debug(\"request backend logout\")\n        if not self.authenticated:\n            logger.warning(\"Unnecessary logout ...\")\n            return True\n\n        endpoint = 'logout'\n\n        _ = self.get_response(method='POST', endpoint=endpoint)\n\n        self.session.close()\n        self.set_token(token=None)\n\n        return True", "response": "Logout from the backend"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets items or item in alignak backend If an error occurs, a BackendException is raised. This method builds a response as a dictionary that always contains: _items and _status:: { u'_items': [ ... ], u'_status': u'OK' } :param endpoint: endpoint (API URL) relative from root endpoint :type endpoint: str :param params: parameters for the backend API :type params: dict :return: dictionary as specified upper :rtype: dict", "response": "def get(self, endpoint, params=None):\n        \"\"\"\n        Get items or item in alignak backend\n\n        If an error occurs, a BackendException is raised.\n\n        This method builds a response as a dictionary that always contains: _items and _status::\n\n            {\n                u'_items': [\n                    ...\n                ],\n                u'_status': u'OK'\n            }\n\n        :param endpoint: endpoint (API URL) relative from root endpoint\n        :type endpoint: str\n        :param params: parameters for the backend API\n        :type params: dict\n        :return: dictionary as specified upper\n        :rtype: dict\n        \"\"\"\n        response = self.get_response(method='GET', endpoint=endpoint, params=params)\n\n        resp = self.decode(response=response)\n        if '_status' not in resp:  # pragma: no cover - need specific backend tests\n            resp['_status'] = u'OK'  # TODO: Sure??\n\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new item in the cache store", "response": "def post(self, endpoint, data, files=None, headers=None):\n        # pylint: disable=unused-argument\n        \"\"\"\n        Create a new item\n\n        :param endpoint: endpoint (API URL)\n        :type endpoint: str\n        :param data: properties of item to create\n        :type data: dict\n        :param files: Not used. To be implemented\n        :type files: None\n        :param headers: headers (example: Content-Type)\n        :type headers: dict\n        :return: response (creation information)\n        :rtype: dict\n        \"\"\"\n        # We let Requests encode data to json\n        response = self.get_response(method='POST', endpoint=endpoint, json=data, headers=headers)\n\n        resp = self.decode(response=response)\n\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef patch(self, endpoint, data):\n        response = self.get_response(method='PATCH', endpoint=endpoint, json=data,\n                                     headers={'Content-Type': 'application/json'})\n\n        if response.status_code == 200:\n            return self.decode(response=response)\n\n        return response", "response": "This method is used to update the _etag of an item in the backend."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sanitize_name(field_name):\n    if not field_name:\n        return field_name\n\n    # Sanitize field name for TSDB (Graphite or Influx):\n    sanitized = field_name.strip()\n    if sanitized.startswith('/'):\n        sanitized = '_' + sanitized[1:]\n    # + becomes a _\n    sanitized = sanitized.replace(\"+\", \"_\")\n    # / becomes a -\n    sanitized = sanitized.replace(\"/\", \"-\")\n    # space becomes a _\n    sanitized = sanitized.replace(\" \", \"_\")\n    # % becomes _pct\n    sanitized = sanitized.replace(\"%\", \"_pct\")\n    # all character not in [a-zA-Z_-0-9.] is removed\n    sanitized = re.sub(r'[^a-zA-Z_\\-0-9\\.\\$]', '', sanitized)\n\n    return sanitized", "response": "Sanitize a field name for a TSDB or Graphite dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize MacroResolver instance with conf.", "response": "def init(self, conf):\n        \"\"\"Initialize MacroResolver instance with conf.\n        Must be called at least once.\n\n        :param conf: configuration to load\n        :type conf: alignak.objects.Config\n        :return: None\n        \"\"\"\n\n        # For searching class and elements for on-demand\n        # we need link to types\n        self.my_conf = conf\n        self.lists_on_demand = []\n        self.hosts = self.my_conf.hosts\n        # For special void host_name handling...\n        self.host_class = self.hosts.inner_class\n        self.lists_on_demand.append(self.hosts)\n        self.services = self.my_conf.services\n        self.contacts = self.my_conf.contacts\n        self.lists_on_demand.append(self.contacts)\n        self.hostgroups = self.my_conf.hostgroups\n        self.lists_on_demand.append(self.hostgroups)\n        self.commands = self.my_conf.commands\n        self.servicegroups = self.my_conf.servicegroups\n        self.lists_on_demand.append(self.servicegroups)\n        self.contactgroups = self.my_conf.contactgroups\n        self.lists_on_demand.append(self.contactgroups)\n        self.illegal_macro_output_chars = self.my_conf.illegal_macro_output_chars\n        self.env_prefix = self.my_conf.env_variables_prefix"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_macros(chain):\n        regex = re.compile(r'(\\$)')\n        elts = regex.split(chain)\n        macros = {}\n        in_macro = False\n        for elt in elts:\n            if elt == '$':\n                in_macro = not in_macro\n            elif in_macro:\n                macros[elt] = {'val': '', 'type': 'unknown'}\n\n        return macros", "response": "Get all macros of a chain and create a dict with the following structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the value of an attribute from an element s property.", "response": "def _get_value_from_element(self, elt, prop):\n        # pylint: disable=too-many-return-statements\n        \"\"\"Get value from an element's property.\n\n        the property may be a function to call.\n\n        If the property is not resolved (because not implemented), this function will return 'n/a'\n\n        :param elt: element\n        :type elt: object\n        :param prop: element property\n        :type prop: str\n        :return: getattr(elt, prop) or getattr(elt, prop)() (call)\n        :rtype: str\n        \"\"\"\n        args = None\n        # We have args to provide to the function\n        if isinstance(prop, tuple):\n            prop, args = prop\n        value = getattr(elt, prop, None)\n        if value is None:\n            return 'n/a'\n\n        try:\n            # If the macro is set to a list property\n            if isinstance(value, list):\n                # Return the list items, comma separated and bracketed\n                return \"[%s]\" % ','.join(value)\n\n            # If the macro is not set as a function to call\n            if not isinstance(value, collections.Callable):\n                return value\n\n            # Case of a function call with no arguments\n            if not args:\n                return value()\n\n            # Case where we need args to the function\n            # ex : HOSTGROUPNAME (we need hostgroups)\n            # ex : SHORTSTATUS (we need hosts and services if bp_rule)\n            real_args = []\n            for arg in args:\n                real_args.append(getattr(self, arg, None))\n            return value(*real_args)\n        except AttributeError:\n            # Commented because there are many unresolved macros and this log is spamming :/\n            # # Raise a warning and return a strange value when macro cannot be resolved\n            # warnings.warn(\n            #     'Error when getting the property value for a macro: %s',\n            #     MacroWarning, stacklevel=2)\n            # Return a strange value when macro cannot be resolved\n            return 'n/a'\n        except UnicodeError:\n            if isinstance(value, string_types):\n                return str(value, 'utf8', errors='ignore')\n\n            return 'n/a'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _delete_unwanted_caracters(self, chain):\n        try:\n            chain = chain.decode('utf8', 'replace')\n        except UnicodeEncodeError:\n            # If it is still encoded correctly, ignore...\n            pass\n        except AttributeError:\n            # Python 3 will raise an exception because the line is still unicode\n            pass\n        for char in self.illegal_macro_output_chars:\n            chain = chain.replace(char, '')\n        return chain", "response": "Remove unwanted characters from chain from chain."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all environment macros from data ::", "response": "def get_env_macros(self, data):\n        \"\"\"Get all environment macros from data\n        For each object in data ::\n\n        * Fetch all macros in object.__class__.macros\n        * Fetch all customs macros in o.custom\n\n        :param data: data to get macro\n        :type data:\n        :return: dict with macro name as key and macro value as value\n        :rtype: dict\n        \"\"\"\n        env = {}\n\n        for obj in data:\n            cls = obj.__class__\n            macros = cls.macros\n            for macro in macros:\n                if macro.startswith(\"USER\"):\n                    continue\n\n                prop = macros[macro]\n                value = self._get_value_from_element(obj, prop)\n                env['%s%s' % (self.env_prefix, macro)] = value\n            if hasattr(obj, 'customs'):\n                # make NAGIOS__HOSTMACADDR from _MACADDR\n                for cmacro in obj.customs:\n                    new_env_name = '%s_%s%s' % (self.env_prefix,\n                                                obj.__class__.__name__.upper(),\n                                                cmacro[1:].upper())\n                    env[new_env_name] = obj.customs[cmacro]\n\n        return env"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplacing macro in the command line with the real value :param c_line: command line to modify :type c_line: str :param data: objects list, use to look for a specific macro :type data: :param macromodulations: the available macro modulations :type macromodulations: dict :param timeperiods: the available timeperiods :type timeperiods: dict :param args: args given to the command line, used to get \"ARGN\" macros. :type args: :return: command line with '$MACRO$' replaced with values :rtype: str", "response": "def resolve_simple_macros_in_string(self, c_line, data, macromodulations, timeperiods,\n                                        args=None):\n        # pylint: disable=too-many-locals, too-many-branches, too-many-nested-blocks\n        \"\"\"Replace macro in the command line with the real value\n\n        :param c_line: command line to modify\n        :type c_line: str\n        :param data: objects list, use to look for a specific macro\n        :type data:\n        :param macromodulations: the available macro modulations\n        :type macromodulations: dict\n        :param timeperiods: the available timeperiods\n        :type timeperiods: dict\n        :param args: args given to the command line, used to get \"ARGN\" macros.\n        :type args:\n        :return: command line with '$MACRO$' replaced with values\n        :rtype: str\n        \"\"\"\n        # Now we prepare the classes for looking at the class.macros\n        data.append(self)  # For getting global MACROS\n        if hasattr(self, 'my_conf'):\n            data.append(self.my_conf)  # For USERN macros\n\n        # we should do some loops for nested macros\n        # like $USER1$ hiding like a ninja in a $ARG2$ Macro. And if\n        # $USER1$ is pointing to $USER34$ etc etc, we should loop\n        # until we reach the bottom. So the last loop is when we do\n        # not still have macros :)\n        still_got_macros = True\n        nb_loop = 0\n        while still_got_macros:\n            nb_loop += 1\n            # Ok, we want the macros in the command line\n            macros = self._get_macros(c_line)\n\n            # Put in the macros the type of macro for all macros\n            self._get_type_of_macro(macros, data)\n\n            # We can get out if we do not have macros this loop\n            still_got_macros = False\n            if macros:\n                still_got_macros = True\n\n            # Now we get values from elements\n            for macro in macros:\n                # If type ARGN, look at ARGN cutting\n                if macros[macro]['type'] == 'ARGN' and args is not None:\n                    macros[macro]['val'] = self._resolve_argn(macro, args)\n                    macros[macro]['type'] = 'resolved'\n                # If object type, get value from a property\n                if macros[macro]['type'] == 'object':\n                    obj = macros[macro]['object']\n                    if obj not in data:\n                        continue\n                    prop = obj.macros[macro]\n                    if not prop:\n                        continue\n                    macros[macro]['val'] = self._get_value_from_element(obj, prop)\n                    # Now check if we do not have a 'output' macro. If so, we must\n                    # delete all special characters that can be dangerous\n                    if macro in self.output_macros:\n                        logger.debug(\"-> macro from: %s, %s = %s\", obj, macro, macros[macro])\n                        macros[macro]['val'] = self._delete_unwanted_caracters(macros[macro]['val'])\n                # If custom type, get value from an object custom variables\n                if macros[macro]['type'] == 'CUSTOM':\n                    cls_type = macros[macro]['class']\n                    # Beware : only cut the first _HOST or _SERVICE or _CONTACT value,\n                    # so the macro name can have it on it..\n                    macro_name = re.split('_' + cls_type, macro, 1)[1].upper()\n                    logger.debug(\" ->: %s - %s\", cls_type, macro_name)\n                    # Ok, we've got the macro like MAC_ADDRESS for _HOSTMAC_ADDRESS\n                    # Now we get the element in data that have the type HOST\n                    # and we check if it got the custom value\n                    for elt in data:\n                        if not elt or elt.__class__.my_type.upper() != cls_type:\n                            continue\n                        logger.debug(\"   : for %s: %s\", elt, elt.customs)\n                        if not getattr(elt, 'customs'):\n                            continue\n                        if '_' + macro_name in elt.customs:\n                            macros[macro]['val'] = elt.customs['_' + macro_name]\n                        logger.debug(\"   : macro %s = %s\", macro, macros[macro]['val'])\n\n                        # Then look on the macromodulations, in reverse order, so\n                        # the last defined will be the first applied\n                        mms = getattr(elt, 'macromodulations', [])\n                        for macromodulation_id in mms[::-1]:\n                            macromodulation = macromodulations[macromodulation_id]\n                            if not macromodulation.is_active(timeperiods):\n                                continue\n                            # Look if the modulation got the value,\n                            # but also if it's currently active\n                            if \"_%s\" % macro_name in macromodulation.customs:\n                                macros[macro]['val'] = macromodulation.customs[\"_%s\" % macro_name]\n                # If on-demand type, get value from an dynamic provided data objects\n                if macros[macro]['type'] == 'ONDEMAND':\n                    macros[macro]['val'] = self._resolve_ondemand(macro, data)\n\n            # We resolved all we can, now replace the macros in the command call\n            for macro in macros:\n                c_line = c_line.replace(\"$%s$\" % macro, \"%s\" % (macros[macro]['val']))\n\n            # A $$ means we want a $, it's not a macro!\n            # We replace $$ by a big dirty thing to be sure to not misinterpret it\n            c_line = c_line.replace(\"$$\", \"DOUBLEDOLLAR\")\n\n            if nb_loop > 32:  # too much loop, we exit\n                still_got_macros = False\n\n        # We now replace the big dirty token we made by only a simple $\n        c_line = c_line.replace(\"DOUBLEDOLLAR\", \"$\")\n\n        return c_line.strip()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresolves a specific command line with the given data.", "response": "def resolve_command(self, com, data, macromodulations, timeperiods):\n        \"\"\"Resolve command macros with data\n\n        :param com: check / event handler or command call object\n        :type com: object\n        :param data: objects list, used to search for a specific macro (custom or object related)\n        :type data:\n        :return: command line with '$MACRO$' replaced with values\n        :param macromodulations: the available macro modulations\n        :type macromodulations: dict\n        :param timeperiods: the available timeperiods\n        :type timeperiods: dict\n        :rtype: str\n        \"\"\"\n        logger.debug(\"Resolving: macros in: %s, arguments: %s\",\n                     com.command.command_line, com.args)\n        return self.resolve_simple_macros_in_string(com.command.command_line, data,\n                                                    macromodulations, timeperiods,\n                                                    args=com.args)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_type_of_macro(macros, objs):\n        for macro in macros:\n            # ARGN Macros\n            if re.match(r'ARG\\d', macro):\n                macros[macro]['type'] = 'ARGN'\n                continue\n            # USERN macros\n            # are managed in the Config class, so no\n            # need to look that here\n            elif re.match(r'_HOST\\w', macro):\n                macros[macro]['type'] = 'CUSTOM'\n                macros[macro]['class'] = 'HOST'\n                continue\n            elif re.match(r'_SERVICE\\w', macro):\n                macros[macro]['type'] = 'CUSTOM'\n                macros[macro]['class'] = 'SERVICE'\n                # value of macro: re.split('_HOST', '_HOSTMAC_ADDRESS')[1]\n                continue\n            elif re.match(r'_CONTACT\\w', macro):\n                macros[macro]['type'] = 'CUSTOM'\n                macros[macro]['class'] = 'CONTACT'\n                continue\n            # On demand macro\n            elif len(macro.split(':')) > 1:\n                macros[macro]['type'] = 'ONDEMAND'\n                continue\n            # OK, classical macro...\n            for obj in objs:\n                if macro in obj.macros:\n                    macros[macro]['type'] = 'object'\n                    macros[macro]['object'] = obj\n                    continue", "response": "Get the type of the object in the given macros list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _resolve_argn(macro, args):\n        # first, get the number of args\n        _id = None\n        matches = re.search(r'ARG(?P<id>\\d+)', macro)\n        if matches is not None:\n            _id = int(matches.group('id')) - 1\n            try:\n                return args[_id]\n            except IndexError:\n                # Required argument not found, returns an empty string\n                return ''\n        return ''", "response": "Get argument from macro name\n        ie : ARG3$ -> args [ 2 ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresolves on demand macro value from data.", "response": "def _resolve_ondemand(self, macro, data):\n        # pylint: disable=too-many-locals\n        \"\"\"Get on demand macro value\n\n        If the macro cannot be resolved, this function will return 'n/a' rather than\n        an empty string, this to alert the caller of a potential problem.\n\n        :param macro: macro to parse\n        :type macro:\n        :param data: data to get value from\n        :type data:\n        :return: macro value\n        :rtype: str\n        \"\"\"\n        elts = macro.split(':')\n        nb_parts = len(elts)\n        macro_name = elts[0]\n        # 3 parts for a service, 2 for all others types...\n        if nb_parts == 3:\n            val = ''\n            (host_name, service_description) = (elts[1], elts[2])\n            # host_name can be void, so it's the host in data\n            # that is important. We use our self.host_class to\n            # find the host in the data :)\n            if host_name == '':\n                for elt in data:\n                    if elt is not None and elt.__class__ == self.host_class:\n                        host_name = elt.host_name\n            # Ok now we get service\n            serv = self.services.find_srv_by_name_and_hostname(host_name, service_description)\n            if serv is not None:\n                cls = serv.__class__\n                prop = cls.macros[macro_name]\n                val = self._get_value_from_element(serv, prop)\n                return val\n        # Ok, service was easy, now hard part\n        else:\n            val = ''\n            elt_name = elts[1]\n            # Special case: elt_name can be void\n            # so it's the host where it apply\n            if elt_name == '':\n                for elt in data:\n                    if elt is not None and elt.__class__ == self.host_class:\n                        elt_name = elt.host_name\n            for od_list in self.lists_on_demand:\n                cls = od_list.inner_class\n                # We search our type by looking at the macro\n                if macro_name in cls.macros:\n                    prop = cls.macros[macro_name]\n                    i = od_list.find_by_name(elt_name)\n                    if i is not None:\n                        val = self._get_value_from_element(i, prop)\n                        # Ok we got our value :)\n                        break\n            return val\n\n        # Return a strange value in this case rather than an empty string\n        return 'n/a'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the number of hosts which are problems and not handled", "response": "def _get_total_hosts_problems_unhandled(self):\n        \"\"\"\n        Get the number of host problems not handled\n\n        :return: Number of hosts which are problems and not handled\n        :rtype: int\n        \"\"\"\n        return sum(1 for h in self.hosts if h.is_problem and not h.problem_has_been_acknowledged)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_total_hosts_problems_handled(self):\n        return sum(1 for h in self.hosts if h.is_problem and h.problem_has_been_acknowledged)", "response": "Get the number of hosts which are problems and not handled"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_total_hosts_not_monitored(self):\n        return sum(1 for h in self.hosts if not h.active_checks_enabled and\n                   not h.passive_checks_enabled)", "response": "Get the number of hosts which are not monitored"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _tot_services_by_state(self, state=None, state_type=None):\n        if state is None and state_type is None:\n            return len(self.services)\n        if state_type:\n            return sum(1 for s in self.services if s.state == state and s.state_type == state_type)\n        return sum(1 for s in self.services if s.state == state)", "response": "Generic function to get the number of services in the specified state"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tot_unhandled_services_by_state(self, state):\n        return sum(1 for s in self.services if s.state == state and\n                   s.is_problem and not s.problem_has_been_acknowledged)", "response": "Generic function to get the number of unhandled problem services in the specified state"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_total_services_problems_unhandled(self):\n        return sum(1 for s in self.services if s.is_problem and not s.problem_has_been_acknowledged)", "response": "Get the number of services that are a problem and that are not acknowledged\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_total_services_problems_handled(self):\n        return sum(1 for s in self.services if s.is_problem and s.problem_has_been_acknowledged)", "response": "Get the number of services which are problems and not handled"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_total_services_not_monitored(self):\n        return sum(1 for s in self.services if not s.active_checks_enabled and\n                   not s.passive_checks_enabled)", "response": "Get the number of services which are not monitored"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_data(self, metric, value, ts=None):\n        if not ts:\n            ts = time.time()\n        if self.__data_lock.acquire():\n            self.__data.append((metric, (ts, value)))\n            self.__data_lock.release()\n            return True\n        return False", "response": "Adds data to the queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a dictionary to the data list", "response": "def add_data_dict(self, dd):  # pragma: no cover - never used...\n        \"\"\"\n        dd must be a dictionary where keys are the metric name,\n        each key contains a dictionary which at least must have 'value' key (optionally 'ts')\n\n        dd = {'experiment1.subsystem.block.metric1': {'value': 12.3, 'ts': 1379491605.55},\n              'experiment1.subsystem.block.metric2': {'value': 1.35},\n             ...}\n        \"\"\"\n        if self.__data_lock.acquire():\n            for k, v in list(dd.items()):\n                ts = v.get('ts', time.time())\n                value = v.get('value')\n                self.__data.append((k, (ts, value)))\n            self.__data_lock.release()\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_data_list(self, dl):  # pragma: no cover - never used...\n        if self.__data_lock.acquire():\n            self.__data.extend(dl)\n            self.__data_lock.release()\n            return True\n        return False", "response": "Adds a list of tuples to the data list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend data to the carbon server.", "response": "def send_data(self, data=None):\n        \"\"\"If data is empty, current buffer is sent. Otherwise data must be like:\n        data = [('metricname', (timestamp, value)),\n              ('metricname', (timestamp, value)),\n              ...]\n        \"\"\"\n        save_in_error = False\n        if not data:\n            if self.__data_lock.acquire():\n                data = self.__data\n                self.__data = []\n                save_in_error = True\n                self.__data_lock.release()\n            else:\n                return False\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        payload = pickle.dumps(data, protocol=2)\n        header = struct.pack(\"!L\", len(payload))\n        message = header + payload\n        s.settimeout(1)\n        s.connect((self.host, self.port))\n        try:\n            s.send(message)\n        except:\n            # log.exception('Error when sending data to carbon')\n            if save_in_error:\n                self.__data.extend(data)\n            return False\n        else:\n            # log.debug('Sent data to {host}:{port}: {0} metrics, {1} bytes'.format(len(data),\n            #   len(message), host = self.host, port=self.port))\n            return True\n        finally:\n            s.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_daemon_name(self, daemon_name):\n        self.daemon_name = daemon_name\n        for instance in self.instances:\n            instance.set_loaded_into(daemon_name)", "response": "Set the daemon name of the manager to daemon_name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_and_init(self, modules):\n        self.load(modules)\n        self.get_instances()\n\n        return len(self.configuration_errors) == 0", "response": "Import instantiate & init the modules we manage\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the modules and check their usability", "response": "def load(self, modules):\n        \"\"\"Load Python modules and check their usability\n\n        :param modules: list of the modules that must be loaded\n        :return:\n        \"\"\"\n        self.modules_assoc = []\n        for module in modules:\n            if not module.enabled:\n                logger.info(\"Module %s is declared but not enabled\", module.name)\n                # Store in our modules list but do not try to load\n                # Probably someone else will load this module later...\n                self.modules[module.uuid] = module\n                continue\n            logger.info(\"Importing Python module '%s' for %s...\", module.python_name, module.name)\n            try:\n                python_module = importlib.import_module(module.python_name)\n\n                # Check existing module properties\n                # Todo: check all mandatory properties\n                if not hasattr(python_module, 'properties'):  # pragma: no cover\n                    self.configuration_errors.append(\"Module %s is missing a 'properties' \"\n                                                     \"dictionary\" % module.python_name)\n                    raise AttributeError\n                logger.info(\"Module properties: %s\", getattr(python_module, 'properties'))\n\n                # Check existing module get_instance method\n                if not hasattr(python_module, 'get_instance') or \\\n                        not isinstance(getattr(python_module, 'get_instance'),\n                                       collections.Callable):  # pragma: no cover\n                    self.configuration_errors.append(\"Module %s is missing a 'get_instance' \"\n                                                     \"function\" % module.python_name)\n                    raise AttributeError\n\n                self.modules_assoc.append((module, python_module))\n                logger.info(\"Imported '%s' for %s\", module.python_name, module.name)\n            except ImportError as exp:  # pragma: no cover, simple protection\n                self.configuration_errors.append(\"Module %s (%s) can't be loaded, Python \"\n                                                 \"importation error: %s\" % (module.python_name,\n                                                                            module.name,\n                                                                            str(exp)))\n            except AttributeError:  # pragma: no cover, simple protection\n                self.configuration_errors.append(\"Module %s (%s) can't be loaded, \"\n                                                 \"module configuration\" % (module.python_name,\n                                                                           module.name))\n            else:\n                logger.info(\"Loaded Python module '%s' (%s)\", module.python_name, module.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to initialize the given module instance.", "response": "def try_instance_init(self, instance, late_start=False):\n        \"\"\"Try to \"initialize\" the given module instance.\n\n        :param instance: instance to init\n        :type instance: object\n        :param late_start: If late_start, don't look for last_init_try\n        :type late_start: bool\n        :return: True on successful init. False if instance init method raised any Exception.\n        :rtype: bool\n        \"\"\"\n        try:\n            instance.init_try += 1\n            # Maybe it's a retry\n            if not late_start and instance.init_try > 1:\n                # Do not try until too frequently, or it's too loopy\n                if instance.last_init_try > time.time() - MODULE_INIT_PERIOD:\n                    logger.info(\"Too early to retry initialization, retry period is %d seconds\",\n                                MODULE_INIT_PERIOD)\n                    # logger.info(\"%s / %s\", instance.last_init_try, time.time())\n                    return False\n            instance.last_init_try = time.time()\n\n            logger.info(\"Trying to initialize module: %s\", instance.name)\n\n            # If it's an external module, create/update Queues()\n            if instance.is_external:\n                instance.create_queues(self.daemon.sync_manager)\n\n            # The module instance init function says if initialization is ok\n            if not instance.init():\n                logger.warning(\"Module %s initialisation failed.\", instance.name)\n                return False\n            logger.info(\"Module %s is initialized.\", instance.name)\n        except Exception as exp:  # pylint: disable=broad-except\n            # pragma: no cover, simple protection\n            msg = \"The module instance %s raised an exception \" \\\n                  \"on initialization: %s, I remove it!\" % (instance.name, str(exp))\n            self.configuration_errors.append(msg)\n            logger.error(msg)\n            logger.exception(exp)\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrequests to remove the given instances list or all instances", "response": "def clear_instances(self, instances=None):\n        \"\"\"Request to \"remove\" the given instances list or all if not provided\n\n        :param instances: instances to remove (all instances are removed if None)\n        :type instances:\n        :return: None\n        \"\"\"\n        if instances is None:\n            instances = self.instances[:]  # have to make a copy of the list\n        for instance in instances:\n            self.remove_instance(instance)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nput an instance to the restart queue", "response": "def set_to_restart(self, instance):\n        \"\"\"Put an instance to the restart queue\n\n        :param instance: instance to restart\n        :type instance: object\n        :return: None\n        \"\"\"\n        self.to_restart.append(instance)\n        if instance.is_external:\n            instance.proc = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate and initialize and return the list of module instances that the caller needs.", "response": "def get_instances(self):\n        \"\"\"Create, init and then returns the list of module instances that the caller needs.\n\n        This method is called once the Python modules are loaded to initialize the modules.\n\n        If an instance can't be created or initialized then only log is doneand that\n        instance is skipped. The previous modules instance(s), if any, are all cleaned.\n\n        :return: module instances list\n        :rtype: list\n        \"\"\"\n        self.clear_instances()\n\n        for (alignak_module, python_module) in self.modules_assoc:\n            alignak_module.properties = python_module.properties.copy()\n            alignak_module.my_daemon = self.daemon\n            logger.info(\"Alignak starting module '%s'\", alignak_module.get_name())\n            if getattr(alignak_module, 'modules', None):\n                modules = []\n                for module_uuid in alignak_module.modules:\n                    if module_uuid in self.modules:\n                        modules.append(self.modules[module_uuid])\n                alignak_module.modules = modules\n            logger.debug(\"Module '%s', parameters: %s\",\n                         alignak_module.get_name(), alignak_module.__dict__)\n            try:\n                instance = python_module.get_instance(alignak_module)\n                if not isinstance(instance, BaseModule):  # pragma: no cover, simple protection\n                    self.configuration_errors.append(\"Module %s instance is not a \"\n                                                     \"BaseModule instance: %s\"\n                                                     % (alignak_module.get_name(),\n                                                        type(instance)))\n                    raise AttributeError\n            # pragma: no cover, simple protection\n            except Exception as exp:  # pylint: disable=broad-except\n                logger.error(\"The module %s raised an exception on loading, I remove it!\",\n                             alignak_module.get_name())\n                logger.exception(\"Exception: %s\", exp)\n                self.configuration_errors.append(\"The module %s raised an exception on \"\n                                                 \"loading: %s, I remove it!\"\n                                                 % (alignak_module.get_name(), str(exp)))\n            else:\n                # Give the module the data to which daemon/module it is loaded into\n                instance.set_loaded_into(self.daemon.name)\n                self.instances.append(instance)\n\n        for instance in self.instances:\n            # External instances are not initialized now, but only when they are started\n            if not instance.is_external and not self.try_instance_init(instance):\n                # If the init failed, we put in in the restart queue\n                logger.warning(\"The module '%s' failed to initialize, \"\n                               \"I will try to restart it later\", instance.name)\n                self.set_to_restart(instance)\n\n        return self.instances"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_external_instances(self, late_start=False):\n        for instance in [i for i in self.instances if i.is_external]:\n            # But maybe the init failed a bit, so bypass this ones from now\n            if not self.try_instance_init(instance, late_start=late_start):\n                logger.warning(\"The module '%s' failed to init, I will try to restart it later\",\n                               instance.name)\n                self.set_to_restart(instance)\n                continue\n\n            # ok, init succeed\n            logger.info(\"Starting external module %s\", instance.name)\n            instance.start()", "response": "Launch external instances that are load correctly"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_instance(self, instance):\n        # External instances need to be close before (process + queues)\n        if instance.is_external:\n            logger.info(\"Request external process to stop for %s\", instance.name)\n            instance.stop_process()\n            logger.info(\"External process stopped.\")\n\n        instance.clear_queues(self.daemon.sync_manager)\n\n        # Then do not listen anymore about it\n        self.instances.remove(instance)", "response": "Request to cleanly remove the given instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck alive instances. If not, log error and try to restart it :return: None", "response": "def check_alive_instances(self):\n        \"\"\"Check alive instances.\n        If not, log error and try to restart it\n\n        :return: None\n        \"\"\"\n        # Only for external\n        for instance in self.instances:\n            if instance in self.to_restart:\n                continue\n\n            if instance.is_external and instance.process and not instance.process.is_alive():\n                logger.error(\"The external module %s died unexpectedly!\", instance.name)\n                logger.info(\"Setting the module %s to restart\", instance.name)\n                # We clean its queues, they are no more useful\n                instance.clear_queues(self.daemon.sync_manager)\n                self.set_to_restart(instance)\n                # Ok, no need to look at queue size now\n                continue\n\n            # Now look for maximum queue size. If above the defined value, the module may have\n            # a huge problem and so bailout. It's not a perfect solution, more a watchdog\n            # If max_queue_size is 0, don't check this\n            if self.daemon.max_queue_size == 0:\n                continue\n\n            # Check for module queue size\n            queue_size = 0\n            try:\n                queue_size = instance.to_q.qsize()\n            except Exception:  # pylint: disable=broad-except\n                pass\n            if queue_size > self.daemon.max_queue_size:\n                logger.error(\"The module %s has a too important queue size (%s > %s max)!\",\n                             instance.name, queue_size, self.daemon.max_queue_size)\n                logger.info(\"Setting the module %s to restart\", instance.name)\n                # We clean its queues, they are no more useful\n                instance.clear_queues(self.daemon.sync_manager)\n                self.set_to_restart(instance)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to restart dead instances in the module.", "response": "def try_to_restart_deads(self):\n        \"\"\"Try to reinit and restart dead instances\n\n        :return: None\n        \"\"\"\n        to_restart = self.to_restart[:]\n        del self.to_restart[:]\n\n        for instance in to_restart:\n            logger.warning(\"Trying to restart module: %s\", instance.name)\n\n            if self.try_instance_init(instance):\n                logger.warning(\"Restarting %s...\", instance.name)\n                # Because it is a restart, clean the module inner process reference\n                instance.process = None\n                # If it's an external module, it will start the process\n                instance.start()\n                # Ok it's good now :)\n            else:\n                # Will retry later...\n                self.to_restart.append(instance)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_internal_instances(self, phase=None):\n        if phase is None:\n            return [instance for instance in self.instances if not instance.is_external]\n\n        return [instance for instance in self.instances\n                if not instance.is_external and phase in instance.phases and\n                instance not in self.to_restart]", "response": "Get a list of internal instances in a specific phase"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_external_instances(self, phase=None):\n        if phase is None:\n            return [instance for instance in self.instances if instance.is_external]\n\n        return [instance for instance in self.instances\n                if instance.is_external and phase in instance.phases and\n                instance not in self.to_restart]", "response": "Get a list of external instances in a specific phase"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop_all(self):\n        logger.info('Shutting down modules...')\n        # Ask internal to quit if they can\n        for instance in self.get_internal_instances():\n            if hasattr(instance, 'quit') and isinstance(instance.quit, collections.Callable):\n                instance.quit()\n\n        self.clear_instances([instance for instance in self.instances if instance.is_external])", "response": "Stop all module instances\n\n        :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the Alignak configuration file and return a list of the attributes that are set.", "response": "def parse(self):\n        # pylint: disable=too-many-branches\n        \"\"\"\n        Check if some extra configuration files are existing in an `alignak.d` sub directory\n        near the found configuration file.\n\n        Parse the Alignak configuration file(s)\n\n        Exit the script if some errors are encountered.\n\n        :return: True/False\n        \"\"\"\n        # Search if some ini files existe in an alignak.d sub-directory\n        sub_directory = 'alignak.d'\n        dir_name = os.path.dirname(self.configuration_file)\n        dir_name = os.path.join(dir_name, sub_directory)\n        self.cfg_files = [self.configuration_file]\n        if os.path.exists(dir_name):\n            for root, _, walk_files in os.walk(dir_name, followlinks=True):\n                for found_file in walk_files:\n                    if not re.search(r\"\\.ini$\", found_file):\n                        continue\n                    self.cfg_files.append(os.path.join(root, found_file))\n        print(\"Loading configuration files: %s \" % self.cfg_files)\n\n        # Read and parse the found configuration files\n        self.config = configparser.ConfigParser()\n        try:\n            self.config.read(self.cfg_files)\n            if self.config._sections == {}:\n                print(\"* bad formatted configuration file: %s \" % self.configuration_file)\n                if self.embedded:\n                    raise ValueError\n                sys.exit(2)\n\n            for section in self.config.sections():\n                if self.verbose:\n                    print(\"- section: %s\" % section)\n                for (key, value) in self.config.items(section):\n                    inner_property = \"%s.%s\" % (section, key)\n\n                    # Set object property\n                    setattr(self, inner_property, value)\n\n                    # Set environment variable\n                    os.environ[inner_property] = value\n\n                    if self.verbose:\n                        print(\"  %s = %s\" % (inner_property, value))\n\n                    if self.export:\n                        # Allowed shell variables may only contain: [a-zA-z0-9_]\n                        inner_property = re.sub('[^0-9a-zA-Z]+', '_', inner_property)\n                        inner_property = inner_property.upper()\n                        print(\"export %s=%s\" % (inner_property, cmd_quote(value)))\n        except configparser.ParsingError as exp:\n            print(\"* parsing error in config file : %s\\n%s\"\n                  % (self.configuration_file, exp.message))\n            if self.embedded:\n                return False\n            sys.exit(3)\n        except configparser.InterpolationMissingOptionError as exp:\n            print(\"* incorrect or missing variable: %s\" % str(exp))\n            if self.embedded:\n                return False\n            sys.exit(3)\n\n        if self.verbose:\n            print(\"Configuration file parsed correctly\")\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the Alignak configuration to a file", "response": "def write(self, env_file):\n        \"\"\"\n        Write the Alignak configuration to a file\n\n        :param env_file: file name to dump the configuration\n        :type env_file: str\n        :return: True/False\n        \"\"\"\n        try:\n            with open(env_file, \"w\") as out_file:\n                self.config.write(out_file)\n        except Exception as exp:  # pylint: disable=broad-except\n            print(\"Dumping environment file raised an error: %s. \" % exp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _search_sections(self, searched_sections=''):\n        found_sections = {}\n        # Get the daemons related properties\n        for section in self.config.sections():\n            if not section.startswith(searched_sections):\n                continue\n\n            if section not in found_sections:\n                found_sections.update({section: {'imported_from': self.configuration_file}})\n            for (key, value) in self.config.items(section):\n                found_sections[section].update({key: value})\n        return found_sections", "response": "Search the configuration sections in the configuration file which name starts with the search criteria\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the Alignak macros.", "response": "def get_alignak_macros(self):\n        \"\"\"\n        Get the Alignak macros.\n\n        :return: a dict containing the Alignak macros\n        \"\"\"\n        macros = self.get_alignak_configuration(macros=True)\n\n        sections = self._search_sections('pack.')\n        for name, _ in list(sections.items()):\n            section_macros = self.get_alignak_configuration(section=name, macros=True)\n            macros.update(section_macros)\n        return macros"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_alignak_configuration(self, section=SECTION_CONFIGURATION,\n                                  legacy_cfg=False, macros=False):\n        \"\"\"\n        Get the Alignak configuration parameters. All the variables included in\n        the SECTION_CONFIGURATION section except the variables starting with 'cfg'\n        and the macros.\n\n        If `lecagy_cfg` is True, this function only returns the variables included in\n        the SECTION_CONFIGURATION section except the variables starting with 'cfg'\n\n        If `macros` is True, this function only returns the variables included in\n        the SECTION_CONFIGURATION section that are considered as macros\n\n        :param section: name of the sectio nto search for\n        :type section: str\n        :param legacy_cfg: only get the legacy cfg declarations\n        :type legacy_cfg: bool\n        :param macros: only get the macros declarations\n        :type macros: bool\n        :return: a dict containing the Alignak configuration parameters\n        \"\"\"\n        configuration = self._search_sections(section)\n        if section not in configuration:\n            return []\n        for prop, _ in list(configuration[section].items()):\n            # Only legacy configuration items\n            if legacy_cfg:\n                if not prop.startswith('cfg'):\n                    configuration[section].pop(prop)\n                continue\n            # Only macro definitions\n            if macros:\n                if not prop.startswith('_') and not prop.startswith('$'):\n                    configuration[section].pop(prop)\n                continue\n            # All values except legacy configuration and macros\n            if prop.startswith('cfg') or prop.startswith('_') or prop.startswith('$'):\n                configuration[section].pop(prop)\n\n        return configuration[section]", "response": "Get the Alignak configuration parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_daemons(self, daemon_name=None, daemon_type=None):\n        if daemon_name is not None:\n            sections = self._search_sections('daemon.%s' % daemon_name)\n            if 'daemon.%s' % daemon_name in sections:\n                return sections['daemon.' + daemon_name]\n            return {}\n\n        if daemon_type is not None:\n            sections = self._search_sections('daemon.')\n            for name, daemon in list(sections.items()):\n                if 'type' not in daemon or not daemon['type'] == daemon_type:\n                    sections.pop(name)\n            return sections\n\n        return self._search_sections('daemon.')", "response": "Get the daemons configuration parameters"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the modules configuration parameters", "response": "def get_modules(self, name=None, daemon_name=None, names_only=True):\n        \"\"\"\n        Get the modules configuration parameters\n\n        If name is provided, get the configuration for this module, else,\n        If daemon_name is provided, get the configuration for all the modules of this daemon, else\n        get the configuration of all the modules.\n\n        :param name: the searched module name\n        :param daemon_name: the modules of this daemon\n        :param names_only: if True only returns the modules names, else all the configuration data\n        :return: a dict containing the module(s) configuration parameters\n        \"\"\"\n        if name is not None:\n            sections = self._search_sections('module.' + name)\n            if 'module.' + name in sections:\n                return sections['module.' + name]\n            return {}\n\n        if daemon_name is not None:\n            section = self.get_daemons(daemon_name)\n            if 'modules' in section and section['modules']:\n                modules = []\n                for module_name in section['modules'].split(','):\n                    if names_only:\n                        modules.append(module_name)\n                    else:\n                        modules.append(self.get_modules(name=module_name))\n                return modules\n            return []\n\n        return self._search_sections('module.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy_shell(self):\n        cls = self.__class__\n        new_i = cls()  # create a new group\n        new_i.uuid = self.uuid  # with the same id\n\n        # Copy all properties\n        for prop in cls.properties:\n            if hasattr(self, prop):\n                if prop in ['members', 'unknown_members']:\n                    setattr(new_i, prop, [])\n                else:\n                    setattr(new_i, prop, getattr(self, prop))\n\n        return new_i", "response": "Copy the group properties EXCEPT the members."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new member to the members list", "response": "def add_members(self, members):\n        \"\"\"Add a new member to the members list\n\n        :param members: member name\n        :type members: str\n        :return: None\n        \"\"\"\n        if not isinstance(members, list):\n            members = [members]\n\n        if not getattr(self, 'members', None):\n            self.members = members\n        else:\n            self.members.extend(members)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_unknown_members(self, members):\n        if not isinstance(members, list):\n            members = [members]\n\n        if not hasattr(self, 'unknown_members'):\n            self.unknown_members = members\n        else:\n            self.unknown_members.extend(members)", "response": "Add a new member to the unknown members list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_correct(self):\n        state = True\n\n        # Make members unique, remove duplicates\n        if self.members:\n            self.members = list(set(self.members))\n\n        if self.unknown_members:\n            for member in self.unknown_members:\n                msg = \"[%s::%s] as %s, got unknown member '%s'\" % (\n                    self.my_type, self.get_name(), self.__class__.my_type, member\n                )\n                self.add_error(msg)\n            state = False\n\n        return super(Itemgroup, self).is_correct() and state", "response": "Check if a group is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_initial_status_brok(self, extra=None):\n        # Here members is a list of identifiers and we need their names\n        if extra and isinstance(extra, Items):\n            members = []\n            for member_id in self.members:\n                member = extra[member_id]\n                members.append((member.uuid, member.get_name()))\n            extra = {'members': members}\n\n        return super(Itemgroup, self).get_initial_status_brok(extra=extra)", "response": "Get a brok with the group properties\n        members contains a list of uuid which we must provide the names."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_dir(self, dirname):\n        try:\n            os.makedirs(dirname)\n            dir_stat = os.stat(dirname)\n            print(\"Created the directory: %s, stat: %s\" % (dirname, dir_stat))\n            if not dir_stat.st_uid == self.uid:\n                os.chown(dirname, self.uid, self.gid)\n                os.chmod(dirname, 0o775)\n                dir_stat = os.stat(dirname)\n                print(\"Changed directory ownership and permissions: %s, stat: %s\"\n                      % (dirname, dir_stat))\n\n            self.pre_log.append((\"DEBUG\",\n                                 \"Daemon '%s' directory %s checking... \"\n                                 \"User uid: %s, directory stat: %s.\"\n                                 % (self.name, dirname, os.getuid(), dir_stat)))\n\n            self.pre_log.append((\"INFO\",\n                                 \"Daemon '%s' directory %s did not exist, I created it. \"\n                                 \"I set ownership for this directory to %s:%s.\"\n                                 % (self.name, dirname, self.user, self.group)))\n        except OSError as exp:\n            if exp.errno == errno.EEXIST and os.path.isdir(dirname):\n                # Directory still exists...\n                pass\n            else:\n                self.pre_log.append((\"ERROR\",\n                                     \"Daemon directory '%s' did not exist, \"\n                                     \"and I could not create. Exception: %s\"\n                                     % (dirname, exp)))\n                self.exit_on_error(\"Daemon directory '%s' did not exist, \"\n                                   \"and I could not create.'. Exception: %s\"\n                                   % (dirname, exp), exit_code=3)", "response": "Check and create a directory and return a new instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_stop(self):\n        logger.info(\"Stopping %s...\", self.name)\n\n        if self.sync_manager:\n            logger.info(\"Shutting down synchronization manager...\")\n            self.sync_manager.shutdown()\n            self.sync_manager = None\n\n        # Maybe the modules manager is not even created!\n        if self.modules_manager:\n            logger.info(\"Shutting down modules manager...\")\n            self.modules_manager.stop_all()\n\n        # todo: daemonize the process thanks to CherryPy plugin\n        if self.http_daemon:\n            logger.info(\"Shutting down HTTP daemon...\")\n            if self.http_daemon.cherrypy_thread:\n                self.http_daemon.stop()\n            self.http_daemon = None\n\n        # todo: daemonize the process thanks to CherryPy plugin\n        if self.http_thread:\n            logger.info(\"Checking HTTP thread...\")\n            # Let a few seconds to exit\n            self.http_thread.join(timeout=3)\n            if self.http_thread.is_alive():  # pragma: no cover, should never happen...\n                logger.warning(\"HTTP thread did not terminated. Force stopping the thread..\")\n                # try:\n                #     self.http_thread._Thread__stop()  # pylint: disable=E1101\n                # except Exception as exp:  # pylint: disable=broad-except\n                #     print(\"Exception: %s\" % exp)\n            else:\n                logger.debug(\"HTTP thread exited\")\n            self.http_thread = None", "response": "Execute the stop of the CherryPy process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_stop(self, message='', exit_code=0):\n        # Log an error message if exit code is not 0\n        # Force output to stderr\n        if exit_code:\n            if message:\n                logger.error(message)\n                try:\n                    sys.stderr.write(message)\n                except Exception:  # pylint: disable=broad-except\n                    pass\n            logger.error(\"Sorry, I bail out, exit code: %d\", exit_code)\n            try:\n                sys.stderr.write(\"Sorry, I bail out, exit code: %d\" % exit_code)\n            except Exception:  # pylint: disable=broad-except\n                pass\n        else:\n            if message:\n                logger.info(message)\n\n        self.unlink()\n        self.do_stop()\n\n        logger.info(\"Stopped %s.\", self.name)\n        sys.exit(exit_code)", "response": "Remove pid and stop daemon\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_links_of_type(self, s_type=''):\n        satellites = {\n            'arbiter': getattr(self, 'arbiters', []),\n            'scheduler': getattr(self, 'schedulers', []),\n            'broker': getattr(self, 'brokers', []),\n            'poller': getattr(self, 'pollers', []),\n            'reactionner': getattr(self, 'reactionners', []),\n            'receiver': getattr(self, 'receivers', [])\n        }\n        if not s_type:\n            result = {}\n            for sat_type in satellites:\n                # if sat_type == self.type:\n                #     continue\n                for sat_uuid in satellites[sat_type]:\n                    result[sat_uuid] = satellites[sat_type][sat_uuid]\n            return result\n        if s_type in satellites:\n            return satellites[s_type]\n\n        return None", "response": "Return the satellite list of the given type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing a connection with the daemon for the provided satellite link.", "response": "def daemon_connection_init(self, s_link, set_wait_new_conf=False):\n        \"\"\"Initialize a connection with the daemon for the provided satellite link\n\n        Initialize the connection (HTTP client) to the daemon and get its running identifier.\n        Returns True if it succeeds else if any error occur or the daemon is inactive\n        it returns False.\n\n        Assume the daemon should be reachable because we are initializing the connection...\n        as such, force set the link reachable property\n\n        If set_wait_new_conf is set, the daemon is requested to wait a new configuration if\n         we get a running identifier. This is used by the arbiter when a new configuration\n         must be dispatched\n\n        NB: if the daemon is configured as passive, or if it is a daemon link that is\n        inactive then it returns False without trying a connection.\n\n        :param s_link: link of the daemon to connect to\n        :type s_link: SatelliteLink\n        :param set_wait_new_conf: if the daemon must got the wait new configuration state\n        :type set_wait_new_conf: bool\n        :return: True if the connection is established, else False\n        \"\"\"\n        logger.debug(\"Daemon connection initialization: %s %s\", s_link.type, s_link.name)\n\n        # If the link is not not active, I do not try to initialize the connection, just useless ;)\n        if not s_link.active:\n            logger.warning(\"%s '%s' is not active, do not initialize its connection!\",\n                           s_link.type, s_link.name)\n            return False\n\n        # Create the daemon connection\n        s_link.create_connection()\n\n        # Get the connection running identifier - first client / server communication\n        logger.debug(\"[%s] Getting running identifier for '%s'\", self.name, s_link.name)\n        # Assume the daemon should be alive and reachable\n        # because we are initializing the connection...\n        s_link.alive = True\n        s_link.reachable = True\n        got_a_running_id = None\n        for _ in range(0, s_link.max_check_attempts):\n            got_a_running_id = s_link.get_running_id()\n            if got_a_running_id:\n                s_link.last_connection = time.time()\n                if set_wait_new_conf:\n                    s_link.wait_new_conf()\n                break\n            time.sleep(0.3)\n\n        return got_a_running_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_load_modules(self, modules):\n        _ts = time.time()\n        logger.info(\"Loading modules...\")\n\n        if self.modules_manager.load_and_init(modules):\n            if self.modules_manager.instances:\n                logger.info(\"I correctly loaded my modules: [%s]\",\n                            ','.join([inst.name for inst in self.modules_manager.instances]))\n            else:\n                logger.info(\"I do not have any module\")\n        else:  # pragma: no cover, not with unit tests...\n            logger.error(\"Errors were encountered when checking and loading modules:\")\n            for msg in self.modules_manager.configuration_errors:\n                logger.error(msg)\n\n        if self.modules_manager.configuration_warnings:  # pragma: no cover, not tested\n            for msg in self.modules_manager.configuration_warnings:\n                logger.warning(msg)\n        statsmgr.gauge('modules.count', len(modules))\n        statsmgr.timer('modules.load-time', time.time() - _ts)", "response": "Wrapper for calling load_and_init method of modules_manager attribute\nSetException"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump_environment(self):\n        # Dump the Alignak configuration to a temporary ini file\n        path = os.path.join(tempfile.gettempdir(),\n                            'dump-env-%s-%s-%d.ini' % (self.type, self.name, int(time.time())))\n\n        try:\n            with open(path, \"w\") as out_file:\n                self.alignak_env.write(out_file)\n        except Exception as exp:  # pylint: disable=broad-except\n            logger.error(\"Dumping daemon environment raised an error: %s. \", exp)", "response": "Dump the Alignak environment to a temporary ini file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef change_to_workdir(self):\n        logger.info(\"Changing working directory to: %s\", self.workdir)\n\n        self.check_dir(self.workdir)\n        try:\n            os.chdir(self.workdir)\n        except OSError as exp:\n            self.exit_on_error(\"Error changing to working directory: %s. Error: %s. \"\n                               \"Check the existence of %s and the %s/%s account \"\n                               \"permissions on this directory.\"\n                               % (self.workdir, str(exp), self.workdir, self.user, self.group),\n                               exit_code=3)\n        self.pre_log.append((\"INFO\", \"Using working directory: %s\" % os.path.abspath(self.workdir)))", "response": "Change the working directory to the working directory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unlink(self):\n        logger.debug(\"Unlinking %s\", self.pid_filename)\n        try:\n            os.unlink(self.pid_filename)\n        except OSError as exp:\n            logger.debug(\"Got an error unlinking our pid file: %s\", exp)", "response": "Remove the daemon s pid file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking the permissions of the current user and the user s shm directory.", "response": "def check_shm():\n        \"\"\" Check /dev/shm right permissions\n\n        :return: None\n        \"\"\"\n        import stat\n        shm_path = '/dev/shm'\n        if os.name == 'posix' and os.path.exists(shm_path):\n            # We get the access rights, and we check them\n            mode = stat.S_IMODE(os.lstat(shm_path)[stat.ST_MODE])\n            if not mode & stat.S_IWUSR or not mode & stat.S_IRUSR:\n                logger.critical(\"The directory %s is not writable or readable.\"\n                                \"Please make it read writable: %s\", shm_path, shm_path)\n                print(\"The directory %s is not writable or readable.\"\n                      \"Please make it read writable: %s\" % (shm_path, shm_path))\n                sys.exit(2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen the pid file in read or write mode.", "response": "def __open_pidfile(self, write=False):\n        \"\"\"Open pid file in read or write mod\n\n        :param write: boolean to open file in write mod (true = write)\n        :type write: bool\n        :return: None\n        \"\"\"\n        # if problem on opening or creating file it'll be raised to the caller:\n        try:\n            self.pre_log.append((\"DEBUG\",\n                                 \"Opening %s pid file: %s\" % ('existing' if\n                                                              os.path.exists(self.pid_filename)\n                                                              else 'missing', self.pid_filename)))\n            # Windows do not manage the rw+ mode,\n            # so we must open in read mode first, then reopen it write mode...\n            if not write and os.path.exists(self.pid_filename):\n                self.fpid = open(self.pid_filename, 'r+')\n            else:\n                # If it doesn't exist too, we create it as void\n                self.fpid = open(self.pid_filename, 'w+')\n        except Exception as exp:  # pylint: disable=broad-except\n            self.exit_on_error(\"Error opening pid file: %s. Error: %s. \"\n                               \"Check the %s:%s account permissions to write this file.\"\n                               % (self.pid_filename, str(exp), self.user, self.group), exit_code=3)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_parallel_run(self):  # pragma: no cover, not with unit tests...\n        # TODO: other daemon run on nt\n        if os.name == 'nt':  # pragma: no cover, not currently tested with Windows...\n            logger.warning(\"The parallel daemon check is not available on Windows\")\n            self.__open_pidfile(write=True)\n            return\n\n        # First open the pid file in open mode\n        self.__open_pidfile()\n        try:\n            pid_var = self.fpid.readline().strip(' \\r\\n')\n            if pid_var:\n                pid = int(pid_var)\n                logger.info(\"Found an existing pid (%s): '%s'\", self.pid_filename, pid_var)\n            else:\n                logger.debug(\"Not found an existing pid: %s\", self.pid_filename)\n                return\n        except (IOError, ValueError) as err:\n            logger.warning(\"PID file is empty or has an invalid content: %s\", self.pid_filename)\n            return\n\n        if pid == os.getpid():\n            self.pid = pid\n            return\n\n        try:\n            logger.debug(\"Testing if the process is running: '%s'\", pid)\n            os.kill(pid, 0)\n        except OSError:\n            # consider any exception as a stale pid file.\n            # this includes :\n            #  * PermissionError when a process with same pid exists but is executed by another user\n            #  * ProcessLookupError: [Errno 3] No such process\n            self.pre_log.append((\"DEBUG\", \"No former instance to replace\"))\n            logger.info(\"A stale pid file exists, reusing the same file\")\n            return\n\n        if not self.do_replace:\n            self.exit_on_error(\"A valid pid file still exists (pid=%s) and \"\n                               \"I am not allowed to replace. Exiting!\" % pid, exit_code=3)\n\n        self.pre_log.append((\"DEBUG\", \"Replacing former instance: %d\" % pid))\n        try:\n            pgid = os.getpgid(pid)\n            # SIGQUIT to terminate and dump core\n            os.killpg(pgid, signal.SIGQUIT)\n        except os.error as err:\n            if err.errno != errno.ESRCH:\n                raise\n\n        self.fpid.close()\n        # TODO: give some time to wait that previous instance finishes?\n        time.sleep(1)\n        # we must also reopen the pid file in write mode\n        # because the previous instance should have deleted it!!\n        self.__open_pidfile(write=True)", "response": "Check if the process is running and if yes kill it."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_pid(self, pid):\n        self.fpid.seek(0)\n        self.fpid.truncate()\n        self.fpid.write(\"%d\" % pid)\n        self.fpid.close()\n        del self.fpid", "response": "Write the pid to the pid file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close_fds(self, skip_close_fds):  # pragma: no cover, not with unit tests...\n        # First we manage the file descriptor, because debug file can be\n        # relative to pwd\n        max_fds = resource.getrlimit(resource.RLIMIT_NOFILE)[1]\n        if max_fds == resource.RLIM_INFINITY:\n            max_fds = 1024\n        self.pre_log.append((\"DEBUG\", \"Maximum file descriptors: %d\" % max_fds))\n\n        # Iterate through and close all file descriptors.\n        for file_d in range(0, max_fds):\n            if file_d in skip_close_fds:\n                self.pre_log.append((\"INFO\", \"Do not close fd: %s\" % file_d))\n                continue\n            try:\n                os.close(file_d)\n            except OSError:  # ERROR, fd wasn't open to begin with (ignored)\n                pass", "response": "Close all the process file descriptors."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngo in \"daemon\" mode: close unused fds, redirect stdout/err, chdir, umask, fork-setsid-fork-writepid Do the double fork to properly go daemon This is 'almost' as recommended by PEP3143 but it would be better to rewrite this daemonization thanks to the python-daemon library! :return: None", "response": "def daemonize(self):  # pragma: no cover, not for unit tests...\n        \"\"\"Go in \"daemon\" mode: close unused fds, redirect stdout/err,\n        chdir, umask, fork-setsid-fork-writepid\n        Do the double fork to properly go daemon\n\n        This is 'almost' as recommended by PEP3143 but it would be better to rewrite this\n        daemonization thanks to the python-daemon library!\n\n        :return: None\n        \"\"\"\n        self.pre_log.append((\"INFO\", \"Daemonizing...\"))\n        print(\"Daemonizing %s...\" % self.name)\n\n        # Set umask\n        os.umask(UMASK)\n\n        # Close all file descriptors except the one we need\n        self.pre_log.append((\"DEBUG\", \"Closing file descriptors...\"))\n        preserved_fds = [1, 2, self.fpid.fileno()]\n        if os.getenv('ALIGNAK_DO_NOT_PRESERVE_STDOUT', None):\n            preserved_fds = [self.fpid.fileno()]\n        if self.debug:\n            # Do not close stdout nor stderr\n            preserved_fds.extend([1, 2])\n        self.close_fds(preserved_fds)\n\n        # Now the double fork magic (fork/setsid/fork)\n        def fork_then_exit_parent(level, error_message):\n            \"\"\" Fork a child process, then exit the parent process.\n                :param error_message: Message for the exception in case of a\n                    detach failure.\n                :return: ``None``.\n                :raise Exception: If the fork fails.\n                \"\"\"\n            try:\n                pid = os.fork()\n                if pid > 0:\n                    if level == 2:\n                        # When forking the grandchild, write our own pid\n                        self.write_pid(pid)\n                    os._exit(0)\n            except OSError as exc:\n                raise Exception(\"Fork error: %s [%d], exception: %s\"\n                                % (error_message, exc.errno, str(exc)))\n\n        fork_then_exit_parent(level=1, error_message=\"Failed first fork\")\n        os.setsid()\n        fork_then_exit_parent(level=2, error_message=\"Failed second fork\")\n\n        self.pid = os.getpid()\n        self.pre_log.append((\"INFO\", \"We are now fully daemonized :) pid=%d\" % self.pid))\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_to_user_group(self):\n        # TODO: change user on nt\n        if os.name == 'nt':  # pragma: no cover, no Windows implementation currently\n            logger.warning(\"You can't change user on this system\")\n            return\n\n        if (self.user == 'root' or self.group == 'root') and not self.idontcareaboutsecurity:\n            logger.error(\"You want the application to run with the root account credentials? \"\n                         \"It is not a safe configuration!\")\n            logger.error(\"If you really want it, set: 'idontcareaboutsecurity=1' \"\n                         \"in the configuration file\")\n            self.exit_on_error(\"You want the application to run with the root account credentials? \"\n                               \"It is not a safe configuration! If you really want it, \"\n                               \"set: 'idontcareaboutsecurity=1' in the configuration file.\",\n                               exit_code=3)\n\n        uid = None\n        try:\n            uid = getpwnam(self.user).pw_uid\n        except KeyError:\n            logger.error(\"The required user %s is unknown\", self.user)\n\n        gid = None\n        try:\n            gid = getgrnam(self.group).gr_gid\n        except KeyError:\n            logger.error(\"The required group %s is unknown\", self.group)\n\n        if uid is None or gid is None:\n            self.exit_on_error(\"Configured user/group (%s/%s) are not valid.\"\n                               % (self.user, self.group), exit_code=1)\n\n        # Maybe the os module got the initgroups function. If so, try to call it.\n        # Do this when we are still root\n        logger.info('Trying to initialize additional groups for the daemon')\n        if hasattr(os, 'initgroups'):\n            try:\n                os.initgroups(self.user, gid)\n            except OSError as err:\n                logger.warning('Cannot call the additional groups setting with initgroups: %s',\n                               err.strerror)\n        elif hasattr(os, 'setgroups'):  # pragma: no cover, not with unit tests on Travis\n            # Else try to call the setgroups if it exists...\n            groups = [gid] + \\\n                     [group.gr_gid for group in get_all_groups() if self.user in group.gr_mem]\n            try:\n                os.setgroups(groups)\n            except OSError as err:\n                logger.warning('Cannot call the additional groups setting with setgroups: %s',\n                               err.strerror)\n        try:\n            # First group, then user :)\n            os.setregid(gid, gid)\n            os.setreuid(uid, uid)\n        except OSError as err:  # pragma: no cover, not with unit tests...\n            self.exit_on_error(\"Cannot change user/group to %s/%s (%s [%d]). Exiting...\"\n                               % (self.user, self.group, err.strerror, err.errno), exit_code=3)", "response": "Change the user and group for the running program."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmanage signals caught by the daemon", "response": "def manage_signal(self, sig, frame):  # pylint: disable=unused-argument\n        \"\"\"Manage signals caught by the daemon\n        signal.SIGUSR1 : dump_environment\n        signal.SIGUSR2 : dump_object (nothing)\n        signal.SIGTERM, signal.SIGINT : terminate process\n\n        :param sig: signal caught by daemon\n        :type sig: str\n        :param frame: current stack frame\n        :type frame:\n        :return: None\n        \"\"\"\n        logger.info(\"received a signal: %s\", SIGNALS_TO_NAMES_DICT[sig])\n        if sig == signal.SIGUSR1:  # if USR1, ask a memory dump\n            self.need_dump_environment = True\n        elif sig == signal.SIGUSR2:  # if USR2, ask objects dump\n            self.need_objects_dump = True\n        elif sig == signal.SIGHUP:  # if HUP, reload the monitoring configuration\n            self.need_config_reload = True\n        else:  # Ok, really ask us to die :)\n            logger.info(\"request to stop the daemon\")\n            self.interrupted = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_proctitle(self, daemon_name=None):\n        logger.debug(\"Setting my process name: %s\", daemon_name)\n        if daemon_name:\n            setproctitle(\"alignak-%s %s\" % (self.type, daemon_name))\n            if self.modules_manager:\n                self.modules_manager.set_daemon_name(daemon_name)\n        else:\n            setproctitle(\"alignak-%s\" % self.type)", "response": "Set the proctitle of the daemon Arc."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the log file header.", "response": "def get_header(self, configuration=False):\n        \"\"\"Get the log file header\n\n        If configuration is True, this returns the daemon configuration\n\n        :return: A string list containing project name, daemon name, version, licence etc.\n        :rtype: list\n        \"\"\"\n        header = [u\"-----\",\n                  u\"   \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557     \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557  \u2588\u2588\u2557\",\n                  u\"  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2554\u255d\",\n                  u\"  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \",\n                  u\"  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \",\n                  u\"  \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2557\",\n                  u\"  \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\",\n                  u\"-----\",\n                  u\"Alignak %s - %s daemon\" % (VERSION, self.name),\n                  u\"Copyright (c) 2015-2019: Alignak Team\",\n                  u\"License: AGPL\",\n                  u\"-----\",\n                  u\"Python: %s.%s\" % (sys.version_info.major, sys.version_info.minor),\n                  u\"-----\",\n                  u\"My pid: %s\" % self.pid]\n\n        if configuration:\n            header = [\"My configuration: \"]\n            for prop, _ in sorted(self.properties.items()):\n                header.append(\" - %s=%s\" % (prop, getattr(self, prop)))\n\n        return header"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef http_daemon_thread(self):\n        logger.debug(\"HTTP thread running\")\n        try:\n            # This function is a blocking function serving HTTP protocol\n            self.http_daemon.run()\n        except PortNotFree as exp:\n            logger.exception('The HTTP daemon port is not free: %s', exp)\n            raise\n        except Exception as exp:  # pylint: disable=broad-except\n            self.exit_on_exception(exp)\n        logger.debug(\"HTTP thread exiting\")", "response": "Main function of the HTTP daemon thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait up to timeout and check for system time change.", "response": "def make_a_pause(self, timeout=0.0001, check_time_change=True):\n        \"\"\" Wait up to timeout and check for system time change.\n\n        This function checks if the system time changed since the last call. If so,\n        the difference is returned to the caller.\n        The duration of this call is removed from the timeout. If this duration is\n        greater than the required timeout, no sleep is executed and the extra time\n        is returned to the caller\n\n        If the required timeout was overlapped, then the first return value will be\n        greater than the required timeout.\n\n        If the required timeout is null, then the timeout value is set as a very short time\n        to keep a nice behavior to the system CPU ;)\n\n        :param timeout: timeout to wait for activity\n        :type timeout: float\n        :param check_time_change: True (default) to check if the system time changed\n        :type check_time_change: bool\n        :return:Returns a 2-tuple:\n        * first value is the time spent for the time change check\n        * second value is the time change difference\n        :rtype: tuple\n        \"\"\"\n        if timeout == 0:\n            timeout = 0.0001\n\n        if not check_time_change:\n            # Time to sleep\n            time.sleep(timeout)\n            self.sleep_time += timeout\n            return 0, 0\n\n        # Check is system time changed\n        before = time.time()\n        time_changed = self.check_for_system_time_change()\n        after = time.time()\n        elapsed = after - before\n\n        if elapsed > timeout:\n            return elapsed, time_changed\n        # Time to sleep\n        time.sleep(timeout - elapsed)\n\n        # Increase our sleep time for the time we slept\n        before += time_changed\n        self.sleep_time += time.time() - before\n\n        return elapsed, time_changed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting for initial configuration from the arbiter.", "response": "def wait_for_initial_conf(self, timeout=1.0):\n        \"\"\"Wait initial configuration from the arbiter.\n        Basically sleep 1.0 and check if new_conf is here\n\n        :param timeout: timeout to wait\n        :type timeout: int\n        :return: None\n        \"\"\"\n        logger.info(\"Waiting for initial configuration\")\n        # Arbiter do not already set our have_conf param\n        _ts = time.time()\n        while not self.new_conf and not self.interrupted:\n            # Make a pause and check if the system time changed\n            _, _ = self.make_a_pause(timeout, check_time_change=True)\n\n        if not self.interrupted:\n            logger.info(\"Got initial configuration, waited for: %.2f seconds\", time.time() - _ts)\n            statsmgr.timer('configuration.initial', time.time() - _ts)\n        else:\n            logger.info(\"Interrupted before getting the initial configuration\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef watch_for_new_conf(self, timeout=0):\n        logger.debug(\"Watching for a new configuration, timeout: %s\", timeout)\n        self.make_a_pause(timeout=timeout, check_time_change=False)\n        return any(self.new_conf)", "response": "Check if a new configuration was posted to the daemon"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hook_point(self, hook_name, handle=None):\n        full_hook_name = 'hook_' + hook_name\n        for module in self.modules_manager.instances:\n            _ts = time.time()\n            if not hasattr(module, full_hook_name):\n                continue\n\n            fun = getattr(module, full_hook_name)\n            try:\n                fun(handle if handle is not None else self)\n            # pylint: disable=broad-except\n            except Exception as exp:  # pragma: no cover, never happen during unit tests...\n                logger.warning('The instance %s raised an exception %s. I disabled it,'\n                               ' and set it to restart later', module.name, str(exp))\n                logger.exception('Exception %s', exp)\n                self.modules_manager.set_to_restart(module)\n            else:\n                statsmgr.timer('hook.%s.%s' % (hook_name, module.name), time.time() - _ts)", "response": "This function is called by the daemon to call the function that may define a hook function for hook_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_id(self, details=False):  # pylint: disable=unused-argument\n        # Modules information\n        res = {\n            \"alignak\": getattr(self, 'alignak_name', 'unknown'),\n            \"type\": getattr(self, 'type', 'unknown'),\n            \"name\": getattr(self, 'name', 'unknown'),\n            \"version\": VERSION\n        }\n        return res", "response": "Get the daemon identification information"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_daemon_stats(self, details=False):  # pylint: disable=unused-argument\n        res = self.get_id()\n        res.update({\n            \"program_start\": self.program_start,\n            \"spare\": self.spare,\n            'counters': {},\n            'metrics': [],\n            'modules': {\n                'internal': {}, 'external': {}\n            }\n        })\n\n        # Modules information\n        modules = res['modules']\n        counters = res['counters']\n        counters['modules'] = len(self.modules_manager.instances)\n        # first get data for all internal modules\n        for instance in self.modules_manager.get_internal_instances():\n            state = {True: 'ok', False: 'stopped'}[(instance\n                                                    not in self.modules_manager.to_restart)]\n            modules['internal'][instance.name] = {'name': instance.name, 'state': state}\n        # Same but for external ones\n        for instance in self.modules_manager.get_external_instances():\n            state = {True: 'ok', False: 'stopped'}[(instance\n                                                    not in self.modules_manager.to_restart)]\n            modules['internal'][instance.name] = {'name': instance.name, 'state': state}\n\n        return res", "response": "Get state of modules and create a scheme for stats data of daemon modules"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exit_ok(self, message, exit_code=None):\n        logger.info(\"Exiting...\")\n        if message:\n            logger.info(\"-----\")\n            logger.error(\"Exit message: %s\", message)\n            logger.info(\"-----\")\n\n        self.request_stop()\n\n        if exit_code is not None:\n            exit(exit_code)", "response": "Log a message and exit with the provided exit code."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exit_on_error(self, message, exit_code=1):\n        # pylint: disable=no-self-use\n        \"\"\"Log generic message when getting an error and exit\n\n        :param exit_code: if not None, exit with the provided value as exit code\n        :type exit_code: int\n        :param message: message for the exit reason\n        :type message: str\n        :return: None\n        \"\"\"\n        log = \"I got an unrecoverable error. I have to exit.\"\n        if message:\n            log += \"\\n-----\\nError message: %s\" % message\n            print(\"Error message: %s\" % message)\n        log += \"-----\\n\"\n        log += \"You can get help at https://github.com/Alignak-monitoring/alignak\\n\"\n        log += \"If you think this is a bug, create a new issue including as much \" \\\n               \"details as possible (version, configuration,...)\"\n        if exit_code is not None:\n            exit(exit_code)", "response": "Log generic message when getting an unrecoverable error and exit with the provided value as exit code."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exit_on_exception(self, raised_exception, message='', exit_code=99):\n        self.exit_on_error(message=message, exit_code=None)\n\n        logger.critical(\"-----\\nException: %s\\nBack trace of the error:\\n%s\",\n                        str(raised_exception), traceback.format_exc())\n\n        exit(exit_code)", "response": "Log generic message when getting an unrecoverable error"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets objects from queues and add them to the internal list.", "response": "def get_objects_from_from_queues(self):\n        \"\"\" Get objects from \"from\" queues and add them.\n\n        :return: True if we got something in the queue, False otherwise.\n        :rtype: bool\n        \"\"\"\n        _t0 = time.time()\n        had_some_objects = False\n        for module in self.modules_manager.get_external_instances():\n            queue = module.from_q\n            if not queue:\n                continue\n            while True:\n                queue_size = queue.qsize()\n                if queue_size:\n                    statsmgr.gauge('queues.from.%s.count' % module.get_name(), queue_size)\n                try:\n                    obj = queue.get_nowait()\n                except Full:\n                    logger.warning(\"Module %s from queue is full\", module.get_name())\n                except Empty:\n                    break\n                except (IOError, EOFError) as exp:\n                    logger.warning(\"Module %s from queue is no more available: %s\",\n                                   module.get_name(), str(exp))\n                except Exception as exp:  # pylint: disable=broad-except\n                    logger.error(\"An external module queue got a problem '%s'\", str(exp))\n                else:\n                    had_some_objects = True\n                    self.add(obj)\n        statsmgr.timer('queues.time', time.time() - _t0)\n\n        return had_some_objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset ref in scheduled downtime and raise downtime log entry", "response": "def enter(self, timeperiods, hosts, services):\n        \"\"\"Set ref in scheduled downtime and raise downtime log entry (start)\n\n        :param hosts: hosts objects to get item ref\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects to get item ref\n        :type services: alignak.objects.service.Services\n        :return: broks\n        :rtype: list of broks\n        \"\"\"\n        if self.ref in hosts:\n            item = hosts[self.ref]\n        else:\n            item = services[self.ref]\n        broks = []\n        self.is_in_effect = True\n        if self.fixed is False:\n            now = time.time()\n            self.real_end_time = now + self.duration\n        item.scheduled_downtime_depth += 1\n        item.in_scheduled_downtime = True\n        if item.scheduled_downtime_depth == 1:\n            item.raise_enter_downtime_log_entry()\n            notification_period = None\n            if getattr(item, 'notification_period', None) is not None:\n                notification_period = timeperiods[item.notification_period]\n            # Notification author data\n            # todo: note that alias and name are not implemented yet\n            author_data = {\n                'author': self.author, 'author_name': u'Not available',\n                'author_alias': u'Not available', 'author_comment': self.comment\n            }\n            item.create_notifications('DOWNTIMESTART', notification_period, hosts, services,\n                                      author_data=author_data)\n            if self.ref in hosts:\n                broks.append(self.get_raise_brok(item.get_name()))\n\n                # For an host, acknowledge the host problem (and its services problems)\n                # Acknowledge the host with a sticky ack and notifications\n                # The acknowledge will expire at the same time as the downtime end\n                item.acknowledge_problem(notification_period, hosts, services, 2, 1, \"Alignak\",\n                                         \"Acknowledged because of an host downtime\")\n            else:\n                broks.append(self.get_raise_brok(item.host_name, item.get_name()))\n        for downtime_id in self.activate_me:\n            for host in hosts:\n                if downtime_id in host.downtimes:\n                    downtime = host.downtimes[downtime_id]\n                    broks.extend(downtime.enter(timeperiods, hosts, services))\n            for service in services:\n                if downtime_id in service.downtimes:\n                    downtime = service.downtimes[downtime_id]\n                    broks.extend(downtime.enter(timeperiods, hosts, services))\n        return broks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exit(self, timeperiods, hosts, services):\n        if self.ref in hosts:\n            item = hosts[self.ref]\n        else:\n            item = services[self.ref]\n\n        broks = []\n        # If not is_in_effect means that ot was probably a flexible downtime which was\n        # not triggered. In this case, nothing special to do...\n        if self.is_in_effect is True:\n            # This was a fixed or a flexible+triggered downtime\n            self.is_in_effect = False\n            item.scheduled_downtime_depth -= 1\n            if item.scheduled_downtime_depth == 0:\n                item.raise_exit_downtime_log_entry()\n                notification_period = timeperiods[item.notification_period]\n                # Notification author data\n                # todo: note that alias and name are not implemented yet\n                author_data = {\n                    'author': self.author, 'author_name': u'Not available',\n                    'author_alias': u'Not available', 'author_comment': self.comment\n                }\n                item.create_notifications(u'DOWNTIMEEND', notification_period, hosts, services,\n                                          author_data=author_data)\n                item.in_scheduled_downtime = False\n                if self.ref in hosts:\n                    broks.append(self.get_expire_brok(item.get_name()))\n                else:\n                    broks.append(self.get_expire_brok(item.host_name, item.get_name()))\n\n        item.del_comment(self.comment_id)\n        self.can_be_deleted = True\n\n        # when a downtime ends and the concerned item was a problem\n        # a notification should be sent with the next critical check\n\n        # So we should set a flag here which informs the consume_result function\n        # to send a notification\n        item.in_scheduled_downtime_during_last_check = True\n        return broks", "response": "Remove ref in scheduled downtime and raise downtime log entry"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncanceling the downtime of this item", "response": "def cancel(self, timeperiods, hosts, services):\n        \"\"\"Remove ref in scheduled downtime and raise downtime log entry (cancel)\n\n        :param hosts: hosts objects to get item ref\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects to get item ref\n        :type services: alignak.objects.service.Services\n        :return: [], always\n        :rtype: list\n        \"\"\"\n        if self.ref in hosts:\n            item = hosts[self.ref]\n        else:\n            item = services[self.ref]\n        broks = []\n        self.is_in_effect = False\n        item.scheduled_downtime_depth -= 1\n        if item.scheduled_downtime_depth == 0:\n            item.raise_cancel_downtime_log_entry()\n            item.in_scheduled_downtime = False\n            if self.ref in hosts:\n                broks.append(self.get_expire_brok(item.get_name()))\n            else:\n                broks.append(self.get_expire_brok(item.host_name, item.get_name()))\n        self.del_automatic_comment(item)\n        self.can_be_deleted = True\n        item.in_scheduled_downtime_during_last_check = True\n        # Nagios does not notify on canceled downtimes\n        # res.extend(self.ref.create_notifications('DOWNTIMECANCELLED'))\n        # Also cancel other downtimes triggered by me\n        for downtime in self.activate_me:\n            broks.extend(downtime.cancel(timeperiods, hosts, services))\n        return broks"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding automatic comment on a specific resource for downtime.", "response": "def add_automatic_comment(self, ref):\n        \"\"\"Add comment on ref for downtime\n\n        :param ref: the host/service we want to link a comment to\n        :type ref: alignak.objects.schedulingitem.SchedulingItem\n\n        :return: None\n        \"\"\"\n        if self.fixed is True:\n            text = (DOWNTIME_FIXED_MESSAGE % (ref.my_type,\n                                              time.strftime(\"%Y-%m-%d %H:%M:%S\",\n                                                            time.localtime(self.start_time)),\n                                              time.strftime(\"%Y-%m-%d %H:%M:%S\",\n                                                            time.localtime(self.end_time)),\n                                              ref.my_type))\n        else:\n            hours, remainder = divmod(self.duration, 3600)\n            minutes, _ = divmod(remainder, 60)\n            text = (DOWNTIME_FLEXIBLE_MESSAGE % (ref.my_type,\n                                                 time.strftime(\"%Y-%m-%d %H:%M:%S\",\n                                                               time.localtime(self.start_time)),\n                                                 time.strftime(\"%Y-%m-%d %H:%M:%S\",\n                                                               time.localtime(self.end_time)),\n                                                 hours, minutes, ref.my_type))\n\n        data = {\n            'comment': text,\n            'comment_type': 1 if ref.my_type == 'host' else 2,\n            'entry_type': 2,\n            'source': 0,\n            'expires': False,\n            'ref': ref.uuid\n        }\n        comment = Comment(data)\n        self.comment_id = comment.uuid\n        ref.comments[comment.uuid] = comment\n        return comment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a start downtime brok with wanted data", "response": "def get_raise_brok(self, host_name, service_name=''):\n        \"\"\"Get a start downtime brok\n\n        :param host_name: host concerned by the downtime\n        :type host_name\n        :param service_name: service concerned by the downtime\n        :type service_name\n        :return: brok with wanted data\n        :rtype: alignak.brok.Brok\n        \"\"\"\n        data = self.serialize()\n        data['host'] = host_name\n        if service_name != '':\n            data['service'] = service_name\n\n        return Brok({'type': 'downtime_raise', 'data': data})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets an expire downtime brok with wanted data", "response": "def get_expire_brok(self, host_name, service_name=''):\n        \"\"\"Get an expire downtime brok\n\n        :param host_name: host concerned by the downtime\n        :type host_name\n        :param service_name: service concerned by the downtime\n        :type service_name\n        :return: brok with wanted data\n        :rtype: alignak.brok.Brok\n        \"\"\"\n        data = self.serialize()\n        data['host'] = host_name\n        if service_name != '':\n            data['service'] = service_name\n\n        return Brok({'type': 'downtime_expire', 'data': data})"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds properties to data if fill_brok of these class properties is same as brok_type", "response": "def fill_data_brok_from(self, data, brok_type):\n        \"\"\"\n        Add properties to data if fill_brok of these class properties\n        is same as brok_type\n\n        :param data: dictionnary of this command\n        :type data: dict\n        :param brok_type: type of brok\n        :type brok_type: str\n        :return: None\n        \"\"\"\n        cls = self.__class__\n        # Now config properties\n        for prop, entry in list(cls.properties.items()):\n            # Is this property intended for broking?\n            # if 'fill_brok' in entry[prop]:\n            if brok_type in entry.fill_brok:\n                if hasattr(self, prop):\n                    data[prop] = getattr(self, prop)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if this object configuration is correct.", "response": "def is_correct(self):\n        \"\"\"Check if this object configuration is correct ::\n\n        * Check our own specific properties\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        state = True\n\n        # _internal_host_check is for having an host check result\n        # without running a check plugin\n        if self.command_name.startswith('_internal_host_check'):\n            # Command line may contain: [state_id][;output]\n            parameters = self.command_line.split(';')\n            if len(parameters) < 2:\n                self.command_name = \"_internal_host_check;0;Host assumed to be UP\"\n                self.add_warning(\"[%s::%s] has no defined state nor output. Changed to %s\"\n                                 % (self.my_type, self.command_name, self.command_name))\n            elif len(parameters) < 3:\n                state = 3\n                try:\n                    state = int(parameters[1])\n                except ValueError:\n                    self.add_warning(\"[%s::%s] required a non integer state: %s. Using 3.\"\n                                     % (self.my_type, self.command_name, parameters[1]))\n\n                if state > 4:\n                    self.add_warning(\"[%s::%s] required an impossible state: %d. Using 3.\"\n                                     % (self.my_type, self.command_name, state))\n\n                output = {0: \"UP\", 1: \"DOWN\", 2: \"DOWN\", 3: \"UNKNOWN\", 4: \"UNREACHABLE\", }[state]\n                self.command_name = \"_internal_host_check;Host assumed to be %s\" % output\n\n                self.add_warning(\"[%s::%s] has no defined output. Changed to %s\"\n                                 % (self.my_type, self.command_name, self.command_name))\n            elif len(parameters) > 3:\n                self.command_name = \"%s;%s;%s\" % (parameters[0], parameters[1], parameters[2])\n\n                self.add_warning(\"[%s::%s] has too many parameters. Changed to %s\"\n                                 % (self.my_type, self.command_name, self.command_name))\n\n        return super(Command, self).is_correct() and state"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_name(self):\n        return getattr(self, 'dependent_host_name', '') + '/'\\\n            + getattr(self, 'dependent_service_description', '') \\\n            + '..' + getattr(self, 'host_name', '') + '/' \\\n            + getattr(self, 'service_description', '')", "response": "Get name based on 4 class attributes\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_service_dependency(self, dep_host_name, dep_service_description,\n                               par_host_name, par_service_description):\n        \"\"\"Instantiate and add a Servicedependency object to the items dict::\n\n        * notification criteria is \"u,c,w\"\n        * inherits_parent is True\n\n        :param dep_host_name: dependent host name\n        :type dep_host_name: str\n        :param dep_service_description: dependent service description\n        :type dep_service_description: str\n        :param par_host_name: host name\n        :type par_host_name: str\n        :param par_service_description: service description\n        :type par_service_description: str\n        :return: None\n        \"\"\"\n        # We create a \"standard\" service_dep\n        prop = {\n            'dependent_host_name':           dep_host_name,\n            'dependent_service_description': dep_service_description,\n            'host_name':                     par_host_name,\n            'service_description':           par_service_description,\n            'notification_failure_criteria': 'u,c,w',\n            'inherits_parent': '1',\n        }\n        servicedep = Servicedependency(prop)\n        self.add_item(servicedep)", "response": "Add a Servicedependency object to the items dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef explode_hostgroup(self, svc_dep, hostgroups):\n        # pylint: disable=too-many-locals\n        \"\"\"Explode a service dependency for each member of hostgroup\n\n        :param svc_dep: service dependency to explode\n        :type svc_dep: alignak.objects.servicedependency.Servicedependency\n        :param hostgroups: used to find hostgroup objects\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :return:None\n        \"\"\"\n        # We will create a service dependency for each host part of the host group\n\n        # First get services\n        snames = [d.strip() for d in svc_dep.service_description.split(',')]\n\n        # And dep services\n        dep_snames = [d.strip() for d in svc_dep.dependent_service_description.split(',')]\n\n        # Now for each host into hostgroup we will create a service dependency object\n        hg_names = [n.strip() for n in svc_dep.hostgroup_name.split(',')]\n        for hg_name in hg_names:\n            hostgroup = hostgroups.find_by_name(hg_name)\n            if hostgroup is None:\n                err = \"ERROR: the servicedependecy got an unknown hostgroup_name '%s'\" % hg_name\n                self.add_error(err)\n                continue\n            hnames = []\n            hnames.extend([m.strip() for m in hostgroup.get_hosts()])\n            for hname in hnames:\n                for dep_sname in dep_snames:\n                    for sname in snames:\n                        new_sd = svc_dep.copy()\n                        new_sd.host_name = hname\n                        new_sd.service_description = sname\n                        new_sd.dependent_host_name = hname\n                        new_sd.dependent_service_description = dep_sname\n                        self.add_item(new_sd)", "response": "Explode a service dependency for each member of hostgroup into a service dependency object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexploding all services dependency for each member of hostgroups", "response": "def explode(self, hostgroups):\n        # pylint: disable=too-many-locals, too-many-branches\n        \"\"\"Explode all service dependency for each member of hostgroups\n        Each member of dependent hostgroup or hostgroup in dependency have to get a copy of\n        service dependencies (quite complex to parse)\n\n        :param hostgroups: used to look for hostgroup\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :return: None\n        \"\"\"\n        # The \"old\" services will be removed. All services with\n        # more than one host or a host group will be in it\n        srvdep_to_remove = []\n\n        # Then for every host create a copy of the service with just the host\n        # because we are adding services, we can't just loop in it\n        servicedeps = list(self.items.keys())\n        for s_id in servicedeps:\n            servicedep = self.items[s_id]\n\n            # First case: we only have to propagate the services dependencies to all the hosts\n            # of some hostgroups\n            # Either a specific property is defined (Shinken) or no dependent hosts groups\n            # is defined\n            if bool(getattr(servicedep, 'explode_hostgroup', 0)) or \\\n                    (hasattr(servicedep, 'hostgroup_name') and\n                     not hasattr(servicedep, 'dependent_hostgroup_name')):\n                self.explode_hostgroup(servicedep, hostgroups)\n                srvdep_to_remove.append(s_id)\n                continue\n\n            # Get the list of all FATHER hosts and service dependenciess\n            hnames = []\n            if hasattr(servicedep, 'hostgroup_name'):\n                hg_names = [n.strip() for n in servicedep.hostgroup_name.split(',')]\n                hg_names = [hg_name.strip() for hg_name in hg_names]\n                for hg_name in hg_names:\n                    hostgroup = hostgroups.find_by_name(hg_name)\n                    if hostgroup is None:\n                        err = \"ERROR: the servicedependecy got an\" \\\n                              \" unknown hostgroup_name '%s'\" % hg_name\n                        hostgroup.add_error(err)\n                        continue\n                    hnames.extend([m.strip() for m in hostgroup.get_hosts()])\n\n            if not hasattr(servicedep, 'host_name'):\n                servicedep.host_name = ''\n\n            if servicedep.host_name != '':\n                hnames.extend([n.strip() for n in servicedep.host_name.split(',')])\n            snames = [d.strip() for d in servicedep.service_description.split(',')]\n            couples = []\n            for hname in hnames:\n                for sname in snames:\n                    couples.append((hname.strip(), sname.strip()))\n\n            if not hasattr(servicedep, 'dependent_hostgroup_name') \\\n                    and hasattr(servicedep, 'hostgroup_name'):\n                servicedep.dependent_hostgroup_name = servicedep.hostgroup_name\n\n            # Now the dependent part (the sons)\n            dep_hnames = []\n            if hasattr(servicedep, 'dependent_hostgroup_name'):\n                hg_names = [n.strip() for n in servicedep.dependent_hostgroup_name.split(',')]\n                hg_names = [hg_name.strip() for hg_name in hg_names]\n                for hg_name in hg_names:\n                    hostgroup = hostgroups.find_by_name(hg_name)\n                    if hostgroup is None:\n                        err = \"ERROR: the servicedependecy got an \" \\\n                              \"unknown dependent_hostgroup_name '%s'\" % hg_name\n                        hostgroup.add_error(err)\n                        continue\n                    dep_hnames.extend([m.strip() for m in hostgroup.get_hosts()])\n\n            if not hasattr(servicedep, 'dependent_host_name'):\n                servicedep.dependent_host_name = getattr(servicedep, 'host_name', '')\n\n            if servicedep.dependent_host_name != '':\n                dep_hnames.extend([n.strip() for n in servicedep.dependent_host_name.split(',')])\n            dep_snames = [d.strip() for d in servicedep.dependent_service_description.split(',')]\n            dep_couples = []\n            for dep_hname in dep_hnames:\n                for dep_sname in dep_snames:\n                    dep_couples.append((dep_hname.strip(), dep_sname.strip()))\n\n            # Create the new service deps from all of this.\n            for (dep_hname, dep_sname) in dep_couples:  # the sons, like HTTP\n                for (hname, sname) in couples:  # the fathers, like MySQL\n                    new_sd = servicedep.copy()\n                    new_sd.host_name = hname\n                    new_sd.service_description = sname\n                    new_sd.dependent_host_name = dep_hname\n                    new_sd.dependent_service_description = dep_sname\n                    self.add_item(new_sd)\n                # Ok so we can remove the old one\n                srvdep_to_remove.append(s_id)\n\n        self.delete_servicesdep_by_id(srvdep_to_remove)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef linkify(self, hosts, services, timeperiods):\n        self.linkify_sd_by_s(hosts, services)\n        self.linkify_sd_by_tp(timeperiods)\n        self.linkify_s_by_sd(services)", "response": "Create link between objects. hosts. Services. Timeperiods."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreplaces dependent_service_description and service_description in service dependency by the real object.", "response": "def linkify_sd_by_s(self, hosts, services):\n        \"\"\"Replace dependent_service_description and service_description\n        in service dependency by the real object\n\n        :param hosts: host list, used to look for a specific one\n        :type hosts: alignak.objects.host.Hosts\n        :param services: service list to look for a specific one\n        :type services: alignak.objects.service.Services\n        :return: None\n        \"\"\"\n        to_del = []\n        errors = self.configuration_errors\n        warns = self.configuration_warnings\n        for servicedep in self:\n            try:\n                s_name = servicedep.dependent_service_description\n                hst_name = servicedep.dependent_host_name\n\n                # The new member list, in id\n                serv = services.find_srv_by_name_and_hostname(hst_name, s_name)\n                if serv is None:\n                    host = hosts.find_by_name(hst_name)\n                    if not (host and host.is_excluded_for_sdesc(s_name)):\n                        errors.append(\"Service %s not found for host %s\" % (s_name, hst_name))\n                    elif host:\n                        warns.append(\"Service %s is excluded from host %s ; \"\n                                     \"removing this servicedependency as it's unusuable.\"\n                                     % (s_name, hst_name))\n                    to_del.append(servicedep)\n                    continue\n                servicedep.dependent_service_description = serv.uuid\n\n                s_name = servicedep.service_description\n                hst_name = servicedep.host_name\n\n                # The new member list, in id\n                serv = services.find_srv_by_name_and_hostname(hst_name, s_name)\n                if serv is None:\n                    host = hosts.find_by_name(hst_name)\n                    if not (host and host.is_excluded_for_sdesc(s_name)):\n                        errors.append(\"Service %s not found for host %s\" % (s_name, hst_name))\n                    elif host:\n                        warns.append(\"Service %s is excluded from host %s ; \"\n                                     \"removing this servicedependency as it's unusuable.\"\n                                     % (s_name, hst_name))\n                    to_del.append(servicedep)\n                    continue\n                servicedep.service_description = serv.uuid\n\n            except AttributeError as err:\n                logger.error(\"[servicedependency] fail to linkify by service %s: %s\",\n                             servicedep, err)\n                to_del.append(servicedep)\n\n        for servicedep in to_del:\n            self.remove_item(servicedep)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef linkify_sd_by_tp(self, timeperiods):\n        for servicedep in self:\n            try:\n                tp_name = servicedep.dependency_period\n                timeperiod = timeperiods.find_by_name(tp_name)\n                if timeperiod:\n                    servicedep.dependency_period = timeperiod.uuid\n                else:\n                    servicedep.dependency_period = ''\n            except AttributeError as exp:\n                logger.error(\"[servicedependency] fail to linkify by timeperiods: %s\", exp)", "response": "Replace dependency_period by a real object in service dependency_node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef linkify_s_by_sd(self, services):\n        for servicedep in self:\n            # Only used for debugging purpose when loops are detected\n            setattr(servicedep, \"service_description_string\", \"undefined\")\n            setattr(servicedep, \"dependent_service_description_string\", \"undefined\")\n\n            if getattr(servicedep, 'service_description', None) is None or\\\n                    getattr(servicedep, 'dependent_service_description', None) is None:\n                continue\n\n            services.add_act_dependency(servicedep.dependent_service_description,\n                                        servicedep.service_description,\n                                        servicedep.notification_failure_criteria,\n                                        getattr(servicedep, 'dependency_period', ''),\n                                        servicedep.inherits_parent)\n\n            services.add_chk_dependency(servicedep.dependent_service_description,\n                                        servicedep.service_description,\n                                        servicedep.execution_failure_criteria,\n                                        getattr(servicedep, 'dependency_period', ''),\n                                        servicedep.inherits_parent)\n\n            # Only used for debugging purpose when loops are detected\n            setattr(servicedep, \"service_description_string\",\n                    services[servicedep.service_description].get_name())\n            setattr(servicedep, \"dependent_service_description_string\",\n                    services[servicedep.dependent_service_description].get_name())", "response": "Link dependency in service objects by service description"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if this servicedependency configuration is correct.", "response": "def is_correct(self):\n        \"\"\"Check if this servicedependency configuration is correct ::\n\n        * Check our own specific properties\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        state = True\n\n        # Internal checks before executing inherited function...\n        loop = self.no_loop_in_parents(\"service_description\", \"dependent_service_description\")\n        if loop:\n            msg = \"Loop detected while checking service dependencies\"\n            self.add_error(msg)\n            state = False\n            for item in self:\n                for elem in loop:\n                    if elem == item.service_description:\n                        msg = \"Service %s is parent service_description in dependency \"\\\n                              \"defined in %s\" % (\n                                  item.service_description_string, item.imported_from\n                              )\n                        self.add_error(msg)\n                    elif elem == item.dependent_service_description:\n                        msg = \"Service %s is child service_description in dependency\"\\\n                              \" defined in %s\" % (\n                                  item.dependent_service_description_string, item.imported_from\n                              )\n                        self.add_error(msg)\n\n        return super(Servicedependencies, self).is_correct() and state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a module instance for the modules manager", "response": "def get_instance(mod_conf):\n    \"\"\"\n    Return a module instance for the modules manager\n\n    :param mod_conf: the module properties as defined globally in this file\n    :return:\n    \"\"\"\n    logger.info(\"Giving an instance of %s for alias: %s\",\n                mod_conf.python_name, mod_conf.module_alias)\n\n    return InnerMetrics(mod_conf)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall by the daemon broker to initialize the module", "response": "def init(self):  # pylint: disable=too-many-branches\n        \"\"\"Called by the daemon broker to initialize the module\"\"\"\n        if not self.enabled:\n            logger.info(\" the module is disabled.\")\n            return True\n\n        try:\n            connections = self.test_connection()\n        except Exception as exp:  # pylint: disable=broad-except\n            logger.error(\"initialization, test connection failed. Error: %s\", str(exp))\n\n        if self.influxdb_enabled:\n            try:\n                # Check that configured TSDB is existing, else creates...\n                dbs = self.influx.get_list_database()\n                for db in dbs:\n                    if db.get('name') == self.influxdb_database:\n                        logger.info(\"the database %s is existing.\", self.influxdb_database)\n                        break\n                else:\n                    # Create the database\n                    logger.info(\"creating database %s...\", self.influxdb_database)\n                    self.influx.create_database(self.influxdb_database)\n\n                # Check that configured TSDB retention is existing, else creates...\n                if self.influxdb_retention_name:\n                    rps = self.influx.get_list_retention_policies()\n                    for rp in rps:\n                        if rp.get('name') == self.influxdb_retention_name:\n                            logger.info(\"the retention policy %s is existing.\",\n                                        self.influxdb_retention_name)\n                            break\n                    else:\n                        # Create a retention policy for this database\n                        logger.info(\"creating database retention policy: %s - %s - %s...\",\n                                    self.influxdb_retention_name, self.influxdb_retention_duration,\n                                    self.influxdb_retention_replication)\n                        self.influx.create_retention_policy(\n                            self.influxdb_retention_name, self.influxdb_retention_duration,\n                            self.influxdb_retention_replication, database=self.influxdb_database)\n\n                # Check that configured TSDB user is existing, else creates...\n                if self.influxdb_username:\n                    users = self.influx.get_list_users()\n                    for user in users:\n                        if user.get('user') == self.influxdb_username:\n                            logger.info(\"the user %s is existing.\",\n                                        self.influxdb_username)\n                            break\n                    else:\n                        # Create a retention policy for this database\n                        logger.info(\"creating user: %s...\", self.influxdb_username)\n                        self.influx.create_user(self.influxdb_username, self.influxdb_password,\n                                                admin=False)\n\n                connections = connections or True\n            except Exception as exp:  # pylint: disable=broad-except\n                logger.error(\"InfluxDB, DB initialization failed. Error: %s\", str(exp))\n\n        return connections"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecodes the performance data to build a list of metrics", "response": "def get_metrics_from_perfdata(self, service, perf_data):\n        \"\"\"Decode the performance data to build a metrics list\"\"\"\n        result = []\n        metrics = PerfDatas(perf_data)\n\n        for metric in metrics:\n            logger.debug(\"service: %s, metric: %s (%s)\", service, metric, metric.__dict__)\n\n            if metric.name in ['time']:\n                metric.name = \"duration\"\n            name = sanitize_name(metric.name)\n            name = self.multiple_values.sub(r'.\\1', name)\n            if not name:\n                continue\n\n            # get metric value and its thresholds values if they exist\n            name_value = {\n                name: metric.value,\n                'uom_' + name: metric.uom\n            }\n\n            # Get or ignore extra values depending upon module configuration\n            if metric.warning and self.send_warning:\n                name_value[name + '_warn'] = metric.warning\n\n            if metric.critical and self.send_critical:\n                name_value[name + '_crit'] = metric.critical\n\n            if metric.min and self.send_min:\n                name_value[name + '_min'] = metric.min\n\n            if metric.max and self.send_max:\n                name_value[name + '_max'] = metric.max\n\n            for key, value in name_value.items():\n                result.append((key, value, metric.uom))\n\n        logger.debug(\"Metrics: %s - %s\", service, result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flush(self, log=False):  # pylint:disable=too-many-branches, too-many-nested-blocks\n        if not self.my_metrics:\n            logger.debug(\"Flushing - no metrics to send\")\n            return True\n\n        now = int(time.time())\n        if self.last_failure and self.last_failure + self.metrics_flush_pause > now:\n            if not self.log_metrics_flush_pause:\n                logger.warning(\"Flush paused on connection error (last failed: %d). \"\n                               \"Inner stored metric: %d. Trying to send...\",\n                               self.last_failure, self.metrics_count)\n                self.log_metrics_flush_pause = True\n                if not self.test_connection():\n                    return False\n\n        metrics_sent = False\n        metrics_saved = False\n\n        # Flushing to Graphite\n        if self.graphite_enabled:\n            try:\n                logger.debug(\"Flushing %d metrics to Graphite/carbon\", self.metrics_count)\n\n                carbon_data = []\n                for metric in self.my_metrics:\n                    # Get path\n                    path = metric['tags']['path']\n                    for name, value in metric['fields'].items():\n                        carbon_data.append(\n                            ('.'.join([self.graphite_prefix, '.'.join([path, name])]),\n                             (metric['time'], value)))\n                self.carbon.add_data_list(carbon_data)\n                if self.carbon.send_data():\n                    metrics_sent = True\n                else:\n                    if log:\n                        logger.warning(\"Failed sending metrics to Graphite/carbon. \"\n                                       \"Inner stored metric: %d\", self.metrics_count)\n                if self.log_metrics_flush_pause:\n                    logger.warning(\"Metrics flush restored. \"\n                                   \"Remaining stored metric: %d\", self.metrics_count)\n                self.last_failure = 0\n                self.log_metrics_flush_pause = False\n            except Exception as exp:  # pylint: disable=broad-except\n                if not self.log_metrics_flush_pause:\n                    logger.warning(\"Failed sending metrics to Graphite/carbon: %s:%d. \"\n                                   \"Inner stored metrics count: %d.\",\n                                   self.graphite_host, self.graphite_port, self.metrics_count)\n                    logger.warning(\"Exception: %s / %s\", str(exp), traceback.print_exc())\n                else:\n                    logger.warning(\"Flush paused on connection error (last failed: %d). \"\n                                   \"Inner stored metric: %d. Trying to send...\",\n                                   self.last_failure, self.metrics_count)\n\n                self.last_failure = now\n                return False\n\n        # Flushing to InfluxDB\n        # pylint: disable=too-many-nested-blocks\n        if self.influxdb_enabled:\n            try:\n                logger.debug(\"Flushing %d metrics to InfluxDB\", self.metrics_count)\n\n                for metric in self.my_metrics:\n                    metric['time'] *= 1000000000\n                    for name, value in metric['fields'].items():\n                        if name.startswith('uom_'):\n                            continue\n                        # Force set float values\n                        if not isinstance(value, float):\n                            try:\n                                value = float(value)\n                            except Exception:  # pylint: disable=broad-except\n                                pass\n                            metric['fields'][name] = value\n\n                    if self.influxdb_tags is not None and isinstance(self.influxdb_tags, dict):\n                        metric['tags'].update(self.influxdb_tags)\n\n                # Write data to InfluxDB\n                metrics_sent = self.influx.write_points(self.my_metrics)\n\n                if self.log_metrics_flush_pause:\n                    logger.warning(\"Metrics flush restored. \"\n                                   \"Remaining stored metric: %d\", self.metrics_count)\n                self.last_failure = 0\n                self.log_metrics_flush_pause = False\n            except Exception as exp:  # pylint: disable=broad-except\n                logger.warning(\"*** Exception: %s\", str(exp))\n                if not self.log_metrics_flush_pause:\n                    logger.warning(\"Failed sending metrics to InfluxDB: %s:%d. \"\n                                   \"Inner stored metrics count: %d.\",\n                                   self.influxdb_host, self.influxdb_port, self.metrics_count)\n                    logger.warning(\"Exception: %s\", str(exp))\n                else:\n                    logger.warning(\"Flush paused on connection error (last failed: %d). \"\n                                   \"Inner stored metric: %d. Trying to send...\",\n                                   self.last_failure, self.metrics_count)\n\n                self.last_failure = now\n                return False\n\n        if self.output_file:\n            try:\n                logger.debug(\"Storing %d metrics to %s\", self.metrics_count, self.output_file)\n                with open(self.output_file, 'a') as fp:\n                    for metric in self.my_metrics:\n                        # Get path\n                        path = metric['tags']['path']\n                        for name, value in metric['fields'].items():\n                            fp.write(\"%s;%s;%s\\n\" % (metric['time'], '.'.join((path, name)), value))\n                metrics_saved = True\n\n            except Exception as exp:  # pylint: disable=broad-except\n                logger.warning(\"Failed writing to a file: %s. \"\n                               \"Inner stored metrics count: %d\\n Exception: %s\",\n                               self.output_file, self.metrics_count, str(exp))\n                return False\n\n        if ((self.graphite_host or self.influxdb_host) and metrics_sent) or \\\n                (self.output_file and metrics_saved):\n            self.my_metrics = []\n\n        return True", "response": "Send the inner stored metrics to the configured Graphite or InfluxDB."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_to_tsdb(self, realm, host, service, metrics, ts, path):\n        if ts is None:\n            ts = int(time.time())\n\n        data = {\n            \"measurement\": service,\n            \"tags\": {\n                \"host\": host,\n                \"service\": service,\n                \"realm\": '.'.join(realm) if isinstance(realm, list) else realm,\n                \"path\": path\n            },\n            \"time\": ts,\n            \"fields\": {}\n        }\n\n        if path is not None:\n            data['tags'].update({\"path\": path})\n\n        for metric, value, _ in metrics:\n            data['fields'].update({metric: value})\n\n        # Flush if necessary\n        logger.debug(\"Data: %s\", data)\n        self.my_metrics.append(data)\n\n        if self.metrics_count >= self.metrics_flush_count:\n            # self.carbon.add_data_list(self.my_metrics)\n            self.flush()", "response": "Send performance data to the time series database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef manage_initial_service_status_brok(self, b):\n        host_name = b.data['host_name']\n        service_description = b.data['service_description']\n        service_id = host_name+\"/\"+service_description\n        logger.debug(\"got initial service status: %s\", service_id)\n\n        if host_name not in self.hosts_cache:\n            logger.error(\"initial service status, host is unknown: %s.\", service_id)\n            return\n\n        self.services_cache[service_id] = {\n        }\n        if 'customs' in b.data:\n            self.services_cache[service_id]['_GRAPHITE_POST'] = \\\n                sanitize_name(b.data['customs'].get('_GRAPHITE_POST', None))\n\n        logger.debug(\"initial service status received: %s\", service_id)", "response": "Manage the initial status of a service in the known services cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef manage_initial_host_status_brok(self, b):\n        host_name = b.data['host_name']\n        logger.debug(\"got initial host status: %s\", host_name)\n\n        self.hosts_cache[host_name] = {\n            'realm_name':\n                sanitize_name(b.data.get('realm_name', b.data.get('realm', 'All'))),\n        }\n        if 'customs' in b.data:\n            self.hosts_cache[host_name]['_GRAPHITE_PRE'] = \\\n                sanitize_name(b.data['customs'].get('_GRAPHITE_PRE', None))\n            self.hosts_cache[host_name]['_GRAPHITE_GROUP'] = \\\n                sanitize_name(b.data['customs'].get('_GRAPHITE_GROUP', None))\n        logger.debug(\"initial host status received: %s\", host_name)", "response": "Manage the initial host status brok"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef manage_service_check_result_brok(self, b):  # pylint: disable=too-many-branches\n        host_name = b.data.get('host_name', None)\n        service_description = b.data.get('service_description', None)\n        if not host_name or not service_description:\n            return\n        service_id = host_name+\"/\"+service_description\n        logger.debug(\"service check result: %s\", service_id)\n\n        # If host and service initial status broks have not been received, ignore ...\n        if not self.ignore_unknown and host_name not in self.hosts_cache:\n            logger.warning(\"received service check result for an unknown host: %s\", service_id)\n            return\n        if service_id not in self.services_cache and not self.ignore_unknown:\n            logger.warning(\"received service check result for an unknown service: %s\", service_id)\n            return\n\n        # Decode received metrics\n        metrics = self.get_metrics_from_perfdata(service_description, b.data['perf_data'])\n        if not metrics:\n            logger.debug(\"no metrics to send ...\")\n            return\n\n        # If checks latency is ignored\n        if self.ignore_latency_limit >= b.data['latency'] > 0:\n            check_time = int(b.data['last_chk']) - int(b.data['latency'])\n        else:\n            check_time = int(b.data['last_chk'])\n\n        # Custom hosts variables\n        hname = sanitize_name(host_name)\n        if host_name in self.hosts_cache:\n            if self.hosts_cache[host_name].get('_GRAPHITE_GROUP', None):\n                hname = \".\".join((self.hosts_cache[host_name].get('_GRAPHITE_GROUP'), hname))\n\n            if self.hosts_cache[host_name].get('_GRAPHITE_PRE', None):\n                hname = \".\".join((self.hosts_cache[host_name].get('_GRAPHITE_PRE'), hname))\n\n        # Custom services variables\n        desc = sanitize_name(service_description)\n        if service_id in self.services_cache:\n            if self.services_cache[service_id].get('_GRAPHITE_POST', None):\n                desc = \".\".join((desc, self.services_cache[service_id].get('_GRAPHITE_POST', None)))\n\n        # Graphite data source\n        if self.graphite_data_source:\n            path = '.'.join((hname, self.graphite_data_source, desc))\n        else:\n            path = '.'.join((hname, desc))\n\n        # Realm as a prefix\n        if self.realms_prefix and self.hosts_cache[host_name].get('realm_name', None):\n            path = '.'.join((self.hosts_cache[host_name].get('realm_name'), path))\n\n        realm_name = None\n        if host_name in self.hosts_cache:\n            realm_name = self.hosts_cache[host_name].get('realm_name', None)\n\n        # Send metrics\n        self.send_to_tsdb(realm_name, host_name, service_description, metrics, check_time, path)", "response": "Manage service check result broks"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmanage the hosts cache data from a host check result brok.", "response": "def manage_host_check_result_brok(self, b):  # pylint: disable=too-many-branches\n        \"\"\"An host check result brok has just arrived...\"\"\"\n        host_name = b.data.get('host_name', None)\n        if not host_name:\n            return\n        logger.debug(\"host check result: %s\", host_name)\n\n        # If host initial status brok has not been received, ignore ...\n        if host_name not in self.hosts_cache and not self.ignore_unknown:\n            logger.warning(\"received host check result for an unknown host: %s\", host_name)\n            return\n\n        # Decode received metrics\n        metrics = self.get_metrics_from_perfdata('host_check', b.data['perf_data'])\n        if not metrics:\n            logger.debug(\"no metrics to send ...\")\n            return\n\n        # If checks latency is ignored\n        if self.ignore_latency_limit >= b.data['latency'] > 0:\n            check_time = int(b.data['last_chk']) - int(b.data['latency'])\n        else:\n            check_time = int(b.data['last_chk'])\n\n        # Custom hosts variables\n        hname = sanitize_name(host_name)\n        if host_name in self.hosts_cache:\n            if self.hosts_cache[host_name].get('_GRAPHITE_GROUP', None):\n                hname = \".\".join((self.hosts_cache[host_name].get('_GRAPHITE_GROUP'), hname))\n\n            if self.hosts_cache[host_name].get('_GRAPHITE_PRE', None):\n                hname = \".\".join((self.hosts_cache[host_name].get('_GRAPHITE_PRE'), hname))\n\n        # Graphite data source\n        if self.graphite_data_source:\n            path = '.'.join((hname, self.graphite_data_source))\n            if self.hostcheck:\n                path = '.'.join((hname, self.graphite_data_source, self.hostcheck))\n        else:\n            path = '.'.join((hname, self.hostcheck))\n\n        # Realm as a prefix\n        if self.realms_prefix and self.hosts_cache[host_name].get('realm_name', None):\n            path = '.'.join((self.hosts_cache[host_name].get('realm_name'), path))\n\n        realm_name = None\n        if host_name in self.hosts_cache:\n            realm_name = self.hosts_cache[host_name].get('realm_name', None)\n\n        # Send metrics\n        self.send_to_tsdb(realm_name, host_name, self.hostcheck, metrics, check_time, path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_comment_brok(self, host_name, service_name=''):\n        data = self.serialize()\n        data['host'] = host_name\n        if service_name:\n            data['service'] = service_name\n\n        return Brok({'type': 'comment', 'data': data})", "response": "Get a comment brok with wanted data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse args and run main daemon function", "response": "def main():\n    \"\"\"Parse args and run main daemon function\n\n    :return: None\n    \"\"\"\n    try:\n        args = parse_daemon_args()\n        daemon = Alignak(**args.__dict__)\n        daemon.main()\n    except Exception as exp:  # pylint: disable=broad-except\n        sys.stderr.write(\"*** Daemon exited because: %s\" % str(exp))\n        traceback.print_exc()\n        exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef want_service_notification(self, timeperiods, timestamp, state, n_type,\n                                  business_impact, cmd=None):\n        # pylint: disable=too-many-return-statements\n        \"\"\"Check if notification options match the state of the service\n        Notification is NOT wanted in ONE of the following case::\n\n        * service notifications are disabled\n        * cmd is not in service_notification_commands\n        * business_impact < self.min_business_impact\n        * service_notification_period is not valid\n        * state does not match service_notification_options for problem, recovery and flapping\n        * state does not match host_notification_options for downtime\n\n        :param timestamp: time we want to notify the contact (usually now)\n        :type timestamp: int\n        :param state: host or service state (\"WARNING\", \"CRITICAL\" ..)\n        :type state: str\n        :param n_type: type of notification (\"PROBLEM\", \"RECOVERY\" ..)\n        :type n_type: str\n        :param business_impact: impact of this service\n        :type business_impact: int\n        :param cmd: command launched to notify the contact\n        :type cmd: str\n        :return: True if no condition is matched, otherwise False\n        :rtype: bool\n        TODO: Simplify function\n        \"\"\"\n        if not self.service_notifications_enabled:\n            return False\n\n        # Maybe the command we ask for are not for us, but for another notification ways\n        # on the same contact. If so, bail out\n        if cmd and cmd not in self.service_notification_commands:\n            return False\n\n        # If the business_impact is not high enough, we bail out\n        if business_impact < self.min_business_impact:\n            return False\n\n        notif_period = timeperiods[self.service_notification_period]\n        in_notification_period = notif_period.is_time_valid(timestamp)\n        if 'n' in self.service_notification_options:\n            return False\n\n        if in_notification_period:\n            short_states = {\n                u'WARNING': 'w', u'UNKNOWN': 'u', u'CRITICAL': 'c',\n                u'RECOVERY': 'r', u'FLAPPING': 'f', u'DOWNTIME': 's'\n            }\n            if n_type == u'PROBLEM' and state in short_states:\n                return short_states[state] in self.service_notification_options\n            if n_type == u'RECOVERY' and n_type in short_states:\n                return short_states[n_type] in self.service_notification_options\n            if n_type == u'ACKNOWLEDGEMENT':\n                return in_notification_period\n            if n_type in (u'FLAPPINGSTART', u'FLAPPINGSTOP', u'FLAPPINGDISABLED'):\n                return 'f' in self.service_notification_options\n            if n_type in (u'DOWNTIMESTART', u'DOWNTIMEEND', u'DOWNTIMECANCELLED'):\n                # No notification when a downtime was cancelled. Is that true??\n                # According to the documentation we need to look at _host_ options\n                return 's' in self.host_notification_options\n\n        return False", "response": "Check if the service notification options match the state of the service."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the host notification options match the state of the service and command.", "response": "def want_host_notification(self, timperiods, timestamp,\n                               state, n_type, business_impact, cmd=None):\n        # pylint: disable=too-many-return-statements\n        \"\"\"Check if notification options match the state of the host\n        Notification is NOT wanted in ONE of the following case::\n\n        * host notifications are disabled\n        * cmd is not in host_notification_commands\n        * business_impact < self.min_business_impact\n        * host_notification_period is not valid\n        * state does not match host_notification_options for problem, recovery, flapping and dt\n\n\n        :param timestamp: time we want to notify the contact (usually now)\n        :type timestamp: int\n        :param state: host or service state (\"WARNING\", \"CRITICAL\" ..)\n        :type state: str\n        :param n_type: type of notification (\"PROBLEM\", \"RECOVERY\" ..)\n        :type n_type: str\n        :param business_impact: impact of this service\n        :type business_impact: int\n        :param cmd: command launched to notify the contact\n        :type cmd: str\n        :return: True if no condition is matched, otherwise False\n        :rtype: bool\n        TODO: Simplify function\n        \"\"\"\n        if not self.host_notifications_enabled:\n            return False\n\n        # If the business_impact is not high enough, we bail out\n        if business_impact < self.min_business_impact:\n            return False\n\n        # Maybe the command we ask for are not for us, but for another notification ways\n        # on the same contact. If so, bail out\n        if cmd and cmd not in self.host_notification_commands:\n            return False\n\n        notif_period = timperiods[self.host_notification_period]\n        in_notification_period = notif_period.is_time_valid(timestamp)\n        if 'n' in self.host_notification_options:\n            return False\n\n        if in_notification_period:\n            short_states = {\n                u'DOWN': 'd', u'UNREACHABLE': 'u', u'RECOVERY': 'r',\n                u'FLAPPING': 'f', u'DOWNTIME': 's'\n            }\n            if n_type == u'PROBLEM' and state in short_states:\n                return short_states[state] in self.host_notification_options\n            if n_type == u'RECOVERY' and n_type in short_states:\n                return short_states[n_type] in self.host_notification_options\n            if n_type == u'ACKNOWLEDGEMENT':\n                return in_notification_period\n            if n_type in (u'FLAPPINGSTART', u'FLAPPINGSTOP', u'FLAPPINGDISABLED'):\n                return 'f' in self.host_notification_options\n            if n_type in (u'DOWNTIMESTART', u'DOWNTIMEEND', u'DOWNTIMECANCELLED'):\n                return 's' in self.host_notification_options\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_notification_commands(self, o_type):\n        # service_notification_commands for service\n        notif_commands_prop = o_type + '_notification_commands'\n        notif_commands = getattr(self, notif_commands_prop)\n        return notif_commands", "response": "Get the list of notification commands for a given object type"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if this object configuration is correct.", "response": "def is_correct(self):\n        # pylint: disable=too-many-branches\n        \"\"\"Check if this object configuration is correct ::\n\n        * Check our own specific properties\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        state = True\n\n        # Do not execute checks if notifications are disabled\n        if (hasattr(self, 'service_notification_options') and\n                self.service_notification_options == ['n']):\n            if (hasattr(self, 'host_notification_options') and\n                    self.host_notification_options == ['n']):\n                return True\n\n        # Internal checks before executing inherited function...\n\n        # Service part\n        if not hasattr(self, 'service_notification_commands'):\n            msg = \"[notificationway::%s] do not have any service_notification_commands defined\" % (\n                self.get_name()\n            )\n            self.add_error(msg)\n            state = False\n        else:\n            for cmd in self.service_notification_commands:\n                if cmd is None:\n                    msg = \"[notificationway::%s] a service_notification_command is missing\" % (\n                        self.get_name()\n                    )\n                    self.add_error(msg)\n                    state = False\n                elif not cmd.is_valid():\n                    msg = \"[notificationway::%s] a service_notification_command is invalid\" % (\n                        self.get_name()\n                    )\n                    self.add_error(msg)\n                    state = False\n\n        if getattr(self, 'service_notification_period', None) is None:\n            msg = \"[notificationway::%s] the service_notification_period is invalid\" % (\n                self.get_name()\n            )\n            self.add_error(msg)\n            state = False\n\n        # Now host part\n        if not hasattr(self, 'host_notification_commands'):\n            msg = \"[notificationway::%s] do not have any host_notification_commands defined\" % (\n                self.get_name()\n            )\n            self.add_error(msg)\n            state = False\n        else:\n            for cmd in self.host_notification_commands:\n                if cmd is None:\n                    msg = \"[notificationway::%s] a host_notification_command is missing\" % (\n                        self.get_name()\n                    )\n                    self.add_error(msg)\n                    state = False\n                elif not cmd.is_valid():\n                    msg = \"[notificationway::%s] a host_notification_command is invalid (%s)\" % (\n                        cmd.get_name(), str(cmd.__dict__)\n                    )\n                    self.add_error(msg)\n                    state = False\n\n        if getattr(self, 'host_notification_period', None) is None:\n            msg = \"[notificationway::%s] the host_notification_period is invalid\" % (\n                self.get_name()\n            )\n            self.add_error(msg)\n            state = False\n\n        return super(NotificationWay, self).is_correct() and state"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates link between objects. Notifies that the notificationways are in the same order as the ones in the notificationways.", "response": "def linkify(self, timeperiods, commands):\n        \"\"\"Create link between objects::\n\n         * notificationways -> timeperiods\n         * notificationways -> commands\n\n        :param timeperiods: timeperiods to link\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param commands: commands to link\n        :type commands: alignak.objects.command.Commands\n        :return: None\n        \"\"\"\n        self.linkify_with_timeperiods(timeperiods, 'service_notification_period')\n        self.linkify_with_timeperiods(timeperiods, 'host_notification_period')\n        self.linkify_command_list_with_commands(commands, 'service_notification_commands')\n        self.linkify_command_list_with_commands(commands, 'host_notification_commands')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new_inner_member(self, name, params):\n        params['notificationway_name'] = name\n        self.add_item(NotificationWay(params))", "response": "Create new instance of NotificationWay with given name and parameters\n        and add it to the item list\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if this object configuration is correct.", "response": "def is_correct(self):\n        \"\"\"\n        Check if this object configuration is correct ::\n\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        state = True\n\n        # Ok just put None as modulation_period, means 24x7\n        if not hasattr(self, 'modulation_period'):\n            self.modulation_period = None\n\n        if not hasattr(self, 'customs') or not self.customs:\n            msg = \"[macromodulation::%s] contains no macro definition\" % (self.get_name())\n            self.add_error(msg)\n            state = False\n\n        return super(MacroModulation, self).is_correct() and state"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize(obj, no_dump=False):\n    # print(\"Serialize (%s): %s\" % (no_dump, obj))\n\n    if hasattr(obj, \"serialize\") and isinstance(obj.serialize, collections.Callable):\n        o_dict = {\n            '__sys_python_module__': \"%s.%s\" % (obj.__class__.__module__, obj.__class__.__name__),\n            'content': obj.serialize()\n        }\n\n    elif isinstance(obj, dict):\n        o_dict = {}\n        for key, value in list(obj.items()):\n            o_dict[key] = serialize(value, True)\n\n    elif isinstance(obj, (list, set)):\n        o_dict = [serialize(item, True) for item in obj]\n\n    else:\n        o_dict = obj\n\n    if no_dump:\n        return o_dict\n\n    result = None\n    try:\n        result = json.dumps(o_dict, ensure_ascii=False)\n    except MemoryError:\n        return {'_error': 'Not enough memory on this computer to correctly manage Alignak '\n                          'objects serialization! '\n                          'Sorry for this, please log an issue in the project repository.'}\n\n    return result", "response": "Serialize an object to a dict or json representation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unserialize(j_obj, no_load=False):\n    if not j_obj:\n        return j_obj\n    # print(\"Unserialize (%s): %s\" % (no_load, j_obj))\n\n    if no_load:\n        data = j_obj\n    else:\n        data = json.loads(j_obj)\n\n    if isinstance(data, dict):\n        if '__sys_python_module__' in data:\n            cls = get_alignak_class(data['__sys_python_module__'])\n            # Awful hack for external commands ... need to be refactored!\n            if data['__sys_python_module__'] in ['alignak.external_command.ExternalCommand']:\n                return cls(data['content']['cmd_line'], data['content']['creation_timestamp'])\n\n            return cls(data['content'], parsing=False)\n\n        data_dict = {}\n        for key, value in list(data.items()):\n            data_dict[key] = unserialize(value, True)\n        return data_dict\n\n    if isinstance(data, list):\n        return [unserialize(item, True) for item in data]\n\n    return data", "response": "Un - serialize a single alignak object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_alignak_class(python_path):\n    a_module, a_class = python_path.rsplit('.', 1)\n\n    if not a_module.startswith('alignak'):  # pragma: no cover - should never happen!\n        raise AlignakClassLookupException(\"Can't recreate object in module: %s. \"\n                                          \"Not an Alignak module\" % a_module)\n\n    if a_module not in sys.modules:  # pragma: no cover - should never happen!\n        raise AlignakClassLookupException(\"Can't recreate object in unknown module: %s. \"\n                                          \"No such Alignak module. Alignak versions may mismatch\" %\n                                          a_module)\n\n    pymodule = sys.modules[a_module]\n\n    if not hasattr(pymodule, a_class):  # pragma: no cover - should never happen!\n        raise AlignakClassLookupException(\"Can't recreate object %s in %s module. \"\n                                          \"Module does not have this attribute. \"\n                                          \"Alignak versions may mismatch\" % (a_class, a_module))\n\n    # Awful hack for external commands ... need to be refactored!\n    if a_class not in ['ExternalCommand']:\n        if not isinstance(getattr(pymodule, a_class), type):  # pragma: no cover - protection\n            raise AlignakClassLookupException(\"Can't recreate object %s in %s module. \"\n                                              \"This type is not a class\" % (a_class, a_module))\n\n    return getattr(pymodule, a_class)", "response": "Get the alignak class from the module."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_event(self):\n        self.prepare()\n        return (self.creation_time, self.data['level'], self.data['message'])", "response": "This function returns an Event from a Brok object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prepare(self):\n        # Maybe the Brok is a old daemon one or was already prepared\n        # if so, the data is already ok\n        if hasattr(self, 'prepared') and not self.prepared:\n            self.data = unserialize(self.data)\n            if self.instance_id:\n                self.data['instance_id'] = self.instance_id\n        self.prepared = True", "response": "Un - serialize data from data attribute and add instance_id key if necessary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget element of this node recursively yielding all elements that are not set.", "response": "def resolve_elements(self):\n        \"\"\"Get element of this node recursively\n        Compute rules with OR or AND rule then NOT rules.\n\n        :return: set of element\n        :rtype: set\n        \"\"\"\n        # If it's a leaf, we just need to dump a set with the content of the node\n        if self.leaf:\n            if not self.content:\n                return set()\n\n            return set(self.content)\n\n        # first got the not ones in a list, and the other in the other list\n        not_nodes = [s for s in self.sons if s.not_value]\n        positiv_nodes = [s for s in self.sons if not s.not_value]  # ok a not not is hard to read..\n\n        # By default we are using a OR rule\n        if not self.operand:\n            self.operand = '|'\n\n        res = set()\n\n        # The operand will change the positiv loop only\n        i = 0\n        for node in positiv_nodes:\n            node_members = node.resolve_elements()\n            if self.operand == '|':\n                res = res.union(node_members)\n            elif self.operand == '&':\n                # The first elements of an AND rule should be used\n                if i == 0:\n                    res = node_members\n                else:\n                    res = res.intersection(node_members)\n            i += 1\n\n        # And we finally remove all NOT elements from the result\n        for node in not_nodes:\n            node_members = node.resolve_elements()\n            res = res.difference(node_members)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse and build a tree of ComplexExpressionNode from a pattern.", "response": "def eval_cor_pattern(self, pattern):  # pylint:disable=too-many-branches\n        \"\"\"Parse and build recursively a tree of ComplexExpressionNode from pattern\n\n        :param pattern: pattern to parse\n        :type pattern: str\n        :return: root node of parsed tree\n        :type: alignak.complexexpression.ComplexExpressionNode\n        \"\"\"\n        pattern = pattern.strip()\n        complex_node = False\n\n        # Look if it's a complex pattern (with rule) or\n        # if it's a leaf of it, like a host/service\n        for char in '()+&|,':\n            if char in pattern:\n                complex_node = True\n\n        node = ComplexExpressionNode()\n\n        # if it's a single expression like !linux or production\n        # (where \"linux\" and \"production\" are hostgroup names)\n        # we will get the objects from it and return a leaf node\n        if not complex_node:\n            # If it's a not value, tag the node and find\n            # the name without this ! operator\n            if pattern.startswith('!'):\n                node.not_value = True\n                pattern = pattern[1:]\n\n            node.operand = self.ctx\n            node.leaf = True\n            obj, error = self.find_object(pattern)\n            if obj is not None:\n                node.content = obj\n            else:\n                node.configuration_errors.append(error)\n            return node\n\n        in_par = False\n        tmp = ''\n        stacked_par = 0\n        for char in pattern:\n            if char in (',', '|'):\n                # Maybe we are in a par, if so, just stack it\n                if in_par:\n                    tmp += char\n                else:\n                    # Oh we got a real cut in an expression, if so, cut it\n                    tmp = tmp.strip()\n                    node.operand = '|'\n                    if tmp != '':\n                        son = self.eval_cor_pattern(tmp)\n                        node.sons.append(son)\n                    tmp = ''\n\n            elif char in ('&', '+'):\n                # Maybe we are in a par, if so, just stack it\n                if in_par:\n                    tmp += char\n                else:\n                    # Oh we got a real cut in an expression, if so, cut it\n                    tmp = tmp.strip()\n                    node.operand = '&'\n                    if tmp != '':\n                        son = self.eval_cor_pattern(tmp)\n                        node.sons.append(son)\n                    tmp = ''\n\n            elif char == '(':\n                stacked_par += 1\n\n                in_par = True\n                tmp = tmp.strip()\n                # Maybe we just start a par, but we got some things in tmp\n                # that should not be good in fact !\n                if stacked_par == 1 and tmp != '':\n                    # TODO : real error\n                    print(\"ERROR : bad expression near\", tmp)\n                    continue\n\n                # If we are already in a par, add this (\n                # but not if it's the first one so\n                if stacked_par > 1:\n                    tmp += char\n\n            elif char == ')':\n                stacked_par -= 1\n\n                if stacked_par < 0:\n                    # TODO : real error\n                    print(\"Error : bad expression near\", tmp, \"too much ')'\")\n                    continue\n\n                if stacked_par == 0:\n                    tmp = tmp.strip()\n                    son = self.eval_cor_pattern(tmp)\n                    node.sons.append(son)\n                    in_par = False\n                    # OK now clean the tmp so we start clean\n                    tmp = ''\n                    continue\n\n                # ok here we are still in a huge par, we just close one sub one\n                tmp += char\n            # Maybe it's a classic character, if so, continue\n            else:\n                tmp += char\n\n        # Be sure to manage the trainling part when the line is done\n        tmp = tmp.strip()\n        if tmp != '':\n            son = self.eval_cor_pattern(tmp)\n            node.sons.append(son)\n\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds a list of host that matches the pattern", "response": "def find_object(self, pattern):\n        \"\"\"Get a list of host corresponding to the pattern regarding the context\n\n        :param pattern: pattern to find\n        :type pattern: str\n        :return: Host list matching pattern (hostgroup name, template, all)\n        :rtype: list[alignak.objects.host.Host]\n        \"\"\"\n        obj = None\n        error = None\n        pattern = pattern.strip()\n\n        if pattern == '*':\n            obj = [h.host_name for h in list(self.all_elements.items.values())\n                   if getattr(h, 'host_name', '') != '' and not h.is_tpl()]\n            return obj, error\n\n        # Ok a more classic way\n\n        if self.ctx == 'hostgroups':\n            # Ok try to find this hostgroup\n            hgr = self.grps.find_by_name(pattern)\n            # Maybe it's an known one?\n            if not hgr:\n                error = \"Error : cannot find the %s of the expression '%s'\" % (self.ctx, pattern)\n                return hgr, error\n            # Ok the group is found, get the elements!\n            elts = hgr.get_hosts()\n            elts = strip_and_uniq(elts)\n\n            # Maybe the hostgroup memebrs is '*', if so expand with all hosts\n            if '*' in elts:\n                elts.extend([h.host_name for h in list(self.all_elements.items.values())\n                             if getattr(h, 'host_name', '') != '' and not h.is_tpl()])\n                # And remove this strange hostname too :)\n                elts.remove('*')\n            return elts, error\n\n        obj = self.grps.find_hosts_that_use_template(pattern)\n\n        return obj, error"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset the scheduler to its initial state.", "response": "def reset(self):\n        # pylint: disable=not-context-manager\n        \"\"\"Reset scheduler::\n\n        * Remove waiting results\n        * Clear checks and actions lists\n\n        :return: None\n        \"\"\"\n        logger.info(\"Scheduling loop reset\")\n        with self.waiting_results.mutex:\n            self.waiting_results.queue.clear()\n        self.checks.clear()\n        self.actions.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an iterator for all my known hosts and services", "response": "def all_my_hosts_and_services(self):\n        \"\"\"Create an iterator for all my known hosts and services\n\n        :return: None\n        \"\"\"\n        for what in (self.hosts, self.services):\n            for item in what:\n                yield item"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the configuration from Arbiter and push it to the scheduler.", "response": "def load_conf(self, instance_id, instance_name, conf):\n        \"\"\"Load configuration received from Arbiter and pushed by our Scheduler daemon\n\n        :param instance_name: scheduler instance name\n        :type instance_name: str\n        :param instance_id: scheduler instance id\n        :type instance_id: str\n        :param conf: configuration to load\n        :type conf: alignak.objects.config.Config\n        :return: None\n        \"\"\"\n        self.pushed_conf = conf\n\n        logger.info(\"loading my configuration (%s / %s):\",\n                    instance_id, self.pushed_conf.instance_id)\n        logger.debug(\"Properties:\")\n        for key in sorted(self.pushed_conf.properties):\n            logger.debug(\"- %s: %s\", key, getattr(self.pushed_conf, key, []))\n        logger.debug(\"Macros:\")\n        for key in sorted(self.pushed_conf.macros):\n            logger.debug(\"- %s: %s\", key, getattr(self.pushed_conf.macros, key, []))\n        logger.debug(\"Objects types:\")\n        for _, _, strclss, _, _ in list(self.pushed_conf.types_creations.values()):\n            if strclss in ['arbiters', 'schedulers', 'brokers',\n                           'pollers', 'reactionners', 'receivers']:\n                continue\n            setattr(self, strclss, getattr(self.pushed_conf, strclss, []))\n            # Internal statistics\n            logger.debug(\"- %d %s\", len(getattr(self, strclss)), strclss)\n            statsmgr.gauge('configuration.%s' % strclss, len(getattr(self, strclss)))\n\n        # We need reversed list for searching in the retention file read\n        # todo: check what it is about...\n        self.services.optimize_service_search(self.hosts)\n\n        # Just deprecated\n        # # Compile the triggers\n        # if getattr(self, 'triggers', None):\n        #     logger.info(\"compiling the triggers...\")\n        #     self.triggers.compile()\n        #     self.triggers.load_objects(self)\n        # else:\n        #     logger.info(\"No triggers\")\n\n        # From the Arbiter configuration. Used for satellites to differentiate the schedulers\n        self.alignak_name = self.pushed_conf.alignak_name\n        self.instance_id = instance_id\n        self.instance_name = instance_name\n\n        self.push_flavor = getattr(self.pushed_conf, 'push_flavor', 'None')\n        logger.info(\"Set my scheduler instance: %s - %s - %s\",\n                    self.instance_id, self.instance_name, self.push_flavor)\n\n        # Tag our monitored hosts/services with our instance_id\n        for item in self.all_my_hosts_and_services():\n            item.instance_id = self.instance_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_recurrent_works_tick(self, conf):\n        for key in self.recurrent_works:\n            (name, fun, _) = self.recurrent_works[key]\n            if isinstance(conf, dict):\n                new_tick = conf.get('tick_%s' % name, None)\n            else:\n                new_tick = getattr(conf, 'tick_%s' % name, None)\n\n            if new_tick is not None:\n                logger.debug(\"Requesting to change the default tick to %d for the action %s\",\n                             int(new_tick), name)\n            else:\n                continue\n            # Update the default scheduler tick for this function\n            try:\n                new_tick = int(new_tick)\n                logger.info(\"Changing the default tick to %d for the action %s\", new_tick, name)\n                self.recurrent_works[key] = (name, fun, new_tick)\n            except ValueError:\n                logger.warning(\"Changing the default tick for '%s' to '%s' failed!\", new_tick, name)", "response": "Modify the tick value for the scheduler recurrent work."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps the scheduler objects into a dump file.", "response": "def dump_objects(self):\n        \"\"\"Dump scheduler objects into a dump (temp) file\n\n        :return: None\n        \"\"\"\n        path = os.path.join(tempfile.gettempdir(),\n                            'dump-obj-scheduler-%s-%d.json' % (self.name, int(time.time())))\n\n        logger.info('Dumping scheduler objects to: %s', path)\n        try:\n            fd = open(path, 'wb')\n            output = 'type:uuid:status:t_to_go:poller_tag:worker:command\\n'\n            fd.write(output.encode('utf-8'))\n            for check in list(self.checks.values()):\n                output = 'check:%s:%s:%s:%s:%s:%s\\n' \\\n                         % (check.uuid, check.status, check.t_to_go, check.poller_tag,\n                            check.command, check.my_worker)\n                fd.write(output.encode('utf-8'))\n            logger.info('- dumped checks')\n            for action in list(self.actions.values()):\n                output = '%s: %s:%s:%s:%s:%s:%s\\n'\\\n                         % (action.__class__.my_type, action.uuid, action.status,\n                            action.t_to_go, action.reactionner_tag, action.command,\n                            action.my_worker)\n                fd.write(output.encode('utf-8'))\n            logger.info('- dumped actions')\n            broks = []\n            for broker in list(self.my_daemon.brokers.values()):\n                for brok in broker.broks:\n                    broks.append(brok)\n            for brok in broks:\n                output = 'BROK: %s:%s\\n' % (brok.uuid, brok.type)\n                fd.write(output.encode('utf-8'))\n            logger.info('- dumped broks')\n            fd.close()\n            logger.info('Dumped')\n        except OSError as exp:  # pragma: no cover, should never happen...\n            logger.critical(\"Error when writing the objects dump file %s : %s\", path, str(exp))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump_config(self):\n        path = os.path.join(tempfile.gettempdir(),\n                            'dump-cfg-scheduler-%s-%d.json' % (self.name, int(time.time())))\n\n        try:\n            self.pushed_conf.dump(path)\n        except (OSError, IndexError) as exp:  # pragma: no cover, should never happen...\n            logger.critical(\"Error when writing the configuration dump file %s: %s\",\n                            path, str(exp))", "response": "Dump the scheduler configuration into a temporary file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_external_commands(self, cmds):\n        if not self.external_commands_manager:\n            return\n\n        try:\n            _t0 = time.time()\n            logger.debug(\"Scheduler '%s' got %d commands\", self.name, len(cmds))\n            for command in cmds:\n                self.external_commands_manager.resolve_command(ExternalCommand(command))\n            statsmgr.counter('external-commands.got.count', len(cmds))\n            statsmgr.timer('external-commands.got.time', time.time() - _t0)\n        except Exception as exp:  # pylint: disable=broad-except\n            logger.warning(\"External command parsing error: %s\", exp)\n            logger.warning(\"Exception: %s / %s\", str(exp), traceback.print_exc())\n            for command in cmds:\n                try:\n                    command = command.decode('utf8', 'ignore')\n                except UnicodeEncodeError:\n                    pass\n                except AttributeError:\n                    pass\n\n                logger.warning(\"Command: %s\", command)", "response": "Run external commands Arbiter or Receiver"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_brok(self, brok, broker_uuid=None):\n        # We tag the brok with our instance_id\n        brok.instance_id = self.instance_id\n        if brok.type == 'monitoring_log':\n            # The brok is a monitoring event\n            with self.my_daemon.events_lock:\n                self.my_daemon.events.append(brok)\n            statsmgr.counter('events', 1)\n            return\n\n        if broker_uuid:\n            if broker_uuid not in self.my_daemon.brokers:\n                logger.info(\"Unknown broker: %s / %s!\", broker_uuid, self.my_daemon.brokers)\n                return\n            broker_link = self.my_daemon.brokers[broker_uuid]\n            logger.debug(\"Adding a brok %s for: %s\", brok.type, broker_uuid)\n            # it's just for one broker\n            self.my_daemon.brokers[broker_link.uuid].broks.append(brok)\n            self.nb_broks += 1\n        else:\n            logger.debug(\"Adding a brok %s to all brokers\", brok.type)\n            # add brok to all brokers\n            for broker_link_uuid in self.my_daemon.brokers:\n                logger.debug(\"- adding to %s\", self.my_daemon.brokers[broker_link_uuid])\n                self.my_daemon.brokers[broker_link_uuid].broks.append(brok)\n                self.nb_broks += 1", "response": "Add a brok into the list of brokers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_notification(self, notification):\n        if notification.uuid in self.actions:\n            logger.warning(\"Already existing notification: %s\", notification)\n            return\n\n        logger.debug(\"Adding a notification: %s\", notification)\n        self.actions[notification.uuid] = notification\n        self.nb_notifications += 1\n\n        # A notification which is not a master one asks for a brok\n        if notification.contact is not None:\n            self.add(notification.get_initial_status_brok())", "response": "Add a notification into the actions list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a check into the scheduler checks list", "response": "def add_check(self, check):\n        \"\"\"Add a check into the scheduler checks list\n\n        :param check: check to add\n        :type check: alignak.check.Check\n        :return: None\n        \"\"\"\n        if check is None:\n            return\n        if check.uuid in self.checks:\n            logger.debug(\"Already existing check: %s\", check)\n            return\n        logger.debug(\"Adding a check: %s\", check)\n\n        # Add a new check to the scheduler checks list\n        self.checks[check.uuid] = check\n        self.nb_checks += 1\n\n        # Raise a brok to inform about a next check is to come ...\n        # but only for items that are actively checked\n        item = self.find_item_by_id(check.ref)\n        if item.active_checks_enabled:\n            self.add(item.get_next_schedule_brok())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an event handler into the actions list", "response": "def add_event_handler(self, action):\n        \"\"\"Add a event handler into actions list\n\n        :param action: event handler to add\n        :type action: alignak.eventhandler.EventHandler\n        :return: None\n        \"\"\"\n        if action.uuid in self.actions:\n            logger.info(\"Already existing event handler: %s\", action)\n            return\n\n        self.actions[action.uuid] = action\n        self.nb_event_handlers += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncleaning up the internal list of all items in the current item.", "response": "def clean_queues(self):\n        # pylint: disable=too-many-locals\n        \"\"\"Reduces internal list size to max allowed\n\n        * checks and broks : 5 * length of hosts + services\n        * actions : 5 * length of hosts + services + contacts\n\n        :return: None\n        \"\"\"\n        # If we set the interval at 0, we bail out\n        if getattr(self.pushed_conf, 'tick_clean_queues', 0) == 0:\n            logger.debug(\"No queues cleaning...\")\n            return\n\n        max_checks = MULTIPLIER_MAX_CHECKS * (len(self.hosts) + len(self.services))\n        max_broks = MULTIPLIER_MAX_BROKS * (len(self.hosts) + len(self.services))\n        max_actions = MULTIPLIER_MAX_ACTIONS * len(self.contacts) * (len(self.hosts) +\n                                                                     len(self.services))\n\n        # For checks, it's not very simple:\n        # For checks, they may be referred to their host/service\n        # We do not just del them in the check list, but also in their service/host\n        # We want id of lower than max_id - 2*max_checks\n        self.nb_checks_dropped = 0\n        if max_checks and len(self.checks) > max_checks:\n            # keys does not ensure sorted keys. Max is slow but we have no other way.\n            to_del_checks = [c for c in list(self.checks.values())]\n            to_del_checks.sort(key=lambda x: x.creation_time)\n            to_del_checks = to_del_checks[:-max_checks]\n            self.nb_checks_dropped = len(to_del_checks)\n            if to_del_checks:\n                logger.warning(\"I have to drop some checks (%d)..., sorry :(\",\n                               self.nb_checks_dropped)\n            for chk in to_del_checks:\n                c_id = chk.uuid\n                items = getattr(self, chk.ref_type + 's')\n                elt = items[chk.ref]\n                # First remove the link in host/service\n                elt.remove_in_progress_check(chk)\n                # Then in dependent checks (I depend on, or check\n                # depend on me)\n                for dependent_checks in chk.depend_on_me:\n                    dependent_checks.depend_on.remove(chk.uuid)\n                for c_temp in chk.depend_on:\n                    c_temp.depend_on_me.remove(chk)\n                del self.checks[c_id]  # Final Bye bye ...\n\n        # For broks and actions, it's more simple\n        # or broks, manage global but also all brokers\n        self.nb_broks_dropped = 0\n        for broker_link in list(self.my_daemon.brokers.values()):\n            if max_broks and len(broker_link.broks) > max_broks:\n                logger.warning(\"I have to drop some broks (%d > %d) for the broker %s \"\n                               \"..., sorry :(\", len(broker_link.broks), max_broks, broker_link)\n\n                kept_broks = sorted(broker_link.broks, key=lambda x: x.creation_time)\n                # Delete the oldest broks to keep the max_broks most recent...\n                # todo: is it a good choice !\n                broker_link.broks = kept_broks[0:max_broks]\n\n        self.nb_actions_dropped = 0\n        if max_actions and len(self.actions) > max_actions:\n            logger.warning(\"I have to del some actions (currently: %d, max: %d)..., sorry :(\",\n                           len(self.actions), max_actions)\n            to_del_actions = [c for c in list(self.actions.values())]\n            to_del_actions.sort(key=lambda x: x.creation_time)\n            to_del_actions = to_del_actions[:-max_actions]\n            self.nb_actions_dropped = len(to_del_actions)\n            for act in to_del_actions:\n                if act.is_a == 'notification':\n                    self.find_item_by_id(act.ref).remove_in_progress_notification(act)\n                del self.actions[act.uuid]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_business_values(self):\n        for elt in self.all_my_hosts_and_services():\n            if not elt.is_problem:\n                was = elt.business_impact\n                elt.update_business_impact_value(self.hosts, self.services,\n                                                 self.timeperiods, self.businessimpactmodulations)\n                new = elt.business_impact\n                # Ok, the business_impact change, we can update the broks\n                if new != was:\n                    self.get_and_register_status_brok(elt)\n\n        # When all impacts and classic elements are updated,\n        # we can update problems (their value depend on impacts, so\n        # they must be done after)\n        for elt in self.all_my_hosts_and_services():\n            # We first update impacts and classic elements\n            if elt.is_problem:\n                was = elt.business_impact\n                elt.update_business_impact_value(self.hosts, self.services,\n                                                 self.timeperiods, self.businessimpactmodulations)\n                new = elt.business_impact\n                # Maybe one of the impacts change it's business_impact to a high value\n                # and so ask for the problem to raise too\n                if new != was:\n                    self.get_and_register_status_brok(elt)", "response": "Iterate over host and service and update business_impact value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scatter_master_notifications(self):\n        now = time.time()\n        # We only want the master scheduled notifications that are immediately launchable\n        notifications = [a for a in self.actions.values()\n                         if a.is_a == u'notification' and a.status == ACT_STATUS_SCHEDULED\n                         and not a.contact and a.is_launchable(now)]\n        if notifications:\n            logger.debug(\"Scatter master notification: %d notifications\",\n                         len(notifications))\n        for notification in notifications:\n            logger.debug(\"Scheduler got a master notification: %s\", notification)\n\n            # This is a \"master\" notification created by an host/service.\n            # We use it to create children notifications (for the contacts and\n            # notification_commands) which are executed in the reactionner.\n            item = self.find_item_by_id(notification.ref)\n            children = []\n            notification_period = None\n            if getattr(item, 'notification_period', None) is not None:\n                notification_period = self.timeperiods[item.notification_period]\n            if not item.is_blocking_notifications(notification_period,\n                                                  self.hosts, self.services,\n                                                  notification.type, now):\n                # If it is possible to send notifications\n                # of this type at the current time, then create\n                # a single notification for each contact of this item.\n                children = item.scatter_notification(\n                    notification, self.contacts, self.notificationways, self.timeperiods,\n                    self.macromodulations, self.escalations,\n                    self.find_item_by_id(getattr(item, \"host\", None))\n                )\n                for notif in children:\n                    logger.debug(\" - child notification: %s\", notif)\n                    notif.status = ACT_STATUS_SCHEDULED\n                    # Add the notification to the scheduler objects\n                    self.add(notif)\n\n            # If we have notification_interval then schedule\n            # the next notification (problems only)\n            if notification.type == u'PROBLEM':\n                # Update the ref notif number after raise the one of the notification\n                if children:\n                    # notif_nb of the master notification\n                    # was already current_notification_number+1.\n                    # If notifications were sent,\n                    # then host/service-counter will also be incremented\n                    item.current_notification_number = notification.notif_nb\n\n                if item.notification_interval and notification.t_to_go is not None:\n                    # We must continue to send notifications.\n                    # Just leave it in the actions list and set it to \"scheduled\"\n                    # and it will be found again later\n                    # Ask the service/host to compute the next notif time. It can be just\n                    # a.t_to_go + item.notification_interval*item.__class__.interval_length\n                    # or maybe before because we have an\n                    # escalation that need to raise up before\n                    notification.t_to_go = item.get_next_notification_time(notification,\n                                                                           self.escalations,\n                                                                           self.timeperiods)\n\n                    notification.notif_nb = item.current_notification_number + 1\n                    logger.debug(\"Repeat master notification: %s\", notification)\n                else:\n                    # Wipe out this master notification. It is a master one\n                    item.remove_in_progress_notification(notification)\n                    logger.debug(\"Remove master notification (no repeat): %s\", notification)\n            else:\n                # Wipe out this master notification.\n                logger.debug(\"Remove master notification (no more a problem): %s\", notification)\n                # We don't repeat recover/downtime/flap/etc...\n                item.remove_in_progress_notification(notification)", "response": "Generate children notifications from a master notification."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_to_run_checks(self, do_checks=False, do_actions=False,\n                          poller_tags=None, reactionner_tags=None,\n                          worker_name='none', module_types=None):\n        # pylint: disable=too-many-branches\n        \"\"\"Get actions/checks for reactionner/poller\n\n        Called by the poller to get checks (do_checks=True) and\n        by the reactionner (do_actions=True) to get actions\n\n        :param do_checks: do we get checks ?\n        :type do_checks: bool\n        :param do_actions: do we get actions ?\n        :type do_actions: bool\n        :param poller_tags: poller tags to filter\n        :type poller_tags: list\n        :param reactionner_tags: reactionner tags to filter\n        :type reactionner_tags: list\n        :param worker_name: worker name to fill check/action (to remember it)\n        :type worker_name: str\n        :param module_types: module type to filter\n        :type module_types: list\n        :return: Check/Action list with poller/reactionner tags matching and module type matching\n        :rtype: list\n        \"\"\"\n        res = []\n        now = time.time()\n\n        if poller_tags is None:\n            poller_tags = ['None']\n        if reactionner_tags is None:\n            reactionner_tags = ['None']\n        if module_types is None:\n            module_types = ['fork']\n        if not isinstance(module_types, list):\n            module_types = [module_types]\n\n        # If a poller wants its checks\n        if do_checks:\n            if self.checks:\n                logger.debug(\"I have %d prepared checks\", len(self.checks))\n\n            for check in list(self.checks.values()):\n                logger.debug(\"Check: %s (%s / %s)\", check.uuid, check.poller_tag, check.module_type)\n\n                if check.internal:\n                    # Do not care about Alignak internally executed checks\n                    continue\n\n                #  If the command is untagged, and the poller too, or if both are tagged\n                #  with same name, go for it\n                # if do_check, call for poller, and so poller_tags by default is ['None']\n                # by default poller_tag is 'None' and poller_tags is ['None']\n                # and same for module_type, the default is the 'fork' type\n                if check.poller_tag not in poller_tags:\n                    logger.debug(\" -> poller tag do not match\")\n                    continue\n                if check.module_type not in module_types:\n                    logger.debug(\" -> module type do not match\")\n                    continue\n\n                logger.debug(\" -> : %s %s (%s)\",\n                             'worker' if not check.internal else 'internal',\n                             check.status,\n                             'now' if check.is_launchable(now) else 'not yet')\n                if check._is_orphan and check.status == ACT_STATUS_SCHEDULED \\\n                        and os.getenv('ALIGNAK_LOG_CHECKS', None):\n                    logger.info(\"--ALC-- orphan check: %s -> : %s %s (%s)\",\n                                check, 'worker' if not check.internal else 'internal',\n                                check.status, 'now' if check.is_launchable(now) else 'not yet')\n\n                # must be ok to launch, and not an internal one (business rules based)\n                if check.status == ACT_STATUS_SCHEDULED and check.is_launchable(now):\n                    logger.debug(\"Check to run: %s\", check)\n                    check.status = ACT_STATUS_POLLED\n                    check.my_worker = worker_name\n                    res.append(check)\n\n                    # Stats\n                    self.nb_checks_launched += 1\n\n                    if 'ALIGNAK_LOG_ACTIONS' in os.environ:\n                        if os.environ['ALIGNAK_LOG_ACTIONS'] == 'WARNING':\n                            logger.warning(\"Check to run: %s\", check)\n                        else:\n                            logger.info(\"Check to run: %s\", check)\n\n            if res:\n                logger.debug(\"-> %d checks to start now\", len(res))\n            else:\n                logger.debug(\"-> no checks to start now\")\n\n        # If a reactionner wants its actions\n        if do_actions:\n            if self.actions:\n                logger.debug(\"I have %d prepared actions\", len(self.actions))\n\n            for action in list(self.actions.values()):\n                logger.debug(\"Action: %s (%s / %s)\",\n                             action.uuid, action.reactionner_tag, action.module_type)\n\n                if action.internal:\n                    # Do not care about Alignak internally executed checks\n                    continue\n\n                is_master = (action.is_a == 'notification' and not action.contact)\n                if is_master:\n                    continue\n\n                # if do_action, call the reactionner,\n                # and so reactionner_tags by default is ['None']\n                # by default reactionner_tag is 'None' and reactionner_tags is ['None'] too\n                # So if not the good one, loop for next :)\n                if action.reactionner_tag not in reactionner_tags:\n                    logger.debug(\" -> reactionner tag do not match\")\n                    continue\n\n                # same for module_type\n                if action.module_type not in module_types:\n                    logger.debug(\" -> module type do not match\")\n                    continue\n\n                # And now look if we can launch or not :)\n                logger.debug(\" -> : worker %s (%s)\",\n                             action.status, 'now' if action.is_launchable(now) else 'not yet')\n                if action._is_orphan and action.status == ACT_STATUS_SCHEDULED and \\\n                        os.getenv('ALIGNAK_LOG_CHECKS', None):\n                    logger.info(\"--ALC-- orphan action: %s\", action)\n\n                if action.status == ACT_STATUS_SCHEDULED and action.is_launchable(now):\n                    # This is for child notifications and eventhandlers\n                    action.status = ACT_STATUS_POLLED\n                    action.my_worker = worker_name\n                    res.append(action)\n\n                    # Stats\n                    self.nb_actions_launched += 1\n\n                    if 'ALIGNAK_LOG_ACTIONS' in os.environ:\n                        if os.environ['ALIGNAK_LOG_ACTIONS'] == 'WARNING':\n                            logger.warning(\"Action to run: %s\", action)\n                        else:\n                            logger.info(\"Action to run: %s\", action)\n\n            if res:\n                logger.debug(\"-> %d actions to start now\", len(res))\n            else:\n                logger.debug(\"-> no actions to start now\")\n\n        return res", "response": "Get the checks and actions for a poller and reactionner."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting result from pollers and reactionners.", "response": "def manage_results(self, action):  # pylint: disable=too-many-branches,too-many-statements\n        \"\"\"Get result from pollers/reactionners (actives ones)\n\n        :param action: check / action / event handler to handle\n        :type action:\n        :return: None\n        \"\"\"\n        logger.debug('manage_results: %s ', action)\n        if action.is_a == 'notification':\n            try:\n                _ = self.actions[action.uuid]\n            except KeyError as exp:  # pragma: no cover, simple protection\n                # Cannot find notification - drop it\n                logger.warning('manage_results:: get unknown notification : %s ', str(exp))\n                for uuid in self.actions:\n                    logger.debug('manage_results:: known action: %s ', self.actions[uuid])\n                return\n\n            # We will only see child notifications here\n            try:\n                timeout = False\n                execution_time = 0\n                if action.status == ACT_STATUS_TIMEOUT:\n                    # Unfortunately the remove_in_progress_notification\n                    # sets the status to zombie, so we need to save it here.\n                    timeout = True\n                    execution_time = action.execution_time\n\n                # Add protection for strange charset\n                try:\n                    action.output = action.output.decode('utf8', 'ignore')\n                except UnicodeDecodeError:\n                    pass\n                except AttributeError:\n                    # Python 3 will raise an exception\n                    pass\n\n                self.actions[action.uuid].get_return_from(action)\n                item = self.find_item_by_id(self.actions[action.uuid].ref)\n                item.remove_in_progress_notification(action)\n                self.actions[action.uuid].status = ACT_STATUS_ZOMBIE\n                item.last_notification = int(action.check_time)\n\n                # And we ask the item to update its state\n                self.get_and_register_status_brok(item)\n\n                # If we' ve got a problem with the notification, raise a Warning log\n                if timeout:\n                    contact = self.find_item_by_id(self.actions[action.uuid].contact)\n                    item = self.find_item_by_id(self.actions[action.uuid].ref)\n\n                    self.nb_actions_results_timeout += 1\n\n                    logger.warning(\"Contact %s %s notification command '%s ' \"\n                                   \"timed out after %.2f seconds\",\n                                   contact.contact_name,\n                                   item.my_type,\n                                   self.actions[action.uuid].command,\n                                   execution_time)\n                else:\n                    self.nb_actions_results += 1\n\n                    if action.exit_status != 0:\n                        logger.warning(\"The notification command '%s' raised an error \"\n                                       \"(exit code=%d): '%s'\",\n                                       action.command, action.exit_status, action.output)\n\n            except (ValueError, AttributeError) as exp:  # pragma: no cover, simple protection\n                # bad object, drop it\n                logger.warning('manage_results:: got bad notification : %s ', str(exp))\n\n        elif action.is_a == 'check':\n            try:\n                check = self.checks[action.uuid]\n            except KeyError as exp:  # pragma: no cover, simple protection\n                # Cannot find check - drop it\n                logger.warning('manage_results:: get unknown check: %s ', action)\n                for uuid in self.checks:\n                    logger.debug('manage_results:: known check: %s ', self.checks[uuid])\n                return\n\n            try:\n                if action.status == ACT_STATUS_TIMEOUT:\n                    ref = self.find_item_by_id(check.ref)\n                    action.long_output = action.output\n                    action.output = \"(%s %s check timed out)\" % (ref.my_type, ref.get_full_name())\n                    action.exit_status = self.pushed_conf.timeout_exit_status\n\n                    self.nb_checks_results_timeout += 1\n\n                    logger.warning(\"Timeout raised for '%s' (check command for the %s '%s'), \"\n                                   \"check status code: %d, execution time: %d seconds\",\n                                   action.command, ref.my_type, ref.get_full_name(),\n                                   action.exit_status, int(action.execution_time))\n                else:\n                    self.nb_checks_results += 1\n                    if action.passive_check:\n                        self.nb_checks_results_passive += 1\n                    else:\n                        self.nb_checks_results_active += 1\n\n                        check.get_return_from(action)\n                check.status = ACT_STATUS_WAIT_CONSUME\n                if check._is_orphan and os.getenv('ALIGNAK_LOG_CHECKS', None):\n                    logger.info(\"--ALC-- got a result for an orphan check: %s\", check)\n\n            except (ValueError, AttributeError) as exp:  # pragma: no cover, simple protection\n                # bad object, drop it\n                logger.warning('manage_results:: got bad check: %s ', str(exp))\n\n        elif action.is_a == 'eventhandler':\n            try:\n                old_action = self.actions[action.uuid]\n                old_action.status = ACT_STATUS_ZOMBIE\n            except KeyError as exp:  # pragma: no cover, simple protection\n                # cannot find old action\n                # bad object, drop it\n                logger.warning('manage_results:: get bad check: %s ', str(exp))\n                return\n\n            try:\n                if action.status == ACT_STATUS_TIMEOUT:\n                    _type = 'event handler'\n                    if action.is_snapshot:\n                        _type = 'snapshot'\n                    ref = self.find_item_by_id(self.checks[action.uuid].ref)\n                    logger.info(\"%s %s command '%s' timed out after %d seconds\",\n                                ref.__class__.my_type.capitalize(),  # pylint: disable=E1101\n                                _type, self.actions[action.uuid].command,\n                                int(action.execution_time))\n\n                    self.nb_actions_results_timeout += 1\n                else:\n                    self.nb_actions_results += 1\n\n                # If it's a snapshot we should get the output and export it\n                if action.is_snapshot:\n                    old_action.get_return_from(action)\n                    s_item = self.find_item_by_id(old_action.ref)\n                    self.add(s_item.get_snapshot_brok(old_action.output, old_action.exit_status))\n            except (ValueError, AttributeError) as exp:  # pragma: no cover, simple protection\n                # bad object, drop it\n                logger.warning('manage_results:: got bad event handler: %s ', str(exp))\n\n        else:  # pragma: no cover, simple protection, should not happen!\n            logger.error(\"The received result type in unknown! %s\", str(action.is_a))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef push_actions_to_passive_satellites(self):\n        # We loop for our passive pollers or reactionners\n        for satellites in [self.my_daemon.pollers, self.my_daemon.reactionners]:\n            s_type = 'poller'\n            if satellites is self.my_daemon.reactionners:\n                s_type = 'reactionner'\n\n            for link in [s for s in list(satellites.values()) if s.passive]:\n                logger.debug(\"Try to send actions to the %s '%s'\", s_type, link.name)\n\n                # Get actions to execute\n                lst = []\n                if s_type == 'poller':\n                    lst = self.get_to_run_checks(do_checks=True, do_actions=False,\n                                                 poller_tags=link.poller_tags,\n                                                 worker_name=link.name)\n                elif s_type == 'reactionner':\n                    lst = self.get_to_run_checks(do_checks=False, do_actions=True,\n                                                 reactionner_tags=link.reactionner_tags,\n                                                 worker_name=link.name)\n                if not lst:\n                    logger.debug(\"Nothing to do...\")\n                    continue\n\n                logger.debug(\"Sending %d actions to the %s '%s'\", len(lst), s_type, link.name)\n                link.push_actions(lst, self.instance_id)", "response": "Send actions to passive poller or reactionners"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_results_from_passive_satellites(self):\n        #  pylint: disable=broad-except\n        \"\"\"Get actions/checks results from passive poller/reactionners\n\n        :return: None\n        \"\"\"\n        # We loop for our passive pollers or reactionners\n        for satellites in [self.my_daemon.pollers, self.my_daemon.reactionners]:\n            s_type = 'poller'\n            if satellites is self.my_daemon.reactionners:\n                s_type = 'reactionner'\n\n            for link in [s for s in list(satellites.values()) if s.passive]:\n                logger.debug(\"Trying to get results from the %s '%s'\", s_type, link.name)\n\n                results = link.get_results(self.instance_id)\n                if results:\n                    logger.debug(\"Got some results: %d results from %s\", len(results), link.name)\n                else:\n                    logger.debug(\"-> no passive results from %s\", link.name)\n                    continue\n\n                results = unserialize(results, no_load=True)\n                if results:\n                    logger.info(\"Received %d passive results from %s\", len(results), link.name)\n\n                for result in results:\n                    logger.debug(\"-> result: %s\", result)\n\n                    # Append to the scheduler result queue\n                    self.waiting_results.put(result)", "response": "Get actions and checks results from passive poller or reactionners."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun internal checks and return internal check info.", "response": "def manage_internal_checks(self):\n        \"\"\"Run internal checks\n\n        :return: None\n        \"\"\"\n        if os.getenv('ALIGNAK_MANAGE_INTERNAL', '1') != '1':\n            return\n        now = time.time()\n        for chk in list(self.checks.values()):\n            if not chk.internal:\n                # Exclude checks that are not internal ones\n                continue\n\n            # Exclude checks that are not yet ready to launch\n            if not chk.is_launchable(now) or chk.status not in [ACT_STATUS_SCHEDULED]:\n                continue\n\n            item = self.find_item_by_id(chk.ref)\n            # Only if active checks are enabled\n            if not item or not item.active_checks_enabled:\n                # Ask to remove the check\n                chk.status = ACT_STATUS_ZOMBIE\n                continue\n            logger.debug(\"Run internal check for %s\", item)\n\n            self.nb_internal_checks += 1\n\n            # Execute internal check\n            item.manage_internal_check(self.hosts, self.services, chk, self.hostgroups,\n                                       self.servicegroups, self.macromodulations,\n                                       self.timeperiods)\n            # Ask to consume the check result\n            chk.status = ACT_STATUS_WAIT_CONSUME"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset topology_change attribute to False in all hosts and services", "response": "def reset_topology_change_flag(self):\n        \"\"\"Set topology_change attribute to False in all hosts and services\n\n        :return: None\n        \"\"\"\n        for i in self.hosts:\n            i.topology_change = False\n        for i in self.services:\n            i.topology_change = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_retention(self):\n        # If we set the retention update to 0, we do not want to manage retention\n        # If we are not forced (like at stopping)\n        if self.pushed_conf.retention_update_interval == 0:\n            logger.debug(\"Should have saved retention but it is not enabled\")\n            return\n\n        _t0 = time.time()\n        self.hook_point('save_retention')\n        statsmgr.timer('hook.retention-save', time.time() - _t0)\n\n        self.add(make_monitoring_log('INFO', 'RETENTION SAVE: %s' % self.my_daemon.name))\n        logger.info('Retention data saved: %.2f seconds', time.time() - _t0)", "response": "Call hook point save_retention."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the retention data from file and db.", "response": "def retention_load(self, forced=False):\n        \"\"\"Call hook point 'load_retention'.\n        Retention modules will read retention (from file, db etc)\n\n        :param forced: is load forced?\n        :type forced: bool\n        :return: None\n        \"\"\"\n        # If we set the retention update to 0, we do not want to manage retention\n        # If we are not forced (like at stopping)\n        if self.pushed_conf.retention_update_interval == 0 and not forced:\n            logger.debug(\"Should have loaded retention but it is not enabled\")\n            return\n\n        _t0 = time.time()\n        self.hook_point('load_retention')\n        statsmgr.timer('hook.retention-load', time.time() - _t0)\n\n        self.add(make_monitoring_log('INFO', 'RETENTION LOAD: %s' % self.my_daemon.name))\n        logger.info('Retention data loaded: %.2f seconds', time.time() - _t0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log_initial_states(self):\n        # Raise hosts initial status broks\n        for elt in self.hosts:\n            elt.raise_initial_state()\n\n        # And then services initial status broks\n        for elt in self.services:\n            elt.raise_initial_state()", "response": "Raise hosts and services initial status logs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all hosts and services data to be sent to the retention storage.", "response": "def get_retention_data(self):  # pylint: disable=too-many-branches,too-many-statements\n        # pylint: disable=too-many-locals\n        \"\"\"Get all hosts and services data to be sent to the retention storage.\n\n        This function only prepares the data because a module is in charge of making\n        the data survive to the scheduler restart.\n\n        todo: Alignak scheduler creates two separate dictionaries: hosts and services\n        It would be better to merge the services into the host dictionary!\n\n        :return: dict containing host and service data\n        :rtype: dict\n        \"\"\"\n        retention_data = {\n            'hosts': {}, 'services': {}\n        }\n        for host in self.hosts:\n            h_dict = {}\n\n            # Get the hosts properties and running properties\n            properties = host.__class__.properties\n            properties.update(host.__class__.running_properties)\n            for prop, entry in list(properties.items()):\n                if not entry.retention:\n                    continue\n\n                val = getattr(host, prop)\n                # If a preparation function exists...\n                prepare_retention = entry.retention_preparation\n                if prepare_retention:\n                    val = prepare_retention(host, val)\n                h_dict[prop] = val\n\n            retention_data['hosts'][host.host_name] = h_dict\n        logger.info('%d hosts sent to retention', len(retention_data['hosts']))\n\n        # Same for services\n        for service in self.services:\n            s_dict = {}\n\n            # Get the services properties and running properties\n            properties = service.__class__.properties\n            properties.update(service.__class__.running_properties)\n            for prop, entry in list(properties.items()):\n                if not entry.retention:\n                    continue\n\n                val = getattr(service, prop)\n                # If a preparation function exists...\n                prepare_retention = entry.retention_preparation\n                if prepare_retention:\n                    val = prepare_retention(service, val)\n                s_dict[prop] = val\n\n            retention_data['services'][(service.host_name, service.service_description)] = s_dict\n        logger.info('%d services sent to retention', len(retention_data['services']))\n\n        return retention_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrestore retention data from configuration", "response": "def restore_retention_data(self, data):\n        \"\"\"Restore retention data\n\n        Data coming from retention will override data coming from configuration\n        It is kinda confusing when you modify an attribute (external command) and it get saved\n        by retention\n\n        :param data: data from retention\n        :type data: dict\n        :return: None\n        \"\"\"\n        if 'hosts' not in data:\n            logger.warning(\"Retention data are not correct, no 'hosts' property!\")\n            return\n\n        for host_name in data['hosts']:\n            # We take the dict of our value to load\n            host = self.hosts.find_by_name(host_name)\n            if host is not None:\n                self.restore_retention_data_item(data['hosts'][host_name], host)\n        statsmgr.gauge('retention.hosts', len(data['hosts']))\n        logger.info('%d hosts restored from retention', len(data['hosts']))\n\n        # Same for services\n        for (host_name, service_description) in data['services']:\n            # We take our dict to load\n            service = self.services.find_srv_by_name_and_hostname(host_name, service_description)\n            if service is not None:\n                self.restore_retention_data_item(data['services'][(host_name, service_description)],\n                                                 service)\n        statsmgr.gauge('retention.services', len(data['services']))\n        logger.info('%d services restored from retention', len(data['services']))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrestore the retention data of the item.", "response": "def restore_retention_data_item(self, data, item):\n        # pylint: disable=too-many-branches, too-many-locals\n        \"\"\"\n        Restore data in item\n\n        :param data: retention data of the item\n        :type data: dict\n        :param item: host or service item\n        :type item: alignak.objects.host.Host | alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        # Manage the properties and running properties\n        properties = item.__class__.properties\n        properties.update(item.__class__.running_properties)\n        for prop, entry in list(properties.items()):\n            if not entry.retention:\n                continue\n\n            if prop not in data:\n                continue\n\n            # If a restoration function exists...\n            restore_retention = entry.retention_restoration\n            if restore_retention:\n                setattr(item, prop, restore_retention(item, data[prop]))\n            else:\n                setattr(item, prop, data[prop])\n\n        # Now manage all linked objects load from/ previous run\n        for notification_uuid in item.notifications_in_progress:\n            notification = item.notifications_in_progress[notification_uuid]\n            # Update the notification referenced object\n            notification['ref'] = item.uuid\n            my_notification = Notification(params=notification)\n            item.notifications_in_progress[notification_uuid] = my_notification\n\n            # Add a notification in the scheduler actions\n            self.add(my_notification)\n\n        # todo: is it useful? We do not save/restore checks in the retention data...\n        item.update_in_checking()\n\n        # And also add downtimes and comments\n        # Downtimes are in a list..\n        for downtime_uuid in data['downtimes']:\n            downtime = data['downtimes'][downtime_uuid]\n\n            # Update the downtime referenced object\n            downtime['ref'] = item.uuid\n            my_downtime = Downtime(params=downtime)\n            if downtime['comment_id']:\n                if downtime['comment_id'] not in data['comments']:\n                    downtime['comment_id'] = ''\n\n            # case comment_id has comment dict instead uuid\n            # todo: This should never happen! Why this code ?\n            if 'uuid' in downtime['comment_id']:\n                data['comments'].append(downtime['comment_id'])\n                downtime['comment_id'] = downtime['comment_id']['uuid']\n            item.add_downtime(my_downtime)\n\n        # Comments are in a list..\n        for comment_uuid in data['comments']:\n            comment = data['comments'][comment_uuid]\n            # Update the comment referenced object\n            comment['ref'] = item.uuid\n            item.add_comment(Comment(comment))\n\n        if item.acknowledgement is not None:\n            # Update the comment referenced object\n            item.acknowledgement['ref'] = item.uuid\n            item.acknowledgement = Acknowledge(item.acknowledgement)\n\n        # Relink the notified_contacts as a set() of true contacts objects\n        # if it was loaded from the retention, it's now a list of contacts\n        # names\n        new_notified_contacts = set()\n        new_notified_contacts_ids = set()\n        for contact_name in item.notified_contacts:\n            contact = self.contacts.find_by_name(contact_name)\n            if contact is not None:\n                new_notified_contacts.add(contact_name)\n                new_notified_contacts_ids.add(contact.uuid)\n        item.notified_contacts = new_notified_contacts\n        item.notified_contacts_ids = new_notified_contacts_ids"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating initial broks for a specific broker.", "response": "def fill_initial_broks(self, broker_name):\n        # pylint: disable=too-many-branches\n        \"\"\"Create initial broks for a specific broker\n\n        :param broker_name: broker name\n        :type broker_name: str\n        :return: number of created broks\n        \"\"\"\n        broker_uuid = None\n        logger.debug(\"My brokers: %s\", self.my_daemon.brokers)\n        for broker_link in list(self.my_daemon.brokers.values()):\n            logger.debug(\"Searching broker: %s\", broker_link)\n            if broker_name == broker_link.name:\n                broker_uuid = broker_link.uuid\n                logger.info(\"Filling initial broks for: %s (%s)\", broker_name, broker_uuid)\n                break\n        else:\n            if self.pushed_conf:\n                # I am yet configured but I do not know this broker ! Something went wrong!!!\n                logger.error(\"Requested initial broks for an unknown broker: %s\", broker_name)\n            else:\n                logger.info(\"Requested initial broks for an unknown broker: %s\", broker_name)\n            return 0\n\n        if self.my_daemon.brokers[broker_uuid].initialized:\n            logger.warning(\"The broker %s still got its initial broks...\", broker_name)\n            return 0\n\n        initial_broks_count = len(self.my_daemon.brokers[broker_uuid].broks)\n\n        # First the program status\n        brok = self.get_program_status_brok()\n        self.add_brok(brok, broker_uuid)\n\n        #  We can't call initial_status from all this types\n        #  The order is important, service need host...\n        initial_status_types = (self.timeperiods, self.commands,\n                                self.contacts, self.contactgroups,\n                                self.hosts, self.hostgroups,\n                                self.services, self.servicegroups)\n\n        self.pushed_conf.skip_initial_broks = getattr(self.pushed_conf, 'skip_initial_broks', False)\n        logger.debug(\"Skipping initial broks? %s\", str(self.pushed_conf.skip_initial_broks))\n        if not self.pushed_conf.skip_initial_broks:\n            #  We call initial_status from all this types\n            #  The order is important, service need host...\n            initial_status_types = (self.realms, self.timeperiods, self.commands,\n                                    self.notificationways, self.contacts, self.contactgroups,\n                                    self.hosts, self.hostgroups, self.hostdependencies,\n                                    self.services, self.servicegroups, self.servicedependencies,\n                                    self.escalations)\n\n            for tab in initial_status_types:\n                for item in tab:\n                    # Awful! simply to get the group members property name... :(\n                    # todo: replace this!\n                    member_items = None\n                    if hasattr(item, 'members'):\n                        member_items = getattr(self, item.my_type.replace(\"group\", \"s\"))\n                    brok = item.get_initial_status_brok(member_items)\n                    self.add_brok(brok, broker_uuid)\n\n        # Add a brok to say that we finished all initial_pass\n        brok = Brok({'type': 'initial_broks_done', 'data': {'instance_id': self.instance_id}})\n        self.add_brok(brok, broker_uuid)\n\n        final_broks_count = len(self.my_daemon.brokers[broker_uuid].broks)\n        self.my_daemon.brokers[broker_uuid].initialized = True\n\n        # Send the initial broks to our modules\n        self.send_broks_to_modules()\n\n        # We now have raised all the initial broks\n        self.raised_initial_broks = True\n\n        logger.info(\"Created %d initial broks for %s\",\n                    final_broks_count - initial_broks_count, broker_name)\n        return final_broks_count - initial_broks_count"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef consume_results(self):  # pylint: disable=too-many-branches\n        # All results are in self.waiting_results\n        # We need to get them first\n        queue_size = self.waiting_results.qsize()\n        for _ in range(queue_size):\n            self.manage_results(self.waiting_results.get())\n\n        # Then we consume them\n        for chk in list(self.checks.values()):\n            if chk.status == ACT_STATUS_WAIT_CONSUME:\n                logger.debug(\"Consuming: %s\", chk)\n                item = self.find_item_by_id(chk.ref)\n                notification_period = None\n                if getattr(item, 'notification_period', None) is not None:\n                    notification_period = self.timeperiods[item.notification_period]\n\n                dep_checks = item.consume_result(chk, notification_period, self.hosts,\n                                                 self.services, self.timeperiods,\n                                                 self.macromodulations, self.checkmodulations,\n                                                 self.businessimpactmodulations,\n                                                 self.resultmodulations, self.checks,\n                                                 self.pushed_conf.log_active_checks and\n                                                 not chk.passive_check)\n\n                # # Raise the log only when the check got consumed!\n                # # Else the item information are not up-to-date :/\n                # if self.pushed_conf.log_active_checks and not chk.passive_check:\n                #     item.raise_check_result()\n                #\n                for check in dep_checks:\n                    logger.debug(\"-> raised a dependency check: %s\", chk)\n                    self.add(check)\n\n        # loop to resolve dependencies\n        have_resolved_checks = True\n        while have_resolved_checks:\n            have_resolved_checks = False\n            # All 'finished' checks (no more dep) raise checks they depend on\n            for chk in list(self.checks.values()):\n                if chk.status == ACT_STATUS_WAITING_ME:\n                    for dependent_checks in chk.depend_on_me:\n                        # Ok, now dependent will no more wait\n                        dependent_checks.depend_on.remove(chk.uuid)\n                        have_resolved_checks = True\n                    # REMOVE OLD DEP CHECK -> zombie\n                    chk.status = ACT_STATUS_ZOMBIE\n\n            # Now, reinteger dep checks\n            for chk in list(self.checks.values()):\n                if chk.status == ACT_STATUS_WAIT_DEPEND and not chk.depend_on:\n                    item = self.find_item_by_id(chk.ref)\n                    notification_period = None\n                    if getattr(item, 'notification_period', None) is not None:\n                        notification_period = self.timeperiods[item.notification_period]\n                    dep_checks = item.consume_result(chk, notification_period, self.hosts,\n                                                     self.services, self.timeperiods,\n                                                     self.macromodulations, self.checkmodulations,\n                                                     self.businessimpactmodulations,\n                                                     self.resultmodulations, self.checks,\n                                                     self.pushed_conf.log_active_checks and\n                                                     not chk.passive_check)\n                    for check in dep_checks:\n                        self.add(check)", "response": "Handle results waiting in waiting_results list and update status of items in self. waiting_results_list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving checks that have a zombie status", "response": "def delete_zombie_checks(self):\n        \"\"\"Remove checks that have a zombie status (usually timeouts)\n\n        :return: None\n        \"\"\"\n        id_to_del = []\n        for chk in list(self.checks.values()):\n            if chk.status == ACT_STATUS_ZOMBIE:\n                id_to_del.append(chk.uuid)\n        # une petite tape dans le dos et tu t'en vas, merci...\n        # *pat pat* GFTO, thks :)\n        for c_id in id_to_del:\n            del self.checks[c_id]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove actions that have a zombie status", "response": "def delete_zombie_actions(self):\n        \"\"\"Remove actions that have a zombie status (usually timeouts)\n\n        :return: None\n        \"\"\"\n        id_to_del = []\n        for act in list(self.actions.values()):\n            if act.status == ACT_STATUS_ZOMBIE:\n                id_to_del.append(act.uuid)\n        # une petite tape dans le dos et tu t'en vas, merci...\n        # *pat pat* GFTO, thks :)\n        for a_id in id_to_del:\n            del self.actions[a_id]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_downtimes_and_comments(self):\n        # pylint: disable=too-many-branches\n        \"\"\"Iter over all hosts and services::\n\n        TODO: add some unit tests for the maintenance period feature.\n\n        * Update downtime status (start / stop) regarding maintenance period\n        * Register new comments in comments list\n\n        :return: None\n        \"\"\"\n        broks = []\n        now = time.time()\n\n        # Check maintenance periods\n        for elt in self.all_my_hosts_and_services():\n            if not elt.maintenance_period:\n                continue\n\n            if elt.in_maintenance == -1:\n                timeperiod = self.timeperiods[elt.maintenance_period]\n                if timeperiod.is_time_valid(now):\n                    start_dt = timeperiod.get_next_valid_time_from_t(now)\n                    end_dt = timeperiod.get_next_invalid_time_from_t(start_dt + 1) - 1\n                    data = {\n                        'ref': elt.uuid, 'ref_type': elt.my_type, 'start_time': start_dt,\n                        'end_time': end_dt, 'fixed': 1, 'trigger_id': '',\n                        'duration': 0, 'author': \"Alignak\",\n                        'comment': \"This downtime was automatically scheduled by Alignak \"\n                                   \"because of a maintenance period.\"\n                    }\n                    downtime = Downtime(data)\n                    self.add(downtime.add_automatic_comment(elt))\n                    elt.add_downtime(downtime)\n                    self.add(downtime)\n                    self.get_and_register_status_brok(elt)\n                    elt.in_maintenance = downtime.uuid\n            else:\n                if elt.in_maintenance not in elt.downtimes:\n                    # the main downtimes has expired or was manually deleted\n                    elt.in_maintenance = -1\n\n        #  Check the validity of contact downtimes\n        for elt in self.contacts:\n            for downtime_id in elt.downtimes:\n                downtime = elt.downtimes[downtime_id]\n                downtime.check_activation(self.contacts)\n\n        # A loop where those downtimes are removed\n        # which were marked for deletion (mostly by dt.exit())\n        for elt in self.all_my_hosts_and_services():\n            for downtime in list(elt.downtimes.values()):\n                if not downtime.can_be_deleted:\n                    continue\n\n                logger.debug(\"Downtime to delete: %s\", downtime.__dict__)\n                elt.del_downtime(downtime.uuid)\n                broks.append(elt.get_update_status_brok())\n\n        # Same for contact downtimes:\n        for elt in self.contacts:\n            for downtime in list(elt.downtimes.values()):\n                if not downtime.can_be_deleted:\n                    continue\n                elt.del_downtime(downtime.uuid)\n                broks.append(elt.get_update_status_brok())\n\n        # Check start and stop times\n        for elt in self.all_my_hosts_and_services():\n            for downtime in list(elt.downtimes.values()):\n                if downtime.real_end_time < now:\n                    # this one has expired\n                    broks.extend(downtime.exit(self.timeperiods, self.hosts, self.services))\n                elif now >= downtime.start_time and downtime.fixed and not downtime.is_in_effect:\n                    # this one has to start now\n                    broks.extend(downtime.enter(self.timeperiods, self.hosts, self.services))\n                    broks.append(self.find_item_by_id(downtime.ref).get_update_status_brok())\n\n        for brok in broks:\n            self.add(brok)", "response": "Update the downtimes and comments list for all hosts and services."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef schedule(self, elements=None):\n        if not elements:\n            elements = self.all_my_hosts_and_services()\n\n        # ask for service and hosts their next check\n        for elt in elements:\n            logger.debug(\"Add check for: %s\", elt)\n            self.add(elt.schedule(self.hosts, self.services, self.timeperiods,\n                                  self.macromodulations, self.checkmodulations, self.checks))", "response": "Iterate over all hosts and services and call schedule method\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall get_new_actions hook point Iter over all hosts and services to add new actions in internal lists", "response": "def get_new_actions(self):\n        \"\"\"Call 'get_new_actions' hook point\n        Iter over all hosts and services to add new actions in internal lists\n\n        :return: None\n        \"\"\"\n        _t0 = time.time()\n        self.hook_point('get_new_actions')\n        statsmgr.timer('hook.get-new-actions', time.time() - _t0)\n        # ask for service and hosts their next check\n        for elt in self.all_my_hosts_and_services():\n            for action in elt.actions:\n                logger.debug(\"Got a new action for %s: %s\", elt, action)\n                self.add(action)\n            # We take all, we can clear it\n            elt.actions = []"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_new_broks(self):\n        # ask for service and hosts their broks waiting\n        # be eaten\n        for elt in self.all_my_hosts_and_services():\n            for brok in elt.broks:\n                self.add(brok)\n            # We got all, clear item broks list\n            elt.broks = []\n\n        # Also fetch broks from contact (like contactdowntime)\n        for contact in self.contacts:\n            for brok in contact.broks:\n                self.add(brok)\n            # We got all, clear contact broks list\n            contact.broks = []", "response": "Iter over all hosts and services to add new broks in internal lists\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck for orphaned checks and actions.", "response": "def check_orphaned(self):\n        \"\"\"Check for orphaned checks/actions::\n\n        * status == 'in_poller' and t_to_go < now - time_to_orphanage (300 by default)\n\n        if so raise a warning log.\n\n        :return: None\n        \"\"\"\n        orphans_count = {}\n        now = int(time.time())\n        actions = list(self.checks.values()) + list(self.actions.values())\n        for chk in actions:\n            if chk.status not in [ACT_STATUS_POLLED]:\n                continue\n\n            time_to_orphanage = self.find_item_by_id(chk.ref).get_time_to_orphanage()\n            if not time_to_orphanage:\n                continue\n\n            if chk.t_to_go > now - time_to_orphanage:\n                continue\n\n            logger.info(\"Orphaned %s (%d s / %s / %s) check for: %s (%s)\",\n                        chk.is_a, time_to_orphanage, chk.t_to_go, now,\n                        self.find_item_by_id(chk.ref).get_full_name(), chk)\n            chk._is_orphan = True\n            chk.status = ACT_STATUS_SCHEDULED\n            if chk.my_worker not in orphans_count:\n                orphans_count[chk.my_worker] = 0\n            orphans_count[chk.my_worker] += 1\n\n        for sta_name in orphans_count:\n            logger.warning(\"%d actions never came back for the satellite '%s'. \"\n                           \"I reenable them for polling.\",\n                           orphans_count[sta_name], sta_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending broks to modules", "response": "def send_broks_to_modules(self):\n        \"\"\"Put broks into module queues\n        Only broks without sent_to_externals to True are sent\n        Only modules that ask for broks will get some\n\n        :return: None\n        \"\"\"\n        t00 = time.time()\n        nb_sent = 0\n        broks = []\n        for broker_link in list(self.my_daemon.brokers.values()):\n            for brok in broker_link.broks:\n                if not getattr(brok, 'sent_to_externals', False):\n                    brok.to_send = True\n                    broks.append(brok)\n        if not broks:\n            return\n        logger.debug(\"sending %d broks to modules...\", len(broks))\n\n        for mod in self.my_daemon.modules_manager.get_external_instances():\n            logger.debug(\"Look for sending to module %s\", mod.get_name())\n            module_queue = mod.to_q\n            if module_queue:\n                to_send = [b for b in broks if mod.want_brok(b)]\n                module_queue.put(to_send)\n                nb_sent += len(to_send)\n\n        # No more need to send them\n        for broker_link in list(self.my_daemon.brokers.values()):\n            for brok in broker_link.broks:\n                if not getattr(brok, 'sent_to_externals', False):\n                    brok.to_send = False\n                    brok.sent_to_externals = True\n        logger.debug(\"Time to send %d broks (after %d secs)\", nb_sent, time.time() - t00)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_scheduler_stats(self, details=False):  # pylint: disable=unused-argument\n        # pylint: disable=too-many-locals, too-many-branches\n        \"\"\"Get the scheduler statistics\n\n        :return: A dict with the following structure\n        ::\n\n           { 'modules': [\n                         {'internal': {'name': \"MYMODULE1\", 'state': 'ok'},\n                         {'external': {'name': \"MYMODULE2\", 'state': 'stopped'},\n                        ]\n             'latency':  {'avg': lat_avg, 'min': lat_min, 'max': lat_max}\n             'hosts': len(self.hosts),\n             'services': len(self.services),\n             'commands': [{'cmd': c, 'u_time': u_time, 's_time': s_time}, ...] (10 first)\n             'livesynthesis': {...}\n           }\n\n        :rtype: dict\n        \"\"\"\n        m_solver = MacroResolver()\n\n        res = {\n            '_freshness': int(time.time()),\n            'counters': {},\n            'latency': self.stats['latency'],\n            'monitored_objects': {},\n            'livesynthesis': {}\n        }\n\n        checks_status_counts = self.get_checks_status_counts()\n\n        # Checks / actions counters\n        for what in (u'actions', u'checks'):\n            res['counters']['%s.count' % what] = len(getattr(self, what))\n            for status in (u'scheduled', u'in_poller', u'zombie'):\n                res['counters']['%s.%s' % (what, status)] = checks_status_counts[status]\n\n        if self.pushed_conf:\n            for _, _, strclss, _, _ in list(self.pushed_conf.types_creations.values()):\n                # Internal statistics\n                res['monitored_objects'][strclss] = len(getattr(self, strclss, []))\n\n            # Scheduler live synthesis\n            res['livesynthesis'] = {\n                'hosts_total': m_solver._get_total_hosts(),\n                'hosts_not_monitored': m_solver._get_total_hosts_not_monitored(),\n                'hosts_up_hard': m_solver._get_total_hosts_up(u'HARD'),\n                'hosts_up_soft': m_solver._get_total_hosts_up(u'SOFT'),\n                'hosts_down_hard': m_solver._get_total_hosts_down(u'HARD'),\n                'hosts_down_soft': m_solver._get_total_hosts_down(u'SOFT'),\n                'hosts_unreachable_hard': m_solver._get_total_hosts_unreachable(u'HARD'),\n                'hosts_unreachable_soft': m_solver._get_total_hosts_unreachable(u'SOFT'),\n\n                'hosts_problems': m_solver._get_total_hosts_problems_unhandled(),\n                'hosts_acknowledged': m_solver._get_total_hosts_problems_handled(),\n                'hosts_in_downtime': m_solver._get_total_hosts_downtimed(),\n                'hosts_flapping': m_solver._get_total_hosts_flapping(),\n\n                'services_total': m_solver._get_total_services(),\n                'services_not_monitored': m_solver._get_total_services_not_monitored(),\n                'services_ok_hard': m_solver._get_total_services_ok(u'HARD'),\n                'services_ok_soft': m_solver._get_total_services_ok(u'SOFT'),\n                'services_warning_hard': m_solver._get_total_services_warning(u'HARD'),\n                'services_warning_soft': m_solver._get_total_services_warning(u'SOFT'),\n                'services_critical_hard': m_solver._get_total_services_critical(u'HARD'),\n                'services_critical_soft': m_solver._get_total_services_critical(u'SOFT'),\n                'services_unknown_hard': m_solver._get_total_services_unknown(u'HARD'),\n                'services_unknown_soft': m_solver._get_total_services_unknown(u'SOFT'),\n                'services_unreachable_hard': m_solver._get_total_services_unreachable(u'HARD'),\n                'services_unreachable_soft': m_solver._get_total_services_unreachable(u'SOFT'),\n\n                'services_problems': m_solver._get_total_services_problems_unhandled(),\n                'services_acknowledged': m_solver._get_total_services_problems_handled(),\n                'services_in_downtime': m_solver._get_total_services_downtimed(),\n                'services_flapping': m_solver._get_total_services_flapping()\n            }\n\n            if details:\n                # Hosts/services problems list\n                all_problems = {}\n                for item in self.hosts:\n                    if item.state_type not in [u'HARD'] or item.state not in ['DOWN']:\n                        continue\n\n                    if item.is_problem and not item.problem_has_been_acknowledged:\n                        all_problems[item.uuid] = {\n                            'host': item.get_name(),\n                            'service': None,\n                            'state': item.state,\n                            'state_type': item.state_type,\n                            'output': item.output,\n                            'last_state': item.last_state,\n                            'last_state_type': item.last_state_type,\n                            'last_state_update': item.last_state_update,\n                            'last_state_change': item.last_state_change,\n                            'last_hard_state_change': item.last_hard_state_change,\n                            'last_hard_state': item.last_hard_state,\n                        }\n\n                for item in self.services:\n                    if item.state_type not in [u'HARD'] or item.state not in ['WARNING',\n                                                                              'CRITICAL']:\n                        continue\n\n                    if item.is_problem and not item.problem_has_been_acknowledged:\n                        all_problems[item.uuid] = {\n                            'host': item.host_name,\n                            'service': item.get_name(),\n                            'output': item.output,\n                            'state': item.state,\n                            'state_type': item.state_type,\n                            'last_state': item.last_state,\n                            'last_state_type': item.last_state_type,\n                            'last_hard_state': item.last_hard_state,\n                            'last_state_update': item.last_state_update,\n                            'last_state_change': item.last_state_change,\n                            'last_hard_state_change': item.last_hard_state_change,\n                        }\n\n                res['problems'] = all_problems\n\n                all_commands = {}\n                # Some checks statistics: user/system time\n                for elt in self.all_my_hosts_and_services():\n                    last_cmd = elt.last_check_command\n                    if not last_cmd:\n                        continue\n                    cmd = os.path.split(last_cmd.split(' ', 1)[0])[1]\n                    u_time = elt.u_time\n                    s_time = elt.s_time\n                    old_u_time, old_s_time = all_commands.get(cmd, (0.0, 0.0))\n                    interval = elt.check_interval\n                    if not interval:\n                        interval = 1\n                    old_u_time += u_time / interval\n                    old_s_time += s_time / interval\n                    all_commands[cmd] = (old_u_time, old_s_time)\n\n                # Return all the commands\n                res['commands'] = all_commands\n\n        return res", "response": "Get the scheduler statistics."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_latency_average_percentile(self):\n        (_, _, time_interval) = self.recurrent_works[21]\n        last_time = time.time() - time_interval\n        latencies = [s.latency for s in self.services if s.last_chk > last_time]\n        lat_avg, lat_min, lat_max = average_percentile(latencies)\n        if lat_avg is not None:\n            self.stats['latency']['avg'] = lat_avg\n            self.stats['latency']['min'] = lat_min\n            self.stats['latency']['max'] = lat_max\n            logger.debug(\"Latency (avg/min/max): %.2f/%.2f/%.2f\", lat_avg, lat_min, lat_max)", "response": "Get an overview of the latencies with just a 95 percentile + min / max values\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_checks_status_counts(self, checks=None):\n        if checks is None:\n            checks = self.checks\n\n        res = defaultdict(int)\n        res[\"total\"] = len(checks)\n        for chk in checks.values():\n            res[chk.status] += 1\n        return res", "response": "Compute the counts of the different checks status and\n        return it as a defaultdict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_item_by_id(self, object_id):\n        # Item id may be an item\n        if isinstance(object_id, Item):\n            return object_id\n\n        # Item id should be a uuid string\n        if not isinstance(object_id, string_types):\n            logger.debug(\"Find an item by id, object_id is not int nor string: %s\", object_id)\n            return object_id\n\n        for items in [self.hosts, self.services, self.actions, self.checks, self.hostgroups,\n                      self.servicegroups, self.contacts, self.contactgroups]:\n            if object_id in items:\n                return items[object_id]\n\n        # raise AttributeError(\"Item with id %s not found\" % object_id)  # pragma: no cover,\n        logger.error(\"Item with id %s not found\", str(object_id))  # pragma: no cover,\n        return None", "response": "Find an item based on its id or uuid"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef before_run(self):\n        # Actions and checks counters\n        self.nb_checks = 0\n        self.nb_internal_checks = 0\n        self.nb_checks_launched = 0\n        self.nb_actions_launched = 0\n\n        self.nb_checks_results = 0\n        self.nb_checks_results_timeout = 0\n        self.nb_checks_results_passive = 0\n        self.nb_checks_results_active = 0\n\n        self.nb_actions_results = 0\n        self.nb_actions_results_timeout = 0\n        self.nb_actions_results_passive = 0\n\n        self.nb_broks_dropped = 0\n        self.nb_checks_dropped = 0\n        self.nb_actions_dropped = 0\n\n        # Broks, notifications, ... counters\n        self.nb_broks = 0\n        self.nb_notifications = 0\n        self.nb_event_handlers = 0\n        self.nb_external_commands = 0\n\n        self.ticks = 0", "response": "Initialize the scheduling process"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an object to the daemon internal list.", "response": "def add(self, elt):\n        \"\"\"Generic function to add objects to the daemon internal lists.\n        Manage Broks, External commands\n\n        :param elt: object to add\n        :type elt: alignak.AlignakObject\n        :return: None\n        \"\"\"\n        # external commands may be received as a dictionary when pushed from the WebUI\n        if isinstance(elt, dict) and 'my_type' in elt and elt['my_type'] == \"externalcommand\":\n            if 'cmd_line' not in elt:\n                logger.debug(\"Received a bad formated external command: %s. \"\n                             \"No cmd_line!\", elt)\n                return\n\n            logger.debug(\"Received a dictionary external command: %s\", elt)\n            if 'creation_timestamp' not in elt:\n                elt['creation_timestamp'] = None\n            elt = ExternalCommand(elt['cmd_line'], elt['creation_timestamp'])\n\n        if isinstance(elt, Brok):\n            # For brok, we tag the brok with our instance_id\n            elt.instance_id = self.instance_id\n            if elt.type == 'monitoring_log':\n                # The brok is a monitoring event\n                with self.events_lock:\n                    self.events.append(elt)\n                statsmgr.counter('events', 1)\n            else:\n                with self.broks_lock:\n                    self.broks.append(elt)\n            statsmgr.counter('broks.added', 1)\n        elif isinstance(elt, ExternalCommand):\n            logger.debug(\"Queuing an external command: %s\", str(ExternalCommand.__dict__))\n            self.unprocessed_external_commands.append(elt)\n            statsmgr.counter('external-commands.added', 1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_new_conf(self):\n        # Execute the base class treatment...\n        super(Receiver, self).setup_new_conf()\n\n        # ...then our own specific treatment!\n        with self.conf_lock:\n            # self_conf is our own configuration from the alignak environment\n            # self_conf = self.cur_conf['self_conf']\n            logger.debug(\"Got config: %s\", self.cur_conf)\n\n            # Configure and start our modules\n            if not self.have_modules:\n                try:\n                    self.modules = unserialize(self.cur_conf['modules'], no_load=True)\n                except AlignakClassLookupException as exp:  # pragma: no cover, simple protection\n                    logger.error('Cannot un-serialize modules configuration '\n                                 'received from arbiter: %s', exp)\n                if self.modules:\n                    logger.info(\"I received some modules configuration: %s\", self.modules)\n                    self.have_modules = True\n\n                    self.do_load_modules(self.modules)\n                    # and start external modules too\n                    self.modules_manager.start_external_instances()\n                else:\n                    logger.info(\"I do not have modules\")\n\n            # Now create the external commands manager\n            # We are a receiver: our role is to get and dispatch commands to the schedulers\n            global_conf = self.cur_conf.get('global_conf', None)\n            if not global_conf:\n                logger.error(\"Received a configuration without any global_conf! \"\n                             \"This may hide a configuration problem with the \"\n                             \"realms and the manage_sub_realms of the satellites!\")\n                global_conf = {\n                    'accept_passive_unknown_check_results': False,\n                    'log_external_commands': True\n                }\n            self.external_commands_manager = \\\n                ExternalCommandManager(None, 'receiver', self,\n                                       global_conf.get(\n                                           'accept_passive_unknown_check_results', False),\n                                       global_conf.get(\n                                           'log_external_commands', False))\n\n            # Initialize connection with all our satellites\n            logger.info(\"Initializing connection with my satellites:\")\n            my_satellites = self.get_links_of_type(s_type='')\n            for satellite in list(my_satellites.values()):\n                logger.info(\"- : %s/%s\", satellite.type, satellite.name)\n                if not self.daemon_connection_init(satellite):\n                    logger.error(\"Satellite connection failed: %s\", satellite)\n\n        # Now I have a configuration!\n        self.have_conf = True", "response": "This method is called by the base class to setup the new configuration needed by the receiver."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget external commands from our arbiters and add them to the receiver s external commands list.", "response": "def get_external_commands_from_arbiters(self):\n        \"\"\"Get external commands from our arbiters\n\n        As of now, only the arbiter are requested to provide their external commands that\n        the receiver will push to all the known schedulers to make them being executed.\n\n        :return: None\n        \"\"\"\n        for arbiter_link_uuid in self.arbiters:\n            link = self.arbiters[arbiter_link_uuid]\n\n            if not link.active:\n                logger.debug(\"The arbiter '%s' is not active, it is not possible to get \"\n                             \"its external commands!\", link.name)\n                continue\n\n            try:\n                logger.debug(\"Getting external commands from: %s\", link.name)\n                external_commands = link.get_external_commands()\n                if external_commands:\n                    logger.debug(\"Got %d commands from: %s\", len(external_commands), link.name)\n                else:\n                    # Simple protection against None value\n                    external_commands = []\n                for external_command in external_commands:\n                    self.add(external_command)\n            except LinkError:\n                logger.warning(\"Arbiter connection failed, I could not get external commands!\")\n            except Exception as exp:  # pylint: disable=broad-except\n                logger.error(\"Arbiter connection failed, I could not get external commands!\")\n                logger.exception(\"Exception: %s\", exp)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef push_external_commands_to_schedulers(self):\n        if not self.unprocessed_external_commands:\n            return\n\n        # Those are the global external commands\n        commands_to_process = self.unprocessed_external_commands\n        self.unprocessed_external_commands = []\n        logger.debug(\"Commands: %s\", commands_to_process)\n\n        # Now get all external commands and put them into the good schedulers\n        logger.debug(\"Commands to process: %d commands\", len(commands_to_process))\n        for ext_cmd in commands_to_process:\n            cmd = self.external_commands_manager.resolve_command(ext_cmd)\n            logger.debug(\"Resolved command: %s, result: %s\", ext_cmd.cmd_line, cmd)\n            if cmd and cmd['global']:\n                # Send global command to all our schedulers\n                for scheduler_link_uuid in self.schedulers:\n                    self.schedulers[scheduler_link_uuid].pushed_commands.append(ext_cmd)\n\n        # Now for all active schedulers, send the commands\n        count_pushed_commands = 0\n        count_failed_commands = 0\n        for scheduler_link_uuid in self.schedulers:\n            link = self.schedulers[scheduler_link_uuid]\n\n            if not link.active:\n                logger.debug(\"The scheduler '%s' is not active, it is not possible to push \"\n                             \"external commands to its connection!\", link.name)\n                continue\n\n            # If there are some commands for this scheduler...\n            commands = [ext_cmd.cmd_line for ext_cmd in link.pushed_commands]\n            if not commands:\n                logger.debug(\"The scheduler '%s' has no commands.\", link.name)\n                continue\n\n            logger.debug(\"Sending %d commands to scheduler %s\", len(commands), link.name)\n            sent = []\n            try:\n                sent = link.push_external_commands(commands)\n            except LinkError:\n                logger.warning(\"Scheduler connection failed, I could not push external commands!\")\n\n            # Whether we sent the commands or not, clean the scheduler list\n            link.pushed_commands = []\n\n            # If we didn't sent them, add the commands to the arbiter list\n            if sent:\n                statsmgr.gauge('external-commands.pushed.%s' % link.name, len(commands))\n                count_pushed_commands = count_pushed_commands + len(commands)\n            else:\n                count_failed_commands = count_failed_commands + len(commands)\n                statsmgr.gauge('external-commands.failed.%s' % link.name, len(commands))\n                # Kepp the not sent commands... for a next try\n                self.external_commands.extend(commands)\n\n        statsmgr.gauge('external-commands.pushed.all', count_pushed_commands)\n        statsmgr.gauge('external-commands.failed.all', count_failed_commands)", "response": "Push external commands to all schedulers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_loop_turn(self):\n\n        # Begin to clean modules\n        self.check_and_del_zombie_modules()\n\n        # Maybe the arbiter pushed a new configuration...\n        if self.watch_for_new_conf(timeout=0.05):\n            logger.info(\"I got a new configuration...\")\n            # Manage the new configuration\n            self.setup_new_conf()\n\n        # Maybe external modules raised 'objects'\n        # we should get them\n        _t0 = time.time()\n        self.get_objects_from_from_queues()\n        statsmgr.timer('core.get-objects-from-queues', time.time() - _t0)\n\n        # Get external commands from the arbiters...\n        _t0 = time.time()\n        self.get_external_commands_from_arbiters()\n        statsmgr.timer('external-commands.got.time', time.time() - _t0)\n        statsmgr.gauge('external-commands.got.count', len(self.unprocessed_external_commands))\n\n        _t0 = time.time()\n        self.push_external_commands_to_schedulers()\n        statsmgr.timer('external-commands.pushed.time', time.time() - _t0)\n\n        # Say to modules it's a new tick :)\n        _t0 = time.time()\n        self.hook_point('tick')\n        statsmgr.timer('hook.tick', time.time() - _t0)", "response": "Main loop of the main loop."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nincrease the stats provided by the base Daemon class", "response": "def get_daemon_stats(self, details=False):\n        \"\"\"Increase the stats provided by the Daemon base class\n\n        :return: stats dictionary\n        :rtype: dict\n        \"\"\"\n        # Call the base Daemon one\n        res = super(Receiver, self).get_daemon_stats(details=details)\n\n        res.update({'name': self.name, 'type': self.type})\n\n        counters = res['counters']\n        counters['external-commands'] = len(self.external_commands)\n        counters['external-commands-unprocessed'] = len(self.unprocessed_external_commands)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_return_from(self, check):\n        for prop in ['exit_status', 'output', 'long_output', 'check_time', 'execution_time',\n                     'perf_data', 'u_time', 's_time']:\n            setattr(self, prop, getattr(check, prop))", "response": "Update check data from action"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serialize(self):\n        # uuid is not in *_properties\n        res = {\n            'uuid': self.uuid\n        }\n        for prop in self.__class__.properties:\n            if not hasattr(self, prop):\n                continue\n\n            res[prop] = getattr(self, prop)\n            if isinstance(self.__class__.properties[prop], SetProp):\n                res[prop] = list(getattr(self, prop))\n\n        return res", "response": "This function serializes into a simple dictionary object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fill_default(self):\n        for prop, entry in self.__class__.properties.items():\n            if hasattr(self, prop):\n                continue\n            if not hasattr(entry, 'default') or entry.default is NONE_OBJECT:\n                continue\n\n            if hasattr(entry.default, '__iter__'):\n                setattr(self, prop, copy(entry.default))\n            else:\n                setattr(self, prop, entry.default)", "response": "Define the object properties with a default value when the property is not yet defined."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert configuration file for unreachable state to x", "response": "def convert_conf_for_unreachable(params):\n        \"\"\"\n        The 'u' state for UNREACHABLE has been rewritten in 'x' in:\n        * flap_detection_options\n        * notification_options\n        * snapshot_criteria\n\n        So convert value from config file to keep compatibility with Nagios\n\n        :param params: parameters of the host before put in properties\n        :type params: dict\n        :return: None\n        \"\"\"\n        if params is None:\n            return\n\n        for prop in ['flap_detection_options', 'notification_options',\n                     'snapshot_criteria', 'stalking_options']:\n            if prop in params:\n                params[prop] = [p.replace('u', 'x') for p in params[prop]]\n\n        if 'initial_state' in params and \\\n                (params['initial_state'] == 'u' or params['initial_state'] == ['u']):\n            params['initial_state'] = 'x'\n\n        if 'freshness_state' in params and \\\n                (params['freshness_state'] == 'u' or params['freshness_state'] == ['u']):\n            params['freshness_state'] = 'x'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfill address with host_name and define state with initial_state and define initial_state", "response": "def fill_predictive_missing_parameters(self):\n        \"\"\"Fill address with host_name if not already set\n        and define state with initial_state\n\n        :return: None\n        \"\"\"\n        if hasattr(self, 'host_name') and not hasattr(self, 'address'):\n            self.address = self.host_name\n        if hasattr(self, 'host_name') and not hasattr(self, 'alias'):\n            self.alias = self.host_name\n        if self.initial_state == 'd':\n            self.state = 'DOWN'\n        elif self.initial_state == 'x':\n            self.state = 'UNREACHABLE'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_correct(self):\n        state = True\n\n        # Internal checks before executing inherited function...\n        cls = self.__class__\n        if hasattr(self, 'host_name'):\n            for char in cls.illegal_object_name_chars:\n                if char in self.host_name:\n                    self.add_error(\"[%s::%s] host_name contains an illegal character: %s\"\n                                   % (self.my_type, self.get_name(), char))\n                    state = False\n\n        # Fred: do not alert about missing check_command for an host... this because 1/ it is\n        # very verbose if hosts are not checked and 2/ because it is the Nagios default behavior\n        # if not self.check_command:\n        #     self.add_warning(\"[%s::%s] has no defined check command\"\n        #                      % (self.my_type, self.get_name()))\n\n        if self.notifications_enabled and not self.contacts:\n            self.add_warning(\"[%s::%s] notifications are enabled but no contacts nor \"\n                             \"contact_groups property is defined for this host\"\n                             % (self.my_type, self.get_name()))\n\n        return super(Host, self).is_correct() and state", "response": "Check if this object configuration is correct."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the name of the host in the cache. Try several attributes before returning UNNAMEDHOST or UNNAMEDHOSTTEMPLATE.", "response": "def get_name(self):\n        \"\"\"Get the host name.\n        Try several attributes before returning UNNAMED*\n\n        :return: The name of the host\n        :rtype: str\n        \"\"\"\n        if not self.is_tpl():\n            try:\n                return self.host_name\n            except AttributeError:  # outch, no hostname\n                return 'UNNAMEDHOST'\n        else:\n            try:\n                return self.name\n            except AttributeError:  # outch, no name for this template\n                return 'UNNAMEDHOSTTEMPLATE'"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_groupnames(self, hostgroups):\n        group_names = []\n        for hostgroup_id in self.hostgroups:\n            hostgroup = hostgroups[hostgroup_id]\n            group_names.append(hostgroup.get_name())\n        return ','.join(sorted(group_names))", "response": "Get names of the host s hostgroups alphabetically sorted by name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget aliases of the host s hostgroups alphabetically sorted by group.", "response": "def get_groupaliases(self, hostgroups):\n        \"\"\"Get aliases of the host's hostgroups\n\n        :return: comma separated aliases of hostgroups alphabetically sorted\n        :rtype: str\n        \"\"\"\n        group_aliases = []\n        for hostgroup_id in self.hostgroups:\n            hostgroup = hostgroups[hostgroup_id]\n            group_aliases.append(hostgroup.alias)\n        return ','.join(sorted(group_aliases))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_excluded_for_sdesc(self, sdesc, is_tpl=False):\n        if not is_tpl and self.service_includes:\n            return sdesc not in self.service_includes\n        if self.service_excludes:\n            return sdesc in self.service_excludes\n        return False", "response": "Check whether this host should have the passed service description excluded or not included."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the state of the current entry in the state of the entry in the status of a check result.", "response": "def set_state_from_exit_status(self, status, notif_period, hosts, services):\n        \"\"\"Set the state in UP, DOWN, or UNREACHABLE according to the status of a check result.\n\n        :param status: integer between 0 and 3 (but not 1)\n        :type status: int\n        :return: None\n        \"\"\"\n        now = time.time()\n\n        # we should put in last_state the good last state:\n        # if not just change the state by an problem/impact\n        # we can take current state. But if it's the case, the\n        # real old state is self.state_before_impact (it's the TRUE\n        # state in fact)\n        # And only if we enable the impact state change\n        cls = self.__class__\n        if (cls.enable_problem_impacts_states_change and\n                self.is_impact and not self.state_changed_since_impact):\n            self.last_state = self.state_before_impact\n        else:\n            self.last_state = self.state\n\n        # There is no 1 case because it should have been managed by the caller for a host\n        # like the schedulingitem::consume method.\n        if status == 0:\n            self.state = u'UP'\n            self.state_id = 0\n            self.last_time_up = int(self.last_state_update)\n            # self.last_time_up = self.last_state_update\n            state_code = 'u'\n        elif status in (2, 3):\n            self.state = u'DOWN'\n            self.state_id = 1\n            self.last_time_down = int(self.last_state_update)\n            # self.last_time_down = self.last_state_update\n            state_code = 'd'\n        elif status == 4:\n            self.state = u'UNREACHABLE'\n            self.state_id = 4\n            self.last_time_unreachable = int(self.last_state_update)\n            # self.last_time_unreachable = self.last_state_update\n            state_code = 'x'\n        else:\n            self.state = u'DOWN'  # exit code UNDETERMINED\n            self.state_id = 1\n            # self.last_time_down = int(self.last_state_update)\n            self.last_time_down = self.last_state_update\n            state_code = 'd'\n        if state_code in self.flap_detection_options:\n            self.add_flapping_change(self.state != self.last_state)\n            # Now we add a value, we update the is_flapping prop\n            self.update_flapping(notif_period, hosts, services)\n\n        if self.state != self.last_state and \\\n                not (self.state == \"DOWN\" and self.last_state == \"UNREACHABLE\"):\n            self.last_state_change = self.last_state_update\n        self.duration_sec = now - self.last_state_change"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_state(self, status):\n        if status == self.state:\n            return True\n        # Now low status\n        if status == 'o' and self.state == u'UP':\n            return True\n        if status == 'd' and self.state == u'DOWN':\n            return True\n        if status in ['u', 'x'] and self.state == u'UNREACHABLE':\n            return True\n        return False", "response": "Return if status is in the current state"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef last_time_non_ok_or_up(self):\n        non_ok_times = [x for x in [self.last_time_down]\n                        if x > self.last_time_up]\n        if not non_ok_times:\n            last_time_non_ok = 0  # todo: program_start would be better?\n        else:\n            last_time_non_ok = min(non_ok_times)\n        return last_time_non_ok", "response": "Get the last time the host was in a non - OK state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef raise_check_result(self):\n        if not self.__class__.log_active_checks:\n            return\n\n        log_level = 'info'\n        if self.state == 'DOWN':\n            log_level = 'error'\n        elif self.state == 'UNREACHABLE':\n            log_level = 'warning'\n        brok = make_monitoring_log(\n            log_level, 'ACTIVE HOST CHECK: %s;%s;%d;%s' % (self.get_name(), self.state,\n                                                           self.attempt, self.output)\n        )\n        self.broks.append(brok)", "response": "Raise ACTIVE CHECK RESULT entry\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raise_alert_log_entry(self):\n        if self.__class__.log_alerts:\n            log_level = 'info'\n            if self.state == 'DOWN':\n                log_level = 'error'\n            if self.state == 'UNREACHABLE':\n                log_level = 'warning'\n            brok = make_monitoring_log(\n                log_level, 'HOST ALERT: %s;%s;%s;%d;%s' % (\n                    self.get_name(), self.state, self.state_type, self.attempt, self.output\n                )\n            )\n            self.broks.append(brok)\n\n        if 'ALIGNAK_LOG_ALERTS' in os.environ:\n            if os.environ['ALIGNAK_LOG_ALERTS'] == 'WARNING':\n                logger.warning('HOST ALERT: %s;%s;%s;%d;%s', self.get_name(), self.state,\n                               self.state_type, self.attempt, self.output)\n            else:\n                logger.info('HOST ALERT: %s;%s;%s;%d;%s', self.get_name(), self.state,\n                            self.state_type, self.attempt, self.output)", "response": "Raise the alert log entry in the appropriate format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nraises the initial state of the current host ALERT entry.", "response": "def raise_initial_state(self):\n        \"\"\"Raise CURRENT HOST ALERT entry (info level)\n        Format is : \"CURRENT HOST STATE: *get_name()*;*state*;*state_type*;*attempt*;*output*\"\n        Example : \"CURRENT HOST STATE: server;DOWN;HARD;1;I don't know what to say...\"\n\n        :return: None\n        \"\"\"\n        if not self.__class__.log_initial_states:\n            return\n\n        log_level = 'info'\n        if self.state == 'DOWN':\n            log_level = 'error'\n        if self.state == 'UNREACHABLE':\n            log_level = 'warning'\n        brok = make_monitoring_log(\n            log_level, 'CURRENT HOST STATE: %s;%s;%s;%d;%s' % (\n                self.get_name(), self.state, self.state_type, self.attempt, self.output\n            )\n        )\n        self.broks.append(brok)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nraising the entry in the log for this snapshot", "response": "def raise_snapshot_log_entry(self, command):\n        \"\"\"Raise HOST SNAPSHOT entry (critical level)\n        Format is : \"HOST SNAPSHOT: *self.get_name()*;*state*;*state_type*;*attempt*;\n                    *command.get_name()*\"\n        Example : \"HOST SNAPSHOT: server;UP;HARD;1;notify-by-rss\"\n\n        :param command: Snapshot command launched\n        :type command: alignak.objects.command.Command\n        :return: None\n        \"\"\"\n        if not self.__class__.log_snapshots:\n            return\n\n        log_level = 'info'\n        if self.state == 'UNREACHABLE':\n            log_level = 'warning'\n        if self.state == 'DOWN':\n            log_level = 'error'\n        brok = make_monitoring_log(\n            log_level, \"HOST SNAPSHOT: %s;%s;%s;%s;%s\" % (\n                self.get_name(), self.state, self.state_type, self.attempt, command.get_name()\n            )\n        )\n        self.broks.append(brok)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef raise_flapping_start_log_entry(self, change_ratio, threshold):\n        if not self.__class__.log_flappings:\n            return\n\n        brok = make_monitoring_log(\n            'info',\n            \"HOST FLAPPING ALERT: %s;STARTED; Host appears to have started \"\n            \"flapping (%.1f%% change >= %.1f%% threshold)\"\n            % (self.get_name(), change_ratio, threshold)\n        )\n        self.broks.append(brok)", "response": "Raise the FLAPPING START log entry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raise_acknowledge_log_entry(self):\n        if not self.__class__.log_acknowledgements:\n            return\n\n        brok = make_monitoring_log(\n            'info', \"HOST ACKNOWLEDGE ALERT: %s;STARTED; \"\n                    \"Host problem has been acknowledged\" % self.get_name()\n        )\n        self.broks.append(brok)", "response": "Raise ACKNOWLEDGE ALERT entry"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef raise_enter_downtime_log_entry(self):\n        if not self.__class__.log_downtimes:\n            return\n\n        brok = make_monitoring_log(\n            'info', \"HOST DOWNTIME ALERT: %s;STARTED; \"\n                    \"Host has entered a period of scheduled downtime\" % (self.get_name())\n        )\n        self.broks.append(brok)", "response": "Raise the entry in the log file that the host has entered a period of downtime"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the host need stalking or not", "response": "def manage_stalking(self, check):\n        \"\"\"Check if the host need stalking or not (immediate recheck)\n        If one stalking_options matches the exit_status ('o' <=> 0 ...) then stalk is needed\n        Raise a log entry (info level) if stalk is needed\n\n        :param check: finished check (check.status == 'waitconsume')\n        :type check: alignak.check.Check\n        :return: None\n        \"\"\"\n        need_stalk = False\n        if check.status == u'waitconsume':\n            if check.exit_status == 0 and 'o' in self.stalking_options:\n                need_stalk = True\n            elif check.exit_status == 1 and 'd' in self.stalking_options:\n                need_stalk = True\n            elif check.exit_status == 2 and 'd' in self.stalking_options:\n                need_stalk = True\n            if check.output != self.output:\n                need_stalk = False\n        if need_stalk:\n            logger.info(\"Stalking %s: %s\", self.get_name(), self.output)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the notification is blocked by this contact.", "response": "def notification_is_blocked_by_contact(self, notifways, timeperiods, notif, contact):\n        \"\"\"Check if the notification is blocked by this contact.\n\n        :param notif: notification created earlier\n        :type notif: alignak.notification.Notification\n        :param contact: contact we want to notify\n        :type notif: alignak.objects.contact.Contact\n        :return: True if the notification is blocked, False otherwise\n        :rtype: bool\n        \"\"\"\n        return not contact.want_host_notification(notifways, timeperiods,\n                                                  self.last_chk, self.state, notif.type,\n                                                  self.business_impact, notif.command_call)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting duration formatted by the duration_sec property", "response": "def get_duration(self):\n        \"\"\"Get duration formatted\n        Format is : \"HHh MMm SSs\"\n        Example : \"10h 20m 40s\"\n\n        :return: Formatted duration\n        :rtype: str\n        \"\"\"\n        mins, secs = divmod(self.duration_sec, 60)\n        hours, mins = divmod(mins, 60)\n        return \"%02dh %02dm %02ds\" % (hours, mins, secs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the number of services in the specified state", "response": "def _tot_services_by_state(self, services, state):\n        \"\"\"Get the number of service in the specified state\n\n        :param state: state to filter service\n        :type state:\n        :return: number of service with s.state_id == state\n        :rtype: int\n        \"\"\"\n        return str(sum(1 for s in self.services\n                       if services[s].state_id == state))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_status(self, hosts, services):\n        if self.got_business_rule:\n            mapping = {\n                0: \"UP\",\n                1: \"DOWN\",\n                4: \"UNREACHABLE\",\n            }\n            return mapping.get(self.business_rule.get_state(hosts, services), \"n/a\")\n\n        return self.state", "response": "Get the status of this host based on host state_id or business_rule state_id or business_rule state_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the overall state of a host.", "response": "def get_overall_state(self, services):\n        \"\"\"Get the host overall state including the host self status\n        and the status of its services\n\n        Compute the host overall state identifier, including:\n        - the acknowledged state\n        - the downtime state\n\n        The host overall state is (prioritized):\n        - an host not monitored (5)\n        - an host down (4)\n        - an host unreachable (3)\n        - an host downtimed (2)\n        - an host acknowledged (1)\n        - an host up (0)\n\n        If the host overall state is <= 2, then the host overall state is the maximum value\n        of the host overall state and all the host services overall states.\n\n        The overall state of an host is:\n        - 0 if the host is UP and all its services are OK\n        - 1 if the host is DOWN or UNREACHABLE and acknowledged or\n            at least one of its services is acknowledged and\n            no other services are WARNING or CRITICAL\n        - 2 if the host is DOWN or UNREACHABLE and in a scheduled downtime or\n            at least one of its services is in a scheduled downtime and no\n            other services are WARNING or CRITICAL\n        - 3 if the host is UNREACHABLE or\n            at least one of its services is WARNING\n        - 4 if the host is DOWN or\n            at least one of its services is CRITICAL\n        - 5 if the host is not monitored\n\n        :param services: a list of known services\n        :type services: alignak.objects.service.Services\n\n        :return: the host overall state\n        :rtype: int\n        \"\"\"\n        overall_state = 0\n\n        if not self.monitored:\n            overall_state = 5\n        elif self.acknowledged:\n            overall_state = 1\n        elif self.downtimed:\n            overall_state = 2\n        elif self.state_type == 'HARD':\n            if self.state == 'UNREACHABLE':\n                overall_state = 3\n            elif self.state == 'DOWN':\n                overall_state = 4\n\n        # Only consider the hosts services state if all is ok (or almost...)\n        if overall_state <= 2:\n            for service in self.services:\n                if service in services:\n                    service = services[service]\n                    # Only for monitored services\n                    if service.overall_state_id < 5:\n                        overall_state = max(overall_state, service.overall_state_id)\n\n        return overall_state"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef linkify(self, timeperiods=None, commands=None, contacts=None,  # pylint: disable=R0913\n                realms=None, resultmodulations=None, businessimpactmodulations=None,\n                escalations=None, hostgroups=None,\n                checkmodulations=None, macromodulations=None):\n        \"\"\"Create link between objects::\n\n         * hosts -> timeperiods\n         * hosts -> hosts (parents, etc)\n         * hosts -> commands (check_command)\n         * hosts -> contacts\n\n        :param timeperiods: timeperiods to link\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param commands: commands to link\n        :type commands: alignak.objects.command.Commands\n        :param contacts: contacts to link\n        :type contacts: alignak.objects.contact.Contacts\n        :param realms: realms to link\n        :type realms: alignak.objects.realm.Realms\n        :param resultmodulations: resultmodulations to link\n        :type resultmodulations: alignak.objects.resultmodulation.Resultmodulations\n        :param businessimpactmodulations: businessimpactmodulations to link\n        :type businessimpactmodulations:\n              alignak.objects.businessimpactmodulation.Businessimpactmodulations\n        :param escalations: escalations to link\n        :type escalations: alignak.objects.escalation.Escalations\n        :param hostgroups: hostgroups to link\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :param checkmodulations: checkmodulations to link\n        :type checkmodulations: alignak.objects.checkmodulation.Checkmodulations\n        :param macromodulations: macromodulations to link\n        :type macromodulations:  alignak.objects.macromodulation.Macromodulations\n        :return: None\n        \"\"\"\n        self.linkify_with_timeperiods(timeperiods, 'notification_period')\n        self.linkify_with_timeperiods(timeperiods, 'check_period')\n        self.linkify_with_timeperiods(timeperiods, 'maintenance_period')\n        self.linkify_with_timeperiods(timeperiods, 'snapshot_period')\n        self.linkify_h_by_h()\n        self.linkify_h_by_hg(hostgroups)\n        self.linkify_one_command_with_commands(commands, 'check_command')\n        self.linkify_one_command_with_commands(commands, 'event_handler')\n        self.linkify_one_command_with_commands(commands, 'snapshot_command')\n\n        self.linkify_with_contacts(contacts)\n        # No more necessary\n        self.linkify_h_by_realms(realms)\n        self.linkify_with_resultmodulations(resultmodulations)\n        self.linkify_with_business_impact_modulations(businessimpactmodulations)\n        # WARNING: all escalations will not be link here\n        # (just the escalation here, not serviceesca or hostesca).\n        # This last one will be link in escalations linkify.\n        self.linkify_with_escalations(escalations)\n        self.linkify_with_checkmodulations(checkmodulations)\n        self.linkify_with_macromodulations(macromodulations)", "response": "Create link between objects::\n\n         * hosts -> timeperiods\n         * hosts -> hosts (parents, etc)\n         * hosts -> commands (check_command)\n         * hosts -> contacts\n\n        :param timeperiods: timeperiods to link\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param commands: commands to link\n        :type commands: alignak.objects.command.Commands\n        :param contacts: contacts to link\n        :type contacts: alignak.objects.contact.Contacts\n        :param realms: realms to link\n        :type realms: alignak.objects.realm.Realms\n        :param resultmodulations: resultmodulations to link\n        :type resultmodulations: alignak.objects.resultmodulation.Resultmodulations\n        :param businessimpactmodulations: businessimpactmodulations to link\n        :type businessimpactmodulations:\n              alignak.objects.businessimpactmodulation.Businessimpactmodulations\n        :param escalations: escalations to link\n        :type escalations: alignak.objects.escalation.Escalations\n        :param hostgroups: hostgroups to link\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :param checkmodulations: checkmodulations to link\n        :type checkmodulations: alignak.objects.checkmodulation.Checkmodulations\n        :param macromodulations: macromodulations to link\n        :type macromodulations:  alignak.objects.macromodulation.Macromodulations\n        :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlink hosts with their parents", "response": "def linkify_h_by_h(self):\n        \"\"\"Link hosts with their parents\n\n        :return: None\n        \"\"\"\n        for host in self:\n            # The new member list\n            new_parents = []\n            for parent in getattr(host, 'parents', []):\n                parent = parent.strip()\n                o_parent = self.find_by_name(parent)\n                if o_parent is not None:\n                    new_parents.append(o_parent.uuid)\n                else:\n                    err = \"the parent '%s' for the host '%s' is unknown!\" % (parent,\n                                                                             host.get_name())\n                    self.add_error(err)\n            # We find the id, we replace the names\n            host.parents = new_parents"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlinks hosts with realms by realms object.", "response": "def linkify_h_by_realms(self, realms):\n        \"\"\"Link hosts with realms\n\n        :param realms: realms object to link with\n        :type realms: alignak.objects.realm.Realms\n        :return: None\n        \"\"\"\n        default_realm = realms.get_default()\n        for host in self:\n            if not getattr(host, 'realm', None):\n                # Applying default realm to an host\n                host.realm = default_realm.uuid if default_realm else ''\n                host.realm_name = default_realm.get_name() if default_realm else ''\n                host.got_default_realm = True\n\n            if host.realm not in realms:\n                realm = realms.find_by_name(host.realm)\n                if not realm:\n                    continue\n                host.realm = realm.uuid\n            else:\n                realm = realms[host.realm]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlink hosts with hostgroups by hostgroup name", "response": "def linkify_h_by_hg(self, hostgroups):\n        \"\"\"Link hosts with hostgroups\n\n        :param hostgroups: hostgroups object to link with\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :return: None\n        \"\"\"\n        # Register host in the hostgroups\n        for host in self:\n            new_hostgroups = []\n            if hasattr(host, 'hostgroups') and host.hostgroups != []:\n                hgs = [n.strip() for n in host.hostgroups if n.strip()]\n                for hg_name in hgs:\n                    # TODO: should an unknown hostgroup raise an error ?\n                    hostgroup = hostgroups.find_by_name(hg_name)\n                    if hostgroup is not None:\n                        new_hostgroups.append(hostgroup.uuid)\n                    else:\n                        err = (\"the hostgroup '%s' of the host '%s' is \"\n                               \"unknown\" % (hg_name, host.host_name))\n                        host.add_error(err)\n            host.hostgroups = new_hostgroups"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexplodes hosts with hostgroups contactgroups and contactgroups", "response": "def explode(self, hostgroups, contactgroups):\n        \"\"\"Explode hosts with hostgroups, contactgroups::\n\n        * Add contact from contactgroups to host contacts\n        * Add host into their hostgroups as hostgroup members\n\n        :param hostgroups: Hostgroups to explode\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :param contactgroups: Contactgorups to explode\n        :type contactgroups: alignak.objects.contactgroup.Contactgroups\n        :return: None\n        \"\"\"\n        for template in list(self.templates.values()):\n            # items::explode_contact_groups_into_contacts\n            # take all contacts from our contact_groups into our contact property\n            self.explode_contact_groups_into_contacts(template, contactgroups)\n\n        # Register host in the hostgroups\n        for host in self:\n            # items::explode_contact_groups_into_contacts\n            # take all contacts from our contact_groups into our contact property\n            self.explode_contact_groups_into_contacts(host, contactgroups)\n\n            if hasattr(host, 'host_name') and hasattr(host, 'hostgroups'):\n                hname = host.host_name\n                for hostgroup in host.hostgroups:\n                    hostgroups.add_member(hname, hostgroup.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloops on hosts and register dependency between parent and son call Host. fill_parents_dependency", "response": "def apply_dependencies(self):\n        \"\"\"Loop on hosts and register dependency between parent and son\n\n        call Host.fill_parents_dependency()\n\n        :return: None\n        \"\"\"\n        for host in self:\n            for parent_id in getattr(host, 'parents', []):\n                if parent_id is None:\n                    continue\n                parent = self[parent_id]\n                if parent.active_checks_enabled:\n                    # Add parent in the list\n                    host.act_depend_of.append((parent_id, ['d', 'x', 's', 'f'], '', True))\n\n                    # Add child in the parent\n                    parent.act_depend_of_me.append((host.uuid, ['d', 'x', 's', 'f'], '', True))\n\n                    # And add the parent/child dep filling too, for broking\n                    parent.child_dependencies.add(host.uuid)\n                    host.parent_dependencies.add(parent_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding hosts that use the template defined in tpl_name", "response": "def find_hosts_that_use_template(self, tpl_name):\n        \"\"\"Find hosts that use the template defined in argument tpl_name\n\n        :param tpl_name: the template name we filter or\n        :type tpl_name: str\n        :return: list of the host_name of the hosts that got the template tpl_name in tags\n        :rtype: list[str]\n        \"\"\"\n        return [h.host_name for h in self if tpl_name in h.tags if hasattr(h, \"host_name\")]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the hosts list configuration is correct.", "response": "def is_correct(self):\n        \"\"\"Check if the hosts list configuration is correct ::\n\n        * check if any loop exists in each host dependencies\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        state = True\n\n        # Internal checks before executing inherited function...\n        loop = self.no_loop_in_parents(\"self\", \"parents\")\n        if loop:\n            self.add_error(\"Loop detected while checking hosts\")\n            state = False\n            for uuid, item in list(self.items.items()):\n                for elem in loop:\n                    if elem == uuid:\n                        self.add_error(\"Host %s is parent in dependency defined in %s\"\n                                       % (item.get_name(), item.imported_from))\n                    elif elem in item.parents:\n                        self.add_error(\"Host %s is child in dependency defined in %s\"\n                                       % (self[elem].get_name(), self[elem].imported_from))\n\n        return super(Hosts, self).is_correct() and state"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_me(self):  # pragma: no cover, seems not to be used anywhere\n        logger.info(\"And arbiter is launched with the hostname:%s \"\n                    \"from an arbiter point of view of addr:%s\", self.host_name, socket.getfqdn())\n        return self.host_name == socket.getfqdn() or self.host_name == socket.gethostname()", "response": "Check if the parameter name if same than name of this object\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_not_run(self):\n        logger.debug(\"[%s] do_not_run\", self.name)\n\n        try:\n            self.con.get('_do_not_run')\n            return True\n        except HTTPClientConnectionException as exp:  # pragma: no cover, simple protection\n            self.add_failed_check_attempt(\"Connection error when \"\n                                          \"sending do not run: %s\" % str(exp))\n            self.set_dead()\n        except HTTPClientTimeoutException as exp:  # pragma: no cover, simple protection\n            self.add_failed_check_attempt(\"Connection timeout when \"\n                                          \"sending do not run: %s\" % str(exp))\n        except HTTPClientException as exp:\n            self.add_failed_check_attempt(\"Error when \"\n                                          \"sending do not run: %s\" % str(exp))\n\n        return False", "response": "Check if satellite running or not\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_broks(self, broker_name):\n        logger.debug(\"Broker %s requests my broks list\", broker_name)\n        res = []\n        if not broker_name:\n            return res\n\n        for broker_link in list(self.brokers.values()):\n            if broker_name == broker_link.name:\n                for brok in sorted(broker_link.broks, key=lambda x: x.creation_time):\n                    # Only provide broks that did not yet sent to our external modules\n                    if getattr(brok, 'sent_to_externals', False):\n                        res.append(brok)\n                        brok.got = True\n                broker_link.broks = [b for b in broker_link.broks if not getattr(b, 'got', False)]\n                logger.debug(\"Providing %d broks to %s\", len(res), broker_name)\n                break\n        else:\n            logger.warning(\"Got a brok request from an unknown broker: %s\", broker_name)\n\n        return res", "response": "Send broks to a specific broker"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compensate_system_time_change(self, difference):  # pragma: no cover,\n        # pylint: disable=too-many-branches\n        # not with unit tests\n        \"\"\"Compensate a system time change of difference for all hosts/services/checks/notifs\n\n        :param difference: difference in seconds\n        :type difference: int\n        :return: None\n        \"\"\"\n        super(Alignak, self).compensate_system_time_change(difference)\n\n        # We only need to change some value\n        self.program_start = max(0, self.program_start + difference)\n\n        if not hasattr(self.sched, \"conf\"):\n            # Race condition where time change before getting conf\n            return\n\n        # Then we compensate all host/services\n        for host in self.sched.hosts:\n            host.compensate_system_time_change(difference)\n        for serv in self.sched.services:\n            serv.compensate_system_time_change(difference)\n\n        # Now all checks and actions\n        for chk in list(self.sched.checks.values()):\n            # Already launch checks should not be touch\n            if chk.status == u'scheduled' and chk.t_to_go is not None:\n                t_to_go = chk.t_to_go\n                ref = self.sched.find_item_by_id(chk.ref)\n                new_t = max(0, t_to_go + difference)\n                timeperiod = self.sched.timeperiods[ref.check_period]\n                if timeperiod is not None:\n                    # But it's no so simple, we must match the timeperiod\n                    new_t = timeperiod.get_next_valid_time_from_t(new_t)\n                # But maybe no there is no more new value! Not good :(\n                # Say as error, with error output\n                if new_t is None:\n                    chk.state = u'waitconsume'\n                    chk.exit_status = 2\n                    chk.output = '(Error: there is no available check time after time change!)'\n                    chk.check_time = time.time()\n                    chk.execution_time = 0\n                else:\n                    chk.t_to_go = new_t\n                    ref.next_chk = new_t\n\n        # Now all checks and actions\n        for act in list(self.sched.actions.values()):\n            # Already launch checks should not be touch\n            if act.status == u'scheduled':\n                t_to_go = act.t_to_go\n\n                #  Event handler do not have ref\n                ref_id = getattr(act, 'ref', None)\n                new_t = max(0, t_to_go + difference)\n\n                # Notification should be check with notification_period\n                if act.is_a == u'notification':\n                    ref = self.sched.find_item_by_id(ref_id)\n                    if ref.notification_period:\n                        # But it's no so simple, we must match the timeperiod\n                        notification_period = self.sched.timeperiods[ref.notification_period]\n                        new_t = notification_period.get_next_valid_time_from_t(new_t)\n                    # And got a creation_time variable too\n                    act.creation_time += difference\n\n                # But maybe no there is no more new value! Not good :(\n                # Say as error, with error output\n                if new_t is None:\n                    act.state = 'waitconsume'\n                    act.exit_status = 2\n                    act.output = '(Error: there is no available check time after time change!)'\n                    act.check_time = time.time()\n                    act.execution_time = 0\n                else:\n                    act.t_to_go = new_t", "response": "Compensate a system time change of difference for all hosts services checks and actions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_loop_turn(self):\n        if not self.first_scheduling:\n            # Ok, now all is initialized, we can make the initial broks\n            logger.info(\"First scheduling launched\")\n            _t0 = time.time()\n            # Program start brok\n            self.sched.initial_program_status()\n            # First scheduling\n            self.sched.schedule()\n            statsmgr.timer('first_scheduling', time.time() - _t0)\n            logger.info(\"First scheduling done\")\n\n            # Connect to our passive satellites if needed\n            for satellite in [s for s in list(self.pollers.values()) if s.passive]:\n                if not self.daemon_connection_init(satellite):\n                    logger.error(\"Passive satellite connection failed: %s\", satellite)\n\n            for satellite in [s for s in list(self.reactionners.values()) if s.passive]:\n                if not self.daemon_connection_init(satellite):\n                    logger.error(\"Passive satellite connection failed: %s\", satellite)\n\n            # Ticks are for recurrent function call like consume, del zombies etc\n            self.sched.ticks = 0\n            self.first_scheduling = True\n\n        # Each loop turn, execute the daemon specific treatment...\n        # only if the daemon has a configuration to manage\n        if self.sched.pushed_conf:\n            # If scheduling is not yet enabled, enable scheduling\n            if not self.sched.must_schedule:\n                self.sched.start_scheduling()\n                self.sched.before_run()\n            self.sched.run()\n        else:\n            logger.warning(\"#%d - No monitoring configuration to scheduler...\",\n                           self.loop_count)", "response": "This function is called by the Alignak daemon when a configuration has been received."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the configurations managed by this scheduler", "response": "def get_managed_configurations(self):\n        \"\"\"Get the configurations managed by this scheduler\n\n        The configuration managed by a scheduler is the self configuration got\n        by the scheduler during the dispatching.\n\n        :return: a dict of scheduler links with instance_id as key and\n        hash, push_flavor and configuration identifier as values\n        :rtype: dict\n        \"\"\"\n        # for scheduler_link in list(self.schedulers.values()):\n        #     res[scheduler_link.instance_id] = {\n        #         'hash': scheduler_link.hash,\n        #         'push_flavor': scheduler_link.push_flavor,\n        #         'managed_conf_id': scheduler_link.managed_conf_id\n        #     }\n\n        res = {}\n        if self.sched.pushed_conf and self.cur_conf and 'instance_id' in self.cur_conf:\n            res[self.cur_conf['instance_id']] = {\n                'hash': self.cur_conf['hash'],\n                'push_flavor': self.cur_conf['push_flavor'],\n                'managed_conf_id': self.cur_conf['managed_conf_id']\n            }\n        logger.debug(\"Get managed configuration: %s\", res)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup_new_conf(self):\n        # pylint: disable=too-many-statements, too-many-branches, too-many-locals\n        \"\"\"Setup new conf received for scheduler\n\n        :return: None\n        \"\"\"\n        # Execute the base class treatment...\n        super(Alignak, self).setup_new_conf()\n\n        # ...then our own specific treatment!\n        with self.conf_lock:\n            # self_conf is our own configuration from the alignak environment\n            # self_conf = self.cur_conf['self_conf']\n            logger.debug(\"Got config: %s\", self.cur_conf)\n            if 'conf_part' not in self.cur_conf:\n                self.cur_conf['conf_part'] = None\n            conf_part = self.cur_conf['conf_part']\n\n            # Ok now we can save the retention data\n            if self.sched.pushed_conf is not None:\n                self.sched.update_retention()\n\n            # Get the monitored objects configuration\n            t00 = time.time()\n            received_conf_part = None\n            try:\n                received_conf_part = unserialize(conf_part)\n                assert received_conf_part is not None\n            except AssertionError as exp:\n                # This to indicate that no configuration is managed by this scheduler...\n                logger.warning(\"No managed configuration received from arbiter\")\n            except AlignakClassLookupException as exp:  # pragma: no cover\n                # This to indicate that the new configuration is not managed...\n                self.new_conf = {\n                    \"_status\": \"Cannot un-serialize configuration received from arbiter\",\n                    \"_error\": str(exp)\n                }\n                logger.error(self.new_conf)\n                logger.error(\"Back trace of the error:\\n%s\", traceback.format_exc())\n                return\n            except Exception as exp:  # pylint: disable=broad-except\n                # This to indicate that the new configuration is not managed...\n                self.new_conf = {\n                    \"_status\": \"Cannot un-serialize configuration received from arbiter\",\n                    \"_error\": str(exp)\n                }\n                logger.error(self.new_conf)\n                self.exit_on_exception(exp, str(self.new_conf))\n\n            # if not received_conf_part:\n            #     return\n\n            logger.info(\"Monitored configuration %s received at %d. Un-serialized in %d secs\",\n                        received_conf_part, t00, time.time() - t00)\n            logger.info(\"Scheduler received configuration : %s\", received_conf_part)\n\n            # Now we create our pollers, reactionners and brokers\n            for link_type in ['pollers', 'reactionners', 'brokers']:\n                if link_type not in self.cur_conf['satellites']:\n                    logger.error(\"Missing %s in the configuration!\", link_type)\n                    continue\n\n                my_satellites = getattr(self, link_type, {})\n                received_satellites = self.cur_conf['satellites'][link_type]\n                for link_uuid in received_satellites:\n                    rs_conf = received_satellites[link_uuid]\n                    logger.debug(\"- received %s - %s: %s\", rs_conf['instance_id'],\n                                 rs_conf['type'], rs_conf['name'])\n\n                    # Must look if we already had a configuration and save our broks\n                    already_got = rs_conf['instance_id'] in my_satellites\n                    broks = []\n                    actions = {}\n                    wait_homerun = {}\n                    external_commands = {}\n                    running_id = 0\n                    if already_got:\n                        logger.warning(\"I already got: %s\", rs_conf['instance_id'])\n                        # Save some information\n                        running_id = my_satellites[link_uuid].running_id\n                        (broks, actions,\n                         wait_homerun, external_commands) = \\\n                            my_satellites[link_uuid].get_and_clear_context()\n                        # Delete the former link\n                        del my_satellites[link_uuid]\n\n                    # My new satellite link...\n                    new_link = SatelliteLink.get_a_satellite_link(link_type[:-1],\n                                                                  rs_conf)\n                    my_satellites[new_link.uuid] = new_link\n                    logger.info(\"I got a new %s satellite: %s\", link_type[:-1], new_link)\n\n                    new_link.running_id = running_id\n                    new_link.external_commands = external_commands\n                    new_link.broks = broks\n                    new_link.wait_homerun = wait_homerun\n                    new_link.actions = actions\n\n                    # Replacing the satellite address and port by those defined in satellite_map\n                    if new_link.name in self.cur_conf['override_conf'].get('satellite_map', {}):\n                        override_conf = self.cur_conf['override_conf']\n                        overriding = override_conf.get('satellite_map')[new_link.name]\n                        logger.warning(\"Do not override the configuration for: %s, with: %s. \"\n                                       \"Please check whether this is necessary!\",\n                                       new_link.name, overriding)\n\n            # First mix conf and override_conf to have our definitive conf\n            for prop in getattr(self.cur_conf, 'override_conf', []):\n                logger.debug(\"Overriden: %s / %s \", prop, getattr(received_conf_part, prop, None))\n                logger.debug(\"Overriding: %s / %s \", prop, self.cur_conf['override_conf'])\n                setattr(received_conf_part, prop, self.cur_conf['override_conf'].get(prop, None))\n\n            # Scheduler modules\n            if not self.have_modules:\n                try:\n                    logger.debug(\"Modules configuration: %s\", self.cur_conf['modules'])\n                    self.modules = unserialize(self.cur_conf['modules'], no_load=True)\n                except AlignakClassLookupException as exp:  # pragma: no cover, simple protection\n                    logger.error('Cannot un-serialize modules configuration '\n                                 'received from arbiter: %s', exp)\n                if self.modules:\n                    logger.debug(\"I received some modules configuration: %s\", self.modules)\n                    self.have_modules = True\n\n                    self.do_load_modules(self.modules)\n                    # and start external modules too\n                    self.modules_manager.start_external_instances()\n                else:\n                    logger.info(\"I do not have modules\")\n\n            if received_conf_part:\n                logger.info(\"Loading configuration...\")\n\n                # Propagate the global parameters to the configuration items\n                received_conf_part.explode_global_conf()\n\n                # We give the configuration to our scheduler\n                self.sched.reset()\n                self.sched.load_conf(self.cur_conf['instance_id'],\n                                     self.cur_conf['instance_name'],\n                                     received_conf_part)\n\n                # Once loaded, the scheduler has an inner pushed_conf object\n                logger.info(\"Loaded: %s\", self.sched.pushed_conf)\n\n                # Update the scheduler ticks according to the daemon configuration\n                self.sched.update_recurrent_works_tick(self)\n\n                # We must update our pushed configuration macros with correct values\n                # from the configuration parameters\n                # self.sched.pushed_conf.fill_resource_macros_names_macros()\n\n                # Creating the Macroresolver Class & unique instance\n                m_solver = MacroResolver()\n                m_solver.init(received_conf_part)\n\n                # Now create the external commands manager\n                # We are an applyer: our role is not to dispatch commands, but to apply them\n                ecm = ExternalCommandManager(\n                    received_conf_part, 'applyer', self.sched,\n                    received_conf_part.accept_passive_unknown_check_results,\n                    received_conf_part.log_external_commands)\n\n                # Scheduler needs to know about this external command manager to use it if necessary\n                self.sched.external_commands_manager = ecm\n\n                # Ok now we can load the retention data\n                self.sched.retention_load()\n\n                # Log hosts/services initial states\n                self.sched.log_initial_states()\n\n            # Create brok new conf\n            brok = Brok({'type': 'new_conf', 'data': {}})\n            self.sched.add_brok(brok)\n\n            # Initialize connection with all our satellites\n            logger.info(\"Initializing connection with my satellites:\")\n            my_satellites = self.get_links_of_type(s_type='')\n            for satellite in list(my_satellites.values()):\n                logger.info(\"- : %s/%s\", satellite.type, satellite.name)\n                if not self.daemon_connection_init(satellite):\n                    logger.error(\"Satellite connection failed: %s\", satellite)\n\n            if received_conf_part:\n                # Enable the scheduling process\n                logger.info(\"Loaded: %s\", self.sched.pushed_conf)\n                self.sched.start_scheduling()\n\n        # Now I have a configuration!\n        self.have_conf = True", "response": "Setup new conf received for scheduler\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans variables from previous configuration", "response": "def clean_previous_run(self):\n        \"\"\"Clean variables from previous configuration\n\n        :return: None\n        \"\"\"\n        # Execute the base class treatment...\n        super(Alignak, self).clean_previous_run()\n\n        # Clean all lists\n        self.pollers.clear()\n        self.reactionners.clear()\n        self.brokers.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_daemon_stats(self, details=False):\n        # Call the base Daemon one\n        res = super(Alignak, self).get_daemon_stats(details=details)\n\n        res.update({'name': self.name, 'type': self.type, 'monitored_objects': {}})\n\n        counters = res['counters']\n\n        # Satellites counters\n        counters['brokers'] = len(self.brokers)\n        counters['pollers'] = len(self.pollers)\n        counters['reactionners'] = len(self.reactionners)\n        counters['receivers'] = len(self.receivers)\n\n        if not self.sched:\n            return res\n\n        # # Hosts/services problems counters\n        # m_solver = MacroResolver()\n        # counters['hosts_problems'] = m_solver._get_total_host_problems()\n        # counters['hosts_unhandled_problems'] = m_solver._get_total_host_problems_unhandled()\n        # counters['services_problems'] = m_solver._get_total_service_problems()\n        # counters['services_unhandled_problems'] = m_solver._get_total_service_problems_unhandled()\n\n        # Get statistics from the scheduler\n        scheduler_stats = self.sched.get_scheduler_stats(details=details)\n        res['counters'].update(scheduler_stats['counters'])\n        scheduler_stats.pop('counters')\n        res.update(scheduler_stats)\n\n        return res", "response": "Increase the stats provided by the base Daemon class and return a dictionary of stats"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_monitoring_problems(self):\n        res = {}\n        if not self.sched:\n            return res\n\n        # Get statistics from the scheduler\n        scheduler_stats = self.sched.get_scheduler_stats(details=True)\n        if 'livesynthesis' in scheduler_stats:\n            res['livesynthesis'] = scheduler_stats['livesynthesis']\n        if 'problems' in scheduler_stats:\n            res['problems'] = scheduler_stats['problems']\n\n        return res", "response": "Get the current scheduler livesynthesis and problems dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge(self, services):\n        for extinfo in self:\n            if hasattr(extinfo, 'register') and not getattr(extinfo, 'register'):\n                # We don't have to merge template\n                continue\n            hosts_names = extinfo.get_name().split(\",\")\n            for host_name in hosts_names:\n                serv = services.find_srv_by_name_and_hostname(host_name,\n                                                              extinfo.service_description)\n                if serv is not None:\n                    # Fusion\n                    self.merge_extinfo(serv, extinfo)", "response": "Merge extended host information into one specific one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_extinfo(service, extinfo):\n        properties = ['notes', 'notes_url', 'icon_image', 'icon_image_alt']\n        # service properties have precedence over serviceextinfo properties\n        for prop in properties:\n            if getattr(service, prop) == '' and getattr(extinfo, prop) != '':\n                setattr(service, prop, getattr(extinfo, prop))", "response": "Merge extended host information into a service object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_a_satellite_link(sat_type, sat_dict):\n        cls = get_alignak_class('alignak.objects.%slink.%sLink' % (sat_type, sat_type.capitalize()))\n        return cls(params=sat_dict, parsing=False)", "response": "Get a SatelliteLink object for a given satellite type and a dictionary of satellite configuration data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the SatelliteLink live state.", "response": "def get_livestate(self):\n        \"\"\"Get the SatelliteLink live state.\n\n        The live state is a tuple information containing a state identifier and a message, where:\n            state is:\n            - 0 for an up and running satellite\n            - 1 if the satellite is not reachale\n            - 2 if the satellite is dead\n            - 3 else (not active)\n\n        :return: tuple\n        \"\"\"\n        livestate = 0\n        if self.active:\n            if not self.reachable:\n                livestate = 1\n            elif not self.alive:\n                livestate = 2\n        else:\n            livestate = 3\n\n        livestate_output = \"%s/%s is %s\" % (self.type, self.name, [\n            \"up and running.\",\n            \"warning because not reachable.\",\n            \"critical because not responding.\",\n            \"not active by configuration.\"\n        ][livestate])\n\n        return (livestate, livestate_output)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the satellite map for this Arbiter.", "response": "def set_arbiter_satellite_map(self, satellite_map=None):\n        \"\"\"\n            satellite_map is the satellites map in current context:\n                - A SatelliteLink is owned by an Arbiter\n                - satellite_map attribute of a SatelliteLink is the map defined\n                IN THE satellite configuration but for creating connections,\n                we need to have the satellites map from the Arbiter point of view\n\n        :return: None\n        \"\"\"\n        self.satellite_map = {\n            'address': self.address, 'port': self.port,\n            'use_ssl': self.use_ssl, 'hard_ssl_name_check': self.hard_ssl_name_check\n        }\n        if satellite_map:\n            self.satellite_map.update(satellite_map)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets and clear all of our broks actions external commands and homeruns", "response": "def get_and_clear_context(self):\n        \"\"\"Get and clean all of our broks, actions, external commands and homerun\n\n        :return: list of all broks of the satellite link\n        :rtype: list\n        \"\"\"\n        res = (self.broks, self.actions, self.wait_homerun, self.pushed_commands)\n        self.broks = []\n        self.actions = {}\n        self.wait_homerun = {}\n        self.pushed_commands = []\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the pushed configuration dictionary with the inner properties that are to be propagated to the satellite link.", "response": "def prepare_for_conf(self):\n        \"\"\"Initialize the pushed configuration dictionary\n        with the inner properties that are to be propagated to the satellite link.\n\n        :return: None\n        \"\"\"\n        logger.debug(\"- preparing: %s\", self)\n        self.cfg = {\n            'self_conf': self.give_satellite_cfg(),\n            'schedulers': {},\n            'arbiters': {}\n        }\n        logger.debug(\"- prepared: %s\", self.cfg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the default information for a satellite.", "response": "def give_satellite_cfg(self):\n        \"\"\"Get the default information for a satellite.\n\n        Overridden by the specific satellites links\n\n        :return: dictionary of information common to all the links\n        :rtype: dict\n        \"\"\"\n        # All the satellite link class properties that are 'to_send' are stored in a\n        # dictionary to be pushed to the satellite when the configuration is dispatched\n        res = {}\n        properties = self.__class__.properties\n        for prop, entry in list(properties.items()):\n            if hasattr(self, prop) and entry.to_send:\n                res[prop] = getattr(self, prop)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef give_satellite_json(self):\n        daemon_properties = ['type', 'name', 'uri', 'spare', 'configuration_sent',\n                             'realm_name', 'manage_sub_realms',\n                             'active', 'reachable', 'alive', 'passive',\n                             'last_check', 'polling_interval', 'max_check_attempts']\n\n        (livestate, livestate_output) = self.get_livestate()\n        res = {\n            \"livestate\": livestate,\n            \"livestate_output\": livestate_output\n        }\n        for sat_prop in daemon_properties:\n            res[sat_prop] = getattr(self, sat_prop, 'not_yet_defined')\n        return res", "response": "Get the json information for a satellite. This is a helper method that returns the json information that will be exposed by all the links."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef manages(self, cfg_part):\n        logger.debug(\"Do I (%s/%s) manage: %s, my managed configuration(s): %s\",\n                     self.type, self.name, cfg_part, self.cfg_managed)\n\n        # If we do not yet manage a configuration\n        if not self.cfg_managed:\n            logger.info(\"I (%s/%s) do not manage (yet) any configuration!\", self.type, self.name)\n            return False\n\n        # Check in the schedulers list configurations\n        for managed_cfg in list(self.cfg_managed.values()):\n            # If not even the cfg_id in the managed_conf, bail out\n            if managed_cfg['managed_conf_id'] == cfg_part.instance_id \\\n                    and managed_cfg['push_flavor'] == cfg_part.push_flavor:\n                logger.debug(\"I do manage this configuration: %s\", cfg_part)\n                break\n        else:\n            logger.warning(\"I (%s/%s) do not manage this configuration: %s\",\n                           self.type, self.name, cfg_part)\n            return False\n\n        return True", "response": "Tells if the satellite manages this configuration part\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_connection(self):\n        # Create the HTTP client for the connection\n        try:\n            self.con = HTTPClient(address=self.satellite_map['address'],\n                                  port=self.satellite_map['port'],\n                                  short_timeout=self.short_timeout, long_timeout=self.long_timeout,\n                                  use_ssl=self.satellite_map['use_ssl'],\n                                  strong_ssl=self.satellite_map['hard_ssl_name_check'])\n            self.uri = self.con.uri\n        except HTTPClientException as exp:\n            # logger.error(\"Error with '%s' when creating client: %s\", self.name, str(exp))\n            # Set the satellite as dead\n            self.set_dead()\n            raise LinkError(\"Error with '%s' when creating client: %s\" % (self.name, str(exp)))", "response": "Initialize a HTTP connection with a satellite and its uri attribute and set its uri attribute"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_alive(self):\n        was_alive = self.alive\n        self.alive = True\n        self.reachable = True\n        self.attempt = 0\n\n        # We came from dead to alive! We must propagate the good news\n        if not was_alive:\n            logger.info(\"Setting %s satellite as alive :)\", self.name)\n            self.broks.append(self.get_update_status_brok())", "response": "Set alive reachable and reset attempts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the satellite into dead state", "response": "def set_dead(self):\n        \"\"\"Set the satellite into dead state:\n        If we change state, raise a status brok update\n\n        :return:None\n        \"\"\"\n        was_alive = self.alive\n        self.alive = False\n        self.reachable = False\n        self.attempt = 0\n        # We will have to create a new connection...\n        self.con = None\n\n        # We are dead now! We must propagate the sad news...\n        if was_alive and not self.stopping:\n            logger.warning(\"Setting the satellite %s as dead :(\", self.name)\n            self.broks.append(self.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_failed_check_attempt(self, reason=''):\n        self.reachable = False\n        self.attempt = self.attempt + 1\n\n        logger.debug(\"Failed attempt for %s (%d/%d), reason: %s\",\n                     self.name, self.attempt, self.max_check_attempts, reason)\n        # Don't need to warn again and again if the satellite is already dead\n        # Only warn when it is alive\n        if self.alive:\n            if not self.stopping:\n                logger.warning(\"Add failed attempt for %s (%d/%d) - %s\",\n                               self.name, self.attempt, self.max_check_attempts, reason)\n            else:\n                logger.info(\"Stopping... failed attempt for %s (%d/%d) - also probably stopping\",\n                            self.name, self.attempt, self.max_check_attempts)\n\n        # If we reached the maximum attempts, set the daemon as dead\n        if self.attempt >= self.max_check_attempts:\n            if not self.stopping:\n                logger.warning(\"Set %s as dead, too much failed attempts (%d), last problem is: %s\",\n                               self.name, self.max_check_attempts, reason)\n            else:\n                logger.info(\"Stopping... set %s as dead, too much failed attempts (%d)\",\n                            self.name, self.max_check_attempts)\n\n            self.set_dead()", "response": "Add a failed attempt to the daemon and set the daemon as unreachable and add a failed attempt\n        to the daemon as dead."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef valid_connection(*outer_args, **outer_kwargs):\n        # pylint: disable=unused-argument, no-method-argument\n        \"\"\"Check if the daemon connection is established and valid\"\"\"\n        def decorator(func):  # pylint: disable=missing-docstring\n            def decorated(*args, **kwargs):  # pylint: disable=missing-docstring\n                # outer_args and outer_kwargs are the decorator arguments\n                # args and kwargs are the decorated function arguments\n                link = args[0]\n                if not link.con:\n                    raise LinkError(\"The connection is not created for %s\" % link.name)\n                if not link.running_id:\n                    raise LinkError(\"The connection is not initialized for %s\" % link.name)\n\n                return func(*args, **kwargs)\n            return decorated\n        return decorator", "response": "Check if the daemon connection is established and valid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef communicate(*outer_args, **outer_kwargs):\n        # pylint: disable=unused-argument, no-method-argument\n        \"\"\"Check if the daemon connection is authorized and valid\"\"\"\n        def decorator(func):  # pylint: disable=missing-docstring\n            def decorated(*args, **kwargs):  # pylint: disable=missing-docstring\n                # outer_args and outer_kwargs are the decorator arguments\n                # args and kwargs are the decorated function arguments\n                fn_name = func.__name__\n                link = args[0]\n                if not link.alive:\n                    logger.warning(\"%s is not alive for %s\", link.name, fn_name)\n                    return None\n\n                try:\n                    if not link.reachable:\n                        raise LinkError(\"The %s %s is not reachable\" % (link.type, link.name))\n\n                    logger.debug(\"[%s] Calling: %s, %s, %s\", link.name, fn_name, args, kwargs)\n                    return func(*args, **kwargs)\n                except HTTPClientConnectionException as exp:\n                    # A Connection error is raised when the daemon connection cannot be established\n                    # No way with the configuration parameters!\n                    if not link.stopping:\n                        logger.warning(\"A daemon (%s/%s) that we must be related with \"\n                                       \"cannot be connected: %s\", link.type, link.name, exp)\n                    else:\n                        logger.info(\"Stopping... daemon (%s/%s) cannot be connected. \"\n                                    \"It is also probably stopping or yet stopped.\",\n                                    link.type, link.name)\n                    link.set_dead()\n                except (LinkError, HTTPClientTimeoutException) as exp:\n                    link.add_failed_check_attempt(\"Connection timeout \"\n                                                  \"with '%s': %s\" % (fn_name, str(exp)))\n                    return False\n                except HTTPClientDataException as exp:\n                    # A Data error is raised when the daemon HTTP reponse is not 200!\n                    # No way with the communication if some problems exist in the daemon interface!\n                    # Abort all\n                    err = \"Some daemons that we must be related with \" \\\n                          \"have some interface problems. Sorry, I bail out\"\n                    logger.error(err)\n                    os.sys.exit(err)\n                except HTTPClientException as exp:\n                    link.add_failed_check_attempt(\"Error with '%s': %s\" % (fn_name, str(exp)))\n\n                return None\n\n            return decorated\n        return decorator", "response": "Check if the daemon connection is authorized and valid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a HTTP request to the satellite (GET /identity) Used to get the daemon running identifier that allows to know if the daemon got restarted This is called on connection initialization or re-connection If the daemon is notreachable, this function will raise an exception and the caller will receive a False as return :return: Boolean indicating if the running id was received :type: bool", "response": "def get_running_id(self):\n        \"\"\"Send a HTTP request to the satellite (GET /identity)\n        Used to get the daemon running identifier that allows to know if the daemon got restarted\n\n        This is called on connection initialization or re-connection\n\n        If the daemon is notreachable, this function will raise an exception and the caller\n        will receive a False as return\n\n        :return: Boolean indicating if the running id was received\n        :type: bool\n        \"\"\"\n        former_running_id = self.running_id\n\n        logger.info(\"  get the running identifier for %s %s.\", self.type, self.name)\n        # An exception is raised in this function if the daemon is not reachable\n        self.running_id = self.con.get('identity')\n        if isinstance(self.running_id, dict):\n            self.running_id = self.running_id['running_id']\n\n        if former_running_id == 0:\n            if self.running_id:\n                logger.info(\"  -> got: %s.\", self.running_id)\n                former_running_id = self.running_id\n\n        # If the daemon has just started or has been restarted: it has a new running_id.\n        if former_running_id != self.running_id:\n            if former_running_id:\n                logger.info(\"  -> The %s %s running identifier changed: %s. \"\n                            \"The daemon was certainly restarted!\",\n                            self.type, self.name, self.running_id)\n            # So we clear all verifications, they are obsolete now.\n            logger.info(\"The running id of the %s %s changed (%s), \"\n                        \"we must clear its context.\",\n                        self.type, self.name, self.running_id)\n            (_, _, _, _) = self.get_and_clear_context()\n\n        # Set the daemon as alive\n        self.set_alive()\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a stop request to the daemon", "response": "def stop_request(self, stop_now=False):\n        \"\"\"Send a stop request to the daemon\n\n        :param stop_now: stop now or go to stop wait mode\n        :type stop_now: bool\n        :return: the daemon response (True)\n        \"\"\"\n        logger.debug(\"Sending stop request to %s, stop now: %s\", self.name, stop_now)\n\n        res = self.con.get('stop_request', {'stop_now': '1' if stop_now else '0'})\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the information of the configuration dictionary.", "response": "def update_infos(self, forced=False, test=False):\n        \"\"\"Update satellite info each self.polling_interval seconds\n        so we smooth arbiter actions for just useful actions.\n\n        Raise a satellite update status Brok\n\n        If forced is True, then ignore the ping period. This is used when the configuration\n        has not yet been dispatched to the Arbiter satellites.\n\n        If test is True, do not really ping the daemon (useful for the unit tests only)\n\n        :param forced: ignore the ping smoothing\n        :type forced: bool\n        :param test:\n        :type test: bool\n        :return:\n        None if the last request is too recent,\n        False if a timeout was raised during the request,\n        else the managed configurations dictionary\n        \"\"\"\n        logger.debug(\"Update informations, forced: %s\", forced)\n\n        # First look if it's not too early to ping\n        now = time.time()\n        if not forced and self.last_check and self.last_check + self.polling_interval > now:\n            logger.debug(\"Too early to ping %s, ping period is %ds!, last check: %d, now: %d\",\n                         self.name, self.polling_interval, self.last_check, now)\n            return None\n\n        self.get_conf(test=test)\n\n        # Update the daemon last check timestamp\n        self.last_check = time.time()\n\n        # Update the state of this element\n        self.broks.append(self.get_update_status_brok())\n\n        return self.cfg_managed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a HTTP request to the satellite to get the daemon statistics", "response": "def get_daemon_stats(self, details=False):\n        \"\"\"Send a HTTP request to the satellite (GET /get_daemon_stats)\n\n        :return: Daemon statistics\n        :rtype: dict\n        \"\"\"\n        logger.debug(\"Get daemon statistics for %s, %s %s\", self.name, self.alive, self.reachable)\n        return self.con.get('stats%s' % ('?details=1' if details else ''))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_initial_broks(self, broker_name):\n        logger.debug(\"Getting initial broks for %s, %s %s\", self.name, self.alive, self.reachable)\n        return self.con.get('_initial_broks', {'broker_name': broker_name}, wait=True)", "response": "Send a HTTP request to the satellite to get the initial broks for a specific broker"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a HTTP request to the satellite to wait for new configuration to be available", "response": "def wait_new_conf(self):\n        \"\"\"Send a HTTP request to the satellite (GET /wait_new_conf)\n\n        :return: True if wait new conf, otherwise False\n        :rtype: bool\n        \"\"\"\n        logger.debug(\"Wait new configuration for %s, %s %s\", self.name, self.alive, self.reachable)\n        return self.con.get('_wait_new_conf')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put_conf(self, configuration, test=False):\n        logger.debug(\"Sending configuration to %s, %s %s\", self.name, self.alive, self.reachable)\n        # ----------\n        if test:\n            setattr(self, 'unit_test_pushed_configuration', configuration)\n            # print(\"*** unit tests - sent configuration %s: %s\" % (self.name, configuration))\n            return True\n        # ----------\n\n        return self.con.post('_push_configuration', {'conf': configuration}, wait=True)", "response": "Send the configuration to the satellite"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a HTTP request to the satellite to know if the satellite has a configuration", "response": "def has_a_conf(self, magic_hash=None):  # pragma: no cover\n        \"\"\"Send a HTTP request to the satellite (GET /have_conf)\n        Used to know if the satellite has a conf\n\n        :param magic_hash: Config hash. Only used for HA arbiter communication\n        :type magic_hash: int\n        :return: Boolean indicating if the satellite has a (specific) configuration\n        :type: bool\n        \"\"\"\n        logger.debug(\"Have a configuration for %s, %s %s\", self.name, self.alive, self.reachable)\n        self.have_conf = self.con.get('_have_conf', {'magic_hash': magic_hash})\n        return self.have_conf"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the managed configuration for a specific unit test", "response": "def get_conf(self, test=False):\n        \"\"\"Send a HTTP request to the satellite (GET /managed_configurations)\n        and update the cfg_managed attribute with the new information\n        Set to {} on failure\n\n        the managed configurations are a dictionary which keys are the scheduler link instance id\n        and the values are the push_flavor\n\n        If test is True, returns the unit test internally stored configuration\n\n        Returns False if a timeout is raised\n\n        :return: see @communicate, or the managed configuration\n        \"\"\"\n        logger.debug(\"Get managed configuration for %s, %s %s\",\n                     self.name, self.alive, self.reachable)\n        # ----------\n        if test:\n            self.cfg_managed = {}\n            self.have_conf = True\n            logger.debug(\"Get managed configuration test ...\")\n            if getattr(self, 'unit_test_pushed_configuration', None) is not None:\n                # Note this is a dict not a SatelliteLink object !\n                for scheduler_link in self.unit_test_pushed_configuration['schedulers'].values():\n                    self.cfg_managed[scheduler_link['instance_id']] = {\n                        'hash': scheduler_link['hash'],\n                        'push_flavor': scheduler_link['push_flavor'],\n                        'managed_conf_id': scheduler_link['managed_conf_id']\n                    }\n            # print(\"*** unit tests - get managed configuration %s: %s\"\n            #       % (self.name, self.cfg_managed))\n        # ----------\n        else:\n            self.cfg_managed = self.con.get('managed_configurations')\n            logger.debug(\"My (%s) fresh managed configuration: %s\", self.name, self.cfg_managed)\n\n        self.have_conf = (self.cfg_managed != {})\n\n        return self.cfg_managed"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef push_broks(self, broks):\n        logger.debug(\"[%s] Pushing %d broks\", self.name, len(broks))\n        return self.con.post('_push_broks', {'broks': broks}, wait=True)", "response": "Send a list of broks to the satellite"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nposts the actions to the satellite.", "response": "def push_actions(self, actions, scheduler_instance_id):\n        \"\"\"Post the actions to execute to the satellite.\n        Indeed, a scheduler post its checks to a poller and its actions to a reactionner.\n\n        :param actions: Action list to send\n        :type actions: list\n        :param scheduler_instance_id: Scheduler instance identifier\n        :type scheduler_instance_id: uuid\n        :return: True on success, False on failure\n        :rtype: bool\n        \"\"\"\n        logger.debug(\"Pushing %d actions from %s\", len(actions), scheduler_instance_id)\n        return self.con.post('_push_actions', {'actions': actions,\n                                               'scheduler_instance_id': scheduler_instance_id},\n                             wait=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef push_results(self, results, scheduler_name):\n        logger.debug(\"Pushing %d results\", len(results))\n        result = self.con.post('put_results', {'results': results, 'from': scheduler_name},\n                               wait=True)\n        return result", "response": "Send a list of results to the satellite"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend external commands to the satellite", "response": "def push_external_commands(self, commands):\n        \"\"\"Send a HTTP request to the satellite (POST /r_un_external_commands)\n        to send the external commands to the satellite\n\n        :param results: Results list to send\n        :type results: list\n        :return: True on success, False on failure\n        :rtype: bool\n        \"\"\"\n        logger.debug(\"Pushing %d external commands\", len(commands))\n        return self.con.post('_run_external_commands', {'cmds': commands}, wait=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a HTTP request to the satellite to get the external commands from the satellite", "response": "def get_external_commands(self):\n        \"\"\"Send a HTTP request to the satellite (GET /_external_commands) to\n        get the external commands from the satellite.\n\n        :return: External Command list on success, [] on failure\n        :rtype: list\n        \"\"\"\n        res = self.con.get('_external_commands', wait=False)\n        logger.debug(\"Got %d external commands from %s: %s\", len(res), self.name, res)\n        return unserialize(res, True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_broks(self, broker_name):\n        res = self.con.get('_broks', {'broker_name': broker_name}, wait=False)\n        logger.debug(\"Got broks from %s: %s\", self.name, res)\n        return unserialize(res, True)", "response": "Send a HTTP request to the satellite to get broks from the satellite."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a HTTP request to the satellite to get monitoring events from the satellite.", "response": "def get_events(self):\n        \"\"\"Send a HTTP request to the satellite (GET /_events)\n        Get monitoring events from the satellite.\n\n        :return: Broks list on success, [] on failure\n        :rtype: list\n        \"\"\"\n        res = self.con.get('_events', wait=False)\n        logger.debug(\"Got events from %s: %s\", self.name, res)\n        return unserialize(res, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_results(self, scheduler_instance_id):\n        res = self.con.get('_results', {'scheduler_instance_id': scheduler_instance_id}, wait=True)\n        logger.debug(\"Got %d results from %s: %s\", len(res), self.name, res)\n        return res", "response": "Send a HTTP request to the satellite to get the results of the actions from the satellite"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_actions(self, params):\n        res = self.con.get('_checks', params, wait=True)\n        logger.debug(\"Got checks to execute from %s: %s\", self.name, res)\n        return unserialize(res, True)", "response": "Send a HTTP request to the satellite GET / _checks"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlinks modules and Satellite links", "response": "def linkify(self, modules):\n        \"\"\"Link modules and Satellite links\n\n        :param modules: Module object list\n        :type modules: alignak.objects.module.Modules\n        :return: None\n        \"\"\"\n        logger.debug(\"Linkify %s with %s\", self, modules)\n        self.linkify_s_by_module(modules)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_return_from(self, notif):\n        self.exit_status = notif.exit_status\n        self.execution_time = notif.execution_time", "response": "Setter of exit_status and execution_time attributes of self."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a initial status brok with wanted data", "response": "def get_initial_status_brok(self):\n        \"\"\"Get a initial status brok\n\n        :return: brok with wanted data\n        :rtype: alignak.brok.Brok\n        \"\"\"\n        data = {'uuid': self.uuid}\n        self.fill_data_brok_from(data, 'full_status')\n        return Brok({'type': 'notification_raise', 'data': data})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef serialize(self):\n        res = super(Notification, self).serialize()\n\n        if res['command_call'] is not None:\n            if not isinstance(res['command_call'], string_types) and \\\n                    not isinstance(res['command_call'], dict):\n                res['command_call'] = res['command_call'].serialize()\n        return res", "response": "This function serialize into a simple dict object. It is used when transferring data to other daemons."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an object to the daemon internal list.", "response": "def add(self, elt):\n        \"\"\"Generic function to add objects to the daemon internal lists.\n        Manage Broks, External commands and Messages (from modules queues)\n\n        :param elt: object to add\n        :type elt: alignak.AlignakObject\n        :return: None\n        \"\"\"\n        if isinstance(elt, Brok):\n            # For brok, we tag the brok with our instance_id\n            elt.instance_id = self.instance_id\n            if elt.type == 'monitoring_log':\n                # The brok is a monitoring event\n                with self.events_lock:\n                    self.events.append(elt)\n                statsmgr.counter('events', 1)\n            else:\n                with self.broks_lock:\n                    self.broks.append(elt)\n            statsmgr.counter('broks.added', 1)\n        elif isinstance(elt, ExternalCommand):\n            logger.debug(\"Queuing an external command '%s'\", str(elt.__dict__))\n            with self.external_commands_lock:\n                self.external_commands.append(elt)\n                statsmgr.counter('external-commands.added', 1)\n        # Maybe we got a Message from the modules, it's way to ask something\n        # like from now a full data from a scheduler for example.\n        elif isinstance(elt, Message):\n            # We got a message, great!\n            logger.debug(str(elt.__dict__))\n            if elt.get_type() == 'NeedData':\n                data = elt.get_data()\n                # Full instance id means: I got no data for this scheduler\n                # so give me all dumb-ass!\n                if 'full_instance_id' in data:\n                    c_id = data['full_instance_id']\n                    source = getattr(elt, 'source', getattr(elt, '_source', None))\n                    logger.info('The module %s is asking me to get all initial data '\n                                'from the scheduler %d',\n                                source, c_id)\n                    # so we just reset the connection and the running_id,\n                    # it will just get all new things\n                    try:\n                        self.schedulers[c_id]['con'] = None\n                        self.schedulers[c_id]['running_id'] = 0\n                    except KeyError:  # maybe this instance was not known, forget it\n                        logger.warning(\"the module %s ask me a full_instance_id \"\n                                       \"for an unknown ID (%d)!\", source, c_id)\n            # Maybe a module tells me that it's dead, I must log its last words...\n            if elt.get_type() == 'ICrash':\n                data = elt.get_data()\n                logger.error('the module %s just crash! Please look at the traceback:',\n                             data['name'])\n                logger.error(data['trace'])\n\n            statsmgr.counter('message.added', 1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a brok. We put it to the modules clfset.", "response": "def manage_brok(self, brok):\n        \"\"\"Get a brok.\n        We put brok data to the modules\n\n        :param brok: object with data\n        :type brok: object\n        :return: None\n        \"\"\"\n        # Unserialize the brok before consuming it\n        brok.prepare()\n\n        for module in self.modules_manager.get_internal_instances():\n            try:\n                _t0 = time.time()\n                module.manage_brok(brok)\n                statsmgr.timer('manage-broks.internal.%s' % module.get_name(), time.time() - _t0)\n            except Exception as exp:  # pylint: disable=broad-except\n                logger.warning(\"The module %s raised an exception: %s, \"\n                               \"I'm tagging it to restart later\", module.get_name(), str(exp))\n                logger.exception(exp)\n                self.modules_manager.set_to_restart(module)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all broks from self. broks_internal_raised and append them to our broks_internal_raised list", "response": "def get_internal_broks(self):\n        \"\"\"Get all broks from self.broks_internal_raised and append them to our broks\n        to manage\n\n        :return: None\n        \"\"\"\n        statsmgr.gauge('get-new-broks-count.broker', len(self.internal_broks))\n        # Add the broks to our global list\n        self.external_broks.extend(self.internal_broks)\n        self.internal_broks = []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_arbiter_broks(self):\n        with self.arbiter_broks_lock:\n            statsmgr.gauge('get-new-broks-count.arbiter', len(self.arbiter_broks))\n            # Add the broks to our global list\n            self.external_broks.extend(self.arbiter_broks)\n            self.arbiter_broks = []", "response": "Get the broks from the arbiters and the arbiter_broks_lock."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_new_broks(self):\n        for satellites in [self.schedulers, self.pollers, self.reactionners, self.receivers]:\n            for satellite_link in list(satellites.values()):\n                logger.debug(\"Getting broks from %s\", satellite_link)\n\n                _t0 = time.time()\n                try:\n                    tmp_broks = satellite_link.get_broks(self.name)\n                except LinkError:\n                    logger.warning(\"Daemon %s connection failed, I could not get the broks!\",\n                                   satellite_link)\n                else:\n                    if tmp_broks:\n                        logger.debug(\"Got %d Broks from %s in %s\",\n                                     len(tmp_broks), satellite_link.name, time.time() - _t0)\n                        statsmgr.gauge('get-new-broks-count.%s'\n                                       % (satellite_link.name), len(tmp_broks))\n                        statsmgr.timer('get-new-broks-time.%s'\n                                       % (satellite_link.name), time.time() - _t0)\n                        for brok in tmp_broks:\n                            brok.instance_id = satellite_link.instance_id\n\n                        # Add the broks to our global list\n                        self.external_broks.extend(tmp_broks)", "response": "Get new broks from our satellites"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup_new_conf(self):\n        # pylint: disable=too-many-branches, too-many-locals\n        \"\"\"Broker custom setup_new_conf method\n\n        This function calls the base satellite treatment and manages the configuration needed\n        for a broker daemon:\n        - get and configure its pollers, reactionners and receivers relation\n        - configure the modules\n\n        :return: None\n        \"\"\"\n        # Execute the base class treatment...\n        super(Broker, self).setup_new_conf()\n\n        # ...then our own specific treatment!\n        with self.conf_lock:\n            # # self_conf is our own configuration from the alignak environment\n            # self_conf = self.cur_conf['self_conf']\n            self.got_initial_broks = False\n\n            # Now we create our pollers, reactionners and receivers\n            for link_type in ['pollers', 'reactionners', 'receivers']:\n                if link_type not in self.cur_conf['satellites']:\n                    logger.error(\"No %s in the configuration!\", link_type)\n                    continue\n\n                my_satellites = getattr(self, link_type, {})\n                received_satellites = self.cur_conf['satellites'][link_type]\n                for link_uuid in received_satellites:\n                    rs_conf = received_satellites[link_uuid]\n                    logger.debug(\"- received %s - %s: %s\", rs_conf['instance_id'],\n                                 rs_conf['type'], rs_conf['name'])\n\n                    # Must look if we already had a configuration and save our broks\n                    already_got = rs_conf['instance_id'] in my_satellites\n                    broks = []\n                    actions = {}\n                    wait_homerun = {}\n                    external_commands = {}\n                    running_id = 0\n                    if already_got:\n                        logger.warning(\"I already got: %s\", rs_conf['instance_id'])\n                        # Save some information\n                        running_id = my_satellites[link_uuid].running_id\n                        (broks, actions,\n                         wait_homerun, external_commands) = \\\n                            my_satellites[link_uuid].get_and_clear_context()\n                        # Delete the former link\n                        del my_satellites[link_uuid]\n\n                    # My new satellite link...\n                    new_link = SatelliteLink.get_a_satellite_link(link_type[:-1],\n                                                                  rs_conf)\n                    my_satellites[new_link.uuid] = new_link\n                    logger.info(\"I got a new %s satellite: %s\", link_type[:-1], new_link)\n\n                    new_link.running_id = running_id\n                    new_link.external_commands = external_commands\n                    new_link.broks = broks\n                    new_link.wait_homerun = wait_homerun\n                    new_link.actions = actions\n\n                    # Replace satellite address and port by those defined in satellite_map\n                    # todo: check if it is really necessary! Add a unit test for this\n                    # Not sure about this because of the daemons/satellites configuration\n                    # if new_link.name in self_conf.get('satellite_map', {}):\n                    #     new_link = dict(new_link)  # make a copy\n                    #     new_link.update(self_conf.get('satellite_map', {})[new_link.name])\n\n            if not self.have_modules:\n                try:\n                    self.modules = unserialize(self.cur_conf['modules'], no_load=True)\n                except AlignakClassLookupException as exp:  # pragma: no cover, simple protection\n                    logger.error('Cannot un-serialize modules configuration '\n                                 'received from arbiter: %s', exp)\n                if self.modules:\n                    logger.info(\"I received some modules configuration: %s\", self.modules)\n                    self.have_modules = True\n\n                    # Ok now start, or restart them!\n                    # Set modules, init them and start external ones\n                    self.do_load_modules(self.modules)\n                    # and start external modules too\n                    self.modules_manager.start_external_instances()\n                else:\n                    logger.info(\"I do not have modules\")\n\n            # Initialize connection with my schedulers first\n            logger.info(\"Initializing connection with my schedulers:\")\n            my_satellites = self.get_links_of_type(s_type='scheduler')\n            for satellite in list(my_satellites.values()):\n                logger.info(\"- %s/%s\", satellite.type, satellite.name)\n                if not self.daemon_connection_init(satellite):\n                    logger.error(\"Satellite connection failed: %s\", satellite)\n\n            # Initialize connection with all our satellites\n            logger.info(\"Initializing connection with my satellites:\")\n            for sat_type in ['arbiter', 'reactionner', 'poller', 'receiver']:\n                my_satellites = self.get_links_of_type(s_type=sat_type)\n                for satellite in list(my_satellites.values()):\n                    logger.info(\"- %s/%s\", satellite.type, satellite.name)\n                    if not self.daemon_connection_init(satellite):\n                        logger.error(\"Satellite connection failed: %s\", satellite)\n\n        # Now I have a configuration!\n        self.have_conf = True", "response": "This method is called by the broker class to create the configuration needed by the broker."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_previous_run(self):\n        # Execute the base class treatment...\n        super(Broker, self).clean_previous_run()\n\n        # Clean all satellites relations\n        self.pollers.clear()\n        self.reactionners.clear()\n        self.receivers.clear()\n\n        # Clean our internal objects\n        self.external_broks = self.external_broks[:]\n        self.internal_broks = self.internal_broks[:]\n        with self.arbiter_broks_lock:\n            self.arbiter_broks = self.arbiter_broks[:]\n        self.external_commands = self.external_commands[:]", "response": "Clean all data from the broker and all related objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nloop used to: * get initial status broks * check if modules are alive, if not restart them * get broks from ourself, the arbiters and our satellites * add broks to the queue of each external module * manage broks with each internal module If the internal broks management is longer than 0.8 seconds, postpone to hte next loop turn to avoid overloading the broker daemon. :return: None", "response": "def do_loop_turn(self):\n        # pylint: disable=too-many-branches\n        \"\"\"Loop used to:\n         * get initial status broks\n         * check if modules are alive, if not restart them\n         * get broks from ourself, the arbiters and our satellites\n         * add broks to the queue of each external module\n         * manage broks with each internal module\n\n         If the internal broks management is longer than 0.8 seconds, postpone to hte next\n         loop turn to avoid overloading the broker daemon.\n\n         :return: None\n        \"\"\"\n        if not self.got_initial_broks:\n            # Asking initial broks from my schedulers\n            my_satellites = self.get_links_of_type(s_type='scheduler')\n            for satellite in list(my_satellites.values()):\n                logger.info(\"Asking my initial broks from '%s'\", satellite.name)\n                _t0 = time.time()\n                try:\n                    my_initial_broks = satellite.get_initial_broks(self.name)\n                    statsmgr.timer('broks.initial.%s.time' % satellite.name, time.time() - _t0)\n                    if not my_initial_broks:\n                        logger.info(\"No initial broks were raised, \"\n                                    \"my scheduler is not yet ready...\")\n                        return\n\n                    self.got_initial_broks = True\n                    logger.debug(\"Got %d initial broks from '%s'\",\n                                 my_initial_broks, satellite.name)\n                    statsmgr.gauge('broks.initial.%s.count' % satellite.name, my_initial_broks)\n                except LinkError as exp:\n                    logger.warning(\"Scheduler connection failed, I could not get initial broks!\")\n\n        logger.debug(\"Begin Loop: still some old broks to manage (%d)\", len(self.external_broks))\n        if self.external_broks:\n            statsmgr.gauge('unmanaged.broks', len(self.external_broks))\n\n        # Try to see if one of my module is dead, and restart previously dead modules\n        self.check_and_del_zombie_modules()\n\n        # Call modules that manage a starting tick pass\n        _t0 = time.time()\n        self.hook_point('tick')\n        statsmgr.timer('hook.tick', time.time() - _t0)\n\n        # Maybe the last loop we did raised some broks internally\n        self.get_internal_broks()\n\n        # Also reap broks sent from the arbiters\n        self.get_arbiter_broks()\n\n        # Now get broks from our distant daemons\n        self.get_new_broks()\n\n        # Get the list of broks not yet sent to our external modules\n        _t0 = time.time()\n        broks_to_send = [brok for brok in self.external_broks if getattr(brok, 'to_be_sent', True)]\n        statsmgr.gauge('get-new-broks-count.to_send', len(broks_to_send))\n\n        # Send the broks to all external modules to_q queue so they can get the whole packet\n        # beware, the sub-process/queue can be die/close, so we put to restart the whole module\n        # instead of killing ourselves :)\n        for module in self.modules_manager.get_external_instances():\n            try:\n                _t00 = time.time()\n                queue_size = module.to_q.qsize()\n                statsmgr.gauge('queues.external.%s.to.size' % module.get_name(), queue_size)\n                module.to_q.put(broks_to_send)\n                statsmgr.timer('queues.external.%s.to.put' % module.get_name(), time.time() - _t00)\n            except Exception as exp:  # pylint: disable=broad-except\n                # first we must find the modules\n                logger.warning(\"Module %s queue exception: %s, I'm tagging it to restart later\",\n                               module.get_name(), str(exp))\n                logger.exception(exp)\n                self.modules_manager.set_to_restart(module)\n\n        # No more need to send them\n        for brok in broks_to_send:\n            brok.to_be_sent = False\n        logger.debug(\"Time to send %s broks (%d secs)\", len(broks_to_send), time.time() - _t0)\n\n        # Make the internal modules manage the broks\n        start = time.time()\n        while self.external_broks:\n            now = time.time()\n            # Do not 'manage' more than 0.8s, we must get new broks almost every second\n            if now - start > 0.8:\n                logger.info(\"I did not yet managed all my broks, still %d broks\",\n                            len(self.external_broks))\n                break\n\n            # Get the first brok in the list\n            brok = self.external_broks.pop(0)\n            if self.modules_manager.get_internal_instances():\n                self.manage_brok(brok)\n                # Make a very short pause to avoid overloading\n                self.make_a_pause(0.01, check_time_change=False)\n            else:\n                if getattr(brok, 'to_be_sent', False):\n                    self.external_broks.append(brok)\n\n        # Maybe our external modules raised 'objects', so get them\n        if self.get_objects_from_from_queues():\n            statsmgr.gauge('external-commands.got.count', len(self.external_commands))\n            statsmgr.gauge('broks.got.count', len(self.external_broks))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nincrease the stats provided by the base Daemon class", "response": "def get_daemon_stats(self, details=False):\n        \"\"\"Increase the stats provided by the Daemon base class\n\n        :return: stats dictionary\n        :rtype: dict\n        \"\"\"\n        # Call the base Daemon one\n        res = super(Broker, self).get_daemon_stats(details=details)\n\n        res.update({'name': self.name, 'type': self.type})\n\n        counters = res['counters']\n        counters['broks-external'] = len(self.external_broks)\n        counters['broks-internal'] = len(self.internal_broks)\n        counters['broks-arbiter'] = len(self.arbiter_broks)\n        counters['satellites.pollers'] = len(self.pollers)\n        counters['satellites.reactionners'] = len(self.reactionners)\n        counters['satellites.receivers'] = len(self.receivers)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_group_members(self, members):\n        if not isinstance(members, list):\n            members = [members]\n\n        if not getattr(self, 'group_members', None):\n            self.group_members = members\n        else:\n            self.group_members.extend(members)", "response": "Adds a new group member to the groups list\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the attributes of a realm object to include the satellite types that are declared in the provided satellites.", "response": "def prepare_satellites(self, satellites):\n        \"\"\"Update the following attributes of a realm::\n\n        * nb_*satellite type*s\n        * self.potential_*satellite type*s\n\n        (satellite types are scheduler, reactionner, poller, broker and receiver)\n\n        :param satellites: dict of SatelliteLink objects\n        :type satellites: dict\n        :return: None\n        \"\"\"\n        for sat_type in [\"scheduler\", \"reactionner\", \"poller\", \"broker\", \"receiver\"]:\n            # We get potential TYPE at realm level first\n            for sat_link_uuid in getattr(self, \"%ss\" % sat_type):\n                if sat_link_uuid not in satellites:\n                    continue\n                sat_link = satellites[sat_link_uuid]\n\n                # Found our declared satellite in the provided satellites\n                if sat_link.active and not sat_link.spare:\n                    # Generic increment : realm.nb_TYPE += 1\n                    setattr(self, \"nb_%ss\" % sat_type, getattr(self, \"nb_%ss\" % sat_type) + 1)\n                    break\n                else:\n                    self.add_error(\"Realm %s, satellite %s declared in the realm is not found \"\n                                   \"in the allowed satellites!\" % (self.name, sat_link.name))\n                    logger.error(\"Satellite %s declared in the realm %s not found \"\n                                 \"in the allowed satellites!\", sat_link.name, self.name)\n\n        logger.info(\" Realm %s: (in/potential) (schedulers:%d/%d) (pollers:%d/%d) \"\n                    \"(reactionners:%d/%d) (brokers:%d/%d) (receivers:%d/%d)\", self.name,\n                    self.nb_schedulers, len(self.potential_schedulers),\n                    self.nb_pollers, len(self.potential_pollers),\n                    self.nb_reactionners, len(self.potential_reactionners),\n                    self.nb_brokers, len(self.potential_brokers),\n                    self.nb_receivers, len(self.potential_receivers))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_realms_by_explosion(self, realms):\n        # If rec_tag is already set, then we detected a loop in the realms hierarchy!\n        if getattr(self, 'rec_tag', False):\n            self.add_error(\"Error: there is a loop in the realm definition %s\" % self.get_name())\n            return None\n\n        # Ok, not in a loop, we tag the realm and parse its members\n        self.rec_tag = True\n\n        # Order realm members list by name\n        self.realm_members = sorted(self.realm_members)\n        for member in self.realm_members:\n            realm = realms.find_by_name(member)\n            if not realm:\n                self.add_unknown_members(member)\n                continue\n\n            children = realm.get_realms_by_explosion(realms)\n            if children is None:\n                # We got a loop in our children definition\n                self.all_sub_members = []\n                self.realm_members = []\n                return None\n\n        # Return the list of all unique members\n        return self.all_sub_members", "response": "Get all members of this realm including members of sub - realms on multi - levels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the realm level in the realms hierarchy.", "response": "def set_level(self, level, realms):\n        \"\"\"Set the realm level in the realms hierarchy\n\n        :return: None\n        \"\"\"\n        self.level = level\n        if not self.level:\n            logger.info(\"- %s\", self.get_name())\n        else:\n            logger.info(\" %s %s\", '+' * self.level, self.get_name())\n        self.all_sub_members = []\n        self.all_sub_members_names = []\n        for child in sorted(self.realm_members):\n            child = realms.find_by_name(child)\n            if not child:\n                continue\n\n            self.all_sub_members.append(child.uuid)\n            self.all_sub_members_names.append(child.get_name())\n            grand_children = child.set_level(self.level + 1, realms)\n            for grand_child in grand_children:\n                if grand_child in self.all_sub_members_names:\n                    continue\n                grand_child = realms.find_by_name(grand_child)\n                if grand_child:\n                    self.all_sub_members_names.append(grand_child.get_name())\n                    self.all_sub_members.append(grand_child.uuid)\n        return self.all_sub_members_names"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_all_subs_satellites_by_type(self, sat_type, realms):\n        res = copy.copy(getattr(self, sat_type))\n        for member in self.all_sub_members:\n            res.extend(realms[member].get_all_subs_satellites_by_type(sat_type, realms))\n        return res", "response": "Get all satellite objects of the wanted type in this realm recursively"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_satellites_by_type(self, s_type):\n\n        if hasattr(self, s_type + 's'):\n            return getattr(self, s_type + 's')\n\n        logger.debug(\"[realm %s] do not have this kind of satellites: %s\", self.name, s_type)\n        return []", "response": "Generic function to access one of the satellite attributes by type"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a configuration dictionary with the pollers reactionners and receivers links for a broker.", "response": "def get_links_for_a_broker(self, pollers, reactionners, receivers, realms,\n                               manage_sub_realms=False):\n        \"\"\"Get a configuration dictionary with pollers, reactionners and receivers links\n        for a broker\n\n        :param pollers: pollers\n        :type pollers:\n        :param reactionners: reactionners\n        :type reactionners:\n        :param receivers: receivers\n        :type receivers:\n        :param realms: realms\n        :type realms:\n        :param manage_sub_realms:\n        :type manage_sub_realms: True if the borker manages sub realms\n\n        :return: dict containing pollers, reactionners and receivers links (key is satellite id)\n        :rtype: dict\n        \"\"\"\n\n        # Create void satellite links\n        cfg = {\n            'pollers': {},\n            'reactionners': {},\n            'receivers': {},\n        }\n\n        # Our self.daemons are only identifiers... that we use to fill the satellite links\n        for poller_id in self.pollers:\n            poller = pollers[poller_id]\n            cfg['pollers'][poller.uuid] = poller.give_satellite_cfg()\n\n        for reactionner_id in self.reactionners:\n            reactionner = reactionners[reactionner_id]\n            cfg['reactionners'][reactionner.uuid] = reactionner.give_satellite_cfg()\n\n        for receiver_id in self.receivers:\n            receiver = receivers[receiver_id]\n            cfg['receivers'][receiver.uuid] = receiver.give_satellite_cfg()\n\n        # If the broker manages sub realms, fill the satellite links...\n        if manage_sub_realms:\n            # Now pollers\n            for poller_id in self.get_all_subs_satellites_by_type('pollers', realms):\n                poller = pollers[poller_id]\n                cfg['pollers'][poller.uuid] = poller.give_satellite_cfg()\n\n            # Now reactionners\n            for reactionner_id in self.get_all_subs_satellites_by_type('reactionners', realms):\n                reactionner = reactionners[reactionner_id]\n                cfg['reactionners'][reactionner.uuid] = reactionner.give_satellite_cfg()\n\n            # Now receivers\n            for receiver_id in self.get_all_subs_satellites_by_type('receivers', realms):\n                receiver = receivers[receiver_id]\n                cfg['receivers'][receiver.uuid] = receiver.give_satellite_cfg()\n\n        return cfg"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_links_for_a_scheduler(self, pollers, reactionners, brokers):\n\n        # Create void satellite links\n        cfg = {\n            'pollers': {},\n            'reactionners': {},\n            'brokers': {},\n        }\n\n        # Our self.daemons are only identifiers... that we use to fill the satellite links\n        try:\n            for poller in self.pollers + self.get_potential_satellites_by_type(pollers, \"poller\"):\n                if poller in pollers:\n                    poller = pollers[poller]\n                cfg['pollers'][poller.uuid] = poller.give_satellite_cfg()\n\n            for reactionner in self.reactionners + self.get_potential_satellites_by_type(\n                    reactionners, \"reactionner\"):\n                if reactionner in reactionners:\n                    reactionner = reactionners[reactionner]\n                cfg['reactionners'][reactionner.uuid] = reactionner.give_satellite_cfg()\n\n            for broker in self.brokers + self.get_potential_satellites_by_type(brokers, \"broker\"):\n                if broker in brokers:\n                    broker = brokers[broker]\n                cfg['brokers'][broker.uuid] = broker.give_satellite_cfg()\n        except Exception as exp:  # pylint: disable=broad-except\n            logger.exception(\"realm.get_links_for_a_scheduler: %s\", exp)\n\n            # for poller in self.get_potential_satellites_by_type(pollers, \"poller\"):\n            #     logger.info(\"Poller: %s\", poller)\n            #     cfg['pollers'][poller.uuid] = poller.give_satellite_cfg()\n            #\n            # for reactionner in self.get_potential_satellites_by_type(reactionners, \"reactionner\"):\n            #     cfg['reactionners'][reactionner.uuid] = reactionner.give_satellite_cfg()\n            #\n            # for broker in self.get_potential_satellites_by_type(brokers, \"broker\"):\n            #     cfg['brokers'][broker.uuid] = broker.give_satellite_cfg()\n\n        return cfg", "response": "Get a configuration dictionary with pollers reactionners and brokers links for a scheduler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef linkify(self):\n        logger.info(\"Known realms:\")\n        for realm in self:\n            for tmp_realm in self:\n                # Ignore if it is me...\n                if tmp_realm == realm:\n                    continue\n                # Ignore if I am a sub realm of another realm\n                if realm.get_name() in tmp_realm.realm_members:\n                    break\n            else:\n                # This realm is not in the children of any realm\n                realm.level = 0\n                realm.set_level(0, self)", "response": "Link the realms in this realm to the realms in the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexplode realms with each realm_members and higher_realms to get all the the realms sub realms.", "response": "def explode(self):\n        \"\"\"Explode realms with each realm_members and higher_realms to get all the\n        realms sub realms.\n\n        :return: None\n        \"\"\"\n        # Manage higher realms where defined\n        for realm in [tmp_realm for tmp_realm in self if tmp_realm.higher_realms]:\n            for parent in realm.higher_realms:\n                higher_realm = self.find_by_name(parent)\n                if higher_realm:\n                    # Add the realm to its parent realm members\n                    higher_realm.realm_members.append(realm.get_name())\n\n        for realm in self:\n            # Set a recursion tag to protect against loop\n            for tmp_realm in self:\n                tmp_realm.rec_tag = False\n            realm.get_realms_by_explosion(self)\n\n        # Clean the recursion tag\n        for tmp_realm in self:\n            del tmp_realm.rec_tag"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default(self, check=False):\n        found = []\n        for realm in sorted(self, key=lambda r: r.level):\n            if getattr(realm, 'default', False):\n                found.append(realm)\n\n        if not found:\n            # Retain as default realm the first realm in name alphabetical order\n            found_names = sorted([r.get_name() for r in self])\n            if not found_names:\n                self.add_error(\"No realm is defined in this configuration! \"\n                               \"This should not be possible!\")\n                return None\n            default_realm_name = found_names[0]\n            default_realm = self.find_by_name(default_realm_name)\n            default_realm.default = True\n            found.append(default_realm)\n\n            if check:\n                self.add_error(\"No realm is defined as the default one! \"\n                               \"I set %s as the default realm\" % default_realm_name)\n\n        default_realm = found[0]\n        if len(found) > 1:\n            # Retain as default realm the first so-called default realms in name alphabetical order\n            found_names = sorted([r.get_name() for r in found])\n            default_realm_name = found_names[0]\n            default_realm = self.find_by_name(default_realm_name)\n\n            # Set all found realms as non-default realms\n            for realm in found:\n                if realm.get_name() != default_realm_name:\n                    realm.default = False\n\n            if check:\n                self.add_warning(\"More than one realm is defined as the default one: %s. \"\n                                 \"I set %s as the default realm.\"\n                                 % (','.join(found_names), default_realm_name))\n\n        self.default = default_realm\n\n        return default_realm", "response": "Get the default realm of Alignak configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nasks to the arbiter to reload the monitored configuration file.", "response": "def reload_configuration(self):\n        \"\"\"Ask to the arbiter to reload the monitored configuration\n\n        **Note** tha the arbiter will not reload its main configuration file (eg. alignak.ini)\n        but it will reload the monitored objects from the Nagios legacy files or from the\n        Alignak backend!\n\n        In case of any error, this function returns an object containing some properties:\n        '_status': 'ERR' because of the error\n        `_message`: some more explanations about the error\n\n        :return: True if configuration reload is accepted\n        \"\"\"\n        # If I'm not the master arbiter, ignore the command and raise a log\n        if not self.app.is_master:\n            message = u\"I received a request to reload the monitored configuration. \" \\\n                      u\"I am not the Master arbiter, I ignore and continue to run.\"\n            logger.warning(message)\n            return {'_status': u'ERR', '_message': message}\n\n        message = \"I received a request to reload the monitored configuration\"\n        if self.app.loading_configuration:\n            message = message + \"and I am still reloading the monitored configuration ;)\"\n        else:\n            self.app.need_config_reload = True\n        logger.warning(message)\n\n        return {'_status': u'OK', '_message': message}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrequesting to execute an external command Allowed parameters are: `command`: mandatory parameter containing the whole command line or only the command name `timestamp`: optional parameter containing the timestamp. If not present, the current timestamp is added in the command line `element`: the targeted element that will be appended after the command name (`command`). If element contains a '/' character it is split to make an host and service. `host`, `service` or `user`: the targeted host, service or user. Takes precedence over the `element` to target a specific element `parameters`: the parameter that will be appended after all the arguments When using this endpoint with the HTTP GET method, the semi colons that are commonly used to separate the parameters must be replace with %3B! This because the ; is an accepted URL query parameters separator... Indeed, the recommended way of using this endpoint is to use the HTTP POST method. In case of any error, this function returns an object containing some properties: '_status': 'ERR' because of the error `_message`: some more explanations about the error The `_status` field is 'OK' with an according `_message` to explain what the Arbiter will do depending upon the notification. The `command` property contains the formatted external command. :return: dict", "response": "def command(self, command=None,\n                timestamp=None, element=None, host=None, service=None, user=None, parameters=None):\n        # pylint: disable=too-many-branches\n        \"\"\" Request to execute an external command\n\n        Allowed parameters are:\n        `command`: mandatory parameter containing the whole command line or only the command name\n\n        `timestamp`: optional parameter containing the timestamp. If not present, the\n        current timestamp is added in the command line\n\n        `element`: the targeted element that will be appended after the command name (`command`).\n        If element contains a '/' character it is split to make an host and service.\n\n        `host`, `service` or `user`: the targeted host, service or user. Takes precedence over\n        the `element` to target a specific element\n\n        `parameters`: the parameter that will be appended after all the arguments\n\n        When using this endpoint with the HTTP GET method, the semi colons that are commonly used\n        to separate the parameters must be replace with %3B! This because the ; is an accepted\n        URL query parameters separator...\n\n        Indeed, the recommended way of using this endpoint is to use the HTTP POST method.\n\n        In case of any error, this function returns an object containing some properties:\n        '_status': 'ERR' because of the error\n        `_message`: some more explanations about the error\n\n        The `_status` field is 'OK' with an according `_message` to explain what the Arbiter\n        will do depending upon the notification. The `command` property contains the formatted\n        external command.\n\n        :return: dict\n        \"\"\"\n        if cherrypy.request.method in [\"POST\"]:\n            if not cherrypy.request.json:\n                return {'_status': u'ERR',\n                        '_message': u'You must POST parameters on this endpoint.'}\n\n        if command is None:\n            try:\n                command = cherrypy.request.json.get('command', None)\n                timestamp = cherrypy.request.json.get('timestamp', None)\n                element = cherrypy.request.json.get('element', None)\n                host = cherrypy.request.json.get('host', None)\n                service = cherrypy.request.json.get('service', None)\n                user = cherrypy.request.json.get('user', None)\n                parameters = cherrypy.request.json.get('parameters', None)\n            except AttributeError:\n                return {'_status': u'ERR', '_message': u'Missing command parameters'}\n\n        if not command:\n            return {'_status': u'ERR', '_message': u'Missing command parameter'}\n\n        fields = split_semicolon(command)\n        command_line = command.replace(fields[0], fields[0].upper())\n        if timestamp:\n            try:\n                timestamp = int(timestamp)\n            except ValueError:\n                return {'_status': u'ERR', '_message': u'Timestamp must be an integer value'}\n            command_line = '[%d] %s' % (timestamp, command_line)\n\n        if host or service or user:\n            if host:\n                command_line = '%s;%s' % (command_line, host)\n            if service:\n                command_line = '%s;%s' % (command_line, service)\n            if user:\n                command_line = '%s;%s' % (command_line, user)\n        elif element:\n            if '/' in element:\n                # Replace only the first /\n                element = element.replace('/', ';', 1)\n            command_line = '%s;%s' % (command_line, element)\n\n        if parameters:\n            command_line = '%s;%s' % (command_line, parameters)\n\n        # Add a command to get managed\n        logger.warning(\"Got an external command: %s\", command_line)\n        self.app.add(ExternalCommand(command_line))\n\n        return {'_status': u'OK',\n                '_message': u\"Got command: %s\" % command_line,\n                'command': command_line}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the Alignak detailed monitoring status of a specific scheduler.", "response": "def monitoring_problems(self):\n        \"\"\"Get Alignak detailed monitoring status\n\n        This will return an object containing the properties of the `identity`, plus a `problems`\n        object which contains 2 properties for each known scheduler:\n        - _freshness, which is the timestamp when the provided data were fetched\n        - problems, which is an object with the scheduler known problems:\n\n        {\n            ...\n\n            \"problems\": {\n                \"scheduler-master\": {\n                    \"_freshness\": 1528903945,\n                    \"problems\": {\n                        \"fdfc986d-4ab4-4562-9d2f-4346832745e6\": {\n                            \"last_state\": \"CRITICAL\",\n                            \"service\": \"dummy_critical\",\n                            \"last_state_type\": \"SOFT\",\n                            \"last_state_update\": 1528902442,\n                            \"last_hard_state\": \"CRITICAL\",\n                            \"last_hard_state_change\": 1528902442,\n                            \"last_state_change\": 1528902381,\n                            \"state\": \"CRITICAL\",\n                            \"state_type\": \"HARD\",\n                            \"host\": \"host-all-8\",\n                            \"output\": \"Hi, checking host-all-8/dummy_critical -> exit=2\"\n                        },\n                        \"2445f2a3-2a3b-4b13-96ed-4cfb60790e7e\": {\n                            \"last_state\": \"WARNING\",\n                            \"service\": \"dummy_warning\",\n                            \"last_state_type\": \"SOFT\",\n                            \"last_state_update\": 1528902463,\n                            \"last_hard_state\": \"WARNING\",\n                            \"last_hard_state_change\": 1528902463,\n                            \"last_state_change\": 1528902400,\n                            \"state\": \"WARNING\",\n                            \"state_type\": \"HARD\",\n                            \"host\": \"host-all-6\",\n                            \"output\": \"Hi, checking host-all-6/dummy_warning -> exit=1\"\n                        },\n                        ...\n                    }\n                }\n            }\n        }\n\n        :return: schedulers live synthesis list\n        :rtype: dict\n        \"\"\"\n        res = self.identity()\n        res['problems'] = {}\n        for scheduler_link in self.app.conf.schedulers:\n            sched_res = scheduler_link.con.get('monitoring_problems', wait=True)\n            res['problems'][scheduler_link.name] = {}\n            if '_freshness' in sched_res:\n                res['problems'][scheduler_link.name].update({'_freshness': sched_res['_freshness']})\n            if 'problems' in sched_res:\n                res['problems'][scheduler_link.name].update({'problems': sched_res['problems']})\n        res['_freshness'] = int(time.time())\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef livesynthesis(self):\n        res = self.identity()\n        res.update(self.app.get_livesynthesis())\n        return res", "response": "Return an Alignak live synthesis object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef object(self, o_type, o_name=None):\n        for scheduler_link in self.app.conf.schedulers:\n            sched_res = scheduler_link.con.get('object', {'o_type': o_type, 'o_name': o_name},\n                                               wait=True)\n            if isinstance(sched_res, dict) and 'content' in sched_res:\n                return sched_res\n        return {'_status': u'ERR', '_message': u'Required %s not found.' % o_type}", "response": "Get a monitored object from the arbiter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump(self, o_name=None, details=False, raw=False):\n        if details is not False:\n            details = bool(details)\n        if raw is not False:\n            raw = bool(raw)\n\n        res = {}\n        for scheduler_link in self.app.conf.schedulers:\n            sched_res = scheduler_link.con.get('dump', {'o_name': o_name,\n                                                        'details': '1' if details else '',\n                                                        'raw': '1' if raw else ''},\n                                               wait=True)\n            if isinstance(sched_res, dict) and \\\n                    '_status' in sched_res and sched_res['_status'] == 'ERR':\n                continue\n            res[scheduler_link.name] = sched_res\n        return res", "response": "Dump an internal host from the arbiter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef status(self, details=False):\n        if details is not False:\n            details = bool(details)\n\n        return self.app.get_alignak_status(details=details)", "response": "Get the overall alignak status"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the most recent Alignak events.", "response": "def events_log(self, details=False, count=0, timestamp=0):\n        \"\"\"Get the most recent Alignak events\n\n        If count is specifies it is the maximum number of events to return.\n\n        If timestamp is specified, events older than this timestamp will not be returned\n\n        The arbiter maintains a list of the most recent Alignak events. This endpoint\n        provides this list.\n\n        The default format is:\n        [\n            \"2018-07-23 15:14:43 - E - SERVICE NOTIFICATION: guest;host_0;dummy_random;CRITICAL;1;\n            notify-service-by-log;Service internal check result: 2\",\n            \"2018-07-23 15:14:43 - E - SERVICE NOTIFICATION: admin;host_0;dummy_random;CRITICAL;1;\n            notify-service-by-log;Service internal check result: 2\",\n            \"2018-07-23 15:14:42 - E - SERVICE ALERT: host_0;dummy_critical;CRITICAL;SOFT;1;\n            host_0-dummy_critical-2\",\n            \"2018-07-23 15:14:42 - E - SERVICE ALERT: host_0;dummy_random;CRITICAL;HARD;2;\n            Service internal check result: 2\",\n            \"2018-07-23 15:14:42 - I - SERVICE ALERT: host_0;dummy_unknown;UNKNOWN;HARD;2;\n            host_0-dummy_unknown-3\"\n        ]\n\n        If you request on this endpoint with the *details* parameter (whatever its value...),\n        you will get a detailed JSON output:\n        [\n            {\n                timestamp: 1535517701.1817362,\n                date: \"2018-07-23 15:16:35\",\n                message: \"SERVICE ALERT: host_11;dummy_echo;UNREACHABLE;HARD;2;\",\n                level: \"info\"\n            },\n            {\n                timestamp: 1535517701.1817362,\n                date: \"2018-07-23 15:16:32\",\n                message: \"SERVICE NOTIFICATION: guest;host_0;dummy_random;OK;0;\n                        notify-service-by-log;Service internal check result: 0\",\n                level: \"info\"\n            },\n            {\n                timestamp: 1535517701.1817362,\n                date: \"2018-07-23 15:16:32\",\n                message: \"SERVICE NOTIFICATION: admin;host_0;dummy_random;OK;0;\n                        notify-service-by-log;Service internal check result: 0\",\n                level: \"info\"\n            },\n            {\n                timestamp: 1535517701.1817362,\n                date: \"2018-07-23 15:16:32\",\n                message: \"SERVICE ALERT: host_0;dummy_random;OK;HARD;2;\n                        Service internal check result: 0\",\n                level: \"info\"\n            },\n            {\n                timestamp: 1535517701.1817362,\n                date: \"2018-07-23 15:16:19\",\n                message: \"SERVICE ALERT: host_11;dummy_random;OK;HARD;2;\n                        Service internal check result: 0\",\n                level: \"info\"\n            }\n        ]\n\n        In this example, only the 5 most recent events are provided whereas the default value is\n        to provide the 100 last events. This default counter may be changed thanks to the\n        ``events_log_count`` configuration variable or\n        ``ALIGNAK_EVENTS_LOG_COUNT`` environment variable.\n\n        The date format may also be changed thanks to the ``events_date_format`` configuration\n        variable.\n\n        :return: list of the most recent events\n        :rtype: list\n        \"\"\"\n        if not count:\n            count = 1 + int(os.environ.get('ALIGNAK_EVENTS_LOG_COUNT',\n                                           self.app.conf.events_log_count))\n        count = int(count)\n        timestamp = float(timestamp)\n        logger.debug('Get max %d events, newer than %s out of %d',\n                     count, timestamp, len(self.app.recent_events))\n\n        res = []\n        for log in reversed(self.app.recent_events):\n            if timestamp and timestamp > log['timestamp']:\n                break\n            if not count:\n                break\n            if details:\n                # Exposes the full object\n                res.append(log)\n            else:\n                res.append(\"%s - %s - %s\"\n                           % (log['date'], log['level'][0].upper(), log['message']))\n\n        logger.debug('Got %d events', len(res))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the arbiter satellite names sorted by type and return a list of the satellites", "response": "def satellites_list(self, daemon_type=''):\n        \"\"\"Get the arbiter satellite names sorted by type\n\n        Returns a list of the satellites as in:\n        {\n            reactionner: [\n                \"reactionner-master\"\n            ],\n            broker: [\n                \"broker-master\"\n            ],\n            arbiter: [\n                \"arbiter-master\"\n            ],\n            scheduler: [\n                \"scheduler-master-3\",\n                \"scheduler-master\",\n                \"scheduler-master-2\"\n            ],\n            receiver: [\n                \"receiver-nsca\",\n                \"receiver-master\"\n            ],\n            poller: [\n                \"poller-master\"\n            ]\n        }\n\n        If a specific daemon type is requested, the list is reduced to this unique daemon type:\n        {\n            scheduler: [\n                \"scheduler-master-3\",\n                \"scheduler-master\",\n                \"scheduler-master-2\"\n            ]\n        }\n\n        :param daemon_type: daemon type to filter\n        :type daemon_type: str\n        :return: dict with key *daemon_type* and value list of daemon name\n        :rtype: dict\n        \"\"\"\n        with self.app.conf_lock:\n            res = {}\n\n            for s_type in ['arbiter', 'scheduler', 'poller', 'reactionner', 'receiver', 'broker']:\n                if daemon_type and daemon_type != s_type:\n                    continue\n                satellite_list = []\n                res[s_type] = satellite_list\n                for daemon_link in getattr(self.app.conf, s_type + 's', []):\n                    satellite_list.append(daemon_link.name)\n            return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef realms(self, details=False):\n        def get_realm_info(realm, realms, satellites, details=False):\n            \"\"\"Get the realm and its children information\n\n            :return: None\n            \"\"\"\n            res = {\n                \"name\": realm.get_name(),\n                \"level\": realm.level,\n                \"hosts\": realm.members,\n                \"hostgroups\": realm.group_members,\n                \"children\": {},\n                \"satellites\": {\n                }\n            }\n            for child in realm.realm_members:\n                child = realms.find_by_name(child)\n                if not child:\n                    continue\n                realm_infos = get_realm_info(child, realms, satellites, details=details)\n                res['children'][child.get_name()] = realm_infos\n\n            for sat_type in ['scheduler', 'reactionner', 'broker', 'receiver', 'poller']:\n                res[\"satellites\"][sat_type + 's'] = []\n\n                sats = realm.get_potential_satellites_by_type(satellites, sat_type)\n                for sat in sats:\n                    if details:\n                        res[\"satellites\"][sat_type + 's'][sat.name] = sat.give_satellite_json()\n                    else:\n                        res[\"satellites\"][sat_type + 's'].append(sat.name)\n\n            return res\n\n        if details is not False:\n            details = bool(details)\n\n        # Report our daemons states, but only if a dispatcher and the configuration is loaded\n        if not getattr(self.app, 'dispatcher', None) or not getattr(self.app, 'conf', None):\n            return {'_status': u'ERR', '_message': \"Not yet available. Please come back later.\"}\n\n        res = {}\n        higher_realms = [realm for realm in self.app.conf.realms if realm.level == 0]\n        for realm in higher_realms:\n            res[realm.get_name()] = get_realm_info(realm, self.app.conf.realms,\n                                                   self.app.dispatcher.all_daemons_links)\n\n        return res", "response": "Return the realms configuration for each realm."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef satellites_configuration(self):\n        res = {}\n        for s_type in ['arbiter', 'scheduler', 'poller', 'reactionner', 'receiver',\n                       'broker']:\n            lst = []\n            res[s_type] = lst\n            for daemon in getattr(self.app.conf, s_type + 's'):\n                cls = daemon.__class__\n                env = {}\n                all_props = [cls.properties, cls.running_properties]\n\n                for props in all_props:\n                    for prop in props:\n                        if not hasattr(daemon, prop):\n                            continue\n                        if prop in [\"realms\", \"conf\", \"con\", \"tags\", \"modules\", \"cfg\",\n                                    \"broks\", \"cfg_to_manage\"]:\n                            continue\n                        val = getattr(daemon, prop)\n                        # give a try to a json able object\n                        try:\n                            json.dumps(val)\n                            env[prop] = val\n                        except TypeError as exp:\n                            logger.warning('satellites_configuration, %s: %s', prop, str(exp))\n                lst.append(env)\n        return res", "response": "Return all the configuration data of satellites"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the external commands from the daemon Use a lock for this function to protect", "response": "def external_commands(self):\n        \"\"\"Get the external commands from the daemon\n\n        Use a lock for this function to protect\n\n        :return: serialized external command list\n        :rtype: str\n        \"\"\"\n        res = []\n        with self.app.external_commands_lock:\n            for cmd in self.app.get_external_commands():\n                res.append(cmd.serialize())\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(self):  # pylint: disable=no-self-use\n        logger.debug(\"Grafana search... %s\", cherrypy.request.method)\n        if cherrypy.request.method == 'OPTIONS':\n            cherrypy.response.headers['Access-Control-Allow-Methods'] = 'GET,POST,PATCH,PUT,DELETE'\n            cherrypy.response.headers['Access-Control-Allow-Headers'] = 'Content-Type,Authorization'\n            cherrypy.response.headers['Access-Control-Allow-Origin'] = '*'\n            cherrypy.request.handler = None\n            return {}\n\n        if getattr(cherrypy.request, 'json', None):\n            logger.debug(\"Posted data: %s\", cherrypy.request.json)\n\n        logger.debug(\"Grafana search returns: %s\", GRAFANA_TARGETS)\n        return GRAFANA_TARGETS", "response": "Search for available target names"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query(self):\n        logger.debug(\"Grafana query... %s\", cherrypy.request.method)\n        if cherrypy.request.method == 'OPTIONS':\n            cherrypy.response.headers['Access-Control-Allow-Methods'] = 'GET,POST,PATCH,PUT,DELETE'\n            cherrypy.response.headers['Access-Control-Allow-Headers'] = 'Content-Type,Authorization'\n            cherrypy.response.headers['Access-Control-Allow-Origin'] = '*'\n            cherrypy.request.handler = None\n            return {}\n\n        if getattr(cherrypy.request, 'json', None):\n            posted_data = cherrypy.request.json\n            logger.debug(\"Posted data: %s\", cherrypy.request.json)\n\n        targets = None\n        target = None\n        try:\n            targets = posted_data.get(\"targets\")\n            assert targets\n            assert len(targets) == 1\n            target = targets[0].get(\"target\")\n        except Exception as exp:  # pylint: disable=broad-except\n            cherrypy.response.status = 409\n            return {'_status': u'ERR', '_message': u'Request error: %s.' % exp}\n\n        resp = []\n        if target in ['events_log']:\n            resp = [{\n                \"type\": \"table\",\n                \"columns\": [\n                    {\n                        \"text\": \"Time\",\n                        \"type\": \"time\",\n                        \"sort\": True,\n                        \"desc\": True\n                    },\n                    {\n                        \"text\": \"Severity\",\n                        \"type\": \"integer\"\n                    },\n                    {\n                        \"text\": \"Message\",\n                        \"type\": \"string\"\n                    }\n                ],\n                \"rows\": []\n            }]\n\n            severity = {\n                \"info\": 0,\n                'warning': 1,\n                'error': 2,\n                'critical': 3\n            }\n            for log in reversed(self.app.recent_events):\n                # 0 for the first required target\n                # timestamp must be precise on ms for Grafana\n                resp[0]['rows'].append([log['timestamp'] * 1000,\n                                        severity.get(log['level'].lower(), 3), log['message']])\n\n        if target in ['problems_log']:\n            resp = [{\n                \"type\": \"table\",\n                \"columns\": [\n                    {\n                        \"text\": \"Raised\",\n                        \"type\": \"time\",\n                        \"sort\": True,\n                        \"desc\": True\n                    },\n                    {\n                        \"text\": \"Severity\",\n                        \"type\": \"integer\"\n                    },\n                    {\n                        \"text\": \"Host\",\n                        \"type\": \"string\"\n                    },\n                    {\n                        \"text\": \"Service\",\n                        \"type\": \"string\"\n                    },\n                    {\n                        \"text\": \"State\",\n                        \"type\": \"integer\"\n                    },\n                    {\n                        \"text\": \"Output\",\n                        \"type\": \"string\"\n                    }\n                ],\n                \"rows\": []\n            }]\n\n            severity = {\n                \"up\": 0,\n                'down': 2,\n                'ok': 0,\n                'warning': 1,\n                'critical': 2\n            }\n\n            problems = {}\n            for scheduler_link in self.app.conf.schedulers:\n                sched_res = scheduler_link.con.get('monitoring_problems', wait=True)\n                if 'problems' in sched_res:\n                    problems.update(sched_res['problems'])\n\n            # todo: add a sorting\n            for problem_uuid in problems:\n                log = problems[problem_uuid]\n\n                # 0 for the first required target\n                resp[0]['rows'].append([log['last_hard_state_change'] * 1000,\n                                        severity.get(log['state'].lower(), 3),\n                                        log['host'], log['service'], log['state'], log['output']])\n\n        return resp", "response": "This function is used to query the database for the current entry in the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild and returns the command line for an host livestate", "response": "def _build_host_livestate(self, host_name, livestate):\n        # pylint: disable=no-self-use, too-many-locals\n        \"\"\"Build and notify the external command for an host livestate\n\n        PROCESS_HOST_CHECK_RESULT;<host_name>;<status_code>;<plugin_output>\n\n        :param host_name: the concerned host name\n        :param livestate: livestate dictionary\n        :return: external command line\n        \"\"\"\n        state = livestate.get('state', 'UP').upper()\n        output = livestate.get('output', '')\n        long_output = livestate.get('long_output', '')\n        perf_data = livestate.get('perf_data', '')\n        try:\n            timestamp = int(livestate.get('timestamp', 'ABC'))\n        except ValueError:\n            timestamp = None\n\n        host_state_to_id = {\n            \"UP\": 0,\n            \"DOWN\": 1,\n            \"UNREACHABLE\": 2\n        }\n        parameters = '%s;%s' % (host_state_to_id.get(state, 3), output)\n        if long_output and perf_data:\n            parameters = '%s|%s\\n%s' % (parameters, perf_data, long_output)\n        elif long_output:\n            parameters = '%s\\n%s' % (parameters, long_output)\n        elif perf_data:\n            parameters = '%s|%s' % (parameters, perf_data)\n\n        command_line = 'PROCESS_HOST_CHECK_RESULT;%s;%s' % (host_name, parameters)\n        if timestamp is not None:\n            command_line = '[%d] %s' % (timestamp, command_line)\n        else:\n            command_line = '[%d] %s' % (int(time.time()), command_line)\n\n        return command_line"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_service_livestate(self, host_name, service_name, livestate):\n        # pylint: disable=no-self-use, too-many-locals\n        \"\"\"Build and notify the external command for a service livestate\n\n        PROCESS_SERVICE_CHECK_RESULT;<host_name>;<service_description>;<return_code>;<plugin_output>\n\n        Create and post a logcheckresult to the backend for the livestate\n\n        :param host_name: the concerned host name\n        :param service_name: the concerned service name\n        :param livestate: livestate dictionary\n        :return: external command line\n        \"\"\"\n        state = livestate.get('state', 'OK').upper()\n        output = livestate.get('output', '')\n        long_output = livestate.get('long_output', '')\n        perf_data = livestate.get('perf_data', '')\n        try:\n            timestamp = int(livestate.get('timestamp', 'ABC'))\n        except ValueError:\n            timestamp = None\n\n        service_state_to_id = {\n            \"OK\": 0,\n            \"WARNING\": 1,\n            \"CRITICAL\": 2,\n            \"UNKNOWN\": 3,\n            \"UNREACHABLE\": 4\n        }\n        parameters = '%s;%s' % (service_state_to_id.get(state, 3), output)\n        if long_output and perf_data:\n            parameters = '%s|%s\\n%s' % (parameters, perf_data, long_output)\n        elif long_output:\n            parameters = '%s\\n%s' % (parameters, long_output)\n        elif perf_data:\n            parameters = '%s|%s' % (parameters, perf_data)\n\n        command_line = 'PROCESS_SERVICE_CHECK_RESULT;%s;%s;%s' % \\\n                       (host_name, service_name, parameters)\n        if timestamp is not None:\n            command_line = '[%d] %s' % (timestamp, command_line)\n        else:\n            command_line = '[%d] %s' % (int(time.time()), command_line)\n\n        return command_line", "response": "Build and notify the backend of a service livestate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a passive check for an host and its services", "response": "def host(self):\n        # pylint: disable=too-many-branches\n        \"\"\"Get a passive checks for an host and its services\n\n        This function builds the external commands corresponding to the host and services\n        provided information\n\n        :param host_name: host name\n        :param data: dictionary of the host properties to be modified\n        :return: command line\n        \"\"\"\n        logger.debug(\"Host status...\")\n        if cherrypy.request.method not in [\"PATCH\", \"POST\"]:\n            cherrypy.response.status = 405\n            return {'_status': 'ERR',\n                    '_error': 'You must only PATCH or POST on this endpoint.'}\n\n        # Update an host\n        # ---\n        if not cherrypy.request.json:\n            return {'_status': 'ERR',\n                    '_error': 'You must send parameters on this endpoint.'}\n\n        host_name = None\n        if cherrypy.request.json.get('name', None) is not None:\n            host_name = cherrypy.request.json.get('name', None)\n\n        if not host_name:\n            return {'_status': 'ERR',\n                    '_error': 'Missing targeted host name.'}\n\n        # Get provided data\n        # ---\n        logger.debug(\"Posted data: %s\", cherrypy.request.json)\n\n        # Check if the host exist in Alignak\n        # ---\n        # todo: Not mandatory but it would be clean...\n\n        # Prepare response\n        # ---\n        ws_result = {'_status': 'OK',\n                     '_result': ['%s is alive :)' % host_name],\n                     '_issues': []}\n\n        # Manage the host livestate\n        # ---\n        # Alert on unordered livestate if several information exist\n        now = int(time.time())\n        livestate = cherrypy.request.json.get('livestate', None)\n        if not livestate:\n            # Create an host live state command\n            livestate = {'state': \"UP\"}\n        if not isinstance(livestate, list):\n            livestate = [livestate]\n\n        last_ts = 0\n        for ls in livestate:\n            if ls.get('state', None) is None:\n                ws_result['_issues'].append(\"Missing state for the host '%s' livestate, \"\n                                            \"assuming host is UP!\" % host_name)\n                ls['state'] = 'UP'\n\n            # Tag our own timestamp\n            ls['_ws_timestamp'] = now\n            try:\n                timestamp = int(ls.get('timestamp', 'ABC'))\n                if timestamp < last_ts:\n                    logger.info(\"Got unordered timestamp for the host '%s'. \"\n                                \"The Alignak scheduler may not handle the check result!\",\n                                host_name)\n                last_ts = timestamp\n            except ValueError:\n                pass\n\n        for ls in livestate:\n            state = ls.get('state').upper()\n            if state not in ['UP', 'DOWN', 'UNREACHABLE']:\n                ws_result['_issues'].append(\"Host state should be UP, DOWN or UNREACHABLE\"\n                                            \", and not '%s'.\" % (state))\n            else:\n                # Create an host live state command\n                command = self._build_host_livestate(host_name, ls)\n                ws_result['_result'].append(\"Raised: %s\" % command)\n                # Notify the external command to our Arbiter daemon\n                self.app.add(ExternalCommand(command))\n\n        services = cherrypy.request.json.get('services', None)\n        if not services:\n            return ws_result\n\n        for service in services:\n            service_name = service.get('name', None)\n            if service_name is None:\n                ws_result['_issues'].append(\"A service does not have a 'name' property\")\n                continue\n\n            livestate = service.get('livestate', None)\n            if not livestate:\n                # Create a service live state command\n                livestate = {'state': \"OK\"}\n            if not isinstance(livestate, list):\n                livestate = [livestate]\n\n            last_ts = 0\n            for ls in livestate:\n                if ls.get('state', None) is None:\n                    ws_result['_issues'].append(\"Missing state for the service %s/%s livestate, \"\n                                                \"assuming service is OK!\"\n                                                % (host_name, service_name))\n                    ls['state'] = 'OK'\n\n                # Tag our own timestamp\n                ls['_ws_timestamp'] = now\n                try:\n                    timestamp = int(ls.get('timestamp', 'ABC'))\n                    if timestamp < last_ts:\n                        logger.info(\"Got unordered timestamp for the service: %s/%s. \"\n                                    \"The Alignak scheduler may not handle the check result!\",\n                                    host_name, service_name)\n                    last_ts = timestamp\n                except ValueError:\n                    pass\n\n            for ls in livestate:\n                state = ls.get('state').upper()\n                if state not in ['OK', 'WARNING', 'CRITICAL', 'UNKNOWN', 'UNREACHABLE']:\n                    ws_result['_issues'].append(\"Service %s/%s state must be OK, WARNING, \"\n                                                \"CRITICAL, UNKNOWN or UNREACHABLE, and not %s.\"\n                                                % (host_name, service_name, state))\n                else:\n                    # Create a service live state command\n                    command = self._build_service_livestate(host_name, service_name, ls)\n                    ws_result['_result'].append(\"Raised: %s\" % command)\n                    # Notify the external command to our Arbiter daemon\n                    self.app.add(ExternalCommand(command))\n\n        return ws_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _wait_new_conf(self):\n        with self.app.conf_lock:\n            logger.warning(\"My master Arbiter wants me to wait for a new configuration.\")\n            self.app.cur_conf = {}", "response": "Ask the daemon to drop its configuration and wait for a new one."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a new configuration to the daemon", "response": "def _push_configuration(self, pushed_configuration=None):\n        \"\"\"Send a new configuration to the daemon\n\n        This overrides the default method from GenericInterface\n\n        Used by the master arbiter to send its configuration to a spare arbiter\n\n        This function is not intended for external use. It is quite complex to\n        build a configuration for a daemon and it is the arbter dispatcher job ;)\n\n        :param pushed_configuration: new conf to send\n        :return: None\n        \"\"\"\n        pushed_configuration = cherrypy.request.json\n        self.app.must_run = False\n        return super(ArbiterInterface, self)._push_configuration(\n            pushed_configuration=pushed_configuration['conf'])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _do_not_run(self):\n        # If I'm the master, ignore the command and raise a log\n        if self.app.is_master:\n            message = \"Received message to not run. \" \\\n                      \"I am the Master arbiter, ignore and continue to run.\"\n            logger.warning(message)\n            return {'_status': u'ERR', '_message': message}\n\n        # Else, I'm just a spare, so I listen to my master\n        logger.debug(\"Received message to not run. I am the spare, stopping.\")\n        self.app.last_master_speak = time.time()\n        self.app.must_run = False\n        return {'_status': u'OK', '_message': message}", "response": "This function is called by the master arbiter when the master arbiter is not running."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlinks a command to a property", "response": "def linkify_one_command_with_commands(self, commands, prop):\n        \"\"\"\n        Link a command to a property (check_command for example)\n\n        :param commands: commands object\n        :type commands: alignak.objects.command.Commands\n        :param prop: property name\n        :type prop: str\n        :param default: default command to use if the property is not defined\n        :type default: str\n        :return: None\n        \"\"\"\n        for i in self:\n            command = getattr(i, prop, '').strip()\n            if command:\n                setattr(i, prop, self.create_commandcall(i, commands, command))\n            else:\n                # No defined command\n                setattr(i, prop, None)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlink a command list with commands in real CommandCalls", "response": "def linkify_command_list_with_commands(self, commands, prop):\n        \"\"\"\n        Link a command list (commands with , between) in real CommandCalls\n\n        :param commands: commands object\n        :type commands: alignak.objects.command.Commands\n        :param prop: property name\n        :type prop: str\n        :return: None\n        \"\"\"\n        for i in self:\n            if not hasattr(i, prop):\n                continue\n\n            commands_list = strip_and_uniq(getattr(i, prop, ''))\n            cmds_list = []\n            for command in commands_list:\n                if not command:\n                    continue\n\n                cmds_list.append(self.create_commandcall(i, commands, command))\n            setattr(i, prop, cmds_list)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a CommandCall object with the specified properties.", "response": "def create_commandcall(prop, commands, command):\n        \"\"\"\n        Create CommandCall object with command\n\n        :param prop: property\n        :type prop: str\n        :param commands: all commands\n        :type commands: alignak.objects.command.Commands\n        :param command: a command object\n        :type command: str\n        :return: a commandCall object\n        :rtype: alignak.objects.commandcallitem.CommandCall\n        \"\"\"\n        cc = {\n            'commands': commands,\n            'call': command\n        }\n\n        if hasattr(prop, 'enable_environment_macros'):\n            cc['enable_environment_macros'] = prop.enable_environment_macros\n\n        if hasattr(prop, 'poller_tag'):\n            cc['poller_tag'] = prop.poller_tag\n        elif hasattr(prop, 'reactionner_tag'):\n            cc['reactionner_tag'] = prop.reactionner_tag\n\n        return CommandCall(cc)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npush the provided broks objects to the broker daemon", "response": "def _push_broks(self):\n        \"\"\"Push the provided broks objects to the broker daemon\n\n        Only used on a Broker daemon by the Arbiter\n\n        :param: broks\n        :type: list\n        :return: None\n        \"\"\"\n        data = cherrypy.request.json\n        with self.app.arbiter_broks_lock:\n            logger.debug(\"Pushing %d broks\", len(data['broks']))\n            self.app.arbiter_broks.extend([unserialize(elem, True) for elem in data['broks']])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clean_params(self, params):\n        clean_p = {}\n        for elt in params:\n            elts = elt.split('=', 1)\n            if len(elts) == 1:  # error, there is no = !\n                self.add_error(\"the parameter %s is malformed! (no = sign)\" % elts[0])\n            else:\n                if elts[1] == '':\n                    self.add_warning(\"the parameter %s is ambiguous! \"\n                                     \"No value after =, assuming an empty string\" % elts[0])\n                clean_p[elts[0]] = elts[1]\n\n        return clean_p", "response": "Convert a list of parameters into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_params(self, params):\n        logger.debug(\"Alignak parameters:\")\n        for key, value in sorted(self.clean_params(params).items()):\n            update_attribute = None\n\n            # Maybe it's a variable as $USER$ or $ANOTHERVARIABLE$\n            # so look at the first character. If it's a $, it is a macro variable\n            # if it ends with $ too\n            if key[0] == '$' and key[-1] == '$':\n                key = key[1:-1]\n                # Update the macros list\n                if key not in self.__class__.macros:\n                    logger.debug(\"New macro %s: %s - %s\", self, key, value)\n                self.__class__.macros[key] = '$%s$' % key\n                key = '$%s$' % key\n\n                logger.debug(\"- macro %s\", key)\n                update_attribute = value\n                # Create a new property to store the macro value\n                if isinstance(value, list):\n                    self.__class__.properties[key] = ListProp(default=value)\n                else:\n                    self.__class__.properties[key] = StringProp(default=value)\n            elif key in self.properties:\n                update_attribute = self.properties[key].pythonize(value)\n            elif key in self.running_properties:\n                logger.warning(\"using a the running property %s in a config file\", key)\n                update_attribute = self.running_properties[key].pythonize(value)\n            elif key.startswith('$') or key in ['cfg_file', 'cfg_dir']:\n                # it's a macro or a useless now param, we don't touch this\n                update_attribute = value\n            else:\n                logger.debug(\"Guessing the property '%s' type because it \"\n                             \"is not in %s object properties\", key, self.__class__.__name__)\n                update_attribute = ToGuessProp().pythonize(value)\n\n            if update_attribute is not None:\n                setattr(self, key, update_attribute)\n                logger.debug(\"- update %s = %s\", key, update_attribute)\n\n        # Change Nagios2 names to Nagios3 ones (before using them)\n        self.old_properties_names_to_new()\n\n        # Fill default for myself - new properties entry becomes a self attribute\n        self.fill_default()", "response": "Load parameters from main configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cut_line(line):\n        # punct = '\"#$%&\\'()*+/<=>?@[\\\\]^`{|}~'\n        if re.search(\"([\\t\\n\\r]+|[\\x0b\\x0c ]{3,})+\", line):\n            tmp = re.split(\"([\\t\\n\\r]+|[\\x0b\\x0c ]{3,})+\", line, 1)\n        else:\n            tmp = re.split(\"[\" + string.whitespace + \"]+\", line, 1)\n        res = [elt.strip() for elt in tmp if elt.strip() != '']\n        return res", "response": "Split the line on whitespaces and remove empty chunks"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread and parse the Nagios legacy configuration files and store their content into a StringIO object which will be returned as the function result", "response": "def read_legacy_cfg_files(self, cfg_files, alignak_env_files=None):\n        # pylint: disable=too-many-nested-blocks,too-many-statements\n        # pylint: disable=too-many-branches, too-many-locals\n        \"\"\"Read and parse the Nagios legacy configuration files\n        and store their content into a StringIO object which content\n        will be returned as the function result\n\n        :param cfg_files: list of file to read\n        :type cfg_files: list\n        :param alignak_env_files: name of the alignak environment file\n        :type alignak_env_files: list\n        :return: a buffer containing all files\n        :rtype: str\n        \"\"\"\n        cfg_buffer = ''\n        if not cfg_files:\n            return cfg_buffer\n\n        # Update configuration with the first legacy configuration file name and path\n        # This will update macro properties\n        self.alignak_env = 'n/a'\n        if alignak_env_files is not None:\n            self.alignak_env = alignak_env_files\n            if not isinstance(alignak_env_files, list):\n                self.alignak_env = [os.path.abspath(alignak_env_files)]\n            else:\n                self.alignak_env = [os.path.abspath(f) for f in alignak_env_files]\n        self.main_config_file = os.path.abspath(cfg_files[0])\n        self.config_base_dir = os.path.dirname(self.main_config_file)\n\n        # Universal newline mode (all new lines are managed internally)\n        res = StringIO(u\"# Configuration cfg_files buffer\", newline=None)\n\n        if not self.read_config_silent and cfg_files:\n            logger.info(\"Reading the configuration cfg_files...\")\n\n        # A first pass to get all the configuration cfg_files in a buffer\n        for cfg_file in cfg_files:\n            # Make sure the configuration cfg_files are not repeated...\n            if os.path.abspath(cfg_file) in self.my_cfg_files:\n                logger.warning(\"- ignoring repeated file: %s\", os.path.abspath(cfg_file))\n                continue\n            self.my_cfg_files.append(os.path.abspath(cfg_file))\n\n            # File header\n            res.write(u\"\\n\")\n            res.write(u\"# imported_from=%s\" % cfg_file)\n            res.write(u\"\\n\")\n\n            if not self.read_config_silent:\n                logger.info(\"- opening '%s' configuration file\", cfg_file)\n            try:\n                # Open in Universal way for Windows, Mac, Linux-based systems\n                file_d = open(cfg_file, 'r')\n                buf = file_d.readlines()\n                file_d.close()\n            except IOError as exp:\n                self.add_error(\"cannot open main file '%s' for reading: %s\" % (cfg_file, exp))\n                continue\n\n            for line in buf:\n                try:\n                    line = line.decode('utf8', 'replace')\n                except AttributeError:\n                    # Python 3 will raise an exception because the line is still unicode\n                    pass\n\n                line = line.strip()\n                res.write(line)\n                res.write(u\"\\n\")\n\n                if (re.search(\"^cfg_file\", line) or re.search(\"^resource_file\", line)) \\\n                        and '=' in line:\n                    elts = line.split('=', 1)\n                    if os.path.isabs(elts[1]):\n                        cfg_file_name = elts[1]\n                    else:\n                        cfg_file_name = os.path.join(self.config_base_dir, elts[1])\n                    cfg_file_name = cfg_file_name.strip()\n                    cfg_file_name = os.path.abspath(cfg_file_name)\n\n                    # Make sure the configuration cfg_files are not repeated...\n                    if cfg_file_name in self.my_cfg_files:\n                        logger.warning(\"- ignoring repeated file: %s\", cfg_file_name)\n                    else:\n                        self.my_cfg_files.append(cfg_file_name)\n\n                        if not self.read_config_silent:\n                            logger.info(\"  reading: %s\", cfg_file_name)\n\n                        try:\n                            # Read the file content to the buffer\n                            file_d = open(cfg_file_name, 'r')\n\n                            # File header\n                            res.write(u\"\\n\")\n                            res.write(u\"# imported_from=%s\" % cfg_file_name)\n                            res.write(u\"\\n\")\n\n                            content = file_d.read()\n                            try:\n                                content = content.decode('utf8', 'replace')\n                            except AttributeError:\n                                # Python 3 will raise an exception\n                                pass\n                            res.write(content)\n                            res.write(u\"\\n\")\n                            file_d.close()\n                        except IOError as exp:\n                            self.add_error(u\"cannot open file '%s' for reading: %s\"\n                                           % (cfg_file_name, exp))\n                elif re.search(\"^cfg_dir\", line) and '=' in line:\n                    elts = line.split('=', 1)\n                    if os.path.isabs(elts[1]):\n                        cfg_dir_name = elts[1]\n                    else:\n                        cfg_dir_name = os.path.join(self.config_base_dir, elts[1])\n                    # Ok, look if it's really a directory\n                    if not os.path.isdir(cfg_dir_name):\n                        self.add_error(u\"cannot open directory '%s' for reading\" % cfg_dir_name)\n                        continue\n\n                    # Now walk for it.\n                    for root, _, walk_files in os.walk(cfg_dir_name, followlinks=True):\n                        for found_file in walk_files:\n                            if not re.search(r\"\\.cfg$\", found_file):\n                                continue\n\n                            cfg_file_name = os.path.join(root, found_file)\n                            # Make sure the configuration cfg_files are not repeated...\n                            if os.path.abspath(cfg_file_name) in self.my_cfg_files:\n                                logger.warning(\"- ignoring repeated file: %s\", cfg_file_name)\n                            else:\n                                self.my_cfg_files.append(cfg_file_name)\n\n                                if not self.read_config_silent:\n                                    logger.info(\"  reading: %s\", cfg_file_name)\n\n                                try:\n                                    # Read the file content to the buffer\n                                    file_d = open(cfg_file_name, 'r')\n\n                                    # File header\n                                    res.write(u\"\\n\")\n                                    res.write(u\"# imported_from=%s\" % cfg_file_name)\n                                    res.write(u\"\\n\")\n\n                                    content = file_d.read()\n                                    try:\n                                        content = content.decode('utf8', 'replace')\n                                    except AttributeError:\n                                        # Python 3 will raise an exception\n                                        pass\n                                    res.write(content)\n                                    res.write(u\"\\n\")\n                                    file_d.close()\n                                except IOError as exp:\n                                    self.add_error(u\"cannot open file '%s' for reading: %s\"\n                                                   % (cfg_file_name, exp))\n\n        cfg_buffer = res.getvalue()\n        res.close()\n\n        return cfg_buffer"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the contents of the given config file into a dictionary of alignak objects.", "response": "def read_config_buf(self, cfg_buffer):\n        # pylint: disable=too-many-locals, too-many-branches\n        \"\"\"The legacy configuration buffer (previously returned by Config.read_config())\n\n        If the buffer is empty, it will return an empty dictionary else it will return a\n        dictionary containing dictionary items tha tmay be used to create Alignak\n        objects\n\n        :param cfg_buffer: buffer containing all data from config files\n        :type cfg_buffer: str\n        :return: dict of alignak objects with the following structure ::\n        { type1 : [{key: value, ..}, {..}],\n          type2 : [ ... ]\n        }\n\n        Example ::\n\n        { 'host' : [{'host_name': 'myhostname', ..}, {..}],\n          'service' : [ ... ]\n        }\n\n        Values are all str for now. It is pythonized at object creation\n\n        :rtype: dict\n        \"\"\"\n        objects = {}\n        if not self.read_config_silent:\n            if cfg_buffer:\n                logger.info(\"Parsing the legacy configuration files...\")\n            else:\n                logger.info(\"No legacy configuration files.\")\n                return objects\n\n        params = []\n        objectscfg = {}\n        for o_type in self.__class__.configuration_types:\n            objectscfg[o_type] = []\n\n        tmp = []\n        tmp_type = 'void'\n        in_define = False\n        almost_in_define = False\n        continuation_line = False\n        tmp_line = ''\n        lines = cfg_buffer.split('\\n')\n        line_nb = 0  # Keep the line number for the file path\n        filefrom = ''\n        for line in lines:\n            if line.startswith(\"# imported_from=\"):\n                filefrom = line.split('=')[1]\n                line_nb = 0  # reset the line number too\n                if not self.read_config_silent:\n                    logger.debug(\"#####\\n# file: %s\", filefrom)\n                continue\n            if not self.read_config_silent:\n                logger.debug(\"- %d: %s\", line_nb, line)\n\n            line_nb += 1\n            # Remove comments\n            line = split_semicolon(line)[0].strip()\n\n            # A backslash means, there is more to come\n            if re.search(r\"\\\\\\s*$\", line) is not None:\n                continuation_line = True\n                line = re.sub(r\"\\\\\\s*$\", \"\", line)\n                line = re.sub(r\"^\\s+\", \" \", line)\n                tmp_line += line\n                continue\n            elif continuation_line:\n                # Now the continuation line is complete\n                line = re.sub(r\"^\\s+\", \"\", line)\n                line = tmp_line + line\n                tmp_line = ''\n                continuation_line = False\n\n            # } alone in a line means stop the object reading\n            if re.search(r\"^\\s*}\\s*$\", line) is not None:\n                in_define = False\n\n            # { alone in a line can mean start object reading\n            if re.search(r\"^\\s*\\{\\s*$\", line) is not None and almost_in_define:\n                almost_in_define = False\n                in_define = True\n                continue\n\n            if re.search(r\"^\\s*#|^\\s*$|^\\s*}\", line) is not None:\n                pass\n            # A define must be catched and the type saved\n            # The old entry must be saved before\n            elif re.search(\"^define\", line) is not None:\n                if re.search(r\".*\\{.*$\", line) is not None:  # pylint: disable=R0102\n                    in_define = True\n                else:\n                    almost_in_define = True\n\n                if tmp_type not in objectscfg:\n                    objectscfg[tmp_type] = []\n                objectscfg[tmp_type].append(tmp)\n                tmp = []\n                tmp.append(\"imported_from %s:%s\" % (filefrom, line_nb))\n                # Get new type\n                elts = re.split(r'\\s', line)\n                # Maybe there was space before and after the type\n                # so we must get all and strip it\n                tmp_type = ' '.join(elts[1:]).strip()\n                tmp_type = tmp_type.split('{')[0].strip()\n            else:\n                if in_define:\n                    tmp.append(line)\n                else:\n                    params.append(line)\n\n        # Maybe the type of the last element is unknown, declare it\n        if tmp_type not in objectscfg:\n            objectscfg[tmp_type] = []\n        objectscfg[tmp_type].append(tmp)\n\n        # Check and load the parameters\n        self.load_params(params)\n\n        for o_type in objectscfg:\n            objects[o_type] = []\n            for items in objectscfg[o_type]:\n                tmp_obj = {}\n                for line in items:\n                    elts = self._cut_line(line)\n                    if elts == []:\n                        continue\n                    prop = elts[0]\n                    if prop not in tmp_obj:\n                        tmp_obj[prop] = []\n                    value = ' '.join(elts[1:])\n                    tmp_obj[prop].append(value)\n                if tmp_obj != {}:\n                    # Create a new object\n                    objects[o_type].append(tmp_obj)\n\n        return objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding self defined command objects for internal processing.", "response": "def add_self_defined_objects(raw_objects):\n        \"\"\"Add self defined command objects for internal processing ;\n        bp_rule, _internal_host_up, _echo, _internal_host_check, _interna_service_check\n\n        :param raw_objects: Raw config objects dict\n        :type raw_objects: dict\n        :return: raw_objects with some more commands\n        :rtype: dict\n        \"\"\"\n        logger.info(\"- creating internally defined commands...\")\n        if 'command' not in raw_objects:\n            raw_objects['command'] = []\n        # Business rule\n        raw_objects['command'].append({\n            'command_name': 'bp_rule',\n            'command_line': 'bp_rule',\n            'imported_from': 'alignak-self'\n        })\n        # Internal host checks\n        raw_objects['command'].append({\n            'command_name': '_internal_host_up',\n            'command_line': '_internal_host_up',\n            'imported_from': 'alignak-self'\n        })\n        raw_objects['command'].append({\n            'command_name': '_internal_host_check',\n            # Command line must contain: state_id;output\n            'command_line': '_internal_host_check;$ARG1$;$ARG2$',\n            'imported_from': 'alignak-self'\n        })\n        # Internal service check\n        raw_objects['command'].append({\n            'command_name': '_echo',\n            'command_line': '_echo',\n            'imported_from': 'alignak-self'\n        })\n        raw_objects['command'].append({\n            'command_name': '_internal_service_check',\n            # Command line must contain: state_id;output\n            'command_line': '_internal_service_check;$ARG1$;$ARG2$',\n            'imported_from': 'alignak-self'\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef early_create_objects(self, raw_objects):\n        types_creations = self.__class__.types_creations\n        early_created_types = self.__class__.early_created_types\n\n        logger.info(\"Creating objects...\")\n        for o_type in sorted(types_creations):\n            if o_type in early_created_types:\n                self.create_objects_for_type(raw_objects, o_type)\n        logger.info(\"Done\")", "response": "Create the objects needed for the post configuration file initialization\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate all the objects got after the post configuration file initialization.", "response": "def create_objects(self, raw_objects):\n        \"\"\"Create all the objects got after the post configuration file initialization\n\n        :param raw_objects:  dict with all object with str values\n        :type raw_objects: dict\n        :return: None\n        \"\"\"\n        types_creations = self.__class__.types_creations\n        early_created_types = self.__class__.early_created_types\n\n        logger.info(\"Creating objects...\")\n\n        # Before really creating the objects, we add some ghost\n        # ones like the bp_rule for correlation\n        self.add_self_defined_objects(raw_objects)\n\n        for o_type in sorted(types_creations):\n            if o_type not in early_created_types:\n                self.create_objects_for_type(raw_objects, o_type)\n        logger.info(\"Done\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares the arbiter for early operations", "response": "def early_arbiter_linking(self, arbiter_name, params):\n        \"\"\" Prepare the arbiter for early operations\n\n        :param arbiter_name: default arbiter name if no arbiter exist in the configuration\n        :type arbiter_name: str\n        :return: None\n        \"\"\"\n\n        if not self.arbiters:\n            params.update({\n                'name': arbiter_name, 'arbiter_name': arbiter_name,\n                'host_name': socket.gethostname(),\n                'address': '127.0.0.1', 'port': 7770,\n                'spare': '0'\n            })\n            logger.warning(\"There is no arbiter, I add myself (%s) reachable on %s:%d\",\n                           arbiter_name, params['address'], params['port'])\n            arb = ArbiterLink(params, parsing=True)\n            self.arbiters = ArbiterLinks([arb])\n\n        # First fill default\n        self.arbiters.fill_default()\n        self.modules.fill_default()\n\n        self.arbiters.linkify(modules=self.modules)\n        self.modules.linkify()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef linkify_one_command_with_commands(self, commands, prop):\n\n        if not hasattr(self, prop):\n            return\n\n        command = getattr(self, prop).strip()\n        if not command:\n            setattr(self, prop, None)\n            return\n\n        data = {\"commands\": commands, \"call\": command}\n        if hasattr(self, 'poller_tag'):\n            data.update({\"poller_tag\": self.poller_tag})\n        if hasattr(self, 'reactionner_tag'):\n            data.update({\"reactionner_tag\": self.reactionner_tag})\n\n        setattr(self, prop, CommandCall(data))", "response": "Link a command with the given commands."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlinking the hosts list with all its services in it", "response": "def linkify(self):\n        \"\"\" Make 'links' between elements, like a host got a services list\n        with all its services in it\n\n        :return: None\n        \"\"\"\n\n        self.services.optimize_service_search(self.hosts)\n\n        # First linkify myself like for some global commands\n        self.linkify_one_command_with_commands(self.commands, 'host_perfdata_command')\n        self.linkify_one_command_with_commands(self.commands, 'service_perfdata_command')\n        self.linkify_one_command_with_commands(self.commands, 'global_host_event_handler')\n        self.linkify_one_command_with_commands(self.commands, 'global_service_event_handler')\n\n        # link hosts with timeperiods and commands\n        self.hosts.linkify(self.timeperiods, self.commands,\n                           self.contacts, self.realms,\n                           self.resultmodulations, self.businessimpactmodulations,\n                           self.escalations, self.hostgroups,\n                           self.checkmodulations, self.macromodulations)\n\n        self.hostsextinfo.merge(self.hosts)\n\n        # Do the simplify AFTER explode groups\n        # link hostgroups with hosts\n        self.hostgroups.linkify(self.hosts, self.realms, self.forced_realms_hostgroups)\n\n        # link services with other objects\n        self.services.linkify(self.hosts, self.commands,\n                              self.timeperiods, self.contacts,\n                              self.resultmodulations, self.businessimpactmodulations,\n                              self.escalations, self.servicegroups,\n                              self.checkmodulations, self.macromodulations)\n\n        self.servicesextinfo.merge(self.services)\n\n        # link servicegroups members with services\n        self.servicegroups.linkify(self.hosts, self.services)\n\n        # link notificationways with timeperiods and commands\n        self.notificationways.linkify(self.timeperiods, self.commands)\n\n        # link notificationways with timeperiods and commands\n        self.checkmodulations.linkify(self.timeperiods, self.commands)\n\n        # Link with timeperiods\n        self.macromodulations.linkify(self.timeperiods)\n\n        # link contacgroups with contacts\n        self.contactgroups.linkify(self.contacts)\n\n        # link contacts with timeperiods and commands\n        self.contacts.linkify(self.commands, self.notificationways)\n\n        # link timeperiods with timeperiods (exclude part)\n        self.timeperiods.linkify()\n\n        self.servicedependencies.linkify(self.hosts, self.services,\n                                         self.timeperiods)\n\n        self.hostdependencies.linkify(self.hosts, self.timeperiods)\n        self.resultmodulations.linkify(self.timeperiods)\n\n        self.businessimpactmodulations.linkify(self.timeperiods)\n\n        self.escalations.linkify(self.timeperiods, self.contacts,\n                                 self.services, self.hosts)\n\n        # Link all satellite links with modules\n        self.schedulers.linkify(self.modules)\n        self.brokers.linkify(self.modules)\n        self.receivers.linkify(self.modules)\n        self.reactionners.linkify(self.modules)\n        self.pollers.linkify(self.modules)\n\n        # Ok, now update all realms with back links of satellites\n        satellites = {}\n        for sat in self.schedulers:\n            satellites[sat.uuid] = sat\n        for sat in self.pollers:\n            satellites[sat.uuid] = sat\n        for sat in self.reactionners:\n            satellites[sat.uuid] = sat\n        for sat in self.receivers:\n            satellites[sat.uuid] = sat\n        for sat in self.brokers:\n            satellites[sat.uuid] = sat\n        self.realms.prepare_satellites(satellites)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps for calling the clean method of services attribute", "response": "def clean(self):\n        \"\"\"Wrapper for calling the clean method of services attribute\n\n        :return: None\n        \"\"\"\n        logger.debug(\"Cleaning configuration objects before configuration sending:\")\n        types_creations = self.__class__.types_creations\n        for o_type in types_creations:\n            (_, _, inner_property, _, _) = types_creations[o_type]\n            logger.debug(\"  . for %s\", inner_property, )\n            inner_object = getattr(self, inner_property)\n            inner_object.clean()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef warn_about_unmanaged_parameters(self):\n        properties = self.__class__.properties\n        unmanaged = []\n        for prop, entry in list(properties.items()):\n            if not entry.managed and hasattr(self, prop):\n                if entry.help:\n                    line = \"%s: %s\" % (prop, entry.help)\n                else:\n                    line = prop\n                unmanaged.append(line)\n        if unmanaged:\n            logger.warning(\"The following Nagios legacy parameter(s) are not currently \"\n                           \"managed by Alignak:\")\n\n            for line in unmanaged:\n                logger.warning('- %s', line)\n\n            logger.warning(\"Those are unmanaged configuration statements, do you really need it? \"\n                           \"Create an issue on the Alignak repository or submit a pull \"\n                           \"request: http://www.github.com/Alignak-monitoring/alignak\")", "response": "used to raise warning if the user got parameter\n        that we do not manage from now\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse to fill groups values on hosts and create new services", "response": "def explode(self):\n        \"\"\"Use to fill groups values on hosts and create new services\n        (for host group ones)\n\n        :return: None\n        \"\"\"\n        # first elements, after groups\n        self.contacts.explode(self.contactgroups, self.notificationways)\n        self.contactgroups.explode()\n\n        self.hosts.explode(self.hostgroups, self.contactgroups)\n\n        self.hostgroups.explode()\n\n        self.services.explode(self.hosts, self.hostgroups, self.contactgroups,\n                              self.servicegroups, self.servicedependencies)\n        self.servicegroups.explode()\n\n        self.timeperiods.explode()\n\n        self.hostdependencies.explode(self.hostgroups)\n\n        self.servicedependencies.explode(self.hostgroups)\n\n        # Serviceescalations hostescalations will create new escalations\n        self.serviceescalations.explode(self.escalations)\n        self.hostescalations.explode(self.escalations)\n        self.escalations.explode(self.hosts, self.hostgroups, self.contactgroups)\n\n        # Now the architecture part\n        self.realms.explode()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates dependencies links between elements.", "response": "def apply_dependencies(self):\n        \"\"\"Creates dependencies links between elements.\n\n        :return: None\n        \"\"\"\n        self.hosts.apply_dependencies()\n        self.services.apply_dependencies(self.hosts)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_inheritance(self):\n        # inheritance properties by template\n        self.hosts.apply_inheritance()\n        self.contacts.apply_inheritance()\n        self.services.apply_inheritance()\n        self.servicedependencies.apply_inheritance()\n        self.hostdependencies.apply_inheritance()\n        # Also timeperiods\n        self.timeperiods.apply_inheritance()\n        # Also \"Hostextinfo\"\n        self.hostsextinfo.apply_inheritance()\n        # Also \"Serviceextinfo\"\n        self.servicesextinfo.apply_inheritance()\n\n        # Now escalations too\n        self.serviceescalations.apply_inheritance()\n        self.hostescalations.apply_inheritance()\n        self.escalations.apply_inheritance()", "response": "Apply inheritance over templates\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfills objects properties with default value if necessary is None", "response": "def fill_default_configuration(self):\n        \"\"\"Fill objects properties with default value if necessary\n\n        :return: None\n        \"\"\"\n        logger.debug(\"Filling the unset properties with their default value:\")\n\n        types_creations = self.__class__.types_creations\n        for o_type in types_creations:\n            (_, _, inner_property, _, _) = types_creations[o_type]\n            # Not yet for the realms and daemons links\n            if inner_property in ['realms', 'arbiters', 'schedulers', 'reactionners',\n                                  'pollers', 'brokers', 'receivers']:\n                continue\n            logger.debug(\"  . for %s\", inner_property,)\n            inner_object = getattr(self, inner_property, None)\n            if inner_object is None:\n                logger.debug(\"No %s to fill with default values\", inner_property)\n                continue\n            inner_object.fill_default()\n\n        # We have all monitored elements, we can create a default realm if none is defined\n        if getattr(self, 'realms', None) is not None:\n            self.fill_default_realm()\n            self.realms.fill_default()\n\n            # Then we create missing satellites, so no other satellites will be created after\n            self.fill_default_satellites(self.launch_missing_daemons)\n\n        types_creations = self.__class__.types_creations\n        for o_type in types_creations:\n            (_, _, inner_property, _, _) = types_creations[o_type]\n            if getattr(self, inner_property, None) is None:\n                logger.debug(\"No %s to fill with default values\", inner_property)\n                continue\n            # Only for the daemons links\n            if inner_property in ['schedulers', 'reactionners', 'pollers', 'brokers', 'receivers']:\n                logger.debug(\"  . for %s\", inner_property,)\n                inner_object = getattr(self, inner_property)\n                inner_object.fill_default()\n\n        # Now fill some fields we can predict (like address for hosts)\n        self.hosts.fill_predictive_missing_parameters()\n        self.services.fill_predictive_missing_parameters()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fill_default_realm(self):\n        if not getattr(self, 'realms', None):\n            # Create a default realm so all hosts without realm will be linked with it\n            default = Realm({\n                'realm_name': u'All', 'alias': u'Self created default realm', 'default': '1'\n            })\n            self.realms = Realms([default])\n            logger.warning(\"No realms defined, I am adding one as %s\", default.get_name())\n\n        # Check that a default realm (and only one) is defined and get this default realm\n        self.realms.fill_default()", "response": "Check if a realm is defined and get this realm\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log_daemons_list(self):\n        daemons = [self.arbiters, self.schedulers, self.pollers,\n                   self.brokers, self.reactionners, self.receivers]\n        for daemons_list in daemons:\n            if not daemons_list:\n                logger.debug(\"- %ss: None\", daemons_list.inner_class.my_type)\n            else:\n                logger.debug(\"- %ss: %s\", daemons_list.inner_class.my_type,\n                             ','.join([daemon.get_name() for daemon in daemons_list]))", "response": "Log Alignak daemons list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fill_default_satellites(self, alignak_launched=False):\n        # pylint: disable=too-many-branches, too-many-locals, too-many-statements\n        \"\"\"If a required satellite is missing in the configuration, we create a new satellite\n        on localhost with some default values\n\n        :param alignak_launched: created daemons are to be launched or not\n        :type alignak_launched: bool\n        :return: None\n        \"\"\"\n\n        # Log all satellites list\n        logger.debug(\"Alignak configured daemons list:\")\n        self.log_daemons_list()\n\n        # We must create relations betweens the realms first. This is necessary to have\n        # an accurate map of the situation!\n        self.realms.linkify()\n        self.realms.get_default(check=True)\n\n        # Get list of known realms\n        # realms_names = [realm.get_name() for realm in self.realms]\n\n        # Create one instance of each satellite type if it does not exist...\n        if not self.schedulers:\n            logger.warning(\"No scheduler defined, I am adding one on 127.0.0.1:%d\",\n                           self.daemons_initial_port)\n            satellite = SchedulerLink({'type': 'scheduler', 'name': 'Default-Scheduler',\n                                       'realm': self.realms.default.get_name(),\n                                       'alignak_launched': alignak_launched,\n                                       'missing_daemon': True,\n                                       'spare': '0', 'manage_sub_realms': '0',\n                                       'address': '127.0.0.1', 'port': self.daemons_initial_port})\n            self.daemons_initial_port = self.daemons_initial_port + 1\n            self.schedulers = SchedulerLinks([satellite])\n            self.missing_daemons.append(satellite)\n        if not self.reactionners:\n            logger.warning(\"No reactionner defined, I am adding one on 127.0.0.1:%d\",\n                           self.daemons_initial_port)\n            satellite = ReactionnerLink({'type': 'reactionner', 'name': 'Default-Reactionner',\n                                         'realm': self.realms.default.get_name(),\n                                         'alignak_launched': alignak_launched,\n                                         'missing_daemon': True,\n                                         'spare': '0', 'manage_sub_realms': '0',\n                                         'address': '127.0.0.1', 'port': self.daemons_initial_port})\n            self.daemons_initial_port = self.daemons_initial_port + 1\n            self.reactionners = ReactionnerLinks([satellite])\n            self.missing_daemons.append(satellite)\n        if not self.pollers:\n            logger.warning(\"No poller defined, I am adding one on 127.0.0.1:%d\",\n                           self.daemons_initial_port)\n            satellite = PollerLink({'type': 'poller', 'name': 'Default-Poller',\n                                    'realm': self.realms.default.get_name(),\n                                    'alignak_launched': alignak_launched,\n                                    'missing_daemon': True,\n                                    'spare': '0', 'manage_sub_realms': '0',\n                                    'address': '127.0.0.1', 'port': self.daemons_initial_port})\n            self.daemons_initial_port = self.daemons_initial_port + 1\n            self.pollers = PollerLinks([satellite])\n            self.missing_daemons.append(satellite)\n        if not self.brokers:\n            logger.warning(\"No broker defined, I am adding one on 127.0.0.1:%d\",\n                           self.daemons_initial_port)\n            satellite = BrokerLink({'type': 'broker', 'name': 'Default-Broker',\n                                    'realm': self.realms.default.get_name(),\n                                    'alignak_launched': alignak_launched,\n                                    'missing_daemon': True,\n                                    'spare': '0', 'manage_sub_realms': '0',\n                                    'address': '127.0.0.1', 'port': self.daemons_initial_port})\n            self.daemons_initial_port = self.daemons_initial_port + 1\n            self.brokers = BrokerLinks([satellite])\n            self.missing_daemons.append(satellite)\n        if not self.receivers:\n            logger.warning(\"No receiver defined, I am adding one on 127.0.0.1:%d\",\n                           self.daemons_initial_port)\n            satellite = ReceiverLink({'type': 'receiver', 'name': 'Default-Receiver',\n                                      'alignak_launched': alignak_launched,\n                                      'missing_daemon': True,\n                                      'spare': '0', 'manage_sub_realms': '0',\n                                      'address': '127.0.0.1', 'port': self.daemons_initial_port})\n            self.daemons_initial_port = self.daemons_initial_port + 1\n            self.receivers = ReceiverLinks([satellite])\n            self.missing_daemons.append(satellite)\n\n        # Assign default realm to the satellites that do not have a defined realm\n        for satellites_list in [self.pollers, self.brokers, self.reactionners,\n                                self.receivers, self.schedulers]:\n            for satellite in satellites_list:\n                # Here the 'realm' property is not yet a real realm object uuid ...\n                # but still a realm name! Make it a realm uuid\n                if not getattr(satellite, 'realm', None):\n                    satellite.realm = self.realms.default.get_name()\n                sat_realm = self.realms.find_by_name(satellite.realm)\n                if not sat_realm:\n                    self.add_error(\"The %s '%s' is affected to an unknown realm: '%s'\"\n                                   % (satellite.type, satellite.name, satellite.realm))\n                    continue\n\n                # satellite.realm_name = sat_realm.get_name()\n                logger.info(\"Tagging satellite '%s' with realm %s\", satellite.name, satellite.realm)\n                satellite.realm = sat_realm.uuid\n                satellite.realm_name = sat_realm.get_name()\n\n                # Alert for spare daemons\n                if getattr(satellite, 'spare', False):\n                    self.add_warning(\"The %s '%s' is declared as a spare daemon. \"\n                                     \"Spare mode is not yet implemented and it will be ignored.\"\n                                     % (satellite.type, satellite.name))\n                    continue\n\n                # Alert for non active daemons\n                if not getattr(satellite, 'active', False):\n                    self.add_warning(\"The %s '%s' is declared as a non active daemon. \"\n                                     \"It will be ignored.\"\n                                     % (satellite.type, satellite.name))\n                    continue\n\n                # And tell the realm that it knows the satellite\n                realm_satellites = getattr(sat_realm, '%ss' % satellite.type)\n                if satellite.uuid not in realm_satellites:\n                    realm_satellites.append(satellite.uuid)\n\n                # If the satellite manages sub realms...\n                # We update the \"potential_\" satellites that may be used for this realm\n                if satellite.manage_sub_realms:\n                    for realm_uuid in sat_realm.all_sub_members:\n                        logger.debug(\"Linkify %s '%s' with realm %s\",\n                                     satellite.type, satellite.name,\n                                     self.realms[realm_uuid].get_name())\n                        realm_satellites = getattr(self.realms[realm_uuid],\n                                                   'potential_%ss' % satellite.type)\n                        if satellite.uuid not in realm_satellites:\n                            realm_satellites.append(satellite.uuid)\n\n        # Parse hosts for realms and set host in the default realm if no realm is set\n        hosts_realms_names = set()\n        logger.debug(\"Hosts realm configuration:\")\n        for host in self.hosts:\n            if not getattr(host, 'realm', None):\n                # todo: perharps checking hostgroups realm (if any) to set an hostgroup realm\n                # rather than the default realm\n                logger.debug(\"Host: %s, realm: %s, hostgroups: %s\",\n                             host.get_name(), host.realm, host.hostgroups)\n                host.realm = self.realms.default.get_name()\n                host.got_default_realm = True\n            host_realm = self.realms.find_by_name(host.realm)\n            if not host_realm:\n                self.add_error(\"The host '%s' is affected to an unknown realm: '%s'\"\n                               % (host.get_name(), host.realm))\n                continue\n            host.realm_name = host_realm.get_name()\n            host_realm.add_members(host.get_name())\n            logger.debug(\"- tagging host '%s' with realm %s\", host.get_name(), host.realm_name)\n            hosts_realms_names.add(host.realm_name)\n\n            logger.debug(\" - %s: realm %s, active %s, passive %s\",\n                         host.get_name(), host_realm.get_name(),\n                         host.active_checks_enabled, host.passive_checks_enabled)\n            host_realm.passively_checked_hosts = \\\n                host_realm.passively_checked_hosts or host.passive_checks_enabled\n            host_realm.actively_checked_hosts = \\\n                host_realm.actively_checked_hosts or host.passive_checks_enabled\n            hosts_realms_names.add(host.realm)\n\n        # Parse hostgroups for realms and set hostgroup in the default realm if no realm is set\n        hostgroups_realms_names = set()\n        logger.debug(\"Hostgroups realm configuration:\")\n        for hostgroup in self.hostgroups:\n            if not getattr(hostgroup, 'realm', None):\n                hostgroup.realm = self.realms.default.get_name()\n                hostgroup.got_default_realm = True\n            hostgroup_realm = self.realms.find_by_name(hostgroup.realm)\n            if not hostgroup_realm:\n                self.add_error(\"The hostgroup '%s' is affected to an unknown realm: '%s'\"\n                               % (hostgroup.get_name(), hostgroup.realm))\n                continue\n            hostgroup.realm_name = hostgroup_realm.get_name()\n            hostgroup_realm.add_group_members(hostgroup.get_name())\n            logger.debug(\"- tagging hostgroup '%s' with realm %s\",\n                         hostgroup.get_name(), hostgroup.realm_name)\n            hostgroups_realms_names.add(hostgroup.realm_name)\n\n        # Check that all daemons and realms are coherent\n        for satellites_list in [self.pollers, self.brokers, self.reactionners,\n                                self.receivers, self.schedulers]:\n            sat_class = satellites_list.inner_class\n            # Collect the names of all the realms that are managed by all the satellites\n            sat_realms_names = set()\n            for satellite in satellites_list:\n                for realm in self.realms:\n                    realm_satellites = getattr(realm, '%ss' % satellite.type)\n                    realm_potential_satellites = getattr(realm, 'potential_%ss' % satellite.type)\n                    if satellite.uuid in realm_satellites or \\\n                            satellite.uuid in realm_potential_satellites:\n                        sat_realms_names.add(realm.get_name())\n\n            if not hosts_realms_names.issubset(sat_realms_names):\n                # Check if a daemon is able to manage the concerned hosts...\n                for realm_name in hosts_realms_names.difference(sat_realms_names):\n                    realm = self.realms.find_by_name(realm_name)\n\n                    self.add_warning(\"Some hosts exist in the realm '%s' but no %s is \"\n                                     \"defined for this realm.\" % (realm_name, sat_class.my_type))\n\n                    if not alignak_launched:\n                        continue\n\n                    # Add a self-generated daemon\n                    logger.warning(\"Adding a %s for the realm: %s\", satellite.type, realm_name)\n                    new_daemon = sat_class({\n                        'type': satellite.type, 'name': '%s-%s' % (satellite.type, realm_name),\n                        'alignak_launched': True, 'missing_daemon': True,\n                        'realm': realm.uuid, 'manage_sub_realms': '0', 'spare': '0',\n                        'address': '127.0.0.1', 'port': self.daemons_initial_port\n                    })\n                    satellites_list.add_item(new_daemon)\n\n                    # And tell the realm that it knows the satellite\n                    realm_satellites = getattr(realm, '%ss' % satellite.type)\n                    if new_daemon.uuid not in realm_satellites:\n                        realm_satellites.append(new_daemon.uuid)\n\n                    self.add_warning(\"Added a %s (%s, %s) for the realm '%s'\"\n                                     % (satellite.type, '%s-%s' % (satellite.type, realm_name),\n                                        satellite.uri, realm_name))\n                    self.daemons_initial_port = self.daemons_initial_port + 1\n                    self.missing_daemons.append(new_daemon)\n\n        logger.debug(\"Realms hosts configuration:\")\n        for realm in self.realms:\n            logger.debug(\"Realm: %s, actively checked hosts %s, passively checked hosts %s\",\n                         realm.get_name(), realm.actively_checked_hosts,\n                         realm.passively_checked_hosts)\n            logger.info(\"Realm: %s, hosts: %s, groups: %s\",\n                        realm.get_name(), realm.members, realm.group_members)\n\n        # Log all satellites list\n        logger.debug(\"Alignak definitive daemons list:\")\n        self.log_daemons_list()", "response": "Create a new satellite for every realm that is not defined in the configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a module type is defined in one of the brokers", "response": "def got_broker_module_type_defined(self, module_type):\n        \"\"\"Check if a module type is defined in one of the brokers\n\n        :param module_type: module type to search for\n        :type module_type: str\n        :return: True if mod_type is found else False\n        :rtype: bool\n        \"\"\"\n        for broker_link in self.brokers:\n            for module in broker_link.modules:\n                if module.is_a_module(module_type):\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef got_scheduler_module_type_defined(self, module_type):\n        for scheduler_link in self.schedulers:\n            for module in scheduler_link.modules:\n                if module.is_a_module(module_type):\n                    return True\n        return False", "response": "Check if a module type is defined in one of the schedulers\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef got_arbiter_module_type_defined(self, module_type):\n        for arbiter in self.arbiters:\n            # Do like the linkify will do after....\n            for module in getattr(arbiter, 'modules', []):\n                # So look at what the arbiter try to call as module\n                module_name = module.get_name()\n                # Ok, now look in modules...\n                for mod in self.modules:\n                    # try to see if this module is the good type\n                    if getattr(mod, 'python_name', '').strip() == module_type.strip():\n                        # if so, the good name?\n                        if getattr(mod, 'name', '').strip() == module_name:\n                            return True\n        return False", "response": "Check if a module type is defined in one of the arbiters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_business_rules(self):\n        self.hosts.create_business_rules(self.hosts, self.services,\n                                         self.hostgroups, self.servicegroups,\n                                         self.macromodulations, self.timeperiods)\n        self.services.create_business_rules(self.hosts, self.services,\n                                            self.hostgroups, self.servicegroups,\n                                            self.macromodulations, self.timeperiods)", "response": "Create business rules for hosts and services."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates business rules dependencies for hosts and services.", "response": "def create_business_rules_dependencies(self):\n        \"\"\"Create business rules dependencies for hosts and services\n\n        :return: None\n        \"\"\"\n\n        for item in itertools.chain(self.hosts, self.services):\n            if not item.got_business_rule:\n                continue\n\n            bp_items = item.business_rule.list_all_elements()\n            for bp_item_uuid in bp_items:\n                if bp_item_uuid in self.hosts:\n                    bp_item = self.hosts[bp_item_uuid]\n                    notif_options = item.business_rule_host_notification_options\n                else:  # We have a service\n                    bp_item = self.services[bp_item_uuid]\n                    notif_options = item.business_rule_service_notification_options\n\n                if notif_options:\n                    bp_item.notification_options = notif_options\n\n                bp_item.act_depend_of_me.append((item.uuid, ['d', 'u', 's', 'f', 'c', 'w', 'x'],\n                                                 '', True))\n\n                # TODO: Is it necessary? We already have this info in act_depend_* attributes\n                item.parent_dependencies.add(bp_item.uuid)\n                bp_item.child_dependencies.add(item.uuid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if modules exist for some of the Nagios legacy parameters.", "response": "def hack_old_nagios_parameters(self):\n        # pylint: disable=too-many-branches\n        \"\"\" Check if modules exist for some of the Nagios legacy parameters.\n\n        If no module of the required type is present, it alerts the user that the parameters will\n        be ignored and the functions will be disabled, else it encourages the user to set the\n        correct parameters in the installed modules.\n\n        Note that some errors are raised if some parameters are used and no module is found\n        to manage the corresponding feature.\n\n        TODO: clean this part of the configuration checking! Nagios ascending compatibility!\n\n        :return: modules list\n        :rtype: list\n        \"\"\"\n        modules = []\n        # For status_dat\n        if getattr(self, 'status_file', None) and getattr(self, 'object_cache_file', None):\n            msg = \"The configuration parameters '%s = %s' and '%s = %s' are deprecated \" \\\n                  \"and will be ignored. Please configure your external 'retention' module \" \\\n                  \"as expected.\" % \\\n                  ('status_file', self.status_file,\n                   'object_cache_file', self.object_cache_file)\n            logger.warning(msg)\n            self.add_warning(msg)\n\n        # Now the log_file\n        if getattr(self, 'log_file', None):\n            msg = \"The configuration parameter '%s = %s' is deprecated \" \\\n                  \"and will be ignored. Please configure your external 'logs' module \" \\\n                  \"as expected.\" % \\\n                  ('log_file', self.log_file)\n            logger.warning(msg)\n            self.add_warning(msg)\n\n        # Now the syslog facility\n        if getattr(self, 'use_syslog', None):\n            msg = \"The configuration parameter '%s = %s' is deprecated \" \\\n                  \"and will be ignored. Please configure your external 'logs' module \" \\\n                  \"as expected.\" % \\\n                  ('use_syslog', self.use_syslog)\n            logger.warning(msg)\n            self.add_warning(msg)\n\n        # Now the host_perfdata or service_perfdata module\n        if getattr(self, 'service_perfdata_file', None) or \\\n                getattr(self, 'host_perfdata_file', None):\n            msg = \"The configuration parameters '%s = %s' and '%s = %s' are Nagios legacy \" \\\n                  \"parameters. Alignak will use its inner 'metrics' module \" \\\n                  \"to match the expected behavior.\" \\\n                  % ('host_perfdata_file', self.host_perfdata_file,\n                     'service_perfdata_file', self.service_perfdata_file)\n            logger.warning(msg)\n            self.add_warning(msg)\n            mod_configuration = {\n                'name': 'inner-metrics',\n                'type': 'metrics',\n                'python_name': 'alignak.modules.inner_metrics',\n                'imported_from': 'inner',\n                'enabled': True\n            }\n            if getattr(self, 'host_perfdata_file', None):\n                mod_configuration['host_perfdata_file'] = \\\n                    getattr(self, 'host_perfdata_file')\n            if getattr(self, 'service_perfdata_file', None):\n                mod_configuration['service_perfdata_file'] = \\\n                    getattr(self, 'service_perfdata_file')\n            logger.debug(\"inner metrics module, configuration: %s\", mod_configuration)\n\n            modules.append((\n                'broker', mod_configuration\n            ))\n\n        # Now the Nagios legacy retention file module\n        if hasattr(self, 'retain_state_information') and self.retain_state_information:\n            # Do not raise a warning log for this, only an information\n            msg = \"The configuration parameter '%s = %s' is a Nagios legacy \" \\\n                  \"parameter. Alignak will use its inner 'retention' module \" \\\n                  \"to match the expected behavior.\" \\\n                  % ('retain_state_information', self.retain_state_information)\n            logger.info(msg)\n            # self.add_warning(msg)\n            mod_configuration = {\n                'name': 'inner-retention',\n                'type': 'retention',\n                'python_name': 'alignak.modules.inner_retention',\n                'imported_from': 'inner',\n                'enabled': True\n            }\n            if getattr(self, 'state_retention_file', None) is not None:\n                mod_configuration['retention_file'] = getattr(self, 'state_retention_file')\n            if getattr(self, 'state_retention_dir', None) is not None:\n                mod_configuration['retention_dir'] = getattr(self, 'state_retention_dir')\n            if getattr(self, 'retention_update_interval', None):\n                self.tick_update_retention = int(self.retention_update_interval) * 60\n                mod_configuration['retention_period'] = int(self.retention_update_interval) * 60\n            logger.debug(\"inner retention module, configuration: %s\", mod_configuration)\n\n            modules.append((\n                'scheduler', mod_configuration\n            ))\n\n        # Now the command_file\n        if hasattr(self, 'command_file') and getattr(self, 'command_file'):\n            msg = \"The configuration parameter '%s = %s' is deprecated \" \\\n                  \"and will be ignored. Please configure an external commands capable \" \\\n                  \"module as expected (eg external-commands, NSCA, or WS module may suit.\" \\\n                  % ('command_file', self.command_file)\n            logger.warning(msg)\n            self.add_warning(msg)\n\n        return modules"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef propagate_timezone_option(self):\n        if self.use_timezone:\n            # first apply myself\n            os.environ['TZ'] = self.use_timezone\n            time.tzset()\n\n            tab = [self.schedulers, self.pollers, self.brokers, self.receivers, self.reactionners]\n            for sat_list in tab:\n                for sat in sat_list:\n                    if sat.use_timezone == 'NOTSET':\n                        setattr(sat, 'use_timezone', self.use_timezone)", "response": "Set our timezone value and give it too to unset satellites\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlink templates with each other", "response": "def linkify_templates(self):\n        \"\"\" Like for normal object, we link templates with each others\n\n        :return: None\n        \"\"\"\n        self.hosts.linkify_templates()\n        self.contacts.linkify_templates()\n        self.services.linkify_templates()\n        self.servicedependencies.linkify_templates()\n        self.hostdependencies.linkify_templates()\n        self.timeperiods.linkify_templates()\n        self.hostsextinfo.linkify_templates()\n        self.servicesextinfo.linkify_templates()\n        self.escalations.linkify_templates()\n        # But also old srv and host escalations\n        self.serviceescalations.linkify_templates()\n        self.hostescalations.linkify_templates()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_error_on_hard_unmanaged_parameters(self):\n        valid = True\n        if self.use_regexp_matching:\n            msg = \"use_regexp_matching parameter is not managed.\"\n            logger.warning(msg)\n            self.add_warning(msg)\n            valid &= False\n        if getattr(self, 'failure_prediction_enabled', None):\n            msg = \"failure_prediction_enabled parameter is not managed.\"\n            logger.warning(msg)\n            self.add_warning(msg)\n            valid &= False\n        if getattr(self, 'obsess_over_hosts', None):\n            msg = \"obsess_over_hosts parameter is not managed.\"\n            logger.warning(msg)\n            self.add_warning(msg)\n            valid &= False\n        if getattr(self, 'ochp_command', None):\n            msg = \"ochp_command parameter is not managed.\"\n            logger.warning(msg)\n            self.add_warning(msg)\n            valid &= False\n        if getattr(self, 'ochp_timeout', None):\n            msg = \"ochp_timeout parameter is not managed.\"\n            logger.warning(msg)\n            self.add_warning(msg)\n            valid &= False\n        if getattr(self, 'obsess_over_services', None):\n            msg = \"obsess_over_services parameter is not managed.\"\n            logger.warning(msg)\n            self.add_warning(msg)\n            valid &= False\n        if getattr(self, 'ocsp_command', None):\n            msg = \"ocsp_command parameter is not managed.\"\n            logger.warning(msg)\n            self.add_warning(msg)\n            valid &= False\n        if getattr(self, 'ocsp_timeout', None):\n            msg = \"ocsp_timeout parameter is not managed.\"\n            logger.warning(msg)\n            self.add_warning(msg)\n            valid &= False\n        return valid", "response": "Check if the hard unmanaged parameters are not managed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if all elements got a good configuration.", "response": "def is_correct(self):  # pylint: disable=too-many-branches, too-many-statements, too-many-locals\n        \"\"\"Check if all elements got a good configuration\n\n        :return: True if the configuration is correct else False\n        :rtype: bool\n        \"\"\"\n        logger.info('Running pre-flight check on configuration data, initial state: %s',\n                    self.conf_is_correct)\n        valid = self.conf_is_correct\n\n        # Check if alignak_name is defined\n        if not self.alignak_name:\n            logger.info('Alignak name is not defined, using the main arbiter name...')\n            for arbiter in self.arbiters:\n                if not arbiter.spare:\n                    self.alignak_name = arbiter.name\n                    break\n        logger.info('Alignak name is: %s', self.alignak_name)\n\n        # Globally unmanaged parameters\n        if not self.read_config_silent:\n            logger.info('Checking global parameters...')\n\n        # Old Nagios legacy unmanaged parameters\n        self.check_error_on_hard_unmanaged_parameters()\n\n        # If we got global event handlers, they should be valid\n        if self.global_host_event_handler and not self.global_host_event_handler.is_valid():\n            msg = \"[%s::%s] global host event_handler '%s' is invalid\" \\\n                  % (self.my_type, self.get_name(), self.global_host_event_handler.command)\n            self.add_error(msg)\n            valid = False\n\n        if self.global_service_event_handler and not self.global_service_event_handler .is_valid():\n            msg = \"[%s::%s] global service event_handler '%s' is invalid\" \\\n                  % (self.my_type, self.get_name(), self.global_service_event_handler .command)\n            self.add_error(msg)\n            valid = False\n\n        if not self.read_config_silent:\n            logger.info('Checked')\n\n        if not self.read_config_silent:\n            logger.info('Checking monitoring configuration...')\n\n        classes = [strclss for _, _, strclss, _, _ in list(self.types_creations.values())]\n        for strclss in sorted(classes):\n            if strclss in ['hostescalations', 'serviceescalations']:\n                logger.debug(\"Ignoring correctness check for '%s'...\", strclss)\n                continue\n\n            if not self.read_config_silent:\n                logger.info('- checking %s...', strclss)\n\n            try:\n                checked_list = getattr(self, strclss)\n            except AttributeError:  # pragma: no cover, simple protection\n                logger.info(\"\\t%s are not present in the configuration\", strclss)\n                continue\n\n            if not checked_list.is_correct():\n                if not self.read_config_silent:\n                    logger.info('Checked %s, configuration is incorrect!', strclss)\n\n                valid = False\n                self.configuration_errors += checked_list.configuration_errors\n                self.add_error(\"%s configuration is incorrect!\" % strclss)\n                logger.error(\"%s configuration is incorrect!\", strclss)\n            if checked_list.configuration_warnings:\n                self.configuration_warnings += checked_list.configuration_warnings\n                logger.info(\"    %d warning(s), total: %d\",\n                            len(checked_list.configuration_warnings),\n                            len(self.configuration_warnings))\n\n            if not self.read_config_silent:\n                try:\n                    dump_list = sorted(checked_list, key=lambda k: k.get_name())\n                except AttributeError:  # pragma: no cover, simple protection\n                    dump_list = checked_list\n\n                # Dump at DEBUG level because some tests break with INFO level, and it is not\n                # really necessary to have information about each object ;\n                for cur_obj in dump_list:\n                    if strclss == 'services':\n                        logger.debug('  %s', cur_obj.get_full_name())\n                    else:\n                        logger.debug('  %s', cur_obj.get_name())\n                if checked_list:\n                    logger.info('  checked %d', len(checked_list))\n                else:\n                    logger.info('  none')\n\n        if not self.read_config_silent:\n            logger.info('Checked')\n\n        # Parse hosts and services for tags and realms\n        hosts_tag = set()\n        services_tag = set()\n        for host in self.hosts:\n            hosts_tag.add(host.poller_tag)\n        for service in self.services:\n            services_tag.add(service.poller_tag)\n\n        # Check that for each poller_tag of a host, a poller exists with this tag\n        pollers_tag = set()\n        for poller in self.pollers:\n            for tag in poller.poller_tags:\n                pollers_tag.add(tag)\n\n        if not hosts_tag.issubset(pollers_tag):\n            for tag in hosts_tag.difference(pollers_tag):\n                self.add_error(\"Error: some hosts have the poller_tag %s but no poller \"\n                               \"has this tag\" % tag)\n                valid = False\n        if not services_tag.issubset(pollers_tag):\n            for tag in services_tag.difference(pollers_tag):\n                self.add_error(\"some services have the poller_tag %s but no poller \"\n                               \"has this tag\" % tag)\n                valid = False\n\n        # Check that all hosts involved in business_rules are from the same realm\n        for item in self.hosts:\n            if not item.got_business_rule:\n                continue\n\n            realm = self.realms[item.realm]\n            if not realm:\n                # Something was wrong in the conf, will be raised elsewhere\n                continue\n\n            for elt_uuid in item.business_rule.list_all_elements():\n                if elt_uuid not in self.hosts:\n                    # An error or a service element\n                    continue\n\n                host = self.hosts[elt_uuid]\n                if host.realm not in self.realms:\n                    # Something was wrong in the conf, will be raised elsewhere\n                    continue\n\n                host_realm = self.realms[host.realm]\n                if host_realm.get_name() != realm.get_name():\n                    logger.error(\"Business_rule '%s' got some hosts from another realm: %s\",\n                                 item.get_full_name(), host_realm.get_name())\n                    self.add_error(\"Error: Business_rule '%s' got hosts from another \"\n                                   \"realm: %s\" % (item.get_full_name(), host_realm.get_name()))\n                    valid = False\n\n        # for lst in [self.services, self.hosts]:\n        #     for item in lst:\n        #         if item.got_business_rule:\n        #             e_ro = self.realms[item.realm]\n        #             # Something was wrong in the conf, will be raised elsewhere\n        #             if not e_ro:\n        #                 continue\n        #             e_r = e_ro.realm_name\n        #             for elt_uuid in item.business_rule.list_all_elements():\n        #                 if elt_uuid in self.hosts:\n        #                     elt = self.hosts[elt_uuid]\n        #                 else:\n        #                     elt = self.services[elt_uuid]\n        #                 r_o = self.realms[elt.realm]\n        #                 # Something was wrong in the conf, will be raised elsewhere\n        #                 if not r_o:\n        #                     continue\n        #                 elt_r = r_o.realm_name\n        #                 if elt_r != e_r:\n        #                     logger.error(\"Business_rule '%s' got hosts from another realm: %s\",\n        #                                  item.get_full_name(), elt_r)\n        #                     self.add_error(\"Error: Business_rule '%s' got hosts from another \"\n        #                                    \"realm: %s\" % (item.get_full_name(), elt_r))\n        #                     valid = False\n\n        if self.configuration_errors:\n            valid = False\n            logger.error(\"Configuration errors:\")\n            for msg in self.configuration_errors:\n                logger.error(msg)\n\n        # If configuration error messages exist, then the configuration is not valid\n        self.conf_is_correct = valid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexploding parameters like cached_service_check_horizon in the Service class in a cached_check_horizon manner o * hp commands etc", "response": "def explode_global_conf(self):\n        \"\"\"Explode parameters like cached_service_check_horizon in the\n        Service class in a cached_check_horizon manner, o*hp commands etc\n\n        :return: None\n        \"\"\"\n        for cls, _, strclss, _, _ in list(self.types_creations.values()):\n            logger.debug(\"Applying global conf for the class '%s'...\", strclss)\n            cls.load_global_conf(self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_templates(self):\n        self.hosts.remove_templates()\n        self.contacts.remove_templates()\n        self.services.remove_templates()\n        self.servicedependencies.remove_templates()\n        self.hostdependencies.remove_templates()\n        self.timeperiods.remove_templates()", "response": "Clean useless elements like templates because they are not needed anymore."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing the configuration warnings and errors.", "response": "def show_errors(self):\n        \"\"\"\n        Loop over configuration warnings and log them as INFO log\n        Loop over configuration errors and log them as INFO log\n\n        Note that the warnings and errors are logged on the fly during the configuration parsing.\n        It is not necessary to log as WARNING and ERROR in this function which is used as a sum-up\n        on the end of configuration parsing when an error has been detected.\n\n        :return:  None\n        \"\"\"\n        if self.configuration_warnings:\n            logger.warning(\"Configuration warnings:\")\n            for msg in self.configuration_warnings:\n                logger.warning(msg)\n        if self.configuration_errors:\n            logger.warning(\"Configuration errors:\")\n            for msg in self.configuration_errors:\n                logger.warning(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a graph of hosts and services and their dependencies.", "response": "def create_packs(self):\n        # pylint: disable=too-many-statements,too-many-locals,too-many-branches, unused-argument\n        \"\"\"Create packs of hosts and services (all dependencies are resolved)\n        It create a graph. All hosts are connected to their\n        parents, and hosts without parent are connected to host 'root'.\n        services are linked to their host. Dependencies between hosts/services are managed.\n        REF: doc/pack-creation.png\n\n        :return: None\n        \"\"\"\n        logger.info(\"- creating hosts packs for the realms:\")\n\n        # We create a graph with host in nodes\n        graph = Graph()\n        graph.add_nodes(list(self.hosts.items.keys()))\n\n        # links will be used for relations between hosts\n        links = set()\n\n        # Now the relations\n        for host in self.hosts:\n            # Add parent relations\n            for parent in getattr(host, 'parents', []):\n                if parent:\n                    links.add((parent, host.uuid))\n            # Add the others dependencies\n            for (dep, _, _, _) in host.act_depend_of:\n                links.add((dep, host.uuid))\n            for (dep, _, _, _, _) in host.chk_depend_of:\n                links.add((dep, host.uuid))\n\n        # For services: they are linked with their own host but we need\n        # to have the hosts of the service dependency in the same pack too\n        for service in self.services:\n            for (dep_id, _, _, _) in service.act_depend_of:\n                if dep_id in self.services:\n                    dep = self.services[dep_id]\n                else:\n                    dep = self.hosts[dep_id]\n                # I don't care about dep host: they are just the host\n                # of the service...\n                if hasattr(dep, 'host'):\n                    links.add((dep.host, service.host))\n            # The other type of dep\n            for (dep_id, _, _, _, _) in service.chk_depend_of:\n                if dep_id in self.services:\n                    dep = self.services[dep_id]\n                else:\n                    dep = self.hosts[dep_id]\n                links.add((dep.host, service.host))\n\n        # For host/service that are business based, we need to link them too\n        for service in [srv for srv in self.services if srv.got_business_rule]:\n            for elem_uuid in service.business_rule.list_all_elements():\n                if elem_uuid in self.services:\n                    elem = self.services[elem_uuid]\n                    if elem.host != service.host:  # do not link a host with itself\n                        links.add((elem.host, service.host))\n                else:  # it's already a host but only if it is in the known hosts list!\n                    if elem_uuid in self.hosts and elem_uuid != service.host:\n                        links.add((elem_uuid, service.host))\n\n        # Same for hosts of course\n        for host in [hst for hst in self.hosts if hst.got_business_rule]:\n            for elem_uuid in host.business_rule.list_all_elements():\n                if elem_uuid in self.services:  # if it's a service\n                    elem = self.services[elem_uuid]\n                    if elem.host != host.uuid:\n                        links.add((elem.host, host.uuid))\n                else:  # e is a host\n                    if elem_uuid != host.uuid:\n                        links.add((elem_uuid, host.uuid))\n\n        # Now we create links in the graph. With links (set)\n        # We are sure to call the less add_edge\n        for (dep, host) in links:\n            graph.add_edge(dep, host)\n            graph.add_edge(host, dep)\n\n        # Now We find the default realm\n        default_realm = self.realms.get_default()\n\n        # Access_list from a node il all nodes that are connected\n        # with it: it's a list of ours mini_packs\n        # Now we look if all elements of all packs have the\n        # same realm. If not, not good!\n        for hosts_pack in graph.get_accessibility_packs():\n            passively_checked_hosts = False\n            actively_checked_hosts = False\n            tmp_realms = set()\n            logger.debug(\" - host pack hosts:\")\n            for host_id in hosts_pack:\n                host = self.hosts[host_id]\n                logger.debug(\"  - %s\", host.get_name())\n                passively_checked_hosts = passively_checked_hosts or host.passive_checks_enabled\n                actively_checked_hosts = actively_checked_hosts or host.active_checks_enabled\n                if host.realm:\n                    tmp_realms.add(host.realm)\n            if len(tmp_realms) > 1:\n                self.add_error(\"Error: the realm configuration of your hosts is not correct \"\n                               \"because there is more than one realm in one pack (host relations):\")\n                for host_id in hosts_pack:\n                    host = self.hosts[host_id]\n                    if not host.realm:\n                        self.add_error(' -> the host %s do not have a realm' % host.get_name())\n                    else:\n                        # Do not use get_name for the realm because it is not an object but a\n                        # string containing the not found realm name if the realm is not existing!\n                        # As of it, it may raise an exception\n                        if host.realm not in self.realms:\n                            self.add_error(' -> the host %s is in the realm %s' %\n                                           (host.get_name(), host.realm))\n                        else:\n                            host_realm = self.realms[host.realm]\n                            self.add_error(' -> the host %s is in the realm %s' %\n                                           (host.get_name(), host_realm.get_name()))\n            if len(tmp_realms) == 1:  # Ok, good\n                tmp_realm = tmp_realms.pop()\n                if tmp_realm in self.realms:\n                    realm = self.realms[tmp_realm]\n                else:\n                    realm = self.realms.find_by_name(tmp_realm)\n                if not realm:\n                    self.add_error(' -> some hosts are in an unknown realm %s!' % tmp_realm)\n                else:\n                    # Set the current hosts pack to its realm\n                    logger.debug(\" - append pack %s to realm %s\", hosts_pack, realm.get_name())\n                    realm.packs.append(hosts_pack)\n                    # Set if the realm only has passively or actively checked hosts...\n                    realm.passively_checked_hosts = passively_checked_hosts\n                    realm.actively_checked_hosts = actively_checked_hosts\n            elif not tmp_realms:  # Hum... no realm value? So default Realm\n                if default_realm is not None:\n                    # Set the current hosts pack to the default realm\n                    default_realm.packs.append(hosts_pack)\n                else:\n                    self.add_error(\"Error: some hosts do not have a realm and you did not \"\n                                   \"defined a default realm!\")\n                    for host in hosts_pack:\n                        self.add_error('    Impacted host: %s ' % host.get_name())\n\n        # The load balancing is for a loop, so all\n        # hosts of a realm (in a pack) will be dispatched\n        # to the schedulers of this realm\n        # REF: doc/pack-aggregation.png\n\n        # Count the numbers of elements in all the realms,\n        # to compare with the total number of hosts\n        nb_elements_all_realms = 0\n        for realm in self.realms:\n            packs = {}\n            # create round-robin iterator for id of cfg\n            # So dispatching is load balanced in a realm\n            # but add a entry in the round-robin tourniquet for\n            # every weight point schedulers (so Weight round robin)\n            weight_list = []\n            no_spare_schedulers = realm.schedulers\n            if not no_spare_schedulers:\n                if realm.potential_schedulers:\n                    no_spare_schedulers = [realm.potential_schedulers[0]]\n            nb_schedulers = len(no_spare_schedulers)\n            if nb_schedulers:\n                logger.info(\"  %d scheduler(s) for the realm %s\", nb_schedulers, realm.get_name())\n            else:\n                logger.warning(\"  no scheduler for the realm %s\", realm.get_name())\n\n            # Maybe there is no scheduler in the realm, it can be a\n            # big problem if there are elements in packs\n            nb_elements = 0\n            for hosts_pack in realm.packs:\n                nb_elements += len(hosts_pack)\n                nb_elements_all_realms += len(hosts_pack)\n            realm.hosts_count = nb_elements\n            if nb_elements:\n                if not nb_schedulers:\n                    self.add_error(\"The realm %s has %d hosts but no scheduler!\"\n                                   % (realm.get_name(), nb_elements))\n                    realm.packs = []  # Dumb pack\n                    continue\n\n                logger.info(\"  %d hosts in the realm %s, distributed in %d linked packs\",\n                            nb_elements, realm.get_name(), len(realm.packs))\n            else:\n                logger.info(\"  no hosts in the realm %s\", realm.get_name())\n\n            # Create a relation between a pack and each scheduler in the realm\n            packindex = 0\n            packindices = {}\n            for s_id in no_spare_schedulers:\n                scheduler = self.schedulers[s_id]\n                logger.debug(\"  scheduler: %s\", scheduler.instance_id)\n                packindices[s_id] = packindex\n                packindex += 1\n                for i in range(0, scheduler.weight):\n                    weight_list.append(s_id)\n            logger.debug(\"  pack indices: %s\", packindices)\n            # packindices is indexed with the scheduler id and contains\n            # the configuration part number to get used: sched1:0, sched2: 1, ...\n\n            round_robin = itertools.cycle(weight_list)\n\n            # We must initialize nb_schedulers packs\n            for i in range(0, nb_schedulers):\n                packs[i] = []\n\n            # Try to load the history association dict so we will try to\n            # send the hosts in the same \"pack\"\n            assoc = {}\n\n            # Now we explode the numerous packs into reals packs:\n            # we 'load balance' them in a round-robin way but with count number of hosts in\n            # case have some packs with too many hosts and other with few\n            realm.packs.sort(reverse=True)\n            pack_higher_hosts = 0\n            for hosts_pack in realm.packs:\n                valid_value = False\n                old_pack = -1\n                for host_id in hosts_pack:\n                    host = self.hosts[host_id]\n                    old_i = assoc.get(host.get_name(), -1)\n                    # Maybe it's a new, if so, don't count it\n                    if old_i == -1:\n                        continue\n                    # Maybe it is the first we look at, if so, take it's value\n                    if old_pack == -1 and old_i != -1:\n                        old_pack = old_i\n                        valid_value = True\n                        continue\n                    if old_i == old_pack:\n                        valid_value = True\n                    if old_i != old_pack:\n                        valid_value = False\n                # If it's a valid sub pack and the pack id really exist, use it!\n                if valid_value and old_pack in packindices:\n                    i = old_pack\n                else:\n                    if isinstance(i, int):\n                        i = next(round_robin)\n                    elif (len(packs[packindices[i]]) + len(hosts_pack)) >= pack_higher_hosts:\n                        pack_higher_hosts = (len(packs[packindices[i]]) + len(hosts_pack))\n                        i = next(round_robin)\n\n                for host_id in hosts_pack:\n                    host = self.hosts[host_id]\n                    packs[packindices[i]].append(host_id)\n                    assoc[host.get_name()] = i\n\n            # Now packs is a dictionary indexed with the configuration part\n            # number and containing the list of hosts\n            realm.packs = packs\n\n        logger.info(\"  total number of hosts in all realms: %d\", nb_elements_all_realms)\n        if len(self.hosts) != nb_elements_all_realms:\n            logger.warning(\"There are %d hosts defined, and %d hosts dispatched in the realms. \"\n                           \"Some hosts have been ignored\", len(self.hosts), nb_elements_all_realms)\n            self.add_error(\"There are %d hosts defined, and %d hosts dispatched in the realms. \"\n                           \"Some hosts have been \"\n                           \"ignored\" % (len(self.hosts), nb_elements_all_realms))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cut_into_parts(self):\n        # pylint: disable=too-many-branches, too-many-locals, too-many-statements\n        \"\"\"Cut conf into part for scheduler dispatch.\n\n        Basically it provides a set of host/services for each scheduler that\n        have no dependencies between them\n\n        :return: None\n        \"\"\"\n        # User must have set a spare if he needed one\n        logger.info(\"Splitting the configuration into parts:\")\n        nb_parts = 0\n        for realm in self.realms:\n            no_spare_schedulers = realm.schedulers\n            if not no_spare_schedulers:\n                if realm.potential_schedulers:\n                    no_spare_schedulers = [realm.potential_schedulers[0]]\n            nb_schedulers = len(no_spare_schedulers)\n            nb_parts += nb_schedulers\n            if nb_schedulers:\n                logger.info(\"  %d scheduler(s) for the realm %s\", nb_schedulers, realm.get_name())\n            else:\n                logger.warning(\"  no scheduler for the realm %s\", realm.get_name())\n\n        if nb_parts == 0:\n            nb_parts = 1\n\n        # We create dummy configurations for schedulers:\n        # they are clone of the master configuration but without hosts and\n        # services (because they are splitted between these configurations)\n        logger.info(\"Splitting the configuration into %d parts...\", nb_parts)\n        self.parts = {}\n        for part_index in range(0, nb_parts):\n            self.parts[part_index] = Config()\n\n            # Now we copy all properties of conf into the new ones\n            for prop, entry in sorted(list(Config.properties.items())):\n                # Do not copy the configuration instance id nor name!\n                if prop in ['instance_id', 'config_name']:\n                    continue\n                # Only the one that are managed and used\n                if entry.managed and not isinstance(entry, UnusedProp):\n                    val = getattr(self, prop, None)\n                    setattr(self.parts[part_index], prop, val)\n\n            # Set the cloned configuration name\n            self.parts[part_index].config_name = \"%s (%d)\" % (self.config_name, part_index)\n            logger.debug(\"- cloning configuration: %s -> %s\",\n                         self.parts[part_index].config_name, self.parts[part_index])\n\n            # Copy the configuration objects lists. We need a deepcopy because each configuration\n            # will have some new groups... but we create a new uuid\n            self.parts[part_index].uuid = get_a_new_object_id()\n\n            types_creations = self.__class__.types_creations\n            for o_type in types_creations:\n                (_, clss, inner_property, _, clonable) = types_creations[o_type]\n                if not clonable:\n                    logger.debug(\"  . do not clone: %s\", inner_property)\n                    continue\n                # todo: Indeed contactgroups should be managed like hostgroups...\n                if inner_property in ['hostgroups', 'servicegroups']:\n                    new_groups = []\n                    for group in getattr(self, inner_property):\n                        new_groups.append(group.copy_shell())\n                    setattr(self.parts[part_index], inner_property, clss(new_groups))\n                elif inner_property in ['hosts', 'services']:\n                    setattr(self.parts[part_index], inner_property, clss([]))\n                else:\n                    setattr(self.parts[part_index], inner_property, getattr(self, inner_property))\n                logger.debug(\"  . cloned %s: %s -> %s\", inner_property,\n                             getattr(self, inner_property),\n                             getattr(self.parts[part_index], inner_property))\n\n            # The elements of the others conf will be tag here\n            self.parts[part_index].other_elements = {}\n\n            # No scheduler has yet accepted the configuration\n            self.parts[part_index].is_assigned = False\n            self.parts[part_index].scheduler_link = None\n            self.parts[part_index].push_flavor = ''\n        # Once parts got created, the current configuration has some 'parts'\n        # self.parts is the configuration split into parts for the schedulers\n\n        # Just create packs. There can be numerous ones\n        # In pack we've got hosts and service and packs are in the realms\n        logger.debug(\"Creating packs for realms...\")\n        self.create_packs()\n        # Once packs got created, all the realms have some 'packs'\n\n        logger.info(\"Realms:\")\n        for realm in self.realms:\n            logger.info(\" - realm: %s\", realm)\n            for idx in realm.packs:\n                logger.info(\"   - pack: %s / %d hosts (%s)\",\n                            idx, len(realm.packs[idx]), ','.join([self.hosts[host_id].get_name()\n                                                                  for host_id in realm.packs[idx]]))\n\n        # We have packs for realms and elements into configurations, let's merge this...\n        logger.info(\"Realms:\")\n        offset = 0\n        for realm in self.realms:\n            logger.info(\" Realm: %s\", realm)\n            for idx in realm.packs:\n                logger.info(\" - pack: %s / %d hosts\", idx, len(realm.packs[idx]))\n                if not realm.packs[idx]:\n                    logger.info(\" - no hosts are declared in this realm pack.\")\n                    # continue\n                try:\n                    instance_id = self.parts[idx + offset].instance_id\n                    for host_id in realm.packs[idx]:\n                        host = self.hosts[host_id]\n                        self.parts[idx + offset].hosts.add_item(host)\n                        for service_id in host.services:\n                            service = self.services[service_id]\n                            self.parts[idx + offset].services.add_item(service)\n                    # Now the conf can be linked with the realm\n                    realm.parts.update({instance_id: self.parts[idx + offset]})\n                    # offset += 1\n                except KeyError:\n                    logger.info(\" - no configuration part is affected \"\n                                \"because of mismatching hosts packs / schedulers count. \"\n                                \"Probably too much schedulers for the hosts count!\")\n\n            offset += len(realm.packs)\n            del realm.packs\n\n        # We've nearly have hosts and services. Now we want real hosts (Class)\n        # And we want groups too\n        for part_index in self.parts:\n            cfg = self.parts[part_index]\n\n            # Fill host groups\n            for ori_hg in self.hostgroups:\n                hostgroup = cfg.hostgroups.find_by_name(ori_hg.get_name())\n                mbrs_id = []\n                for host in ori_hg.members:\n                    if host != '':\n                        mbrs_id.append(host)\n                for host in cfg.hosts:\n                    if host.uuid in mbrs_id:\n                        hostgroup.members.append(host.uuid)\n\n            # And also relink the hosts with the valid hostgroups\n            for host in cfg.hosts:\n                orig_hgs = host.hostgroups\n                nhgs = []\n                for ohg_id in orig_hgs:\n                    ohg = self.hostgroups[ohg_id]\n                    nhg = cfg.hostgroups.find_by_name(ohg.get_name())\n                    nhgs.append(nhg.uuid)\n                host.hostgroups = nhgs\n\n            # Fill servicegroup\n            for ori_sg in self.servicegroups:\n                servicegroup = cfg.servicegroups.find_by_name(ori_sg.get_name())\n                mbrs = ori_sg.members\n                mbrs_id = []\n                for service in mbrs:\n                    if service != '':\n                        mbrs_id.append(service)\n                for service in cfg.services:\n                    if service.uuid in mbrs_id:\n                        servicegroup.members.append(service.uuid)\n\n            # And also relink the services with the valid servicegroups\n            for host in cfg.services:\n                orig_hgs = host.servicegroups\n                nhgs = []\n                for ohg_id in orig_hgs:\n                    ohg = self.servicegroups[ohg_id]\n                    nhg = cfg.servicegroups.find_by_name(ohg.get_name())\n                    nhgs.append(nhg.uuid)\n                host.servicegroups = nhgs\n\n        # Now we fill other_elements by host (service are with their host\n        # so they are not tagged)\n        logger.info(\"Configuration parts:\")\n        for part_index in self.parts:\n            for host in self.parts[part_index].hosts:\n                for j in [j for j in self.parts if j != part_index]:  # So other than i\n                    self.parts[part_index].other_elements[host.get_name()] = part_index\n            logger.info(\"- part: %d - %s, %d hosts\", part_index, self.parts[part_index],\n                        len(self.parts[part_index].hosts))", "response": "This function cuts the configuration into parts for scheduler dispatch."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare the configuration for sending to a spare arbiter", "response": "def prepare_for_sending(self):\n        \"\"\"The configuration needs to be serialized before being sent to a spare arbiter\n\n        :return: None\n        \"\"\"\n        if [arbiter_link for arbiter_link in self.arbiters if arbiter_link.spare]:\n            logger.info('Serializing the configuration for my spare arbiter...')\n\n            # Now serialize the whole configuration, for sending to spare arbiters\n            self.spare_arbiter_conf = serialize(self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump(self, dump_file_name=None):\n        config_dump = {}\n\n        for _, _, category, _, _ in list(self.types_creations.values()):\n            try:\n                objs = [jsonify_r(i) for i in getattr(self, category)]\n            except (TypeError, AttributeError):  # pragma: no cover, simple protection\n                logger.warning(\"Dumping configuration, '%s' not present in the configuration\",\n                               category)\n                continue\n\n            container = getattr(self, category)\n            if category == \"services\":\n                objs = sorted(objs,\n                              key=lambda o: \"%s/%s\" % (o[\"host_name\"], o[\"service_description\"]))\n            elif hasattr(container, \"name_property\"):\n                name_prop = container.name_property\n                objs = sorted(objs, key=lambda o, prop=name_prop: getattr(o, prop, ''))\n            config_dump[category] = objs\n\n        if not dump_file_name:\n            dump_file_name = os.path.join(tempfile.gettempdir(),\n                                          'alignak-%s-cfg-dump-%d.json'\n                                          % (self.name, int(time.time())))\n        try:\n            logger.info('Dumping configuration to: %s', dump_file_name)\n            fd = open(dump_file_name, \"w\")\n            fd.write(json.dumps(config_dump, indent=4, separators=(',', ': '), sort_keys=True))\n            fd.close()\n            logger.info('Dumped')\n        except (OSError, IndexError) as exp:  # pragma: no cover, should never happen...\n            logger.critical(\"Error when dumping configuration to %s: %s\", dump_file_name, str(exp))", "response": "Dump the configuration to a JSON file in a JSON format."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds an object to the daemon internal list.", "response": "def add(self, elt):\n        \"\"\"Generic function to add objects to the daemon internal lists.\n        Manage Broks, External commands\n\n        :param elt: objects to add\n        :type elt: alignak.AlignakObject\n        :return: None\n        \"\"\"\n        if isinstance(elt, Brok):\n            # For brok, we tag the brok with our instance_id\n            elt.instance_id = self.instance_id\n            if elt.type == 'monitoring_log':\n                # The brok is a monitoring event\n                with self.events_lock:\n                    self.events.append(elt)\n                statsmgr.counter('events', 1)\n            else:\n                with self.broks_lock:\n                    self.broks.append(elt)\n            statsmgr.counter('broks.added', 1)\n        elif isinstance(elt, ExternalCommand):\n            logger.debug(\"Queuing an external command '%s'\", str(elt.__dict__))\n            with self.external_commands_lock:\n                self.external_commands.append(elt)\n                statsmgr.counter('external-commands.added', 1)\n        else:  # pragma: no cover, simple dev alerting\n            logger.error('Do not manage object type %s (%s)', type(elt), elt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends all broks from arbiter internal list to all the brokers", "response": "def push_broks_to_broker(self):  # pragma: no cover - not used!\n        \"\"\"Send all broks from arbiter internal list to broker\n\n        The arbiter get some broks and then pushes them to all the brokers.\n\n        :return: None\n        \"\"\"\n        someone_is_concerned = False\n        sent = False\n        for broker_link in self.conf.brokers:\n            # Send only if the broker is concerned...\n            if not broker_link.manage_arbiters:\n                continue\n\n            someone_is_concerned = True\n            if broker_link.reachable:\n                logger.debug(\"Sending %d broks to the broker %s\", len(self.broks), broker_link.name)\n                if broker_link.push_broks(self.broks):\n                    statsmgr.counter('broks.pushed.count', len(self.broks))\n                    sent = True\n\n        if not someone_is_concerned or sent:\n            # No one is anymore interested with...\n            del self.broks[:]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend external commands to all schedulers", "response": "def push_external_commands_to_schedulers(self):  # pragma: no cover - not used!\n        \"\"\"Send external commands to schedulers\n\n        :return: None\n        \"\"\"\n        # Now get all external commands and push them to the schedulers\n        for external_command in self.external_commands:\n            self.external_commands_manager.resolve_command(external_command)\n\n        # Now for all reachable schedulers, send the commands\n        sent = False\n        for scheduler_link in self.conf.schedulers:\n            ext_cmds = scheduler_link.external_commands\n            if ext_cmds and scheduler_link.reachable:\n                logger.debug(\"Sending %d commands to the scheduler %s\",\n                             len(ext_cmds), scheduler_link.name)\n                if scheduler_link.push_external_commands(ext_cmds):\n                    statsmgr.counter('external-commands.pushed.count', len(ext_cmds))\n                    sent = True\n            if sent:\n                # Clean the pushed commands\n                scheduler_link.external_commands.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting broks from all the known satellites", "response": "def get_broks_from_satellites(self):  # pragma: no cover - not used!\n        \"\"\"Get broks from my all internal satellite links\n\n        The arbiter get the broks from ALL the known satellites\n\n        :return: None\n        \"\"\"\n        for satellites in [self.conf.brokers, self.conf.schedulers,\n                           self.conf.pollers, self.conf.reactionners, self.conf.receivers]:\n            for satellite in satellites:\n                # Get only if reachable...\n                if not satellite.reachable:\n                    continue\n                logger.debug(\"Getting broks from: %s\", satellite.name)\n                new_broks = satellite.get_and_clear_broks()\n                if new_broks:\n                    logger.debug(\"Got %d broks from: %s\", len(new_broks), satellite.name)\n                for brok in new_broks:\n                    self.add(brok)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_initial_broks_from_satellites(self):\n        for satellites in [self.conf.brokers, self.conf.schedulers,\n                           self.conf.pollers, self.conf.reactionners, self.conf.receivers]:\n            for satellite in satellites:\n                # Get only if reachable...\n                if not satellite.reachable:\n                    continue\n                logger.debug(\"Getting initial brok from: %s\", satellite.name)\n                brok = satellite.get_initial_status_brok()\n                logger.debug(\"Satellite '%s' initial brok: %s\", satellite.name, brok)\n                self.add(brok)", "response": "Get initial broks from my internal satellite links"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the main configuration file and create the appropriate objects.", "response": "def load_monitoring_config_file(self, clean=True):\n        # pylint: disable=too-many-branches,too-many-statements, too-many-locals\n        \"\"\"Load main configuration file (alignak.cfg)::\n\n        * Read all files given in the -c parameters\n        * Read all .cfg files in cfg_dir\n        * Read all files in cfg_file\n        * Create objects (Arbiter, Module)\n        * Set HTTP links info (ssl etc)\n        * Load its own modules\n        * Execute read_configuration hook (for arbiter modules)\n        * Create all objects (Service, Host, Realms ...)\n        * \"Compile\" configuration (Linkify, explode, apply inheritance, fill default values ...)\n        * Cut conf into parts and prepare it for sending\n\n        The clean parameter is useful to load a configuration without removing the properties\n        only used to parse the configuration and create the objects. Some utilities (like\n        alignak-backend-import script) may need to avoid the cleaning ;)\n\n        :param clean: set True to clean the created items\n        :type clean: bool\n\n        :return: None\n        \"\"\"\n        self.loading_configuration = True\n        _t_configuration = time.time()\n\n        if self.verify_only:\n            # Force adding a console handler to the Alignak logger\n            set_log_console(logging.INFO if not self.debug else logging.DEBUG)\n            # Force the global logger at INFO level\n            set_log_level(logging.INFO if not self.debug else logging.DEBUG)\n            logger.info(\"-----\")\n            logger.info(\"Arbiter is in configuration check mode\")\n            logger.info(\"Arbiter log level got increased to a minimum of INFO\")\n            logger.info(\"-----\")\n\n        # Maybe we do not have environment file\n        # if not self.alignak_env:\n        #     self.exit_on_error(\"*** No Alignak environment file. Exiting...\", exit_code=2)\n        # else:\n        #     logger.info(\"Environment file: %s\", self.env_filename)\n        if self.legacy_cfg_files:\n            logger.info(\"Loading monitored system configuration from legacy files: %s\",\n                        self.legacy_cfg_files)\n        else:\n            logger.info(\"No legacy file(s) configured for monitored system configuration\")\n\n        # Alignak global environment file\n        # -------------------------------\n        # Here we did not yet read the Alignak configuration file (except for the Arbiter daemon\n        # configuration.\n        # We must get the Alignak macros and global configuration parameters\n        # ---------------------\n        # Manage Alignak macros; this before loading the legacy configuration files\n        # with their own potential macros\n        # ---------------------\n        macros = []\n        # Get the macros / variables declared in the Alignak environment (alignak.ini) file!\n        if self.alignak_env:\n            # The properties defined in the alignak.cfg file are not yet set! So we set the one\n            # got from the environment\n            logger.info(\"Getting Alignak macros...\")\n            alignak_macros = self.alignak_env.get_alignak_macros()\n            if alignak_macros:\n                # Remove the leading and trailing underscores\n                for key in sorted(alignak_macros.keys()):\n                    value = alignak_macros[key]\n                    if key[0] == '_' or key[0] == '$':\n                        key = key[1:]\n                    if key[-1] == '_' or key[-1] == '$':\n                        key = key[:-1]\n                    # Create an old legacy macro format\n                    macros.append('$%s$=%s' % (key.upper(), value))\n                    logger.debug(\"- Alignak macro '$%s$' = %s\", key.upper(), value)\n\n            # and then the global configuration.\n            # The properties defined in the alignak.cfg file are not yet set! So we set the one\n            # got from the appropriate section of the Alignak environment file\n            logger.info(\"Getting Alignak configuration...\")\n            alignak_configuration = self.alignak_env.get_alignak_configuration()\n            if alignak_configuration:\n                for key in sorted(alignak_configuration.keys()):\n                    value = alignak_configuration[key]\n                    if key.startswith('_'):\n                        # Ignore configuration variables prefixed with _\n                        continue\n                    if key in self.conf.properties:\n                        entry = self.conf.properties[key]\n                        setattr(self.conf, key, entry.pythonize(value))\n                    else:\n                        setattr(self.conf, key, value)\n                    logger.debug(\"- setting '%s' as %s\", key, getattr(self.conf, key))\n                logger.info(\"Got Alignak global configuration\")\n\n        self.alignak_name = getattr(self.conf, \"alignak_name\", self.name)\n        logger.info(\"Configuration for Alignak: %s\", self.alignak_name)\n\n        if macros:\n            self.conf.load_params(macros)\n\n        # Here we got the macros and alignak configuration variables from the\n        # alignak.ini configuration!\n        # The self Config object is now initialized with the global Alignak variables.\n\n        # We can now read and parse the legacy configuration files (if any...)\n        raw_objects = self.conf.read_config_buf(\n            self.conf.read_legacy_cfg_files(self.legacy_cfg_files,\n                                            self.alignak_env.cfg_files if self.alignak_env\n                                            else None)\n        )\n\n        if self.alignak_name != getattr(self.conf, \"alignak_name\", self.name):\n            self.alignak_name = getattr(self.conf, \"alignak_name\", self.name)\n            logger.warning(\"Alignak name changed from the legacy Cfg files: %s\", self.alignak_name)\n\n        # Maybe conf is already invalid\n        if not self.conf.conf_is_correct:\n            self.conf.show_errors()\n            self.request_stop(\"*** One or more problems were encountered while \"\n                              \"processing the configuration (first check)...\", exit_code=1)\n\n        if self.legacy_cfg_files:\n            logger.info(\"I correctly loaded the legacy configuration files\")\n\n        # Hacking some global parameters inherited from Nagios to create\n        # on the fly some Broker modules like for status.dat parameters\n        # or nagios.log one if there are none already available\n        if 'module' not in raw_objects:\n            raw_objects['module'] = []\n        extra_modules = self.conf.hack_old_nagios_parameters()\n        if extra_modules:\n            logger.info(\"Some inner modules were configured for Nagios legacy parameters\")\n            for _, module in extra_modules:\n                raw_objects['module'].append(module)\n        logger.debug(\"Extra modules: %s\", extra_modules)\n\n        # Alignak global environment file\n        # -------------------------------\n        # Here we got the monitored system configuration from the legacy configuration files\n        # We must overload this configuration for the daemons and modules with the configuration\n        # declared in the Alignak environment (alignak.ini) file!\n        if self.alignak_env:\n            # Update the daemons legacy configuration if not complete\n            for daemon_type in ['arbiter', 'scheduler', 'broker',\n                                'poller', 'reactionner', 'receiver']:\n                if daemon_type not in raw_objects:\n                    raw_objects[daemon_type] = []\n\n            # Get all the Alignak daemons from the configuration\n            logger.info(\"Getting daemons configuration...\")\n            some_daemons = False\n            for daemon_name, daemon_cfg in list(self.alignak_env.get_daemons().items()):\n                logger.info(\"Got a daemon configuration for %s\", daemon_name)\n                if 'type' not in daemon_cfg:\n                    self.conf.add_error(\"Ignoring daemon with an unknown type: %s\" % daemon_name)\n                    continue\n                some_daemons = True\n                daemon_type = daemon_cfg['type']\n                daemon_name = daemon_cfg['name']\n                logger.info(\"- got a %s named %s, spare: %s\",\n                            daemon_type, daemon_name, daemon_cfg.get('spare', False))\n\n                # If this daemon is found in the legacy configuration, replace this\n                new_cfg_daemons = []\n                for cfg_daemon in raw_objects[daemon_type]:\n                    if cfg_daemon.get('name', 'unset') == daemon_name \\\n                            or cfg_daemon.get(\"%s_name\" % daemon_type,\n                                              'unset') == [daemon_name]:\n                        logger.info(\"  updating daemon Cfg file configuration\")\n                    else:\n                        new_cfg_daemons.append(cfg_daemon)\n                new_cfg_daemons.append(daemon_cfg)\n                raw_objects[daemon_type] = new_cfg_daemons\n\n            logger.debug(\"Checking daemons configuration:\")\n            some_legacy_daemons = False\n            for daemon_type in ['arbiter', 'scheduler', 'broker',\n                                'poller', 'reactionner', 'receiver']:\n                for cfg_daemon in raw_objects[daemon_type]:\n                    some_legacy_daemons = True\n                    if 'name' not in cfg_daemon:\n                        cfg_daemon['name'] = cfg_daemon['%s_name' % daemon_type]\n\n                    cfg_daemon['modules'] = \\\n                        self.alignak_env.get_modules(daemon_name=cfg_daemon['name'])\n                    for module_daemon_type, module in extra_modules:\n                        if module_daemon_type == daemon_type:\n                            cfg_daemon['modules'].append(module['name'])\n                            logger.info(\"- added an Alignak inner module '%s' to the %s: %s\",\n                                        module['name'], daemon_type, cfg_daemon['name'])\n                    logger.debug(\"- %s / %s: \", daemon_type, cfg_daemon['name'])\n                    logger.debug(\"  %s\", cfg_daemon)\n            if not some_legacy_daemons:\n                logger.debug(\"- No legacy configured daemons.\")\n            else:\n                logger.info(\"- some dameons are configured in legacy Cfg files. \"\n                            \"You should update the configuration with the new Alignak \"\n                            \"configuration file.\")\n            if not some_daemons and not some_legacy_daemons:\n                logger.info(\"- No configured daemons.\")\n\n            # and then get all modules from the configuration\n            logger.info(\"Getting modules configuration...\")\n            if 'module' in raw_objects and raw_objects['module']:\n                # Manage the former parameters module_alias and module_types\n                # - replace with name and type\n                for module_cfg in raw_objects['module']:\n                    if 'module_alias' not in module_cfg and 'name' not in module_cfg:\n                        self.conf.add_error(\"Module declared without any 'name' or 'module_alias'\")\n                        continue\n                    else:\n                        if 'name' not in module_cfg:\n                            module_cfg['name'] = module_cfg['module_alias']\n                            module_cfg.pop('module_alias')\n\n                    if 'module_types' in module_cfg and 'type' not in module_cfg:\n                        module_cfg['type'] = module_cfg['module_types']\n                        module_cfg.pop('module_types')\n                    logger.debug(\"Module cfg %s params: %s\", module_cfg['name'], module_cfg)\n\n            for _, module_cfg in list(self.alignak_env.get_modules().items()):\n                logger.info(\"- got a module %s, type: %s\",\n                            module_cfg.get('name', 'unset'), module_cfg.get('type', 'untyped'))\n                # If this module is found in the former Cfg files, replace the former configuration\n                for cfg_module in raw_objects['module']:\n                    if cfg_module.get('name', 'unset') == [module_cfg['name']]:\n                        logger.info(\"  updating module Cfg file configuration\")\n                        cfg_module = module_cfg\n                        logger.info(\"Module %s updated parameters: %s\",\n                                    module_cfg['name'], module_cfg)\n                        break\n                else:\n                    raw_objects['module'].append(module_cfg)\n                    logger.debug(\"Module env %s params: %s\", module_cfg['name'], module_cfg)\n            if 'module' in raw_objects and not raw_objects['module']:\n                logger.info(\"- No configured modules.\")\n\n        # Create objects for our arbiters and modules\n        self.conf.early_create_objects(raw_objects)\n\n        # Check that an arbiter link exists and create the appropriate relations\n        # If no arbiter exists, create one with the provided data\n        params = {}\n        if self.alignak_env:\n            params = self.alignak_env.get_alignak_configuration()\n        self.conf.early_arbiter_linking(self.name, params)\n\n        # Search which arbiter I am in the arbiter links list\n        for lnk_arbiter in self.conf.arbiters:\n            logger.debug(\"I have an arbiter in my configuration: %s\", lnk_arbiter.name)\n            if lnk_arbiter.name != self.name:\n                # Arbiter is not me!\n                logger.info(\"I found another arbiter (%s) in my (%s) configuration\",\n                            lnk_arbiter.name, self.name)\n                # And this arbiter needs to receive a configuration\n                lnk_arbiter.need_conf = True\n                continue\n\n            logger.info(\"I found myself in the configuration: %s\", lnk_arbiter.name)\n            if self.link_to_myself is None:\n                # I update only if it does not yet exist (first configuration load)!\n                # I will not change myself because I am simply reloading a configuration ;)\n                self.link_to_myself = lnk_arbiter\n                self.link_to_myself.instance_id = self.name\n                self.link_to_myself.push_flavor = ''.encode('utf-8')\n                # self.link_to_myself.hash = self.conf.hash\n            # Set myself as alive ;)\n            self.link_to_myself.set_alive()\n\n            # We consider that this arbiter is a master one...\n            self.is_master = not self.link_to_myself.spare\n            if self.is_master:\n                logger.info(\"I am the master Arbiter.\")\n            else:\n                logger.info(\"I am a spare Arbiter.\")\n\n            # ... and that this arbiter do not need to receive a configuration\n            lnk_arbiter.need_conf = False\n\n        if not self.link_to_myself:\n            self.conf.show_errors()\n            self.request_stop(\"Error: I cannot find my own configuration (%s), I bail out. \"\n                              \"To solve this, please change the arbiter name parameter in \"\n                              \"the Alignak configuration file (certainly alignak.ini) \"\n                              \"with the value '%s'.\"\n                              \" Thanks.\" % (self.name, socket.gethostname()), exit_code=1)\n\n        # Whether I am a spare arbiter, I will parse the whole configuration. This may be useful\n        # if the master fails before sending its configuration to me!\n        # An Arbiter which is not a master one will not go further...\n        # todo: is it a good choice?:\n        # 1/ why reading all the configuration files stuff?\n        # 2/ why not loading configuration data from the modules?\n        # -> Indeed, here, only the main configuration has been fetched by the arbiter.\n        # Perharps, loading only the alignak.ini would be enough for a spare arbiter.\n        # And it will make it simpler to configure...\n        if not self.is_master:\n            logger.info(\"I am not the master arbiter, I stop parsing the configuration\")\n            self.loading_configuration = False\n            return\n\n        # We load our own modules\n        self.do_load_modules(self.link_to_myself.modules)\n\n        # Call modules that manage this read configuration pass\n        _ts = time.time()\n        self.hook_point('read_configuration')\n        statsmgr.timer('hook.read_configuration', time.time() - _ts)\n\n        # Call modules get_alignak_configuration() to load Alignak configuration parameters\n        # todo: re-enable this feature if it is really needed. It is a bit tricky to manage\n        # configuration from our own configuration file and from an external source :(\n        # (example modules: alignak_backend)\n        # _t0 = time.time()\n        # self.load_modules_alignak_configuration()\n        # statsmgr.timer('core.hook.get_alignak_configuration', time.time() - _t0)\n\n        # Call modules get_objects() to load new objects our own modules\n        # (example modules: alignak_backend)\n        self.load_modules_configuration_objects(raw_objects)\n\n        # Create objects for all the configuration\n        self.conf.create_objects(raw_objects)\n\n        # Maybe configuration is already invalid\n        if not self.conf.conf_is_correct:\n            self.conf.show_errors()\n            self.request_stop(\"*** One or more problems were encountered while processing \"\n                              \"the configuration (second check)...\", exit_code=1)\n\n        # Manage all post-conf modules\n        self.hook_point('early_configuration')\n\n        # Here we got all our Alignak configuration and the monitored system configuration\n        # from the legacy configuration files and extra modules.\n        logger.info(\"Preparing configuration...\")\n\n        # Create Template links\n        self.conf.linkify_templates()\n\n        # All inheritances\n        self.conf.apply_inheritance()\n\n        # Explode between types\n        self.conf.explode()\n\n        # Implicit inheritance for services\n        self.conf.apply_implicit_inheritance()\n\n        # Fill default values for all the configuration objects\n        self.conf.fill_default_configuration()\n\n        # Remove templates from config\n        self.conf.remove_templates()\n\n        # Overrides specific service instances properties\n        self.conf.override_properties()\n\n        # Linkify objects to each other\n        self.conf.linkify()\n\n        # applying dependencies\n        self.conf.apply_dependencies()\n\n        # Raise warning about currently unmanaged parameters\n        if self.verify_only:\n            self.conf.warn_about_unmanaged_parameters()\n\n        # Explode global configuration parameters into Classes\n        self.conf.explode_global_conf()\n\n        # set our own timezone and propagate it to other satellites\n        self.conf.propagate_timezone_option()\n\n        # Look for business rules, and create the dep tree\n        self.conf.create_business_rules()\n        # And link them\n        self.conf.create_business_rules_dependencies()\n\n        # Set my own parameters from the loaded configuration\n        # Last monitoring events\n        self.recent_events = deque(maxlen=int(os.environ.get('ALIGNAK_EVENTS_LOG_COUNT',\n                                                             self.conf.events_log_count)))\n\n        # Manage all post-conf modules\n        self.hook_point('late_configuration')\n\n        # Configuration is correct?\n        logger.info(\"Checking configuration...\")\n        self.conf.is_correct()\n\n        # Clean objects of temporary/unnecessary attributes for live work:\n        if clean:\n            logger.info(\"Cleaning configuration objects...\")\n            self.conf.clean()\n\n        # Dump Alignak macros\n        logger.debug(\"Alignak global macros:\")\n\n        macro_resolver = MacroResolver()\n        macro_resolver.init(self.conf)\n        for macro_name in sorted(self.conf.macros):\n            macro_value = macro_resolver.resolve_simple_macros_in_string(\"$%s$\" % macro_name, [],\n                                                                         None, None)\n            logger.debug(\"- $%s$ = %s\", macro_name, macro_value)\n        statsmgr.timer('configuration.loading', time.time() - _t_configuration)\n\n        # REF: doc/alignak-conf-dispatching.png (2)\n        logger.info(\"Splitting configuration...\")\n        self.conf.cut_into_parts()\n        # Here, the self.conf.parts exist\n        # And the realms have some 'packs'\n\n        # Check if all the configuration daemons will be available\n        if not self.daemons_start(run_daemons=False):\n            self.conf.show_errors()\n            self.request_stop(\"*** Alignak will not be able to manage the configured daemons. \"\n                              \"Check and update your configuration!\", exit_code=1)\n\n        # Some properties need to be prepared (somehow \"flatten\"...) before being sent,\n        # This to prepare the configuration that will be sent to our spare arbiter (if any)\n        self.conf.prepare_for_sending()\n        statsmgr.timer('configuration.spliting', time.time() - _t_configuration)\n        # Here, the self.conf.spare_arbiter_conf exist\n\n        # Still a last configuration check because some things may have changed when\n        # we cut the configuration into parts (eg. hosts and realms consistency) and\n        # when we prepared the configuration for sending\n        if not self.conf.conf_is_correct:  # pragma: no cover, not with unit tests.\n            self.conf.show_errors()\n            self.request_stop(\"Configuration is incorrect, sorry, I bail out\", exit_code=1)\n\n        logger.info(\"Things look okay - \"\n                    \"No serious problems were detected during the pre-flight check\")\n\n        # Exit if we are just here for config checking\n        if self.verify_only:\n            logger.info(\"Arbiter %s checked the configuration\", self.name)\n            if self.conf.missing_daemons:\n                logger.warning(\"Some missing daemons were detected in the parsed configuration. \"\n                               \"Nothing to worry about, but you should define them, \"\n                               \"else Alignak will use its default configuration.\")\n\n            # Display found warnings and errors\n            self.conf.show_errors()\n            self.request_stop()\n\n        del raw_objects\n\n        # Display found warnings and errors\n        self.conf.show_errors()\n\n        # Now I have a configuration!\n        self.have_conf = True\n        self.loading_configuration = False\n        statsmgr.timer('configuration.available', time.time() - _t_configuration)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_modules_configuration_objects(self, raw_objects):  # pragma: no cover,\n        # not yet with unit tests.\n        \"\"\"Load configuration objects from arbiter modules\n        If module implements get_objects arbiter will call it and add create\n        objects\n\n        :param raw_objects: raw objects we got from reading config files\n        :type raw_objects: dict\n        :return: None\n        \"\"\"\n        # Now we ask for configuration modules if they\n        # got items for us\n        for instance in self.modules_manager.instances:\n            logger.debug(\"Getting objects from the module: %s\", instance.name)\n            if not hasattr(instance, 'get_objects'):\n                logger.debug(\"The module '%s' do not provide any objects.\", instance.name)\n                return\n\n            try:\n                logger.info(\"Getting Alignak monitored configuration objects from module '%s'\",\n                            instance.name)\n                got_objects = instance.get_objects()\n            except Exception as exp:  # pylint: disable=broad-except\n                logger.exception(\"Module %s get_objects raised an exception %s. \"\n                                 \"Log and continue to run.\", instance.name, exp)\n                continue\n\n            if not got_objects:\n                logger.warning(\"The module '%s' did not provided any objects.\", instance.name)\n                return\n\n            types_creations = self.conf.types_creations\n            for o_type in types_creations:\n                (_, _, prop, _, _) = types_creations[o_type]\n                if prop in ['arbiters', 'brokers', 'schedulers',\n                            'pollers', 'reactionners', 'receivers', 'modules']:\n                    continue\n                if prop not in got_objects:\n                    logger.warning(\"Did not get any '%s' objects from %s\", prop, instance.name)\n                    continue\n                for obj in got_objects[prop]:\n                    # test if raw_objects[k] are already set - if not, add empty array\n                    if o_type not in raw_objects:\n                        raw_objects[o_type] = []\n                    # Update the imported_from property if the module did not set\n                    if 'imported_from' not in obj:\n                        obj['imported_from'] = 'module:%s' % instance.name\n                    # Append to the raw objects\n                    raw_objects[o_type].append(obj)\n                logger.debug(\"Added %i %s objects from %s\",\n                             len(got_objects[prop]), o_type, instance.name)", "response": "Load configuration objects from arbiter modules and add them to raw_objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_modules_alignak_configuration(self):  # pragma: no cover, not yet with unit tests.\n        alignak_cfg = {}\n        # Ask configured modules if they got configuration for us\n        for instance in self.modules_manager.instances:\n            if not hasattr(instance, 'get_alignak_configuration'):\n                return\n\n            try:\n                logger.info(\"Getting Alignak global configuration from module '%s'\", instance.name)\n                cfg = instance.get_alignak_configuration()\n                alignak_cfg.update(cfg)\n            except Exception as exp:  # pylint: disable=broad-except\n                logger.error(\"Module %s get_alignak_configuration raised an exception %s. \"\n                             \"Log and continue to run\", instance.name, str(exp))\n                output = io.StringIO()\n                traceback.print_exc(file=output)\n                logger.error(\"Back trace of this remove: %s\", output.getvalue())\n                output.close()\n                continue\n\n        params = []\n        if alignak_cfg:\n            logger.info(\"Got Alignak global configuration:\")\n            for key, value in sorted(alignak_cfg.items()):\n                logger.info(\"- %s = %s\", key, value)\n                # properties starting with an _ character are \"transformed\" to macro variables\n                if key.startswith('_'):\n                    key = '$' + key[1:].upper() + '$'\n                # properties valued as None are filtered\n                if value is None:\n                    continue\n                # properties valued as None string are filtered\n                if value == 'None':\n                    continue\n                # properties valued as empty strings are filtered\n                if value == '':\n                    continue\n                # set properties as legacy Shinken configuration files\n                params.append(\"%s=%s\" % (key, value))\n            self.conf.load_params(params)", "response": "Load Alignak configuration from the arbiter modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request_stop(self, message='', exit_code=0):\n        # Only a master arbiter can stop the daemons\n        if self.is_master:\n            # Stop the daemons\n            self.daemons_stop(timeout=self.conf.daemons_stop_timeout)\n\n        # Request the daemon stop\n        super(Arbiter, self).request_stop(message, exit_code)", "response": "Stop the Arbiter daemons and daemons_stop_timeout seconds"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmanages the list of detected missing daemons and start the daemon", "response": "def start_daemon(self, satellite):\n        \"\"\"Manage the list of detected missing daemons\n\n         If the daemon does not in exist `my_daemons`, then:\n          - prepare daemon start arguments (port, name and log file)\n          - start the daemon\n          - make sure it started correctly\n\n        :param satellite: the satellite for which a daemon is to be started\n        :type satellite: SatelliteLink\n\n        :return: True if the daemon started correctly\n        \"\"\"\n        logger.info(\"  launching a daemon for: %s/%s...\", satellite.type, satellite.name)\n\n        # The daemon startup script location may be defined in the configuration\n        daemon_script_location = getattr(self.conf, 'daemons_script_location', self.bindir)\n        if not daemon_script_location:\n            daemon_script_location = \"alignak-%s\" % satellite.type\n        else:\n            daemon_script_location = \"%s/alignak-%s\" % (daemon_script_location, satellite.type)\n\n        # Some extra arguments may be defined in the Alignak configuration\n        daemon_arguments = getattr(self.conf, 'daemons_arguments', '')\n\n        args = [daemon_script_location,\n                \"--name\", satellite.name,\n                \"--environment\", self.env_filename,\n                \"--host\", str(satellite.host),\n                \"--port\", str(satellite.port)]\n        if daemon_arguments:\n            args.append(daemon_arguments)\n        logger.info(\"  ... with some arguments: %s\", args)\n        try:\n            process = psutil.Popen(args, stdin=None, stdout=None, stderr=None)\n            # A brief pause...\n            time.sleep(0.1)\n        except Exception as exp:  # pylint: disable=broad-except\n            logger.error(\"Error when launching %s: %s\", satellite.name, exp)\n            logger.error(\"Command: %s\", args)\n            return False\n\n        logger.info(\"  %s launched (pid=%d, gids=%s)\",\n                    satellite.name, process.pid, process.gids())\n\n        # My satellites/daemons map\n        self.my_daemons[satellite.name] = {\n            'satellite': satellite,\n            'process': process\n        }\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if all the daemons are running and if so starts the daemon.", "response": "def daemons_start(self, run_daemons=True):\n        \"\"\"Manage the list of the daemons in the configuration\n\n        Check if the daemon needs to be started by the Arbiter.\n\n        If so, starts the daemon if `run_daemons` is True\n\n        :param run_daemons: run the daemons or make a simple check\n        :type run_daemons: bool\n\n        :return: True if all daemons are running, else False. always True for a simple check\n        \"\"\"\n        result = True\n\n        if run_daemons:\n            logger.info(\"Alignak configured daemons start:\")\n        else:\n            logger.info(\"Alignak configured daemons check:\")\n\n        # Parse the list of the missing daemons and try to run the corresponding processes\n        for satellites_list in [self.conf.arbiters, self.conf.receivers, self.conf.reactionners,\n                                self.conf.pollers, self.conf.brokers, self.conf.schedulers]:\n            for satellite in satellites_list:\n                logger.info(\"- found %s, to be launched: %s, address: %s\",\n                            satellite.name, satellite.alignak_launched, satellite.uri)\n\n                if satellite == self.link_to_myself:\n                    # Ignore myself ;)\n                    continue\n\n                if satellite.alignak_launched and \\\n                        satellite.address not in ['127.0.0.1', 'localhost']:\n                    logger.error(\"Alignak is required to launch a daemon for %s %s \"\n                                 \"but the satelitte is defined on an external address: %s\",\n                                 satellite.type, satellite.name, satellite.address)\n                    result = False\n                    continue\n\n                if not run_daemons:\n                    # When checking, ignore the daemon launch part...\n                    continue\n\n                if not satellite.alignak_launched:\n                    logger.debug(\"Alignak will not launch '%s'\")\n                    continue\n\n                if not satellite.active:\n                    logger.warning(\"- daemon '%s' is declared but not set as active, \"\n                                   \"do not start...\", satellite.name)\n                    continue\n\n                if satellite.name in self.my_daemons:\n                    logger.warning(\"- daemon '%s' is already running\", satellite.name)\n                    continue\n\n                started = self.start_daemon(satellite)\n                result = result and started\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef daemons_check(self):\n        # First look if it's not too early to ping\n        start = time.time()\n        if self.daemons_last_check \\\n                and self.daemons_last_check + self.conf.daemons_check_period > start:\n            logger.debug(\"Too early to check daemons, check period is %.2f seconds\",\n                         self.conf.daemons_check_period)\n            return True\n\n        logger.debug(\"Alignak launched daemons check\")\n        result = True\n\n        procs = [psutil.Process()]\n        for daemon in list(self.my_daemons.values()):\n            # Get only the daemon (not useful for its children processes...)\n            # procs = daemon['process'].children()\n            procs.append(daemon['process'])\n            for proc in procs:\n                try:\n                    logger.debug(\"Process %s is %s\", proc.name(), proc.status())\n                    # logger.debug(\"Process listening:\", proc.name(), proc.status())\n                    # for connection in proc.connections():\n                    #     l_addr, l_port = connection.laddr if connection.laddr else ('', 0)\n                    #     r_addr, r_port = connection.raddr if connection.raddr else ('', 0)\n                    #     logger.debug(\"- %s:%s <-> %s:%s, %s\", l_addr, l_port, r_addr, r_port,\n                    #                  connection.status)\n                    # Reset the daemon connection if it got broked...\n                    if not daemon['satellite'].con:\n                        if self.daemon_connection_init(daemon['satellite']):\n                            # Set my satellite as alive :)\n                            daemon['satellite'].set_alive()\n                except psutil.NoSuchProcess:\n                    pass\n                except psutil.AccessDenied:\n                    # Probably stopping...\n                    if not self.will_stop and proc == daemon['process']:\n                        logger.warning(\"Daemon %s/%s is not running!\",\n                                       daemon['satellite'].type, daemon['satellite'].name)\n                        logger.debug(\"Access denied - Process %s is %s\", proc.name(), proc.status())\n                        if not self.start_daemon(daemon['satellite']):\n                            # Set my satellite as dead :(\n                            daemon['satellite'].set_dead()\n                            result = False\n                        else:\n                            logger.info(\"I restarted %s/%s\",\n                                        daemon['satellite'].type, daemon['satellite'].name)\n                            logger.info(\"Pausing %.2f seconds...\", 0.5)\n                            time.sleep(0.5)\n                    else:\n                        logger.info(\"Child process %s is %s\", proc.name(), proc.status())\n\n        # Set the last check as now\n        self.daemons_last_check = start\n\n        logger.debug(\"Checking daemons duration: %.2f seconds\", time.time() - start)\n\n        return result", "response": "Manage the list of Alignak launched daemons and check if all daemons are running."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef daemons_stop(self, timeout=30, kill_children=False):\n        def on_terminate(proc):\n            \"\"\"Process termination callback function\"\"\"\n            logger.debug(\"process %s terminated with exit code %s\", proc.pid, proc.returncode)\n\n        result = True\n\n        if self.my_daemons:\n            logger.info(\"Alignak self-launched daemons stop:\")\n\n            start = time.time()\n            for daemon in list(self.my_daemons.values()):\n                # Terminate the daemon and its children process\n                procs = []\n                if kill_children:\n                    procs = daemon['process'].children()\n                procs.append(daemon['process'])\n                for process in procs:\n                    try:\n                        logger.info(\"- terminating process %s\", process.name())\n                        process.terminate()\n                    except psutil.AccessDenied:\n                        logger.warning(\"Process %s is %s\", process.name(), process.status())\n\n            procs = []\n            for daemon in list(self.my_daemons.values()):\n                # Stop the daemon and its children process\n                if kill_children:\n                    procs = daemon['process'].children()\n                procs.append(daemon['process'])\n            _, alive = psutil.wait_procs(procs, timeout=timeout, callback=on_terminate)\n            if alive:\n                # Kill processes\n                for process in alive:\n                    logger.warning(\"Process %s did not stopped, trying to kill\", process.name())\n                    process.kill()\n                _, alive = psutil.wait_procs(alive, timeout=timeout, callback=on_terminate)\n                if alive:\n                    # give up\n                    for process in alive:\n                        logger.warning(\"process %s survived SIGKILL; giving up\", process.name())\n                        result = False\n\n            logger.debug(\"Stopping daemons duration: %.2f seconds\", time.time() - start)\n\n        return result", "response": "Stop the Alignak daemons\n\n         Iterate over the self-launched daemons and their children list to send a TERM\n         Wait for daemons to terminate and then send a KILL for those that are not yet stopped\n\n         As a default behavior, only the launched daemons are killed, not their children.\n         Each daemon will manage its children killing\n\n        :param timeout: delay to wait before killing a daemon\n        :type timeout: int\n\n        :param kill_children: also kill the children (defaults to False)\n        :type kill_children: bool\n\n        :return: True if all daemons stopped"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef daemons_reachability_check(self):\n        # First look if it's not too early to ping\n        start = time.time()\n        if self.daemons_last_reachable_check and \\\n                self.daemons_last_reachable_check + self.conf.daemons_check_period > start:\n            logger.debug(\"Too early to check daemons reachability, check period is %.2f seconds\",\n                         self.conf.daemons_check_period)\n            return True\n\n        _t0 = time.time()\n        logger.debug(\"Alignak daemons reachability check\")\n        result = self.dispatcher.check_reachable()\n        statsmgr.timer('dispatcher.check-alive', time.time() - _t0)\n\n        _t0 = time.time()\n        logger.debug(\"Alignak daemons status get\")\n        events = self.dispatcher.check_status_and_get_events()\n        duration = time.time() - _t0\n        statsmgr.timer('dispatcher.check-status', duration)\n        logger.debug(\"Getting daemons status duration: %.2f seconds\", duration)\n\n        # Send the collected events to the Alignak logger\n        for event in events:\n            event.prepare()\n            make_monitoring_log(event.data['level'], event.data['message'],\n                                timestamp=event.creation_time, to_logger=True)\n\n            # Add to the recent events for the WS endpoint\n            event.data['timestamp'] = event.creation_time\n            event.data['date'] = datetime.fromtimestamp(event.creation_time).\\\n                strftime(self.conf.events_date_format)\n            event.data.pop('instance_id')\n            self.recent_events.append(event.data)\n\n        # Set the last check as now\n        self.daemons_last_reachable_check = start\n\n        logger.debug(\"Checking daemons reachability duration: %.2f seconds\", time.time() - start)\n\n        return result", "response": "Manage the list of Alignak launched daemons\n            and return True if all daemons are running False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_new_conf(self):\n        # pylint: disable=too-many-locals\n        \"\"\" Setup a new configuration received from a Master arbiter.\n\n        TODO: perharps we should not accept the configuration or raise an error if we do not\n        find our own configuration data in the data. Thus this should never happen...\n        :return: None\n        \"\"\"\n        # Execute the base class treatment...\n        super(Arbiter, self).setup_new_conf()\n\n        with self.conf_lock:\n            logger.info(\"I received a new configuration from my master\")\n\n            # Get the new configuration\n            self.cur_conf = self.new_conf\n            # self_conf is our own configuration from the alignak environment\n            # Arbiters do not have this property in the received configuration because\n            # they already loaded a configuration on daemon load\n            self_conf = self.cur_conf.get('self_conf', None)\n            if not self_conf:\n                self_conf = self.conf\n\n            # whole_conf contains the full configuration load by my master\n            whole_conf = self.cur_conf['whole_conf']\n\n            logger.debug(\"Received a new configuration, containing:\")\n            for key in self.cur_conf:\n                logger.debug(\"- %s: %s\", key, self.cur_conf[key])\n            logger.debug(\"satellite self configuration part: %s\", self_conf)\n\n            # Update Alignak name\n            self.alignak_name = self.cur_conf['alignak_name']\n            logger.info(\"My Alignak instance: %s\", self.alignak_name)\n\n            # This to indicate that the new configuration got managed...\n            self.new_conf = {}\n\n            # Get the whole monitored objects configuration\n            t00 = time.time()\n            try:\n                received_conf_part = unserialize(whole_conf)\n            except AlignakClassLookupException as exp:  # pragma: no cover, simple protection\n                # This to indicate that the new configuration is not managed...\n                self.new_conf = {\n                    \"_status\": \"Cannot un-serialize configuration received from arbiter\"\n                }\n                logger.error(self.new_conf['_status'])\n                logger.error(\"Back trace of the error:\\n%s\", traceback.format_exc())\n                return\n            except Exception as exp:  # pylint: disable=broad-except\n                # This to indicate that the new configuration is not managed...\n                self.new_conf = {\n                    \"_status\": \"Cannot un-serialize configuration received from arbiter\"\n                }\n                logger.error(self.new_conf['_status'])\n                logger.error(self.new_conf)\n                self.exit_on_exception(exp, self.new_conf)\n            logger.info(\"Monitored configuration %s received at %d. Un-serialized in %d secs\",\n                        received_conf_part, t00, time.time() - t00)\n\n            # Now we create our arbiters and schedulers links\n            my_satellites = getattr(self, 'arbiters', {})\n            received_satellites = self.cur_conf['arbiters']\n            for link_uuid in received_satellites:\n                rs_conf = received_satellites[link_uuid]\n                logger.debug(\"- received %s - %s: %s\", rs_conf['instance_id'],\n                             rs_conf['type'], rs_conf['name'])\n\n                # Must look if we already had a configuration and save our broks\n                already_got = rs_conf['instance_id'] in my_satellites\n                broks = []\n                actions = {}\n                wait_homerun = {}\n                external_commands = {}\n                running_id = 0\n                if already_got:\n                    logger.warning(\"I already got: %s\", rs_conf['instance_id'])\n                    # Save some information\n                    running_id = my_satellites[link_uuid].running_id\n                    (broks, actions,\n                     wait_homerun, external_commands) = \\\n                        my_satellites[link_uuid].get_and_clear_context()\n                    # Delete the former link\n                    del my_satellites[link_uuid]\n\n                # My new satellite link...\n                new_link = SatelliteLink.get_a_satellite_link('arbiter', rs_conf)\n                my_satellites[new_link.uuid] = new_link\n                logger.info(\"I got a new arbiter satellite: %s\", new_link)\n\n                new_link.running_id = running_id\n                new_link.external_commands = external_commands\n                new_link.broks = broks\n                new_link.wait_homerun = wait_homerun\n                new_link.actions = actions\n\n                # # replacing satellite address and port by those defined in satellite_map\n                # if new_link.name in self_conf.satellite_map:\n                #     overriding = self_conf.satellite_map[new_link.name]\n                #     # satellite = dict(satellite)  # make a copy\n                #     # new_link.update(self_conf.get('satellite_map', {})[new_link.name])\n                #     logger.warning(\"Do not override the configuration for: %s, with: %s. \"\n                #                    \"Please check whether this is necessary!\",\n                #                    new_link.name, overriding)\n\n            # for arbiter_link in received_conf_part.arbiters:\n            #     logger.info(\"I have arbiter links in my configuration: %s\", arbiter_link.name)\n            #     if arbiter_link.name != self.name and not arbiter_link.spare:\n            #         # Arbiter is not me!\n            #         logger.info(\"I found my master arbiter in the configuration: %s\",\n            #                     arbiter_link.name)\n            #         continue\n            #\n            #     logger.info(\"I found myself in the received configuration: %s\", arbiter_link.name)\n            #     self.link_to_myself = arbiter_link\n            #     # We received a configuration s we are not a master !\n            #     self.is_master = False\n            #     self.link_to_myself.spare = True\n            #     # Set myself as alive ;)\n            #     self.link_to_myself.set_alive()\n\n        # Now I have a configuration!\n        self.have_conf = True", "response": "Setup a new configuration from a Master arbiter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwait for a master death and take the lead if necessary.", "response": "def wait_for_master_death(self):\n        \"\"\"Wait for a master timeout and take the lead if necessary\n\n        :return: None\n        \"\"\"\n        logger.info(\"Waiting for master death\")\n        timeout = 1.0\n        self.last_master_ping = time.time()\n\n        master_timeout = 300\n        for arbiter_link in self.conf.arbiters:\n            if not arbiter_link.spare:\n                master_timeout = \\\n                    arbiter_link.spare_check_interval * arbiter_link.spare_max_check_attempts\n        logger.info(\"I'll wait master death for %d seconds\", master_timeout)\n\n        while not self.interrupted:\n            # Make a pause and check if the system time changed\n            _, tcdiff = self.make_a_pause(timeout)\n            # If there was a system time change then we have to adapt last_master_ping:\n            if tcdiff:\n                self.last_master_ping += tcdiff\n\n            if self.new_conf:\n                self.setup_new_conf()\n\n            sys.stdout.write(\".\")\n            sys.stdout.flush()\n\n            # Now check if master is dead or not\n            now = time.time()\n            if now - self.last_master_ping > master_timeout:\n                logger.info(\"Arbiter Master is dead. The arbiter %s takes the lead!\",\n                            self.link_to_myself.name)\n                for arbiter_link in self.conf.arbiters:\n                    if not arbiter_link.spare:\n                        arbiter_link.alive = False\n                self.must_run = True\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nraise log for timeperiod change", "response": "def check_and_log_tp_activation_change(self):\n        \"\"\"Raise log for timeperiod change (useful for debug)\n\n        :return: None\n        \"\"\"\n        for timeperiod in self.conf.timeperiods:\n            brok = timeperiod.check_and_log_activation_change()\n            if brok:\n                self.add(brok)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef manage_signal(self, sig, frame):\n        # Request the arbiter to stop\n        if sig in [signal.SIGINT, signal.SIGTERM]:\n            logger.info(\"received a signal: %s\", SIGNALS_TO_NAMES_DICT[sig])\n            self.kill_request = True\n            self.kill_timestamp = time.time()\n            logger.info(\"request to stop in progress\")\n        else:\n            Daemon.manage_signal(self, sig, frame)", "response": "This method is called by the process when it receives a sigkill or sigterm."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef configuration_dispatch(self, not_configured=None):\n        if not not_configured:\n            self.dispatcher = Dispatcher(self.conf, self.link_to_myself)\n            # I set my own dispatched configuration as the provided one...\n            # because I will not push a configuration to myself :)\n            self.cur_conf = self.conf\n\n            # Loop for the first configuration dispatching, if the first dispatch fails, bail out!\n            # Without a correct configuration, Alignak daemons will not run correctly\n            first_connection_try_count = 0\n            logger.info(\"Connecting to my satellites...\")\n            while True:\n                first_connection_try_count += 1\n\n                # Initialize connection with all our satellites\n                self.all_connected = True\n                for satellite in self.dispatcher.all_daemons_links:\n                    if satellite == self.link_to_myself:\n                        continue\n                    if not satellite.active:\n                        continue\n                    connected = self.daemon_connection_init(satellite, set_wait_new_conf=True)\n                    logger.debug(\"  %s is %s\", satellite, connected)\n                    self.all_connected = self.all_connected and connected\n\n                if self.all_connected:\n                    logger.info(\"- satellites connection #%s is ok\", first_connection_try_count)\n                    break\n                else:\n                    logger.warning(\"- satellites connection #%s is not correct; \"\n                                   \"let's give another chance after %d seconds...\",\n                                   first_connection_try_count,\n                                   self.link_to_myself.polling_interval)\n                    if first_connection_try_count >= 3:\n                        self.request_stop(\"All the daemons connections could not be established \"\n                                          \"despite %d tries! \"\n                                          \"Sorry, I bail out!\" % first_connection_try_count,\n                                          exit_code=4)\n                    time.sleep(self.link_to_myself.polling_interval)\n\n            # Now I have a connection with all the daemons I need to contact them,\n            # check they are alive and ready to run\n            _t0 = time.time()\n            self.all_connected = self.dispatcher.check_reachable()\n            statsmgr.timer('dispatcher.check-alive', time.time() - _t0)\n\n            _t0 = time.time()\n            # Preparing the configuration for dispatching\n            logger.info(\"Preparing the configuration for dispatching...\")\n            self.dispatcher.prepare_dispatch()\n            statsmgr.timer('dispatcher.prepare-dispatch', time.time() - _t0)\n            logger.info(\"- configuration is ready to dispatch\")\n\n        # Loop for the first configuration dispatching, if the first dispatch fails, bail out!\n        # Without a correct configuration, Alignak daemons will not run correctly\n        first_dispatch_try_count = 0\n        logger.info(\"Dispatching the configuration to my satellites...\")\n        while True:\n            first_dispatch_try_count += 1\n\n            # Check reachable - if a configuration is prepared, this will force the\n            # daemons communication, and the dispatching will be launched\n            _t0 = time.time()\n            logger.info(\"- configuration dispatching #%s...\", first_dispatch_try_count)\n            self.dispatcher.check_reachable(forced=True)\n            statsmgr.timer('dispatcher.dispatch', time.time() - _t0)\n\n            # Make a pause to let our satellites get ready...\n            pause = max(1, max(self.conf.daemons_dispatch_timeout, len(self.my_daemons) * 0.5))\n            # pause = len(self.my_daemons) * 0.2\n            logger.info(\"- pausing %d seconds...\", pause)\n            time.sleep(pause)\n\n            _t0 = time.time()\n            logger.info(\"- checking configuration dispatch...\")\n            # Checking the dispatch is accepted\n            self.dispatcher.check_dispatch()\n            statsmgr.timer('dispatcher.check-dispatch', time.time() - _t0)\n            if self.dispatcher.dispatch_ok:\n                logger.info(\"- configuration dispatching #%s is ok\", first_dispatch_try_count)\n                break\n            else:\n                logger.warning(\"- configuration dispatching #%s is not correct; \"\n                               \"let's give another chance...\", first_dispatch_try_count)\n                if first_dispatch_try_count >= 3:\n                    self.request_stop(\"The configuration could not be dispatched despite %d tries! \"\n                                      \"Sorry, I bail out!\" % first_connection_try_count,\n                                      exit_code=4)", "response": "Monitored configuration preparation and dispatching."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls before the main loop.", "response": "def do_before_loop(self):\n        \"\"\"Called before the main daemon loop.\n\n        :return: None\n        \"\"\"\n        logger.info(\"I am the arbiter: %s\", self.link_to_myself.name)\n\n        # If I am a spare, I do not have anything to do here...\n        if not self.is_master:\n            logger.debug(\"Waiting for my master death...\")\n            return\n\n        # Arbiter check if some daemons need to be started\n        if not self.daemons_start(run_daemons=True):\n            self.request_stop(message=\"Some Alignak daemons did not started correctly.\",\n                              exit_code=4)\n\n        if not self.daemons_check():\n            self.request_stop(message=\"Some Alignak daemons cannot be checked.\",\n                              exit_code=4)\n\n        # Make a pause to let our started daemons get ready...\n        pause = max(1, max(self.conf.daemons_start_timeout, len(self.my_daemons) * 0.5))\n        if pause:\n            logger.info(\"Pausing %.2f seconds...\", pause)\n            time.sleep(pause)\n\n        # Prepare and dispatch the monitored configuration\n        self.configuration_dispatch()\n\n        # Now we can get all initial broks for our satellites\n        _t0 = time.time()\n        self.get_initial_broks_from_satellites()\n        statsmgr.timer('broks.get-initial', time.time() - _t0)\n\n        # Now create the external commands manager\n        # We are a dispatcher: our role is to dispatch commands to the schedulers\n        self.external_commands_manager = ExternalCommandManager(\n            self.conf, 'dispatcher', self, self.conf.accept_passive_unknown_check_results,\n            self.conf.log_external_commands)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_loop_turn(self):\n        # pylint: disable=too-many-branches, too-many-statements, too-many-locals\n        \"\"\"Loop turn for Arbiter\n\n        If not a master daemon, wait for my master death...\n        Else, run:\n        * Check satellites are alive\n        * Check and dispatch (if needed) the configuration\n        * Get broks and external commands from the satellites\n        * Push broks and external commands to the satellites\n\n        :return: None\n        \"\"\"\n        # If I am a spare, I only wait for the master arbiter to die...\n        if not self.is_master:\n            logger.debug(\"Waiting for my master death...\")\n            self.wait_for_master_death()\n            return\n\n        if self.loop_count % self.alignak_monitor_period == 1:\n            self.get_alignak_status(details=True)\n\n        # Maybe an external process requested Alignak stop...\n        if self.kill_request:\n            logger.info(\"daemon stop mode ...\")\n            if not self.dispatcher.stop_request_sent:\n                logger.info(\"entering daemon stop mode, time before exiting: %s\",\n                            self.conf.daemons_stop_timeout)\n                self.dispatcher.stop_request()\n            if time.time() > self.kill_timestamp + self.conf.daemons_stop_timeout:\n                logger.info(\"daemon stop mode delay reached, immediate stop\")\n                self.dispatcher.stop_request(stop_now=True)\n                time.sleep(1)\n                self.interrupted = True\n                logger.info(\"exiting...\")\n\n        if not self.kill_request:\n            # Main loop treatment\n            # Try to see if one of my module is dead, and restart previously dead modules\n            self.check_and_del_zombie_modules()\n\n            # Call modules that manage a starting tick pass\n            _t0 = time.time()\n            self.hook_point('tick')\n            statsmgr.timer('hook.tick', time.time() - _t0)\n\n            # Look for logging timeperiods activation change (active/inactive)\n            self.check_and_log_tp_activation_change()\n\n            # Check that my daemons are alive\n            if not self.daemons_check():\n                if self.conf.daemons_failure_kill:\n                    self.request_stop(message=\"Some Alignak daemons cannot be checked.\",\n                                      exit_code=4)\n                else:\n                    logger.warning(\"Should have killed my children if \"\n                                   \"'daemons_failure_kill' were set!\")\n\n            # Now the dispatcher job - check if all daemons are reachable and have a configuration\n            if not self.daemons_reachability_check():\n                logger.warning(\"A new configuration dispatch is required!\")\n\n                # Prepare and dispatch the monitored configuration\n                self.configuration_dispatch(self.dispatcher.not_configured)\n\n            # Now get things from our module instances\n            _t0 = time.time()\n            self.get_objects_from_from_queues()\n            statsmgr.timer('get-objects-from-queues', time.time() - _t0)\n\n            # Maybe our satellites raised new broks. Reap them...\n            _t0 = time.time()\n            self.get_broks_from_satellites()\n            statsmgr.timer('broks.got.time', time.time() - _t0)\n\n            # One broker is responsible for our broks, we give him our broks\n            _t0 = time.time()\n            self.push_broks_to_broker()\n            statsmgr.timer('broks.pushed.time', time.time() - _t0)\n\n            # # We push our external commands to our schedulers...\n            # _t0 = time.time()\n            # self.push_external_commands_to_schedulers()\n            # statsmgr.timer('external-commands.pushed.time', time.time() - _t0)\n\n        if self.system_health and (self.loop_count % self.system_health_period == 1):\n            perfdatas = []\n            cpu_count = psutil.cpu_count()\n            perfdatas.append(\"'cpu_count'=%d\" % cpu_count)\n            logger.debug(\"  . cpu count: %d\", cpu_count)\n\n            cpu_percents = psutil.cpu_percent(percpu=True)\n            cpu = 1\n            for percent in cpu_percents:\n                perfdatas.append(\"'cpu_%d_percent'=%.2f%%\" % (cpu, percent))\n                cpu += 1\n\n            cpu_times_percent = psutil.cpu_times_percent(percpu=True)\n            cpu = 1\n            for cpu_times_percent in cpu_times_percent:\n                logger.debug(\"  . cpu time percent: %s\", cpu_times_percent)\n                for key in cpu_times_percent._fields:\n                    perfdatas.append(\n                        \"'cpu_%d_%s_percent'=%.2f%%\" % (cpu, key,\n                                                        getattr(cpu_times_percent, key)))\n                cpu += 1\n\n            logger.info(\"%s cpu|%s\", self.name, \" \".join(perfdatas))\n\n            perfdatas = []\n            disk_partitions = psutil.disk_partitions(all=False)\n            for disk_partition in disk_partitions:\n                logger.debug(\"  . disk partition: %s\", disk_partition)\n\n                disk = getattr(disk_partition, 'mountpoint')\n                disk_usage = psutil.disk_usage(disk)\n                logger.debug(\"  . disk usage: %s\", disk_usage)\n                for key in disk_usage._fields:\n                    if 'percent' in key:\n                        perfdatas.append(\"'disk_%s_percent_used'=%.2f%%\"\n                                         % (disk, getattr(disk_usage, key)))\n                    else:\n                        perfdatas.append(\"'disk_%s_%s'=%dB\"\n                                         % (disk, key, getattr(disk_usage, key)))\n\n            logger.info(\"%s disks|%s\", self.name, \" \".join(perfdatas))\n\n            perfdatas = []\n            virtual_memory = psutil.virtual_memory()\n            logger.debug(\"  . memory: %s\", virtual_memory)\n            for key in virtual_memory._fields:\n                if 'percent' in key:\n                    perfdatas.append(\"'mem_percent_used_%s'=%.2f%%\"\n                                     % (key, getattr(virtual_memory, key)))\n                else:\n                    perfdatas.append(\"'mem_%s'=%dB\"\n                                     % (key, getattr(virtual_memory, key)))\n\n            swap_memory = psutil.swap_memory()\n            logger.debug(\"  . memory: %s\", swap_memory)\n            for key in swap_memory._fields:\n                if 'percent' in key:\n                    perfdatas.append(\"'swap_used_%s'=%.2f%%\"\n                                     % (key, getattr(swap_memory, key)))\n                else:\n                    perfdatas.append(\"'swap_%s'=%dB\"\n                                     % (key, getattr(swap_memory, key)))\n\n            logger.info(\"%s memory|%s\", self.name, \" \".join(perfdatas))", "response": "Main loop turn for Arbiter base class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_daemon_stats(self, details=False):  # pylint: disable=too-many-branches\n        now = int(time.time())\n        # Call the base Daemon one\n        res = super(Arbiter, self).get_daemon_stats(details=details)\n\n        res.update({\n            'name': self.link_to_myself.get_name() if self.link_to_myself else self.name,\n            'type': self.type,\n            'daemons_states': {}\n        })\n\n        if details:\n            res['monitoring_objects'] = {}\n\n            for _, _, strclss, _, _ in list(self.conf.types_creations.values()):\n                if strclss in ['hostescalations', 'serviceescalations']:\n                    logger.debug(\"Ignoring count for '%s'...\", strclss)\n                    continue\n\n                objects_list = getattr(self.conf, strclss, [])\n                res['monitoring_objects'][strclss] = {\n                    'count': len(objects_list)\n                }\n                res['monitoring_objects'][strclss].update({'items': []})\n\n                try:\n                    dump_list = sorted(objects_list, key=lambda k: k.get_name())\n                except AttributeError:  # pragma: no cover, simple protection\n                    dump_list = objects_list\n\n                # Dump at DEBUG level because some tests break with INFO level, and it is not\n                # really necessary to have information about each object ;\n                for cur_obj in dump_list:\n                    if strclss == 'services':\n                        res['monitoring_objects'][strclss]['items'].append(cur_obj.get_full_name())\n                    else:\n                        res['monitoring_objects'][strclss]['items'].append(cur_obj.get_name())\n\n        # Arbiter counters, including the loaded configuration objects and the dispatcher data\n        counters = res['counters']\n        counters['external-commands'] = len(self.external_commands)\n        counters['broks'] = len(self.broks)\n        for _, _, strclss, _, _ in list(self.conf.types_creations.values()):\n            if strclss in ['hostescalations', 'serviceescalations']:\n                logger.debug(\"Ignoring count for '%s'...\", strclss)\n                continue\n\n            objects_list = getattr(self.conf, strclss, [])\n            counters[strclss] = len(objects_list)\n\n        # Configuration dispatch counters\n        if getattr(self, \"dispatcher\", None):\n            for sat_type in ('arbiters', 'schedulers', 'reactionners',\n                             'brokers', 'receivers', 'pollers'):\n                counters[\"dispatcher.%s\" % sat_type] = len(getattr(self.dispatcher, sat_type))\n\n        # Report our daemons states, but only if a dispatcher exists\n        if getattr(self, 'dispatcher', None):\n            # Daemon properties that we are interested in\n            res['daemons_states'] = {}\n            for satellite in self.dispatcher.all_daemons_links:\n                if satellite == self.link_to_myself:\n                    continue\n                # Get the information to be published for a satellite\n                res['daemons_states'][satellite.name] = satellite.give_satellite_json()\n\n            res['livestate'] = {\n                \"timestamp\": now,\n                \"daemons\": {}\n            }\n            state = 0\n            for satellite in self.dispatcher.all_daemons_links:\n                if satellite == self.link_to_myself:\n                    continue\n\n                livestate = 0\n                if satellite.active:\n                    if not satellite.reachable:\n                        livestate = 1\n                    elif not satellite.alive:\n                        livestate = 2\n                    state = max(state, livestate)\n                else:\n                    livestate = 3\n\n                res['livestate']['daemons'][satellite.name] = livestate\n            res['livestate'].update({\n                \"state\": state,\n                \"output\": [\n                    \"all daemons are up and running.\",\n                    \"warning because some daemons are not reachable.\",\n                    \"critical because some daemons not responding.\"\n                ][state],\n                # \"long_output\": \"Long output...\",\n                # \"perf_data\": \"'counter'=1\"\n            })\n\n        return res", "response": "Increase the stats provided by the base Daemon class and return a dictionary of the daemon stats."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_monitoring_problems(self):\n        res = self.get_id()\n        res['problems'] = {}\n\n        # Report our schedulers information, but only if a dispatcher exists\n        if getattr(self, 'dispatcher', None) is None:\n            return res\n\n        for satellite in self.dispatcher.all_daemons_links:\n            if satellite.type not in ['scheduler']:\n                continue\n            if not satellite.active:\n                continue\n\n            if satellite.statistics and 'problems' in satellite.statistics:\n                res['problems'][satellite.name] = {\n                    '_freshness': satellite.statistics['_freshness'],\n                    'problems': satellite.statistics['problems']\n                }\n\n        return res", "response": "Get the schedulers satellites problems list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the schedulers satellites live synthesis dictionary.", "response": "def get_livesynthesis(self):\n        \"\"\"Get the schedulers satellites live synthesis\n\n        :return: compiled livesynthesis dictionary\n        :rtype: dict\n        \"\"\"\n        res = self.get_id()\n        res['livesynthesis'] = {\n            '_overall': {\n                '_freshness': int(time.time()),\n                'livesynthesis': {\n                    'hosts_total': 0,\n                    'hosts_not_monitored': 0,\n                    'hosts_up_hard': 0,\n                    'hosts_up_soft': 0,\n                    'hosts_down_hard': 0,\n                    'hosts_down_soft': 0,\n                    'hosts_unreachable_hard': 0,\n                    'hosts_unreachable_soft': 0,\n                    'hosts_problems': 0,\n                    'hosts_acknowledged': 0,\n                    'hosts_in_downtime': 0,\n                    'hosts_flapping': 0,\n                    'services_total': 0,\n                    'services_not_monitored': 0,\n                    'services_ok_hard': 0,\n                    'services_ok_soft': 0,\n                    'services_warning_hard': 0,\n                    'services_warning_soft': 0,\n                    'services_critical_hard': 0,\n                    'services_critical_soft': 0,\n                    'services_unknown_hard': 0,\n                    'services_unknown_soft': 0,\n                    'services_unreachable_hard': 0,\n                    'services_unreachable_soft': 0,\n                    'services_problems': 0,\n                    'services_acknowledged': 0,\n                    'services_in_downtime': 0,\n                    'services_flapping': 0,\n                }\n            }\n        }\n\n        # Report our schedulers information, but only if a dispatcher exists\n        if getattr(self, 'dispatcher', None) is None:\n            return res\n\n        for satellite in self.dispatcher.all_daemons_links:\n            if satellite.type not in ['scheduler']:\n                continue\n            if not satellite.active:\n                continue\n\n            if 'livesynthesis' in satellite.statistics:\n                # Scheduler detailed live synthesis\n                res['livesynthesis'][satellite.name] = {\n                    '_freshness': satellite.statistics['_freshness'],\n                    'livesynthesis': satellite.statistics['livesynthesis']\n                }\n                # Cumulated live synthesis\n                for prop in res['livesynthesis']['_overall']['livesynthesis']:\n                    if prop in satellite.statistics['livesynthesis']:\n                        res['livesynthesis']['_overall']['livesynthesis'][prop] += \\\n                            satellite.statistics['livesynthesis'][prop]\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(self):\n        try:\n            # Start the daemon\n            if not self.verify_only and not self.do_daemon_init_and_start():\n                self.exit_on_error(message=\"Daemon initialization error\", exit_code=3)\n\n            if self.verify_only:\n                self.setup_alignak_logger()\n\n            # Load monitoring configuration files\n            self.load_monitoring_config_file()\n\n            # Set my own process title\n            self.set_proctitle(self.name)\n\n            # Now we can start our \"external\" modules (if any):\n            self.modules_manager.start_external_instances()\n\n            # Now we can load the retention data\n            self.hook_point('load_retention')\n\n            # And go for the main loop\n            while True:\n                self.do_main_loop()\n                logger.info(\"Exited from the main loop.\")\n\n                # Exiting the main loop because of a configuration reload\n                if not self.need_config_reload:\n                    # If no configuration reload is required, stop the arbiter daemon\n                    self.request_stop()\n                else:\n                    # Loop if a configuration reload is raised while\n                    # still reloading the configuration\n                    while self.need_config_reload:\n                        # Clear the former configuration\n                        self.need_config_reload = False\n                        self.link_to_myself = None\n                        self.conf = Config()\n                        # Load monitoring configuration files\n                        _ts = time.time()\n                        logger.warning('--- Reloading configuration...')\n                        self.load_monitoring_config_file()\n                        duration = int(time.time() - _ts)\n                        self.add(make_monitoring_log('info', 'CONFIGURATION RELOAD;%d' % duration))\n                        logger.warning('--- Configuration reloaded, %d seconds', duration)\n\n                        # Make a pause to let our satellites get ready...\n                        pause = max(1, self.conf.daemons_new_conf_timeout)\n                        if pause:\n                            logger.info(\"Pausing %.2f seconds...\", pause)\n                            time.sleep(pause)\n\n        except Exception as exp:  # pragma: no cover, this should never happen indeed ;)\n            # Only a master arbiter can stop the daemons\n            if self.is_master:\n                # Stop the daemons\n                self.daemons_stop(timeout=self.conf.daemons_stop_timeout)\n            self.exit_on_exception(raised_exception=exp)\n            raise", "response": "Main arbiter function::\n\n        * Set logger\n        * Init daemon\n        * Launch modules\n        * Endless main process loop\n\n        :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef overall_state_id(self):\n        overall_state = 0\n        if not self.monitored:\n            overall_state = 5\n        elif self.acknowledged:\n            overall_state = 1\n        elif self.downtimed:\n            overall_state = 2\n        elif self.state_type == 'HARD':\n            if self.state == 'WARNING':\n                overall_state = 3\n            elif self.state == 'CRITICAL':\n                overall_state = 4\n            elif self.state == 'UNKNOWN':\n                overall_state = 3\n            elif self.state == 'UNREACHABLE':\n                overall_state = 4\n\n        return overall_state", "response": "Get the service overall state identifier."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fill_predictive_missing_parameters(self):\n        if self.initial_state == 'w':\n            self.state = u'WARNING'\n        elif self.initial_state == 'u':\n            self.state = u'UNKNOWN'\n        elif self.initial_state == 'c':\n            self.state = u'CRITICAL'\n        elif self.initial_state == 'x':\n            self.state = u'UNREACHABLE'", "response": "define state with initial_state\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the full name for debugging", "response": "def get_full_name(self):\n        \"\"\"Get the full name for debugging (host_name/service_description)\n\n        :return: service full name\n        :rtype: str\n        \"\"\"\n        if self.is_tpl():\n            return \"tpl-%s/%s\" % (getattr(self, 'host_name', 'XxX'), self.name)\n        if hasattr(self, 'host_name') and hasattr(self, 'service_description'):\n            return \"%s/%s\" % (self.host_name, self.service_description)\n        return 'UNKNOWN-SERVICE'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget servicegroups list :return: comma separated list of servicegroups :rtype: str", "response": "def get_groupnames(self, sgs):\n        \"\"\"Get servicegroups list\n\n        :return: comma separated list of servicegroups\n        :rtype: str\n        \"\"\"\n        return ','.join([sgs[sg].get_name() for sg in self.servicegroups])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if this object configuration is correct.", "response": "def is_correct(self):\n        \"\"\"Check if this object configuration is correct ::\n\n        * Check our own specific properties\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        state = True\n        cls = self.__class__\n\n        hname = getattr(self, 'host_name', '')\n        hgname = getattr(self, 'hostgroup_name', '')\n        sdesc = getattr(self, 'service_description', '')\n\n        if not sdesc:\n            self.add_error(\"a %s has been defined without service_description, from: %s\"\n                           % (cls, self.imported_from))\n        elif not hname:\n            self.add_error(\"[%s::%s] not bound to any host.\"\n                           % (self.my_type, self.get_name()))\n        elif not hname and not hgname:\n            self.add_error(\"a %s has been defined without host_name nor \"\n                           \"hostgroup_name, from: %s\" % (self.my_type, self.imported_from))\n        elif self.host is None:\n            self.add_error(\"[%s::%s] unknown host_name '%s'\"\n                           % (self.my_type, self.get_name(), self.host_name))\n\n        # Set display_name if needed\n        if not getattr(self, 'display_name', ''):\n            self.display_name = \"%s/%s\" % (hname, sdesc)\n\n        for char in cls.illegal_object_name_chars:\n            if char not in self.service_description:\n                continue\n\n            self.add_error(\"[%s::%s] service_description got an illegal character: %s\"\n                           % (self.my_type, self.get_name(), char))\n\n        return super(Service, self).is_correct() and state"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef duplicate(self, host):\n        # pylint: disable=too-many-locals\n        \"\"\"For a given host, look for all copy we must create for for_each property\n\n        :param host: alignak host object\n        :type host: alignak.objects.host.Host\n        :return: list\n        :rtype: list\n        \"\"\"\n\n        duplicates = []\n\n        # In macro, it's all in UPPER case\n        prop = self.duplicate_foreach.strip().upper()\n        if prop not in host.customs:  # If I do not have the property, we bail out\n            return duplicates\n\n        # Get the list entry, and the not one if there is one\n        entry = host.customs[prop]\n        # Look at the list of the key we do NOT want maybe,\n        # for _disks it will be _!disks\n        not_entry = host.customs.get('_' + '!' + prop[1:], '').split(',')\n        not_keys = strip_and_uniq(not_entry)\n\n        default_value = getattr(self, 'default_value', '')\n        # Transform the generator string to a list\n        # Missing values are filled with the default value\n        try:\n            key_values = tuple(generate_key_value_sequences(entry, default_value))\n        except KeyValueSyntaxError as exc:\n            fmt_dict = {\n                'prop': self.duplicate_foreach,\n                'host': host.get_name(),\n                'svc': self.service_description,\n                'entry': entry,\n                'exc': exc,\n            }\n            err = (\n                \"The custom property %(prop)r of the \"\n                \"host %(host)r is not a valid entry for a service generator: %(exc)s, \"\n                \"with entry=%(entry)r\") % fmt_dict\n            logger.warning(err)\n            host.add_error(err)\n            return duplicates\n\n        for key_value in key_values:\n            key = key_value['KEY']\n            # Maybe this key is in the NOT list, if so, skip it\n            if key in not_keys:\n                continue\n            new_s = self.copy()\n            new_s.host_name = host.get_name()\n            if self.is_tpl():  # if template, the new one is not\n                new_s.register = 1\n            for key in key_value:\n                if key == 'KEY':\n                    if hasattr(self, 'service_description'):\n                        # We want to change all illegal chars to a _ sign.\n                        # We can't use class.illegal_obj_char\n                        # because in the \"explode\" phase, we do not have access to this data! :(\n                        safe_key_value = re.sub(r'[' + \"`~!$%^&*\\\"|'<>?,()=\" + ']+', '_',\n                                                key_value[key])\n                        new_s.service_description = self.service_description.replace(\n                            '$' + key + '$', safe_key_value\n                        )\n                # Here is a list of property where we will expand the $KEY$ by the value\n                _the_expandables = ['check_command', 'aggregation', 'event_handler']\n                for prop in _the_expandables:\n                    if hasattr(self, prop):\n                        # here we can replace VALUE, VALUE1, VALUE2,...\n                        setattr(new_s, prop, getattr(new_s, prop).replace('$' + key + '$',\n                                                                          key_value[key]))\n                if hasattr(self, 'service_dependencies'):\n                    for i, servicedep in enumerate(new_s.service_dependencies):\n                        new_s.service_dependencies[i] = servicedep.replace(\n                            '$' + key + '$', key_value[key]\n                        )\n            # And then add in our list this new service\n            duplicates.append(new_s)\n\n        return duplicates", "response": "This method creates a copy of the object with the same properties and returns a list of the duplicate ones."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_state_from_exit_status(self, status, notif_period, hosts, services):\n        now = time.time()\n\n        # we should put in last_state the good last state:\n        # if not just change the state by an problem/impact\n        # we can take current state. But if it's the case, the\n        # real old state is self.state_before_impact (it's the TRUE\n        # state in fact)\n        # but only if the global conf have enable the impact state change\n        cls = self.__class__\n        if cls.enable_problem_impacts_states_change \\\n                and self.is_impact \\\n                and not self.state_changed_since_impact:\n            self.last_state = self.state_before_impact\n        else:  # standard case\n            self.last_state = self.state\n\n        # The last times are kept as integer values rather than float... no need for ms!\n        if status == 0:\n            self.state = u'OK'\n            self.state_id = 0\n            self.last_time_ok = int(self.last_state_update)\n            # self.last_time_ok = self.last_state_update\n            state_code = 'o'\n        elif status == 1:\n            self.state = u'WARNING'\n            self.state_id = 1\n            self.last_time_warning = int(self.last_state_update)\n            # self.last_time_warning = self.last_state_update\n            state_code = 'w'\n        elif status == 2:\n            self.state = u'CRITICAL'\n            self.state_id = 2\n            self.last_time_critical = int(self.last_state_update)\n            # self.last_time_critical = self.last_state_update\n            state_code = 'c'\n        elif status == 3:\n            self.state = u'UNKNOWN'\n            self.state_id = 3\n            self.last_time_unknown = int(self.last_state_update)\n            # self.last_time_unknown = self.last_state_update\n            state_code = 'u'\n        elif status == 4:\n            self.state = u'UNREACHABLE'\n            self.state_id = 4\n            self.last_time_unreachable = int(self.last_state_update)\n            # self.last_time_unreachable = self.last_state_update\n            state_code = 'x'\n        else:\n            self.state = u'CRITICAL'  # exit code UNDETERMINED\n            self.state_id = 2\n            self.last_time_critical = int(self.last_state_update)\n            # self.last_time_critical = self.last_state_update\n            state_code = 'c'\n\n        if state_code in self.flap_detection_options:\n            self.add_flapping_change(self.state != self.last_state)\n            # Now we add a value, we update the is_flapping prop\n            self.update_flapping(notif_period, hosts, services)\n        if self.state != self.last_state:\n            self.last_state_change = self.last_state_update\n\n        self.duration_sec = now - self.last_state_change", "response": "Set the state of the current object based on the status of a check result."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_state(self, status):\n        # pylint: disable=too-many-return-statements\n        \"\"\"Return True if status match the current service status\n\n        :param status: status to compare ( \"o\", \"c\", \"w\", \"u\", \"x\"). Usually comes from config files\n        :type status: str\n        :return: True if status <=> self.status, otherwise False\n        :rtype: bool\n        \"\"\"\n        if status == self.state:\n            return True\n        # Now low status\n        if status == 'o' and self.state == u'OK':\n            return True\n        if status == 'c' and self.state == u'CRITICAL':\n            return True\n        if status == 'w' and self.state == u'WARNING':\n            return True\n        if status == 'u' and self.state == u'UNKNOWN':\n            return True\n        if status == 'x' and self.state == u'UNREACHABLE':\n            return True\n        return False", "response": "Return True if status is in the current state False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef last_time_non_ok_or_up(self):\n        non_ok_times = [x for x in [self.last_time_warning,\n                                    self.last_time_critical,\n                                    self.last_time_unknown]\n                        if x > self.last_time_ok]\n        if not non_ok_times:\n            last_time_non_ok = 0  # todo: program_start would be better?\n        else:\n            last_time_non_ok = min(non_ok_times)\n        return last_time_non_ok", "response": "Get the last time the service was not ok or up."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef raise_check_result(self):\n        if not self.__class__.log_active_checks:\n            return\n\n        log_level = 'info'\n        if self.state in [u'WARNING', u'UNREACHABLE']:\n            log_level = 'warning'\n        elif self.state == u'CRITICAL':\n            log_level = 'error'\n        brok = make_monitoring_log(\n            log_level, 'ACTIVE SERVICE CHECK: %s;%s;%s;%d;%s' % (self.host_name, self.get_name(),\n                                                                 self.state, self.attempt,\n                                                                 self.output)\n        )\n        self.broks.append(brok)", "response": "Raise ACTIVE CHECK RESULT entry\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nraising SERVICE NOTIFICATION entry.", "response": "def raise_notification_log_entry(self, notif, contact, host_ref):\n        \"\"\"Raise SERVICE NOTIFICATION entry (critical level)\n        Format is : \"SERVICE NOTIFICATION: *contact.get_name()*;*host_name*;*self.get_name()*\n                    ;*state*;*command.get_name()*;*output*\"\n        Example : \"SERVICE NOTIFICATION: superadmin;server;Load;UP;notify-by-rss;no output\"\n\n        :param notif: notification object created by service alert\n        :type notif: alignak.objects.notification.Notification\n        :return: None\n        \"\"\"\n        if self.__class__.log_notifications:\n            log_level = 'info'\n            command = notif.command_call\n            if notif.type in [u'DOWNTIMESTART', u'DOWNTIMEEND', u'DOWNTIMECANCELLED',\n                              u'CUSTOM', u'ACKNOWLEDGEMENT',\n                              u'FLAPPINGSTART', u'FLAPPINGSTOP', u'FLAPPINGDISABLED']:\n                state = '%s (%s)' % (notif.type, self.state)\n            else:\n                state = self.state\n                if self.state == 'WARNING':\n                    log_level = 'warning'\n                if self.state == 'CRITICAL':\n                    log_level = 'error'\n\n            brok = make_monitoring_log(\n                log_level, \"SERVICE NOTIFICATION: %s;%s;%s;%s;%s;%s;%s\" % (\n                    contact.get_name(), host_ref.get_name(), self.get_name(), state,\n                    notif.notif_nb, command.get_name(), self.output\n                )\n            )\n            self.broks.append(brok)\n\n        if 'ALIGNAK_LOG_NOTIFICATIONS' in os.environ:\n            if os.environ['ALIGNAK_LOG_NOTIFICATIONS'] == 'WARNING':\n                logger.warning(\"SERVICE NOTIFICATION: %s;%s;%s;%s;%s;%s;%s\",\n                               contact.get_name(), host_ref.get_name(), self.get_name(), state,\n                               notif.notif_nb, command.get_name(), self.output)\n            else:\n                logger.info(\"SERVICE NOTIFICATION: %s;%s;%s;%s;%s;%s;%s\",\n                            contact.get_name(), host_ref.get_name(), self.get_name(), state,\n                            notif.notif_nb, command.get_name(), self.output)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nraise SERVICE EVENT HANDLER entry", "response": "def raise_event_handler_log_entry(self, command):\n        \"\"\"Raise SERVICE EVENT HANDLER entry (critical level)\n        Format is : \"SERVICE EVENT HANDLER: *host_name*;*self.get_name()*;*state*;*state_type*\n                    ;*attempt*;*command.get_name()*\"\n        Example : \"SERVICE EVENT HANDLER: server;Load;UP;HARD;1;notify-by-rss\"\n\n        :param command: Handler launched\n        :type command: alignak.objects.command.Command\n        :return: None\n        \"\"\"\n        if not self.__class__.log_event_handlers:\n            return\n\n        log_level = 'info'\n        if self.state == 'WARNING':\n            log_level = 'warning'\n        if self.state == 'CRITICAL':\n            log_level = 'error'\n        brok = make_monitoring_log(\n            log_level, \"SERVICE EVENT HANDLER: %s;%s;%s;%s;%s;%s\" % (\n                self.host_name, self.get_name(),\n                self.state, self.state_type,\n                self.attempt, command.get_name()\n            )\n        )\n        self.broks.append(brok)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_data_for_notifications(self, contact, notif, host_ref):\n        if not host_ref:\n            return [self, contact, notif]\n        return [host_ref, self, contact, notif]", "response": "Get data for a notification"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a notification is blocked by a service.", "response": "def is_blocking_notifications(self, notification_period, hosts, services, n_type, t_wished):\n        # pylint: disable=too-many-return-statements\n        \"\"\"Check if a notification is blocked by the service.\n        Conditions are ONE of the following::\n\n        * enable_notification is False (global)\n        * not in a notification_period\n        * notifications_enable is False (local)\n        * notification_options is 'n' or matches the state ('UNKNOWN' <=> 'u' ...)\n          (include flapping and downtimes)\n        * state goes ok and type is 'ACKNOWLEDGEMENT' (no sense)\n        * scheduled_downtime_depth > 0 and flapping (host is in downtime)\n        * scheduled_downtime_depth > 1 and not downtime end (deep downtime)\n        * scheduled_downtime_depth > 0 and problem or recovery (host is in downtime)\n        * SOFT state of a problem (we raise notification ony on HARD state)\n        * ACK notification when already ACK (don't raise again ACK)\n        * not flapping notification in a flapping state\n        * business rule smart notifications is enabled and all its children have been acknowledged\n          or are under downtime\n        * linked host is not up\n        * linked host is in downtime\n\n        :param n_type: notification type\n        :type n_type:\n        :param t_wished: the time we should like to notify the host (mostly now)\n        :type t_wished: float\n        :return: True if ONE of the above condition was met, otherwise False\n        :rtype: bool\n        TODO: Refactor this, a lot of code duplication with Host.is_blocking_notifications\n        \"\"\"\n        logger.debug(\"Checking if a service %s (%s) notification is blocked...\",\n                     self.get_full_name(), self.state)\n        host = hosts[self.host]\n        if t_wished is None:\n            t_wished = time.time()\n\n        #  TODO\n        # forced notification\n        # pass if this is a custom notification\n\n        # Block if notifications are program-wide disabled\n        # Block if notifications are disabled for this service\n        # Block if the current status is in the notification_options w,u,c,r,f,s\n        if not self.enable_notifications or \\\n                not self.notifications_enabled or \\\n                'n' in self.notification_options:\n            logger.debug(\"Service: %s, notification %s sending is blocked by configuration\",\n                         self.get_name(), n_type)\n            return True\n\n        # Does the notification period allow sending out this notification?\n        if notification_period is not None and not notification_period.is_time_valid(t_wished):\n            logger.debug(\"Service: %s, notification %s sending is blocked by globals\",\n                         self.get_name(), n_type)\n            return True\n\n        if n_type in (u'PROBLEM', u'RECOVERY') and (\n                self.state == u'UNKNOWN' and 'u' not in self.notification_options or\n                self.state == u'WARNING' and 'w' not in self.notification_options or\n                self.state == u'CRITICAL' and 'c' not in self.notification_options or\n                self.state == u'OK' and 'r' not in self.notification_options or\n                self.state == u'UNREACHABLE' and 'x' not in self.notification_options):\n            logger.debug(\"Service: %s, notification %s sending is blocked by options: %s\",\n                         self.get_name(), n_type, self.notification_options)\n            return True\n\n        if (n_type in [u'FLAPPINGSTART', u'FLAPPINGSTOP', u'FLAPPINGDISABLED'] and\n                'f' not in self.notification_options):\n            logger.debug(\"Service: %s, notification %s sending is blocked by options: %s\",\n                         n_type, self.get_full_name(), self.notification_options)\n            return True\n        if (n_type in [u'DOWNTIMESTART', u'DOWNTIMEEND', u'DOWNTIMECANCELLED'] and\n                's' not in self.notification_options):\n            logger.debug(\"Service: %s, notification %s sending is blocked by options: %s\",\n                         n_type, self.get_full_name(), self.notification_options)\n            return True\n\n        # Acknowledgements make no sense when the status is ok/up\n        if n_type in [u'ACKNOWLEDGEMENT'] and self.state == self.ok_up:\n            logger.debug(\"Host: %s, notification %s sending is blocked by current state\",\n                         self.get_name(), n_type)\n            return True\n\n        # Block if host is in a scheduled downtime\n        if host.scheduled_downtime_depth > 0:\n            logger.debug(\"Service: %s, notification %s sending is blocked by downtime\",\n                         self.get_name(), n_type)\n            return True\n\n        # When in deep downtime, only allow end-of-downtime notifications\n        # In depth 1 the downtime just started and can be notified\n        if self.scheduled_downtime_depth > 1 and n_type not in (u'DOWNTIMEEND',\n                                                                u'DOWNTIMECANCELLED'):\n            logger.debug(\"Service: %s, notification %s sending is blocked by deep downtime\",\n                         self.get_name(), n_type)\n            return True\n\n        # Block if in a scheduled downtime and a problem arises, or flapping event\n        if self.scheduled_downtime_depth > 0 and n_type in \\\n                [u'PROBLEM', u'RECOVERY', u'ACKNOWLEDGEMENT',\n                 u'FLAPPINGSTART', u'FLAPPINGSTOP', u'FLAPPINGDISABLED']:\n            logger.debug(\"Service: %s, notification %s sending is blocked by downtime\",\n                         self.get_name(), n_type)\n            return True\n\n        # Block if the status is SOFT\n        # Block if the problem has already been acknowledged\n        # Block if flapping\n        # Block if host is down\n        if self.state_type == u'SOFT' and n_type == u'PROBLEM' or \\\n                self.problem_has_been_acknowledged and n_type != u'ACKNOWLEDGEMENT' or \\\n                self.is_flapping and n_type not in [u'FLAPPINGSTART',\n                                                    u'FLAPPINGSTOP',\n                                                    u'FLAPPINGDISABLED'] or \\\n                host.state != host.ok_up:\n            logger.debug(\"Service: %s, notification %s sending is blocked by soft state, \"\n                         \"acknowledgement, flapping or host DOWN\", self.get_name(), n_type)\n            return True\n\n        # Block if business rule smart notifications is enabled and all its\n        # children have been acknowledged or are under downtime.\n        if self.got_business_rule is True \\\n                and self.business_rule_smart_notifications is True \\\n                and self.business_rule_notification_is_blocked(hosts, services) is True \\\n                and n_type == u'PROBLEM':\n            logger.debug(\"Service: %s, notification %s sending is blocked by business rules\",\n                         self.get_name(), n_type)\n            return True\n\n        logger.debug(\"Service: %s, notification %s sending is not blocked\", self.get_name(), n_type)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_short_status(self, hosts, services):\n        mapping = {\n            0: \"O\",\n            1: \"W\",\n            2: \"C\",\n            3: \"U\",\n            4: \"N\",\n        }\n        if self.got_business_rule:\n            return mapping.get(self.business_rule.get_state(hosts, services), \"n/a\")\n\n        return mapping.get(self.state_id, \"n/a\")", "response": "Get the short status of this host based on service state_id or business_rule state_id or n/a"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_status(self, hosts, services):\n\n        if self.got_business_rule:\n            mapping = {\n                0: u'OK',\n                1: u'WARNING',\n                2: u'CRITICAL',\n                3: u'UNKNOWN',\n                4: u'UNREACHABLE',\n            }\n            return mapping.get(self.business_rule.get_state(hosts, services), \"n/a\")\n\n        return self.state", "response": "Get the status of this host based on the service state_id or business_rule state_id or return the status of this host"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a template to the internal list of templates.", "response": "def add_template(self, tpl):\n        \"\"\"\n        Adds and index a template into the `templates` container.\n\n        This implementation takes into account that a service has two naming\n        attribute: `host_name` and `service_description`.\n\n        :param tpl: The template to add\n        :type tpl:\n        :return: None\n        \"\"\"\n        objcls = self.inner_class.my_type\n        name = getattr(tpl, 'name', '')\n        sdesc = getattr(tpl, 'service_description', '')\n        hname = getattr(tpl, 'host_name', '')\n        logger.debug(\"Adding a %s template: host_name: %s, name: %s, service_description: %s\",\n                     objcls, hname, name, sdesc)\n        if not name and not hname:\n            msg = \"a %s template has been defined without name nor host_name. from: %s\" \\\n                  % (objcls, tpl.imported_from)\n            tpl.add_error(msg)\n        elif not name and not sdesc:\n            msg = \"a %s template has been defined without name nor service_description. from: %s\" \\\n                  % (objcls, tpl.imported_from)\n            tpl.add_error(msg)\n        elif not name:\n            # If name is not defined, use the host_name_service_description as name (fix #791)\n            setattr(tpl, 'name', \"%s_%s\" % (hname, sdesc))\n            tpl = self.index_template(tpl)\n        elif name:\n            tpl = self.index_template(tpl)\n        self.templates[tpl.uuid] = tpl\n        logger.debug('\\tAdded service template #%d %s', len(self.templates), tpl)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_srvs_by_hostname(self, host_name):\n        if hasattr(self, 'hosts'):\n            host = self.hosts.find_by_name(host_name)\n            if host is None:\n                return None\n            return host.get_services()\n        return None", "response": "Get all services from a host based on a host name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_srv_by_name_and_hostname(self, host_name, sdescr):\n        key = (host_name, sdescr)\n        return self.name_to_item.get(key, None)", "response": "Find a specific service based on a host name and service description"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef linkify(self, hosts, commands, timeperiods, contacts,  # pylint: disable=R0913\n                resultmodulations, businessimpactmodulations, escalations,\n                servicegroups, checkmodulations, macromodulations):\n        \"\"\"Create link between objects::\n\n         * service -> host\n         * service -> command\n         * service -> timeperiods\n         * service -> contacts\n\n        :param hosts: hosts to link\n        :type hosts: alignak.objects.host.Hosts\n        :param timeperiods: timeperiods to link\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param commands: commands to link\n        :type commands: alignak.objects.command.Commands\n        :param contacts: contacts to link\n        :type contacts: alignak.objects.contact.Contacts\n        :param resultmodulations: resultmodulations to link\n        :type resultmodulations: alignak.objects.resultmodulation.Resultmodulations\n        :param businessimpactmodulations: businessimpactmodulations to link\n        :type businessimpactmodulations:\n              alignak.objects.businessimpactmodulation.Businessimpactmodulations\n        :param escalations: escalations to link\n        :type escalations: alignak.objects.escalation.Escalations\n        :param servicegroups: servicegroups to link\n        :type servicegroups: alignak.objects.servicegroup.Servicegroups\n        :param checkmodulations: checkmodulations to link\n        :type checkmodulations: alignak.objects.checkmodulation.Checkmodulations\n        :param macromodulations: macromodulations to link\n        :type macromodulations:  alignak.objects.macromodulation.Macromodulations\n        :return: None\n        \"\"\"\n        self.linkify_with_timeperiods(timeperiods, 'notification_period')\n        self.linkify_with_timeperiods(timeperiods, 'check_period')\n        self.linkify_with_timeperiods(timeperiods, 'maintenance_period')\n        self.linkify_with_timeperiods(timeperiods, 'snapshot_period')\n        self.linkify_s_by_hst(hosts)\n        self.linkify_s_by_sg(servicegroups)\n        self.linkify_one_command_with_commands(commands, 'check_command')\n        self.linkify_one_command_with_commands(commands, 'event_handler')\n        self.linkify_one_command_with_commands(commands, 'snapshot_command')\n        self.linkify_with_contacts(contacts)\n        self.linkify_with_resultmodulations(resultmodulations)\n        self.linkify_with_business_impact_modulations(businessimpactmodulations)\n        # WARNING: all escalations will not be link here\n        # (just the escalation here, not serviceesca or hostesca).\n        # This last one will be link in escalations linkify.\n        self.linkify_with_escalations(escalations)\n        self.linkify_with_checkmodulations(checkmodulations)\n        self.linkify_with_macromodulations(macromodulations)", "response": "Create link between objects::\n\n         * service -> host\n         * service -> command\n         * service -> timeperiods\n         * service -> contacts\n\n        :param hosts: hosts to link\n        :type hosts: alignak.objects.host.Hosts\n        :param timeperiods: timeperiods to link\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param commands: commands to link\n        :type commands: alignak.objects.command.Commands\n        :param contacts: contacts to link\n        :type contacts: alignak.objects.contact.Contacts\n        :param resultmodulations: resultmodulations to link\n        :type resultmodulations: alignak.objects.resultmodulation.Resultmodulations\n        :param businessimpactmodulations: businessimpactmodulations to link\n        :type businessimpactmodulations:\n              alignak.objects.businessimpactmodulation.Businessimpactmodulations\n        :param escalations: escalations to link\n        :type escalations: alignak.objects.escalation.Escalations\n        :param servicegroups: servicegroups to link\n        :type servicegroups: alignak.objects.servicegroup.Servicegroups\n        :param checkmodulations: checkmodulations to link\n        :type checkmodulations: alignak.objects.checkmodulation.Checkmodulations\n        :param macromodulations: macromodulations to link\n        :type macromodulations:  alignak.objects.macromodulation.Macromodulations\n        :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef override_properties(self, hosts):\n        ovr_re = re.compile(r'^([^,]+),([^\\s]+)\\s+(.*)$')\n        ovr_hosts = [h for h in hosts if getattr(h, 'service_overrides', None)]\n        for host in ovr_hosts:\n            # We're only looking for hosts having service overrides defined\n            if isinstance(host.service_overrides, list):\n                service_overrides = host.service_overrides\n            else:\n                service_overrides = [host.service_overrides]\n            for ovr in service_overrides:\n                # Checks service override syntax\n                match = ovr_re.search(ovr)\n                if match is None:\n                    host.add_error(\"Error: invalid service override syntax: %s\" % ovr)\n                    continue\n                sdescr, prop, value = match.groups()\n                # Looks for corresponding service\n                service = self.find_srv_by_name_and_hostname(getattr(host, \"host_name\", \"\"), sdescr)\n                if service is None:\n                    host.add_error(\"Error: trying to override property '%s' on service '%s' \"\n                                   \"but it's unknown for this host\" % (prop, sdescr))\n                    continue\n                # Checks if override is allowed\n                excludes = ['host_name', 'service_description', 'use',\n                            'servicegroups', 'trigger_name']\n                if prop in excludes:\n                    host.add_error(\"Error: trying to override '%s', \"\n                                   \"a forbidden property for service '%s'\" % (prop, sdescr))\n                    continue\n\n                # Pythonize the value because here value is str.\n                setattr(service, prop, service.properties[prop].pythonize(value))", "response": "Handle service_overrides property for hosts and services and apply override properties for relevant services."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlink services with their parent host.", "response": "def linkify_s_by_hst(self, hosts):\n        \"\"\"Link services with their parent host\n\n        :param hosts: Hosts to look for simple host\n        :type hosts: alignak.objects.host.Hosts\n        :return: None\n        \"\"\"\n        for serv in self:\n            # If we do not have a host_name, we set it as\n            # a template element to delete. (like Nagios)\n            if not hasattr(serv, 'host_name'):\n                serv.host = None\n                continue\n            try:\n                hst_name = serv.host_name\n                # The new member list, in id\n                hst = hosts.find_by_name(hst_name)\n                # Let the host know we are his service\n                if hst is not None:\n                    serv.host = hst.uuid\n                    hst.add_service_link(serv.uuid)\n                else:  # Ok, the host do not exists!\n                    err = \"Warning: the service '%s' got an invalid host_name '%s'\" % \\\n                          (serv.get_name(), hst_name)\n                    serv.configuration_warnings.append(err)\n                    continue\n            except AttributeError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlink services with servicegroups xid", "response": "def linkify_s_by_sg(self, servicegroups):\n        \"\"\"Link services with servicegroups\n\n        :param servicegroups: Servicegroups\n        :type servicegroups: alignak.objects.servicegroup.Servicegroups\n        :return: None\n        \"\"\"\n        for serv in self:\n            new_servicegroups = []\n            if hasattr(serv, 'servicegroups') and serv.servicegroups != '':\n                for sg_name in serv.servicegroups:\n                    sg_name = sg_name.strip()\n                    servicegroup = servicegroups.find_by_name(sg_name)\n                    if servicegroup is not None:\n                        new_servicegroups.append(servicegroup.uuid)\n                    else:\n                        err = \"Error: the servicegroup '%s' of the service '%s' is unknown\" %\\\n                              (sg_name, serv.get_dbg_name())\n                        serv.add_error(err)\n            serv.servicegroups = new_servicegroups"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_implicit_inheritance(self, hosts):\n        for prop in ('contacts', 'contact_groups', 'notification_interval',\n                     'notification_period', 'resultmodulations', 'business_impact_modulations',\n                     'escalations', 'poller_tag', 'reactionner_tag', 'check_period',\n                     'business_impact', 'maintenance_period'):\n            for serv in self:\n                if hasattr(serv, 'host_name') and not getattr(serv, prop, None):\n                    host = hosts.find_by_name(serv.host_name)\n                    if host is not None and hasattr(host, prop):\n                        logger.debug(\"Implicit inheritance for %s/%s: %s = %s\",\n                                     serv.host_name, serv, prop, getattr(host, prop))\n                        setattr(serv, prop, getattr(host, prop))", "response": "Applies implicit inheritance for special properties contact_groups notification_interval notification_period and service_tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_dependencies(self, hosts):\n        for service in self:\n            if service.host and service.host_dependency_enabled:\n                host = hosts[service.host]\n                if host.active_checks_enabled:\n                    service.act_depend_of.append(\n                        (service.host, ['d', 'x', 's', 'f'], '', True)\n                    )\n                    host.act_depend_of_me.append(\n                        (service.uuid, ['d', 'x', 's', 'f'], '', True)\n                    )\n                    host.child_dependencies.add(service.uuid)\n                    service.parent_dependencies.add(service.host)", "response": "Wrapper to loop over services and call Service. fill_daddy_dependency"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean(self):\n        to_del = []\n        for serv in self:\n            if not serv.host:\n                to_del.append(serv.uuid)\n        for service_uuid in to_del:\n            del self.items[service_uuid]", "response": "Remove services without host object linked to\n\n            Note this should not happen!"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexploding a service based on a list of hosts.", "response": "def explode_services_from_hosts(self, hosts, service, hnames):\n        \"\"\"\n        Explodes a service based on a list of hosts.\n\n        :param hosts: The hosts container\n        :type hosts:\n        :param service: The base service to explode\n        :type service:\n        :param hnames:  The host_name list to explode service on\n        :type hnames: str\n        :return: None\n        \"\"\"\n        duplicate_for_hosts = []  # get the list of our host_names if more than 1\n        not_hosts = []  # the list of !host_name so we remove them after\n        for hname in hnames:\n            hname = hname.strip()\n\n            # If the name begin with a !, we put it in\n            # the not list\n            if hname.startswith('!'):\n                not_hosts.append(hname[1:])\n            else:  # the standard list\n                duplicate_for_hosts.append(hname)\n\n        # remove duplicate items from duplicate_for_hosts:\n        duplicate_for_hosts = list(set(duplicate_for_hosts))\n\n        # Ok now we clean the duplicate_for_hosts with all hosts\n        # of the not\n        for hname in not_hosts:\n            try:\n                duplicate_for_hosts.remove(hname)\n            except IndexError:\n                pass\n\n        # Now we duplicate the service for all host_names\n        for hname in duplicate_for_hosts:\n            host = hosts.find_by_name(hname)\n            if host is None:\n                service.add_error(\"Error: The hostname %s is unknown for the service %s!\"\n                                  % (hname, service.get_name()))\n                continue\n            if host.is_excluded_for(service):\n                continue\n            new_s = service.copy()\n            new_s.host_name = hname\n            self.add_item(new_s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new service based on a host_name and service instance.", "response": "def _local_create_service(self, hosts, host_name, service):\n        \"\"\"Create a new service based on a host_name and service instance.\n\n        :param hosts: The hosts items instance.\n        :type hosts: alignak.objects.host.Hosts\n        :param host_name: The host_name to create a new service.\n        :type host_name: str\n        :param service: The service to be used as template.\n        :type service: Service\n        :return: The new service created.\n        :rtype: alignak.objects.service.Service\n        \"\"\"\n        host = hosts.find_by_name(host_name.strip())\n        if host.is_excluded_for(service):\n            return None\n\n        # Creates a real service instance from the template\n        new_s = service.copy()\n        new_s.host_name = host_name\n        new_s.register = 1\n        self.add_item(new_s)\n        return new_s"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexplode services from templates.", "response": "def explode_services_from_templates(self, hosts, service_template):\n        \"\"\"\n        Explodes services from templates. All hosts holding the specified\n        templates are bound with the service.\n\n        :param hosts: The hosts container.\n        :type hosts: alignak.objects.host.Hosts\n        :param service_template: The service to explode.\n        :type service_template: alignak.objects.service.Service\n        :return: None\n        \"\"\"\n        hname = getattr(service_template, \"host_name\", None)\n        if not hname:\n            logger.debug(\"Service template %s is declared without an host_name\",\n                         service_template.get_name())\n            return\n\n        logger.debug(\"Explode services %s for the host: %s\", service_template.get_name(), hname)\n\n        # Now really create the services\n        if is_complex_expr(hname):\n            hnames = self.evaluate_hostgroup_expression(\n                hname.strip(), hosts, hosts.templates, look_in='templates')\n            for name in hnames:\n                self._local_create_service(hosts, name, service_template)\n        else:\n            hnames = [n.strip() for n in hname.split(',') if n.strip()]\n            for hname in hnames:\n                for name in hosts.find_hosts_that_use_template(hname):\n                    self._local_create_service(hosts, name, service_template)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef explode_services_duplicates(self, hosts, service):\n        hname = getattr(service, \"host_name\", None)\n        if hname is None:\n            return\n\n        # the generator case, we must create several new services\n        # we must find our host, and get all key:value we need\n        host = hosts.find_by_name(hname.strip())\n        if host is None:\n            service.add_error('Error: The hostname %s is unknown for the service %s!'\n                              % (hname, service.get_name()))\n            return\n\n        # Duplicate services\n        for new_s in service.duplicate(host):\n            if host.is_excluded_for(new_s):\n                continue\n            # Adds concrete instance\n            self.add_item(new_s)", "response": "Explode services holding a duplicate_foreach clause."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_service_into_servicegroups(service, servicegroups):\n        if hasattr(service, 'service_description'):\n            sname = service.service_description\n            shname = getattr(service, 'host_name', '')\n            if hasattr(service, 'servicegroups'):\n                # Todo: See if we can remove this if\n                if isinstance(service.servicegroups, list):\n                    sgs = service.servicegroups\n                else:\n                    sgs = service.servicegroups.split(',')\n                for servicegroup in sgs:\n                    servicegroups.add_member([shname, sname], servicegroup.strip())", "response": "Registers a service into the servicegroups declared in its\n        servicegroups attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a service dependencies.", "response": "def register_service_dependencies(service, servicedependencies):\n        \"\"\"\n        Registers a service dependencies.\n\n        :param service: The service to register\n        :type service:\n        :param servicedependencies: The servicedependencies container\n        :type servicedependencies:\n        :return: None\n        \"\"\"\n        # We explode service_dependencies into Servicedependency\n        # We just create serviceDep with goods values (as STRING!),\n        # the link pass will be done after\n        sdeps = [d.strip() for d in getattr(service, \"service_dependencies\", [])]\n        # %2=0 are for hosts, !=0 are for service_description\n        i = 0\n        hname = ''\n        for elt in sdeps:\n            if i % 2 == 0:  # host\n                hname = elt\n            else:  # description\n                desc = elt\n                # we can register it (service) (depend on) -> (hname, desc)\n                # If we do not have enough data for service, it'service no use\n                if hasattr(service, 'service_description') and hasattr(service, 'host_name'):\n                    if hname == '':\n                        hname = service.host_name\n                    servicedependencies.add_service_dependency(\n                        service.host_name, service.service_description, hname, desc)\n            i += 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexplode the hosts hostgroups contactgroups and servicedependencies into a new object.", "response": "def explode(self, hosts, hostgroups, contactgroups, servicegroups, servicedependencies):\n        # pylint: disable=too-many-locals\n        \"\"\"\n        Explodes services, from host, hostgroups, contactgroups, servicegroups and dependencies.\n\n        :param hosts: The hosts container\n        :type hosts: [alignak.object.host.Host]\n        :param hostgroups: The hosts goups container\n        :type hostgroups: [alignak.object.hostgroup.Hostgroup]\n        :param contactgroups: The contacts goups container\n        :type contactgroups: [alignak.object.contactgroup.Contactgroup]\n        :param servicegroups: The services goups container\n        :type servicegroups: [alignak.object.servicegroup.Servicegroup]\n        :param servicedependencies: The services dependencies container\n        :type servicedependencies: [alignak.object.servicedependency.Servicedependency]\n        :return: None\n        \"\"\"\n        # Then for every service create a copy of the service with just the host\n        # because we are adding services, we can't just loop in it\n        itemkeys = list(self.items.keys())\n        for s_id in itemkeys:\n            serv = self.items[s_id]\n            # items::explode_host_groups_into_hosts\n            # take all hosts from our hostgroup_name into our host_name property\n            self.explode_host_groups_into_hosts(serv, hosts, hostgroups)\n\n            # items::explode_contact_groups_into_contacts\n            # take all contacts from our contact_groups into our contact property\n            self.explode_contact_groups_into_contacts(serv, contactgroups)\n\n            hnames = getattr(serv, \"host_name\", '')\n            hnames = list(set([n.strip() for n in hnames.split(',') if n.strip()]))\n            # hnames = strip_and_uniq(hnames)\n            # We will duplicate if we have multiple host_name\n            # or if we are a template (so a clean service)\n            if len(hnames) == 1:\n                self.index_item(serv)\n            else:\n                if len(hnames) >= 2:\n                    self.explode_services_from_hosts(hosts, serv, hnames)\n                # Delete expanded source service, even if some errors exist\n                self.remove_item(serv)\n\n        for s_id in self.templates:\n            template = self.templates[s_id]\n            self.explode_contact_groups_into_contacts(template, contactgroups)\n            self.explode_services_from_templates(hosts, template)\n\n        # Explode services that have a duplicate_foreach clause\n        duplicates = [serv.uuid for serv in self if getattr(serv, 'duplicate_foreach', '')]\n        for s_id in duplicates:\n            serv = self.items[s_id]\n            self.explode_services_duplicates(hosts, serv)\n            if not serv.configuration_errors:\n                self.remove_item(serv)\n\n        to_remove = []\n        for service in self:\n            host = hosts.find_by_name(service.host_name)\n            if host and host.is_excluded_for(service):\n                to_remove.append(service)\n        for service in to_remove:\n            self.remove_item(service)\n\n        # Servicegroups property need to be fulfill for got the information\n        # And then just register to this service_group\n        for serv in self:\n            self.register_service_into_servicegroups(serv, servicegroups)\n            self.register_service_dependencies(serv, servicedependencies)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_eligible(self, timestamp, status, notif_number, in_notif_time, interval, escal_period):\n        # pylint: disable=too-many-return-statements\n        \"\"\"Check if the escalation is eligible (notification is escalated or not)\n\n        Escalation is NOT eligible in ONE of the following condition is fulfilled::\n\n        * escalation is not time based and notification number not in range\n          [first_notification;last_notification] (if last_notif == 0, it's infinity)\n        * escalation is time based and notification time not in range\n          [first_notification_time;last_notification_time] (if last_notif_time == 0, it's infinity)\n        * status does not matches escalation_options ('WARNING' <=> 'w' ...)\n        * escalation_period is not legit for this time (now usually)\n\n        :param timestamp: timestamp to check if timeperiod is valid\n        :type timestamp: int\n        :param status: item status (one of the small_states key)\n        :type status: str\n        :param notif_number: current notification number\n        :type notif_number: int\n        :param in_notif_time: current notification time\n        :type in_notif_time: int\n        :param interval: time interval length\n        :type interval: int\n        :return: True if no condition has been fulfilled, otherwise False\n        :rtype: bool\n        \"\"\"\n        short_states = {\n            u'WARNING': 'w', u'UNKNOWN': 'u', u'CRITICAL': 'c',\n            u'RECOVERY': 'r', u'FLAPPING': 'f', u'DOWNTIME': 's',\n            u'DOWN': 'd', u'UNREACHABLE': 'x', u'OK': 'o', u'UP': 'o'\n        }\n\n        # If we are not time based, we check notification numbers:\n        if not self.time_based:\n            # Begin with the easy cases\n            if notif_number < self.first_notification:\n                return False\n\n            # self.last_notification = 0 mean no end\n            if self.last_notification and notif_number > self.last_notification:\n                return False\n        # Else we are time based, we must check for the good value\n        else:\n            # Begin with the easy cases\n            if in_notif_time < self.first_notification_time * interval:\n                return False\n\n            if self.last_notification_time and \\\n                    in_notif_time > self.last_notification_time * interval:\n                return False\n\n        # If our status is not good, we bail out too\n        if status in short_states and short_states[status] not in self.escalation_options:\n            return False\n\n        # Maybe the time is not in our escalation_period\n        if escal_period is not None and not escal_period.is_time_valid(timestamp):\n            return False\n\n        # Ok, I do not see why not escalade. So it's True :)\n        return True", "response": "Check if the escalation is eligible for the current item."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the next time for the next notification for the escalation", "response": "def get_next_notif_time(self, t_wished, status, creation_time, interval, escal_period):\n        \"\"\"Get the next notification time for the escalation\n        Only legit for time based escalation\n\n        :param t_wished: time we would like to send a new notification (usually now)\n        :type t_wished:\n        :param status: status of the host or service\n        :type status:\n        :param creation_time: time the notification was created\n        :type creation_time:\n        :param interval: time interval length\n        :type interval: int\n        :return: timestamp for next notification or None\n        :rtype: int | None\n        \"\"\"\n        short_states = {u'WARNING': 'w', u'UNKNOWN': 'u', u'CRITICAL': 'c',\n                        u'RECOVERY': 'r', u'FLAPPING': 'f', u'DOWNTIME': 's',\n                        u'DOWN': 'd', u'UNREACHABLE': 'u', u'OK': 'o', u'UP': 'o'}\n\n        # If we are not time based, we bail out!\n        if not self.time_based:\n            return None\n\n        # Check if we are valid\n        if status in short_states and short_states[status] not in self.escalation_options:\n            return None\n\n        # Look for the min of our future validity\n        start = self.first_notification_time * interval + creation_time\n\n        # If we are after the classic next time, we are not asking for a smaller interval\n        if start > t_wished:\n            return None\n\n        # Maybe the time we found is not a valid one....\n        if escal_period is not None and not escal_period.is_time_valid(start):\n            return None\n\n        # Ok so I ask for my start as a possibility for the next notification time\n        return start"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if this object configuration is correct.", "response": "def is_correct(self):\n        \"\"\"Check if this object configuration is correct ::\n\n        * Check our own specific properties\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        state = True\n\n        # Internal checks before executing inherited function...\n\n        # If we got the _time parameters, we are time based. Unless, we are not :)\n        if hasattr(self, 'first_notification_time') or hasattr(self, 'last_notification_time'):\n            self.time_based = True\n\n        # Ok now we manage special cases...\n        if not hasattr(self, 'contacts') and not hasattr(self, 'contact_groups'):\n            self.add_error('%s: I do not have contacts nor contact_groups' % (self.get_name()))\n            state = False\n\n        # If time_based or not, we do not check all properties\n        if self.time_based:\n            if not hasattr(self, 'first_notification_time'):\n                self.add_error('%s: I do not have first_notification_time' % (self.get_name()))\n                state = False\n            if not hasattr(self, 'last_notification_time'):\n                self.add_error('%s: I do not have last_notification_time' % (self.get_name()))\n                state = False\n        else:  # we check classical properties\n            if not hasattr(self, 'first_notification'):\n                self.add_error('%s: I do not have first_notification' % (self.get_name()))\n                state = False\n            if not hasattr(self, 'last_notification'):\n                self.add_error('%s: I do not have last_notification' % (self.get_name()))\n                state = False\n\n        # Change the special_properties definition according to time_based ...\n        save_special_properties = self.special_properties\n        if self.time_based:\n            self.special_properties = self.special_properties_time_based\n\n        state_parent = super(Escalation, self).is_correct()\n\n        if self.time_based:\n            self.special_properties = save_special_properties\n\n        return state_parent and state"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates link between objects :: timeperiods contacts services hosts", "response": "def linkify(self, timeperiods, contacts, services, hosts):\n        \"\"\"Create link between objects::\n\n         * escalation -> host\n         * escalation -> service\n         * escalation -> timeperiods\n         * escalation -> contact\n\n        :param timeperiods: timeperiods to link\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param contacts: contacts to link\n        :type contacts: alignak.objects.contact.Contacts\n        :param services: services to link\n        :type services: alignak.objects.service.Services\n        :param hosts: hosts to link\n        :type hosts: alignak.objects.host.Hosts\n        :return: None\n        \"\"\"\n        self.linkify_with_timeperiods(timeperiods, 'escalation_period')\n        self.linkify_with_contacts(contacts)\n        self.linkify_es_by_s(services)\n        self.linkify_es_by_h(hosts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlinks each escalation object into service. escalation attribute.", "response": "def linkify_es_by_s(self, services):\n        \"\"\"Add each escalation object into service.escalation attribute\n\n        :param services: service list, used to look for a specific service\n        :type services: alignak.objects.service.Services\n        :return: None\n        \"\"\"\n        for escalation in self:\n            # If no host, no hope of having a service\n            if not hasattr(escalation, 'host_name'):\n                continue\n\n            es_hname, sdesc = escalation.host_name, escalation.service_description\n            if not es_hname.strip() or not sdesc.strip():\n                continue\n\n            for hname in strip_and_uniq(es_hname.split(',')):\n                if sdesc.strip() == '*':\n                    slist = services.find_srvs_by_hostname(hname)\n                    if slist is not None:\n                        slist = [services[serv] for serv in slist]\n                        for serv in slist:\n                            serv.escalations.append(escalation.uuid)\n                else:\n                    for sname in strip_and_uniq(sdesc.split(',')):\n                        serv = services.find_srv_by_name_and_hostname(hname, sname)\n                        if serv is not None:\n                            serv.escalations.append(escalation.uuid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef linkify_es_by_h(self, hosts):\n        for escal in self:\n            # If no host, no hope of having a service\n            if (not hasattr(escal, 'host_name') or escal.host_name.strip() == '' or\n                    (hasattr(escal, 'service_description')\n                     and escal.service_description.strip() != '')):\n                continue\n            # I must be NOT a escalation on for service\n            for hname in strip_and_uniq(escal.host_name.split(',')):\n                host = hosts.find_by_name(hname)\n                if host is not None:\n                    host.escalations.append(escal.uuid)", "response": "Link each escalation object into host. escalation attribute"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloops over all escalation and explode hostsgroups in hostgroup and contactgroups in contacts", "response": "def explode(self, hosts, hostgroups, contactgroups):\n        \"\"\"Loop over all escalation and explode hostsgroups in host\n        and contactgroups in contacts\n\n        Call Item.explode_host_groups_into_hosts and Item.explode_contact_groups_into_contacts\n\n        :param hosts: host list to explode\n        :type hosts: alignak.objects.host.Hosts\n        :param hostgroups: hostgroup list to explode\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :param contactgroups: contactgroup list to explode\n        :type contactgroups: alignak.objects.contactgroup.Contactgroups\n        :return: None\n        \"\"\"\n        for i in self:\n            # items::explode_host_groups_into_hosts\n            # take all hosts from our hostgroup_name into our host_name property\n            self.explode_host_groups_into_hosts(i, hosts, hostgroups)\n\n            # items::explode_contact_groups_into_contacts\n            # take all contacts from our contact_groups into our contact property\n            self.explode_contact_groups_into_contacts(i, contactgroups)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget hosts of this group and all hostgroups that have the same name.", "response": "def get_hosts_by_explosion(self, hostgroups):\n        # pylint: disable=access-member-before-definition\n        \"\"\"\n        Get hosts of this group\n\n        :param hostgroups: Hostgroup object\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :return: list of hosts of this group\n        :rtype: list\n        \"\"\"\n        # First we tag the hg so it will not be explode\n        # if a son of it already call it\n        self.already_exploded = True\n\n        # Now the recursive part\n        # rec_tag is set to False every HG we explode\n        # so if True here, it must be a loop in HG\n        # calls... not GOOD!\n        if self.rec_tag:\n            logger.error(\"[hostgroup::%s] got a loop in hostgroup definition\", self.get_name())\n            return self.get_hosts()\n\n        # Ok, not a loop, we tag it and continue\n        self.rec_tag = True\n\n        hg_mbrs = self.get_hostgroup_members()\n        for hg_mbr in hg_mbrs:\n            hostgroup = hostgroups.find_by_name(hg_mbr.strip())\n            if hostgroup is not None:\n                value = hostgroup.get_hosts_by_explosion(hostgroups)\n                if value is not None:\n                    self.add_members(value)\n\n        return self.get_hosts()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_member(self, host_name, hostgroup_name):\n        hostgroup = self.find_by_name(hostgroup_name)\n        if not hostgroup:\n            hostgroup = Hostgroup({'hostgroup_name': hostgroup_name,\n                                   'alias': hostgroup_name,\n                                   'members': host_name})\n            self.add(hostgroup)\n        else:\n            hostgroup.add_members(host_name)", "response": "Add a host string to a hostgroup member\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all members of a group which name is given in parameter", "response": "def get_members_of_group(self, gname):\n        \"\"\"Get all members of a group which name is given in parameter\n\n        :param gname: name of the group\n        :type gname: str\n        :return: list of the hosts in the group\n        :rtype: list[alignak.objects.host.Host]\n        \"\"\"\n        hostgroup = self.find_by_name(gname)\n        if hostgroup:\n            return hostgroup.get_hosts()\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlink hostgroups with hosts and realms", "response": "def linkify(self, hosts=None, realms=None, forced_realms_hostgroups=True):\n        \"\"\"Link hostgroups with hosts and realms\n\n        :param hosts: all Hosts\n        :type hosts: alignak.objects.host.Hosts\n        :param realms: all Realms\n        :type realms: alignak.objects.realm.Realms\n        :return: None\n        \"\"\"\n        self.linkify_hostgroups_hosts(hosts)\n        self.linkify_hostgroups_realms_hosts(realms, hosts, forced_realms_hostgroups)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef linkify_hostgroups_hosts(self, hosts):\n        for hostgroup in self:\n            members = hostgroup.get_hosts()\n            # The new members identifiers list\n            new_members = []\n            for member in members:\n                # member is an host name\n                member = member.strip()\n                if not member:  # void entry, skip this\n                    continue\n\n                if member == '*':\n                    # All the hosts identifiers list\n                    new_members.extend(list(hosts.items.keys()))\n                else:\n                    host = hosts.find_by_name(member)\n                    if host is not None:\n                        new_members.append(host.uuid)\n                        if hostgroup.uuid not in host.hostgroups:\n                            host.hostgroups.append(hostgroup.uuid)\n                    else:\n                        hostgroup.add_unknown_members(member)\n\n            # Make members unique\n            new_members = list(set(new_members))\n\n            # We find the id, we replace the names\n            hostgroup.replace_members(new_members)", "response": "We just search for each hostgroup and replace the names by the found identifiers"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlinks between hostgroups and hosts", "response": "def linkify_hostgroups_realms_hosts(self, realms, hosts, forced_realms_hostgroups=True):\n        # pylint: disable=too-many-locals, too-many-nested-blocks, too-many-branches\n        \"\"\"Link between an hostgroup and a realm is already done in the configuration parsing\n        function that defines and checks the default satellites, realms, hosts and hosts groups\n        consistency.\n\n        This function will only raise some alerts if hosts groups and hosts that are contained\n        do not belong the same realm !\n\n        :param realms: object Realms\n        :type realms: alignak.objects.realm.Realms\n        :param hosts: object Realms\n        :type hosts: alignak.objects.host.Hosts\n        :return: None\n        \"\"\"\n        logger.info(\"Hostgroups / hosts / realms relation\")\n        for hostgroup in self:\n            hostgroup_realm_name = hostgroup.realm\n            if hostgroup.realm not in realms:\n                realm = realms.find_by_name(hostgroup.realm)\n                if not realm:\n                    continue\n                hostgroup.realm = realm.uuid\n            else:\n                hostgroup_realm_name = realms[hostgroup.realm].get_name()\n            logger.info(\"- hg: %s in the realm: %s \",\n                        hostgroup.get_name(),\n                        hostgroup_realm_name + (\" (*)\" if hostgroup.got_default_realm else ''))\n\n            hostgroup_hosts_errors = []\n            hostgroup_new_realm_name = None\n            hostgroup_new_realm_failed = False\n            for host_uuid in hostgroup:\n                if host_uuid not in hosts:\n                    continue\n                host = hosts[host_uuid]\n                host_realm_name = host.realm\n                if host.realm not in realms:\n                    host_realm = realms.find_by_name(host.realm)\n                    if not host_realm:\n                        # Host realm is unknown, an error will be raised elsewhere!\n                        continue\n                else:\n                    host_realm_name = realms[host.realm].get_name()\n\n                logger.info(\"  host %s is in the realm: %s\",\n                            host.get_name(),\n                            host_realm_name + (\" (*)\" if host.got_default_realm else ''))\n\n                if host.got_default_realm:\n                    # If the host got a default realm it means that no realm is specifically\n                    # declared for this host. Thus it can inherit its realm from the one of its\n                    # hostgroup :)\n                    logger.debug(\"- apply the realm %s to the host %s from a hostgroup rule (%s)\",\n                                 hostgroup_realm_name, host.get_name(), hostgroup.get_name())\n                    host.realm = hostgroup.realm\n                else:\n                    # If the host has a realm that is specifically declared then it must the same\n                    # as its hostgroup one!\n                    if host.realm != hostgroup.realm:\n                        # If the hostgroup had a specified realm\n                        if not hostgroup.got_default_realm:\n                            # raise an error !\n                            hostgroup.add_error(\n                                \"host %s (realm: %s) is not in the same realm than its \"\n                                \"hostgroup %s (realm: %s)\"\n                                % (host.get_name(), host_realm_name,\n                                   hostgroup.get_name(), hostgroup_realm_name))\n                        else:\n                            # The hosts group had no realm set, it got the default All realm\n                            if forced_realms_hostgroups:\n                                # Temporary log an error...\n                                hostgroup_hosts_errors.append(\n                                    \"host %s (realm: %s) is not in the same realm than its \"\n                                    \"hostgroup %s (realm: %s)\"\n                                    % (host.get_name(), host_realm_name,\n                                       hostgroup.get_name(), hostgroup_realm_name))\n\n                                if not hostgroup_new_realm_name or \\\n                                        hostgroup_new_realm_name == host_realm_name:\n                                    # Potential new host group realm\n                                    hostgroup_new_realm_name = host_realm_name\n                                else:\n                                    # It still exists a candidate realm for the hostgroup,\n                                    # raise an error !\n                                    hostgroup.add_error(\"hostgroup %s got the default realm but \"\n                                                        \"it has some hosts that are from different \"\n                                                        \"realms: %s and %s. The defined realm \"\n                                                        \"cannot be adjusted!\"\n                                                        % (hostgroup.get_name(),\n                                                           hostgroup_new_realm_name,\n                                                           host_realm_name))\n                                    hostgroup_new_realm_failed = True\n                                    break\n                            else:\n                                # I tolerate some hosts from different realms in an hostgroup\n                                # that is in the default realm\n                                # Temporary log an error...\n                                hostgroup_hosts_errors.append(\n                                    \"host %s (realm: %s) is not in the same realm as its \"\n                                    \"hostgroup %s (realm: %s)\"\n                                    % (host.get_name(), host_realm_name,\n                                       hostgroup.get_name(), hostgroup_realm_name))\n\n            if not forced_realms_hostgroups:\n                for error in hostgroup_hosts_errors:\n                    # hostgroup.add_warning(error)\n                    logger.info(error)\n            else:\n                if hostgroup_new_realm_name is None:\n                    # Do not change the hostgroup realm, it is not possible,\n                    # so raise the host individual errors!\n                    for error in hostgroup_hosts_errors:\n                        hostgroup.add_error(error)\n                elif hostgroup_new_realm_name:\n                    if not hostgroup_new_realm_failed:\n                        # Change the hostgroup realm to suit its hosts\n                        hostgroup.add_warning(\"hostgroup %s gets the realm of its hosts: %s\"\n                                              % (hostgroup.get_name(), hostgroup_new_realm_name))\n                        hostgroup_new_realm = realms.find_by_name(hostgroup_new_realm_name)\n                        hostgroup.realm = hostgroup_new_realm.uuid"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef explode(self):\n        # We do not want a same hostgroup to be exploded again and again\n        # so we tag it\n        for tmp_hg in list(self.items.values()):\n            tmp_hg.already_exploded = False\n\n        for hostgroup in list(self.items.values()):\n            if hostgroup.already_exploded:\n                continue\n\n            # get_hosts_by_explosion is a recursive\n            # function, so we must tag hg so we do not loop\n            for tmp_hg in list(self.items.values()):\n                tmp_hg.rec_tag = False\n            hostgroup.get_hosts_by_explosion(self)\n\n        # We clean the tags\n        for tmp_hg in list(self.items.values()):\n            if hasattr(tmp_hg, 'rec_tag'):\n                del tmp_hg.rec_tag\n            del tmp_hg.already_exploded", "response": "Explode the items with the items in the items_by_explosion method."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps to start the CherryPy server", "response": "def run(self):\n        \"\"\"Wrapper to start the CherryPy server\n\n        This function throws a PortNotFree exception if any socket error is raised.\n\n        :return: None\n        \"\"\"\n        def _started_callback():\n            \"\"\"Callback function when Cherrypy Engine is started\"\"\"\n            cherrypy.log(\"CherryPy engine started and listening...\")\n\n        self.cherrypy_thread = None\n        try:\n            cherrypy.log(\"Starting CherryPy engine on %s\" % (self.uri))\n            self.cherrypy_thread = cherrypy.engine.start_with_callback(_started_callback)\n            cherrypy.engine.block()\n            cherrypy.log(\"Exited from the engine block\")\n        except socket.error as exp:\n            raise PortNotFree(\"Error: Sorry, the HTTP server did not started correctly: error: %s\"\n                              % (str(exp)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop(self):  # pylint: disable=no-self-use\n        cherrypy.log(\"Stopping CherryPy engine (current state: %s)...\" % cherrypy.engine.state)\n        try:\n            cherrypy.engine.exit()\n        except RuntimeWarning:\n            pass\n        except SystemExit:\n            cherrypy.log('SystemExit raised: shutting down bus')\n        cherrypy.log(\"Stopped\")", "response": "Wrapper to stop the CherryPy server"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_queues(self, manager=None):\n        self.clear_queues(manager)\n        # If no Manager() object, go with classic Queue()\n        if not manager:\n            self.from_q = Queue()\n            self.to_q = Queue()\n        else:\n            self.from_q = manager.Queue()\n            self.to_q = manager.Queue()", "response": "Create the shared queues that will be used by alignak daemons and this module process."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear_queues(self, manager):\n        for queue in (self.to_q, self.from_q):\n            if queue is None:\n                continue\n            # If we got no manager, we directly call the clean\n            if not manager:\n                try:\n                    queue.close()\n                    queue.join_thread()\n                except AttributeError:\n                    pass\n            # else:\n            #    q._callmethod('close')\n            #    q._callmethod('join_thread')\n        self.to_q = self.from_q = None", "response": "Release the resources associated to the queues of this instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping for _main function. Catch and raise any exception occurring in the main function", "response": "def start_module(self):\n        \"\"\"Wrapper for _main function.\n        Catch and raise any exception occurring in the main function\n\n        :return: None\n        \"\"\"\n        try:\n            self._main()\n        except Exception as exp:\n            logger.exception('%s', traceback.format_exc())\n            raise Exception(exp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts the process if the module is external", "response": "def start(self, http_daemon=None):  # pylint: disable=unused-argument\n        \"\"\"Actually restart the process if the module is external\n        Try first to stop the process and create a new Process instance\n        with target start_module.\n        Finally start process.\n\n        :param http_daemon: Not used here but can be used in other modules\n        :type http_daemon: None | object\n        :return: None\n        \"\"\"\n\n        if not self.is_external:\n            return\n\n        if self.process:\n            self.stop_process()\n        logger.info(\"Starting external process for module %s...\", self.name)\n        proc = Process(target=self.start_module, args=(), group=None)\n\n        # Under windows we should not call start() on an object that got its process\n        # as an object, so we remove it and we set it in a earlier start\n        try:\n            del self.properties['process']\n        except KeyError:\n            pass\n\n        proc.start()\n        # We save the process data AFTER the fork()\n        self.process = proc\n        self.properties['process'] = proc\n        logger.info(\"%s is now started (pid=%d)\", self.name, proc.pid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef kill(self):\n\n        logger.info(\"Killing external module (pid=%d) for module %s...\",\n                    self.process.pid, self.name)\n        if os.name == 'nt':\n            self.process.terminate()\n        else:\n            self.process.terminate()\n            # Wait for 10 seconds before killing the process abruptly\n            self.process.join(timeout=KILL_TIME)\n            # You do not let me another choice guy...\n            if self.process.is_alive():\n                logger.warning(\"%s is still living %d seconds after a normal kill, \"\n                               \"I help it to die\", self.name, KILL_TIME)\n                os.kill(self.process.pid, signal.SIGKILL)\n                self.process.join(1)\n                if self.process.is_alive():\n                    logger.error(\"%s still living after brutal kill, I leave it.\", self.name)\n            logger.info(\"External module killed\")", "response": "Kills the external module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop_process(self):\n        if not self.process:\n            return\n\n        logger.info(\"I'm stopping module %r (pid=%d)\", self.name, self.process.pid)\n        self.kill()\n        # Clean inner process reference\n        self.process = None", "response": "Request the module process to stop and release it\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrequest the module to manage the given brok.", "response": "def manage_brok(self, brok):\n        \"\"\"Request the module to manage the given brok.\n        There are a lot of different possible broks to manage. The list is defined\n        in the Brok class.\n\n        An internal module may redefine this function or, easier, define only the function\n        for the brok it is interested with. Hence a module interested in the `service_check_result`\n        broks will only need to define a function named as `manage_service_check_result_brok`\n\n        :param brok:\n        :type brok:\n        :return:\n        :rtype:\n        \"\"\"\n\n        manage = getattr(self, 'manage_' + brok.type + '_brok', None)\n        if not manage:\n            return False\n\n        # Be sure the brok is prepared before calling the function\n        brok.prepare()\n        return manage(brok)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the signal handler to manage_signal", "response": "def set_signal_handler(self, sigs=None):\n        \"\"\"Set the signal handler to manage_signal (defined in this class)\n\n        Only set handlers for:\n        - signal.SIGTERM, signal.SIGINT\n        - signal.SIGUSR1, signal.SIGUSR2\n        - signal.SIGHUP\n\n        :return: None\n        \"\"\"\n        if sigs is None:\n            sigs = (signal.SIGTERM, signal.SIGINT, signal.SIGUSR1, signal.SIGUSR2, signal.SIGHUP)\n\n        func = self.manage_signal\n        if os.name == \"nt\":  # pragma: no cover, no Windows implementation currently\n            try:\n                import win32api\n                win32api.SetConsoleCtrlHandler(func, True)\n            except ImportError:\n                version = \".\".join([str(i) for i in os.sys.version_info[:2]])\n                raise Exception(\"pywin32 not installed for Python \" + version)\n        else:\n            for sig in sigs:\n                signal.signal(sig, func)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _main(self):\n        self.set_proctitle(self.name)\n        self.set_signal_handler()\n\n        logger.info(\"process for module %s is now running (pid=%d)\", self.name, os.getpid())\n\n        # Will block here!\n        try:\n            self.main()\n        except (IOError, EOFError):\n            pass\n            # logger.warning('[%s] EOF exception: %s', self.name, traceback.format_exc())\n        except Exception as exp:  # pylint: disable=broad-except\n            logger.exception('main function exception: %s', exp)\n\n        self.do_stop()\n\n        logger.info(\"process for module %s is now exiting (pid=%d)\", self.name, os.getpid())\n        exit()", "response": "Main method. Only used by external modules."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to read a file descriptor in a non blocking mode and return the data read from it.", "response": "def no_block_read(output):\n    \"\"\"Try to read a file descriptor in a non blocking mode\n\n    If the fcntl is available (unix only) we try to read in a\n    asynchronous mode, so we won't block the PIPE at 64K buffer\n    (deadlock...)\n\n    :param output: file or socket to read from\n    :type output: file\n    :return: data read from fd\n    :rtype: str\n    \"\"\"\n    _buffer = \"\"\n    if not fcntl:\n        return _buffer\n\n    o_fd = output.fileno()\n    o_fl = fcntl.fcntl(o_fd, fcntl.F_GETFL)\n    fcntl.fcntl(o_fd, fcntl.F_SETFL, o_fl | os.O_NONBLOCK)\n    try:\n        _buffer = output.read()\n    except Exception:  # pylint: disable=broad-except\n        pass\n\n    return _buffer"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the environment variables that are not already set in the local environment.", "response": "def get_local_environnement(self):\n        \"\"\"\n        Mix the environment and the environment variables into a new local\n        environment dictionary\n\n        Note: We cannot just update the global os.environ because this\n        would effect all other checks.\n\n        :return: local environment variables\n        :rtype: dict\n        \"\"\"\n        # Do not use copy.copy() here, as the resulting copy still\n        # changes the real environment (it is still a os._Environment\n        # instance).\n        local_env = os.environ.copy()\n        for local_var in self.env:\n            local_env[local_var] = self.env[local_var]\n        return local_env"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts this action command in a subprocess.", "response": "def execute(self):\n        \"\"\"Start this action command in a subprocess.\n\n        :raise: ActionError\n            'toomanyopenfiles' if too many opened files on the system\n            'no_process_launched' if arguments parsing failed\n            'process_launch_failed': if the process launch failed\n\n        :return: reference to the started process\n        :rtype: psutil.Process\n        \"\"\"\n        self.status = ACT_STATUS_LAUNCHED\n        self.check_time = time.time()\n        self.wait_time = 0.0001\n        self.last_poll = self.check_time\n\n        # Get a local env variables with our additional values\n        self.local_env = self.get_local_environnement()\n\n        # Initialize stdout and stderr.\n        self.stdoutdata = ''\n        self.stderrdata = ''\n\n        logger.debug(\"Launch command: '%s', ref: %s, timeout: %s\",\n                     self.command, self.ref, self.timeout)\n        if self.log_actions:\n            if os.environ['ALIGNAK_LOG_ACTIONS'] == 'WARNING':\n                logger.warning(\"Launch command: '%s'\", self.command)\n            else:\n                logger.info(\"Launch command: '%s'\", self.command)\n\n        return self._execute()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_outputs(self, out, max_plugins_output_length):\n        # Squeeze all output after max_plugins_output_length\n        out = out[:max_plugins_output_length]\n        # manage escaped pipes\n        out = out.replace(r'\\|', '___PROTECT_PIPE___')\n        # Then cuts by lines\n        elts = out.split('\\n')\n        # For perf data\n        elts_line1 = elts[0].split('|')\n\n        # First line before | is output, strip it\n        self.output = elts_line1[0].strip().replace('___PROTECT_PIPE___', '|')\n        try:\n            self.output = self.output.decode('utf8', 'ignore')\n        except UnicodeEncodeError:\n            pass\n        except AttributeError:\n            pass\n\n        # Init perfdata as empty\n        self.perf_data = ''\n        # After | it is perfdata, strip it\n        if len(elts_line1) > 1:\n            self.perf_data = elts_line1[1].strip().replace('___PROTECT_PIPE___', '|')\n\n        # Now manage others lines. Before the | it's long_output\n        # And after it's all perf_data, \\n joined\n        long_output = []\n        in_perfdata = False\n        for line in elts[1:]:\n            # if already in perfdata, direct append\n            if in_perfdata:\n                self.perf_data += ' ' + line.strip().replace('___PROTECT_PIPE___', '|')\n            else:  # not already in perf_data, search for the | part :)\n                elts = line.split('|', 1)\n                # The first part will always be long_output\n                long_output.append(elts[0].strip().replace('___PROTECT_PIPE___', '|'))\n                if len(elts) > 1:\n                    in_perfdata = True\n                    self.perf_data += ' ' + elts[1].strip().replace('___PROTECT_PIPE___', '|')\n\n        # long_output is all non output and performance data, joined with \\n\n        self.long_output = '\\n'.join(long_output)\n        # Get sure the performance data are stripped\n        self.perf_data = self.perf_data.strip()\n\n        logger.debug(\"Command result for '%s': %d, %s\", self.command, self.exit_status, self.output)\n\n        if self.log_actions:\n            if os.environ['ALIGNAK_LOG_ACTIONS'] == 'WARNING':\n                logger.warning(\"Check result for '%s': %d, %s\",\n                               self.command, self.exit_status, self.output)\n                if self.perf_data:\n                    logger.warning(\"Performance data for '%s': %s\", self.command, self.perf_data)\n            else:\n                logger.info(\"Check result for '%s': %d, %s\",\n                            self.command, self.exit_status, self.output)\n                if self.perf_data:\n                    logger.info(\"Performance data for '%s': %s\", self.command, self.perf_data)", "response": "Get check outputs from single output."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_finished(self, max_plugins_output_length):\n        # pylint: disable=too-many-branches\n        \"\"\"Handle action if it is finished (get stdout, stderr, exit code...)\n\n        :param max_plugins_output_length: max plugin data length\n        :type max_plugins_output_length: int\n        :return: None\n        \"\"\"\n        self.last_poll = time.time()\n\n        _, _, child_utime, child_stime, _ = os.times()\n\n        # Not yet finished...\n        if self.process.poll() is None:\n            # We must wait, but checks are variable in time so we do not wait the same\n            # for a little check or a long ping. So we do like TCP: slow start with a very\n            # shot time (0.0001 s) increased *2 but do not wait more than 0.5 s.\n            self.wait_time = min(self.wait_time * 2, 0.5)\n            now = time.time()\n            # This log is really spamming... uncomment if you really need this information :)\n            # logger.debug(\"%s - Process pid=%d is still alive\", now, self.process.pid)\n\n            # Get standard outputs in non blocking mode from the process streams\n            stdout = no_block_read(self.process.stdout)\n            stderr = no_block_read(self.process.stderr)\n\n            try:\n                self.stdoutdata += stdout.decode(\"utf-8\")\n                self.stderrdata += stderr.decode(\"utf-8\")\n            except AttributeError:\n                pass\n\n            if (now - self.check_time) > self.timeout:\n                logger.warning(\"Process pid=%d spent too much time: %.2f seconds\",\n                               self.process.pid, now - self.check_time)\n                self._in_timeout = True\n                self._kill()\n                self.status = ACT_STATUS_TIMEOUT\n                self.execution_time = now - self.check_time\n                self.exit_status = 3\n\n                if self.log_actions:\n                    if os.environ['ALIGNAK_LOG_ACTIONS'] == 'WARNING':\n                        logger.warning(\"Action '%s' exited on timeout (%d s)\",\n                                       self.command, self.timeout)\n                    else:\n                        logger.info(\"Action '%s' exited on timeout (%d s)\",\n                                    self.command, self.timeout)\n\n                # Do not keep the process objcet\n                del self.process\n\n                # Replace stdout with stderr if stdout is empty\n                self.stdoutdata = self.stdoutdata.strip()\n                if not self.stdoutdata:\n                    self.stdoutdata = self.stderrdata\n\n                # Now grep what we want in the output\n                self.get_outputs(self.stdoutdata, max_plugins_output_length)\n\n                # We can clean the useless properties now\n                del self.stdoutdata\n                del self.stderrdata\n\n                # Get the user and system time\n                _, _, n_child_utime, n_child_stime, _ = os.times()\n                self.u_time = n_child_utime - child_utime\n                self.s_time = n_child_stime - child_stime\n\n                return\n            return\n\n        logger.debug(\"Process pid=%d exited with %d\", self.process.pid, self.process.returncode)\n\n        if fcntl:\n            # Get standard outputs in non blocking mode from the process streams\n            stdout = no_block_read(self.process.stdout)\n            stderr = no_block_read(self.process.stderr)\n        else:\n            # Get standard outputs from the communicate function\n            (stdout, stderr) = self.process.communicate()\n\n        try:\n            self.stdoutdata += stdout.decode(\"utf-8\")\n        except (UnicodeDecodeError, AttributeError):\n            self.stdoutdata += stdout\n\n        try:\n            self.stderrdata += stderr.decode(\"utf-8\")\n        except (UnicodeDecodeError, AttributeError):\n            self.stderrdata += stderr\n\n        self.exit_status = self.process.returncode\n        if self.log_actions:\n            if os.environ['ALIGNAK_LOG_ACTIONS'] == 'WARNING':\n                logger.warning(\"Action '%s' exited with code %d\", self.command, self.exit_status)\n            else:\n                logger.info(\"Action '%s' exited with code %d\",\n                            self.command, self.exit_status)\n\n        # We do not need the process now\n        del self.process\n\n        # check for bad syntax in command line:\n        if (self.stderrdata.find('sh: -c: line 0: unexpected EOF') >= 0 or\n                (self.stderrdata.find('sh: -c: ') >= 0 and\n                 self.stderrdata.find(': Syntax') >= 0 or\n                 self.stderrdata.find('Syntax error: Unterminated quoted string') >= 0)):\n            logger.warning(\"Bad syntax in command line!\")\n            # Very, very ugly. But subprocess._handle_exitstatus does\n            # not see a difference between a regular \"exit 1\" and a\n            # bailing out shell. Strange, because strace clearly shows\n            # a difference. (exit_group(1) vs. exit_group(257))\n            self.stdoutdata = self.stdoutdata + self.stderrdata\n            self.exit_status = 3\n\n        # Make sure that exit code is a valid exit code\n        if self.exit_status not in VALID_EXIT_STATUS:\n            self.exit_status = 3\n\n        # Replace stdout with stderr if stdout is empty\n        self.stdoutdata = self.stdoutdata.strip()\n        if not self.stdoutdata:\n            self.stdoutdata = self.stderrdata\n\n        # Now grep what we want in the output\n        self.get_outputs(self.stdoutdata, max_plugins_output_length)\n\n        # We can clean the useless properties now\n        del self.stdoutdata\n        del self.stderrdata\n\n        self.status = ACT_STATUS_DONE\n        self.execution_time = time.time() - self.check_time\n\n        # Also get the system and user times\n        _, _, n_child_utime, n_child_stime, _ = os.times()\n        self.u_time = n_child_utime - child_utime\n        self.s_time = n_child_stime - child_stime", "response": "Handle action if process is finished."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy_shell__(self, new_i):\n        for prop in ONLY_COPY_PROP:\n            setattr(new_i, prop, getattr(self, prop))\n        return new_i", "response": "Create all attributes listed in ONLY_COPY_PROP and return new_i with these attributes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting contacts of this group and all contactgroups", "response": "def get_contacts_by_explosion(self, contactgroups):\n        # pylint: disable=access-member-before-definition\n        \"\"\"\n        Get contacts of this group\n\n        :param contactgroups: Contactgroups object, use to look for a specific one\n        :type contactgroups: alignak.objects.contactgroup.Contactgroups\n        :return: list of contact of this group\n        :rtype: list[alignak.objects.contact.Contact]\n        \"\"\"\n        # First we tag the hg so it will not be explode\n        # if a son of it already call it\n        self.already_exploded = True\n\n        # Now the recursive part\n        # rec_tag is set to False every CG we explode\n        # so if True here, it must be a loop in HG\n        # calls... not GOOD!\n        if self.rec_tag:\n            logger.error(\"[contactgroup::%s] got a loop in contactgroup definition\",\n                         self.get_name())\n            if hasattr(self, 'members'):\n                return self.members\n\n            return ''\n        # Ok, not a loop, we tag it and continue\n        self.rec_tag = True\n\n        cg_mbrs = self.get_contactgroup_members()\n        for cg_mbr in cg_mbrs:\n            contactgroup = contactgroups.find_by_name(cg_mbr.strip())\n            if contactgroup is not None:\n                value = contactgroup.get_contacts_by_explosion(contactgroups)\n                if value is not None:\n                    self.add_members(value)\n        if hasattr(self, 'members'):\n            return self.members\n\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_member(self, contact_name, contactgroup_name):\n        contactgroup = self.find_by_name(contactgroup_name)\n        if not contactgroup:\n            contactgroup = Contactgroup({'contactgroup_name': contactgroup_name,\n                                         'alias': contactgroup_name,\n                                         'members': contact_name})\n            self.add_contactgroup(contactgroup)\n        else:\n            contactgroup.add_members(contact_name)", "response": "Add a contact string to a contact group if the contact group does not exist create it"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget all members of a group which name is given in parameter", "response": "def get_members_of_group(self, gname):\n        \"\"\"Get all members of a group which name is given in parameter\n\n        :param gname: name of the group\n        :type gname: str\n        :return: list of contacts in the group\n        :rtype: list[alignak.objects.contact.Contact]\n        \"\"\"\n        contactgroup = self.find_by_name(gname)\n        if contactgroup:\n            return contactgroup.get_contacts()\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef linkify_contactgroups_contacts(self, contacts):\n        for contactgroup in self:\n            mbrs = contactgroup.get_contacts()\n\n            # The new member list, in id\n            new_mbrs = []\n            for mbr in mbrs:\n                mbr = mbr.strip()  # protect with strip at the beginning so don't care about spaces\n                if mbr == '':  # void entry, skip this\n                    continue\n                member = contacts.find_by_name(mbr)\n                # Maybe the contact is missing, if so, must be put in unknown_members\n                if member is not None:\n                    new_mbrs.append(member.uuid)\n                else:\n                    contactgroup.add_unknown_members(mbr)\n\n            # Make members uniq\n            new_mbrs = list(set(new_mbrs))\n\n            # We find the id, we replace the names\n            contactgroup.replace_members(new_mbrs)", "response": "Link the contacts with contactgroups"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef explode(self):\n        # We do not want a same hg to be explode again and again\n        # so we tag it\n        for tmp_cg in list(self.items.values()):\n            tmp_cg.already_exploded = False\n\n        for contactgroup in list(self.items.values()):\n            if contactgroup.already_exploded:\n                continue\n\n            # get_contacts_by_explosion is a recursive\n            # function, so we must tag hg so we do not loop\n            for tmp_cg in list(self.items.values()):\n                tmp_cg.rec_tag = False\n            contactgroup.get_contacts_by_explosion(self)\n\n        # We clean the tags\n        for tmp_cg in list(self.items.values()):\n            if hasattr(tmp_cg, 'rec_tag'):\n                del tmp_cg.rec_tag\n            del tmp_cg.already_exploded", "response": "Explode the items with the members of the contacts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the return data from an event handler.", "response": "def get_return_from(self, e_handler):\n        \"\"\"Setter of the following attributes::\n\n        * exit_status\n        * output\n        * long_output\n        * check_time\n        * execution_time\n        * perf_data\n\n        :param e_handler: event handler to get data from\n        :type e_handler: alignak.eventhandler.EventHandler\n        :return: None\n        \"\"\"\n        for prop in ['exit_status', 'output', 'long_output', 'check_time', 'execution_time',\n                     'perf_data']:\n            setattr(self, prop, getattr(e_handler, prop))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a flapping sample and keep cls. flap_history", "response": "def add_flapping_change(self, sample):\n        \"\"\"Add a flapping sample and keep cls.flap_history samples\n\n        :param sample: Sample to add\n        :type sample: bool\n        :return: None\n        \"\"\"\n        cls = self.__class__\n\n        # If this element is not in flapping check, or\n        # the flapping is globally disable, bailout\n        if not self.flap_detection_enabled or not cls.enable_flap_detection:\n            return\n\n        self.flapping_changes.append(sample)\n\n        # Keep just 20 changes (global flap_history value)\n        flap_history = cls.flap_history\n\n        if len(self.flapping_changes) > flap_history:\n            self.flapping_changes.pop(0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the flapping state of the given notif_period.", "response": "def update_flapping(self, notif_period, hosts, services):\n        \"\"\"Compute the sample list (self.flapping_changes) and determine\n        whether the host/service is flapping or not\n\n        :param notif_period: notification period object for this host/service\n        :type notif_period: alignak.object.timeperiod.Timeperiod\n        :param hosts: Hosts objects, used to create notification if necessary\n        :type hosts: alignak.objects.host.Hosts\n        :param services: Services objects, used to create notification if necessary\n        :type services: alignak.objects.service.Services\n        :return: None\n        :rtype: Nonetype\n        \"\"\"\n        flap_history = self.__class__.flap_history\n        # We compute the flapping change in %\n        res = 0.0\n        i = 0\n        for has_changed in self.flapping_changes:\n            i += 1\n            if has_changed:\n                res += i * (1.2 - 0.8) / flap_history + 0.8\n        res = res / flap_history\n        res *= 100\n\n        # We can update our value\n        self.percent_state_change = res\n\n        # Look if we are full in our states, because if not\n        # the value is not accurate\n        is_full = len(self.flapping_changes) >= flap_history\n\n        # Now we get the low_flap_threshold and high_flap_threshold values\n        # They can be from self, or class\n        (low_flap_threshold, high_flap_threshold) = (self.low_flap_threshold,\n                                                     self.high_flap_threshold)\n        # TODO: no more useful because a default value is defined, but is it really correct?\n        if low_flap_threshold == -1:  # pragma: no cover, never used\n            cls = self.__class__\n            low_flap_threshold = cls.global_low_flap_threshold\n        if high_flap_threshold == -1:  # pragma: no cover, never used\n            cls = self.__class__\n            high_flap_threshold = cls.global_high_flap_threshold\n\n        # Now we check is flapping change, but only if we got enough\n        # states to look at the value accuracy\n        if self.is_flapping and res < low_flap_threshold and is_full:\n            self.is_flapping = False\n            # We also raise a log entry\n            self.raise_flapping_stop_log_entry(res, low_flap_threshold)\n            # and a notification\n            self.remove_in_progress_notifications(master=True)\n            self.create_notifications('FLAPPINGSTOP', notif_period, hosts, services)\n            # And update our status for modules\n            self.broks.append(self.get_update_status_brok())\n\n        if not self.is_flapping and res >= high_flap_threshold and is_full:\n            self.is_flapping = True\n            # We also raise a log entry\n            self.raise_flapping_start_log_entry(res, high_flap_threshold)\n            # and a notification\n            self.remove_in_progress_notifications(master=True)\n            self.create_notifications('FLAPPINGSTART', notif_period, hosts, services)\n            # And update our status for modules\n            self.broks.append(self.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_attempt(self):\n        self.attempt += 1\n        self.attempt = min(self.attempt, self.max_check_attempts)", "response": "Add an attempt when a object is a non - ok state\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the item is in a fresh state.", "response": "def do_check_freshness(self, hosts, services, timeperiods, macromodulations, checkmodulations,\n                           checks, when):\n        # pylint: disable=too-many-nested-blocks, too-many-branches\n        \"\"\"Check freshness and schedule a check now if necessary.\n\n        This function is called by the scheduler if Alignak is configured to check the freshness.\n\n        It is called for hosts that have the freshness check enabled if they are only\n        passively checked.\n\n        It is called for services that have the freshness check enabled if they are only\n        passively checked and if their depending host is not in a freshness expired state\n        (freshness_expiry = True).\n\n        A log is raised when the freshess expiry is detected and the item is set as\n        freshness_expiry.\n\n        :param hosts: hosts objects, used to launch checks\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used launch checks\n        :type services: alignak.objects.service.Services\n        :param timeperiods: Timeperiods objects, used to get check_period\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param macromodulations: Macro modulations objects, used in commands (notif, check)\n        :type macromodulations: alignak.objects.macromodulation.Macromodulations\n        :param checkmodulations: Checkmodulations objects, used to change check command if necessary\n        :type checkmodulations: alignak.objects.checkmodulation.Checkmodulations\n        :param checks: checks dict, used to get checks_in_progress for the object\n        :type checks: dict\n        :return: A check or None\n        :rtype: None | object\n        \"\"\"\n        now = when\n\n        # Before, check if class (host or service) have check_freshness OK\n        # Then check if item want freshness, then check freshness\n        cls = self.__class__\n        if not self.in_checking and self.freshness_threshold and not self.freshness_expired:\n            # logger.debug(\"Checking freshness for %s, last state update: %s, now: %s.\",\n            #              self.get_full_name(), self.last_state_update, now)\n            if os.getenv('ALIGNAK_LOG_CHECKS', None):\n                logger.info(\"--ALC-- -> checking freshness for: %s\", self.get_full_name())\n            # If we never checked this item, we begin the freshness period\n            if not self.last_state_update:\n                self.last_state_update = int(now)\n            if self.last_state_update < now - \\\n                    (self.freshness_threshold + cls.additional_freshness_latency):\n                timeperiod = timeperiods[self.check_period]\n                if timeperiod is None or timeperiod.is_time_valid(now):\n                    # Create a new check for the scheduler\n                    chk = self.launch_check(now, hosts, services, timeperiods,\n                                            macromodulations, checkmodulations, checks)\n                    if not chk:\n                        logger.warning(\"No raised freshness check for: %s\", self)\n                        return None\n                    chk.freshness_expiry_check = True\n                    chk.check_time = time.time()\n                    chk.output = \"Freshness period expired: %s\" % (\n                        datetime.utcfromtimestamp(int(chk.check_time)).strftime(\n                            \"%Y-%m-%d %H:%M:%S %Z\"))\n                    if self.my_type == 'host':\n                        if self.freshness_state == 'o':\n                            chk.exit_status = 0\n                        elif self.freshness_state == 'd':\n                            chk.exit_status = 2\n                        elif self.freshness_state in ['u', 'x']:\n                            chk.exit_status = 4\n                        else:\n                            chk.exit_status = 3\n                    else:\n                        if self.freshness_state == 'o':\n                            chk.exit_status = 0\n                        elif self.freshness_state == 'w':\n                            chk.exit_status = 1\n                        elif self.freshness_state == 'c':\n                            chk.exit_status = 2\n                        elif self.freshness_state == 'u':\n                            chk.exit_status = 3\n                        elif self.freshness_state == 'x':\n                            chk.exit_status = 4\n                        else:\n                            chk.exit_status = 3\n\n                    return chk\n                else:\n                    logger.debug(\"Ignored freshness check for %s, because \"\n                                 \"we are not in the check period.\", self.get_full_name())\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nraising all impacts from my error and register myself as a problem.", "response": "def set_myself_as_problem(self, hosts, services, timeperiods, bi_modulations):\n        # pylint: disable=too-many-locals\n        \"\"\" Raise all impact from my error. I'm setting myself\n        as a problem, and I register myself as this in all\n        hosts/services that depend_on_me. So they are now my\n        impacts\n\n        :param hosts: hosts objects, used to get impacts\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used to get impacts\n        :type services: alignak.objects.service.Services\n        :param timeperiods: Timeperiods objects, used to get act_depend_of_me timeperiod\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param bi_modulations: business impact modulations objects\n        :type bi_modulations: alignak.object.businessimpactmodulation.Businessimpactmodulations\n        :return: None\n        \"\"\"\n        now = time.time()\n\n        self.is_problem = True\n        # we should warn potentials impact of our problem\n        # and they should be cool to register them so I've got\n        # my impacts list\n        impacts = list(self.impacts)\n        for (impact_id, status, timeperiod_id, _) in self.act_depend_of_me:\n            # Check if the status is ok for impact\n            if impact_id in hosts:\n                impact = hosts[impact_id]\n            elif impact_id in services:\n                impact = services[impact_id]\n            else:\n                logger.warning(\"Problem with my impacts: %s\", self)\n            timeperiod = timeperiods[timeperiod_id]\n            for stat in status:\n                if self.is_state(stat):\n                    # now check if we should bailout because of a\n                    # not good timeperiod for dep\n                    if timeperiod is None or timeperiod.is_time_valid(now):\n                        new_impacts = impact.register_a_problem(self, hosts, services, timeperiods,\n                                                                bi_modulations)\n                        impacts.extend(new_impacts)\n\n        # Only update impacts and create new brok if impacts changed.\n        s_impacts = set(impacts)\n        if s_impacts == set(self.impacts):\n            return\n        self.impacts = list(s_impacts)\n\n        # We can update our business_impact value now\n        self.update_business_impact_value(hosts, services, timeperiods, bi_modulations)\n\n        # And we register a new broks for update status\n        self.broks.append(self.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate our business_impact value with the max of the impacts business_impact if we got impacts before we save our value.", "response": "def update_business_impact_value(self, hosts, services, timeperiods, bi_modulations):\n        \"\"\"We update our 'business_impact' value with the max of\n        the impacts business_impact if we got impacts. And save our 'configuration'\n        business_impact if we do not have do it before\n        If we do not have impacts, we revert our value\n\n        :param hosts: hosts objects, used to get impacts\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used to get impacts\n        :type services: alignak.objects.service.Services\n        :param timeperiods: Timeperiods objects, used to get modulation_period\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param bi_modulations: business impact modulations objects\n        :type bi_modulations: alignak.object.businessimpactmodulation.Businessimpactmodulations\n        :return: None\n        TODO: SchedulingItem object should not handle other schedulingitem obj.\n              We should call obj.register* on both obj.\n              This is 'Java' style\n        \"\"\"\n        # First save our business_impact if not already do\n        if self.my_own_business_impact == -1:\n            self.my_own_business_impact = self.business_impact\n\n        # We look at our crit modulations. If one apply, we take apply it\n        # and it's done\n        in_modulation = False\n        for bi_modulation_id in self.business_impact_modulations:\n            bi_modulation = bi_modulations[bi_modulation_id]\n            now = time.time()\n            period = timeperiods[bi_modulation.modulation_period]\n            if period is None or period.is_time_valid(now):\n                self.business_impact = bi_modulation.business_impact\n                in_modulation = True\n                # We apply the first available, that's all\n                break\n\n        # If we truly have impacts, we get the max business_impact\n        # if it's huge than ourselves\n        if self.impacts:\n            bp_impacts = [hosts[elem].business_impact for elem in self.impacts if elem in hosts]\n            bp_impacts.extend([services[elem].business_impact for elem in self.impacts\n                               if elem in services])\n            self.business_impact = max(self.business_impact, max(bp_impacts))\n            return\n\n        # If we are not a problem, we setup our own_crit if we are not in a\n        # modulation period\n        if self.my_own_business_impact != -1 and not in_modulation:\n            self.business_impact = self.my_own_business_impact"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove this objects as an impact for other SchedulingItem object.", "response": "def no_more_a_problem(self, hosts, services, timeperiods, bi_modulations):\n        \"\"\"Remove this objects as an impact for other schedulingitem.\n\n        :param hosts: hosts objects, used to get impacts\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used to get impacts\n        :type services: alignak.objects.service.Services\n        :param timeperiods: Timeperiods objects, used for update_business_impact_value\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param bi_modulations: business impact modulation are used when setting myself as problem\n        :type bi_modulations: alignak.object.businessimpactmodulation.Businessimpactmodulations\n        :return: None\n        TODO: SchedulingItem object should not handle other schedulingitem obj.\n              We should call obj.register* on both obj.\n              This is 'Java' style\n        \"\"\"\n        was_pb = self.is_problem\n        if self.is_problem:\n            self.is_problem = False\n\n            # we warn impacts that we are no more a problem\n            for impact_id in self.impacts:\n                if impact_id in hosts:\n                    impact = hosts[impact_id]\n                else:\n                    impact = services[impact_id]\n                impact.unregister_a_problem(self)\n\n            # we can just drop our impacts list\n            self.impacts = []\n\n        # We update our business_impact value, it's not a huge thing :)\n        self.update_business_impact_value(hosts, services, timeperiods, bi_modulations)\n\n        # If we were a problem, we say to everyone\n        # our new status, with good business_impact value\n        if was_pb:\n            # And we register a new broks for update status\n            self.broks.append(self.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_a_problem(self, prob, hosts, services, timeperiods, bi_modulations):\n        # pylint: disable=too-many-locals\n        \"\"\"Call recursively by potentials impacts so they\n        update their source_problems list. But do not\n        go below if the problem is not a real one for me\n        like If I've got multiple parents for examples\n\n        :param prob: problem to register\n        :type prob: alignak.objects.schedulingitem.SchedulingItem\n        :param hosts: hosts objects, used to get object in act_depend_of_me\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used to get object in act_depend_of_me\n        :type services: alignak.objects.service.Services\n        :param timeperiods: Timeperiods objects, used for all kind of timeperiod (notif, check)\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param bi_modulations: business impact modulation are used when setting myself as problem\n        :type bi_modulations: alignak.object.businessimpactmodulation.Businessimpactmodulations\n        :return: list of host/service that are impacts\n        :rtype: list[alignak.objects.schedulingitem.SchedulingItem]\n        TODO: SchedulingItem object should not handle other schedulingitem obj.\n              We should call obj.register* on both obj.\n              This is 'Java' style\n        \"\"\"\n        # Maybe we already have this problem? If so, bailout too\n        if prob.uuid in self.source_problems:\n            return []\n\n        now = time.time()\n        was_an_impact = self.is_impact\n        # Our father already look if he impacts us. So if we are here,\n        # it's that we really are impacted\n        self.is_impact = True\n\n        impacts = []\n        # Ok, if we are impacted, we can add it in our\n        # problem list\n        # TODO: remove this unused check\n        if self.is_impact:\n            logger.debug(\"I am impacted: %s\", self)\n            # Maybe I was a problem myself, now I can say: not my fault!\n            if self.is_problem:\n                self.no_more_a_problem(hosts, services, timeperiods, bi_modulations)\n\n            # Ok, we are now impacted, we should take the good state\n            # but only when we just go to the impacted state\n            if not was_an_impact:\n                self.set_impact_state()\n\n            # Ok now we can be a simple impact\n            impacts.append(self.uuid)\n            if prob.uuid not in self.source_problems:\n                self.source_problems.append(prob.uuid)\n            # we should send this problem to all potential impacted that\n            # depend on us\n            for (impacted_item_id, status, timeperiod_id, _) in self.act_depend_of_me:\n                # Check if the status is ok for impact\n                if impacted_item_id in hosts:\n                    impact = hosts[impacted_item_id]\n                else:\n                    impact = services[impacted_item_id]\n                timeperiod = timeperiods[timeperiod_id]\n                for stat in status:\n                    if self.is_state(stat):\n                        # now check if we should bailout because of a\n                        # not good timeperiod for dep\n                        if timeperiod is None or timeperiod.is_time_valid(now):\n                            new_impacts = impact.register_a_problem(prob, hosts,\n                                                                    services, timeperiods,\n                                                                    bi_modulations)\n                            impacts.extend(new_impacts)\n\n            # And we register a new broks for update status\n            self.broks.append(self.get_update_status_brok())\n\n        # now we return all impacts (can be void of course)\n        return impacts", "response": "This method is called by the user to register a problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves the problem from our list and check if we are still impacted", "response": "def unregister_a_problem(self, prob):\n        \"\"\"Remove the problem from our problems list\n        and check if we are still 'impacted'\n\n        :param prob: problem to remove\n        :type prob: alignak.objects.schedulingitem.SchedulingItem\n        :return: None\n        \"\"\"\n        self.source_problems.remove(prob.uuid)\n\n        # For know if we are still an impact, maybe our dependencies\n        # are not aware of the remove of the impact state because it's not ordered\n        # so we can just look at if we still have some problem in our list\n        if not self.source_problems:\n            self.is_impact = False\n            # No more an impact, we can unset the impact state\n            self.unset_impact_state()\n\n        # And we register a new broks for update status\n        self.broks.append(self.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_enable_action_dependent(self, hosts, services):\n        # Use to know if notification is raise or not\n        enable_action = False\n        for (dep_id, status, _, _) in self.act_depend_of:\n            if 'n' in status:\n                enable_action = True\n            else:\n                if dep_id in hosts:\n                    dep = hosts[dep_id]\n                else:\n                    dep = services[dep_id]\n                p_is_down = False\n                dep_match = [dep.is_state(stat) for stat in status]\n                # check if the parent match a case, so he is down\n                if True in dep_match:\n                    p_is_down = True\n                if not p_is_down:\n                    enable_action = True\n        return enable_action", "response": "Check if all dependencies states match dependencies statuses\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_and_set_unreachability(self, hosts, services):\n        parent_is_down = []\n        for (dep_id, _, _, _) in self.act_depend_of:\n            if dep_id in hosts:\n                dep = hosts[dep_id]\n            else:\n                dep = services[dep_id]\n            if dep.state in ['d', 'DOWN', 'c', 'CRITICAL', 'u', 'UNKNOWN', 'x', 'UNREACHABLE']:\n                parent_is_down.append(True)\n            else:\n                parent_is_down.append(False)\n        if False in parent_is_down:\n            return\n        # all parents down\n        self.set_unreachable()", "response": "Check if all dependencies are down set this object\n        as unreachable."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if this object or one of its dependencies matters", "response": "def do_i_raise_dependency(self, status, inherit_parents, hosts, services, timeperiods):\n        # pylint: disable=too-many-locals\n        \"\"\"Check if this object or one of its dependency state (chk dependencies) match the status\n\n        :param status: state list where dependency matters (notification failure criteria)\n        :type status: list\n        :param inherit_parents: recurse over parents\n        :type inherit_parents: bool\n        :param hosts: hosts objects, used to raise dependency check\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used to raise dependency check\n        :type services: alignak.objects.service.Services\n        :param timeperiods: Timeperiods objects, used for all kind of timeperiod (notif, check)\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :return: True if one state matched the status list, otherwise False\n        :rtype: bool\n        \"\"\"\n        # Do I raise dep?\n        for stat in status:\n            if self.is_state(stat):\n                return True\n\n        # If we do not inherit parent, we have no reason to be blocking\n        if not inherit_parents:\n            return False\n\n        # Ok, I do not raise dep, but my dep maybe raise me\n        now = time.time()\n        for (dep_id, dep_status, _, timeperiod_id, inh_parent) in self.chk_depend_of:\n            if dep_id in hosts:\n                dep = hosts[dep_id]\n            else:\n                dep = services[dep_id]\n            timeperiod = timeperiods[timeperiod_id]\n            if dep.do_i_raise_dependency(dep_status, inh_parent, hosts, services, timeperiods):\n                if timeperiod is None or timeperiod.is_time_valid(now):\n                    return True\n\n        # No, I really do not raise...\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if there is some host or service that this object depend on.", "response": "def is_no_check_dependent(self, hosts, services, timeperiods):\n        \"\"\"Check if there is some host/service that this object depend on\n        has a state in the status list .\n\n        :param hosts: hosts objects, used to raise dependency check\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used to raise dependency check\n        :type services: alignak.objects.service.Services\n        :param timeperiods: Timeperiods objects, used for all kind of timeperiod (notif, check)\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :return: True if this object has a check dependency, otherwise False\n        :rtype: bool\n        \"\"\"\n        now = time.time()\n        for (dep_id, status, _, timeperiod_id, inh_parent) in self.chk_depend_of:\n            timeperiod = timeperiods[timeperiod_id]\n            if timeperiod is None or timeperiod.is_time_valid(now):\n                if dep_id in hosts:\n                    dep = hosts[dep_id]\n                else:\n                    dep = services[dep_id]\n                if dep.do_i_raise_dependency(status, inh_parent, hosts, services, timeperiods):\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises the dependency check if all of the items in the ref_check depend on the object.", "response": "def raise_dependencies_check(self, ref_check, hosts, services, timeperiods, macromodulations,\n                                 checkmodulations, checks):\n        # pylint: disable=too-many-locals, too-many-nested-blocks\n        \"\"\"Get checks that we depend on if EVERY following conditions is met::\n\n        * timeperiod is valid\n        * dep.last_state_update < now - cls.cached_check_horizon (check of dependency is \"old\")\n\n        :param ref_check: Check we want to get dependency from\n        :type ref_check: alignak.check.Check\n        :param hosts: hosts objects, used for almost every operation\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used for almost every operation\n        :type services: alignak.objects.service.Services\n        :param timeperiods: Timeperiods objects, used for all kind of timeperiod (notif, check)\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param macromodulations: Macro modulations objects, used in commands (notif, check)\n        :type macromodulations: alignak.objects.macromodulation.Macromodulations\n        :param checkmodulations: Checkmodulations objects, used to change check command if necessary\n        :type checkmodulations: alignak.objects.checkmodulation.Checkmodulations\n        :param checks: checks dict, used to get checks_in_progress for the object\n        :type checks: dict\n        :return: check created and check in_checking\n        :rtype: dict\n        \"\"\"\n        now = time.time()\n        cls = self.__class__\n        new_checks = []\n        checking_checks = []\n        for (dep_id, _, timeperiod_id, _) in self.act_depend_of:\n            if dep_id in hosts:\n                dep_item = hosts[dep_id]\n            else:\n                dep_item = services[dep_id]\n            timeperiod = timeperiods[timeperiod_id]\n            # If the dep_item timeperiod is not valid, do not raise the dep,\n            # None=everytime\n            if timeperiod is None or timeperiod.is_time_valid(now):\n                # if the update is 'fresh', do not raise dep,\n                # cached_check_horizon = cached_service_check_horizon for service\n                if dep_item.last_state_update < now - cls.cached_check_horizon:\n                    # Do not launch the check if it depends on a passive check of if a check\n                    # is yet planned\n                    if dep_item.active_checks_enabled:\n                        if not dep_item.in_checking:\n                            newchk = dep_item.launch_check(now, hosts, services, timeperiods,\n                                                           macromodulations, checkmodulations,\n                                                           checks, ref_check, dependent=True)\n                            if newchk is not None:\n                                new_checks.append(newchk)\n                        else:\n                            if dep_item.checks_in_progress:\n                                check_uuid = dep_item.checks_in_progress[0]\n                                checks[check_uuid].depend_on_me.append(ref_check)\n                                checking_checks.append(check_uuid)\n        return {'new': new_checks, 'checking': checking_checks}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef schedule(self, hosts, services, timeperiods, macromodulations, checkmodulations,\n                 checks, force=False, force_time=None):\n        # pylint: disable=too-many-branches, too-many-arguments, too-many-locals\n        \"\"\"Main scheduling function\n        If a check is in progress, or active check are disabled, do not schedule a check.\n        The check interval change with HARD state::\n\n        * SOFT: retry_interval\n        * HARD: check_interval\n\n        The first scheduling is evenly distributed, so all checks\n        are not launched at the same time.\n\n\n        :param hosts: hosts objects, used for almost every operation\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used for almost every operation\n        :type services: alignak.objects.service.Services\n        :param timeperiods: Timeperiods objects, used for all kind of timeperiod (notif, check)\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param macromodulations: Macro modulations objects, used in commands (notif, check)\n        :type macromodulations: alignak.objects.macromodulation.Macromodulations\n        :param checkmodulations: Checkmodulations objects, used to change check command if necessary\n        :type checkmodulations: alignak.objects.checkmodulation.Checkmodulations\n        :param checks: checks dict, used to get checks_in_progress for the object\n        :type checks: dict\n        :param force: tell if we forced this object to schedule a check\n        :type force: bool\n        :param force_time: time we would like the check to be scheduled\n        :type force_time: None | int\n        :return: None\n        \"\"\"\n        # next_chk is already set, do not change\n        # unless we force the check or the time\n        if self.in_checking and not (force or force_time):\n            return None\n\n        cls = self.__class__\n        # if no active check and no force, no check\n        if (not self.active_checks_enabled or not cls.execute_checks) and not force:\n            logger.debug(\"No check for %s\", self.get_full_name())\n            return None\n\n        now = time.time()\n        current_next_check = self.next_chk\n\n        # If check_interval is 0, we should not add a check for a service\n        # but suppose a 5 min check interval for an host\n        if self.check_interval == 0 and not force:\n            if cls.my_type == 'service':\n                return None\n\n            self.check_interval = 300 / cls.interval_length\n\n        # Interval change is in a HARD state or not\n        # If the retry is 0, take the normal value\n        if self.state_type == 'HARD' or self.retry_interval == 0:\n            interval = self.check_interval * cls.interval_length\n        else:\n            interval = self.retry_interval * cls.interval_length\n\n        # Determine when a new check (randomize and distribute next check time)\n        # or recurring check should happen.\n        if self.next_chk == 0:\n            # At the start, we cannot have an interval more than cls.max_check_spread\n            # Global service_max_check_spread or host_max_check_spread in configuration\n            # is set as max_check_spread in the objects.\n            interval = min(interval, cls.max_check_spread * cls.interval_length)\n            time_add = interval * random.uniform(0.0, 1.0)\n        else:\n            time_add = interval\n\n        # Do the actual Scheduling now\n\n        # If not force_time, try to schedule\n        if force_time is None:\n            check_period = None\n            if getattr(self, 'check_period', None) is not None:\n                check_period = timeperiods[self.check_period]\n\n            # Do not calculate next_chk based on current time, but\n            # based on the last check execution time.\n            # Important for consistency of data for trending.\n            if self.next_chk == 0 or self.next_chk is None:\n                self.next_chk = now\n\n            # If the neck_chk is already in the future, do not touch it.\n            # But if == 0, means was 0 in fact, schedule it too\n            if self.next_chk <= now:\n                # maybe we do not have a check_period, if so, take always good (24x7)\n                if check_period:\n                    self.next_chk = check_period.get_next_valid_time_from_t(\n                        self.next_chk + time_add)\n                else:\n                    self.next_chk = int(self.next_chk + time_add)\n\n            # Maybe we load next_chk from retention and the\n            # value of the next_chk is still in the past even after adding an interval\n            if self.next_chk < now:\n                interval = min(interval, cls.max_check_spread * cls.interval_length)\n\n                time_add = interval * random.uniform(0.0, 1.0)\n\n                # if we got a check period, use it, if now, use now\n                if check_period:\n                    self.next_chk = check_period.get_next_valid_time_from_t(now + time_add)\n                else:\n                    self.next_chk = int(now + time_add)\n            # else: keep the self.next_chk value in the future\n        else:\n            self.next_chk = int(force_time)\n\n        # If next time is None, do not go\n        if self.next_chk is None:\n            # Nagios do not raise it, I'm wondering if we should\n            return None\n\n        logger.debug(\"-> schedule: %s / %s (interval: %d, added: %d)\",\n                     self.get_full_name(),\n                     datetime.utcfromtimestamp(self.next_chk).strftime('%Y-%m-%d %H:%M:%S'),\n                     interval, time_add)\n\n        if current_next_check != self.next_chk and os.getenv('ALIGNAK_LOG_CHECKS', None):\n            logger.info(\"--ALC-- -> scheduled the next check for %s \"\n                        \"at %s (interval: %d, added: %d)\",\n                        self.get_full_name(),\n                        datetime.utcfromtimestamp(self.next_chk).strftime('%Y-%m-%d %H:%M:%S'),\n                        interval, time_add)\n\n        # Get the command to launch, and put it in queue\n        return self.launch_check(self.next_chk, hosts, services, timeperiods, macromodulations,\n                                 checkmodulations, checks, force=force)", "response": "Main function to schedule a check for the given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compensate_system_time_change(self, difference):  # pragma: no cover,\n        # not with unit tests\n        \"\"\"If a system time change occurs we have to update\n        properties time related to reflect change\n\n        :param difference: difference between new time and old time\n        :type difference:\n        :return: None\n        \"\"\"\n        # We only need to change some value\n        for prop in ('last_notification', 'last_state_change', 'last_hard_state_change'):\n            val = getattr(self, prop)  # current value\n            # Do not go below 1970 :)\n            val = max(0, val + difference)  # diff may be negative\n            setattr(self, prop, val)", "response": "Compensate that a system time change occurs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisabling active checks for this host or service", "response": "def disable_active_checks(self, checks):\n        \"\"\"Disable active checks for this host/service\n        Update check in progress with current object information\n\n        :param checks: Checks object, to change all checks in progress\n        :type checks: alignak.objects.check.Checks\n        :return: None\n        \"\"\"\n        self.active_checks_enabled = False\n        for chk_id in self.checks_in_progress:\n            chk = checks[chk_id]\n            chk.status = ACT_STATUS_WAIT_CONSUME\n            chk.exit_status = self.state_id\n            chk.output = self.output\n            chk.check_time = time.time()\n            chk.execution_time = 0\n            chk.perf_data = self.perf_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_in_progress_check(self, check):\n        # The check is consumed, update the in_checking properties\n        if check in self.checks_in_progress:\n            self.checks_in_progress.remove(check)\n        self.update_in_checking()", "response": "Removes a check from check in progress."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_in_progress_notification(self, notification):\n        if notification.uuid in self.notifications_in_progress:\n            notification.status = ACT_STATUS_ZOMBIE\n            del self.notifications_in_progress[notification.uuid]", "response": "Removes a notification from the list of in - progress notifications and marks them as zombie."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_in_progress_notifications(self, master=True):\n        for notification in list(self.notifications_in_progress.values()):\n            if master and notification.contact:\n                continue\n            # Do not remove some specific notifications\n            if notification.type in [u'DOWNTIMESTART', u'DOWNTIMEEND', u'DOWNTIMECANCELLED',\n                                     u'CUSTOM', u'ACKNOWLEDGEMENT']:\n                continue\n            self.remove_in_progress_notification(notification)", "response": "Removes all notifications in progress from the notifications_in_progress list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_event_handlers(self, hosts, macromodulations, timeperiods, ext_cmd=False):\n        cls = self.__class__\n\n        # The external command always pass\n        # if not, only if we enable them (auto launch)\n        if not ext_cmd and (not self.event_handler_enabled or not cls.enable_event_handlers):\n            logger.debug(\"Event handler is disabled for %s\", self.get_full_name())\n            return\n\n        # If we do not force and we are in downtime, bailout\n        # if the no_event_handlers_during_downtimes is set in the configuration\n        if not ext_cmd and self.in_scheduled_downtime and cls.no_event_handlers_during_downtimes:\n            logger.debug(\"Event handler will not be launched. \"\n                         \"The item %s is in a scheduled downtime\", self.get_full_name())\n            return\n\n        if self.event_handler is not None:\n            event_handler = self.event_handler\n        elif cls.global_event_handler is not None:\n            event_handler = cls.global_event_handler\n        else:\n            return\n\n        macroresolver = MacroResolver()\n        data = self.get_data_for_event_handler(hosts)\n        cmd = macroresolver.resolve_command(event_handler, data, macromodulations, timeperiods)\n\n        event_h = EventHandler({\n            'command': cmd,\n            'timeout': cls.event_handler_timeout,\n            'ref': self.uuid,\n            'reactionner_tag': event_handler.reactionner_tag\n        })\n        self.raise_event_handler_log_entry(event_handler)\n\n        # ok we can put it in our temp action queue\n        self.actions.append(event_h)", "response": "Raise event handlers if none are met"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_snapshot(self, hosts, macromodulations, timeperiods):  # pragma: no cover, not yet!\n        # We should have a snapshot_command, to be enabled and of course\n        # in the good time and state :D\n        if self.snapshot_command is None:\n            return\n\n        if not self.snapshot_enabled:\n            return\n\n        # look at if one state is matching the criteria\n        boolmap = [self.is_state(s) for s in self.snapshot_criteria]\n        if True not in boolmap:\n            return\n\n        # Time based checks now, we should be in the period and not too far\n        # from the last_snapshot\n        now = int(time.time())\n        cls = self.__class__\n        if self.last_snapshot > now - self.snapshot_interval * cls.interval_length:  # too close\n            return\n\n        # no period means 24x7 :)\n        timeperiod = timeperiods[self.snapshot_period]\n        if timeperiod is not None and not timeperiod.is_time_valid(now):\n            return\n\n        cls = self.__class__\n        macroresolver = MacroResolver()\n        data = self.get_data_for_event_handler(hosts)\n        cmd = macroresolver.resolve_command(self.snapshot_command, data, macromodulations,\n                                            timeperiods)\n        reac_tag = self.snapshot_command.reactionner_tag\n        event_h = EventHandler({\n            'command': cmd,\n            'timeout': cls.event_handler_timeout,\n            'ref': self.uuid,\n            'reactionner_tag': reac_tag,\n            'is_snapshot': True\n        })\n        self.raise_snapshot_log_entry(self.snapshot_command)\n\n        # we save the time we launch the snap\n        self.last_snapshot = now\n\n        # ok we can put it in our temp action queue\n        self.actions.append(event_h)", "response": "Get the current state of the current snapshot."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if a downtime is needed and raise start notification.", "response": "def check_for_flexible_downtime(self, timeperiods, hosts, services):\n        \"\"\"Enter in a downtime if necessary and raise start notification\n        When a non Ok state occurs we try to raise a flexible downtime.\n\n        :param timeperiods: Timeperiods objects, used for downtime period\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param hosts: hosts objects, used to enter downtime\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used to enter downtime\n        :type services: alignak.objects.service.Services\n        :return: None\n        \"\"\"\n        status_updated = False\n        for downtime_id in self.downtimes:\n            downtime = self.downtimes[downtime_id]\n            # Activate flexible downtimes (do not activate triggered downtimes)\n            # Note: only activate if we are between downtime start and end time!\n            if downtime.fixed or downtime.is_in_effect:\n                continue\n            if downtime.start_time <= self.last_chk and downtime.end_time >= self.last_chk \\\n                    and self.state_id != 0 and downtime.trigger_id in ['', '0']:\n                # returns downtimestart notifications\n                self.broks.extend(downtime.enter(timeperiods, hosts, services))\n                status_updated = True\n        if status_updated is True:\n            self.broks.append(self.get_update_status_brok())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_hard_unknown_phase_state(self):\n        self.was_in_hard_unknown_reach_phase = self.in_hard_unknown_reach_phase\n\n        # We do not care about SOFT state at all\n        # and we are sure we are no more in such a phase\n        if self.state_type != 'HARD' or self.last_state_type != 'HARD':\n            self.in_hard_unknown_reach_phase = False\n\n        # So if we are not in already in such a phase, we check for\n        # a start or not. So here we are sure to be in a HARD/HARD following\n        # state\n        if not self.in_hard_unknown_reach_phase:\n            if self.state == 'UNKNOWN' and self.last_state != 'UNKNOWN' \\\n                    or self.state == 'UNREACHABLE' and self.last_state != 'UNREACHABLE':\n                self.in_hard_unknown_reach_phase = True\n                # We also backup with which state we was before enter this phase\n                self.state_before_hard_unknown_reach_phase = self.last_state\n                return\n        else:\n            # if we were already in such a phase, look for its end\n            if self.state != 'UNKNOWN' and self.state != 'UNREACHABLE':\n                self.in_hard_unknown_reach_phase = False\n\n        # If we just exit the phase, look if we exit with a different state\n        # than we enter or not. If so, lie and say we were not in such phase\n        # because we need so to raise a new notif\n        if not self.in_hard_unknown_reach_phase and self.was_in_hard_unknown_reach_phase:\n            if self.state != self.state_before_hard_unknown_reach_phase:\n                self.was_in_hard_unknown_reach_phase = False", "response": "Update in_hard_unknown_reach_phase attribute and was_in_hard_unknown_reach_phase attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconsuming a check return and send action in return", "response": "def consume_result(self, chk, notification_period, hosts,\n                       services, timeperiods, macromodulations, checkmodulations, bi_modulations,\n                       res_modulations, checks, raise_log):\n        # pylint: disable=too-many-locals, too-many-arguments\n        # pylint: disable=too-many-branches, too-many-statements\n        \"\"\"Consume a check return and send action in return\n        main function of reaction of checks like raise notifications\n\n        Special cases::\n\n        * is_flapping: immediate notif when problem\n        * is_in_scheduled_downtime: no notification\n        * is_volatile: notif immediately (service only)\n\n        Basically go through all cases (combination of last_state, current_state, attempt number)\n        and do necessary actions (add attempt, raise notification., change state type.)\n\n        :param chk: check to handle\n        :type chk: alignak.objects.check.Check\n        :param notification_period: notification period for this host/service\n        :type notification_period: alignak.objects.timeperiod.Timeperiod\n        :param hosts: hosts objects, used for almost every operation\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used for almost every operation\n        :type services: alignak.objects.service.Services\n        :param timeperiods: Timeperiods objects, used for all kind of timeperiod (notif, check)\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param macromodulations: Macro modulations objects, used in commands (notif, check)\n        :type macromodulations: alignak.objects.macromodulation.Macromodulations\n        :param checkmodulations: Checkmodulations objects, used to change check command if necessary\n        :type checkmodulations: alignak.objects.checkmodulation.Checkmodulations\n        :param bi_modulations: business impact modulation are used when setting myself as problem\n        :type bi_modulations: alignak.object.businessimpactmodulation.Businessimpactmodulations\n        :param res_modulations: result modulation are used to change the ouput of a check\n        :type res_modulations: alignak.object.resultmodulation.Resultmodulations\n        :param checks: checks dict, used to get checks_in_progress for the object\n        :type checks: dict\n        :return: Dependent checks\n        :rtype list[alignak.check.Check]\n        \"\"\"\n        ok_up = self.__class__.ok_up  # OK for service, UP for host\n        now = int(time.time())\n        if not chk.freshness_expiry_check:\n            self.freshness_expired = False\n\n        if 'ALIGNAK_LOG_ACTIONS' in os.environ:\n            if os.environ['ALIGNAK_LOG_ACTIONS'] == 'WARNING':\n                logger.warning(\"Got check result: %d for %s\",\n                               chk.exit_status, self.get_full_name())\n            else:\n                logger.info(\"Got check result: %d for %s\",\n                            chk.exit_status, self.get_full_name())\n\n        if os.getenv('ALIGNAK_LOG_CHECKS', None):\n            level = ['info', 'warning', 'error', 'critical'][min(chk.exit_status, 3)]\n            func = getattr(logger, level)\n            func(\"--ALC-- check result for %s, exit: %d, output: %s\",\n                 self.get_full_name(), chk.exit_status, chk.output)\n\n        # ============ MANAGE THE CHECK ============ #\n        # Check is not OK, waiting to consume the results but it has some dependencies\n        # We put this check in waitdep state, and we create the checks of dependent items\n        # and nothing else ;)\n        if chk.exit_status != 0 and chk.status == ACT_STATUS_WAIT_CONSUME and self.act_depend_of:\n            chk.status = ACT_STATUS_WAIT_DEPEND\n            # Make sure the check know about his dep\n            # C is my check, and he wants dependencies\n            deps_checks = self.raise_dependencies_check(chk, hosts, services, timeperiods,\n                                                        macromodulations, checkmodulations,\n                                                        checks)\n            # Get checks_id of dep\n            for check in deps_checks['new']:\n                chk.depend_on.append(check.uuid)\n            for check_uuid in deps_checks['checking']:\n                chk.depend_on.append(check_uuid)\n            # we must wait dependent check checked and consumed\n            return deps_checks['new']\n\n        # We check for stalking if necessary\n        # so if check is here\n        self.manage_stalking(chk)\n\n        # ============ UPDATE ITEM INFORMATION ============ #\n\n        # Latency can be <0 is we get a check from the retention file\n        # so if <0, set 0\n        try:\n            self.latency = max(0, chk.check_time - chk.t_to_go)\n        except TypeError:  # pragma: no cover, simple protection\n            pass\n\n        # Ok, the first check is done\n        self.has_been_checked = 1\n\n        # Now get data from check\n        self.execution_time = chk.execution_time\n        self.u_time = chk.u_time\n        self.s_time = chk.s_time\n        self.last_chk = int(chk.check_time)\n        self.output = chk.output\n        self.long_output = chk.long_output\n        if self.__class__.process_performance_data and self.process_perf_data:\n            self.last_perf_data = self.perf_data\n            self.perf_data = chk.perf_data\n\n        # Before setting state, modulate them\n        for resultmod_id in self.resultmodulations:\n            resultmod = res_modulations[resultmod_id]\n            if resultmod is not None:\n                chk.exit_status = resultmod.module_return(chk.exit_status, timeperiods)\n\n        if not chk.freshness_expiry_check:\n            # Only update the last state date if not in freshness expiry\n            self.last_state_update = now\n            if chk.exit_status == 1 and self.__class__.my_type == 'host':\n                chk.exit_status = 2\n\n        self.set_state_from_exit_status(chk.exit_status, notification_period, hosts, services)\n\n        self.last_state_type = self.state_type\n        self.return_code = chk.exit_status\n\n        # Raise the log only when the item information are up-to-date :/\n        if raise_log:\n            self.raise_check_result()\n\n        # we change the state, do whatever we are or not in\n        # an impact mode, we can put it\n        self.state_changed_since_impact = True\n\n        # The check is consumed, update the in_checking properties\n        self.remove_in_progress_check(chk.uuid)\n\n        # Used to know if a notification is raised or not\n        enable_action = True\n\n        # This check was waiting for a check of items it depends\n        if chk.status == ACT_STATUS_WAIT_DEPEND:\n            # Check dependencies\n            enable_action = self.is_enable_action_dependent(hosts, services)\n            # If all dependencies not ok, define item as UNREACHABLE\n            self.check_and_set_unreachability(hosts, services)\n\n        if chk.status in [ACT_STATUS_WAIT_CONSUME, ACT_STATUS_WAIT_DEPEND]:\n            # check waiting consume or waiting result of dependencies\n            if chk.depend_on_me != []:\n                # one or more checks wait this check (dependency)\n                chk.status = ACT_STATUS_WAITING_ME\n            else:\n                # the check go in zombie state to be removed later\n                chk.status = ACT_STATUS_ZOMBIE\n\n        # from UP/OK/PENDING\n        # to UP/OK\n        if chk.exit_status == 0 and self.last_state in (ok_up, 'PENDING'):\n            self.unacknowledge_problem()\n            # action in return can be notification or other checks (dependencies)\n            if (self.state_type == 'SOFT') and self.last_state != 'PENDING':\n                if self.is_max_attempts() and self.state_type == 'SOFT':\n                    self.state_type = 'HARD'\n                else:\n                    self.state_type = 'SOFT'\n            else:\n                self.attempt = 1\n                self.state_type = 'HARD'\n\n        # from WARNING/CRITICAL/UNKNOWN/UNREACHABLE/DOWN\n        # to UP/OK\n        elif chk.exit_status == 0 and self.last_state not in (ok_up, 'PENDING'):\n            self.unacknowledge_problem()\n            if self.state_type == 'SOFT':\n                # previous check in SOFT\n                if not chk.is_dependent():\n                    self.add_attempt()\n                self.raise_alert_log_entry()\n                # Eventhandler gets OK;SOFT;++attempt, no notification needed\n                self.get_event_handlers(hosts, macromodulations, timeperiods)\n                # Now we are UP/OK HARD\n                self.state_type = 'HARD'\n                self.attempt = 1\n            elif self.state_type == 'HARD':\n                # previous check in HARD\n                self.raise_alert_log_entry()\n                # Eventhandler and notifications get OK;HARD;maxattempts\n                # Ok, so current notifications are not needed, we 'zombie' them\n                self.remove_in_progress_notifications(master=True)\n                if enable_action:\n                    self.create_notifications('RECOVERY', notification_period, hosts, services)\n                self.get_event_handlers(hosts, macromodulations, timeperiods)\n                # We stay in HARD\n                self.attempt = 1\n\n                # I'm no more a problem if I was one\n                self.no_more_a_problem(hosts, services, timeperiods, bi_modulations)\n\n        # Volatile part\n        # Only for service\n        elif chk.exit_status != 0 and getattr(self, 'is_volatile', False):\n            # There are no repeated attempts, so the first non-ok results\n            # in a hard state\n            self.attempt = 1\n            self.state_type = 'HARD'\n            # status != 0 so add a log entry (before actions that can also raise log\n            # it is smarter to log error before notification)\n            self.raise_alert_log_entry()\n            self.check_for_flexible_downtime(timeperiods, hosts, services)\n            self.remove_in_progress_notifications(master=True)\n            if enable_action:\n                self.create_notifications('PROBLEM', notification_period, hosts, services)\n            # Ok, event handlers here too\n            self.get_event_handlers(hosts, macromodulations, timeperiods)\n\n            # PROBLEM/IMPACT\n            # I'm a problem only if I'm the root problem,\n            if enable_action:\n                self.set_myself_as_problem(hosts, services, timeperiods, bi_modulations)\n\n        # from UP/OK\n        # to WARNING/CRITICAL/UNKNOWN/UNREACHABLE/DOWN\n        elif chk.exit_status != 0 and self.last_state in (ok_up, 'PENDING'):\n            self.attempt = 1\n            if self.is_max_attempts():\n                # Now we are in HARD\n                self.state_type = 'HARD'\n                self.raise_alert_log_entry()\n                self.remove_in_progress_notifications(master=True)\n                self.check_for_flexible_downtime(timeperiods, hosts, services)\n                if enable_action:\n                    self.create_notifications('PROBLEM', notification_period, hosts, services)\n                # Oh? This is the typical go for a event handler :)\n                self.get_event_handlers(hosts, macromodulations, timeperiods)\n\n                # PROBLEM/IMPACT\n                # I'm a problem only if I'm the root problem,\n                if enable_action:\n                    self.set_myself_as_problem(hosts, services, timeperiods, bi_modulations)\n\n            else:\n                # This is the first NON-OK result. Initiate the SOFT-sequence\n                # Also launch the event handler, he might fix it.\n                self.state_type = 'SOFT'\n                if self.is_max_attempts():\n                    self.state_type = 'HARD'\n                self.raise_alert_log_entry()\n                self.get_event_handlers(hosts, macromodulations, timeperiods)\n\n        # from WARNING/CRITICAL/UNKNOWN/UNREACHABLE/DOWN\n        # to WARNING/CRITICAL/UNKNOWN/UNREACHABLE/DOWN\n        elif chk.exit_status != 0 and self.last_state != ok_up:\n            if self.state_type == 'SOFT':\n                if not chk.is_dependent():\n                    self.add_attempt()\n                # Cases where go:\n                #  * warning soft => critical hard\n                #  * warning soft => critical soft\n                if self.state != self.last_state:\n                    self.unacknowledge_problem_if_not_sticky()\n                if self.is_max_attempts():\n                    # Ok here is when we just go to the hard state\n                    self.state_type = 'HARD'\n                    self.raise_alert_log_entry()\n                    self.remove_in_progress_notifications(master=True)\n                    self.check_for_flexible_downtime(timeperiods, hosts, services)\n                    if enable_action:\n                        self.create_notifications('PROBLEM', notification_period, hosts, services)\n                    # So event handlers here too\n                    self.get_event_handlers(hosts, macromodulations, timeperiods)\n\n                    # PROBLEM/IMPACT\n                    # I'm a problem only if I'm the root problem,\n                    if enable_action:\n                        self.set_myself_as_problem(hosts, services, timeperiods, bi_modulations)\n\n                else:\n                    self.raise_alert_log_entry()\n                    # eventhandler is launched each time during the soft state\n                    self.get_event_handlers(hosts, macromodulations, timeperiods)\n\n            else:\n                # Send notifications whenever the state has changed. (W -> C)\n                # but not if the current state is UNKNOWN (hard C-> hard U -> hard C should\n                # not restart notifications)\n                if self.state != self.last_state:\n                    self.update_hard_unknown_phase_state()\n                    if not self.in_hard_unknown_reach_phase and not \\\n                            self.was_in_hard_unknown_reach_phase:\n                        self.unacknowledge_problem_if_not_sticky()\n                        self.raise_alert_log_entry()\n                        self.remove_in_progress_notifications(master=True)\n                        if enable_action:\n                            self.create_notifications('PROBLEM', notification_period,\n                                                      hosts, services)\n                        self.get_event_handlers(hosts, macromodulations, timeperiods)\n\n                elif self.in_scheduled_downtime_during_last_check is True:\n                    # during the last check I was in a downtime. but now\n                    # the status is still critical and notifications\n                    # are possible again. send an alert immediately\n                    self.remove_in_progress_notifications(master=True)\n                    if enable_action:\n                        self.create_notifications('PROBLEM', notification_period,\n                                                  hosts, services)\n\n                # PROBLEM/IMPACT\n                # Forces problem/impact registration even if no state change\n                # was detected as we may have a non OK state restored from\n                # retention data. This way, we rebuild problem/impact hierarchy.\n                # I'm a problem only if I'm the root problem,\n                if enable_action:\n                    self.set_myself_as_problem(hosts, services, timeperiods, bi_modulations)\n\n                # case no notification exist but notifications are enabled (for example, we\n                # enable notifications with external command)\n                if enable_action and self.notifications_enabled and \\\n                        self.current_notification_number == 0:\n                    self.remove_in_progress_notifications(master=True)\n                    self.create_notifications('PROBLEM', notification_period,\n                                              hosts, services)\n\n        self.update_hard_unknown_phase_state()\n        # Reset this flag. If it was true, actions were already taken\n        self.in_scheduled_downtime_during_last_check = False\n\n        # now is the time to update state_type_id\n        # and our last_hard_state\n        if self.state_type == 'HARD':\n            self.state_type_id = 1\n            self.last_hard_state = self.state\n            self.last_hard_state_id = self.state_id\n        else:\n            self.state_type_id = 0\n\n        # Fill last_hard_state_change to now\n        # if we just change from SOFT->HARD or\n        # in HARD we change of state (Warning->critical, or critical->ok, etc etc)\n        if self.state_type == 'HARD' and \\\n                (self.last_state_type == 'SOFT' or self.last_state != self.state):\n            self.last_hard_state_change = int(time.time())\n\n        if self.state_type == 'HARD':\n            # If the check is a freshness one, set freshness as expired\n            if chk.freshness_expiry_check:\n                self.freshness_expired = True\n                self.last_hard_state_change = int(time.time())\n\n        # update event/problem-counters\n        self.update_event_and_problem_id()\n\n        # Raise a log if freshness check expired\n        if chk.freshness_expiry_check:\n            if os.getenv('ALIGNAK_LOG_CHECKS', None):\n                logger.info(\"--ALC-- freshness expired for %s, when: %s, last checked: %s\",\n                            self.get_full_name(),\n                            datetime.utcfromtimestamp(\n                                self.last_hard_state_change).strftime('%Y-%m-%d %H:%M:%S'),\n                            datetime.utcfromtimestamp(\n                                self.last_state_update).strftime('%Y-%m-%d %H:%M:%S'))\n\n            self.raise_freshness_log_entry(int(now - self.last_state_update -\n                                               self.freshness_threshold))\n\n        self.broks.append(self.get_check_result_brok())\n\n        self.get_perfdata_command(hosts, macromodulations, timeperiods)\n        # Also snapshot if needed :)\n        self.get_snapshot(hosts, macromodulations, timeperiods)\n\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_event_and_problem_id(self):\n        ok_up = self.__class__.ok_up  # OK for service, UP for host\n        if (self.state != self.last_state and self.last_state != 'PENDING' or\n                self.state != ok_up and self.last_state == 'PENDING'):\n            SchedulingItem.current_event_id += 1\n            self.last_event_id = self.current_event_id\n            self.current_event_id = SchedulingItem.current_event_id\n            # now the problem_id\n            if self.state != ok_up and self.last_state == 'PENDING':\n                # broken ever since i can remember\n                SchedulingItem.current_problem_id += 1\n                self.last_problem_id = self.current_problem_id\n                self.current_problem_id = SchedulingItem.current_problem_id\n            elif ok_up not in (self.state, self.last_state):\n                # State transitions between non-OK states\n                # (e.g. WARNING to CRITICAL) do not cause\n                # this problem id to increase.\n                pass\n            elif self.state == ok_up:\n                # If the service is currently in an OK state,\n                # this macro will be set to zero (0).\n                self.last_problem_id = self.current_problem_id\n                self.current_problem_id = 0\n            else:\n                # Every time a service (or host) transitions from\n                # an OK or UP state to a problem state, a global\n                # problem ID number is incremented by one (1).\n                SchedulingItem.current_problem_id += 1\n                self.last_problem_id = self.current_problem_id\n                self.current_problem_id = SchedulingItem.current_problem_id", "response": "Update current_event_id and current_problem_id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_notification_for_sending(self, notif, contact, macromodulations, timeperiods,\n                                         host_ref):\n        \"\"\"Used by scheduler when a notification is ok to be sent (to reactionner).\n        Here we update the command with status of now, and we add the contact to set of\n        contact we notified. And we raise the log entry\n\n        :param notif: notification to send\n        :type notif: alignak.objects.notification.Notification\n        :param macromodulations: Macro modulations objects, used in the notification command\n        :type macromodulations: alignak.objects.macromodulation.Macromodulations\n        :param timeperiods: Timeperiods objects, used to get modulation period\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param host_ref: reference host (used for a service)\n        :type host_ref: alignak.object.host.Host\n        :return: None\n        \"\"\"\n        if notif.status == ACT_STATUS_POLLED:\n            self.update_notification_command(notif, contact, macromodulations, timeperiods,\n                                             host_ref)\n            self.notified_contacts.add(contact.get_name())\n            self.notified_contacts_ids.add(contact.uuid)\n            self.raise_notification_log_entry(notif, contact, host_ref)", "response": "Prepare the notification for sending."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the notification command by resolving Macros", "response": "def update_notification_command(self, notif, contact, macromodulations, timeperiods,\n                                    host_ref=None):\n        \"\"\"Update the notification command by resolving Macros\n        And because we are just launching the notification, we can say\n        that this contact has been notified\n\n        :param notif: notification to send\n        :type notif: alignak.objects.notification.Notification\n        :param contact: contact for this host/service\n        :type contact: alignak.object.contact.Contact\n        :param macromodulations: Macro modulations objects, used in the notification command\n        :type macromodulations: alignak.objects.macromodulation.Macromodulations\n        :param timeperiods: Timeperiods objects, used to get modulation period\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param host_ref: reference host (used for a service)\n        :type host_ref: alignak.object.host.Host\n        :return: None\n        \"\"\"\n        cls = self.__class__\n        macrosolver = MacroResolver()\n        data = self.get_data_for_notifications(contact, notif, host_ref)\n        notif.command = macrosolver.resolve_command(notif.command_call, data, macromodulations,\n                                                    timeperiods)\n        if cls.enable_environment_macros or notif.enable_environment_macros:\n            notif.env = macrosolver.get_env_macros(data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_escalable(self, notification, escalations, timeperiods):\n        cls = self.__class__\n\n        # We search since when we are in notification for escalations\n        # that are based on time\n        in_notif_time = time.time() - notification.creation_time\n\n        # Check is an escalation match the current_notification_number\n        for escalation_id in self.escalations:\n            escalation = escalations[escalation_id]\n            escalation_period = timeperiods[escalation.escalation_period]\n            if escalation.is_eligible(notification.t_to_go, self.state, notification.notif_nb,\n                                      in_notif_time, cls.interval_length, escalation_period):\n                return True\n\n        return False", "response": "Check if a notification can be escalated."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_next_notification_time(self, notif, escalations, timeperiods):\n        # pylint: disable=too-many-locals\n        \"\"\"Get the next notification time for a notification\n        Take the standard notification_interval or ask for our escalation\n        if one of them need a smaller value to escalade\n\n        :param notif: Notification we need time\n        :type notif: alignak.objects.notification.Notification\n        :param escalations: Esclations objects, used to get escalation objects (interval, period)\n        :type escalations: alignak.objects.escalation.Escalations\n        :param timeperiods: Timeperiods objects, used to get escalation period\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :return: Timestamp of next notification\n        :rtype: int\n        \"\"\"\n        res = None\n        now = time.time()\n        cls = self.__class__\n\n        # Look at the minimum notification interval\n        notification_interval = self.notification_interval\n        # and then look for currently active notifications, and take notification_interval\n        # if filled and less than the self value\n        in_notif_time = time.time() - notif.creation_time\n        for escalation_id in self.escalations:\n            escalation = escalations[escalation_id]\n            escalation_period = timeperiods[escalation.escalation_period]\n            if escalation.is_eligible(notif.t_to_go, self.state, notif.notif_nb,\n                                      in_notif_time, cls.interval_length, escalation_period):\n                if escalation.notification_interval != -1 and \\\n                        escalation.notification_interval < notification_interval:\n                    notification_interval = escalation.notification_interval\n\n        # So take the by default time\n        std_time = notif.t_to_go + notification_interval * cls.interval_length\n\n        # Maybe the notification comes from retention data and\n        # next notification alert is in the past\n        # if so let use the now value instead\n        if std_time < now:\n            std_time = now + notification_interval * cls.interval_length\n\n        # standard time is a good one\n        res = std_time\n\n        creation_time = notif.creation_time\n        in_notif_time = now - notif.creation_time\n\n        for escalation_id in self.escalations:\n            escalation = escalations[escalation_id]\n            # If the escalation was already raised, we do not look for a new \"early start\"\n            if escalation.get_name() not in notif.already_start_escalations:\n                escalation_period = timeperiods[escalation.escalation_period]\n                next_t = escalation.get_next_notif_time(std_time, self.state,\n                                                        creation_time, cls.interval_length,\n                                                        escalation_period)\n                # If we got a real result (time base escalation), we add it\n                if next_t is not None and now < next_t < res:\n                    res = next_t\n\n        # And we take the minimum of this result. Can be standard or escalation asked\n        return res", "response": "Get the next notification time for a given notification"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all contacts that can be notified for this notification", "response": "def get_escalable_contacts(self, notification, escalations, timeperiods):\n        \"\"\"Get all contacts (uniq) from eligible escalations\n\n        :param notification: Notification to get data from (notif number...)\n        :type notification: alignak.objects.notification.Notification\n        :param escalations: Esclations objects, used to get escalation objects (contact, period)\n        :type escalations: alignak.objects.escalation.Escalations\n        :param timeperiods: Timeperiods objects, used to get escalation period\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n\n        :return: Contact uuid list that can be notified for escalation\n        :rtype: list\n        \"\"\"\n        cls = self.__class__\n\n        # We search since when we are in notification for escalations\n        # that are based on this time\n        in_notif_time = time.time() - notification.creation_time\n\n        contacts = set()\n        for escalation_id in self.escalations:\n            escalation = escalations[escalation_id]\n\n            escalation_period = timeperiods[escalation.escalation_period]\n            if escalation.is_eligible(notification.t_to_go, self.state, notification.notif_nb,\n                                      in_notif_time, cls.interval_length, escalation_period):\n                contacts.update(escalation.contacts)\n                # And we tag this escalations as started now\n                notification.already_start_escalations.add(escalation.get_name())\n\n        return list(contacts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a master notification for the given type of notification.", "response": "def create_notifications(self, n_type, notification_period, hosts, services,\n                             t_wished=None, author_data=None):\n        \"\"\"Create a \"master\" notification here, which will later\n        (immediately before the reactionner gets it) be split up\n        in many \"child\" notifications, one for each contact.\n\n        :param n_type: notification type (\"PROBLEM\", \"RECOVERY\" ...)\n        :type n_type: str\n        :param notification_period: notification period for this host/service\n        :type notification_period: alignak.objects.timeperiod.Timeperiod\n        :param hosts: hosts objects, used to check if a notif is blocked\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services objects, used to check if a notif is blocked\n        :type services: alignak.objects.service.Services\n        :param t_wished: time we want to notify\n        :type t_wished: int\n        :param author_data: notification author data (eg. for a downtime notification)\n        :type author_data: dict (containing author, author_name ad a comment)\n        :return: None\n        \"\"\"\n        cls = self.__class__\n        # t_wished==None for the first notification launch after consume\n        # here we must look at the self.notification_period\n        if t_wished is None:\n            t_wished = time.time()\n            # if first notification, we must add first_notification_delay\n            if self.current_notification_number == 0 and n_type == 'PROBLEM':\n                last_time_non_ok_or_up = self.last_time_non_ok_or_up()\n                if last_time_non_ok_or_up:\n                    # last_time_non_ok_or_up is an integer value - set the next second\n                    t_wished = last_time_non_ok_or_up + 1\n                t_wished = t_wished + self.first_notification_delay * cls.interval_length\n\n            if notification_period is None:\n                new_t = t_wished\n            else:\n                new_t = notification_period.get_next_valid_time_from_t(t_wished)\n        else:\n            # We follow our order\n            new_t = t_wished\n\n        if self.is_blocking_notifications(notification_period, hosts, services,\n                                          n_type, t_wished) and \\\n                self.first_notification_delay == 0 and self.notification_interval == 0:\n            # If notifications are blocked on the host/service level somehow\n            # and repeated notifications are not configured,\n            # we can silently drop this one\n            return\n\n        if n_type == u'PROBLEM':\n            # Create the notification with an incremented notification_number.\n            # The current_notification_number  of the item itself will only\n            # be incremented when this notification (or its children)\n            # have actually been sent.\n            next_notif_nb = self.current_notification_number + 1\n        elif n_type == u'RECOVERY':\n            # Recovery resets the notification counter to zero\n            self.current_notification_number = 0\n            next_notif_nb = self.current_notification_number\n        else:\n            # downtime/flap/etc do not change the notification number\n            next_notif_nb = self.current_notification_number\n\n        data = {\n            'status': u'scheduled',\n            'type': n_type,\n            'command': u'VOID',\n            'ref': self.uuid,\n            't_to_go': new_t,\n            'timeout': cls.notification_timeout,\n            'notif_nb': next_notif_nb,\n            'host_name': getattr(self, 'host_name', ''),\n            'service_description': getattr(self, 'service_description', ''),\n        }\n        if author_data and n_type in [u'DOWNTIMESTART', u'DOWNTIMEEND']:\n            data.update(author_data)\n\n        notif = Notification(data)\n        logger.debug(\"Created a %s notification: %s\", self.my_type, n_type)\n\n        # Keep a trace in our notifications queue\n        self.notifications_in_progress[notif.uuid] = notif\n        # and put it in our queue for the scheduler to pick it up\n        self.actions.append(notif)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef launch_check(self, timestamp, hosts, services, timeperiods,\n                     macromodulations, checkmodulations, checks, ref_check=None, force=False,\n                     dependent=False):\n        # pylint: disable=too-many-locals, too-many-arguments\n        # pylint: disable=too-many-branches, too-many-return-statements\n        \"\"\"Launch a check (command)\n\n        :param timestamp:\n        :type timestamp: int\n        :param checkmodulations: Checkmodulations objects, used to change check command if necessary\n        :type checkmodulations: alignak.objects.checkmodulation.Checkmodulations\n        :param ref_check:\n        :type ref_check:\n        :param force:\n        :type force: bool\n        :param dependent:\n        :type dependent: bool\n        :return: None or alignak.check.Check\n        :rtype: None | alignak.check.Check\n        \"\"\"\n        chk = None\n        cls = self.__class__\n\n        # Look if we are in check or not\n        self.update_in_checking()\n\n        # the check is being forced, so we just replace next_chk time by now\n        if force and self.in_checking:\n            try:\n                c_in_progress = checks[self.checks_in_progress[0]]\n                c_in_progress.t_to_go = time.time()\n                return c_in_progress\n            except KeyError:\n                pass\n\n        # If I'm already in checking, Why launch a new check?\n        # If ref_check_id is not None , this is a dependency_ check\n        # If none, it might be a forced check, so OK, I do a new\n\n        # Dependency check, we have to create a new check that will be launched only once (now)\n        # Otherwise it will delay the next real check. this can lead to an infinite SOFT state.\n        if not force and (self.in_checking and ref_check is not None):\n\n            c_in_progress = checks[self.checks_in_progress[0]]\n\n            # c_in_progress has almost everything we need but we cant copy.deepcopy() it\n            # we need another c.uuid\n            data = {\n                'command': c_in_progress.command,\n                'timeout': c_in_progress.timeout,\n                'poller_tag': c_in_progress.poller_tag,\n                'env': c_in_progress.env,\n                'module_type': c_in_progress.module_type,\n                't_to_go': timestamp,\n                'depend_on_me': [ref_check],\n                'ref': self.uuid,\n                'ref_type': self.my_type,\n                'dependency_check': True,\n                'internal': self.got_business_rule or c_in_progress.command.startswith('_')\n            }\n            chk = Check(data)\n\n            self.actions.append(chk)\n\n            if os.getenv('ALIGNAK_LOG_CHECKS', None):\n                logger.info(\"--ALC-- -> added a check action for %s (%s)\",\n                            self.get_full_name(), chk.uuid)\n            return chk\n\n        if force or (not self.is_no_check_dependent(hosts, services, timeperiods)):\n            if self.my_type == 'service' and not self.check_command:\n                # This should never happen because of configuration check!\n                logger.debug(\"Service check is for a service that has no check command (%s/%s), \"\n                             \"do not launch the check !\", self.host_name, self.service_description)\n                return None\n\n            if self.my_type == 'host' and not self.check_command:\n                if self.active_checks_enabled:\n                    logger.debug(\"Host check is for an host that has no check command (%s), \"\n                                 \"do not launch the check !\", self.host_name)\n                    return None\n\n            # Fred : passive only checked host dependency\n            if dependent and self.my_type == 'host' and \\\n                    self.passive_checks_enabled and not self.active_checks_enabled:\n                logger.debug(\"Host check (dependent) is for an host that is only passively \"\n                             \"checked (%s), do not launch the check !\", self.host_name)\n                return None\n\n            # By default env is void\n            env = {}\n            poller_tag = u'None'\n            module_type = None\n\n            # By default we will use our default check_command\n            self.last_check_command = None\n            check_command = self.check_command\n            command_line = ''\n            if check_command:\n                poller_tag = check_command.poller_tag\n                module_type = check_command.module_type\n\n                # But if a checkway is available, use this one instead.\n                # Take the first available\n                for chkmod_id in self.checkmodulations:\n                    chkmod = checkmodulations[chkmod_id]\n                    c_cw = chkmod.get_check_command(timeperiods, timestamp)\n                    if c_cw:\n                        check_command = c_cw\n                        break\n\n                # Get the command to launch\n                macroresolver = MacroResolver()\n                data = self.get_data_for_checks(hosts)\n                command_line = macroresolver.resolve_command(check_command, data,\n                                                             macromodulations, timeperiods)\n\n                # remember it, for pure debugging purpose\n                self.last_check_command = command_line\n\n            # And get all environment variables only if needed\n            if cls.enable_environment_macros or (check_command and\n                                                 check_command.enable_environment_macros):\n                env = macroresolver.get_env_macros(data)\n\n            # By default we take the global timeout, but we use the command one if it\n            # is defined (default is -1 for no timeout)\n            timeout = cls.check_timeout\n            if check_command and check_command.timeout != -1:\n                timeout = check_command.timeout\n\n            # Make the Check object and put the service in checking\n            # Make the check inherit poller_tag from the command\n            # And reactionner_tag too\n            data = {\n                'command': command_line,\n                'timeout': timeout,\n                'poller_tag': poller_tag,\n                'env': env,\n                'module_type': module_type,\n                't_to_go': timestamp,\n                'depend_on_me': [ref_check] if ref_check else [],\n                'ref': self.uuid,\n                'ref_type': self.my_type,\n                'internal': self.got_business_rule or command_line.startswith('_')\n            }\n            chk = Check(data)\n\n            self.checks_in_progress.append(chk.uuid)\n\n        self.update_in_checking()\n\n        # We need to put this new check in our actions queue\n        # so scheduler can take it\n        if chk is not None:\n            self.actions.append(chk)\n\n            if os.getenv('ALIGNAK_LOG_CHECKS', None):\n                logger.info(\"--ALC-- -> added a check action for %s (%s)\",\n                            self.get_full_name(), chk.uuid)\n            return chk\n        # None mean I already take it into account\n        return None", "response": "Launch a check for the given timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding event_handler to process performance data if necessary", "response": "def get_perfdata_command(self, hosts, macromodulations, timeperiods):\n        \"\"\"Add event_handler to process performance data if necessary (not disabled)\n\n        :param macromodulations: Macro modulations objects, used in commands (notif, check)\n        :type macromodulations: alignak.objects.macromodulation.Macromodulations\n        :return: None\n        \"\"\"\n        cls = self.__class__\n        if not cls.process_performance_data or not self.process_perf_data:\n            return\n\n        if cls.perfdata_command is not None:\n            macroresolver = MacroResolver()\n            data = self.get_data_for_event_handler(hosts)\n            cmd = macroresolver.resolve_command(cls.perfdata_command, data, macromodulations,\n                                                timeperiods)\n            reactionner_tag = cls.perfdata_command.reactionner_tag\n            event_h = EventHandler({\n                'command': cmd,\n                'timeout': cls.perfdata_timeout,\n                'ref': self.uuid,\n                'reactionner_tag': reactionner_tag\n            })\n\n            # ok we can put it in our temp action queue\n            self.actions.append(event_h)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_business_rules(self, hosts, services, hostgroups, servicegroups,\n                              macromodulations, timeperiods, running=False):\n        # pylint: disable=too-many-locals\n        \"\"\"Create business rules if necessary (cmd contains bp_rule)\n\n        :param hosts: Hosts object to look for objects\n        :type hosts: alignak.objects.host.Hosts\n        :param services: Services object to look for objects\n        :type services: alignak.objects.service.Services\n        :param running: flag used in eval_cor_pattern function\n        :type running: bool\n        :return: None\n        \"\"\"\n        cmdcall = getattr(self, 'check_command', None)\n\n        # If we do not have a command, we bailout\n        if cmdcall is None:\n            return\n\n        # we get our base command, like\n        # bp_rule!(host,svc & host, svc) -> bp_rule\n        cmd = cmdcall.call\n        elts = cmd.split('!')\n        base_cmd = elts[0]\n\n        # If it's bp_rule, we got a rule :)\n        if base_cmd == 'bp_rule':\n            self.got_business_rule = True\n            rule = ''\n            if len(elts) >= 2:\n                rule = '!'.join(elts[1:])\n            # Only (re-)evaluate the business rule if it has never been\n            # evaluated before, or it contains a macro.\n            if re.match(r\"\\$[\\w\\d_-]+\\$\", rule) or self.business_rule is None:\n                macroresolver = MacroResolver()\n                data = self.get_data_for_checks(hosts)\n                rule = macroresolver.resolve_simple_macros_in_string(rule, data,\n                                                                     macromodulations,\n                                                                     timeperiods)\n                prev = getattr(self, \"processed_business_rule\", \"\")\n                if rule == prev:\n                    # Business rule did not changed (no macro was modulated)\n                    return\n\n                fact = DependencyNodeFactory(self)\n                node = fact.eval_cor_pattern(rule, hosts, services,\n                                             hostgroups, servicegroups, running)\n                self.processed_business_rule = rule\n                self.business_rule = node", "response": "Create business rules if necessary"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string for business rules based items formatted for the given service and macromodulations and timeperiods.", "response": "def get_business_rule_output(self, hosts, services, macromodulations, timeperiods):\n        # pylint: disable=too-many-locals, too-many-branches\n        \"\"\"\n        Returns a status string for business rules based items formatted\n        using business_rule_output_template attribute as template.\n\n        The template may embed output formatting for itself, and for its child\n        (dependent) items. Child format string is expanded into the $( and )$,\n        using the string between brackets as format string.\n\n        Any business rule based item or child macro may be used. In addition,\n        the $STATUS$, $SHORTSTATUS$ and $FULLNAME$ macro which name is common\n        to hosts and services may be used to ease template writing.\n\n        Caution: only children in state not OK are displayed.\n\n        Example:\n          A business rule with a format string looking like\n              \"$STATUS$ [ $($TATUS$: $HOSTNAME$,$SERVICEDESC$ )$ ]\"\n          Would return\n              \"CRITICAL [ CRITICAL: host1,srv1 WARNING: host2,srv2  ]\"\n\n        :param hosts: Hosts object to look for objects\n        :type hosts: alignak.objects.host.Hosts\n        :param services: Services object to look for objects\n        :type services: alignak.objects.service.Services\n        :param macromodulations: Macromodulations object to look for objects\n        :type macromodulations: alignak.objects.macromodulation.Macromodulations\n        :param timeperiods: Timeperiods object to look for objects\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :return: status for business rules\n        :rtype: str\n        \"\"\"\n        got_business_rule = getattr(self, 'got_business_rule', False)\n        # Checks that the service is a business rule.\n        if got_business_rule is False or self.business_rule is None:\n            return \"\"\n        # Checks that the business rule has a format specified.\n        output_template = self.business_rule_output_template\n        if not output_template:\n            return \"\"\n        macroresolver = MacroResolver()\n\n        # Extracts children template strings\n        elts = re.findall(r\"\\$\\((.*)\\)\\$\", output_template)\n        if not elts:\n            child_template_string = \"\"\n        else:\n            child_template_string = elts[0]\n\n        # Processes child services output\n        children_output = \"\"\n        ok_count = 0\n        # Expands child items format string macros.\n        items = self.business_rule.list_all_elements()\n        for item_uuid in items:\n            if item_uuid in hosts:\n                item = hosts[item_uuid]\n            elif item_uuid in services:\n                item = services[item_uuid]\n\n            # Do not display children in OK state\n            # todo: last_hard_state ? why not current state if state type is hard ?\n            if item.last_hard_state_id == 0:\n                ok_count += 1\n                continue\n            data = item.get_data_for_checks(hosts)\n            children_output += macroresolver.resolve_simple_macros_in_string(child_template_string,\n                                                                             data,\n                                                                             macromodulations,\n                                                                             timeperiods)\n\n        if ok_count == len(items):\n            children_output = \"all checks were successful.\"\n\n        # Replaces children output string\n        template_string = re.sub(r\"\\$\\(.*\\)\\$\", children_output, output_template)\n        data = self.get_data_for_checks(hosts)\n        output = macroresolver.resolve_simple_macros_in_string(template_string, data,\n                                                               macromodulations, timeperiods)\n        return output.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fill_data_brok_from(self, data, brok_type):\n        super(SchedulingItem, self).fill_data_brok_from(data, brok_type)\n        # workaround/easy trick to have the command_name of this\n        # SchedulingItem in its check_result brok\n        if brok_type == 'check_result':\n            data['command_name'] = ''\n            if self.check_command:\n                data['command_name'] = self.check_command.command.command_name", "response": "Fill data brok dependent on the brok_type"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef acknowledge_problem(self, notification_period, hosts, services, sticky, notify, author,\n                            comment, end_time=0):\n        # pylint: disable=too-many-arguments\n        \"\"\"\n        Add an acknowledge\n\n        :param sticky: acknowledge will be always present is host return in UP state\n        :type sticky: integer\n        :param notify: if to 1, send a notification\n        :type notify: integer\n        :param author: name of the author or the acknowledge\n        :type author: str\n        :param comment: comment (description) of the acknowledge\n        :type comment: str\n        :param end_time: end (timeout) of this acknowledge in seconds(timestamp) (0 to never end)\n        :type end_time: int\n        :return: None | alignak.comment.Comment\n        \"\"\"\n        comm = None\n        logger.debug(\"Acknowledge requested for %s %s.\", self.my_type, self.get_name())\n\n        if self.state != self.ok_up:\n            # case have yet an acknowledge\n            if self.problem_has_been_acknowledged and self.acknowledgement:\n                self.del_comment(getattr(self.acknowledgement, 'comment_id', None))\n\n            if notify:\n                self.create_notifications('ACKNOWLEDGEMENT',\n                                          notification_period, hosts, services)\n\n            self.problem_has_been_acknowledged = True\n            sticky = sticky == 2\n\n            data = {\n                'ref': self.uuid, 'sticky': sticky, 'author': author, 'comment': comment,\n                'end_time': end_time, 'notify': notify\n            }\n            self.acknowledgement = Acknowledge(data)\n            if self.my_type == 'host':\n                comment_type = 1\n                self.broks.append(self.acknowledgement.get_raise_brok(self.get_name()))\n            else:\n                comment_type = 2\n                self.broks.append(self.acknowledgement.get_raise_brok(self.host_name,\n                                                                      self.get_name()))\n            data = {\n                'author': author, 'comment': comment, 'comment_type': comment_type, 'entry_type': 4,\n                'source': 0, 'expires': False, 'ref': self.uuid\n            }\n            comm = Comment(data)\n            self.acknowledgement.comment_id = comm.uuid\n            self.comments[comm.uuid] = comm\n            self.broks.append(self.get_update_status_brok())\n            self.raise_acknowledge_log_entry()\n        else:\n            logger.debug(\"Acknowledge requested for %s %s but element state is OK/UP.\",\n                         self.my_type, self.get_name())\n\n        # For an host, acknowledge all its services that are problems\n        if self.my_type == 'host':\n            for service_uuid in self.services:\n                if service_uuid not in services:\n                    continue\n                services[service_uuid].acknowledge_problem(notification_period, hosts, services,\n                                                           sticky, notify, author, comment,\n                                                           end_time)\n        return comm", "response": "This function is called by alignak to acknowledge a problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_for_expire_acknowledge(self):\n        if (self.acknowledgement and\n                self.acknowledgement.end_time != 0 and\n                self.acknowledgement.end_time < time.time()):\n            self.unacknowledge_problem()", "response": "Check if the acknowledge and is expired."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unacknowledge_problem(self):\n        if self.problem_has_been_acknowledged:\n            logger.debug(\"[item::%s] deleting acknowledge of %s\",\n                         self.get_name(),\n                         self.get_full_name())\n            self.problem_has_been_acknowledged = False\n            if self.my_type == 'host':\n                self.broks.append(self.acknowledgement.get_expire_brok(self.get_name()))\n            else:\n                self.broks.append(self.acknowledgement.get_expire_brok(self.host_name,\n                                                                       self.get_name()))\n\n            # delete the comment of the item related with the acknowledge\n            if hasattr(self.acknowledgement, 'comment_id') and \\\n                    self.acknowledgement.comment_id in self.comments:\n                del self.comments[self.acknowledgement.comment_id]\n\n            # Should not be deleted, a None is Good\n            self.acknowledgement = None\n            self.broks.append(self.get_update_status_brok())\n            self.raise_unacknowledge_log_entry()", "response": "Remove the acknowledge and reset the flag."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unacknowledge_problem_if_not_sticky(self):\n        if hasattr(self, 'acknowledgement') and self.acknowledgement is not None:\n            if not self.acknowledgement.sticky:\n                self.unacknowledge_problem()", "response": "Remove the acknowledge if it is not sticky"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises the freshness alert entry.", "response": "def raise_freshness_log_entry(self, t_stale_by):\n        \"\"\"Raise freshness alert entry (warning level)\n\n        Example : \"The freshness period of host 'host_name' is expired\n                   by 0d 0h 17m 6s (threshold=0d 1h 0m 0s).\n                   Attempt: 1 / 1.\n                   I'm forcing the state to freshness state (d / HARD)\"\n\n        :param t_stale_by: time in seconds the host has been in a stale state\n        :type t_stale_by: int\n        :return: None\n        \"\"\"\n        logger.warning(\"The freshness period of %s '%s' is expired by %ss \"\n                       \"(threshold=%ss + %ss). Attempt: %s / %s. \"\n                       \"I'm forcing the state to freshness state (%s / %s).\",\n                       self.my_type, self.get_full_name(),\n                       t_stale_by, self.freshness_threshold,\n                       self.additional_freshness_latency,\n                       self.attempt, self.max_check_attempts,\n                       self.freshness_state, self.state_type)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_impact_state(self):\n        cls = self.__class__\n        if cls.enable_problem_impacts_states_change:\n            logger.debug(\"%s is impacted and goes UNREACHABLE\", self)\n\n            # Track the old state (problem occured before a new check)\n            self.state_before_impact = self.state\n            self.state_id_before_impact = self.state_id\n            # This flag will know if we override the impact state\n            self.state_changed_since_impact = False\n            # Set unreachable\n            self.set_unreachable()", "response": "Set the impact state of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_unreachable(self):\n        self.state_id = 4\n        self.state = u'UNREACHABLE'\n        self.last_time_unreachable = int(time.time())", "response": "Set unreachable: all our parents (dependencies) are not ok\n        Unreachable is different from down/critical\n\n        :return:None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if this object configuration is correct.", "response": "def is_correct(self):\n        # pylint: disable=too-many-branches\n        \"\"\"\n        Check if this object configuration is correct ::\n\n        * Check our own specific properties\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        state = True\n\n        if hasattr(self, 'trigger') and getattr(self, 'trigger', None):\n            self.add_warning(\"[%s::%s] 'trigger' property is not allowed\"\n                             % (self.my_type, self.get_name()))\n\n        # If no notif period, set it to None, mean 24x7\n        if not hasattr(self, 'notification_period'):\n            self.notification_period = None\n\n        # If freshness_threshold is not set, use check interval or retry interval\n        if hasattr(self, 'freshness_threshold') and not self.freshness_threshold:\n            if getattr(self, 'check_interval', 0):\n                self.freshness_threshold = self.check_interval * 60\n                # self.add_warning(\"[%s::%s] using check interval as a freshness threshold: %d s\"\n                #                  % (self.my_type, self.get_name(), self.freshness_threshold))\n            elif getattr(self, 'retry_interval', 0):\n                self.freshness_threshold = self.retry_interval * 60\n                # self.add_warning(\"[%s::%s] using retry interval as a freshness threshold: %d s\"\n                #                  % (self.my_type, self.get_name(), self.freshness_threshold))\n\n        # If we got an event handler, it should be valid\n        if getattr(self, 'event_handler', None) and not self.event_handler.is_valid():\n            self.add_error(\"[%s::%s] event_handler '%s' is invalid\"\n                           % (self.my_type, self.get_name(), self.event_handler.command))\n            state = False\n\n        if not hasattr(self, 'check_command'):\n            # todo: This should never happen because the default exists as an empty string\n            self.add_error(\"[%s::%s] no property check_command\" % (self.my_type, self.get_name()))\n            state = False\n        # Ok got a command, but maybe it's invalid\n        else:\n            # if not self.check_command:\n            #     self.add_warning(\"[%s::%s] no check_command, will always be considered as Up\"\n            #                      % (self.my_type, self.get_name()))\n            if self.check_command and not self.check_command.is_valid():\n                self.add_error(\"[%s::%s] check_command '%s' invalid\"\n                               % (self.my_type, self.get_name(), self.check_command.command))\n                state = False\n            if self.got_business_rule:\n                if not self.business_rule.is_valid():\n                    self.add_error(\"[%s::%s] business_rule invalid\"\n                                   % (self.my_type, self.get_name()))\n                    for bperror in self.business_rule.configuration_errors:\n                        self.add_error(\"[%s::%s]: %s\" % (self.my_type, self.get_name(), bperror))\n                    state = False\n\n        if not hasattr(self, 'notification_interval') \\\n                and self.notifications_enabled is True:  # pragma: no cover, should never happen\n            self.add_error(\"[%s::%s] no notification_interval but notifications enabled\"\n                           % (self.my_type, self.get_name()))\n            state = False\n\n        # if no check_period, means 24x7, like for services\n        if not hasattr(self, 'check_period'):\n            self.check_period = None\n\n        state = super(SchedulingItem, self).is_correct()\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_by_filter(self, filters, all_items):\n        items = []\n        for i in self:\n            failed = False\n            if hasattr(i, \"host\"):\n                all_items[\"service\"] = i\n            else:\n                all_items[\"host\"] = i\n            for filt in filters:\n                if not filt(all_items):\n                    failed = True\n                    break\n            if failed is False:\n                items.append(i)\n        return items", "response": "Find items by filters"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_act_dependency(self, son_id, parent_id, notif_failure_criteria, dep_period,\n                           inherits_parents):\n        \"\"\"\n        Add a logical dependency for actions between two hosts or services.\n\n        :param son_id: uuid of son host\n        :type son_id: str\n        :param parent_id: uuid of parent host\n        :type parent_id: str\n        :param notif_failure_criteria: notification failure criteria,\n        notification for a dependent host may vary\n        :type notif_failure_criteria: list\n        :param dep_period: dependency period. Timeperiod for dependency may vary\n        :type dep_period: str | None\n        :param inherits_parents: if this dep will inherit from parents (timeperiod, status)\n        :type inherits_parents: bool\n        :return:\n        \"\"\"\n        if son_id in self:\n            son = self[son_id]\n        else:\n            msg = \"Dependency son (%s) unknown, configuration error\" % son_id\n            self.add_error(msg)\n        parent = self[parent_id]\n        son.act_depend_of.append((parent_id, notif_failure_criteria, dep_period, inherits_parents))\n        parent.act_depend_of_me.append((son_id, notif_failure_criteria, dep_period,\n                                        inherits_parents))\n\n        # TODO: Is it necessary? We already have this info in act_depend_* attributes\n        son.parent_dependencies.add(parent_id)\n        parent.child_dependencies.add(son_id)", "response": "Add a logical dependency for actions between two hosts or services."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving act_dependency between two hosts or services.", "response": "def del_act_dependency(self, son_id, parent_id):  # pragma: no cover, not yet tested\n        \"\"\"Remove act_dependency between two hosts or services.\n\n        TODO: do we really intend to remove dynamically ?\n\n        :param son_id: uuid of son host/service\n        :type son_id: str\n        :param parent_id: uuid of parent host/service\n        :type parent_id: str\n        :return: None\n        \"\"\"\n        son = self[son_id]\n        parent = self[parent_id]\n        to_del = []\n        # First we remove in my list\n        for (host, status, timeperiod, inherits_parent) in son.act_depend_of:\n            if host == parent_id:\n                to_del.append((host, status, timeperiod, inherits_parent))\n        for tup in to_del:\n            son.act_depend_of.remove(tup)\n\n        # And now in the father part\n        to_del = []\n        for (host, status, timeperiod, inherits_parent) in parent.act_depend_of_me:\n            if host == son_id:\n                to_del.append((host, status, timeperiod, inherits_parent))\n        for tup in to_del:\n            parent.act_depend_of_me.remove(tup)\n\n        # Remove in child/parents dependencies too\n        # Me in father list\n        parent.child_dependencies.remove(son_id)\n        # and father list in mine\n        son.parent_dependencies.remove(parent_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_chk_dependency(self, son_id, parent_id, notif_failure_criteria, dep_period,\n                           inherits_parents):\n        \"\"\"\n        Add a logical dependency for checks between two hosts or services.\n\n        :param son_id: uuid of son host/service\n        :type son_id: str\n        :param parent_id: uuid of parent host/service\n        :type parent_id: str\n        :param notif_failure_criteria: notification failure criteria,\n        notification for a dependent host may vary\n        :type notif_failure_criteria: list\n        :param dep_period: dependency period. Timeperiod for dependency may vary\n        :type dep_period: str\n        :param inherits_parents: if this dep will inherit from parents (timeperiod, status)\n        :type inherits_parents: bool\n        :return:\n        \"\"\"\n        son = self[son_id]\n        parent = self[parent_id]\n        son.chk_depend_of.append((parent_id, notif_failure_criteria, 'logic_dep', dep_period,\n                                  inherits_parents))\n        parent.chk_depend_of_me.append((son_id, notif_failure_criteria, 'logic_dep', dep_period,\n                                        inherits_parents))\n\n        # TODO: Is it necessary? We already have this info in act_depend_* attributes\n        son.parent_dependencies.add(parent_id)\n        parent.child_dependencies.add(son_id)", "response": "Adds a logical dependency between two hosts or services."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates business rules for the given hosts services macromodulations and timeperiods.", "response": "def create_business_rules(self, hosts, services, hostgroups, servicegroups,\n                              macromodulations, timeperiods):\n        \"\"\"\n        Loop on hosts or services and call SchedulingItem.create_business_rules\n\n        :param hosts: hosts to link to\n        :type hosts: alignak.objects.host.Hosts\n        :param services: services to link to\n        :type services: alignak.objects.service.Services\n        :param hostgroups: hostgroups to link to\n        :type hostgroups: alignak.objects.hostgroup.Hostgroups\n        :param servicegroups: servicegroups to link to\n        :type servicegroups: alignak.objects.servicegroup.Servicegroups\n        :param macromodulations: macromodulations to link to\n        :type macromodulations: alignak.objects.macromodulation.Macromodulations\n        :param timeperiods: timeperiods to link to\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :return: None\n        \"\"\"\n        for item in self:\n            item.create_business_rules(hosts, services, hostgroups,\n                                       servicegroups, macromodulations, timeperiods)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all services of this servicegroup and add it in members container", "response": "def get_services_by_explosion(self, servicegroups):\n        # pylint: disable=access-member-before-definition\n        \"\"\"\n        Get all services of this servicegroup and add it in members container\n\n        :param servicegroups: servicegroups object\n        :type servicegroups: alignak.objects.servicegroup.Servicegroups\n        :return: return empty string or list of members\n        :rtype: str or list\n        \"\"\"\n        # First we tag the hg so it will not be explode\n        # if a son of it already call it\n        self.already_exploded = True\n\n        # Now the recursive part\n        # rec_tag is set to False every HG we explode\n        # so if True here, it must be a loop in HG\n        # calls... not GOOD!\n        if self.rec_tag:\n            logger.error(\"[servicegroup::%s] got a loop in servicegroup definition\",\n                         self.get_name())\n            if hasattr(self, 'members'):\n                return self.members\n\n            return ''\n        # Ok, not a loop, we tag it and continue\n        self.rec_tag = True\n\n        sg_mbrs = self.get_servicegroup_members()\n        for sg_mbr in sg_mbrs:\n            servicegroup = servicegroups.find_by_name(sg_mbr.strip())\n            if servicegroup is not None:\n                value = servicegroup.get_services_by_explosion(servicegroups)\n                if value is not None:\n                    self.add_members(value)\n\n        if hasattr(self, 'members'):\n            return self.members\n\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a member to a servicegroup", "response": "def add_member(self, service_name, servicegroup_name):\n        \"\"\"Add a member (service) to this servicegroup\n\n        :param service_name: member (service) name\n        :type service_name: str\n        :param servicegroup_name: servicegroup name\n        :type servicegroup_name: str\n        :return: None\n        \"\"\"\n        servicegroup = self.find_by_name(servicegroup_name)\n        if not servicegroup:\n            servicegroup = Servicegroup({'servicegroup_name': servicegroup_name,\n                                         'alias': servicegroup_name,\n                                         'members': service_name})\n            self.add(servicegroup)\n        else:\n            servicegroup.add_members(service_name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget all members of a group which name is given in parameter", "response": "def get_members_of_group(self, gname):\n        \"\"\"Get all members of a group which name is given in parameter\n\n        :param gname: name of the group\n        :type gname: str\n        :return: list of the services in the group\n        :rtype: list[alignak.objects.service.Service]\n        \"\"\"\n        hostgroup = self.find_by_name(gname)\n        if hostgroup:\n            return hostgroup.get_services()\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef linkify_servicegroups_services(self, hosts, services):\n        for servicegroup in self:\n            mbrs = servicegroup.get_services()\n            # The new member list, in id\n            new_mbrs = []\n            seek = 0\n            host_name = ''\n            if len(mbrs) == 1 and mbrs[0] != '':\n                servicegroup.add_unknown_members('%s' % mbrs[0])\n\n            for mbr in mbrs:\n                if not mbr:\n                    continue\n                if seek % 2 == 0:\n                    host_name = mbr.strip()\n                else:\n                    service_desc = mbr.strip()\n                    find = services.find_srv_by_name_and_hostname(host_name, service_desc)\n                    if find is not None:\n                        new_mbrs.append(find.uuid)\n                    else:\n                        host = hosts.find_by_name(host_name)\n                        if not (host and host.is_excluded_for_sdesc(service_desc)):\n                            servicegroup.add_unknown_members('%s,%s' % (host_name, service_desc))\n                        elif host:\n                            self.add_warning('servicegroup %r : %s is excluded from the '\n                                             'services of the host %s'\n                                             % (servicegroup, service_desc, host_name))\n                seek += 1\n\n            # Make members uniq\n            new_mbrs = list(set(new_mbrs))\n\n            # We find the id, we replace the names\n            servicegroup.replace_members(new_mbrs)\n            for srv_id in servicegroup.members:\n                serv = services[srv_id]\n                serv.servicegroups.append(servicegroup.uuid)\n                # and make this uniq\n                serv.servicegroups = list(set(serv.servicegroups))", "response": "Link the services of the servicegroups with the servicegroups."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef explode(self):\n        # We do not want a same service group to be exploded again and again\n        # so we tag it\n        for tmp_sg in list(self.items.values()):\n            tmp_sg.already_exploded = False\n\n        for servicegroup in list(self.items.values()):\n            if servicegroup.already_exploded:\n                continue\n\n            # get_services_by_explosion is a recursive\n            # function, so we must tag hg so we do not loop\n            for tmp_sg in list(self.items.values()):\n                tmp_sg.rec_tag = False\n            servicegroup.get_services_by_explosion(self)\n\n        # We clean the tags\n        for tmp_sg in list(self.items.values()):\n            if hasattr(tmp_sg, 'rec_tag'):\n                del tmp_sg.rec_tag\n            del tmp_sg.already_exploded", "response": "Get services and put them in members container\n\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    try:\n        args = parse_daemon_args(True)\n\n        # Protect for windows multiprocessing that will RELAUNCH all\n        while True:\n            daemon = Arbiter(**args.__dict__)\n            daemon.main()\n            if not daemon.need_config_reload:\n                break\n            daemon = None\n    except Exception as exp:  # pylint: disable=broad-except\n        sys.stderr.write(\"*** Daemon exited because: %s\" % str(exp))\n        traceback.print_exc()\n        exit(1)", "response": "Parse args and run main daemon function\notope"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup_logger(logger_configuration_file, log_dir=None, process_name='', log_file=''):\n    # pylint: disable=too-many-branches\n    \"\"\"\n    Configure the provided logger\n    - get and update the content of the Json configuration file\n    - configure the logger with this file\n\n    If a log_dir and process_name are provided, the format and filename in the configuration file\n    are updated with the provided values if they contain the patterns %(logdir)s and %(daemon)s\n\n    If no log_dir and process_name are provided, this function will truncate the log file\n    defined in the configuration file.\n\n    If a log file name is provided, it will override the default defined log file name.\n\n    At first, this function checks if the logger is still existing and initialized to\n    update the handlers and formatters. This mainly happens during the unit tests.\n\n    :param logger_configuration_file: Python Json logger configuration file\n    :rtype logger_configuration_file: str\n    :param log_dir: default log directory to update the defined logging handlers\n    :rtype log_dir: str\n    :param process_name: process name to update the defined logging formatters\n    :rtype process_name: str\n    :param log_file: log file name to update the defined log file\n    :rtype log_file: str\n    :return: None\n    \"\"\"\n    logger_ = logging.getLogger(ALIGNAK_LOGGER_NAME)\n    for handler in logger_.handlers:\n        if not process_name:\n            break\n        # Logger is already configured?\n        if getattr(handler, '_name', None) == 'daemons':\n            # Update the declared formats and file names with the process name\n            # This is for unit tests purpose only: alignak_tests will be replaced\n            # with the provided process name\n            for hdlr in logger_.handlers:\n                # print(\"- handler : %s (%s)\" % (hdlr, hdlr.formatter._fmt))\n                if 'alignak_tests' in hdlr.formatter._fmt:\n                    formatter = logging.Formatter(hdlr.formatter._fmt.replace(\"alignak_tests\",\n                                                                              process_name))\n                    hdlr.setFormatter(formatter)\n                if getattr(hdlr, 'filename', None) and 'alignak_tests' in hdlr.filename:\n                    hdlr.filename = hdlr.filename._fmt.replace(\"alignak_tests\", process_name)\n                #     print(\"- handler : %s (%s) -> %s\" % (hdlr, hdlr.formatter._fmt,\n                #                                          hdlr.filename))\n                # else:\n                #     print(\"- handler : %s (%s)\" % (hdlr, hdlr.formatter._fmt))\n            break\n    else:\n        if not logger_configuration_file or not os.path.exists(logger_configuration_file):\n            print(\"The logger configuration file does not exist: %s\" % logger_configuration_file)\n            return\n\n        with open(logger_configuration_file, 'rt') as _file:\n            config = json.load(_file)\n            truncate = False\n            if not process_name and not log_dir:\n                truncate = True\n            if not process_name:\n                process_name = 'alignak_tests'\n            if not log_dir:\n                log_dir = '/tmp'\n            # Update the declared formats with the process name\n            for formatter in config['formatters']:\n                if 'format' not in config['formatters'][formatter]:\n                    continue\n                config['formatters'][formatter]['format'] = \\\n                    config['formatters'][formatter]['format'].replace(\"%(daemon)s\", process_name)\n\n            # Update the declared log file names with the log directory\n            for hdlr in config['handlers']:\n                if 'filename' not in config['handlers'][hdlr]:\n                    continue\n                if log_file and hdlr == 'daemons':\n                    config['handlers'][hdlr]['filename'] = log_file\n                else:\n                    config['handlers'][hdlr]['filename'] = \\\n                        config['handlers'][hdlr]['filename'].replace(\"%(logdir)s\", log_dir)\n                config['handlers'][hdlr]['filename'] = \\\n                    config['handlers'][hdlr]['filename'].replace(\"%(daemon)s\", process_name)\n                if truncate and os.path.exists(config['handlers'][hdlr]['filename']):\n                    with open(config['handlers'][hdlr]['filename'], \"w\") as file_log_file:\n                        file_log_file.truncate()\n\n        # Configure the logger, any error will raise an exception\n        logger_dictConfig(config)", "response": "Configure the logger with the provided configuration file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the Alignak daemons logger have a console log handler.", "response": "def set_log_console(log_level=logging.INFO):\n    \"\"\"Set the Alignak daemons logger have a console log handler.\n\n    This is only used for the arbiter verify mode to add a console log handler.\n\n    :param log_level: log level\n    :return: n/a\n    \"\"\"\n    # Change the logger and all its handlers log level\n    logger_ = logging.getLogger(ALIGNAK_LOGGER_NAME)\n    logger_.setLevel(log_level)\n\n    # Adding a console logger...\n    csh = ColorStreamHandler(sys.stdout)\n    csh.setFormatter(Formatter('[%(asctime)s] %(levelname)s: [%(name)s] %(message)s',\n                               \"%Y-%m-%d %H:%M:%S\"))\n    logger_.addHandler(csh)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_log_level(log_level=logging.INFO, handlers=None):\n    # print(\"Setting log level: %s\" % (log_level))\n    # Change the logger and all its handlers log level\n    logger_ = logging.getLogger(ALIGNAK_LOGGER_NAME)\n    logger_.setLevel(log_level)\n\n    if handlers is not None:\n        for handler in logger_.handlers:\n            if getattr(handler, '_name', None) in handlers:\n                handler.setLevel(log_level)", "response": "Set the Alignak logger log level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning used to build a monitoring log.", "response": "def make_monitoring_log(level, message, timestamp=None, to_logger=False):\n    \"\"\"\n    Function used to build the monitoring log.\n\n    Emit a log message with the provided level to the monitoring log logger.\n    Build a Brok typed as monitoring_log with the provided message\n\n    When to_logger is True, the information is sent to the python logger, else a monitoring_log\n    Brok is returned. The Brok is managed by the daemons to build an Event that will br logged\n    by the Arbiter when it collects all the events.\n\n    TODO: replace with dedicated brok for each event to log - really useful?\n\n    :param level: log level as defined in logging\n    :type level: str\n    :param message: message to send to the monitoring log logger\n    :type message: str\n    :param to_logger: when set, send to the logger, else raise a brok\n    :type to_logger: bool\n    :param timestamp: if set, force the log event timestamp\n    :return: a monitoring_log Brok\n    :rtype: alignak.brok.Brok\n    \"\"\"\n    level = level.lower()\n    if level not in ['debug', 'info', 'warning', 'error', 'critical']:\n        return False\n\n    if to_logger:\n        logging.getLogger(ALIGNAK_LOGGER_NAME).debug(\"Monitoring log: %s / %s\", level, message)\n\n        # Emit to our monitoring log logger\n        message = message.replace('\\r', '\\\\r')\n        message = message.replace('\\n', '\\\\n')\n        logger_ = logging.getLogger(MONITORING_LOGGER_NAME)\n        logging_function = getattr(logger_, level)\n        try:\n            message = message.decode('utf8', 'ignore')\n        except UnicodeEncodeError:\n            pass\n        except AttributeError:\n            # Python 3 raises an exception!\n            pass\n\n        if timestamp:\n            st = datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n            logging_function(message, extra={'my_date': st})\n        else:\n            logging_function(message)\n\n        return True\n\n    # ... and returns a brok\n    return Brok({'type': 'monitoring_log', 'data': {'level': level, 'message': message}})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the service notification options match the state of the service", "response": "def want_service_notification(self, notifways, timeperiods,\n                                  timestamp, state, n_type, business_impact, cmd=None):\n        \"\"\"Check if notification options match the state of the service\n\n        :param timestamp: time we want to notify the contact (usually now)\n        :type timestamp: int\n        :param state: host or service state (\"WARNING\", \"CRITICAL\" ..)\n        :type state: str\n        :param n_type: type of notification (\"PROBLEM\", \"RECOVERY\" ..)\n        :type n_type: str\n        :param business_impact: impact of this service\n        :type business_impact: int\n        :param cmd: command launched to notify the contact\n        :type cmd: str\n        :return: True if contact wants notification, otherwise False\n        :rtype: bool\n        \"\"\"\n        if not self.service_notifications_enabled:\n            return False\n\n        # If we are in downtime, we do not want notification\n        for downtime_id in self.downtimes:\n            downtime = self.downtimes[downtime_id]\n            if downtime.is_in_effect:\n                self.in_scheduled_downtime = True\n                return False\n        self.in_scheduled_downtime = False\n\n        # Now the rest is for sub notificationways. If one is OK, we are ok\n        # We will filter in another phase\n        for notifway_id in self.notificationways:\n            notifway = notifways[notifway_id]\n            nw_b = notifway.want_service_notification(timeperiods, timestamp,\n                                                      state, n_type, business_impact, cmd)\n            if nw_b:\n                return True\n\n        # Oh... no one is ok for it? so no, sorry\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef want_host_notification(self, notifways, timeperiods, timestamp, state, n_type,\n                               business_impact, cmd=None):\n        \"\"\"Check if notification options match the state of the host\n\n        :param timestamp: time we want to notify the contact (usually now)\n        :type timestamp: int\n        :param state: host or service state (\"UP\", \"DOWN\" ..)\n        :type state: str\n        :param n_type: type of notification (\"PROBLEM\", \"RECOVERY\" ..)\n        :type n_type: str\n        :param business_impact: impact of this host\n        :type business_impact: int\n        :param cmd: command launch to notify the contact\n        :type cmd: str\n        :return: True if contact wants notification, otherwise False\n        :rtype: bool\n        \"\"\"\n        if not self.host_notifications_enabled:\n            return False\n\n        # If we are in downtime, we do not want notification\n        for downtime in self.downtimes:\n            if downtime.is_in_effect:\n                self.in_scheduled_downtime = True\n                return False\n        self.in_scheduled_downtime = False\n\n        # Now it's all for sub notificationways. If one is OK, we are OK\n        # We will filter in another phase\n        for notifway_id in self.notificationways:\n            notifway = notifways[notifway_id]\n            nw_b = notifway.want_host_notification(timeperiods, timestamp,\n                                                   state, n_type, business_impact, cmd)\n            if nw_b:\n                return True\n\n        # Oh, nobody..so NO :)\n        return False", "response": "Check if the contact wants notification"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_notification_commands(self, notifways, n_type, command_name=False):\n        res = []\n\n        for notifway_id in self.notificationways:\n            notifway = notifways[notifway_id]\n            res.extend(notifway.get_notification_commands(n_type))\n\n        # Update inner notification commands property with command name or command\n        if command_name:\n            setattr(self, n_type + '_notification_commands', [c.get_name() for c in res])\n        else:\n            setattr(self, n_type + '_notification_commands', res)\n\n        return res", "response": "Get the list of NotificationCommands for the given object type"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_correct(self):\n        state = True\n        cls = self.__class__\n\n        # Internal checks before executing inherited function...\n\n        # There is a case where there is no nw: when there is not special_prop defined\n        # at all!!\n        if not self.notificationways:\n            for prop in self.special_properties:\n                if not hasattr(self, prop):\n                    msg = \"[contact::%s] %s property is missing\" % (self.get_name(), prop)\n                    self.add_error(msg)\n                    state = False\n\n        if not hasattr(self, 'contact_name'):\n            if hasattr(self, 'alias'):\n                # Use the alias if we miss the contact_name\n                self.contact_name = self.alias\n\n        for char in cls.illegal_object_name_chars:\n            if char not in self.contact_name:\n                continue\n\n            msg = \"[contact::%s] %s character not allowed in contact_name\" \\\n                  % (self.get_name(), char)\n            self.add_error(msg)\n            state = False\n\n        return super(Contact, self).is_correct() and state", "response": "Check if this object configuration is correct."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nraise CONTACT DOWNTIME ALERT entry", "response": "def raise_enter_downtime_log_entry(self):\n        \"\"\"Raise CONTACT DOWNTIME ALERT entry (info level)\n        Format is : \"CONTACT DOWNTIME ALERT: *get_name()*;STARTED;\n                      Contact has entered a period of scheduled downtime\"\n        Example : \"CONTACT DOWNTIME ALERT: test_contact;STARTED;\n                    Contact has entered a period of scheduled downtime\"\n\n        :return: None\n        \"\"\"\n        brok = make_monitoring_log(\n            'info', \"CONTACT DOWNTIME ALERT: %s;STARTED; \"\n                    \"Contact has entered a period of scheduled downtime\" % self.get_name()\n        )\n        self.broks.append(brok)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef raise_exit_downtime_log_entry(self):\n        brok = make_monitoring_log(\n            'info', \"CONTACT DOWNTIME ALERT: %s;STOPPED; \"\n                    \"Contact has exited from a period of scheduled downtime\" % self.get_name()\n        )\n        self.broks.append(brok)", "response": "Raise entry to log and add it to broks list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nraise a CONTACT DOWNTIME ALERT entry.", "response": "def raise_cancel_downtime_log_entry(self):\n        \"\"\"Raise CONTACT DOWNTIME ALERT entry (info level)\n        Format is : \"CONTACT DOWNTIME ALERT: *get_name()*;CANCELLED;\n                      Contact has entered a period of scheduled downtime\"\n        Example : \"CONTACT DOWNTIME ALERT: test_contact;CANCELLED;\n                    Contact has entered a period of scheduled downtime\"\n\n        :return: None\n        \"\"\"\n        brok = make_monitoring_log(\n            'info', \"CONTACT DOWNTIME ALERT: %s;CANCELLED; \"\n                    \"Scheduled downtime for contact has been cancelled.\" % self.get_name()\n        )\n        self.broks.append(brok)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlinking between objects :: * contacts and commands", "response": "def linkify(self, commands, notificationways):\n        \"\"\"Create link between objects::\n\n         * contacts -> notificationways\n\n        :param notificationways: notificationways to link\n        :type notificationways: alignak.objects.notificationway.Notificationways\n        :return: None\n        TODO: Clean this function\n        \"\"\"\n        self.linkify_with_notificationways(notificationways)\n        self.linkify_command_list_with_commands(commands, 'service_notification_commands')\n        self.linkify_command_list_with_commands(commands, 'host_notification_commands')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlink hosts with notificationways", "response": "def linkify_with_notificationways(self, notificationways):\n        \"\"\"Link hosts with realms\n\n        :param notificationways: notificationways object to link with\n        :type notificationways: alignak.objects.notificationway.Notificationways\n        :return: None\n        \"\"\"\n        for i in self:\n            if not hasattr(i, 'notificationways'):\n                continue\n            new_notificationways = []\n            for nw_name in strip_and_uniq(i.notificationways):\n                notifway = notificationways.find_by_name(nw_name)\n                if notifway is not None:\n                    new_notificationways.append(notifway.uuid)\n                else:\n                    err = \"The 'notificationways' of the %s '%s' named '%s' is unknown!\" %\\\n                          (i.__class__.my_type, i.get_name(), nw_name)\n                    i.add_error(err)\n            # Get the list, but first make elements unique\n            i.notificationways = list(set(new_notificationways))\n\n            # Update the contact host/service notification commands properties\n            i.get_notification_commands(notificationways, 'host', command_name=True)\n            i.get_notification_commands(notificationways, 'service', command_name=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexplodes all contact for each contactsgroup and notificationways.", "response": "def explode(self, contactgroups, notificationways):\n        \"\"\"Explode all contact for each contactsgroup\n\n        :param contactgroups: contactgroups to explode\n        :type contactgroups: alignak.objects.contactgroup.Contactgroups\n        :param notificationways: notificationways to explode\n        :type notificationways: alignak.objects.notificationway.Notificationways\n        :return: None\n        \"\"\"\n        # Contactgroups property need to be fulfill for got the information\n        self.apply_partial_inheritance('contactgroups')\n        # _special properties maybe came from a template, so\n        # import them before grok ourselves\n        for prop in Contact.special_properties:\n            if prop == 'contact_name':\n                continue\n            self.apply_partial_inheritance(prop)\n\n        # Register ourselves into the contactsgroups we are in\n        for contact in self:\n            if not (hasattr(contact, 'contact_name') and hasattr(contact, 'contactgroups')):\n                continue\n            for contactgroup in contact.contactgroups:\n                contactgroups.add_member(contact.contact_name, contactgroup.strip())\n\n        # Now create a notification way with the simple parameter of the\n        # contacts\n        for contact in self:\n            need_notificationway = False\n            params = {}\n            for param in Contact.simple_way_parameters:\n                if hasattr(contact, param):\n                    need_notificationway = True\n                    params[param] = getattr(contact, param)\n                elif contact.properties[param].has_default:  # put a default text value\n                    # Remove the value and put a default value\n                    setattr(contact, param, contact.properties[param].default)\n\n            if need_notificationway:\n                cname = getattr(contact, 'contact_name', getattr(contact, 'alias', ''))\n                nw_name = cname + '_inner_nw'\n                notificationways.new_inner_member(nw_name, params)\n\n                if not hasattr(contact, 'notificationways'):\n                    contact.notificationways = [nw_name]\n                else:\n                    contact.notificationways = list(contact.notificationways)\n                    contact.notificationways.append(nw_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zlib_processor(entity):  # pragma: no cover, not used in the testing environment...\n    if not entity.headers.get(ntou(\"Content-Length\"), ntou(\"\")):\n        raise cherrypy.HTTPError(411)\n\n    body = entity.fp.read()\n    try:\n        body = zlib.decompress(body)\n    except zlib.error:\n        raise cherrypy.HTTPError(400, 'Invalid zlib data')\n    try:\n        raw_params = json.loads(body)\n    except ValueError:\n        raise cherrypy.HTTPError(400, 'Invalid JSON document in zlib data')\n\n    try:\n        params = {}\n        for key, value in list(raw_params.items()):\n            params[key] = unserialize(value.encode(\"utf8\"))\n    except TypeError:\n        raise cherrypy.HTTPError(400, 'Invalid serialized data in JSON document')\n    except AlignakClassLookupException as exp:\n        cherrypy.HTTPError(400, 'Cannot un-serialize data received: %s' % exp)\n\n    # Now that all values have been successfully parsed and decoded,\n    # apply them to the entity.params dict.\n    for key, value in list(params.items()):\n        if key in entity.params:\n            if not isinstance(entity.params[key], list):\n                entity.params[key] = [entity.params[key]]\n            entity.params[key].append(value)\n        else:\n            entity.params[key] = value", "response": "Read application / zlib data and put content into entity. params dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_instance(mod_conf):\n    logger.info(\"Giving an instance of %s for alias: %s\",\n                mod_conf.python_name, mod_conf.module_alias)\n\n    return InnerRetention(mod_conf)", "response": "Returns an instance of the module manager for the given module properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hook_load_retention(self, scheduler):  # pylint: disable=too-many-locals, too-many-branches\n        if not self.enabled:\n            logger.warning(\"Alignak retention module is not enabled.\"\n                           \"Loading objects state is not possible.\")\n            return None\n\n        if self.retention_file and not os.path.isfile(self.retention_file):\n            logger.info(\"The configured state retention file (%s) does not exist. \"\n                        \"Loading objects state is not available.\", self.retention_file)\n            return None\n\n        if self.retention_dir and not os.path.isdir(self.retention_dir):\n            logger.info(\"The configured state retention directory (%s) does not exist. \"\n                        \"Loading objects state is not available.\", self.retention_dir)\n            return None\n\n        all_data = {'hosts': {}, 'services': {}}\n\n        retention_files = []\n        if self.retention_file:\n            retention_files = [self.retention_file]\n        else:\n            if self.retention_dir:\n                for root, _, walk_files in os.walk(self.retention_dir, followlinks=True):\n                    for found_file in walk_files:\n                        if not re.search(r\"\\.json$\", found_file):\n                            continue\n                        retention_files.append(os.path.join(root, found_file))\n        logger.debug(\"Loading retention files: %s \", retention_files)\n        if retention_files:\n            logger.info(\"Loading retention data from %d files\", len(retention_files))\n\n        start_time = time.time()\n\n        for retention_file in retention_files:\n            # Get data from the retention files\n            try:\n                logger.debug('Loading data from: %s', retention_file)\n                with open(retention_file, \"r\") as fd:\n                    response = json.load(fd)\n\n                if not isinstance(response, list):\n                    response = [response]\n\n                # Is looks like a list of host dictionaries ?\n                if isinstance(response[0], dict) and 'name' in response[0]:\n                    logger.debug('Loaded: %s', response)\n                else:\n                    logger.info(\"Supposed retention file %s is not correctly encoded! Got: %s\",\n                                retention_file, response[0])\n                    continue\n            except Exception as exp:  # pylint: disable=broad-except\n                # pragma: no cover, should never happen...\n                logger.warning(\"Error when loading retention data from %s\", retention_file)\n                logger.exception(exp)\n            else:\n                for host in response:\n                    hostname = host['name']\n                    service_key = 'services'\n                    if 'retention_services' in host:\n                        service_key = 'retention_services'\n                    if service_key in host:\n                        for service in host[service_key]:\n                            all_data['services'][(host['name'], service)] = \\\n                                host[service_key][service]\n                    all_data['hosts'][hostname] = host\n                    logger.debug('- loaded: %s', host)\n\n        try:\n            logger.info('%d hosts loaded from retention', len(all_data['hosts']))\n            self.statsmgr.counter('retention-load.hosts', len(all_data['hosts']))\n            logger.info('%d services loaded from retention', len(all_data['services']))\n            self.statsmgr.counter('retention-load.services', len(all_data['services']))\n            self.statsmgr.timer('retention-load.time', time.time() - start_time)\n\n            # Restore the scheduler objects\n            scheduler.restore_retention_data(all_data)\n            logger.info(\"Retention data loaded in %s seconds\", (time.time() - start_time))\n        except Exception as exp:  # pylint: disable=broad-except\n            logger.warning(\"Retention load failed: %s\", exp)\n            logger.exception(exp)\n            return False\n\n        return True", "response": "Load the retention data from a file or directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hook_save_retention(self, scheduler):\n        if not self.enabled:\n            logger.warning(\"Alignak retention module is not enabled.\"\n                           \"Saving objects state is not possible.\")\n            return None\n\n        try:\n            start_time = time.time()\n\n            # Get retention data from the scheduler\n            data_to_save = scheduler.get_retention_data()\n            if not data_to_save:\n                logger.warning(\"Alignak retention data to save are not containing any information.\")\n                return None\n\n            # Move services data to their respective hosts dictionary\n            # Alignak scheduler do not merge the services into the host dictionary!\n            for host_name in data_to_save['hosts']:\n                data_to_save['hosts'][host_name]['services'] = {}\n                data_to_save['hosts'][host_name]['name'] = host_name\n            for host_name, service_description in data_to_save['services']:\n                data_to_save['hosts'][host_name]['services'][service_description] = \\\n                    data_to_save['services'][(host_name, service_description)]\n\n            try:\n                if not self.retention_file:\n                    logger.info('Saving retention data to: %s', self.retention_dir)\n                    for host_name in data_to_save['hosts']:\n                        file_name = os.path.join(self.retention_dir,\n                                                 self.retention_file,\n                                                 \"%s.json\" % host_name)\n                        with open(file_name, \"w\") as fd:\n                            fd.write(json.dumps(data_to_save['hosts'][host_name],\n                                                indent=2, separators=(',', ': '),\n                                                sort_keys=True))\n                        logger.debug('- saved: %s', file_name)\n                    logger.info('Saved')\n                else:\n                    logger.info('Saving retention data to: %s', self.retention_file)\n                    with open(self.retention_file, \"w\") as fd:\n                        fd.write(json.dumps(data_to_save['hosts'],\n                                            indent=2, separators=(',', ': '), sort_keys=True))\n                    logger.info('Saved')\n            except Exception as exp:  # pylint: disable=broad-except\n                # pragma: no cover, should never happen...\n                logger.warning(\"Error when saving retention data to %s\", self.retention_file)\n                logger.exception(exp)\n\n            logger.info('%d hosts saved in retention', len(data_to_save['hosts']))\n            self.statsmgr.counter('retention-save.hosts', len(data_to_save['hosts']))\n            logger.info('%d services saved in retention', len(data_to_save['services']))\n            self.statsmgr.counter('retention-save.services', len(data_to_save['services']))\n            self.statsmgr.timer('retention-save.time', time.time() - start_time)\n\n            logger.info(\"Retention data saved in %s seconds\", (time.time() - start_time))\n        except Exception as exp:  # pylint: disable=broad-except\n            self.enabled = False\n            logger.warning(\"Retention save failed: %s\", exp)\n            logger.exception(exp)\n            return False\n\n        return True", "response": "Save the retention data to a Json formated file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_check_command(self, timeperiods, t_to_go):\n        if not self.check_period or timeperiods[self.check_period].is_time_valid(t_to_go):\n            return self.check_command\n        return None", "response": "Get the check_command if we are in the check period"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the current object configuration is correct.", "response": "def is_correct(self):\n        \"\"\"Check if this object configuration is correct ::\n\n        * Check our own specific properties\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        state = True\n\n        # Internal checks before executing inherited function...\n        if not hasattr(self, 'check_command'):\n            msg = \"[checkmodulation::%s] do not have any check_command defined\" % (\n                self.get_name()\n            )\n            self.add_error(msg)\n            state = False\n        else:\n            if self.check_command is None:\n                msg = \"[checkmodulation::%s] a check_command is missing\" % (self.get_name())\n                self.add_error(msg)\n                state = False\n            if self.check_command and not self.check_command.is_valid():\n                msg = \"[checkmodulation::%s] a check_command is invalid\" % (self.get_name())\n                self.add_error(msg)\n                state = False\n\n        # Ok just put None as check_period, means 24x7\n        if not hasattr(self, 'check_period'):\n            self.check_period = None\n\n        return super(CheckModulation, self).is_correct() and state"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef linkify(self, timeperiods, commands):\n        self.linkify_with_timeperiods(timeperiods, 'check_period')\n        self.linkify_one_command_with_commands(commands, 'check_command')", "response": "Link the given timeperiods and commands into each CheckModulation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_inner_member(self, name=None, params=None):\n        if name is None:\n            name = 'Generated_checkmodulation_%s' % uuid.uuid4()\n\n        if params is None:\n            params = {}\n\n        params['checkmodulation_name'] = name\n        checkmodulation = CheckModulation(params)\n        self.add_item(checkmodulation)", "response": "Create a CheckModulation object and add it to items\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef open(self):\n\n        if not self._is_connected:\n            \n            print(\"Connecting to arduino on {}... \".format(self.device),end=\"\")\n\n            self.comm = serial.Serial()\n            self.comm.port = self.device\n            self.comm.baudrate = self.baud_rate\n            self.comm.timeout = self.timeout\n            self.dtr = self.enable_dtr\n            self.comm.open()\n\n            time.sleep(self.settle_time)\n            self._is_connected = True\n\n            print(\"done.\")", "response": "Open the serial connection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self,cmd,*args,arg_formats=None):\n\n        # Turn the command into an integer.\n        try:\n            command_as_int = self._cmd_name_to_int[cmd]\n        except KeyError:\n            err = \"Command '{}' not recognized.\\n\".format(cmd)\n            raise ValueError(err)\n\n        # Figure out what formats to use for each argument.  \n        arg_format_list = []\n        if arg_formats != None:\n\n            # The user specified formats\n            arg_format_list = list(arg_formats)\n\n        else:\n            try:\n                # See if class was initialized with a format for arguments to this\n                # command\n                arg_format_list = self._cmd_name_to_format[cmd]\n            except KeyError:\n                # if not, guess for all arguments\n                arg_format_list = [\"g\" for i in range(len(args))]\n  \n        # Deal with \"*\" format  \n        arg_format_list = self._treat_star_format(arg_format_list,args)\n\n        if len(args) > 0:\n            if len(arg_format_list) != len(args):\n                err = \"Number of argument formats must match the number of arguments.\"\n                raise ValueError(err)\n\n        # Go through each argument and create a bytes representation in the\n        # proper format to send.  Escape appropriate characters. \n        fields = [\"{}\".format(command_as_int).encode(\"ascii\")]\n        for i, a in enumerate(args):\n            fields.append(self._send_methods[arg_format_list[i]](a))\n            fields[-1] = self._escape_re.sub(self._byte_escape_sep + r\"\\1\".encode(\"ascii\"),fields[-1])\n\n        # Make something that looks like cmd,field1,field2,field3;\n        compiled_bytes = self._byte_field_sep.join(fields) + self._byte_command_sep\n\n        # Send the message.\n        self.board.write(compiled_bytes)", "response": "Sends a command to an arbitrary set of parameters to an arbitrary set of arduino classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef receive(self,arg_formats=None):\n\n        # Read serial input until a command separator or empty character is\n        # reached \n        msg = [[]]\n        raw_msg = []\n        escaped = False\n        command_sep_found = False\n        while True:\n\n            tmp = self.board.read()\n            raw_msg.append(tmp)\n\n            if escaped:\n\n                # Either drop the escape character or, if this wasn't really\n                # an escape, keep previous escape character and new character\n                if tmp in self._escaped_characters:\n                    msg[-1].append(tmp)\n                    escaped = False\n                else:\n                    msg[-1].append(self._byte_escape_sep)\n                    msg[-1].append(tmp)\n                    escaped = False\n            else:\n\n                # look for escape character\n                if tmp == self._byte_escape_sep:\n                    escaped = True\n\n                # or field separator\n                elif tmp == self._byte_field_sep:\n                    msg.append([])\n\n                # or command separator\n                elif tmp == self._byte_command_sep:\n                    command_sep_found = True\n                    break\n\n                # or any empty characater \n                elif tmp == b'':\n                    break\n\n                # okay, must be something\n                else:\n                    msg[-1].append(tmp)\n  \n        # No message received given timeouts\n        if len(msg) == 1 and len(msg[0]) == 0:\n            return None\n\n        # Make sure the message terminated properly\n        if not command_sep_found:\n          \n            # empty message (likely from line endings being included) \n            joined_raw = b''.join(raw_msg) \n            if joined_raw.strip() == b'':\n                return  None\n           \n            err = \"Incomplete message ({})\".format(joined_raw.decode())\n            raise EOFError(err)\n\n        # Turn message into fields\n        fields = [b''.join(m) for m in msg]\n\n        # Get the command name.\n        cmd = fields[0].strip().decode()\n        try:\n            cmd_name = self._int_to_cmd_name[int(cmd)]\n        except (ValueError,IndexError):\n\n            if self.give_warnings:\n                cmd_name = \"unknown\"\n                w = \"Recieved unrecognized command ({}).\".format(cmd)\n                warnings.warn(w,Warning)\n        \n        # Figure out what formats to use for each argument.  \n        arg_format_list = []\n        if arg_formats != None:\n\n            # The user specified formats\n            arg_format_list = list(arg_formats)\n\n        else:\n            try:\n                # See if class was initialized with a format for arguments to this\n                # command\n                arg_format_list = self._cmd_name_to_format[cmd_name]\n            except KeyError:\n                # if not, guess for all arguments\n                arg_format_list = [\"g\" for i in range(len(fields[1:]))]\n\n        # Deal with \"*\" format  \n        arg_format_list = self._treat_star_format(arg_format_list,fields[1:])\n\n        if len(fields[1:]) > 0:\n            if len(arg_format_list) != len(fields[1:]):\n                err = \"Number of argument formats must match the number of recieved arguments.\"\n                raise ValueError(err)\n\n        received = []\n        for i, f in enumerate(fields[1:]):\n            received.append(self._recv_methods[arg_format_list[i]](f))\n        \n        # Record the time the message arrived\n        message_time = time.time()\n\n        return cmd_name, received, message_time", "response": "Read a message from the serial port and return a list of the fields that were parsed into the appropriate types."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndealing with star format if specified.", "response": "def _treat_star_format(self,arg_format_list,args):\n        \"\"\"\n        Deal with \"*\" format if specified.\n        \"\"\"\n\n        num_stars = len([a for a in arg_format_list if a == \"*\"])\n        if num_stars > 0:\n\n            # Make sure the repeated format argument only occurs once, is last,\n            # and that there is at least one format in addition to it.\n            if num_stars == 1 and arg_format_list[-1] == \"*\" and len(arg_format_list) > 1:\n\n                # Trim * from end\n                arg_format_list = arg_format_list[:-1]\n\n                # If we need extra arguments...\n                if len(arg_format_list) < len(args):\n                    f = arg_format_list[-1]\n                    len_diff = len(args) - len(arg_format_list)\n                    tmp = list(arg_format_list)\n                    tmp.extend([f for i in range(len_diff)])\n                    arg_format_list = \"\".join(tmp)\n            else:\n                err = \"'*' format must occur only once, be at end of string, and be preceded by at least one other format.\"\n                raise ValueError(err)\n\n        return arg_format_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a single char to a bytes object.", "response": "def _send_char(self,value):\n        \"\"\"\n        Convert a single char to a bytes object.\n        \"\"\"\n\n        if type(value) != str and type(value) != bytes:\n            err = \"char requires a string or bytes array of length 1\"\n            raise ValueError(err)\n\n        if len(value) != 1:\n            err = \"char must be a single character, not \\\"{}\\\"\".format(value)\n            raise ValueError(err)\n\n        if type(value) != bytes:\n            value = value.encode(\"ascii\")\n\n        if value in self._escaped_characters:\n            err = \"Cannot send a control character as a single char to arduino.  Send as string instead.\"\n            raise OverflowError(err)\n\n        return struct.pack('c',value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a numerical value into an integer then to a byte object. Check bounds for byte.", "response": "def _send_byte(self,value):\n        \"\"\"\n        Convert a numerical value into an integer, then to a byte object. Check\n        bounds for byte.\n        \"\"\"\n\n        # Coerce to int. This will throw a ValueError if the value can't\n        # actually be converted.\n        if type(value) != int:\n            new_value = int(value)\n\n            if self.give_warnings:\n                w = \"Coercing {} into int ({})\".format(value,new_value)\n                warnings.warn(w,Warning)\n                value = new_value\n\n        # Range check\n        if value > 255 or value < 0:\n            err = \"Value {} exceeds the size of the board's byte.\".format(value)\n            raise OverflowError(err)\n\n        return struct.pack(\"B\",value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _send_int(self,value):\n\n        # Coerce to int. This will throw a ValueError if the value can't \n        # actually be converted.\n        if type(value) != int:\n            new_value = int(value)\n\n            if self.give_warnings:\n                w = \"Coercing {} into int ({})\".format(value,new_value)\n                warnings.warn(w,Warning)\n                value = new_value\n\n        # Range check\n        if value > self.board.int_max or value < self.board.int_min:\n            err = \"Value {} exceeds the size of the board's int.\".format(value)\n            raise OverflowError(err)\n           \n        return struct.pack(self.board.int_type,value)", "response": "Convert a numerical value into an integer then to a bytes object Check bounds for signed int."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a numerical value into an integer then to a bytes object. Check bounds for unsigned int.", "response": "def _send_unsigned_int(self,value):\n        \"\"\"\n        Convert a numerical value into an integer, then to a bytes object. Check\n        bounds for unsigned int.\n        \"\"\"\n        # Coerce to int. This will throw a ValueError if the value can't \n        # actually be converted.\n        if type(value) != int:\n            new_value = int(value)\n\n            if self.give_warnings:\n                w = \"Coercing {} into int ({})\".format(value,new_value)\n                warnings.warn(w,Warning)\n                value = new_value\n\n        # Range check\n        if value > self.board.unsigned_int_max or value < self.board.unsigned_int_min:\n            err = \"Value {} exceeds the size of the board's unsigned int.\".format(value)\n            raise OverflowError(err)\n           \n        return struct.pack(self.board.unsigned_int_type,value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a numerical value into an integer then to a bytes object. Check bounds for signed long.", "response": "def _send_long(self,value):\n        \"\"\"\n        Convert a numerical value into an integer, then to a bytes object. Check\n        bounds for signed long.\n        \"\"\"\n\n        # Coerce to int. This will throw a ValueError if the value can't \n        # actually be converted.\n        if type(value) != int:\n            new_value = int(value)\n            \n            if self.give_warnings:\n                w = \"Coercing {} into int ({})\".format(value,new_value)\n                warnings.warn(w,Warning)\n                value = new_value\n\n        # Range check\n        if value > self.board.long_max or value < self.board.long_min:\n            err = \"Value {} exceeds the size of the board's long.\".format(value)\n            raise OverflowError(err)\n           \n        return struct.pack(self.board.long_type,value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _send_unsigned_long(self,value):\n\n        # Coerce to int. This will throw a ValueError if the value can't \n        # actually be converted.\n        if type(value) != int:\n            new_value = int(value)\n\n            if self.give_warnings:\n                w = \"Coercing {} into int ({})\".format(value,new_value)\n                warnings.warn(w,Warning)\n                value = new_value\n\n        # Range check\n        if value > self.board.unsigned_long_max or value < self.board.unsigned_long_min:\n            err = \"Value {} exceeds the size of the board's unsigned long.\".format(value)\n            raise OverflowError(err)\n          \n        return struct.pack(self.board.unsigned_long_type,value)", "response": "Convert a numerical value into an integer then to a bytes object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the value to a float and return a bytes object.", "response": "def _send_float(self,value):\n        \"\"\"\n        Return a float as a IEEE 754 format bytes object.\n        \"\"\"\n\n        # convert to float. this will throw a ValueError if the type is not \n        # readily converted\n        if type(value) != float:\n            value = float(value)\n\n        # Range check\n        if value > self.board.float_max or value < self.board.float_min:\n            err = \"Value {} exceeds the size of the board's float.\".format(value)\n            raise OverflowError(err)\n\n        return struct.pack(self.board.float_type,value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _send_double(self,value):\n\n        # convert to float. this will throw a ValueError if the type is not \n        # readily converted\n        if type(value) != float:\n            value = float(value)\n\n        # Range check\n        if value > self.board.float_max or value < self.board.float_min:\n            err = \"Value {} exceeds the size of the board's float.\".format(value)\n            raise OverflowError(err)\n\n        return struct.pack(self.board.double_type,value)", "response": "Convert the value to a float and return a bytes object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _send_string(self,value):\n\n        if type(value) != bytes:\n            value = \"{}\".format(value).encode(\"ascii\")\n\n        return value", "response": "Convert a string to a bytes object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _send_bool(self,value):\n\n        # Sanity check.\n        if type(value) != bool and value not in [0,1]:\n            err = \"{} is not boolean.\".format(value)\n            raise ValueError(err)\n\n        return struct.pack(\"?\",value)", "response": "Convert a boolean value into a bytes object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _send_guess(self,value):\n\n        if type(value) != str and type(value) != bytes and self.give_warnings:\n            w = \"Warning: Sending {} as a string. This can give wildly incorrect values. Consider specifying a format and sending binary data.\".format(value)\n            warnings.warn(w,Warning)\n\n        if type(value) == float:\n            return \"{:.10e}\".format(value).encode(\"ascii\")\n        elif type(value) == bool:\n            return \"{}\".format(int(value)).encode(\"ascii\")\n        else:\n            return self._send_string(value)", "response": "Send the argument as a string in a way that should be processed properly by CmdMessenger. readBinArg<CAST > and use the CmdMessenger. _send_string method that should be called when the command line is called."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makePickle(self, record):\n        gelf_dict = self._make_gelf_dict(record)\n        packed = self._pack_gelf_dict(gelf_dict)\n        pickle = zlib.compress(packed) if self.compress else packed\n        return pickle", "response": "Convert a logging. LogRecord into a bytes object representing a GELF log."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a dictionary representing a Graylog GELF log from a logging. LogRecord.", "response": "def _make_gelf_dict(self, record):\n        \"\"\"Create a dictionary representing a Graylog GELF log from a\n        python :class:`logging.LogRecord`\n\n        :param record: :class:`logging.LogRecord` to create a Graylog GELF\n            log from.\n        :type record: logging.LogRecord\n\n        :return: dictionary representing a Graylog GELF log.\n        :rtype: dict\n        \"\"\"\n        # construct the base GELF format\n        gelf_dict = {\n            'version': \"1.0\",\n            'host': BaseGELFHandler._resolve_host(self.fqdn, self.localname),\n            'short_message': self.formatter.format(record) if self.formatter else record.getMessage(),\n            'timestamp': record.created,\n            'level': SYSLOG_LEVELS.get(record.levelno, record.levelno),\n            'facility': self.facility or record.name,\n        }\n\n        # add in specified optional extras\n        self._add_full_message(gelf_dict, record)\n        if self.level_names:\n            self._add_level_names(gelf_dict, record)\n        if self.facility is not None:\n            self._set_custom_facility(gelf_dict, self.facility, record)\n        if self.debugging_fields:\n            self._add_debugging_fields(gelf_dict, record)\n        if self.extra_fields:\n            self._add_extra_fields(gelf_dict, record)\n        return gelf_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds the full_message field to the given gelf_dict if any traceback information exists within the GELF log.", "response": "def _add_full_message(gelf_dict, record):\n        \"\"\"Add the ``full_message`` field to the ``gelf_dict`` if any\n        traceback information exists within the logging record\n\n        :param gelf_dict: dictionary representation of a GELF log.\n        :type gelf_dict: dict\n\n        :param record: :class:`logging.LogRecord` to extract a full\n            logging message from to insert into the given ``gelf_dict``.\n        :type record: logging.LogRecord\n        \"\"\"\n        # if a traceback exists add it to the log as the full_message field\n        full_message = None\n        # format exception information if present\n        if record.exc_info:\n            full_message = '\\n'.join(\n                traceback.format_exception(*record.exc_info))\n        # use pre-formatted exception information in cases where the primary\n        # exception information was removed, eg. for LogRecord serialization\n        if record.exc_text:\n            full_message = record.exc_text\n        if full_message:\n            gelf_dict[\"full_message\"] = full_message"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresolve the host GELF field.", "response": "def _resolve_host(fqdn, localname):\n        \"\"\"Resolve the ``host`` GELF field\n\n        :param fqdn: Boolean indicating whether to use :meth:`socket.getfqdn`\n            to obtain the ``host`` GELF field.\n        :type fqdn: bool\n\n        :param localname: Use specified hostname as the ``host`` GELF field.\n        :type localname: str or None\n\n        :return: String value representing the ``host`` GELF field.\n        :rtype: str\n        \"\"\"\n        if fqdn:\n            return socket.getfqdn()\n        elif localname is not None:\n            return localname\n        return socket.gethostname()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd debugging fields to the given dictionary representation of a GELF log record.", "response": "def _add_debugging_fields(gelf_dict, record):\n        \"\"\"Add debugging fields to the given ``gelf_dict``\n\n        :param gelf_dict: dictionary representation of a GELF log.\n        :type gelf_dict: dict\n\n        :param record: :class:`logging.LogRecord` to extract debugging\n            fields from to insert into the given ``gelf_dict``.\n        :type record: logging.LogRecord\n        \"\"\"\n        gelf_dict.update({\n            'file': record.pathname,\n            'line': record.lineno,\n            '_function': record.funcName,\n            '_pid': record.process,\n            '_thread_name': record.threadName,\n        })\n        # record.processName was added in Python 2.6.2\n        pn = getattr(record, 'processName', None)\n        if pn is not None:\n            gelf_dict['_process_name'] = pn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding extra fields to the given gelf_dict.", "response": "def _add_extra_fields(gelf_dict, record):\n        \"\"\"Add extra fields to the given ``gelf_dict``\n\n        However, this does not add additional fields in to ``message_dict``\n        that are either duplicated from standard :class:`logging.LogRecord`\n        attributes, duplicated from the python logging module source\n        (e.g. ``exc_text``), or violate GLEF format (i.e. ``id``).\n\n        .. seealso::\n\n            The list of standard :class:`logging.LogRecord` attributes can be\n            found at:\n\n                http://docs.python.org/library/logging.html#logrecord-attributes\n\n        :param gelf_dict: dictionary representation of a GELF log.\n        :type gelf_dict: dict\n\n        :param record: :class:`logging.LogRecord` to extract extra fields\n            from to insert into the given ``gelf_dict``.\n        :type record: logging.LogRecord\n        \"\"\"\n\n        # skip_list is used to filter additional fields in a log message.\n        skip_list = (\n            'args', 'asctime', 'created', 'exc_info', 'exc_text', 'filename',\n            'funcName', 'id', 'levelname', 'levelno', 'lineno', 'module',\n            'msecs', 'message', 'msg', 'name', 'pathname', 'process',\n            'processName', 'relativeCreated', 'thread', 'threadName')\n\n        for key, value in record.__dict__.items():\n            if key not in skip_list and not key.startswith('_'):\n                gelf_dict['_%s' % key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _pack_gelf_dict(gelf_dict):\n        gelf_dict = BaseGELFHandler._sanitize_to_unicode(gelf_dict)\n        packed = json.dumps(\n            gelf_dict,\n            separators=',:',\n            default=BaseGELFHandler._object_to_json\n        )\n        return packed.encode('utf-8')", "response": "Convert a given dictionary into a JSON - encoded string thus making a uncompressed GELF log ready for consumption by Graylog."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sanitize_to_unicode(obj):\n        if isinstance(obj, dict):\n            return dict((BaseGELFHandler._sanitize_to_unicode(k), BaseGELFHandler._sanitize_to_unicode(v)) for k, v in obj.items())\n        if isinstance(obj, (list, tuple)):\n            return obj.__class__([BaseGELFHandler._sanitize_to_unicode(i) for i in obj])\n        if isinstance(obj, data):\n            obj = obj.decode('utf-8', errors='replace')\n        return obj", "response": "Convert all strings records of the object to unicode."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an object that cannot be natively serialized into JSON using the string representation of the object.", "response": "def _object_to_json(obj):\n        \"\"\"Convert objects that cannot be natively serialized into JSON\n        into their string representation\n\n        For datetime based objects convert them into their ISO formatted\n        string as specified by :meth:`datetime.datetime.isoformat`.\n\n        :param obj: object to convert into a JSON via getting its string\n            representation.\n        :type obj: object\n\n        :return: String value representing the given object ready to be\n            encoded into a JSON.\n        :rtype: str\n        \"\"\"\n        if isinstance(obj, datetime.datetime):\n            return obj.isoformat()\n        return repr(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides SocketHandler. makeSocket to allow creating wrappedARKsockets", "response": "def makeSocket(self, timeout=1):\n        \"\"\"Override SocketHandler.makeSocket, to allow creating wrapped\n        TLS sockets\"\"\"\n        plain_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n        if hasattr(plain_socket, 'settimeout'):\n            plain_socket.settimeout(timeout)\n\n        wrapped_socket = ssl.wrap_socket(\n            plain_socket,\n            ca_certs=self.ca_certs,\n            cert_reqs=self.reqs,\n            keyfile=self.keyfile,\n            certfile=self.certfile\n        )\n        wrapped_socket.connect((self.host, self.port))\n\n        return wrapped_socket"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a logging. LogRecord into GELF and emit it to Graylog.", "response": "def emit(self, record):\n        \"\"\"Convert a :class:`logging.LogRecord` to GELF and emit it to Graylog\n        via an HTTP POST request\n\n        :param record: :class:`logging.LogRecord` to convert into a\n            Graylog GELF log and emit to Graylog via HTTP POST.\n        :type record: logging.LogRecord\n        \"\"\"\n        pickle = self.makePickle(record)\n        connection = httplib.HTTPConnection(\n            host=self.host,\n            port=self.port,\n            timeout=self.timeout\n        )\n        connection.request('POST', self.path, pickle, self.headers)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure a passed string is unicode", "response": "def to_unicode(string):\n    \"\"\"\n    Ensure a passed string is unicode\n    \"\"\"\n    if isinstance(string, six.binary_type):\n        return string.decode('utf8')\n    if isinstance(string, six.text_type):\n        return string\n    if six.PY2:\n        return unicode(string)\n    return str(string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencodes a string as a UTF8 bytestring.", "response": "def to_utf8(string):\n    \"\"\"\n    Encode a string as a UTF8 bytestring.  This function could be passed a\n    bytestring or unicode string so must distinguish between the two.\n    \"\"\"\n    if isinstance(string, six.text_type):\n        return string.encode('utf8')\n    if isinstance(string, six.binary_type):\n        return string\n    return str(string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a dictionary of items into unicode.", "response": "def dict_to_unicode(raw_dict):\n    \"\"\"\n    Ensure all keys and values in a dict are unicode.\n\n    The passed dict is assumed to have lists for all values.\n    \"\"\"\n    decoded = {}\n    for key, value in raw_dict.items():\n        decoded[to_unicode(key)] = map(\n            to_unicode, value)\n    return decoded"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unicode_urlencode(query, doseq=True):\n    pairs = []\n    for key, value in query.items():\n        if isinstance(value, list):\n            value = list(map(to_utf8, value))\n        else:\n            value = to_utf8(value)\n        pairs.append((to_utf8(key), value))\n    encoded_query = dict(pairs)\n    xx = urlencode(encoded_query, doseq)\n    return xx", "response": "URL encode a dictionary of items in a sequence of unicode strings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting all parts from a URL string and return them as a dictionary", "response": "def parse(url_str):\n    \"\"\"\n    Extract all parts from a URL string and return them as a dictionary\n    \"\"\"\n    url_str = to_unicode(url_str)\n    result = urlparse(url_str)\n    netloc_parts = result.netloc.rsplit('@', 1)\n    if len(netloc_parts) == 1:\n        username = password = None\n        host = netloc_parts[0]\n    else:\n        user_and_pass = netloc_parts[0].split(':')\n        if len(user_and_pass) == 2:\n            username, password = user_and_pass\n        elif len(user_and_pass) == 1:\n            username = user_and_pass[0]\n            password = None\n        host = netloc_parts[1]\n\n    if host and ':' in host:\n        host = host.split(':')[0]\n\n    return {'host': host,\n            'username': username,\n            'password': password,\n            'scheme': result.scheme,\n            'port': result.port,\n            'path': result.path,\n            'query': result.query,\n            'fragment': result.fragment}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef netloc(self):\n        url = self._tuple\n        if url.username and url.password:\n            netloc = '%s:%s@%s' % (url.username, url.password, url.host)\n        elif url.username and not url.password:\n            netloc = '%s@%s' % (url.username, url.host)\n        else:\n            netloc = url.host\n        if url.port:\n            netloc = '%s:%s' % (netloc, url.port)\n        return netloc", "response": "Return the netloc of the current object"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the new host string", "response": "def host(self, value=None):\n        \"\"\"\n        Return the host\n\n        :param string value: new host string\n        \"\"\"\n        if value is not None:\n            return URL._mutate(self, host=value)\n        return self._tuple.host"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns or set the username of the URL instance.", "response": "def username(self, value=None):\n        \"\"\"\n        Return or set the username\n\n        :param string value: the new username to use\n        :returns: string or new :class:`URL` instance\n        \"\"\"\n        if value is not None:\n            return URL._mutate(self, username=value)\n        return unicode_unquote(self._tuple.username)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns or set the password of the URL instance.", "response": "def password(self, value=None):\n        \"\"\"\n        Return or set the password\n\n        :param string value: the new password to use\n        :returns: string or new :class:`URL` instance\n        \"\"\"\n        if value is not None:\n            return URL._mutate(self, password=value)\n        return unicode_unquote(self._tuple.password)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of subdomains or sets the subdomains and returns a new URL instance.", "response": "def subdomains(self, value=None):\n        \"\"\"\n        Returns a list of subdomains or set the subdomains and returns a\n        new :class:`URL` instance.\n\n        :param list value: a list of subdomains\n        \"\"\"\n        if value is not None:\n            return URL._mutate(self, host='.'.join(value))\n        return self.host().split('.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a subdomain or set a new value and return a new URL instance.", "response": "def subdomain(self, index, value=None):\n        \"\"\"\n        Return a subdomain or set a new value and return a new :class:`URL`\n        instance.\n\n        :param integer index: 0-indexed subdomain\n        :param string value: New subdomain\n        \"\"\"\n        if value is not None:\n            subdomains = self.subdomains()\n            subdomains[index] = value\n            return URL._mutate(self, host='.'.join(subdomains))\n        return self.subdomains()[index]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scheme(self, value=None):\n        if value is not None:\n            return URL._mutate(self, scheme=value)\n        return self._tuple.scheme", "response": "Return or set the scheme."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef path(self, value=None):\n        if value is not None:\n            if not value.startswith('/'):\n                value = '/' + value\n            encoded_value = unicode_quote(value)\n            return URL._mutate(self, path=encoded_value)\n        return self._tuple.path", "response": "Return or set the path to use\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning or set the query string", "response": "def query(self, value=None):\n        \"\"\"\n        Return or set the query string\n\n        :param string value: the new query string to use\n        :returns: string or new :class:`URL` instance\n        \"\"\"\n        if value is not None:\n            return URL._mutate(self, query=value)\n        return self._tuple.query"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef port(self, value=None):\n        if value is not None:\n            return URL._mutate(self, port=value)\n        return self._tuple.port", "response": "Return or set the port of the URL instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning or set the fragment of the URL instance.", "response": "def fragment(self, value=None):\n        \"\"\"\n        Return or set the fragment (hash)\n\n        :param string value: the new fragment to use\n        :returns: string or new :class:`URL` instance\n        \"\"\"\n        if value is not None:\n            return URL._mutate(self, fragment=value)\n        return unicode_unquote(self._tuple.fragment)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the path segment at the given index.", "response": "def path_segment(self, index, value=None, default=None):\n        \"\"\"\n        Return the path segment at the given index\n\n        :param integer index:\n        :param string value: the new segment value\n        :param string default: the default value to return if no path segment exists with the given index\n        \"\"\"\n        if value is not None:\n            segments = list(self.path_segments())\n            segments[index] = unicode_quote_path_segment(value)\n            new_path = '/' + '/'.join(segments)\n            if self._tuple.path.endswith('/'):\n                new_path += '/'\n            return URL._mutate(self, path=new_path)\n        try:\n            return self.path_segments()[index]\n        except IndexError:\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef path_segments(self, value=None):\n        if value is not None:\n            encoded_values = map(unicode_quote_path_segment, value)\n            new_path = '/' + '/'.join(encoded_values)\n            return URL._mutate(self, path=new_path)\n        parts = self._tuple.path.split('/')\n        segments = parts[1:]\n        if self._tuple.path.endswith('/'):\n            segments.pop()\n        segments = map(unicode_unquote, segments)\n        return tuple(segments)", "response": "Return the path segments of the current URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a new path segment to the end of the current string", "response": "def add_path_segment(self, value):\n        \"\"\"\n        Add a new path segment to the end of the current string\n\n        :param string value: the new path segment to use\n\n        Example::\n\n            >>> u = URL('http://example.com/foo/')\n            >>> u.add_path_segment('bar').as_string()\n            'http://example.com/foo/bar'\n        \"\"\"\n        segments = self.path_segments() + (to_unicode(value),)\n        return self.path_segments(segments)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns or set a query parameter for the given key.", "response": "def query_param(self, key, value=None, default=None, as_list=False):\n        \"\"\"\n        Return or set a query parameter for the given key\n\n        The value can be a list.\n\n        :param string key: key to look for\n        :param string default: value to return if ``key`` isn't found\n        :param boolean as_list: whether to return the values as a list\n        :param string value: the new query parameter to use\n        \"\"\"\n        parse_result = self.query_params()\n        if value is not None:\n            # Need to ensure all strings are unicode\n            if isinstance(value, (list, tuple)):\n                value = list(map(to_unicode, value))\n            else:\n                value = to_unicode(value)\n            parse_result[to_unicode(key)] = value\n            return URL._mutate(\n                self, query=unicode_urlencode(parse_result, doseq=True))\n\n        try:\n            result = parse_result[key]\n        except KeyError:\n            return default\n        if as_list:\n            return result\n        return result[0] if len(result) == 1 else result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef append_query_param(self, key, value):\n        values = self.query_param(key, as_list=True, default=[])\n        values.append(value)\n        return self.query_param(key, values)", "response": "Append a query parameter to the end of the list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef query_params(self, value=None):\n        if value is not None:\n            return URL._mutate(self, query=unicode_urlencode(value, doseq=True))\n        query = '' if self._tuple.query is None else self._tuple.query\n\n        # In Python 2.6, urlparse needs a bytestring so we encode and then\n        # decode the result.\n        if not six.PY3:\n            result = parse_qs(to_utf8(query), True)\n            return dict_to_unicode(result)\n\n        return parse_qs(query, True)", "response": "Return or set a dictionary of query params\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_query_param(self, key, value=None):\n        parse_result = self.query_params()\n        if value is not None:\n            index = parse_result[key].index(value)\n            del parse_result[key][index]\n        else:\n            del parse_result[key]\n        return URL._mutate(self, query=unicode_urlencode(parse_result, doseq=True))", "response": "Removes a query parameter from a URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expand(template, variables=None):\n    if variables is None:\n        variables = {}\n    return patterns.sub(functools.partial(_replace, variables), template)", "response": "Expand a URL template string using the passed variables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat a key value pair but don t include the equals sign", "response": "def _format_pair_no_equals(explode, separator, escape, key, value):\n    \"\"\"\n    Format a key, value pair but don't include the equals sign\n    when there is no value\n    \"\"\"\n    if not value:\n        return key\n    return _format_pair(explode, separator, escape, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat a key value pair including the equals sign when there is no value", "response": "def _format_pair_with_equals(explode, separator, escape, key, value):\n    \"\"\"\n    Format a key, value pair including the equals sign\n    when there is no value\n    \"\"\"\n    if not value:\n        return key + '='\n    return _format_pair(explode, separator, escape, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _split_basic(string):\n    tuples = []\n    for word in string.split(','):\n        # Attempt to split on colon\n        parts = word.split(':', 2)\n        key, modifier_fn, explode = parts[0], _identity, False\n        if len(parts) > 1:\n            modifier_fn = functools.partial(\n                _truncate, num_chars=int(parts[1]))\n        if word[len(word) - 1] == '*':\n            key = word[:len(word) - 1]\n            explode = True\n        tuples.append((key, modifier_fn, explode))\n    return tuples", "response": "Split a string into a list of tuples of the form key modifier_fn explode"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the appropriate replacement for match using the passed variables.", "response": "def _replace(variables, match):\n    \"\"\"\n    Return the appropriate replacement for `match` using the passed variables\n    \"\"\"\n    expression = match.group(1)\n\n    # Look-up chars and functions for the specified operator\n    (prefix_char, separator_char, split_fn, escape_fn,\n     format_fn) = operator_map.get(expression[0], defaults)\n\n    replacements = []\n    for key, modify_fn, explode in split_fn(expression):\n        if key in variables:\n            variable = modify_fn(variables[key])\n            replacement = format_fn(\n                explode, separator_char, escape_fn, key, variable)\n            replacements.append(replacement)\n    if not replacements:\n        return ''\n    return prefix_char + separator_char.join(replacements)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict(self, document_path: str, model_name: str, consent_id: str = None) -> Prediction:\n\n        content_type = self._get_content_type(document_path)\n        consent_id = consent_id or str(uuid4())\n        document_id = self._upload_document(document_path, content_type, consent_id)\n        prediction_response = self.post_predictions(document_id, model_name)\n        return Prediction(document_id, consent_id, model_name, prediction_response)", "response": "Run inference and create prediction on a document."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending feedback to the model.", "response": "def send_feedback(self, document_id: str, feedback: List[Field]) -> dict:\n        \"\"\"Send feedback to the model.\n        This method takes care of sending feedback related to document specified by document_id.\n        Feedback consists of ground truth values for the document specified as a list of Field instances.\n\n        >>> from las import ApiClient\n        >>> api_client = ApiClient(endpoint='<api endpoint>')\n        >>> feedback = [Field(label='total_amount', value='120.00'), Field(label='purchase_date', value='2019-03-10')]\n        >>> api_client.send_feedback('<document id>', feedback)\n\n        :param document_id: The document id of the document that will receive the feedback\n        :type document_id: str\n        :param feedback: A list of :py:class:`~las.Field` representing the ground truth values for the document\n        :type feedback: List[Field]\n        :return: Feedback response\n        :rtype: dict\n        :raises InvalidCredentialsException: If the credentials are invalid\n        :raises TooManyRequestsException: If limit of requests per second is reached\n        :raises LimitExceededException: If limit of total requests per month is reached\n        :raises requests.exception.RequestException: If error was raised by requests\n        \"\"\"\n\n        return self.post_document_id(document_id, feedback)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncoding mostly copied from imghdr. what", "response": "def extra_what(file, h=None):\n    \"\"\"Code mostly copied from imghdr.what\"\"\"\n    tests = []\n\n    def test_pdf(h, f):\n        if b'PDF' in h[0:10]:\n            return 'pdf'\n\n    tests.append(test_pdf)\n\n    f = None\n    try:\n        if h is None:\n            if isinstance(file, (str, PathLike)):\n                f = open(file, 'rb')\n                h = f.read(32)\n            else:\n                location = file.tell()\n                h = file.read(32)\n                file.seek(location)\n        for tf in tests:\n            res = tf(h, f)\n            if res:\n                return res\n    finally:\n        if f:\n            f.close()\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef post_documents(self, content_type: str, consent_id: str) -> dict:\n\n        body = json.dumps({'contentType': content_type, 'consentId': consent_id}).encode()\n        uri, headers = self._create_signing_headers('POST', '/documents', body)\n\n        post_documents_response = requests.post(\n            url=uri.geturl(),\n            headers=headers,\n            data=body\n        )\n\n        response = _json_decode(post_documents_response)\n        return response", "response": "Creates a document handle and uploads it to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put_document(document_path: str, content_type: str, presigned_url: str) -> str:\n\n        body = pathlib.Path(document_path).read_bytes()\n        headers = {'Content-Type': content_type}\n        put_document_response = requests.put(presigned_url, data=body, headers=headers)\n        put_document_response.raise_for_status()\n        return put_document_response.content.decode()", "response": "Convenience method for uploading a document to a presigned url."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post_predictions(self, document_id: str, model_name: str) -> dict:\n\n        body = json.dumps({'documentId': document_id, 'modelName': model_name}).encode()\n        uri, headers = self._create_signing_headers('POST', '/predictions', body)\n\n        post_predictions_response = requests.post(\n            url=uri.geturl(),\n            headers=headers,\n            data=body\n        )\n\n        response = _json_decode(post_predictions_response)\n        return response", "response": "Run inference and create a prediction on a document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nposting feedback to the REST API.", "response": "def post_document_id(self, document_id: str, feedback: List[Dict[str, str]]) -> dict:\n        \"\"\"Post feedback to the REST API, calls the POST /documents/{documentId} endpoint.\n        Posting feedback means posting the ground truth data for the particular document.\n        This enables the API to learn from past mistakes.\n\n        >>> from las import Client\n        >>> client = Client(endpoint='<api endpoint>')\n        >>> feedback = [{'label': 'total_amount', 'value': '156.00'}, {'label': 'invoice_date', 'value': '2018-10-23'}]\n        >>> client.post_document_id(document_id='<document id>', feedback=feedback)\n\n        :param document_id: The document id to run inference and create a prediction on\n        :type document_id: str\n        :param feedback: A list of feedback items\n        :type feedback: List[Dict[str, str]]\n        :return: Feedback response from REST API\n        :rtype: dict\n        :raises InvalidCredentialsException: If the credentials are invalid\n        :raises TooManyRequestsException: If limit of requests per second is reached\n        :raises LimitExceededException: If limit of total requests per month is reached\n        :raises requests.exception.RequestException: If error was raised by requests\n        \"\"\"\n\n        body = json.dumps({'feedback': feedback}).encode()\n        uri, headers = self._create_signing_headers('POST', f'/documents/{document_id}', body)\n\n        post_document_id_response = requests.post(\n            url=uri.geturl(),\n            headers=headers,\n            data=body\n        )\n\n        response = _json_decode(post_document_id_response)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete the document with this consent_id", "response": "def delete_consent_id(self, consent_id: str) -> dict:\n        \"\"\"Delete documents with this consent_id, calls the DELETE /consent/{consentId} endpoint.\n\n        >>> from las import Client\n        >>> client = Client(endpoint='<api endpoint>')\n        >>> client.delete_consent_id('<consent id>')\n\n        :param consent_id: Delete documents with this consent_id\n        :type consent_id: str\n        :return: Delete consent id response from REST API\n        :rtype: dict\n        :raises InvalidCredentialsException: If the credentials are invalid\n        :raises TooManyRequestsException: If limit of requests per second is reached\n        :raises LimitExceededException: If limit of total requests per month is reached\n        :raises requests.exception.RequestException: If error was raised by requests\n        \"\"\"\n\n        body = json.dumps({}).encode()\n        uri, headers = self._create_signing_headers('DELETE', f'/consents/{consent_id}', body)\n\n        delete_consent_id_consent = requests.delete(\n            url=uri.geturl(),\n            headers=headers,\n            data=body\n        )\n\n        response = _json_decode(delete_consent_id_consent)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_expiration(self):\n        exp = self._get_int('expiration')\n        if exp is not None:\n            return datetime.datetime.fromtimestamp(\n                exp\n            )\n        return None", "response": "Returns the expiration date."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nauthenticate to ownCloud. This will create a session on the server. :param user_id: user id :param password: password :raises: HTTPResponseError in case an HTTP error status was returned", "response": "def login(self, user_id, password):\n        \"\"\"Authenticate to ownCloud.\n        This will create a session on the server.\n\n        :param user_id: user id\n        :param password: password\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n\n        self._session = requests.session()\n        self._session.verify = self._verify_certs\n        self._session.auth = (user_id, password)\n\n        try:\n            self._update_capabilities()\n\n            url_components = parse.urlparse(self.url)\n            if self._dav_endpoint_version == 1:\n                self._davpath = url_components.path + 'remote.php/dav/files/' + parse.quote(user_id)\n                self._webdav_url = self.url + 'remote.php/dav/files/' + parse.quote(user_id)\n            else:\n                self._davpath = url_components.path + 'remote.php/webdav'\n                self._webdav_url = self.url + 'remote.php/webdav'\n\n        except HTTPResponseError as e:\n            self._session.close()\n            self._session = None\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the file info for the given remote file or None if the file is not found.", "response": "def file_info(self, path):\n        \"\"\"Returns the file info for the given remote file\n\n        :param path: path to the remote file\n        :returns: file info\n        :rtype: :class:`FileInfo` object or `None` if file\n            was not found\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        res = self._make_dav_request('PROPFIND', path, headers={'Depth': '0'})\n        if res:\n            return res[0]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the listing of the given remote directory", "response": "def list(self, path, depth=1):\n        \"\"\"Returns the listing/contents of the given remote directory\n\n        :param path: path to the remote directory\n        :param depth: depth of the listing, integer or \"infinity\"\n        :returns: directory listing\n        :rtype: array of :class:`FileInfo` objects\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        if not path.endswith('/'):\n            path += '/'\n\n        headers = {}\n        if isinstance(depth, int) or depth == \"infinity\":\n            headers['Depth'] = str(depth)\n\n        res = self._make_dav_request('PROPFIND', path, headers=headers)\n        # first one is always the root, remove it from listing\n        if res:\n            return res[1:]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_file_contents(self, path):\n        path = self._normalize_path(path)\n        res = self._session.get(\n            self._webdav_url + parse.quote(self._encode_string(path))\n        )\n        if res.status_code == 200:\n            return res.content\n        elif res.status_code >= 400:\n            raise HTTPResponseError(res)\n        return False", "response": "Returns the contents of a remote file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_directory_as_zip(self, remote_path, local_file):\n        remote_path = self._normalize_path(remote_path)\n        url = self.url + 'index.php/apps/files/ajax/download.php?dir=' \\\n              + parse.quote(remote_path)\n        res = self._session.get(url, stream=True)\n        if res.status_code == 200:\n            if local_file is None:\n                # use downloaded file name from Content-Disposition\n                # targetFile = res.headers['content-disposition']\n                local_file = os.path.basename(remote_path)\n\n            file_handle = open(local_file, 'wb', 8192)\n            for chunk in res.iter_content(8192):\n                file_handle.write(chunk)\n            file_handle.close()\n            return True\n        elif res.status_code >= 400:\n            raise HTTPResponseError(res)\n        return False", "response": "Downloads a remote directory as zip and saves it to local file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put_file(self, remote_path, local_source_file, **kwargs):\n        if kwargs.get('chunked', True):\n            return self._put_file_chunked(\n                remote_path,\n                local_source_file,\n                **kwargs\n            )\n\n        stat_result = os.stat(local_source_file)\n\n        headers = {}\n        if kwargs.get('keep_mtime', True):\n            headers['X-OC-MTIME'] = str(int(stat_result.st_mtime))\n\n        if remote_path[-1] == '/':\n            remote_path += os.path.basename(local_source_file)\n        file_handle = open(local_source_file, 'rb', 8192)\n        res = self._make_dav_request(\n            'PUT',\n            remote_path,\n            data=file_handle,\n            headers=headers\n        )\n        file_handle.close()\n        return res", "response": "Uploads a file to the cannaclite server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload a directory with all its contents into the local directory.", "response": "def put_directory(self, target_path, local_directory, **kwargs):\n        \"\"\"Upload a directory with all its contents\n\n        :param target_path: path of the directory to upload into\n        :param local_directory: path to the local directory to upload\n        :param \\*\\*kwargs: optional arguments that ``put_file`` accepts\n        :returns: True if the operation succeeded, False otherwise\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        target_path = self._normalize_path(target_path)\n        if not target_path.endswith('/'):\n            target_path += '/'\n        gathered_files = []\n\n        if not local_directory.endswith('/'):\n            local_directory += '/'\n\n        basedir = os.path.basename(local_directory[0: -1]) + '/'\n        # gather files to upload\n        for path, _, files in os.walk(local_directory):\n            gathered_files.append(\n                (path, basedir + path[len(local_directory):], files)\n            )\n\n        for path, remote_path, files in gathered_files:\n            self.mkdir(target_path + remote_path + '/')\n            for name in files:\n                if not self.put_file(target_path + remote_path + '/',\n                                     path + '/' + name, **kwargs):\n                    return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _put_file_chunked(self, remote_path, local_source_file, **kwargs):\n        chunk_size = kwargs.get('chunk_size', 10 * 1024 * 1024)\n        result = True\n        transfer_id = int(time.time())\n\n        remote_path = self._normalize_path(remote_path)\n        if remote_path.endswith('/'):\n            remote_path += os.path.basename(local_source_file)\n\n        stat_result = os.stat(local_source_file)\n\n        file_handle = open(local_source_file, 'rb', 8192)\n        file_handle.seek(0, os.SEEK_END)\n        size = file_handle.tell()\n        file_handle.seek(0)\n\n        headers = {}\n        if kwargs.get('keep_mtime', True):\n            headers['X-OC-MTIME'] = str(int(stat_result.st_mtime))\n\n        if size == 0:\n            return self._make_dav_request(\n                'PUT',\n                remote_path,\n                data='',\n                headers=headers\n            )\n\n        chunk_count = int(math.ceil(float(size) / float(chunk_size)))\n\n        if chunk_count > 1:\n            headers['OC-CHUNKED'] = '1'\n\n        for chunk_index in range(0, int(chunk_count)):\n            data = file_handle.read(chunk_size)\n            if chunk_count > 1:\n                chunk_name = '%s-chunking-%s-%i-%i' % \\\n                             (remote_path, transfer_id, chunk_count,\n                              chunk_index)\n            else:\n                chunk_name = remote_path\n\n            if not self._make_dav_request(\n                    'PUT',\n                    chunk_name,\n                    data=data,\n                    headers=headers\n            ):\n                result = False\n                break\n\n        file_handle.close()\n        return result", "response": "Uploads a file using chunks."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists all pending remote shares", "response": "def list_open_remote_share(self):\n        \"\"\"List all pending remote shares\n\n        :returns: array of pending remote shares\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n\n        res = self._make_ocs_request(\n            'GET',\n            self.OCS_SERVICE_SHARE,\n            'remote_shares/pending'\n        )\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree)\n            shares = []\n            for element in tree.find('data').iter('element'):\n                share_attr = {}\n                for child in element:\n                    key = child.tag\n                    value = child.text\n                    share_attr[key] = value\n                shares.append(share_attr)\n            return shares\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accept_remote_share(self, share_id):\n        if not isinstance(share_id, int):\n            return False\n\n        res = self._make_ocs_request(\n            'POST',\n            self.OCS_SERVICE_SHARE,\n            'remote_shares/pending/' + str(share_id)\n        )\n        if res.status_code == 200:\n            return res\n        raise HTTPResponseError(res)", "response": "Accepts a remote share and returns True if the operation succeeded False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating a given share s internal file attributes.", "response": "def update_share(self, share_id, **kwargs):\n        \"\"\"Updates a given share\n\n        :param share_id: (int) Share ID\n        :param perms: (int) update permissions (see share_file_with_user() below)\n        :param password: (string) updated password for public link Share\n        :param public_upload: (boolean) enable/disable public upload for public shares\n        :returns: True if the operation succeeded, False otherwise\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n\n        perms = kwargs.get('perms', None)\n        password = kwargs.get('password', None)\n        public_upload = kwargs.get('public_upload', None)\n        if (isinstance(perms, int)) and (perms > self.OCS_PERMISSION_ALL):\n            perms = None\n        if not (perms or password or (public_upload is not None)):\n            return False\n        if not isinstance(share_id, int):\n            return False\n\n        data = {}\n        if perms:\n            data['permissions'] = perms\n        if isinstance(password, six.string_types):\n            data['password'] = password\n        if (public_upload is not None) and (isinstance(public_upload, bool)):\n            data['publicUpload'] = str(public_upload).lower()\n\n        res = self._make_ocs_request(\n            'PUT',\n            self.OCS_SERVICE_SHARE,\n            'shares/' + str(share_id),\n            data=data\n        )\n        if res.status_code == 200:\n            return True\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshares a remote file with a link", "response": "def share_file_with_link(self, path, **kwargs):\n        \"\"\"Shares a remote file with link\n\n        :param path: path to the remote file to share\n        :param perms (optional): permission of the shared object\n        defaults to read only (1)\n        :param public_upload (optional): allows users to upload files or folders\n        :param password (optional): sets a password\n        http://doc.owncloud.org/server/6.0/admin_manual/sharing_api/index.html\n        :returns: instance of :class:`ShareInfo` with the share info\n            or False if the operation failed\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        perms = kwargs.get('perms', None)\n        public_upload = kwargs.get('public_upload', 'false')\n        password = kwargs.get('password', None)\n\n        path = self._normalize_path(path)\n        post_data = {\n            'shareType': self.OCS_SHARE_TYPE_LINK,\n            'path': self._encode_string(path),\n        }\n        if (public_upload is not None) and (isinstance(public_upload, bool)):\n            post_data['publicUpload'] = str(public_upload).lower()\n        if isinstance(password, six.string_types):\n            post_data['password'] = password\n        if perms:\n            post_data['permissions'] = perms\n\n        res = self._make_ocs_request(\n            'POST',\n            self.OCS_SERVICE_SHARE,\n            'shares',\n            data=post_data\n        )\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree)\n            data_el = tree.find('data')\n            return ShareInfo(\n                                {\n                                    'id': data_el.find('id').text,\n                                    'path': path,\n                                    'url': data_el.find('url').text,\n                                    'token': data_el.find('token').text\n                                }\n            )\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks whether a path is already shared by another user.", "response": "def is_shared(self, path):\n        \"\"\"Checks whether a path is already shared\n\n        :param path: path to the share to be checked\n        :returns: True if the path is already shared, else False\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        # make sure that the path exist - if not, raise HTTPResponseError\n        self.file_info(path)\n        try:\n            result = self.get_shares(path)\n            if result:\n                return len(result) > 0\n        except OCSResponseError as e:\n            if e.status_code != 404:\n                raise e\n            return False\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_share(self, share_id):\n        if (share_id is None) or not (isinstance(share_id, int)):\n            return None\n\n        res = self._make_ocs_request(\n                'GET',\n                self.OCS_SERVICE_SHARE,\n                'shares/' + str(share_id)\n                )\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree)\n            return self._get_shareinfo(tree.find('data').find('element'))\n        raise HTTPResponseError(res)", "response": "Returns the details about a known share"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of shares from the given path.", "response": "def get_shares(self, path='', **kwargs):\n        \"\"\"Returns array of shares\n\n        :param path: path to the share to be checked\n        :param reshares: (optional, boolean) returns not only the shares from\n            the current user but all shares from the given file (default: False)\n        :param subfiles: (optional, boolean) returns all shares within\n            a folder, given that path defines a folder (default: False)\n        :param shared_with_me: (optional, boolean) returns all shares which are\n            shared with me (default: False)\n        :returns: array of shares ShareInfo instances or empty array if the operation failed\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        if not (isinstance(path, six.string_types)):\n            return None\n\n        data = 'shares'\n        if path != '':\n            data += '?'\n            path = self._encode_string(self._normalize_path(path))\n            args = {'path': path}\n            reshares = kwargs.get('reshares', False)\n            if isinstance(reshares, bool) and reshares:\n                args['reshares'] = reshares\n            subfiles = kwargs.get('subfiles', False)\n            if isinstance(subfiles, bool) and subfiles:\n                args['subfiles'] = str(subfiles).lower()\n\n            shared_with_me = kwargs.get('shared_with_me', False)\n            if isinstance(shared_with_me, bool) and shared_with_me:\n                args['shared_with_me'] = \"true\"\n                del args['path']\n\n            data += parse.urlencode(args)\n\n        res = self._make_ocs_request(\n            'GET',\n            self.OCS_SERVICE_SHARE,\n            data\n        )\n\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree)\n            shares = []\n            for element in tree.find('data').iter('element'):\n                '''share_attr = {}\n                for child in element:\n                    key = child.tag\n                    value = child.text\n                    share_attr[key] = value\n                shares.append(share_attr)'''\n                shares.append(self._get_shareinfo(element))\n            return shares\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_user(self, user_name, initial_password):\n        res = self._make_ocs_request(\n            'POST',\n            self.OCS_SERVICE_CLOUD,\n            'users',\n            data={'password': initial_password, 'userid': user_name}\n        )\n\n        # We get 200 when the user was just created.\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree, [100])\n            return True\n\n        raise HTTPResponseError(res)", "response": "Creates a user with an initial password via provisioning API."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a user via provisioning API.", "response": "def delete_user(self, user_name):\n        \"\"\"Deletes a user via provisioning API.\n        If you get back an error 999, then the provisioning API is not enabled.\n\n        :param user_name:  name of user to be deleted\n        :returns: True on success\n        :raises: HTTPResponseError in case an HTTP error status was returned\n\n        \"\"\"\n        res = self._make_ocs_request(\n            'DELETE',\n            self.OCS_SERVICE_CLOUD,\n            'users/' + user_name\n        )\n\n        # We get 200 when the user was deleted.\n        if res.status_code == 200:\n            return True\n\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search_users(self, user_name):\n        action_path = 'users'\n        if user_name:\n            action_path += '?search={}'.format(user_name)\n\n        res = self._make_ocs_request(\n            'GET',\n            self.OCS_SERVICE_CLOUD,\n            action_path\n        )\n\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            users = [x.text for x in tree.findall('data/users/element')]\n\n            return users\n\n        raise HTTPResponseError(res)", "response": "Searches for users via provisioning API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset a user attribute", "response": "def set_user_attribute(self, user_name, key, value):\n        \"\"\"Sets a user attribute\n\n        :param user_name: name of user to modify\n        :param key: key of the attribute to set\n        :param value: value to set\n        :returns: True if the operation succeeded, False otherwise\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n\n        res = self._make_ocs_request(\n            'PUT',\n            self.OCS_SERVICE_CLOUD,\n            'users/' + parse.quote(user_name),\n            data={'key': self._encode_string(key),\n                  'value': self._encode_string(value)}\n        )\n\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree, [100])\n            return True\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a user to a group.", "response": "def add_user_to_group(self, user_name, group_name):\n        \"\"\"Adds a user to a group.\n\n        :param user_name:  name of user to be added\n        :param group_name:  name of group user is to be added to\n        :returns: True if user added\n        :raises: HTTPResponseError in case an HTTP error status was returned\n\n        \"\"\"\n\n        res = self._make_ocs_request(\n            'POST',\n            self.OCS_SERVICE_CLOUD,\n            'users/' + user_name + '/groups',\n            data={'groupid': group_name}\n        )\n\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree, [100])\n            return True\n\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_user_groups(self, user_name):\n\n        res = self._make_ocs_request(\n            'GET',\n            self.OCS_SERVICE_CLOUD,\n            'users/' + user_name + '/groups',\n        )\n\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree, [100])\n            return [group.text for group in tree.find('data/groups')]\n\n        raise HTTPResponseError(res)", "response": "Get a list of groups associated to a user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_user(self, user_name):\n        res = self._make_ocs_request(\n            'GET',\n            self.OCS_SERVICE_CLOUD,\n            'users/' + parse.quote(user_name),\n            data={}\n        )\n\n        tree = ET.fromstring(res.content)\n        self._check_ocs_status(tree)\n        # <ocs><meta><statuscode>100</statuscode><status>ok</status></meta>\n        # <data>\n        # <email>frank@example.org</email><quota>0</quota><enabled>true</enabled>\n        # </data>\n        # </ocs>\n\n        data_element = tree.find('data')\n        return self._xml_to_dict(data_element)", "response": "Retrieves information about a user in the national system."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of subadmin groups associated to a user.", "response": "def get_user_subadmin_groups(self, user_name):\n        \"\"\"Get a list of subadmin groups associated to a user.\n\n        :param user_name:  name of user\n        :returns: list of subadmin groups\n        :raises: HTTPResponseError in case an HTTP error status was returned\n\n        \"\"\"\n\n        res = self._make_ocs_request(\n            'GET',\n            self.OCS_SERVICE_CLOUD,\n            'users/' + user_name + '/subadmins',\n        )\n\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree, [100])\n\n            groups = tree.find('data')\n\n            return groups\n\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshare a remote file with a user", "response": "def share_file_with_user(self, path, user, **kwargs):\n        \"\"\"Shares a remote file with specified user\n\n        :param path: path to the remote file to share\n        :param user: name of the user whom we want to share a file/folder\n        :param perms (optional): permissions of the shared object\n            defaults to read only (1)\n            http://doc.owncloud.org/server/6.0/admin_manual/sharing_api/index.html\n        :param remote_user (optional): True if it is a federated users\n            defaults to False if it is a local user\n        :returns: instance of :class:`ShareInfo` with the share info\n            or False if the operation failed\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        remote_user = kwargs.get('remote_user', False)\n        perms = kwargs.get('perms', self.OCS_PERMISSION_READ)\n        if (((not isinstance(perms, int)) or (perms > self.OCS_PERMISSION_ALL))\n                or ((not isinstance(user, six.string_types)) or (user == ''))):\n            return False\n\n        if remote_user and (not user.endswith('/')):\n            user = user + '/'\n        path = self._normalize_path(path)\n        post_data = {\n            'shareType': self.OCS_SHARE_TYPE_REMOTE if remote_user else\n            self.OCS_SHARE_TYPE_USER,\n            'shareWith': user,\n            'path': self._encode_string(path),\n            'permissions': perms\n        }\n\n        res = self._make_ocs_request(\n            'POST',\n            self.OCS_SERVICE_SHARE,\n            'shares',\n            data=post_data\n        )\n\n        if self._debug:\n            print('OCS share_file request for file %s with permissions %i '\n                  'returned: %i' % (path, perms, res.status_code))\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree)\n            data_el = tree.find('data')\n            return ShareInfo(\n                                {\n                                    'id': data_el.find('id').text,\n                                    'path': path,\n                                    'permissions': perms\n                                }\n            )\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a group via provisioning API.", "response": "def delete_group(self, group_name):\n        \"\"\"Delete a group via provisioning API.\n        If you get back an error 999, then the provisioning API is not enabled.\n\n        :param group_name:  name of group to be deleted\n        :returns: True if group deleted\n        :raises: HTTPResponseError in case an HTTP error status was returned\n\n        \"\"\"\n        res = self._make_ocs_request(\n            'DELETE',\n            self.OCS_SERVICE_CLOUD,\n            'groups/' + group_name\n        )\n\n        # We get 200 when the group was just deleted.\n        if res.status_code == 200:\n            return True\n\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_groups(self):\n        res = self._make_ocs_request(\n            'GET',\n            self.OCS_SERVICE_CLOUD,\n            'groups'\n        )\n\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            groups = [x.text for x in tree.findall('data/groups/element')]\n\n            return groups\n\n        raise HTTPResponseError(res)", "response": "Get the groups via provisioning API."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets group members via provisioning API.", "response": "def get_group_members(self, group_name):\n        \"\"\"Get group members via provisioning API.\n        If you get back an error 999, then the provisioning API is not enabled.\n\n        :param group_name:  name of group to list members\n        :returns: list of group members\n        :raises: HTTPResponseError in case an HTTP error status was returned\n\n        \"\"\"\n        res = self._make_ocs_request(\n            'GET',\n            self.OCS_SERVICE_CLOUD,\n            'groups/' + group_name\n        )\n\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree, [100])\n            return [group.text for group in tree.find('data/users')]\n\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a group exists via provisioning API.", "response": "def group_exists(self, group_name):\n        \"\"\"Checks a group via provisioning API.\n        If you get back an error 999, then the provisioning API is not enabled.\n\n        :param group_name:  name of group to be checked\n        :returns: True if group exists\n        :raises: HTTPResponseError in case an HTTP error status was returned\n\n        \"\"\"\n        res = self._make_ocs_request(\n            'GET',\n            self.OCS_SERVICE_CLOUD,\n            'groups?search=' + group_name\n        )\n\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n\n            for code_el in tree.findall('data/groups/element'):\n                if code_el is not None and code_el.text == group_name:\n                    return True\n\n            return False\n\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshare a remote file with a group", "response": "def share_file_with_group(self, path, group, **kwargs):\n        \"\"\"Shares a remote file with specified group\n\n        :param path: path to the remote file to share\n        :param group: name of the group with which we want to share a file/folder\n        :param perms (optional): permissions of the shared object\n            defaults to read only (1)\n            http://doc.owncloud.org/server/6.0/admin_manual/sharing_api/index.html\n        :returns: instance of :class:`ShareInfo` with the share info\n            or False if the operation failed\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        perms = kwargs.get('perms', self.OCS_PERMISSION_READ)\n        if (((not isinstance(perms, int)) or (perms > self.OCS_PERMISSION_ALL))\n                or ((not isinstance(group, six.string_types)) or (group == ''))):\n            return False\n\n        path = self._normalize_path(path)\n        post_data = {'shareType': self.OCS_SHARE_TYPE_GROUP,\n                     'shareWith': group,\n                     'path': path,\n                     'permissions': perms}\n\n        res = self._make_ocs_request(\n            'POST',\n            self.OCS_SERVICE_SHARE,\n            'shares',\n            data=post_data\n        )\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree)\n            data_el = tree.find('data')\n            return ShareInfo(\n                                {\n                                    'id': data_el.find('id').text,\n                                    'path': path,\n                                    'permissions': perms\n                                }\n            )\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_config(self):\n        path = 'config'\n        res = self._make_ocs_request(\n            'GET',\n            '',\n            path\n        )\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree)\n            values = []\n\n            element = tree.find('data')\n            if element is not None:\n                keys = ['version', 'website', 'host', 'contact', 'ssl']\n                for key in keys:\n                    text = element.find(key).text or ''\n                    values.append(text)\n                return zip(keys, values)\n            else:\n                return None\n        raise HTTPResponseError(res)", "response": "Returns ownCloud config information\n            e. g. 1. 7 website host contact ssl false"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an application attribute value for the given application id and attribute key.", "response": "def get_attribute(self, app=None, key=None):\n        \"\"\"Returns an application attribute\n\n        :param app: application id\n        :param key: attribute key or None to retrieve all values for the\n            given application\n        :returns: attribute value if key was specified, or an array of tuples\n            (key, value) for each attribute\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        path = 'getattribute'\n        if app is not None:\n            path += '/' + parse.quote(app, '')\n            if key is not None:\n                path += '/' + parse.quote(self._encode_string(key), '')\n        res = self._make_ocs_request(\n            'GET',\n            self.OCS_SERVICE_PRIVATEDATA,\n            path\n        )\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree)\n            values = []\n            for element in tree.find('data').iter('element'):\n                app_text = element.find('app').text\n                key_text = element.find('key').text\n                value_text = element.find('value').text or ''\n                if key is None:\n                    if app is None:\n                        values.append((app_text, key_text, value_text))\n                    else:\n                        values.append((key_text, value_text))\n                else:\n                    return value_text\n\n            if len(values) == 0 and key is not None:\n                return None\n            return values\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset an application attribute", "response": "def set_attribute(self, app, key, value):\n        \"\"\"Sets an application attribute\n\n        :param app: application id\n        :param key: key of the attribute to set\n        :param value: value to set\n        :returns: True if the operation succeeded, False otherwise\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        path = 'setattribute/' + parse.quote(app, '') + '/' + parse.quote(\n            self._encode_string(key), '')\n        res = self._make_ocs_request(\n            'POST',\n            self.OCS_SERVICE_PRIVATEDATA,\n            path,\n            data={'value': self._encode_string(value)}\n        )\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree)\n            return True\n        raise HTTPResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_apps(self):\n        ena_apps = {}\n\n        res = self._make_ocs_request('GET', self.OCS_SERVICE_CLOUD, 'apps')\n        if res.status_code != 200:\n            raise HTTPResponseError(res)\n        tree = ET.fromstring(res.content)\n        self._check_ocs_status(tree)\n        # <data><apps><element>files</element><element>activity</element> ...\n        for el in tree.findall('data/apps/element'):\n            ena_apps[el.text] = False\n\n        res = self._make_ocs_request('GET', self.OCS_SERVICE_CLOUD,\n                                     'apps?filter=enabled')\n        if res.status_code != 200:\n            raise HTTPResponseError(res)\n        tree = ET.fromstring(res.content)\n        self._check_ocs_status(tree)\n        for el in tree.findall('data/apps/element'):\n            ena_apps[el.text] = True\n\n        return ena_apps", "response": "List all enabled apps through the provisioning api."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enable_app(self, appname):\n        res = self._make_ocs_request('POST', self.OCS_SERVICE_CLOUD,\n                                     'apps/' + appname)\n        if res.status_code == 200:\n            return True\n\n        raise HTTPResponseError(res)", "response": "Enable an app through provisioning_api\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _normalize_path(path):\n        if isinstance(path, FileInfo):\n            path = path.path\n        if len(path) == 0:\n            return '/'\n        if not path.startswith('/'):\n            path = '/' + path\n        return path", "response": "Makes sure the path starts with a \"/\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding a unicode instance to utf - 8.", "response": "def _encode_string(s):\n        \"\"\"Encodes a unicode instance to utf-8. If a str is passed it will\n        simply be returned\n\n        :param s: str or unicode to encode\n        :returns: encoded output as str\n        \"\"\"\n        if six.PY2 and isinstance(s, unicode):\n            return s.encode('utf-8')\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_ocs_status(tree, accepted_codes=[100]):\n        code_el = tree.find('meta/statuscode')\n        if code_el is not None and int(code_el.text) not in accepted_codes:\n            r = requests.Response()\n            msg_el = tree.find('meta/message')\n            if msg_el is None:\n                msg_el = tree  # fallback to the entire ocs response, if we find no message.\n            r._content = ET.tostring(msg_el)\n            r.status_code = int(code_el.text)\n            raise OCSResponseError(r)", "response": "Checks the status code of an OCS request"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a OCS API request and analyses the response", "response": "def make_ocs_request(self, method, service, action, **kwargs):\n        \"\"\"Makes a OCS API request and analyses the response\n\n        :param method: HTTP method\n        :param service: service name\n        :param action: action path\n        :param \\*\\*kwargs: optional arguments that ``requests.Request.request`` accepts\n        :returns :class:`requests.Response` instance\n        \"\"\"\n\n        accepted_codes = kwargs.pop('accepted_codes', [100])\n\n        res = self._make_ocs_request(method, service, action, **kwargs)\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree, accepted_codes=accepted_codes)\n            return res\n\n        raise OCSResponseError(res)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_ocs_request(self, method, service, action, **kwargs):\n        slash = ''\n        if service:\n            slash = '/'\n        path = self.OCS_BASEPATH + service + slash + action\n\n        attributes = kwargs.copy()\n\n        if 'headers' not in attributes:\n            attributes['headers'] = {}\n\n        attributes['headers']['OCS-APIREQUEST'] = 'true'\n\n        if self._debug:\n            print('OCS request: %s %s %s' % (method, self.url + path,\n                                             attributes))\n\n        res = self._session.request(method, self.url + path, **attributes)\n        return res", "response": "Makes a HTTP request to the OCS API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_dav_request(self, method, path, **kwargs):\n        if self._debug:\n            print('DAV request: %s %s' % (method, path))\n            if kwargs.get('headers'):\n                print('Headers: ', kwargs.get('headers'))\n\n        path = self._normalize_path(path)\n        res = self._session.request(\n            method,\n            self._webdav_url + parse.quote(self._encode_string(path)),\n            **kwargs\n        )\n        if self._debug:\n            print('DAV status: %i' % res.status_code)\n        if res.status_code in [200, 207]:\n            return self._parse_dav_response(res)\n        if res.status_code in [204, 201]:\n            return True\n        raise HTTPResponseError(res)", "response": "Makes a WebDAV request to the specified path and returns a list of FileInfo objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_dav_response(self, res):\n        if res.status_code == 207:\n            tree = ET.fromstring(res.content)\n            items = []\n            for child in tree:\n                items.append(self._parse_dav_element(child))\n            return items\n        return False", "response": "Parses the response from a multi - status response to a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a single DAV element and returns a FileInfo object.", "response": "def _parse_dav_element(self, dav_response):\n        \"\"\"Parses a single DAV element\n\n        :param dav_response: DAV response\n        :returns :class:`FileInfo`\n        \"\"\"\n        href = parse.unquote(\n            self._strip_dav_path(dav_response.find('{DAV:}href').text)\n        )\n\n        if six.PY2:\n            href = href.decode('utf-8')\n\n        file_type = 'file'\n        if href[-1] == '/':\n            file_type = 'dir'\n\n        file_attrs = {}\n        attrs = dav_response.find('{DAV:}propstat')\n        attrs = attrs.find('{DAV:}prop')\n        for attr in attrs:\n            file_attrs[attr.tag] = attr.text\n\n        return FileInfo(href, file_type, file_attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the leading remote. php / webdav path from the given path", "response": "def _strip_dav_path(self, path):\n        \"\"\"Removes the leading \"remote.php/webdav\" path from the given path\n\n        :param path: path containing the remote DAV path \"remote.php/webdav\"\n        :returns: path stripped of the remote DAV path\n        \"\"\"\n        if path.startswith(self._davpath):\n            return path[len(self._davpath):]\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _webdav_move_copy(self, remote_path_source, remote_path_target,\n                           operation):\n        \"\"\"Copies or moves a remote file or directory\n\n        :param remote_path_source: source file or folder to copy / move\n        :param remote_path_target: target file to which to copy / move\n        :param operation: MOVE or COPY\n\n        :returns: True if the operation succeeded, False otherwise\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n\n        if operation != \"MOVE\" and operation != \"COPY\":\n            return False\n\n        if remote_path_target[-1] == '/':\n            remote_path_target += os.path.basename(remote_path_source)\n\n        if not (remote_path_target[0] == '/'):\n            remote_path_target = '/' + remote_path_target\n\n        remote_path_source = self._normalize_path(remote_path_source)\n        headers = {\n            'Destination': self._webdav_url + parse.quote(\n                self._encode_string(remote_path_target))\n        }\n\n        return self._make_dav_request(\n            operation,\n            remote_path_source,\n            headers=headers\n        )", "response": "Copies or moves a remote file or folder to a new location."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _xml_to_dict(self, element):\n        return_dict = {}\n        for el in element:\n            return_dict[el.tag] = None\n            children = el.getchildren()\n            if children:\n                return_dict[el.tag] = self._xml_to_dict(children)\n            else:\n                return_dict[el.tag] = el.text\n        return return_dict", "response": "Takes an XML element iterate over it and build a dictionary containing the keys of the sameCOOKIE and the values of the sameCOOKIE."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_shareinfo(self, data_el):\n        if (data_el is None) or not (isinstance(data_el, ET.Element)):\n            return None\n        return ShareInfo(self._xml_to_dict(data_el))", "response": "Simple helper which returns instance of ShareInfo class\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall all the connected slots with the provided args and kwargs", "response": "def emit(self, *args, **kwargs):\n        \"\"\"\n        Calls all the connected slots with the provided args and kwargs unless block is activated\n        \"\"\"\n\n        if self._block:\n            return\n\n        for slot in self._slots:\n            if not slot:\n                continue\n            elif isinstance(slot, partial):\n                slot()\n            elif isinstance(slot, weakref.WeakKeyDictionary):\n                # For class methods, get the class object and call the method accordingly.\n                for obj, method in slot.items():\n                    method(obj, *args, **kwargs)\n            elif isinstance(slot, weakref.ref):\n                # If it's a weakref, call the ref to get the instance and then call the func\n                # Don't wrap in try/except so we don't risk masking exceptions from the actual func call\n                if (slot() is not None):\n                    slot()(*args, **kwargs)\n            else:\n                # Else call it in a standard way. Should be just lambdas at this point\n                slot(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects the signal to any callable object", "response": "def connect(self, slot):\n        \"\"\"\n        Connects the signal to any callable object\n        \"\"\"\n        if not callable(slot):\n            raise ValueError(\"Connection to non-callable '%s' object failed\" % slot.__class__.__name__)\n\n        if (isinstance(slot, partial) or '<' in slot.__name__):\n            # If it's a partial or a lambda. The '<' check is the only py2 and py3 compatible way I could find\n            if slot not in self._slots:\n                self._slots.append(slot)\n        elif inspect.ismethod(slot):\n            # Check if it's an instance method and store it with the instance as the key\n            slotSelf = slot.__self__\n            slotDict = weakref.WeakKeyDictionary()\n            slotDict[slotSelf] = slot.__func__\n            if slotDict not in self._slots:\n                self._slots.append(slotDict)\n        else:\n            # If it's just a function then just store it as a weakref.\n            newSlotRef = weakref.ref(slot)\n            if newSlotRef not in self._slots:\n                self._slots.append(newSlotRef)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisconnects the slot from the signal", "response": "def disconnect(self, slot):\n        \"\"\"\n        Disconnects the slot from the signal\n        \"\"\"\n        if not callable(slot):\n            return\n\n        if inspect.ismethod(slot):\n            # If it's a method, then find it by its instance\n            slotSelf = slot.__self__\n            for s in self._slots:\n                if isinstance(s, weakref.WeakKeyDictionary) and (slotSelf in s) and (s[slotSelf] is slot.__func__):\n                    self._slots.remove(s)\n                    break\n        elif isinstance(slot, partial) or '<' in slot.__name__:\n            # If it's a partial or lambda, try to remove directly\n            try:\n                self._slots.remove(slot)\n            except ValueError:\n                pass\n        else:\n            # It's probably a function, so try to remove by weakref\n            try:\n                self._slots.remove(weakref.ref(slot))\n            except ValueError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters a given signal with the given set of slots.", "response": "def register(self, name, *slots):\n        \"\"\"\n        Registers a given signal\n        :param name: the signal to register\n        \"\"\"\n        # setdefault initializes the object even if it exists. This is more efficient\n        if name not in self:\n            self[name] = Signal()\n\n        for slot in slots:\n            self[name].connect(slot)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nemit a signal by name if it exists. Any additional args or kwargs are passed to the signal.", "response": "def emit(self, signalName, *args, **kwargs):\n        \"\"\"\n        Emits a signal by name if it exists. Any additional args or kwargs are passed to the signal\n        :param signalName: the signal name to emit\n        \"\"\"\n        assert signalName in self, \"%s is not a registered signal\" % signalName\n        self[signalName].emit(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect(self, signalName, slot):\n        assert signalName in self, \"%s is not a registered signal\" % signalName\n        self[signalName].connect(slot)", "response": "Connect a given signal to a given slot"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the block on any provided signals or to all signals", "response": "def block(self, signals=None, isBlocked=True):\n        \"\"\"\n        Sets the block on any provided signals, or to all signals\n\n        :param signals: defaults to all signals. Accepts either a single string or a list of strings\n        :param isBlocked: the state to set the signal to\n        \"\"\"\n        if signals:\n            try:\n                if isinstance(signals, basestring):\n                    signals = [signals]\n            except NameError:\n                if isinstance(signals, str):\n                    signals = [signals]\n\n        signals = signals or self.keys()\n\n        for signal in signals:\n            if signal not in self:\n                raise RuntimeError(\"Could not find signal matching %s\" % signal)\n            self[signal].block(isBlocked)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens a file handle or use an existing file - like object.", "response": "def _open(file_or_str, **kwargs):\n    '''Either open a file handle, or use an existing file-like object.\n\n    This will behave as the `open` function if `file_or_str` is a string.\n\n    If `file_or_str` has the `read` attribute, it will return `file_or_str`.\n\n    Otherwise, an `IOError` is raised.\n    '''\n    if hasattr(file_or_str, 'read'):\n        yield file_or_str\n    elif isinstance(file_or_str, six.string_types):\n        with open(file_or_str, **kwargs) as file_desc:\n            yield file_desc\n    else:\n        raise IOError('Invalid file-or-str object: {}'.format(file_or_str))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_delimited(filename, converters, delimiter=r'\\s+'):\n    # Initialize list of empty lists\n    n_columns = len(converters)\n    columns = tuple(list() for _ in range(n_columns))\n\n    # Create re object for splitting lines\n    splitter = re.compile(delimiter)\n\n    # Note: we do io manually here for two reasons.\n    #   1. The csv module has difficulties with unicode, which may lead\n    #      to failures on certain annotation strings\n    #\n    #   2. numpy's text loader does not handle non-numeric data\n    #\n    with _open(filename, mode='r') as input_file:\n        for row, line in enumerate(input_file, 1):\n            # Split each line using the supplied delimiter\n            data = splitter.split(line.strip(), n_columns - 1)\n\n            # Throw a helpful error if we got an unexpected # of columns\n            if n_columns != len(data):\n                raise ValueError('Expected {} columns, got {} at '\n                                 '{}:{:d}:\\n\\t{}'.format(n_columns, len(data),\n                                                         filename, row, line))\n\n            for value, column, converter in zip(data, columns, converters):\n                # Try converting the value, throw a helpful error on failure\n                try:\n                    converted_value = converter(value)\n                except:\n                    raise ValueError(\"Couldn't convert value {} using {} \"\n                                     \"found at {}:{:d}:\\n\\t{}\".format(\n                                         value, converter.__name__, filename,\n                                         row, line))\n                column.append(converted_value)\n\n    # Sane output\n    if n_columns == 1:\n        return columns[0]\n    else:\n        return columns", "response": "r Utility function for loading in data from an annotation file where the number of columns is inferred from the length of the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_events(filename, delimiter=r'\\s+'):\n    # Use our universal function to load in the events\n    events = load_delimited(filename, [float], delimiter)\n    events = np.array(events)\n    # Validate them, but throw a warning in place of an error\n    try:\n        util.validate_events(events)\n    except ValueError as error:\n        warnings.warn(error.args[0])\n\n    return events", "response": "r Import time - stamp events from an annotation file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_time_series(filename, delimiter=r'\\s+'):\n    # Use our universal function to load in the events\n    times, values = load_delimited(filename, [float, float], delimiter)\n    times = np.array(times)\n    values = np.array(values)\n\n    return times, values", "response": "r Import a time series from an annotation file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_patterns(filename):\n\n    # List with all the patterns\n    pattern_list = []\n    # Current pattern, which will contain all occs\n    pattern = []\n    # Current occurrence, containing (onset, midi)\n    occurrence = []\n    with _open(filename, mode='r') as input_file:\n        for line in input_file.readlines():\n            if \"pattern\" in line:\n                if occurrence != []:\n                    pattern.append(occurrence)\n                if pattern != []:\n                    pattern_list.append(pattern)\n                occurrence = []\n                pattern = []\n                continue\n            if \"occurrence\" in line:\n                if occurrence != []:\n                    pattern.append(occurrence)\n                occurrence = []\n                continue\n            string_values = line.split(\",\")\n            onset_midi = (float(string_values[0]), float(string_values[1]))\n            occurrence.append(onset_midi)\n\n        # Add last occurrence and pattern to pattern_list\n        if occurrence != []:\n            pattern.append(occurrence)\n        if pattern != []:\n            pattern_list.append(pattern)\n\n    return pattern_list", "response": "Loads the patters contained in the file and puts them into a list\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_wav(path, mono=True):\n\n    fs, audio_data = scipy.io.wavfile.read(path)\n    # Make float in range [-1, 1]\n    if audio_data.dtype == 'int8':\n        audio_data = audio_data/float(2**8)\n    elif audio_data.dtype == 'int16':\n        audio_data = audio_data/float(2**16)\n    elif audio_data.dtype == 'int32':\n        audio_data = audio_data/float(2**24)\n    else:\n        raise ValueError('Got unexpected .wav data type '\n                         '{}'.format(audio_data.dtype))\n    # Optionally convert to mono\n    if mono and audio_data.ndim != 1:\n        audio_data = audio_data.mean(axis=1)\n    return audio_data, fs", "response": "Loads a. wav file as a numpy array using scipy. io. wavfile."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_tempo(filename, delimiter=r'\\s+'):\n    # Use our universal function to load the key and mode strings\n    t1, t2, weight = load_delimited(filename, [float, float, float], delimiter)\n\n    weight = weight[0]\n    tempi = np.concatenate([t1, t2])\n\n    if len(t1) != 1:\n        raise ValueError('Tempo file should contain only one line.')\n\n    # Validate them, but throw a warning in place of an error\n    try:\n        tempo.validate_tempi(tempi)\n    except ValueError as error:\n        warnings.warn(error.args[0])\n\n    if not 0 <= weight <= 1:\n        raise ValueError('Invalid weight: {}'.format(weight))\n\n    return tempi, weight", "response": "r Loads tempo estimates from an annotation file in MIREX format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _pitch_classes():\n    r'''Map from pitch class (str) to semitone (int).'''\n    pitch_classes = ['C', 'D', 'E', 'F', 'G', 'A', 'B']\n    semitones = [0, 2, 4, 5, 7, 9, 11]\n    return dict([(c, s) for c, s in zip(pitch_classes, semitones)])", "response": "Map from pitch class to semitone."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _scale_degrees():\n    r'''Mapping from scale degrees (str) to semitones (int).'''\n    degrees = ['1', '2', '3',  '4',  '5',  '6', '7',\n               '8', '9', '10', '11', '12', '13']\n    semitones = [0, 2, 4, 5, 7, 9, 11, 12, 14, 16, 17, 19, 21]\n    return dict([(d, s) for d, s in zip(degrees, semitones)])", "response": "Mapping from scale degrees to semitones."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pitch_class_to_semitone(pitch_class):\n    r'''Convert a pitch class to semitone.\n\n    Parameters\n    ----------\n    pitch_class : str\n        Spelling of a given pitch class, e.g. 'C#', 'Gbb'\n\n    Returns\n    -------\n    semitone : int\n        Semitone value of the pitch class.\n\n    '''\n    semitone = 0\n    for idx, char in enumerate(pitch_class):\n        if char == '#' and idx > 0:\n            semitone += 1\n        elif char == 'b' and idx > 0:\n            semitone -= 1\n        elif idx == 0:\n            semitone = PITCH_CLASSES.get(char)\n        else:\n            raise InvalidChordException(\n                \"Pitch class improperly formed: %s\" % pitch_class)\n    return semitone % 12", "response": "r Convert a pitch class to semitone."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scale_degree_to_semitone(scale_degree):\n    semitone = 0\n    offset = 0\n    if scale_degree.startswith(\"#\"):\n        offset = scale_degree.count(\"#\")\n        scale_degree = scale_degree.strip(\"#\")\n    elif scale_degree.startswith('b'):\n        offset = -1 * scale_degree.count(\"b\")\n        scale_degree = scale_degree.strip(\"b\")\n\n    semitone = SCALE_DEGREES.get(scale_degree, None)\n    if semitone is None:\n        raise InvalidChordException(\n            \"Scale degree improperly formed: {}, expected one of {}.\"\n            .format(scale_degree, list(SCALE_DEGREES.keys())))\n    return semitone + offset", "response": "r Converts a scale degree to a semitone."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scale_degree_to_bitmap(scale_degree, modulo=False, length=BITMAP_LENGTH):\n    sign = 1\n    if scale_degree.startswith(\"*\"):\n        sign = -1\n        scale_degree = scale_degree.strip(\"*\")\n    edit_map = [0] * length\n    sd_idx = scale_degree_to_semitone(scale_degree)\n    if sd_idx < length or modulo:\n        edit_map[sd_idx % length] = sign\n    return np.array(edit_map)", "response": "Create a bitmap representation of a relative scale degree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef quality_to_bitmap(quality):\n    if quality not in QUALITIES:\n        raise InvalidChordException(\n            \"Unsupported chord quality shorthand: '%s' \"\n            \"Did you mean to reduce extended chords?\" % quality)\n    return np.array(QUALITIES[quality])", "response": "Return the bitmap representation of a given quality."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest for well - formedness of a chord label.", "response": "def validate_chord_label(chord_label):\n    \"\"\"Test for well-formedness of a chord label.\n\n    Parameters\n    ----------\n    chord : str\n        Chord label to validate.\n\n    \"\"\"\n\n    # This monster regexp is pulled from the JAMS chord namespace,\n    # which is in turn derived from the context-free grammar of\n    # Harte et al., 2005.\n\n    pattern = re.compile(r'''^((N|X)|(([A-G](b*|#*))((:(maj|min|dim|aug|1|5|sus2|sus4|maj6|min6|7|maj7|min7|dim7|hdim7|minmaj7|aug7|9|maj9|min9|11|maj11|min11|13|maj13|min13)(\\((\\*?((b*|#*)([1-9]|1[0-3]?))(,\\*?((b*|#*)([1-9]|1[0-3]?)))*)\\))?)|(:\\((\\*?((b*|#*)([1-9]|1[0-3]?))(,\\*?((b*|#*)([1-9]|1[0-3]?)))*)\\)))?((/((b*|#*)([1-9]|1[0-3]?)))?)?))$''')  # nopep8\n\n    if not pattern.match(chord_label):\n        raise InvalidChordException('Invalid chord label: '\n                                    '{}'.format(chord_label))\n    pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a chord label into its four constituent parts.", "response": "def split(chord_label, reduce_extended_chords=False):\n    \"\"\"Parse a chord label into its four constituent parts:\n        - root\n        - quality shorthand\n        - scale degrees\n        - bass\n\n    Note: Chords lacking quality AND interval information are major.\n      - If a quality is specified, it is returned.\n      - If an interval is specified WITHOUT a quality, the quality field is\n        empty.\n\n    Some examples::\n\n        'C' -> ['C', 'maj', {}, '1']\n        'G#:min(*b3,*5)/5' -> ['G#', 'min', {'*b3', '*5'}, '5']\n        'A:(3)/6' -> ['A', '', {'3'}, '6']\n\n    Parameters\n    ----------\n    chord_label : str\n        A chord label.\n    reduce_extended_chords : bool\n        Whether to map the upper voicings of extended chords (9's, 11's, 13's)\n        to semitone extensions. (Default value = False)\n\n    Returns\n    -------\n    chord_parts : list\n        Split version of the chord label.\n\n    \"\"\"\n    chord_label = str(chord_label)\n    validate_chord_label(chord_label)\n    if chord_label == NO_CHORD:\n        return [chord_label, '', set(), '']\n\n    bass = '1'\n    if \"/\" in chord_label:\n        chord_label, bass = chord_label.split(\"/\")\n\n    scale_degrees = set()\n    omission = False\n    if \"(\" in chord_label:\n        chord_label, scale_degrees = chord_label.split(\"(\")\n        omission = \"*\" in scale_degrees\n        scale_degrees = scale_degrees.strip(\")\")\n        scale_degrees = set([i.strip() for i in scale_degrees.split(\",\")])\n\n    # Note: Chords lacking quality AND added interval information are major.\n    #   If a quality shorthand is specified, it is returned.\n    #   If an interval is specified WITHOUT a quality, the quality field is\n    #     empty.\n    #   Intervals specifying omissions MUST have a quality.\n    if omission and \":\" not in chord_label:\n        raise InvalidChordException(\n            \"Intervals specifying omissions MUST have a quality.\")\n    quality = '' if scale_degrees else 'maj'\n    if \":\" in chord_label:\n        chord_root, quality_name = chord_label.split(\":\")\n        # Extended chords (with \":\"s) may not explicitly have Major qualities,\n        # so only overwrite the default if the string is not empty.\n        if quality_name:\n            quality = quality_name.lower()\n    else:\n        chord_root = chord_label\n\n    if reduce_extended_chords:\n        quality, addl_scale_degrees = reduce_extended_quality(quality)\n        scale_degrees.update(addl_scale_degrees)\n\n    return [chord_root, quality, scale_degrees, bass]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef join(chord_root, quality='', extensions=None, bass=''):\n    chord_label = chord_root\n    if quality or extensions:\n        chord_label += \":%s\" % quality\n    if extensions:\n        chord_label += \"(%s)\" % \",\".join(extensions)\n    if bass and bass != '1':\n        chord_label += \"/%s\" % bass\n    validate_chord_label(chord_label)\n    return chord_label", "response": "r Joins the parts of a chord into a complete chord label."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntranslate a chord label to numerical representations for evaluation.", "response": "def encode(chord_label, reduce_extended_chords=False,\n           strict_bass_intervals=False):\n    \"\"\"Translate a chord label to numerical representations for evaluation.\n\n    Parameters\n    ----------\n    chord_label : str\n        Chord label to encode.\n    reduce_extended_chords : bool\n        Whether to map the upper voicings of extended chords (9's, 11's, 13's)\n        to semitone extensions.\n        (Default value = False)\n    strict_bass_intervals : bool\n        Whether to require that the bass scale degree is present in the chord.\n        (Default value = False)\n\n    Returns\n    -------\n    root_number : int\n        Absolute semitone of the chord's root.\n    semitone_bitmap : np.ndarray, dtype=int\n        12-dim vector of relative semitones in the chord spelling.\n    bass_number : int\n        Relative semitone of the chord's bass note, e.g. 0=root, 7=fifth, etc.\n\n    \"\"\"\n\n    if chord_label == NO_CHORD:\n        return NO_CHORD_ENCODED\n    if chord_label == X_CHORD:\n        return X_CHORD_ENCODED\n    chord_root, quality, scale_degrees, bass = split(\n        chord_label, reduce_extended_chords=reduce_extended_chords)\n\n    root_number = pitch_class_to_semitone(chord_root)\n    bass_number = scale_degree_to_semitone(bass) % 12\n\n    semitone_bitmap = quality_to_bitmap(quality)\n    semitone_bitmap[0] = 1\n\n    for scale_degree in scale_degrees:\n        semitone_bitmap += scale_degree_to_bitmap(scale_degree,\n                                                  reduce_extended_chords)\n\n    semitone_bitmap = (semitone_bitmap > 0).astype(np.int)\n    if not semitone_bitmap[bass_number] and strict_bass_intervals:\n        raise InvalidChordException(\n            \"Given bass scale degree is absent from this chord: \"\n            \"%s\" % chord_label, chord_label)\n    else:\n        semitone_bitmap[bass_number] = 1\n    return root_number, semitone_bitmap, bass_number"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode_many(chord_labels, reduce_extended_chords=False):\n    num_items = len(chord_labels)\n    roots, basses = np.zeros([2, num_items], dtype=np.int)\n    semitones = np.zeros([num_items, 12], dtype=np.int)\n    local_cache = dict()\n    for i, label in enumerate(chord_labels):\n        result = local_cache.get(label, None)\n        if result is None:\n            result = encode(label, reduce_extended_chords)\n            local_cache[label] = result\n        roots[i], semitones[i], basses[i] = result\n    return roots, semitones, basses", "response": "Translate a set of chord labels to numerical representations for sane\n    evaluation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rotate_bitmaps_to_roots(bitmaps, roots):\n    abs_bitmaps = []\n    for bitmap, chord_root in zip(bitmaps, roots):\n        abs_bitmaps.append(rotate_bitmap_to_root(bitmap, chord_root))\n    return np.asarray(abs_bitmaps)", "response": "Circularly shift a relative bitmaps to asbolute pitch classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(reference_labels, estimated_labels):\n    N = len(reference_labels)\n    M = len(estimated_labels)\n    if N != M:\n        raise ValueError(\n            \"Chord comparison received different length lists: \"\n            \"len(reference)=%d\\tlen(estimates)=%d\" % (N, M))\n    for labels in [reference_labels, estimated_labels]:\n        for chord_label in labels:\n            validate_chord_label(chord_label)\n    # When either label list is empty, warn the user\n    if len(reference_labels) == 0:\n        warnings.warn('Reference labels are empty')\n    if len(estimated_labels) == 0:\n        warnings.warn('Estimated labels are empty')", "response": "Checks that the input annotations to a comparison function look like\n    valid chord labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the weighted accuracy of a list of chord comparisons.", "response": "def weighted_accuracy(comparisons, weights):\n    \"\"\"Compute the weighted accuracy of a list of chord comparisons.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> est_intervals, est_labels = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, ref_intervals.min(),\n    ...     ref_intervals.max(), mir_eval.chord.NO_CHORD,\n    ...     mir_eval.chord.NO_CHORD)\n    >>> (intervals,\n    ...  ref_labels,\n    ...  est_labels) = mir_eval.util.merge_labeled_intervals(\n    ...      ref_intervals, ref_labels, est_intervals, est_labels)\n    >>> durations = mir_eval.util.intervals_to_durations(intervals)\n    >>> # Here, we're using the \"thirds\" function to compare labels\n    >>> # but any of the comparison functions would work.\n    >>> comparisons = mir_eval.chord.thirds(ref_labels, est_labels)\n    >>> score = mir_eval.chord.weighted_accuracy(comparisons, durations)\n\n    Parameters\n    ----------\n    comparisons : np.ndarray\n        List of chord comparison scores, in [0, 1] or -1\n    weights : np.ndarray\n        Weights (not necessarily normalized) for each comparison.\n        This can be a list of interval durations\n\n    Returns\n    -------\n    score : float\n        Weighted accuracy\n\n    \"\"\"\n    N = len(comparisons)\n    # There should be as many weights as comparisons\n    if weights.shape[0] != N:\n        raise ValueError('weights and comparisons should be of the same'\n                         ' length. len(weights) = {} but len(comparisons)'\n                         ' = {}'.format(weights.shape[0], N))\n    if (weights < 0).any():\n        raise ValueError('Weights should all be positive.')\n    if np.sum(weights) == 0:\n        warnings.warn('No nonzero weights, returning 0')\n        return 0\n    # Find all comparison scores which are valid\n    valid_idx = (comparisons >= 0)\n    # If no comparable chords were provided, warn and return 0\n    if valid_idx.sum() == 0:\n        warnings.warn(\"No reference chords were comparable \"\n                      \"to estimated chords, returning 0.\")\n        return 0\n    # Remove any uncomparable labels\n    comparisons = comparisons[valid_idx]\n    weights = weights[valid_idx]\n    # Normalize the weights\n    total_weight = float(np.sum(weights))\n    normalized_weights = np.asarray(weights, dtype=float)/total_weight\n    # Score is the sum of all weighted comparisons\n    return np.sum(comparisons*normalized_weights)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomparing chords along root & third relationships.", "response": "def thirds(reference_labels, estimated_labels):\n    \"\"\"Compare chords along root & third relationships.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> est_intervals, est_labels = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, ref_intervals.min(),\n    ...     ref_intervals.max(), mir_eval.chord.NO_CHORD,\n    ...     mir_eval.chord.NO_CHORD)\n    >>> (intervals,\n    ...  ref_labels,\n    ...  est_labels) = mir_eval.util.merge_labeled_intervals(\n    ...      ref_intervals, ref_labels, est_intervals, est_labels)\n    >>> durations = mir_eval.util.intervals_to_durations(intervals)\n    >>> comparisons = mir_eval.chord.thirds(ref_labels, est_labels)\n    >>> score = mir_eval.chord.weighted_accuracy(comparisons, durations)\n\n    Parameters\n    ----------\n    reference_labels : list, len=n\n        Reference chord labels to score against.\n    estimated_labels : list, len=n\n        Estimated chord labels to score against.\n\n    Returns\n    -------\n    comparison_scores : np.ndarray, shape=(n,), dtype=float\n        Comparison scores, in [0.0, 1.0]\n\n    \"\"\"\n    validate(reference_labels, estimated_labels)\n    ref_roots, ref_semitones = encode_many(reference_labels, False)[:2]\n    est_roots, est_semitones = encode_many(estimated_labels, False)[:2]\n\n    eq_roots = ref_roots == est_roots\n    eq_thirds = ref_semitones[:, 3] == est_semitones[:, 3]\n    comparison_scores = (eq_roots * eq_thirds).astype(np.float)\n\n    # Ignore 'X' chords\n    comparison_scores[np.any(ref_semitones < 0, axis=1)] = -1.0\n    return comparison_scores"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the score of chords along root third and bass relationships.", "response": "def thirds_inv(reference_labels, estimated_labels):\n    \"\"\"Score chords along root, third, & bass relationships.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> est_intervals, est_labels = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, ref_intervals.min(),\n    ...     ref_intervals.max(), mir_eval.chord.NO_CHORD,\n    ...     mir_eval.chord.NO_CHORD)\n    >>> (intervals,\n    ...  ref_labels,\n    ...  est_labels) = mir_eval.util.merge_labeled_intervals(\n    ...      ref_intervals, ref_labels, est_intervals, est_labels)\n    >>> durations = mir_eval.util.intervals_to_durations(intervals)\n    >>> comparisons = mir_eval.chord.thirds_inv(ref_labels, est_labels)\n    >>> score = mir_eval.chord.weighted_accuracy(comparisons, durations)\n\n    Parameters\n    ----------\n    reference_labels : list, len=n\n        Reference chord labels to score against.\n    estimated_labels : list, len=n\n        Estimated chord labels to score against.\n\n    Returns\n    -------\n    scores : np.ndarray, shape=(n,), dtype=float\n        Comparison scores, in [0.0, 1.0]\n\n    \"\"\"\n    validate(reference_labels, estimated_labels)\n    ref_roots, ref_semitones, ref_bass = encode_many(reference_labels, False)\n    est_roots, est_semitones, est_bass = encode_many(estimated_labels, False)\n\n    eq_root = ref_roots == est_roots\n    eq_bass = ref_bass == est_bass\n    eq_third = ref_semitones[:, 3] == est_semitones[:, 3]\n    comparison_scores = (eq_root * eq_third * eq_bass).astype(np.float)\n\n    # Ignore 'X' chords\n    comparison_scores[np.any(ref_semitones < 0, axis=1)] = -1.0\n    return comparison_scores"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompare chords along triad.", "response": "def triads(reference_labels, estimated_labels):\n    \"\"\"Compare chords along triad (root & quality to #5) relationships.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> est_intervals, est_labels = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, ref_intervals.min(),\n    ...     ref_intervals.max(), mir_eval.chord.NO_CHORD,\n    ...     mir_eval.chord.NO_CHORD)\n    >>> (intervals,\n    ...  ref_labels,\n    ...  est_labels) = mir_eval.util.merge_labeled_intervals(\n    ...      ref_intervals, ref_labels, est_intervals, est_labels)\n    >>> durations = mir_eval.util.intervals_to_durations(intervals)\n    >>> comparisons = mir_eval.chord.triads(ref_labels, est_labels)\n    >>> score = mir_eval.chord.weighted_accuracy(comparisons, durations)\n\n    Parameters\n    ----------\n    reference_labels : list, len=n\n        Reference chord labels to score against.\n    estimated_labels : list, len=n\n        Estimated chord labels to score against.\n\n    Returns\n    -------\n    comparison_scores : np.ndarray, shape=(n,), dtype=float\n        Comparison scores, in [0.0, 1.0]\n\n    \"\"\"\n    validate(reference_labels, estimated_labels)\n    ref_roots, ref_semitones = encode_many(reference_labels, False)[:2]\n    est_roots, est_semitones = encode_many(estimated_labels, False)[:2]\n\n    eq_roots = ref_roots == est_roots\n    eq_semitones = np.all(\n        np.equal(ref_semitones[:, :8], est_semitones[:, :8]), axis=1)\n    comparison_scores = (eq_roots * eq_semitones).astype(np.float)\n\n    # Ignore 'X' chords\n    comparison_scores[np.any(ref_semitones < 0, axis=1)] = -1.0\n    return comparison_scores"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the inverse of the triads of a single set of chords.", "response": "def triads_inv(reference_labels, estimated_labels):\n    \"\"\"Score chords along triad (root, quality to #5, & bass) relationships.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> est_intervals, est_labels = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, ref_intervals.min(),\n    ...     ref_intervals.max(), mir_eval.chord.NO_CHORD,\n    ...     mir_eval.chord.NO_CHORD)\n    >>> (intervals,\n    ...  ref_labels,\n    ...  est_labels) = mir_eval.util.merge_labeled_intervals(\n    ...      ref_intervals, ref_labels, est_intervals, est_labels)\n    >>> durations = mir_eval.util.intervals_to_durations(intervals)\n    >>> comparisons = mir_eval.chord.triads_inv(ref_labels, est_labels)\n    >>> score = mir_eval.chord.weighted_accuracy(comparisons, durations)\n\n    Parameters\n    ----------\n    reference_labels : list, len=n\n        Reference chord labels to score against.\n    estimated_labels : list, len=n\n        Estimated chord labels to score against.\n\n    Returns\n    -------\n    scores : np.ndarray, shape=(n,), dtype=float\n        Comparison scores, in [0.0, 1.0]\n\n    \"\"\"\n    validate(reference_labels, estimated_labels)\n    ref_roots, ref_semitones, ref_bass = encode_many(reference_labels, False)\n    est_roots, est_semitones, est_bass = encode_many(estimated_labels, False)\n\n    eq_roots = ref_roots == est_roots\n    eq_basses = ref_bass == est_bass\n    eq_semitones = np.all(\n        np.equal(ref_semitones[:, :8], est_semitones[:, :8]), axis=1)\n    comparison_scores = (eq_roots * eq_semitones * eq_basses).astype(np.float)\n\n    # Ignore 'X' chords\n    comparison_scores[np.any(ref_semitones < 0, axis=1)] = -1.0\n    return comparison_scores"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompare chords according to roots.", "response": "def root(reference_labels, estimated_labels):\n    \"\"\"Compare chords according to roots.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> est_intervals, est_labels = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, ref_intervals.min(),\n    ...     ref_intervals.max(), mir_eval.chord.NO_CHORD,\n    ...     mir_eval.chord.NO_CHORD)\n    >>> (intervals,\n    ...  ref_labels,\n    ...  est_labels) = mir_eval.util.merge_labeled_intervals(\n    ...      ref_intervals, ref_labels, est_intervals, est_labels)\n    >>> durations = mir_eval.util.intervals_to_durations(intervals)\n    >>> comparisons = mir_eval.chord.root(ref_labels, est_labels)\n    >>> score = mir_eval.chord.weighted_accuracy(comparisons, durations)\n\n    Parameters\n    ----------\n    reference_labels : list, len=n\n        Reference chord labels to score against.\n    estimated_labels : list, len=n\n        Estimated chord labels to score against.\n\n    Returns\n    -------\n    comparison_scores : np.ndarray, shape=(n,), dtype=float\n        Comparison scores, in [0.0, 1.0], or -1 if the comparison is out of\n        gamut.\n\n    \"\"\"\n\n    validate(reference_labels, estimated_labels)\n    ref_roots, ref_semitones = encode_many(reference_labels, False)[:2]\n    est_roots = encode_many(estimated_labels, False)[0]\n    comparison_scores = (ref_roots == est_roots).astype(np.float)\n\n    # Ignore 'X' chords\n    comparison_scores[np.any(ref_semitones < 0, axis=1)] = -1.0\n    return comparison_scores"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mirex(reference_labels, estimated_labels):\n    validate(reference_labels, estimated_labels)\n    # TODO(?): Should this be an argument?\n    min_intersection = 3\n    ref_data = encode_many(reference_labels, False)\n    ref_chroma = rotate_bitmaps_to_roots(ref_data[1], ref_data[0])\n    est_data = encode_many(estimated_labels, False)\n    est_chroma = rotate_bitmaps_to_roots(est_data[1], est_data[0])\n\n    eq_chroma = (ref_chroma * est_chroma).sum(axis=-1)\n\n    # Chroma matching for set bits\n    comparison_scores = (eq_chroma >= min_intersection).astype(np.float)\n\n    # No-chord matching; match -1 roots, SKIP_CHORDS dropped next\n    no_root = np.logical_and(ref_data[0] == -1, est_data[0] == -1)\n    comparison_scores[no_root] = 1.0\n\n    # Skip chords where the number of active semitones `n` is\n    #   0 < n < `min_intersection`.\n    ref_semitone_count = (ref_data[1] > 0).sum(axis=1)\n    skip_idx = np.logical_and(ref_semitone_count > 0,\n                              ref_semitone_count < min_intersection)\n    # Also ignore 'X' chords.\n    np.logical_or(skip_idx, np.any(ref_data[1] < 0, axis=1), skip_idx)\n    comparison_scores[skip_idx] = -1.0\n    return comparison_scores", "response": "Compare chords along MIREX rules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare chords along major - minor rules.", "response": "def majmin(reference_labels, estimated_labels):\n    \"\"\"Compare chords along major-minor rules. Chords with qualities outside\n    Major/minor/no-chord are ignored.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> est_intervals, est_labels = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, ref_intervals.min(),\n    ...     ref_intervals.max(), mir_eval.chord.NO_CHORD,\n    ...     mir_eval.chord.NO_CHORD)\n    >>> (intervals,\n    ...  ref_labels,\n    ...  est_labels) = mir_eval.util.merge_labeled_intervals(\n    ...      ref_intervals, ref_labels, est_intervals, est_labels)\n    >>> durations = mir_eval.util.intervals_to_durations(intervals)\n    >>> comparisons = mir_eval.chord.majmin(ref_labels, est_labels)\n    >>> score = mir_eval.chord.weighted_accuracy(comparisons, durations)\n\n    Parameters\n    ----------\n    reference_labels : list, len=n\n        Reference chord labels to score against.\n    estimated_labels : list, len=n\n        Estimated chord labels to score against.\n\n    Returns\n    -------\n    comparison_scores : np.ndarray, shape=(n,), dtype=float\n        Comparison scores, in [0.0, 1.0], or -1 if the comparison is out of\n        gamut.\n\n    \"\"\"\n    validate(reference_labels, estimated_labels)\n    maj_semitones = np.array(QUALITIES['maj'][:8])\n    min_semitones = np.array(QUALITIES['min'][:8])\n\n    ref_roots, ref_semitones, _ = encode_many(reference_labels, False)\n    est_roots, est_semitones, _ = encode_many(estimated_labels, False)\n\n    eq_root = ref_roots == est_roots\n    eq_quality = np.all(np.equal(ref_semitones[:, :8],\n                                 est_semitones[:, :8]), axis=1)\n    comparison_scores = (eq_root * eq_quality).astype(np.float)\n\n    # Test for Major / Minor / No-chord\n    is_maj = np.all(np.equal(ref_semitones[:, :8], maj_semitones), axis=1)\n    is_min = np.all(np.equal(ref_semitones[:, :8], min_semitones), axis=1)\n    is_none = np.logical_and(ref_roots < 0, np.all(ref_semitones == 0, axis=1))\n\n    # Only keep majors, minors, and Nones (NOR)\n    comparison_scores[(is_maj + is_min + is_none) == 0] = -1\n\n    # Disable chords that disrupt this quality (apparently)\n    # ref_voicing = np.all(np.equal(ref_qualities[:, :8],\n    #                               ref_notes[:, :8]), axis=1)\n    # comparison_scores[ref_voicing == 0] = -1\n    # est_voicing = np.all(np.equal(est_qualities[:, :8],\n    #                               est_notes[:, :8]), axis=1)\n    # comparison_scores[est_voicing == 0] = -1\n    return comparison_scores"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompares chords along major - minor rules with inversions.", "response": "def majmin_inv(reference_labels, estimated_labels):\n    \"\"\"Compare chords along major-minor rules, with inversions. Chords with\n    qualities outside Major/minor/no-chord are ignored, and the bass note must\n    exist in the triad (bass in [1, 3, 5]).\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> est_intervals, est_labels = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, ref_intervals.min(),\n    ...     ref_intervals.max(), mir_eval.chord.NO_CHORD,\n    ...     mir_eval.chord.NO_CHORD)\n    >>> (intervals,\n    ...  ref_labels,\n    ...  est_labels) = mir_eval.util.merge_labeled_intervals(\n    ...      ref_intervals, ref_labels, est_intervals, est_labels)\n    >>> durations = mir_eval.util.intervals_to_durations(intervals)\n    >>> comparisons = mir_eval.chord.majmin_inv(ref_labels, est_labels)\n    >>> score = mir_eval.chord.weighted_accuracy(comparisons, durations)\n\n    Parameters\n    ----------\n    reference_labels : list, len=n\n        Reference chord labels to score against.\n    estimated_labels : list, len=n\n        Estimated chord labels to score against.\n\n    Returns\n    -------\n    comparison_scores : np.ndarray, shape=(n,), dtype=float\n        Comparison scores, in [0.0, 1.0], or -1 if the comparison is out of\n        gamut.\n\n    \"\"\"\n    validate(reference_labels, estimated_labels)\n    maj_semitones = np.array(QUALITIES['maj'][:8])\n    min_semitones = np.array(QUALITIES['min'][:8])\n\n    ref_roots, ref_semitones, ref_bass = encode_many(reference_labels, False)\n    est_roots, est_semitones, est_bass = encode_many(estimated_labels, False)\n\n    eq_root_bass = (ref_roots == est_roots) * (ref_bass == est_bass)\n    eq_semitones = np.all(np.equal(ref_semitones[:, :8],\n                                   est_semitones[:, :8]), axis=1)\n    comparison_scores = (eq_root_bass * eq_semitones).astype(np.float)\n\n    # Test for Major / Minor / No-chord\n    is_maj = np.all(np.equal(ref_semitones[:, :8], maj_semitones), axis=1)\n    is_min = np.all(np.equal(ref_semitones[:, :8], min_semitones), axis=1)\n    is_none = np.logical_and(ref_roots < 0, np.all(ref_semitones == 0, axis=1))\n\n    # Only keep majors, minors, and Nones (NOR)\n    comparison_scores[(is_maj + is_min + is_none) == 0] = -1\n\n    # Disable inversions that are not part of the quality\n    valid_inversion = np.ones(ref_bass.shape, dtype=bool)\n    bass_idx = ref_bass >= 0\n    valid_inversion[bass_idx] = ref_semitones[bass_idx, ref_bass[bass_idx]]\n    comparison_scores[valid_inversion == 0] = -1\n    return comparison_scores"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sevenths(reference_labels, estimated_labels):\n    validate(reference_labels, estimated_labels)\n    seventh_qualities = ['maj', 'min', 'maj7', '7', 'min7', '']\n    valid_semitones = np.array([QUALITIES[name] for name in seventh_qualities])\n\n    ref_roots, ref_semitones = encode_many(reference_labels, False)[:2]\n    est_roots, est_semitones = encode_many(estimated_labels, False)[:2]\n\n    eq_root = ref_roots == est_roots\n    eq_semitones = np.all(np.equal(ref_semitones, est_semitones), axis=1)\n    comparison_scores = (eq_root * eq_semitones).astype(np.float)\n\n    # Test for reference chord inclusion\n    is_valid = np.array([np.all(np.equal(ref_semitones, semitones), axis=1)\n                         for semitones in valid_semitones])\n    # Drop if NOR\n    comparison_scores[np.sum(is_valid, axis=0) == 0] = -1\n    return comparison_scores", "response": "Compare chords along MIREX sevenths rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sevenths_inv(reference_labels, estimated_labels):\n    validate(reference_labels, estimated_labels)\n    seventh_qualities = ['maj', 'min', 'maj7', '7', 'min7', '']\n    valid_semitones = np.array([QUALITIES[name] for name in seventh_qualities])\n\n    ref_roots, ref_semitones, ref_basses = encode_many(reference_labels, False)\n    est_roots, est_semitones, est_basses = encode_many(estimated_labels, False)\n\n    eq_roots_basses = (ref_roots == est_roots) * (ref_basses == est_basses)\n    eq_semitones = np.all(np.equal(ref_semitones, est_semitones), axis=1)\n    comparison_scores = (eq_roots_basses * eq_semitones).astype(np.float)\n\n    # Test for Major / Minor / No-chord\n    is_valid = np.array([np.all(np.equal(ref_semitones, semitones), axis=1)\n                         for semitones in valid_semitones])\n    comparison_scores[np.sum(is_valid, axis=0) == 0] = -1\n\n    # Disable inversions that are not part of the quality\n    valid_inversion = np.ones(ref_basses.shape, dtype=bool)\n    bass_idx = ref_basses >= 0\n    valid_inversion[bass_idx] = ref_semitones[bass_idx, ref_basses[bass_idx]]\n    comparison_scores[valid_inversion == 0] = -1\n    return comparison_scores", "response": "Compare chords along MIREX sevenths rules."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the directional hamming distance between reference and estimated intervals as defined by [ #harte2010towards_ and used for MIREX OverSeg UnderSeg and MeanSeg measures.", "response": "def directional_hamming_distance(reference_intervals, estimated_intervals):\n    \"\"\"Compute the directional hamming distance between reference and\n    estimated intervals as defined by [#harte2010towards]_ and used for MIREX\n    'OverSeg', 'UnderSeg' and 'MeanSeg' measures.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> overseg = 1 - mir_eval.chord.directional_hamming_distance(\n    ...     ref_intervals, est_intervals)\n    >>> underseg = 1 - mir_eval.chord.directional_hamming_distance(\n    ...     est_intervals, ref_intervals)\n    >>> seg = min(overseg, underseg)\n\n    Parameters\n    ----------\n    reference_intervals : np.ndarray, shape=(n, 2), dtype=float\n        Reference chord intervals to score against.\n    estimated_intervals : np.ndarray, shape=(m, 2), dtype=float\n        Estimated chord intervals to score against.\n\n    Returns\n    -------\n    directional hamming distance : float\n        directional hamming distance between reference intervals and\n        estimated intervals.\n    \"\"\"\n    util.validate_intervals(estimated_intervals)\n    util.validate_intervals(reference_intervals)\n\n    # make sure chord intervals do not overlap\n    if len(reference_intervals) > 1 and (reference_intervals[:-1, 1] >\n                                         reference_intervals[1:, 0]).any():\n        raise ValueError('Chord Intervals must not overlap')\n\n    est_ts = np.unique(estimated_intervals.flatten())\n    seg = 0.\n    for start, end in reference_intervals:\n        dur = end - start\n        between_start_end = est_ts[(est_ts >= start) & (est_ts < end)]\n        seg_ts = np.hstack([start, between_start_end, end])\n        seg += dur - np.diff(seg_ts).max()\n    return seg / (reference_intervals[-1, 1] - reference_intervals[0, 0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the MIREX MeanSeg score.", "response": "def seg(reference_intervals, estimated_intervals):\n    \"\"\"Compute the MIREX 'MeanSeg' score.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> score = mir_eval.chord.seg(ref_intervals, est_intervals)\n\n    Parameters\n    ----------\n    reference_intervals : np.ndarray, shape=(n, 2), dtype=float\n        Reference chord intervals to score against.\n    estimated_intervals : np.ndarray, shape=(m, 2), dtype=float\n        Estimated chord intervals to score against.\n\n    Returns\n    -------\n    segmentation score : float\n        Comparison score, in [0.0, 1.0], where 1.0 means perfect segmentation.\n    \"\"\"\n\n    return min(underseg(reference_intervals, estimated_intervals),\n               overseg(reference_intervals, estimated_intervals))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmerges consecutive chord intervals if they represent the same chord.", "response": "def merge_chord_intervals(intervals, labels):\n    \"\"\"\n    Merge consecutive chord intervals if they represent the same chord.\n\n    Parameters\n    ----------\n    intervals : np.ndarray, shape=(n, 2), dtype=float\n        Chord intervals to be merged, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    labels : list, shape=(n,)\n        Chord labels to be merged, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    Returns\n    -------\n    merged_ivs : np.ndarray, shape=(k, 2), dtype=float\n        Merged chord intervals, k <= n\n\n    \"\"\"\n    roots, semitones, basses = encode_many(labels, True)\n    merged_ivs = []\n    prev_rt = None\n    prev_st = None\n    prev_ba = None\n    for s, e, rt, st, ba in zip(intervals[:, 0], intervals[:, 1],\n                                roots, semitones, basses):\n        if rt != prev_rt or (st != prev_st).any() or ba != prev_ba:\n            prev_rt, prev_st, prev_ba = rt, st, ba\n            merged_ivs.append([s, e])\n        else:\n            merged_ivs[-1][-1] = e\n    return np.array(merged_ivs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate the weighted accuracy for all comparison functions for the given reference and estimated annotations.", "response": "def evaluate(ref_intervals, ref_labels, est_intervals, est_labels, **kwargs):\n    \"\"\"Computes weighted accuracy for all comparison functions for the given\n    reference and estimated annotations.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> scores = mir_eval.chord.evaluate(ref_intervals, ref_labels,\n    ...                                  est_intervals, est_labels)\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n, 2)\n        Reference chord intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    ref_labels : list, shape=(n,)\n        reference chord labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    est_intervals : np.ndarray, shape=(m, 2)\n        estimated chord intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    est_labels : list, shape=(m,)\n        estimated chord labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    kwargs\n        Additional keyword arguments which will be passed to the\n        appropriate metric or preprocessing functions.\n\n    Returns\n    -------\n    scores : dict\n        Dictionary of scores, where the key is the metric name (str) and\n        the value is the (float) score achieved.\n\n    \"\"\"\n    # Append or crop estimated intervals so their span is the same as reference\n    est_intervals, est_labels = util.adjust_intervals(\n        est_intervals, est_labels, ref_intervals.min(), ref_intervals.max(),\n        NO_CHORD, NO_CHORD)\n    # use merged intervals for segmentation evaluation\n    merged_ref_intervals = merge_chord_intervals(ref_intervals, ref_labels)\n    merged_est_intervals = merge_chord_intervals(est_intervals, est_labels)\n    # Adjust the labels so that they span the same intervals\n    intervals, ref_labels, est_labels = util.merge_labeled_intervals(\n        ref_intervals, ref_labels, est_intervals, est_labels)\n    # Convert intervals to durations (used as weights)\n    durations = util.intervals_to_durations(intervals)\n\n    # Store scores for each comparison function\n    scores = collections.OrderedDict()\n\n    scores['thirds'] = weighted_accuracy(thirds(ref_labels, est_labels),\n                                         durations)\n    scores['thirds_inv'] = weighted_accuracy(thirds_inv(ref_labels,\n                                                        est_labels), durations)\n    scores['triads'] = weighted_accuracy(triads(ref_labels, est_labels),\n                                         durations)\n    scores['triads_inv'] = weighted_accuracy(triads_inv(ref_labels,\n                                                        est_labels), durations)\n    scores['tetrads'] = weighted_accuracy(tetrads(ref_labels, est_labels),\n                                          durations)\n    scores['tetrads_inv'] = weighted_accuracy(tetrads_inv(ref_labels,\n                                                          est_labels),\n                                              durations)\n    scores['root'] = weighted_accuracy(root(ref_labels, est_labels), durations)\n    scores['mirex'] = weighted_accuracy(mirex(ref_labels, est_labels),\n                                        durations)\n    scores['majmin'] = weighted_accuracy(majmin(ref_labels, est_labels),\n                                         durations)\n    scores['majmin_inv'] = weighted_accuracy(majmin_inv(ref_labels,\n                                                        est_labels), durations)\n    scores['sevenths'] = weighted_accuracy(sevenths(ref_labels, est_labels),\n                                           durations)\n    scores['sevenths_inv'] = weighted_accuracy(sevenths_inv(ref_labels,\n                                                            est_labels),\n                                               durations)\n    scores['underseg'] = underseg(merged_ref_intervals, merged_est_intervals)\n    scores['overseg'] = overseg(merged_ref_intervals, merged_est_intervals)\n    scores['seg'] = min(scores['overseg'], scores['underseg'])\n\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the number of onset_midi objects in a pattern.", "response": "def _n_onset_midi(patterns):\n    \"\"\"Computes the number of onset_midi objects in a pattern\n\n    Parameters\n    ----------\n    patterns :\n        A list of patterns using the format returned by\n        :func:`mir_eval.io.load_patterns()`\n\n    Returns\n    -------\n    n_onsets : int\n        Number of onsets within the pattern.\n\n    \"\"\"\n    return len([o_m for pat in patterns for occ in pat for o_m in occ])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(reference_patterns, estimated_patterns):\n    # Warn if pattern lists are empty\n    if _n_onset_midi(reference_patterns) == 0:\n        warnings.warn('Reference patterns are empty.')\n    if _n_onset_midi(estimated_patterns) == 0:\n        warnings.warn('Estimated patterns are empty.')\n    for patterns in [reference_patterns, estimated_patterns]:\n        for pattern in patterns:\n            if len(pattern) <= 0:\n                raise ValueError(\"Each pattern must contain at least one \"\n                                 \"occurrence.\")\n            for occurrence in pattern:\n                for onset_midi in occurrence:\n                    if len(onset_midi) != 2:\n                        raise ValueError(\"The (onset, midi) tuple must \"\n                                         \"contain exactly 2 elements.\")", "response": "Checks that the input annotations to a metric look like valid pattern\n    lists and throws helpful errors if not."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _occurrence_intersection(occ_P, occ_Q):\n    set_P = set([tuple(onset_midi) for onset_midi in occ_P])\n    set_Q = set([tuple(onset_midi) for onset_midi in occ_Q])\n    return set_P & set_Q", "response": "Computes the intersection between two occurrences."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_score_matrix(P, Q, similarity_metric=\"cardinality_score\"):\n    sm = np.zeros((len(P), len(Q)))     # The score matrix\n    for iP, occ_P in enumerate(P):\n        for iQ, occ_Q in enumerate(Q):\n            if similarity_metric == \"cardinality_score\":\n                denom = float(np.max([len(occ_P), len(occ_Q)]))\n                # Compute the score\n                sm[iP, iQ] = len(_occurrence_intersection(occ_P, occ_Q)) / \\\n                    denom\n            # TODO: More scores: 'normalised matching socre'\n            else:\n                raise ValueError(\"The similarity metric (%s) can only be: \"\n                                 \"'cardinality_score'.\")\n    return sm", "response": "Compute the score matrix between the patterns P and Q."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef standard_FPR(reference_patterns, estimated_patterns, tol=1e-5):\n    validate(reference_patterns, estimated_patterns)\n    nP = len(reference_patterns)    # Number of patterns in the reference\n    nQ = len(estimated_patterns)    # Number of patterns in the estimation\n    k = 0                           # Number of patterns that match\n\n    # If no patterns were provided, metric is zero\n    if _n_onset_midi(reference_patterns) == 0 or \\\n       _n_onset_midi(estimated_patterns) == 0:\n        return 0., 0., 0.\n\n    # Find matches of the prototype patterns\n    for ref_pattern in reference_patterns:\n        P = np.asarray(ref_pattern[0])      # Get reference prototype\n        for est_pattern in estimated_patterns:\n            Q = np.asarray(est_pattern[0])  # Get estimation prototype\n\n            if len(P) != len(Q):\n                continue\n\n            # Check transposition given a certain tolerance\n            if (len(P) == len(Q) == 1 or\n                    np.max(np.abs(np.diff(P - Q, axis=0))) < tol):\n                k += 1\n                break\n\n    # Compute the standard measures\n    precision = k / float(nQ)\n    recall = k / float(nP)\n    f_measure = util.f_measure(precision, recall)\n    return f_measure, precision, recall", "response": "Standard F1 Score Precision and Recall. This metric checks if the prototype patterns of the reference match possible translated patterns in the estimations of the reference."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef establishment_FPR(reference_patterns, estimated_patterns,\n                      similarity_metric=\"cardinality_score\"):\n    \"\"\"Establishment F1 Score, Precision and Recall.\n\n    Examples\n    --------\n    >>> ref_patterns = mir_eval.io.load_patterns(\"ref_pattern.txt\")\n    >>> est_patterns = mir_eval.io.load_patterns(\"est_pattern.txt\")\n    >>> F, P, R = mir_eval.pattern.establishment_FPR(ref_patterns,\n    ...                                              est_patterns)\n\n\n    Parameters\n    ----------\n    reference_patterns : list\n        The reference patterns in the format returned by\n        :func:`mir_eval.io.load_patterns()`\n\n    estimated_patterns : list\n        The estimated patterns in the same format\n\n    similarity_metric : str\n        A string representing the metric to be used when computing the\n        similarity matrix. Accepted values:\n\n            - \"cardinality_score\": Count of the intersection\n              between occurrences.\n\n        (Default value = \"cardinality_score\")\n\n\n    Returns\n    -------\n    f_measure : float\n        The establishment F1 Score\n    precision : float\n        The establishment Precision\n    recall : float\n        The establishment Recall\n\n    \"\"\"\n    validate(reference_patterns, estimated_patterns)\n    nP = len(reference_patterns)    # Number of elements in reference\n    nQ = len(estimated_patterns)    # Number of elements in estimation\n    S = np.zeros((nP, nQ))          # Establishment matrix\n\n    # If no patterns were provided, metric is zero\n    if _n_onset_midi(reference_patterns) == 0 or \\\n       _n_onset_midi(estimated_patterns) == 0:\n        return 0., 0., 0.\n\n    for iP, ref_pattern in enumerate(reference_patterns):\n        for iQ, est_pattern in enumerate(estimated_patterns):\n            s = _compute_score_matrix(ref_pattern, est_pattern,\n                                      similarity_metric)\n            S[iP, iQ] = np.max(s)\n\n    # Compute scores\n    precision = np.mean(np.max(S, axis=0))\n    recall = np.mean(np.max(S, axis=1))\n    f_measure = util.f_measure(precision, recall)\n    return f_measure, precision, recall", "response": "This function calculates the F1 Score Precision and Recall for a set of reference and estimated patterns."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef first_n_three_layer_P(reference_patterns, estimated_patterns, n=5):\n\n    validate(reference_patterns, estimated_patterns)\n    # If no patterns were provided, metric is zero\n    if _n_onset_midi(reference_patterns) == 0 or \\\n       _n_onset_midi(estimated_patterns) == 0:\n        return 0., 0., 0.\n\n    # Get only the first n patterns from the estimated results\n    fn_est_patterns = estimated_patterns[:min(len(estimated_patterns), n)]\n\n    # Compute the three-layer scores for the first n estimated patterns\n    F, P, R = three_layer_FPR(reference_patterns, fn_est_patterns)\n\n    return P", "response": "This function computes the first n three - layer precision for a given set of reference patterns and estimated patterns."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef evaluate(ref_patterns, est_patterns, **kwargs):\n\n    # Compute all the metrics\n    scores = collections.OrderedDict()\n\n    # Standard scores\n    scores['F'], scores['P'], scores['R'] = \\\n        util.filter_kwargs(standard_FPR, ref_patterns, est_patterns, **kwargs)\n\n    # Establishment scores\n    scores['F_est'], scores['P_est'], scores['R_est'] = \\\n        util.filter_kwargs(establishment_FPR, ref_patterns, est_patterns,\n                           **kwargs)\n\n    # Occurrence scores\n    # Force these values for thresh\n    kwargs['thresh'] = .5\n    scores['F_occ.5'], scores['P_occ.5'], scores['R_occ.5'] = \\\n        util.filter_kwargs(occurrence_FPR, ref_patterns, est_patterns,\n                           **kwargs)\n    kwargs['thresh'] = .75\n    scores['F_occ.75'], scores['P_occ.75'], scores['R_occ.75'] = \\\n        util.filter_kwargs(occurrence_FPR, ref_patterns, est_patterns,\n                           **kwargs)\n\n    # Three-layer scores\n    scores['F_3'], scores['P_3'], scores['R_3'] = \\\n        util.filter_kwargs(three_layer_FPR, ref_patterns, est_patterns,\n                           **kwargs)\n\n    # First Five Patterns scores\n    # Set default value of n\n    if 'n' not in kwargs:\n        kwargs['n'] = 5\n    scores['FFP'] = util.filter_kwargs(first_n_three_layer_P, ref_patterns,\n                                       est_patterns, **kwargs)\n    scores['FFTP_est'] = \\\n        util.filter_kwargs(first_n_target_proportion_R, ref_patterns,\n                           est_patterns, **kwargs)\n\n    return scores", "response": "Load data and perform the evaluation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(ref_intervals, ref_pitches, ref_velocities, est_intervals,\n             est_pitches, est_velocities):\n    \"\"\"Checks that the input annotations have valid time intervals, pitches,\n    and velocities, and throws helpful errors if not.\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    ref_pitches : np.ndarray, shape=(n,)\n        Array of reference pitch values in Hertz\n    ref_velocities : np.ndarray, shape=(n,)\n        Array of MIDI velocities (i.e. between 0 and 127) of reference notes\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    est_pitches : np.ndarray, shape=(m,)\n        Array of estimated pitch values in Hertz\n    est_velocities : np.ndarray, shape=(m,)\n        Array of MIDI velocities (i.e. between 0 and 127) of estimated notes\n    \"\"\"\n    transcription.validate(ref_intervals, ref_pitches, est_intervals,\n                           est_pitches)\n    # Check that velocities have the same length as intervals/pitches\n    if not ref_velocities.shape[0] == ref_pitches.shape[0]:\n        raise ValueError('Reference velocities must have the same length as '\n                         'pitches and intervals.')\n    if not est_velocities.shape[0] == est_pitches.shape[0]:\n        raise ValueError('Estimated velocities must have the same length as '\n                         'pitches and intervals.')\n    # Check that the velocities are positive\n    if ref_velocities.size > 0 and np.min(ref_velocities) < 0:\n        raise ValueError('Reference velocities must be positive.')\n    if est_velocities.size > 0 and np.min(est_velocities) < 0:\n        raise ValueError('Estimated velocities must be positive.')", "response": "Validates that the input annotations have valid time intervals pitches and velocities and throws helpful errors if not."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmatching notes for the given set of reference notes and estimated notes.", "response": "def match_notes(\n        ref_intervals, ref_pitches, ref_velocities, est_intervals, est_pitches,\n        est_velocities, onset_tolerance=0.05, pitch_tolerance=50.0,\n        offset_ratio=0.2, offset_min_tolerance=0.05, strict=False,\n        velocity_tolerance=0.1):\n    \"\"\"Match notes, taking note velocity into consideration.\n\n    This function first calls :func:`mir_eval.transcription.match_notes` to\n    match notes according to the supplied intervals, pitches, onset, offset,\n    and pitch tolerances. The velocities of the matched notes are then used to\n    estimate a slope and intercept which can rescale the estimated velocities\n    so that they are as close as possible (in L2 sense) to their matched\n    reference velocities. Velocities are then normalized to the range [0, 1]. A\n    estimated note is then further only considered correct if its velocity is\n    within ``velocity_tolerance`` of its matched (according to pitch and\n    timing) reference note.\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    ref_pitches : np.ndarray, shape=(n,)\n        Array of reference pitch values in Hertz\n    ref_velocities : np.ndarray, shape=(n,)\n        Array of MIDI velocities (i.e. between 0 and 127) of reference notes\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    est_pitches : np.ndarray, shape=(m,)\n        Array of estimated pitch values in Hertz\n    est_velocities : np.ndarray, shape=(m,)\n        Array of MIDI velocities (i.e. between 0 and 127) of estimated notes\n    onset_tolerance : float > 0\n        The tolerance for an estimated note's onset deviating from the\n        reference note's onset, in seconds. Default is 0.05 (50 ms).\n    pitch_tolerance : float > 0\n        The tolerance for an estimated note's pitch deviating from the\n        reference note's pitch, in cents. Default is 50.0 (50 cents).\n    offset_ratio : float > 0 or None\n        The ratio of the reference note's duration used to define the\n        offset_tolerance. Default is 0.2 (20%), meaning the\n        ``offset_tolerance`` will equal the ``ref_duration * 0.2``, or 0.05 (50\n        ms), whichever is greater. If ``offset_ratio`` is set to ``None``,\n        offsets are ignored in the matching.\n    offset_min_tolerance : float > 0\n        The minimum tolerance for offset matching. See offset_ratio description\n        for an explanation of how the offset tolerance is determined. Note:\n        this parameter only influences the results if ``offset_ratio`` is not\n        ``None``.\n    strict : bool\n        If ``strict=False`` (the default), threshold checks for onset, offset,\n        and pitch matching are performed using ``<=`` (less than or equal). If\n        ``strict=True``, the threshold checks are performed using ``<`` (less\n        than).\n    velocity_tolerance : float > 0\n        Estimated notes are considered correct if, after rescaling and\n        normalization to [0, 1], they are within ``velocity_tolerance`` of a\n        matched reference note.\n\n    Returns\n    -------\n    matching : list of tuples\n        A list of matched reference and estimated notes.\n        ``matching[i] == (i, j)`` where reference note ``i`` matches estimated\n        note ``j``.\n    \"\"\"\n    # Compute note matching as usual using standard transcription function\n    matching = transcription.match_notes(\n        ref_intervals, ref_pitches, est_intervals, est_pitches,\n        onset_tolerance, pitch_tolerance, offset_ratio, offset_min_tolerance,\n        strict)\n\n    # Rescale reference velocities to the range [0, 1]\n    min_velocity, max_velocity = np.min(ref_velocities), np.max(ref_velocities)\n    # Make the smallest possible range 1 to avoid divide by zero\n    velocity_range = max(1, max_velocity - min_velocity)\n    ref_velocities = (ref_velocities - min_velocity)/float(velocity_range)\n\n    # Convert matching list-of-tuples to array for fancy indexing\n    matching = np.array(matching)\n    # When there is no matching, return an empty list\n    if matching.size == 0:\n        return []\n    # Grab velocities for matched notes\n    ref_matched_velocities = ref_velocities[matching[:, 0]]\n    est_matched_velocities = est_velocities[matching[:, 1]]\n    # Find slope and intercept of line which produces best least-squares fit\n    # between matched est and ref velocities\n    slope, intercept = np.linalg.lstsq(\n        np.vstack([est_matched_velocities,\n                   np.ones(len(est_matched_velocities))]).T,\n        ref_matched_velocities)[0]\n    # Re-scale est velocities to match ref\n    est_matched_velocities = slope*est_matched_velocities + intercept\n    # Compute the absolute error of (rescaled) estimated velocities vs.\n    # normalized reference velocities. Error will be in [0, 1]\n    velocity_diff = np.abs(est_matched_velocities - ref_matched_velocities)\n    # Check whether each error is within the provided tolerance\n    velocity_within_tolerance = (velocity_diff < velocity_tolerance)\n    # Only keep matches whose velocity was within the provided tolerance\n    matching = matching[velocity_within_tolerance]\n    # Convert back to list-of-tuple format\n    matching = [tuple(_) for _ in matching]\n\n    return matching"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef evaluate(ref_intervals, ref_pitches, ref_velocities, est_intervals,\n             est_pitches, est_velocities, **kwargs):\n    \"\"\"Compute all metrics for the given reference and estimated annotations.\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    ref_pitches : np.ndarray, shape=(n,)\n        Array of reference pitch values in Hertz\n    ref_velocities : np.ndarray, shape=(n,)\n        Array of MIDI velocities (i.e. between 0 and 127) of reference notes\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    est_pitches : np.ndarray, shape=(m,)\n        Array of estimated pitch values in Hertz\n    est_velocities : np.ndarray, shape=(n,)\n        Array of MIDI velocities (i.e. between 0 and 127) of estimated notes\n    kwargs\n        Additional keyword arguments which will be passed to the\n        appropriate metric or preprocessing functions.\n\n    Returns\n    -------\n    scores : dict\n        Dictionary of scores, where the key is the metric name (str) and\n        the value is the (float) score achieved.\n    \"\"\"\n    # Compute all the metrics\n    scores = collections.OrderedDict()\n\n    # Precision, recall and f-measure taking note offsets into account\n    kwargs.setdefault('offset_ratio', 0.2)\n    if kwargs['offset_ratio'] is not None:\n        (scores['Precision'],\n         scores['Recall'],\n         scores['F-measure'],\n         scores['Average_Overlap_Ratio']) = util.filter_kwargs(\n             precision_recall_f1_overlap, ref_intervals, ref_pitches,\n             ref_velocities, est_intervals, est_pitches, est_velocities,\n             **kwargs)\n\n    # Precision, recall and f-measure NOT taking note offsets into account\n    kwargs['offset_ratio'] = None\n    (scores['Precision_no_offset'],\n     scores['Recall_no_offset'],\n     scores['F-measure_no_offset'],\n     scores['Average_Overlap_Ratio_no_offset']) = util.filter_kwargs(\n         precision_recall_f1_overlap, ref_intervals, ref_pitches,\n         ref_velocities, est_intervals, est_pitches, est_velocities, **kwargs)\n\n    return scores", "response": "Compute all metrics for the given reference and estimated annotations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that the input annotations to a metric look like valid beat time arrays and throws helpful errors if not.", "response": "def validate(reference_beats, estimated_beats):\n    \"\"\"Checks that the input annotations to a metric look like valid beat time\n    arrays, and throws helpful errors if not.\n\n    Parameters\n    ----------\n    reference_beats : np.ndarray\n        reference beat times, in seconds\n    estimated_beats : np.ndarray\n        estimated beat times, in seconds\n    \"\"\"\n    # If reference or estimated beats are empty,\n    # warn because metric will be 0\n    if reference_beats.size == 0:\n        warnings.warn(\"Reference beats are empty.\")\n    if estimated_beats.size == 0:\n        warnings.warn(\"Estimated beats are empty.\")\n    for beats in [reference_beats, estimated_beats]:\n        util.validate_events(beats, MAX_TIME)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the metric variations of the reference beats in the base base", "response": "def _get_reference_beat_variations(reference_beats):\n    \"\"\"Return metric variations of the reference beats\n\n    Parameters\n    ----------\n    reference_beats : np.ndarray\n        beat locations in seconds\n\n    Returns\n    -------\n    reference_beats : np.ndarray\n        Original beat locations\n    off_beat : np.ndarray\n        180 degrees out of phase from the original beat locations\n    double : np.ndarray\n        Beats at 2x the original tempo\n    half_odd : np.ndarray\n        Half tempo, odd beats\n    half_even : np.ndarray\n        Half tempo, even beats\n\n    \"\"\"\n\n    # Create annotations at twice the metric level\n    interpolated_indices = np.arange(0, reference_beats.shape[0]-.5, .5)\n    original_indices = np.arange(0, reference_beats.shape[0])\n    double_reference_beats = np.interp(interpolated_indices,\n                                       original_indices,\n                                       reference_beats)\n    # Return metric variations:\n    # True, off-beat, double tempo, half tempo odd, and half tempo even\n    return (reference_beats,\n            double_reference_beats[1::2],\n            double_reference_beats,\n            reference_beats[::2],\n            reference_beats[1::2])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef f_measure(reference_beats,\n              estimated_beats,\n              f_measure_threshold=0.07):\n    \"\"\"Compute the F-measure of correct vs incorrectly predicted beats.\n    \"Correctness\" is determined over a small window.\n\n    Examples\n    --------\n    >>> reference_beats = mir_eval.io.load_events('reference.txt')\n    >>> reference_beats = mir_eval.beat.trim_beats(reference_beats)\n    >>> estimated_beats = mir_eval.io.load_events('estimated.txt')\n    >>> estimated_beats = mir_eval.beat.trim_beats(estimated_beats)\n    >>> f_measure = mir_eval.beat.f_measure(reference_beats,\n                                            estimated_beats)\n\n    Parameters\n    ----------\n    reference_beats : np.ndarray\n        reference beat times, in seconds\n    estimated_beats : np.ndarray\n        estimated beat times, in seconds\n    f_measure_threshold : float\n        Window size, in seconds\n        (Default value = 0.07)\n\n    Returns\n    -------\n    f_score : float\n        The computed F-measure score\n\n    \"\"\"\n    validate(reference_beats, estimated_beats)\n    # When estimated beats are empty, no beats are correct; metric is 0\n    if estimated_beats.size == 0 or reference_beats.size == 0:\n        return 0.\n    # Compute the best-case matching between reference and estimated locations\n    matching = util.match_events(reference_beats,\n                                 estimated_beats,\n                                 f_measure_threshold)\n\n    precision = float(len(matching))/len(estimated_beats)\n    recall = float(len(matching))/len(reference_beats)\n    return util.f_measure(precision, recall)", "response": "Compute the F - measure of correct vs incorrectly predicted beats. Correctness is determined over a small window."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cemgil(reference_beats,\n           estimated_beats,\n           cemgil_sigma=0.04):\n    \"\"\"Cemgil's score, computes a gaussian error of each estimated beat.\n    Compares against the original beat times and all metrical variations.\n\n    Examples\n    --------\n    >>> reference_beats = mir_eval.io.load_events('reference.txt')\n    >>> reference_beats = mir_eval.beat.trim_beats(reference_beats)\n    >>> estimated_beats = mir_eval.io.load_events('estimated.txt')\n    >>> estimated_beats = mir_eval.beat.trim_beats(estimated_beats)\n    >>> cemgil_score, cemgil_max = mir_eval.beat.cemgil(reference_beats,\n                                                        estimated_beats)\n\n    Parameters\n    ----------\n    reference_beats : np.ndarray\n        reference beat times, in seconds\n    estimated_beats : np.ndarray\n        query beat times, in seconds\n    cemgil_sigma : float\n        Sigma parameter of gaussian error windows\n        (Default value = 0.04)\n\n    Returns\n    -------\n    cemgil_score : float\n        Cemgil's score for the original reference beats\n    cemgil_max : float\n        The best Cemgil score for all metrical variations\n    \"\"\"\n    validate(reference_beats, estimated_beats)\n    # When estimated beats are empty, no beats are correct; metric is 0\n    if estimated_beats.size == 0 or reference_beats.size == 0:\n        return 0., 0.\n    # We'll compute Cemgil's accuracy for each variation\n    accuracies = []\n    for reference_beats in _get_reference_beat_variations(reference_beats):\n        accuracy = 0\n        # Cycle through beats\n        for beat in reference_beats:\n            # Find the error for the closest beat to the reference beat\n            beat_diff = np.min(np.abs(beat - estimated_beats))\n            # Add gaussian error into the accuracy\n            accuracy += np.exp(-(beat_diff**2)/(2.0*cemgil_sigma**2))\n        # Normalize the accuracy\n        accuracy /= .5*(estimated_beats.shape[0] + reference_beats.shape[0])\n        # Add it to our list of accuracy scores\n        accuracies.append(accuracy)\n    # Return raw accuracy with non-varied annotations\n    # and maximal accuracy across all variations\n    return accuracies[0], np.max(accuracies)", "response": "Cemgil s score computes a gaussian error of each estimated beat."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating Goto s score a binary 1 or 0 depending on some specific heuristic criteria", "response": "def goto(reference_beats,\n         estimated_beats,\n         goto_threshold=0.35,\n         goto_mu=0.2,\n         goto_sigma=0.2):\n    \"\"\"Calculate Goto's score, a binary 1 or 0 depending on some specific\n    heuristic criteria\n\n    Examples\n    --------\n    >>> reference_beats = mir_eval.io.load_events('reference.txt')\n    >>> reference_beats = mir_eval.beat.trim_beats(reference_beats)\n    >>> estimated_beats = mir_eval.io.load_events('estimated.txt')\n    >>> estimated_beats = mir_eval.beat.trim_beats(estimated_beats)\n    >>> goto_score = mir_eval.beat.goto(reference_beats, estimated_beats)\n\n    Parameters\n    ----------\n    reference_beats : np.ndarray\n        reference beat times, in seconds\n    estimated_beats : np.ndarray\n        query beat times, in seconds\n    goto_threshold : float\n        Threshold of beat error for a beat to be \"correct\"\n        (Default value = 0.35)\n    goto_mu : float\n        The mean of the beat errors in the continuously correct\n        track must be less than this\n        (Default value = 0.2)\n    goto_sigma : float\n        The std of the beat errors in the continuously correct track must\n        be less than this\n        (Default value = 0.2)\n\n    Returns\n    -------\n    goto_score : float\n        Either 1.0 or 0.0 if some specific criteria are met\n    \"\"\"\n    validate(reference_beats, estimated_beats)\n    # When estimated beats are empty, no beats are correct; metric is 0\n    if estimated_beats.size == 0 or reference_beats.size == 0:\n        return 0.\n    # Error for each beat\n    beat_error = np.ones(reference_beats.shape[0])\n    # Flag for whether the reference and estimated beats are paired\n    paired = np.zeros(reference_beats.shape[0])\n    # Keep track of Goto's three criteria\n    goto_criteria = 0\n    for n in range(1, reference_beats.shape[0]-1):\n        # Get previous inner-reference-beat-interval\n        previous_interval = 0.5*(reference_beats[n] - reference_beats[n-1])\n        # Window start - in the middle of the current beat and the previous\n        window_min = reference_beats[n] - previous_interval\n        # Next inter-reference-beat-interval\n        next_interval = 0.5*(reference_beats[n+1] - reference_beats[n])\n        # Window end - in the middle of the current beat and the next\n        window_max = reference_beats[n] + next_interval\n        # Get estimated beats in the window\n        beats_in_window = np.logical_and((estimated_beats >= window_min),\n                                         (estimated_beats < window_max))\n        # False negative/positive\n        if beats_in_window.sum() == 0 or beats_in_window.sum() > 1:\n            paired[n] = 0\n            beat_error[n] = 1\n        else:\n            # Single beat is paired!\n            paired[n] = 1\n            # Get offset of the estimated beat and the reference beat\n            offset = estimated_beats[beats_in_window] - reference_beats[n]\n            # Scale by previous or next interval\n            if offset < 0:\n                beat_error[n] = offset/previous_interval\n            else:\n                beat_error[n] = offset/next_interval\n    # Get indices of incorrect beats\n    incorrect_beats = np.flatnonzero(np.abs(beat_error) > goto_threshold)\n    # All beats are correct (first and last will be 0 so always correct)\n    if incorrect_beats.shape[0] < 3:\n        # Get the track of correct beats\n        track = beat_error[incorrect_beats[0] + 1:incorrect_beats[-1] - 1]\n        goto_criteria = 1\n    else:\n        # Get the track of maximal length\n        track_len = np.max(np.diff(incorrect_beats))\n        track_start = np.flatnonzero(np.diff(incorrect_beats) == track_len)[0]\n        # Is the track length at least 25% of the song?\n        if track_len - 1 > .25*(reference_beats.shape[0] - 2):\n            goto_criteria = 1\n            start_beat = incorrect_beats[track_start]\n            end_beat = incorrect_beats[track_start + 1]\n            track = beat_error[start_beat:end_beat + 1]\n    # If we have a track\n    if goto_criteria:\n        # Are mean and std of the track less than the required thresholds?\n        if np.mean(np.abs(track)) < goto_mu \\\n           and np.std(track, ddof=1) < goto_sigma:\n            goto_criteria = 3\n    # If all criteria are met, score is 100%!\n    return 1.0*(goto_criteria == 3)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef p_score(reference_beats,\n            estimated_beats,\n            p_score_threshold=0.2):\n    \"\"\"Get McKinney's P-score.\n    Based on the autocorrelation of the reference and estimated beats\n\n    Examples\n    --------\n    >>> reference_beats = mir_eval.io.load_events('reference.txt')\n    >>> reference_beats = mir_eval.beat.trim_beats(reference_beats)\n    >>> estimated_beats = mir_eval.io.load_events('estimated.txt')\n    >>> estimated_beats = mir_eval.beat.trim_beats(estimated_beats)\n    >>> p_score = mir_eval.beat.p_score(reference_beats, estimated_beats)\n\n    Parameters\n    ----------\n    reference_beats : np.ndarray\n        reference beat times, in seconds\n    estimated_beats : np.ndarray\n        query beat times, in seconds\n    p_score_threshold : float\n        Window size will be\n        ``p_score_threshold*np.median(inter_annotation_intervals)``,\n        (Default value = 0.2)\n\n    Returns\n    -------\n    correlation : float\n        McKinney's P-score\n\n    \"\"\"\n    validate(reference_beats, estimated_beats)\n    # Warn when only one beat is provided for either estimated or reference,\n    # report a warning\n    if reference_beats.size == 1:\n        warnings.warn(\"Only one reference beat was provided, so beat intervals\"\n                      \" cannot be computed.\")\n    if estimated_beats.size == 1:\n        warnings.warn(\"Only one estimated beat was provided, so beat intervals\"\n                      \" cannot be computed.\")\n    # When estimated or reference beats have <= 1 beats, can't compute the\n    # metric, so return 0\n    if estimated_beats.size <= 1 or reference_beats.size <= 1:\n        return 0.\n    # Quantize beats to 10ms\n    sampling_rate = int(1.0/0.010)\n    # Shift beats so that the minimum in either sequence is zero\n    offset = min(estimated_beats.min(), reference_beats.min())\n    estimated_beats = np.array(estimated_beats - offset)\n    reference_beats = np.array(reference_beats - offset)\n    # Get the largest time index\n    end_point = np.int(np.ceil(np.max([np.max(estimated_beats),\n                                       np.max(reference_beats)])))\n    # Make impulse trains with impulses at beat locations\n    reference_train = np.zeros(end_point*sampling_rate + 1)\n    beat_indices = np.ceil(reference_beats*sampling_rate).astype(np.int)\n    reference_train[beat_indices] = 1.0\n    estimated_train = np.zeros(end_point*sampling_rate + 1)\n    beat_indices = np.ceil(estimated_beats*sampling_rate).astype(np.int)\n    estimated_train[beat_indices] = 1.0\n    # Window size to take the correlation over\n    # defined as .2*median(inter-annotation-intervals)\n    annotation_intervals = np.diff(np.flatnonzero(reference_train))\n    win_size = int(np.round(p_score_threshold*np.median(annotation_intervals)))\n    # Get full correlation\n    train_correlation = np.correlate(reference_train, estimated_train, 'full')\n    # Get the middle element - note we are rounding down on purpose here\n    middle_lag = train_correlation.shape[0]//2\n    # Truncate to only valid lags (those corresponding to the window)\n    start = middle_lag - win_size\n    end = middle_lag + win_size + 1\n    train_correlation = train_correlation[start:end]\n    # Compute and return the P-score\n    n_beats = np.max([estimated_beats.shape[0], reference_beats.shape[0]])\n    return np.sum(train_correlation)/n_beats", "response": "Compute the P - score of the given reference and estimated beats."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef continuity(reference_beats,\n               estimated_beats,\n               continuity_phase_threshold=0.175,\n               continuity_period_threshold=0.175):\n    \"\"\"Get metrics based on how much of the estimated beat sequence is\n    continually correct.\n\n    Examples\n    --------\n    >>> reference_beats = mir_eval.io.load_events('reference.txt')\n    >>> reference_beats = mir_eval.beat.trim_beats(reference_beats)\n    >>> estimated_beats = mir_eval.io.load_events('estimated.txt')\n    >>> estimated_beats = mir_eval.beat.trim_beats(estimated_beats)\n    >>> CMLc, CMLt, AMLc, AMLt = mir_eval.beat.continuity(reference_beats,\n                                                          estimated_beats)\n\n    Parameters\n    ----------\n    reference_beats : np.ndarray\n        reference beat times, in seconds\n    estimated_beats : np.ndarray\n        query beat times, in seconds\n    continuity_phase_threshold : float\n        Allowable ratio of how far is the estimated beat\n        can be from the reference beat\n        (Default value = 0.175)\n    continuity_period_threshold : float\n        Allowable distance between the inter-beat-interval\n        and the inter-annotation-interval\n        (Default value = 0.175)\n\n    Returns\n    -------\n    CMLc : float\n        Correct metric level, continuous accuracy\n    CMLt : float\n        Correct metric level, total accuracy (continuity not required)\n    AMLc : float\n        Any metric level, continuous accuracy\n    AMLt : float\n        Any metric level, total accuracy (continuity not required)\n    \"\"\"\n    validate(reference_beats, estimated_beats)\n    # Warn when only one beat is provided for either estimated or reference,\n    # report a warning\n    if reference_beats.size == 1:\n        warnings.warn(\"Only one reference beat was provided, so beat intervals\"\n                      \" cannot be computed.\")\n    if estimated_beats.size == 1:\n        warnings.warn(\"Only one estimated beat was provided, so beat intervals\"\n                      \" cannot be computed.\")\n    # When estimated or reference beats have <= 1 beats, can't compute the\n    # metric, so return 0\n    if estimated_beats.size <= 1 or reference_beats.size <= 1:\n        return 0., 0., 0., 0.\n    # Accuracies for each variation\n    continuous_accuracies = []\n    total_accuracies = []\n    # Get accuracy for each variation\n    for reference_beats in _get_reference_beat_variations(reference_beats):\n        # Annotations that have been used\n        n_annotations = np.max([reference_beats.shape[0],\n                               estimated_beats.shape[0]])\n        used_annotations = np.zeros(n_annotations)\n        # Whether or not we are continuous at any given point\n        beat_successes = np.zeros(n_annotations)\n        for m in range(estimated_beats.shape[0]):\n            # Is this beat correct?\n            beat_success = 0\n            # Get differences for this beat\n            beat_differences = np.abs(estimated_beats[m] - reference_beats)\n            # Get nearest annotation index\n            nearest = np.argmin(beat_differences)\n            min_difference = beat_differences[nearest]\n            # Have we already used this annotation?\n            if used_annotations[nearest] == 0:\n                # Is this the first beat or first annotation?\n                # If so, look forward.\n                if m == 0 or nearest == 0:\n                    # How far is the estimated beat from the reference beat,\n                    # relative to the inter-annotation-interval?\n                    if nearest + 1 < reference_beats.shape[0]:\n                        reference_interval = (reference_beats[nearest + 1] -\n                                              reference_beats[nearest])\n                    else:\n                        # Special case when nearest + 1 is too large - use the\n                        # previous interval instead\n                        reference_interval = (reference_beats[nearest] -\n                                              reference_beats[nearest - 1])\n                    # Handle this special case when beats are not unique\n                    if reference_interval == 0:\n                        if min_difference == 0:\n                            phase = 1\n                        else:\n                            phase = np.inf\n                    else:\n                        phase = np.abs(min_difference/reference_interval)\n                    # How close is the inter-beat-interval\n                    # to the inter-annotation-interval?\n                    if m + 1 < estimated_beats.shape[0]:\n                        estimated_interval = (estimated_beats[m + 1] -\n                                              estimated_beats[m])\n                    else:\n                        # Special case when m + 1 is too large - use the\n                        # previous interval\n                        estimated_interval = (estimated_beats[m] -\n                                              estimated_beats[m - 1])\n                    # Handle this special case when beats are not unique\n                    if reference_interval == 0:\n                        if estimated_interval == 0:\n                            period = 0\n                        else:\n                            period = np.inf\n                    else:\n                        period = \\\n                            np.abs(1 - estimated_interval/reference_interval)\n                    if phase < continuity_phase_threshold and \\\n                       period < continuity_period_threshold:\n                        # Set this annotation as used\n                        used_annotations[nearest] = 1\n                        # This beat is matched\n                        beat_success = 1\n                # This beat/annotation is not the first\n                else:\n                    # How far is the estimated beat from the reference beat,\n                    # relative to the inter-annotation-interval?\n                    reference_interval = (reference_beats[nearest] -\n                                          reference_beats[nearest - 1])\n                    phase = np.abs(min_difference/reference_interval)\n                    # How close is the inter-beat-interval\n                    # to the inter-annotation-interval?\n                    estimated_interval = (estimated_beats[m] -\n                                          estimated_beats[m - 1])\n                    reference_interval = (reference_beats[nearest] -\n                                          reference_beats[nearest - 1])\n                    period = np.abs(1 - estimated_interval/reference_interval)\n                    if phase < continuity_phase_threshold and \\\n                       period < continuity_period_threshold:\n                        # Set this annotation as used\n                        used_annotations[nearest] = 1\n                        # This beat is matched\n                        beat_success = 1\n            # Set whether this beat is matched or not\n            beat_successes[m] = beat_success\n        # Add 0s at the begnning and end\n        # so that we at least find the beginning/end of the estimated beats\n        beat_successes = np.append(np.append(0, beat_successes), 0)\n        # Where is the beat not a match?\n        beat_failures = np.nonzero(beat_successes == 0)[0]\n        # Take out those zeros we added\n        beat_successes = beat_successes[1:-1]\n        # Get the continuous accuracy as the longest track of successful beats\n        longest_track = np.max(np.diff(beat_failures)) - 1\n        continuous_accuracy = longest_track/(1.0*beat_successes.shape[0])\n        continuous_accuracies.append(continuous_accuracy)\n        # Get the total accuracy - all sequences\n        total_accuracy = np.sum(beat_successes)/(1.0*beat_successes.shape[0])\n        total_accuracies.append(total_accuracy)\n    # Grab accuracy scores\n    return (continuous_accuracies[0],\n            total_accuracies[0],\n            np.max(continuous_accuracies),\n            np.max(total_accuracies))", "response": "Get metrics based on how much of the estimated beat sequence is continually correct."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the information gain - K - L divergence of the beat error histogram containing the estimated and reference beats.", "response": "def information_gain(reference_beats,\n                     estimated_beats,\n                     bins=41):\n    \"\"\"Get the information gain - K-L divergence of the beat error histogram\n    to a uniform histogram\n\n    Examples\n    --------\n    >>> reference_beats = mir_eval.io.load_events('reference.txt')\n    >>> reference_beats = mir_eval.beat.trim_beats(reference_beats)\n    >>> estimated_beats = mir_eval.io.load_events('estimated.txt')\n    >>> estimated_beats = mir_eval.beat.trim_beats(estimated_beats)\n    >>> information_gain = mir_eval.beat.information_gain(reference_beats,\n                                                          estimated_beats)\n\n    Parameters\n    ----------\n    reference_beats : np.ndarray\n        reference beat times, in seconds\n    estimated_beats : np.ndarray\n        query beat times, in seconds\n    bins : int\n        Number of bins in the beat error histogram\n        (Default value = 41)\n\n    Returns\n    -------\n    information_gain_score : float\n        Entropy of beat error histogram\n    \"\"\"\n    validate(reference_beats, estimated_beats)\n    # If an even number of bins is provided,\n    # there will be no bin centered at zero, so warn the user.\n    if not bins % 2:\n        warnings.warn(\"bins parameter is even, \"\n                      \"so there will not be a bin centered at zero.\")\n    # Warn when only one beat is provided for either estimated or reference,\n    # report a warning\n    if reference_beats.size == 1:\n        warnings.warn(\"Only one reference beat was provided, so beat intervals\"\n                      \" cannot be computed.\")\n    if estimated_beats.size == 1:\n        warnings.warn(\"Only one estimated beat was provided, so beat intervals\"\n                      \" cannot be computed.\")\n    # When estimated or reference beats have <= 1 beats, can't compute the\n    # metric, so return 0\n    if estimated_beats.size <= 1 or reference_beats.size <= 1:\n        return 0.\n    # Get entropy for reference beats->estimated beats\n    # and estimated beats->reference beats\n    forward_entropy = _get_entropy(reference_beats, estimated_beats, bins)\n    backward_entropy = _get_entropy(estimated_beats, reference_beats, bins)\n    # Pick the larger of the entropies\n    norm = np.log2(bins)\n    if forward_entropy > backward_entropy:\n        # Note that the beat evaluation toolbox does not normalize\n        information_gain_score = (norm - forward_entropy)/norm\n    else:\n        information_gain_score = (norm - backward_entropy)/norm\n    return information_gain_score"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_entropy(reference_beats, estimated_beats, bins):\n    beat_error = np.zeros(estimated_beats.shape[0])\n    for n in range(estimated_beats.shape[0]):\n        # Get index of closest annotation to this beat\n        beat_distances = estimated_beats[n] - reference_beats\n        closest_beat = np.argmin(np.abs(beat_distances))\n        absolute_error = beat_distances[closest_beat]\n        # If the first annotation is closest...\n        if closest_beat == 0:\n            # Inter-annotation interval - space between first two beats\n            interval = .5*(reference_beats[1] - reference_beats[0])\n        # If last annotation is closest...\n        if closest_beat == (reference_beats.shape[0] - 1):\n            interval = .5*(reference_beats[-1] - reference_beats[-2])\n        else:\n            if absolute_error < 0:\n                # Closest annotation is the one before the current beat\n                # so look at previous inner-annotation-interval\n                start = reference_beats[closest_beat]\n                end = reference_beats[closest_beat - 1]\n                interval = .5*(start - end)\n            else:\n                # Closest annotation is the one after the current beat\n                # so look at next inner-annotation-interval\n                start = reference_beats[closest_beat + 1]\n                end = reference_beats[closest_beat]\n                interval = .5*(start - end)\n        # The actual error of this beat\n        beat_error[n] = .5*absolute_error/interval\n    # Put beat errors in range (-.5, .5)\n    beat_error = np.mod(beat_error + .5, -1) + .5\n    # Note these are slightly different the beat evaluation toolbox\n    # (they are uniform)\n    histogram_bin_edges = np.linspace(-.5, .5, bins + 1)\n    # Get the histogram\n    raw_bin_values = np.histogram(beat_error, histogram_bin_edges)[0]\n    # Turn into a proper probability distribution\n    raw_bin_values = raw_bin_values/(1.0*np.sum(raw_bin_values))\n    # Set zero-valued bins to 1 to make the entropy calculation well-behaved\n    raw_bin_values[raw_bin_values == 0] = 1\n    # Calculate entropy\n    return -np.sum(raw_bin_values * np.log2(raw_bin_values))", "response": "Helper function for information gain\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute all metrics for the given reference and estimated annotations.", "response": "def evaluate(reference_beats, estimated_beats, **kwargs):\n    \"\"\"Compute all metrics for the given reference and estimated annotations.\n\n    Examples\n    --------\n    >>> reference_beats = mir_eval.io.load_events('reference.txt')\n    >>> estimated_beats = mir_eval.io.load_events('estimated.txt')\n    >>> scores = mir_eval.beat.evaluate(reference_beats, estimated_beats)\n\n    Parameters\n    ----------\n    reference_beats : np.ndarray\n        Reference beat times, in seconds\n    estimated_beats : np.ndarray\n        Query beat times, in seconds\n    kwargs\n        Additional keyword arguments which will be passed to the\n        appropriate metric or preprocessing functions.\n\n    Returns\n    -------\n    scores : dict\n        Dictionary of scores, where the key is the metric name (str) and\n        the value is the (float) score achieved.\n\n    \"\"\"\n\n    # Trim beat times at the beginning of the annotations\n    reference_beats = util.filter_kwargs(trim_beats, reference_beats, **kwargs)\n    estimated_beats = util.filter_kwargs(trim_beats, estimated_beats, **kwargs)\n\n    # Now compute all the metrics\n\n    scores = collections.OrderedDict()\n\n    # F-Measure\n    scores['F-measure'] = util.filter_kwargs(f_measure, reference_beats,\n                                             estimated_beats, **kwargs)\n\n    # Cemgil\n    scores['Cemgil'], scores['Cemgil Best Metric Level'] = \\\n        util.filter_kwargs(cemgil, reference_beats, estimated_beats, **kwargs)\n\n    # Goto\n    scores['Goto'] = util.filter_kwargs(goto, reference_beats,\n                                        estimated_beats, **kwargs)\n\n    # P-Score\n    scores['P-score'] = util.filter_kwargs(p_score, reference_beats,\n                                           estimated_beats, **kwargs)\n\n    # Continuity metrics\n    (scores['Correct Metric Level Continuous'],\n     scores['Correct Metric Level Total'],\n     scores['Any Metric Level Continuous'],\n     scores['Any Metric Level Total']) = util.filter_kwargs(continuity,\n                                                            reference_beats,\n                                                            estimated_beats,\n                                                            **kwargs)\n\n    # Information gain\n    scores['Information gain'] = util.filter_kwargs(information_gain,\n                                                    reference_beats,\n                                                    estimated_beats,\n                                                    **kwargs)\n\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a list of string identifiers into numerical indices.", "response": "def index_labels(labels, case_sensitive=False):\n    \"\"\"Convert a list of string identifiers into numerical indices.\n\n    Parameters\n    ----------\n    labels : list of strings, shape=(n,)\n        A list of annotations, e.g., segment or chord labels from an\n        annotation file.\n\n    case_sensitive : bool\n        Set to True to enable case-sensitive label indexing\n        (Default value = False)\n\n    Returns\n    -------\n    indices : list, shape=(n,)\n        Numerical representation of ``labels``\n    index_to_label : dict\n        Mapping to convert numerical indices back to labels.\n        ``labels[i] == index_to_label[indices[i]]``\n\n    \"\"\"\n\n    label_to_index = {}\n    index_to_label = {}\n\n    # If we're not case-sensitive,\n    if not case_sensitive:\n        labels = [str(s).lower() for s in labels]\n\n    # First, build the unique label mapping\n    for index, s in enumerate(sorted(set(labels))):\n        label_to_index[s] = index\n        index_to_label[index] = s\n\n    # Remap the labels to indices\n    indices = [label_to_index[s] for s in labels]\n\n    # Return the converted labels, and the inverse mapping\n    return indices, index_to_label"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_labels(items, prefix='__'):\n    return ['{}{}'.format(prefix, n) for n in range(len(items))]", "response": "Given an array of items generate a synthetic label for each item in the list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts an array of labeled time intervals to annotated samples.", "response": "def intervals_to_samples(intervals, labels, offset=0, sample_size=0.1,\n                         fill_value=None):\n    \"\"\"Convert an array of labeled time intervals to annotated samples.\n\n    Parameters\n    ----------\n    intervals : np.ndarray, shape=(n, d)\n        An array of time intervals, as returned by\n        :func:`mir_eval.io.load_intervals()` or\n        :func:`mir_eval.io.load_labeled_intervals()`.\n        The ``i`` th interval spans time ``intervals[i, 0]`` to\n        ``intervals[i, 1]``.\n\n    labels : list, shape=(n,)\n        The annotation for each interval\n\n    offset : float > 0\n        Phase offset of the sampled time grid (in seconds)\n        (Default value = 0)\n\n    sample_size : float > 0\n        duration of each sample to be generated (in seconds)\n        (Default value = 0.1)\n\n    fill_value : type(labels[0])\n        Object to use for the label with out-of-range time points.\n        (Default value = None)\n\n    Returns\n    -------\n    sample_times : list\n        list of sample times\n\n    sample_labels : list\n        array of labels for each generated sample\n\n    Notes\n    -----\n        Intervals will be rounded down to the nearest multiple\n        of ``sample_size``.\n\n    \"\"\"\n\n    # Round intervals to the sample size\n    num_samples = int(np.floor(intervals.max() / sample_size))\n    sample_indices = np.arange(num_samples, dtype=np.float32)\n    sample_times = (sample_indices*sample_size + offset).tolist()\n    sampled_labels = interpolate_intervals(\n        intervals, labels, sample_times, fill_value)\n\n    return sample_times, sampled_labels"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interpolate_intervals(intervals, labels, time_points, fill_value=None):\n\n    # Verify that time_points is sorted\n    time_points = np.asarray(time_points)\n\n    if np.any(time_points[1:] < time_points[:-1]):\n        raise ValueError('time_points must be in non-decreasing order')\n\n    aligned_labels = [fill_value] * len(time_points)\n\n    starts = np.searchsorted(time_points, intervals[:, 0], side='left')\n    ends = np.searchsorted(time_points, intervals[:, 1], side='right')\n\n    for (start, end, lab) in zip(starts, ends, labels):\n        aligned_labels[start:end] = [lab] * (end - start)\n\n    return aligned_labels", "response": "Assign labels to a set of time points given a set of intervals."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sort_labeled_intervals(intervals, labels=None):\n    '''Sort intervals, and optionally, their corresponding labels\n    according to start time.\n\n    Parameters\n    ----------\n    intervals : np.ndarray, shape=(n, 2)\n        The input intervals\n\n    labels : list, optional\n        Labels for each interval\n\n    Returns\n    -------\n    intervals_sorted or (intervals_sorted, labels_sorted)\n        Labels are only returned if provided as input\n    '''\n\n    idx = np.argsort(intervals[:, 0])\n\n    intervals_sorted = intervals[idx]\n\n    if labels is None:\n        return intervals_sorted\n    else:\n        return intervals_sorted, [labels[_] for _ in idx]", "response": "Sort intervals and optionally their corresponding labels\n    according to start time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef f_measure(precision, recall, beta=1.0):\n\n    if precision == 0 and recall == 0:\n        return 0.0\n\n    return (1 + beta**2)*precision*recall/((beta**2)*precision + recall)", "response": "Compute the f - measure from precision and recall scores."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef intervals_to_boundaries(intervals, q=5):\n\n    return np.unique(np.ravel(np.round(intervals, decimals=q)))", "response": "Convert interval times into boundaries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert an array of event times into intervals.", "response": "def boundaries_to_intervals(boundaries):\n    \"\"\"Convert an array of event times into intervals\n\n    Parameters\n    ----------\n    boundaries : list-like\n        List-like of event times.  These are assumed to be unique\n        timestamps in ascending order.\n\n    Returns\n    -------\n    intervals : np.ndarray, shape=(n_intervals, 2)\n        Start and end time for each interval\n    \"\"\"\n\n    if not np.allclose(boundaries, np.unique(boundaries)):\n        raise ValueError('Boundary times are not unique or not ascending.')\n\n    intervals = np.asarray(list(zip(boundaries[:-1], boundaries[1:])))\n\n    return intervals"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadjust a list of time intervals to span the range t_min t_max.", "response": "def adjust_intervals(intervals,\n                     labels=None,\n                     t_min=0.0,\n                     t_max=None,\n                     start_label='__T_MIN',\n                     end_label='__T_MAX'):\n    \"\"\"Adjust a list of time intervals to span the range ``[t_min, t_max]``.\n\n    Any intervals lying completely outside the specified range will be removed.\n\n    Any intervals lying partially outside the specified range will be cropped.\n\n    If the specified range exceeds the span of the provided data in either\n    direction, additional intervals will be appended.  If an interval is\n    appended at the beginning, it will be given the label ``start_label``; if\n    an interval is appended at the end, it will be given the label\n    ``end_label``.\n\n    Parameters\n    ----------\n    intervals : np.ndarray, shape=(n_events, 2)\n        Array of interval start and end-times\n    labels : list, len=n_events or None\n        List of labels\n        (Default value = None)\n    t_min : float or None\n        Minimum interval start time.\n        (Default value = 0.0)\n    t_max : float or None\n        Maximum interval end time.\n        (Default value = None)\n    start_label : str or float or int\n        Label to give any intervals appended at the beginning\n        (Default value = '__T_MIN')\n    end_label : str or float or int\n        Label to give any intervals appended at the end\n        (Default value = '__T_MAX')\n\n    Returns\n    -------\n    new_intervals : np.ndarray\n        Intervals spanning ``[t_min, t_max]``\n    new_labels : list\n        List of labels for ``new_labels``\n\n    \"\"\"\n\n    # When supplied intervals are empty and t_max and t_min are supplied,\n    # create one interval from t_min to t_max with the label start_label\n    if t_min is not None and t_max is not None and intervals.size == 0:\n        return np.array([[t_min, t_max]]), [start_label]\n    # When intervals are empty and either t_min or t_max are not supplied,\n    # we can't append new intervals\n    elif (t_min is None or t_max is None) and intervals.size == 0:\n        raise ValueError(\"Supplied intervals are empty, can't append new\"\n                         \" intervals\")\n\n    if t_min is not None:\n        # Find the intervals that end at or after t_min\n        first_idx = np.argwhere(intervals[:, 1] >= t_min)\n\n        if len(first_idx) > 0:\n            # If we have events below t_min, crop them out\n            if labels is not None:\n                labels = labels[int(first_idx[0]):]\n            # Clip to the range (t_min, +inf)\n            intervals = intervals[int(first_idx[0]):]\n        intervals = np.maximum(t_min, intervals)\n\n        if intervals.min() > t_min:\n            # Lowest boundary is higher than t_min:\n            # add a new boundary and label\n            intervals = np.vstack(([t_min, intervals.min()], intervals))\n            if labels is not None:\n                labels.insert(0, start_label)\n\n    if t_max is not None:\n        # Find the intervals that begin after t_max\n        last_idx = np.argwhere(intervals[:, 0] > t_max)\n\n        if len(last_idx) > 0:\n            # We have boundaries above t_max.\n            # Trim to only boundaries <= t_max\n            if labels is not None:\n                labels = labels[:int(last_idx[0])]\n            # Clip to the range (-inf, t_max)\n            intervals = intervals[:int(last_idx[0])]\n\n        intervals = np.minimum(t_max, intervals)\n\n        if intervals.max() < t_max:\n            # Last boundary is below t_max: add a new boundary and label\n            intervals = np.vstack((intervals, [intervals.max(), t_max]))\n            if labels is not None:\n                labels.append(end_label)\n\n    return intervals, labels"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef adjust_events(events, labels=None, t_min=0.0,\n                  t_max=None, label_prefix='__'):\n    \"\"\"Adjust the given list of event times to span the range\n    ``[t_min, t_max]``.\n\n    Any event times outside of the specified range will be removed.\n\n    If the times do not span ``[t_min, t_max]``, additional events will be\n    added with the prefix ``label_prefix``.\n\n    Parameters\n    ----------\n    events : np.ndarray\n        Array of event times (seconds)\n    labels : list or None\n        List of labels\n        (Default value = None)\n    t_min : float or None\n        Minimum valid event time.\n        (Default value = 0.0)\n    t_max : float or None\n        Maximum valid event time.\n        (Default value = None)\n    label_prefix : str\n        Prefix string to use for synthetic labels\n        (Default value = '__')\n\n    Returns\n    -------\n    new_times : np.ndarray\n        Event times corrected to the given range.\n\n    \"\"\"\n    if t_min is not None:\n        first_idx = np.argwhere(events >= t_min)\n\n        if len(first_idx) > 0:\n            # We have events below t_min\n            # Crop them out\n            if labels is not None:\n                labels = labels[int(first_idx[0]):]\n            events = events[int(first_idx[0]):]\n\n        if events[0] > t_min:\n            # Lowest boundary is higher than t_min:\n            # add a new boundary and label\n            events = np.concatenate(([t_min], events))\n            if labels is not None:\n                labels.insert(0, '%sT_MIN' % label_prefix)\n\n    if t_max is not None:\n        last_idx = np.argwhere(events > t_max)\n\n        if len(last_idx) > 0:\n            # We have boundaries above t_max.\n            # Trim to only boundaries <= t_max\n            if labels is not None:\n                labels = labels[:int(last_idx[0])]\n            events = events[:int(last_idx[0])]\n\n        if events[-1] < t_max:\n            # Last boundary is below t_max: add a new boundary and label\n            events = np.concatenate((events, [t_max]))\n            if labels is not None:\n                labels.append('%sT_MAX' % label_prefix)\n\n    return events, labels", "response": "Adjust the given list of event times to span the range t_min and t_max."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the intersection of two set of filepaths.", "response": "def intersect_files(flist1, flist2):\n    \"\"\"Return the intersection of two sets of filepaths, based on the file name\n    (after the final '/') and ignoring the file extension.\n\n    Examples\n    --------\n     >>> flist1 = ['/a/b/abc.lab', '/c/d/123.lab', '/e/f/xyz.lab']\n     >>> flist2 = ['/g/h/xyz.npy', '/i/j/123.txt', '/k/l/456.lab']\n     >>> sublist1, sublist2 = mir_eval.util.intersect_files(flist1, flist2)\n     >>> print sublist1\n     ['/e/f/xyz.lab', '/c/d/123.lab']\n     >>> print sublist2\n     ['/g/h/xyz.npy', '/i/j/123.txt']\n\n    Parameters\n    ----------\n    flist1 : list\n        first list of filepaths\n    flist2 : list\n        second list of filepaths\n\n    Returns\n    -------\n    sublist1 : list\n        subset of filepaths with matching stems from ``flist1``\n    sublist2 : list\n        corresponding filepaths from ``flist2``\n\n    \"\"\"\n    def fname(abs_path):\n        \"\"\"Returns the filename given an absolute path.\n\n        Parameters\n        ----------\n        abs_path :\n\n\n        Returns\n        -------\n\n        \"\"\"\n        return os.path.splitext(os.path.split(abs_path)[-1])[0]\n\n    fmap = dict([(fname(f), f) for f in flist1])\n    pairs = [list(), list()]\n    for f in flist2:\n        if fname(f) in fmap:\n            pairs[0].append(fmap[fname(f)])\n            pairs[1].append(f)\n\n    return pairs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind maximum cardinality matching of a bipartite graph.", "response": "def _bipartite_match(graph):\n    \"\"\"Find maximum cardinality matching of a bipartite graph (U,V,E).\n    The input format is a dictionary mapping members of U to a list\n    of their neighbors in V.\n\n    The output is a dict M mapping members of V to their matches in U.\n\n    Parameters\n    ----------\n    graph : dictionary : left-vertex -> list of right vertices\n        The input bipartite graph.  Each edge need only be specified once.\n\n    Returns\n    -------\n    matching : dictionary : right-vertex -> left vertex\n        A maximal bipartite matching.\n\n    \"\"\"\n    # Adapted from:\n    #\n    # Hopcroft-Karp bipartite max-cardinality matching and max independent set\n    # David Eppstein, UC Irvine, 27 Apr 2002\n\n    # initialize greedy matching (redundant, but faster than full search)\n    matching = {}\n    for u in graph:\n        for v in graph[u]:\n            if v not in matching:\n                matching[v] = u\n                break\n\n    while True:\n        # structure residual graph into layers\n        # pred[u] gives the neighbor in the previous layer for u in U\n        # preds[v] gives a list of neighbors in the previous layer for v in V\n        # unmatched gives a list of unmatched vertices in final layer of V,\n        # and is also used as a flag value for pred[u] when u is in the first\n        # layer\n        preds = {}\n        unmatched = []\n        pred = dict([(u, unmatched) for u in graph])\n        for v in matching:\n            del pred[matching[v]]\n        layer = list(pred)\n\n        # repeatedly extend layering structure by another pair of layers\n        while layer and not unmatched:\n            new_layer = {}\n            for u in layer:\n                for v in graph[u]:\n                    if v not in preds:\n                        new_layer.setdefault(v, []).append(u)\n            layer = []\n            for v in new_layer:\n                preds[v] = new_layer[v]\n                if v in matching:\n                    layer.append(matching[v])\n                    pred[matching[v]] = v\n                else:\n                    unmatched.append(v)\n\n        # did we finish layering without finding any alternating paths?\n        if not unmatched:\n            unlayered = {}\n            for u in graph:\n                for v in graph[u]:\n                    if v not in preds:\n                        unlayered[v] = None\n            return matching\n\n        def recurse(v):\n            \"\"\"Recursively search backward through layers to find alternating\n            paths.  recursion returns true if found path, false otherwise\n            \"\"\"\n            if v in preds:\n                L = preds[v]\n                del preds[v]\n                for u in L:\n                    if u in pred:\n                        pu = pred[u]\n                        del pred[u]\n                        if pu is unmatched or recurse(pu):\n                            matching[v] = u\n                            return True\n            return False\n\n        for v in unmatched:\n            recurse(v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _outer_distance_mod_n(ref, est, modulus=12):\n    ref_mod_n = np.mod(ref, modulus)\n    est_mod_n = np.mod(est, modulus)\n    abs_diff = np.abs(np.subtract.outer(ref_mod_n, est_mod_n))\n    return np.minimum(abs_diff, modulus - abs_diff)", "response": "Compute the absolute outer distance modulo n."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef match_events(ref, est, window, distance=None):\n    if distance is not None:\n        # Compute the indices of feasible pairings\n        hits = np.where(distance(ref, est) <= window)\n    else:\n        hits = _fast_hit_windows(ref, est, window)\n\n    # Construct the graph input\n    G = {}\n    for ref_i, est_i in zip(*hits):\n        if est_i not in G:\n            G[est_i] = []\n        G[est_i].append(ref_i)\n\n    # Compute the maximum matching\n    matching = sorted(_bipartite_match(G).items())\n\n    return matching", "response": "Compute a maximum matching between reference and estimated event times."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _fast_hit_windows(ref, est, window):\n    '''Fast calculation of windowed hits for time events.\n\n    Given two lists of event times ``ref`` and ``est``, and a\n    tolerance window, computes a list of pairings\n    ``(i, j)`` where ``|ref[i] - est[j]| <= window``.\n\n    This is equivalent to, but more efficient than the following:\n\n    >>> hit_ref, hit_est = np.where(np.abs(np.subtract.outer(ref, est))\n    ...                             <= window)\n\n    Parameters\n    ----------\n    ref : np.ndarray, shape=(n,)\n        Array of reference values\n    est : np.ndarray, shape=(m,)\n        Array of estimated values\n    window : float >= 0\n        Size of the tolerance window\n\n    Returns\n    -------\n    hit_ref : np.ndarray\n    hit_est : np.ndarray\n        indices such that ``|hit_ref[i] - hit_est[i]| <= window``\n    '''\n\n    ref = np.asarray(ref)\n    est = np.asarray(est)\n    ref_idx = np.argsort(ref)\n    ref_sorted = ref[ref_idx]\n\n    left_idx = np.searchsorted(ref_sorted, est - window, side='left')\n    right_idx = np.searchsorted(ref_sorted, est + window, side='right')\n\n    hit_ref, hit_est = [], []\n\n    for j, (start, end) in enumerate(zip(left_idx, right_idx)):\n        hit_ref.extend(ref_idx[start:end])\n        hit_est.extend([j] * (end - start))\n\n    return hit_ref, hit_est", "response": "Fast calculation of windowed hits for time events."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_intervals(intervals):\n\n    # Validate interval shape\n    if intervals.ndim != 2 or intervals.shape[1] != 2:\n        raise ValueError('Intervals should be n-by-2 numpy ndarray, '\n                         'but shape={}'.format(intervals.shape))\n\n    # Make sure no times are negative\n    if (intervals < 0).any():\n        raise ValueError('Negative interval times found')\n\n    # Make sure all intervals have strictly positive duration\n    if (intervals[:, 1] <= intervals[:, 0]).any():\n        raise ValueError('All interval durations must be strictly positive')", "response": "Checks that an interval ndarray is well - formed and raises errors\n    if not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_events(events, max_time=30000.):\n    # Make sure no event times are huge\n    if (events > max_time).any():\n        raise ValueError('An event at time {} was found which is greater than '\n                         'the maximum allowable time of max_time = {} (did you'\n                         ' supply event times in '\n                         'seconds?)'.format(events.max(), max_time))\n    # Make sure event locations are 1-d np ndarrays\n    if events.ndim != 1:\n        raise ValueError('Event times should be 1-d numpy ndarray, '\n                         'but shape={}'.format(events.shape))\n    # Make sure event times are increasing\n    if (np.diff(events) < 0).any():\n        raise ValueError('Events should be in increasing order.')", "response": "Validates that a 1 - d event location ndarray is well - formed and raises a ValueError if not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_frequencies(frequencies, max_freq, min_freq,\n                         allow_negatives=False):\n    \"\"\"Checks that a 1-d frequency ndarray is well-formed, and raises\n    errors if not.\n\n    Parameters\n    ----------\n    frequencies : np.ndarray, shape=(n,)\n        Array of frequency values\n    max_freq : float\n        If a frequency is found above this pitch, a ValueError will be raised.\n        (Default value = 5000.)\n    min_freq : float\n        If a frequency is found below this pitch, a ValueError will be raised.\n        (Default value = 20.)\n    allow_negatives : bool\n        Whether or not to allow negative frequency values.\n    \"\"\"\n    # If flag is true, map frequencies to their absolute value.\n    if allow_negatives:\n        frequencies = np.abs(frequencies)\n    # Make sure no frequency values are huge\n    if (np.abs(frequencies) > max_freq).any():\n        raise ValueError('A frequency of {} was found which is greater than '\n                         'the maximum allowable value of max_freq = {} (did '\n                         'you supply frequency values in '\n                         'Hz?)'.format(frequencies.max(), max_freq))\n    # Make sure no frequency values are tiny\n    if (np.abs(frequencies) < min_freq).any():\n        raise ValueError('A frequency of {} was found which is less than the '\n                         'minimum allowable value of min_freq = {} (did you '\n                         'supply frequency values in '\n                         'Hz?)'.format(frequencies.min(), min_freq))\n    # Make sure frequency values are 1-d np ndarrays\n    if frequencies.ndim != 1:\n        raise ValueError('Frequencies should be 1-d numpy ndarray, '\n                         'but shape={}'.format(frequencies.shape))", "response": "Validates that a 1 - d frequency array is well - formed and raises a ValueError if not."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_kwargs(function):\n    r'''Determine whether a function has \\*\\*kwargs.\n\n    Parameters\n    ----------\n    function : callable\n        The function to test\n\n    Returns\n    -------\n    True if function accepts arbitrary keyword arguments.\n    False otherwise.\n    '''\n\n    if six.PY2:\n        return inspect.getargspec(function).keywords is not None\n    else:\n        sig = inspect.signature(function)\n\n        for param in sig.parameters.values():\n            if param.kind == param.VAR_KEYWORD:\n                return True\n\n        return False", "response": "r Returns True if a function accepts arbitrary keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a function and args and keyword args to pass to it, call the function but using only the keyword arguments which it accepts. This is equivalent to redefining the function with an additional \\*\\*kwargs to accept slop keyword args. If the target function already accepts \\*\\*kwargs parameters, no filtering is performed. Parameters ---------- _function : callable Function to call. Can take in any number of args or kwargs", "response": "def filter_kwargs(_function, *args, **kwargs):\n    \"\"\"Given a function and args and keyword args to pass to it, call the function\n    but using only the keyword arguments which it accepts.  This is equivalent\n    to redefining the function with an additional \\*\\*kwargs to accept slop\n    keyword args.\n\n    If the target function already accepts \\*\\*kwargs parameters, no filtering\n    is performed.\n\n    Parameters\n    ----------\n    _function : callable\n        Function to call.  Can take in any number of args or kwargs\n\n    \"\"\"\n\n    if has_kwargs(_function):\n        return _function(*args, **kwargs)\n\n    # Get the list of function arguments\n    func_code = six.get_function_code(_function)\n    function_args = func_code.co_varnames[:func_code.co_argcount]\n    # Construct a dict of those kwargs which appear in the function\n    filtered_kwargs = {}\n    for kwarg, value in list(kwargs.items()):\n        if kwarg in function_args:\n            filtered_kwargs[kwarg] = value\n    # Call the function with the supplied args and the filtered kwarg dict\n    return _function(*args, **filtered_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef intervals_to_durations(intervals):\n    validate_intervals(intervals)\n    return np.abs(np.diff(intervals, axis=-1)).flatten()", "response": "Converts an array of n intervals to their n durations."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate that the input data to a metric are valid and throws helpful errors if not.", "response": "def validate(reference_sources, estimated_sources):\n    \"\"\"Checks that the input data to a metric are valid, and throws helpful\n    errors if not.\n\n    Parameters\n    ----------\n    reference_sources : np.ndarray, shape=(nsrc, nsampl)\n        matrix containing true sources\n    estimated_sources : np.ndarray, shape=(nsrc, nsampl)\n        matrix containing estimated sources\n\n    \"\"\"\n\n    if reference_sources.shape != estimated_sources.shape:\n        raise ValueError('The shape of estimated sources and the true '\n                         'sources should match.  reference_sources.shape '\n                         '= {}, estimated_sources.shape '\n                         '= {}'.format(reference_sources.shape,\n                                       estimated_sources.shape))\n\n    if reference_sources.ndim > 3 or estimated_sources.ndim > 3:\n        raise ValueError('The number of dimensions is too high (must be less '\n                         'than 3). reference_sources.ndim = {}, '\n                         'estimated_sources.ndim '\n                         '= {}'.format(reference_sources.ndim,\n                                       estimated_sources.ndim))\n\n    if reference_sources.size == 0:\n        warnings.warn(\"reference_sources is empty, should be of size \"\n                      \"(nsrc, nsample).  sdr, sir, sar, and perm will all \"\n                      \"be empty np.ndarrays\")\n    elif _any_source_silent(reference_sources):\n        raise ValueError('All the reference sources should be non-silent (not '\n                         'all-zeros), but at least one of the reference '\n                         'sources is all 0s, which introduces ambiguity to the'\n                         ' evaluation. (Otherwise we can add infinitely many '\n                         'all-zero sources.)')\n\n    if estimated_sources.size == 0:\n        warnings.warn(\"estimated_sources is empty, should be of size \"\n                      \"(nsrc, nsample).  sdr, sir, sar, and perm will all \"\n                      \"be empty np.ndarrays\")\n    elif _any_source_silent(estimated_sources):\n        raise ValueError('All the estimated sources should be non-silent (not '\n                         'all-zeros), but at least one of the estimated '\n                         'sources is all 0s. Since we require each reference '\n                         'source to be non-silent, having a silent estimated '\n                         'source will result in an underdetermined system.')\n\n    if (estimated_sources.shape[0] > MAX_SOURCES or\n            reference_sources.shape[0] > MAX_SOURCES):\n        raise ValueError('The supplied matrices should be of shape (nsrc,'\n                         ' nsampl) but reference_sources.shape[0] = {} and '\n                         'estimated_sources.shape[0] = {} which is greater '\n                         'than mir_eval.separation.MAX_SOURCES = {}.  To '\n                         'override this check, set '\n                         'mir_eval.separation.MAX_SOURCES to a '\n                         'larger value.'.format(reference_sources.shape[0],\n                                                estimated_sources.shape[0],\n                                                MAX_SOURCES))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _any_source_silent(sources):\n    return np.any(np.all(np.sum(\n        sources, axis=tuple(range(2, sources.ndim))) == 0, axis=1))", "response": "Returns true if the parameter sources have any silent first dimensions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bss_eval_sources(reference_sources, estimated_sources,\n                     compute_permutation=True):\n    \"\"\"\n    Ordering and measurement of the separation quality for estimated source\n    signals in terms of filtered true source, interference and artifacts.\n\n    The decomposition allows a time-invariant filter distortion of length\n    512, as described in Section III.B of [#vincent2006performance]_.\n\n    Passing ``False`` for ``compute_permutation`` will improve the computation\n    performance of the evaluation; however, it is not always appropriate and\n    is not the way that the BSS_EVAL Matlab toolbox computes bss_eval_sources.\n\n    Examples\n    --------\n    >>> # reference_sources[n] should be an ndarray of samples of the\n    >>> # n'th reference source\n    >>> # estimated_sources[n] should be the same for the n'th estimated\n    >>> # source\n    >>> (sdr, sir, sar,\n    ...  perm) = mir_eval.separation.bss_eval_sources(reference_sources,\n    ...                                               estimated_sources)\n\n    Parameters\n    ----------\n    reference_sources : np.ndarray, shape=(nsrc, nsampl)\n        matrix containing true sources (must have same shape as\n        estimated_sources)\n    estimated_sources : np.ndarray, shape=(nsrc, nsampl)\n        matrix containing estimated sources (must have same shape as\n        reference_sources)\n    compute_permutation : bool, optional\n        compute permutation of estimate/source combinations (True by default)\n\n    Returns\n    -------\n    sdr : np.ndarray, shape=(nsrc,)\n        vector of Signal to Distortion Ratios (SDR)\n    sir : np.ndarray, shape=(nsrc,)\n        vector of Source to Interference Ratios (SIR)\n    sar : np.ndarray, shape=(nsrc,)\n        vector of Sources to Artifacts Ratios (SAR)\n    perm : np.ndarray, shape=(nsrc,)\n        vector containing the best ordering of estimated sources in\n        the mean SIR sense (estimated source number ``perm[j]`` corresponds to\n        true source number ``j``). Note: ``perm`` will be ``[0, 1, ...,\n        nsrc-1]`` if ``compute_permutation`` is ``False``.\n\n    References\n    ----------\n    .. [#] Emmanuel Vincent, Shoko Araki, Fabian J. Theis, Guido Nolte, Pau\n        Bofill, Hiroshi Sawada, Alexey Ozerov, B. Vikrham Gowreesunker, Dominik\n        Lutter and Ngoc Q.K. Duong, \"The Signal Separation Evaluation Campaign\n        (2007-2010): Achievements and remaining challenges\", Signal Processing,\n        92, pp. 1928-1936, 2012.\n\n    \"\"\"\n\n    # make sure the input is of shape (nsrc, nsampl)\n    if estimated_sources.ndim == 1:\n        estimated_sources = estimated_sources[np.newaxis, :]\n    if reference_sources.ndim == 1:\n        reference_sources = reference_sources[np.newaxis, :]\n\n    validate(reference_sources, estimated_sources)\n    # If empty matrices were supplied, return empty lists (special case)\n    if reference_sources.size == 0 or estimated_sources.size == 0:\n        return np.array([]), np.array([]), np.array([]), np.array([])\n\n    nsrc = estimated_sources.shape[0]\n\n    # does user desire permutations?\n    if compute_permutation:\n        # compute criteria for all possible pair matches\n        sdr = np.empty((nsrc, nsrc))\n        sir = np.empty((nsrc, nsrc))\n        sar = np.empty((nsrc, nsrc))\n        for jest in range(nsrc):\n            for jtrue in range(nsrc):\n                s_true, e_spat, e_interf, e_artif = \\\n                    _bss_decomp_mtifilt(reference_sources,\n                                        estimated_sources[jest],\n                                        jtrue, 512)\n                sdr[jest, jtrue], sir[jest, jtrue], sar[jest, jtrue] = \\\n                    _bss_source_crit(s_true, e_spat, e_interf, e_artif)\n\n        # select the best ordering\n        perms = list(itertools.permutations(list(range(nsrc))))\n        mean_sir = np.empty(len(perms))\n        dum = np.arange(nsrc)\n        for (i, perm) in enumerate(perms):\n            mean_sir[i] = np.mean(sir[perm, dum])\n        popt = perms[np.argmax(mean_sir)]\n        idx = (popt, dum)\n        return (sdr[idx], sir[idx], sar[idx], np.asarray(popt))\n    else:\n        # compute criteria for only the simple correspondence\n        # (estimate 1 is estimate corresponding to reference source 1, etc.)\n        sdr = np.empty(nsrc)\n        sir = np.empty(nsrc)\n        sar = np.empty(nsrc)\n        for j in range(nsrc):\n            s_true, e_spat, e_interf, e_artif = \\\n                _bss_decomp_mtifilt(reference_sources,\n                                    estimated_sources[j],\n                                    j, 512)\n            sdr[j], sir[j], sar[j] = \\\n                _bss_source_crit(s_true, e_spat, e_interf, e_artif)\n\n        # return the default permutation for compatibility\n        popt = np.arange(nsrc)\n        return (sdr, sir, sar, popt)", "response": "This function computes the BSS_EVAL decomposition of the estimated source and the true source signals in terms of filtered true source interference and artifacts."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bss_eval_sources_framewise(reference_sources, estimated_sources,\n                               window=30*44100, hop=15*44100,\n                               compute_permutation=False):\n    \"\"\"Framewise computation of bss_eval_sources\n\n    Please be aware that this function does not compute permutations (by\n    default) on the possible relations between reference_sources and\n    estimated_sources due to the dangers of a changing permutation. Therefore\n    (by default), it assumes that ``reference_sources[i]`` corresponds to\n    ``estimated_sources[i]``. To enable computing permutations please set\n    ``compute_permutation`` to be ``True`` and check that the returned ``perm``\n    is identical for all windows.\n\n    NOTE: if ``reference_sources`` and ``estimated_sources`` would be evaluated\n    using only a single window or are shorter than the window length, the\n    result of :func:`mir_eval.separation.bss_eval_sources` called on\n    ``reference_sources`` and ``estimated_sources`` (with the\n    ``compute_permutation`` parameter passed to\n    :func:`mir_eval.separation.bss_eval_sources`) is returned.\n\n    Examples\n    --------\n    >>> # reference_sources[n] should be an ndarray of samples of the\n    >>> # n'th reference source\n    >>> # estimated_sources[n] should be the same for the n'th estimated\n    >>> # source\n    >>> (sdr, sir, sar,\n    ...  perm) = mir_eval.separation.bss_eval_sources_framewise(\n             reference_sources,\n    ...      estimated_sources)\n\n    Parameters\n    ----------\n    reference_sources : np.ndarray, shape=(nsrc, nsampl)\n        matrix containing true sources (must have the same shape as\n        ``estimated_sources``)\n    estimated_sources : np.ndarray, shape=(nsrc, nsampl)\n        matrix containing estimated sources (must have the same shape as\n        ``reference_sources``)\n    window : int, optional\n        Window length for framewise evaluation (default value is 30s at a\n        sample rate of 44.1kHz)\n    hop : int, optional\n        Hop size for framewise evaluation (default value is 15s at a\n        sample rate of 44.1kHz)\n    compute_permutation : bool, optional\n        compute permutation of estimate/source combinations for all windows\n        (False by default)\n\n    Returns\n    -------\n    sdr : np.ndarray, shape=(nsrc, nframes)\n        vector of Signal to Distortion Ratios (SDR)\n    sir : np.ndarray, shape=(nsrc, nframes)\n        vector of Source to Interference Ratios (SIR)\n    sar : np.ndarray, shape=(nsrc, nframes)\n        vector of Sources to Artifacts Ratios (SAR)\n    perm : np.ndarray, shape=(nsrc, nframes)\n        vector containing the best ordering of estimated sources in\n        the mean SIR sense (estimated source number ``perm[j]`` corresponds to\n        true source number ``j``).  Note: ``perm`` will be ``range(nsrc)`` for\n        all windows if ``compute_permutation`` is ``False``\n\n    \"\"\"\n\n    # make sure the input is of shape (nsrc, nsampl)\n    if estimated_sources.ndim == 1:\n        estimated_sources = estimated_sources[np.newaxis, :]\n    if reference_sources.ndim == 1:\n        reference_sources = reference_sources[np.newaxis, :]\n\n    validate(reference_sources, estimated_sources)\n    # If empty matrices were supplied, return empty lists (special case)\n    if reference_sources.size == 0 or estimated_sources.size == 0:\n        return np.array([]), np.array([]), np.array([]), np.array([])\n\n    nsrc = reference_sources.shape[0]\n\n    nwin = int(\n        np.floor((reference_sources.shape[1] - window + hop) / hop)\n    )\n    # if fewer than 2 windows would be evaluated, return the sources result\n    if nwin < 2:\n        result = bss_eval_sources(reference_sources,\n                                  estimated_sources,\n                                  compute_permutation)\n        return [np.expand_dims(score, -1) for score in result]\n\n    # compute the criteria across all windows\n    sdr = np.empty((nsrc, nwin))\n    sir = np.empty((nsrc, nwin))\n    sar = np.empty((nsrc, nwin))\n    perm = np.empty((nsrc, nwin))\n\n    # k iterates across all the windows\n    for k in range(nwin):\n        win_slice = slice(k * hop, k * hop + window)\n        ref_slice = reference_sources[:, win_slice]\n        est_slice = estimated_sources[:, win_slice]\n        # check for a silent frame\n        if (not _any_source_silent(ref_slice) and\n                not _any_source_silent(est_slice)):\n            sdr[:, k], sir[:, k], sar[:, k], perm[:, k] = bss_eval_sources(\n                ref_slice, est_slice, compute_permutation\n            )\n        else:\n            # if we have a silent frame set results as np.nan\n            sdr[:, k] = sir[:, k] = sar[:, k] = perm[:, k] = np.nan\n\n    return sdr, sir, sar, perm", "response": "Framewise evaluation of a sequence of true sources and estimated sources."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _project(reference_sources, estimated_source, flen):\n    nsrc = reference_sources.shape[0]\n    nsampl = reference_sources.shape[1]\n\n    # computing coefficients of least squares problem via FFT ##\n    # zero padding and FFT of input data\n    reference_sources = np.hstack((reference_sources,\n                                   np.zeros((nsrc, flen - 1))))\n    estimated_source = np.hstack((estimated_source, np.zeros(flen - 1)))\n    n_fft = int(2**np.ceil(np.log2(nsampl + flen - 1.)))\n    sf = scipy.fftpack.fft(reference_sources, n=n_fft, axis=1)\n    sef = scipy.fftpack.fft(estimated_source, n=n_fft)\n    # inner products between delayed versions of reference_sources\n    G = np.zeros((nsrc * flen, nsrc * flen))\n    for i in range(nsrc):\n        for j in range(nsrc):\n            ssf = sf[i] * np.conj(sf[j])\n            ssf = np.real(scipy.fftpack.ifft(ssf))\n            ss = toeplitz(np.hstack((ssf[0], ssf[-1:-flen:-1])),\n                          r=ssf[:flen])\n            G[i * flen: (i+1) * flen, j * flen: (j+1) * flen] = ss\n            G[j * flen: (j+1) * flen, i * flen: (i+1) * flen] = ss.T\n    # inner products between estimated_source and delayed versions of\n    # reference_sources\n    D = np.zeros(nsrc * flen)\n    for i in range(nsrc):\n        ssef = sf[i] * np.conj(sef)\n        ssef = np.real(scipy.fftpack.ifft(ssef))\n        D[i * flen: (i+1) * flen] = np.hstack((ssef[0], ssef[-1:-flen:-1]))\n\n    # Computing projection\n    # Distortion filters\n    try:\n        C = np.linalg.solve(G, D).reshape(flen, nsrc, order='F')\n    except np.linalg.linalg.LinAlgError:\n        C = np.linalg.lstsq(G, D)[0].reshape(flen, nsrc, order='F')\n    # Filtering\n    sproj = np.zeros(nsampl + flen - 1)\n    for i in range(nsrc):\n        sproj += fftconvolve(C[:, i], reference_sources[i])[:nsampl + flen - 1]\n    return sproj", "response": "Project the data into a single subspace of the same length."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprojects the images in the same subspace as the estimated source.", "response": "def _project_images(reference_sources, estimated_source, flen, G=None):\n    \"\"\"Least-squares projection of estimated source on the subspace spanned by\n    delayed versions of reference sources, with delays between 0 and flen-1.\n    Passing G as all zeros will populate the G matrix and return it so it can\n    be passed into the next call to avoid recomputing G (this will only works\n    if not computing permutations).\n    \"\"\"\n    nsrc = reference_sources.shape[0]\n    nsampl = reference_sources.shape[1]\n    nchan = reference_sources.shape[2]\n    reference_sources = np.reshape(np.transpose(reference_sources, (2, 0, 1)),\n                                   (nchan*nsrc, nsampl), order='F')\n\n    # computing coefficients of least squares problem via FFT ##\n    # zero padding and FFT of input data\n    reference_sources = np.hstack((reference_sources,\n                                   np.zeros((nchan*nsrc, flen - 1))))\n    estimated_source = \\\n        np.hstack((estimated_source.transpose(), np.zeros((nchan, flen - 1))))\n    n_fft = int(2**np.ceil(np.log2(nsampl + flen - 1.)))\n    sf = scipy.fftpack.fft(reference_sources, n=n_fft, axis=1)\n    sef = scipy.fftpack.fft(estimated_source, n=n_fft)\n\n    # inner products between delayed versions of reference_sources\n    if G is None:\n        saveg = False\n        G = np.zeros((nchan * nsrc * flen, nchan * nsrc * flen))\n        for i in range(nchan * nsrc):\n            for j in range(i+1):\n                ssf = sf[i] * np.conj(sf[j])\n                ssf = np.real(scipy.fftpack.ifft(ssf))\n                ss = toeplitz(np.hstack((ssf[0], ssf[-1:-flen:-1])),\n                              r=ssf[:flen])\n                G[i * flen: (i+1) * flen, j * flen: (j+1) * flen] = ss\n                G[j * flen: (j+1) * flen, i * flen: (i+1) * flen] = ss.T\n    else:  # avoid recomputing G (only works if no permutation is desired)\n        saveg = True  # return G\n        if np.all(G == 0):  # only compute G if passed as 0\n            G = np.zeros((nchan * nsrc * flen, nchan * nsrc * flen))\n            for i in range(nchan * nsrc):\n                for j in range(i+1):\n                    ssf = sf[i] * np.conj(sf[j])\n                    ssf = np.real(scipy.fftpack.ifft(ssf))\n                    ss = toeplitz(np.hstack((ssf[0], ssf[-1:-flen:-1])),\n                                  r=ssf[:flen])\n                    G[i * flen: (i+1) * flen, j * flen: (j+1) * flen] = ss\n                    G[j * flen: (j+1) * flen, i * flen: (i+1) * flen] = ss.T\n\n    # inner products between estimated_source and delayed versions of\n    # reference_sources\n    D = np.zeros((nchan * nsrc * flen, nchan))\n    for k in range(nchan * nsrc):\n        for i in range(nchan):\n            ssef = sf[k] * np.conj(sef[i])\n            ssef = np.real(scipy.fftpack.ifft(ssef))\n            D[k * flen: (k+1) * flen, i] = \\\n                np.hstack((ssef[0], ssef[-1:-flen:-1])).transpose()\n\n    # Computing projection\n    # Distortion filters\n    try:\n        C = np.linalg.solve(G, D).reshape(flen, nchan*nsrc, nchan, order='F')\n    except np.linalg.linalg.LinAlgError:\n        C = np.linalg.lstsq(G, D)[0].reshape(flen, nchan*nsrc, nchan,\n                                             order='F')\n    # Filtering\n    sproj = np.zeros((nchan, nsampl + flen - 1))\n    for k in range(nchan * nsrc):\n        for i in range(nchan):\n            sproj[i] += fftconvolve(C[:, k, i].transpose(),\n                                    reference_sources[k])[:nsampl + flen - 1]\n    # return G only if it was passed in\n    if saveg:\n        return sproj, G\n    else:\n        return sproj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _bss_source_crit(s_true, e_spat, e_interf, e_artif):\n    # energy ratios\n    s_filt = s_true + e_spat\n    sdr = _safe_db(np.sum(s_filt**2), np.sum((e_interf + e_artif)**2))\n    sir = _safe_db(np.sum(s_filt**2), np.sum(e_interf**2))\n    sar = _safe_db(np.sum((s_filt + e_interf)**2), np.sum(e_artif**2))\n    return (sdr, sir, sar)", "response": "Measurement of the separation quality for a given source in terms of\n    filtered true source interference and artifacts."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes all metrics for the given reference and estimated signals.", "response": "def evaluate(reference_sources, estimated_sources, **kwargs):\n    \"\"\"Compute all metrics for the given reference and estimated signals.\n\n    NOTE: This will always compute :func:`mir_eval.separation.bss_eval_images`\n    for any valid input and will additionally compute\n    :func:`mir_eval.separation.bss_eval_sources` for valid input with fewer\n    than 3 dimensions.\n\n    Examples\n    --------\n    >>> # reference_sources[n] should be an ndarray of samples of the\n    >>> # n'th reference source\n    >>> # estimated_sources[n] should be the same for the n'th estimated source\n    >>> scores = mir_eval.separation.evaluate(reference_sources,\n    ...                                       estimated_sources)\n\n    Parameters\n    ----------\n    reference_sources : np.ndarray, shape=(nsrc, nsampl[, nchan])\n        matrix containing true sources\n    estimated_sources : np.ndarray, shape=(nsrc, nsampl[, nchan])\n        matrix containing estimated sources\n    kwargs\n        Additional keyword arguments which will be passed to the\n        appropriate metric or preprocessing functions.\n\n    Returns\n    -------\n    scores : dict\n        Dictionary of scores, where the key is the metric name (str) and\n        the value is the (float) score achieved.\n\n    \"\"\"\n    # Compute all the metrics\n    scores = collections.OrderedDict()\n\n    sdr, isr, sir, sar, perm = util.filter_kwargs(\n        bss_eval_images,\n        reference_sources,\n        estimated_sources,\n        **kwargs\n    )\n    scores['Images - Source to Distortion'] = sdr.tolist()\n    scores['Images - Image to Spatial'] = isr.tolist()\n    scores['Images - Source to Interference'] = sir.tolist()\n    scores['Images - Source to Artifact'] = sar.tolist()\n    scores['Images - Source permutation'] = perm.tolist()\n\n    sdr, isr, sir, sar, perm = util.filter_kwargs(\n        bss_eval_images_framewise,\n        reference_sources,\n        estimated_sources,\n        **kwargs\n    )\n    scores['Images Frames - Source to Distortion'] = sdr.tolist()\n    scores['Images Frames - Image to Spatial'] = isr.tolist()\n    scores['Images Frames - Source to Interference'] = sir.tolist()\n    scores['Images Frames - Source to Artifact'] = sar.tolist()\n    scores['Images Frames - Source permutation'] = perm.tolist()\n\n    # Verify we can compute sources on this input\n    if reference_sources.ndim < 3 and estimated_sources.ndim < 3:\n        sdr, sir, sar, perm = util.filter_kwargs(\n            bss_eval_sources_framewise,\n            reference_sources,\n            estimated_sources,\n            **kwargs\n        )\n        scores['Sources Frames - Source to Distortion'] = sdr.tolist()\n        scores['Sources Frames - Source to Interference'] = sir.tolist()\n        scores['Sources Frames - Source to Artifact'] = sar.tolist()\n        scores['Sources Frames - Source permutation'] = perm.tolist()\n\n        sdr, sir, sar, perm = util.filter_kwargs(\n            bss_eval_sources,\n            reference_sources,\n            estimated_sources,\n            **kwargs\n        )\n        scores['Sources - Source to Distortion'] = sdr.tolist()\n        scores['Sources - Source to Interference'] = sir.tolist()\n        scores['Sources - Source to Artifact'] = sar.tolist()\n        scores['Sources - Source permutation'] = perm.tolist()\n\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a vector of the signal that is placed at each specified time in the specified time series.", "response": "def clicks(times, fs, click=None, length=None):\n    \"\"\"Returns a signal with the signal 'click' placed at each specified time\n\n    Parameters\n    ----------\n    times : np.ndarray\n        times to place clicks, in seconds\n    fs : int\n        desired sampling rate of the output signal\n    click : np.ndarray\n        click signal, defaults to a 1 kHz blip\n    length : int\n        desired number of samples in the output signal,\n        defaults to ``times.max()*fs + click.shape[0] + 1``\n\n    Returns\n    -------\n    click_signal : np.ndarray\n        Synthesized click signal\n\n    \"\"\"\n    # Create default click signal\n    if click is None:\n        # 1 kHz tone, 100ms\n        click = np.sin(2*np.pi*np.arange(fs*.1)*1000/(1.*fs))\n        # Exponential decay\n        click *= np.exp(-np.arange(fs*.1)/(fs*.01))\n    # Set default length\n    if length is None:\n        length = int(times.max()*fs + click.shape[0] + 1)\n\n    # Pre-allocate click signal\n    click_signal = np.zeros(length)\n    # Place clicks\n    for time in times:\n        # Compute the boundaries of the click\n        start = int(time*fs)\n        end = start + click.shape[0]\n        # Make sure we don't try to output past the end of the signal\n        if start >= length:\n            break\n        if end >= length:\n            click_signal[start:] = click[:length - start]\n            break\n        # Normally, just add a click here\n        click_signal[start:end] = click\n    return click_signal"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef time_frequency(gram, frequencies, times, fs, function=np.sin, length=None,\n                   n_dec=1):\n    \"\"\"Reverse synthesis of a time-frequency representation of a signal\n\n    Parameters\n    ----------\n    gram : np.ndarray\n        ``gram[n, m]`` is the magnitude of ``frequencies[n]``\n        from ``times[m]`` to ``times[m + 1]``\n\n        Non-positive magnitudes are interpreted as silence.\n\n    frequencies : np.ndarray\n        array of size ``gram.shape[0]`` denoting the frequency of\n        each row of gram\n    times : np.ndarray, shape= ``(gram.shape[1],)`` or ``(gram.shape[1], 2)``\n        Either the start time of each column in the gram,\n        or the time interval corresponding to each column.\n    fs : int\n        desired sampling rate of the output signal\n    function : function\n        function to use to synthesize notes, should be :math:`2\\pi`-periodic\n    length : int\n        desired number of samples in the output signal,\n        defaults to ``times[-1]*fs``\n    n_dec : int\n        the number of decimals used to approximate each sonfied frequency.\n        Defaults to 1 decimal place. Higher precision will be slower.\n\n    Returns\n    -------\n    output : np.ndarray\n        synthesized version of the piano roll\n\n    \"\"\"\n    # Default value for length\n    if times.ndim == 1:\n        # Convert to intervals\n        times = util.boundaries_to_intervals(times)\n\n    if length is None:\n        length = int(times[-1, 1] * fs)\n\n    times, _ = util.adjust_intervals(times, t_max=length)\n\n    # Truncate times so that the shape matches gram\n    n_times = gram.shape[1]\n    times = times[:n_times]\n\n    def _fast_synthesize(frequency):\n        \"\"\"A faster way to synthesize a signal.\n            Generate one cycle, and simulate arbitrary repetitions\n            using array indexing tricks.\n        \"\"\"\n        # hack so that we can ensure an integer number of periods and samples\n        # rounds frequency to 1st decimal, s.t. 10 * frequency will be an int\n        frequency = np.round(frequency, n_dec)\n\n        # Generate 10*frequency periods at this frequency\n        # Equivalent to n_samples = int(n_periods * fs / frequency)\n        # n_periods = 10*frequency is the smallest integer that guarantees\n        # that n_samples will be an integer, since assuming 10*frequency\n        # is an integer\n        n_samples = int(10.0**n_dec * fs)\n\n        short_signal = function(2.0 * np.pi * np.arange(n_samples) *\n                                frequency / fs)\n\n        # Calculate the number of loops we need to fill the duration\n        n_repeats = int(np.ceil(length/float(short_signal.shape[0])))\n\n        # Simulate tiling the short buffer by using stride tricks\n        long_signal = as_strided(short_signal,\n                                 shape=(n_repeats, len(short_signal)),\n                                 strides=(0, short_signal.itemsize))\n\n        # Use a flatiter to simulate a long 1D buffer\n        return long_signal.flat\n\n    def _const_interpolator(value):\n        \"\"\"Return a function that returns `value`\n            no matter the input.\n        \"\"\"\n        def __interpolator(x):\n            return value\n        return __interpolator\n\n    # Threshold the tfgram to remove non-positive values\n    gram = np.maximum(gram, 0)\n\n    # Pre-allocate output signal\n    output = np.zeros(length)\n    time_centers = np.mean(times, axis=1) * float(fs)\n\n    for n, frequency in enumerate(frequencies):\n        # Get a waveform of length samples at this frequency\n        wave = _fast_synthesize(frequency)\n\n        # Interpolate the values in gram over the time grid\n        if len(time_centers) > 1:\n            gram_interpolator = interp1d(\n                time_centers, gram[n, :],\n                kind='linear', bounds_error=False,\n                fill_value=0.0)\n        # If only one time point, create constant interpolator\n        else:\n            gram_interpolator = _const_interpolator(gram[n, 0])\n\n        # Scale each time interval by the piano roll magnitude\n        for m, (start, end) in enumerate((times * fs).astype(int)):\n            # Clip the timings to make sure the indices are valid\n            start, end = max(start, 0), min(end, length)\n            # add to waveform\n            output[start:end] += (\n                wave[start:end] * gram_interpolator(np.arange(start, end)))\n\n    # Normalize, but only if there's non-zero values\n    norm = np.abs(output).max()\n    if norm >= np.finfo(output.dtype).tiny:\n        output /= norm\n\n    return output", "response": "Synthesize a time - frequency representation of a signal."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pitch_contour(times, frequencies, fs, amplitudes=None, function=np.sin,\n                  length=None, kind='linear'):\n    '''Sonify a pitch contour.\n\n    Parameters\n    ----------\n    times : np.ndarray\n        time indices for each frequency measurement, in seconds\n\n    frequencies : np.ndarray\n        frequency measurements, in Hz.\n        Non-positive measurements will be interpreted as un-voiced samples.\n\n    fs : int\n        desired sampling rate of the output signal\n\n    amplitudes : np.ndarray\n        amplitude measurments, nonnegative\n        defaults to ``np.ones((length,))``\n\n    function : function\n        function to use to synthesize notes, should be :math:`2\\pi`-periodic\n\n    length : int\n        desired number of samples in the output signal,\n        defaults to ``max(times)*fs``\n\n    kind : str\n        Interpolation mode for the frequency and amplitude values.\n        See: ``scipy.interpolate.interp1d`` for valid settings.\n\n    Returns\n    -------\n    output : np.ndarray\n        synthesized version of the pitch contour\n    '''\n\n    fs = float(fs)\n\n    if length is None:\n        length = int(times.max() * fs)\n\n    # Squash the negative frequencies.\n    # wave(0) = 0, so clipping here will un-voice the corresponding instants\n    frequencies = np.maximum(frequencies, 0.0)\n\n    # Build a frequency interpolator\n    f_interp = interp1d(times * fs, 2 * np.pi * frequencies / fs, kind=kind,\n                        fill_value=0.0, bounds_error=False, copy=False)\n\n    # Estimate frequency at sample points\n    f_est = f_interp(np.arange(length))\n\n    if amplitudes is None:\n        a_est = np.ones((length, ))\n    else:\n        # build an amplitude interpolator\n        a_interp = interp1d(\n            times * fs, amplitudes, kind=kind,\n            fill_value=0.0, bounds_error=False, copy=False)\n        a_est = a_interp(np.arange(length))\n\n    # Sonify the waveform\n    return a_est * function(np.cumsum(f_est))", "response": "Sonify a pitch contour."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chroma(chromagram, times, fs, **kwargs):\n    # We'll just use time_frequency with a Shepard tone-gram\n    # To create the Shepard tone-gram, we copy the chromagram across 7 octaves\n    n_octaves = 7\n    # starting from C2\n    base_note = 24\n    # and weight each octave by a normal distribution\n    # The normal distribution has mean 72 (one octave above middle C)\n    # and std 6 (one half octave)\n    mean = 72\n    std = 6\n    notes = np.arange(12*n_octaves) + base_note\n    shepard_weight = np.exp(-(notes - mean)**2./(2.*std**2.))\n    # Copy the chromagram matrix vertically n_octaves times\n    gram = np.tile(chromagram.T, n_octaves).T\n    # This fixes issues if the supplied chromagram is int type\n    gram = gram.astype(float)\n    # Apply Sheppard weighting\n    gram *= shepard_weight.reshape(-1, 1)\n    # Compute frequencies\n    frequencies = 440.0*(2.0**((notes - 69)/12.0))\n    return time_frequency(gram, frequencies, times, fs, **kwargs)", "response": "Reverse synthesis of a chromagram"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chords(chord_labels, intervals, fs, **kwargs):\n    util.validate_intervals(intervals)\n\n    # Convert from labels to chroma\n    roots, interval_bitmaps, _ = chord.encode_many(chord_labels)\n    chromagram = np.array([np.roll(interval_bitmap, root)\n                           for (interval_bitmap, root)\n                           in zip(interval_bitmaps, roots)]).T\n\n    return chroma(chromagram, intervals, fs, **kwargs)", "response": "Synthesizes a list of chord labels at a given interval."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that the input annotations to a metric look like valid onset arrays and throws helpful errors if not.", "response": "def validate(reference_onsets, estimated_onsets):\n    \"\"\"Checks that the input annotations to a metric look like valid onset time\n    arrays, and throws helpful errors if not.\n\n    Parameters\n    ----------\n    reference_onsets : np.ndarray\n        reference onset locations, in seconds\n    estimated_onsets : np.ndarray\n        estimated onset locations, in seconds\n\n    \"\"\"\n    # If reference or estimated onsets are empty, warn because metric will be 0\n    if reference_onsets.size == 0:\n        warnings.warn(\"Reference onsets are empty.\")\n    if estimated_onsets.size == 0:\n        warnings.warn(\"Estimated onsets are empty.\")\n    for onsets in [reference_onsets, estimated_onsets]:\n        util.validate_events(onsets, MAX_TIME)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the F - measure of correct vs incorrectly predicted onsets. Corectness is determined over a small window.", "response": "def f_measure(reference_onsets, estimated_onsets, window=.05):\n    \"\"\"Compute the F-measure of correct vs incorrectly predicted onsets.\n    \"Corectness\" is determined over a small window.\n\n    Examples\n    --------\n    >>> reference_onsets = mir_eval.io.load_events('reference.txt')\n    >>> estimated_onsets = mir_eval.io.load_events('estimated.txt')\n    >>> F, P, R = mir_eval.onset.f_measure(reference_onsets,\n    ...                                    estimated_onsets)\n\n    Parameters\n    ----------\n    reference_onsets : np.ndarray\n        reference onset locations, in seconds\n    estimated_onsets : np.ndarray\n        estimated onset locations, in seconds\n    window : float\n        Window size, in seconds\n        (Default value = .05)\n\n    Returns\n    -------\n    f_measure : float\n        2*precision*recall/(precision + recall)\n    precision : float\n        (# true positives)/(# true positives + # false positives)\n    recall : float\n        (# true positives)/(# true positives + # false negatives)\n\n    \"\"\"\n    validate(reference_onsets, estimated_onsets)\n    # If either list is empty, return 0s\n    if reference_onsets.size == 0 or estimated_onsets.size == 0:\n        return 0., 0., 0.\n    # Compute the best-case matching between reference and estimated onset\n    # locations\n    matching = util.match_events(reference_onsets, estimated_onsets, window)\n\n    precision = float(len(matching))/len(estimated_onsets)\n    recall = float(len(matching))/len(reference_onsets)\n    # Compute F-measure and return all statistics\n    return util.f_measure(precision, recall), precision, recall"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef evaluate(reference_onsets, estimated_onsets, **kwargs):\n    # Compute all metrics\n    scores = collections.OrderedDict()\n\n    (scores['F-measure'],\n     scores['Precision'],\n     scores['Recall']) = util.filter_kwargs(f_measure, reference_onsets,\n                                            estimated_onsets, **kwargs)\n\n    return scores", "response": "Compute all metrics for the given reference and estimated annotations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate(ref_intervals, ref_pitches, est_intervals, est_pitches):\n    # Validate intervals\n    validate_intervals(ref_intervals, est_intervals)\n\n    # Make sure intervals and pitches match in length\n    if not ref_intervals.shape[0] == ref_pitches.shape[0]:\n        raise ValueError('Reference intervals and pitches have different '\n                         'lengths.')\n    if not est_intervals.shape[0] == est_pitches.shape[0]:\n        raise ValueError('Estimated intervals and pitches have different '\n                         'lengths.')\n\n    # Make sure all pitch values are positive\n    if ref_pitches.size > 0 and np.min(ref_pitches) <= 0:\n        raise ValueError(\"Reference contains at least one non-positive pitch \"\n                         \"value\")\n    if est_pitches.size > 0 and np.min(est_pitches) <= 0:\n        raise ValueError(\"Estimate contains at least one non-positive pitch \"\n                         \"value\")", "response": "Validates that the input annotations to a metric look like time intervals\n    and a pitch list and throws helpful errors if not."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that the input annotations to a metric look like time intervals and throws helpful errors if not.", "response": "def validate_intervals(ref_intervals, est_intervals):\n    \"\"\"Checks that the input annotations to a metric look like time intervals,\n    and throws helpful errors if not.\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    \"\"\"\n    # If reference or estimated notes are empty, warn\n    if ref_intervals.size == 0:\n        warnings.warn(\"Reference notes are empty.\")\n    if est_intervals.size == 0:\n        warnings.warn(\"Estimated notes are empty.\")\n\n    # Validate intervals\n    util.validate_intervals(ref_intervals)\n    util.validate_intervals(est_intervals)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef match_note_offsets(ref_intervals, est_intervals, offset_ratio=0.2,\n                       offset_min_tolerance=0.05, strict=False):\n    \"\"\"Compute a maximum matching between reference and estimated notes,\n    only taking note offsets into account.\n\n    Given two note sequences represented by ``ref_intervals`` and\n    ``est_intervals`` (see :func:`mir_eval.io.load_valued_intervals`), we seek\n    the largest set of correspondences ``(i, j)`` such that the offset of\n    reference note ``i`` has to be within ``offset_tolerance`` of the offset of\n    estimated note ``j``, where ``offset_tolerance`` is equal to\n    ``offset_ratio`` times the reference note's duration, i.e.  ``offset_ratio\n    * ref_duration[i]`` where ``ref_duration[i] = ref_intervals[i, 1] -\n    ref_intervals[i, 0]``. If the resulting ``offset_tolerance`` is less than\n    ``offset_min_tolerance`` (50 ms by default) then ``offset_min_tolerance``\n    is used instead.\n\n    Every reference note is matched against at most one estimated note.\n\n    Note there are separate functions :func:`match_note_onsets` and\n    :func:`match_notes` for matching notes based on onsets only or based on\n    onset, offset, and pitch, respectively. This is because the rules for\n    matching note onsets and matching note offsets are different.\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    offset_ratio : float > 0\n        The ratio of the reference note's duration used to define the\n        ``offset_tolerance``. Default is 0.2 (20%), meaning the\n        ``offset_tolerance`` will equal the ``ref_duration * 0.2``, or 0.05 (50\n        ms), whichever is greater.\n    offset_min_tolerance : float > 0\n        The minimum tolerance for offset matching. See ``offset_ratio``\n        description for an explanation of how the offset tolerance is\n        determined.\n    strict : bool\n        If ``strict=False`` (the default), threshold checks for offset\n        matching are performed using ``<=`` (less than or equal). If\n        ``strict=True``, the threshold checks are performed using ``<`` (less\n        than).\n\n    Returns\n    -------\n    matching : list of tuples\n        A list of matched reference and estimated notes.\n        ``matching[i] == (i, j)`` where reference note ``i`` matches estimated\n        note ``j``.\n    \"\"\"\n    # set the comparison function\n    if strict:\n        cmp_func = np.less\n    else:\n        cmp_func = np.less_equal\n\n    # check for offset matches\n    offset_distances = np.abs(np.subtract.outer(ref_intervals[:, 1],\n                                                est_intervals[:, 1]))\n    # Round distances to a target precision to avoid the situation where\n    # if the distance is exactly 50ms (and strict=False) it erroneously\n    # doesn't match the notes because of precision issues.\n    offset_distances = np.around(offset_distances, decimals=N_DECIMALS)\n    ref_durations = util.intervals_to_durations(ref_intervals)\n    offset_tolerances = np.maximum(offset_ratio * ref_durations,\n                                   offset_min_tolerance)\n    offset_hit_matrix = (\n        cmp_func(offset_distances, offset_tolerances.reshape(-1, 1)))\n\n    # check for hits\n    hits = np.where(offset_hit_matrix)\n\n    # Construct the graph input\n    # Flip graph so that 'matching' is a list of tuples where the first item\n    # in each tuple is the reference note index, and the second item is the\n    # estimated note index.\n    G = {}\n    for ref_i, est_i in zip(*hits):\n        if est_i not in G:\n            G[est_i] = []\n        G[est_i].append(ref_i)\n\n    # Compute the maximum matching\n    matching = sorted(util._bipartite_match(G).items())\n\n    return matching", "response": "Compute a maximum matching between two reference notes and estimated notes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef match_note_onsets(ref_intervals, est_intervals, onset_tolerance=0.05,\n                      strict=False):\n    \"\"\"Compute a maximum matching between reference and estimated notes,\n    only taking note onsets into account.\n\n    Given two note sequences represented by ``ref_intervals`` and\n    ``est_intervals`` (see :func:`mir_eval.io.load_valued_intervals`), we see\n    the largest set of correspondences ``(i,j)`` such that the onset of\n    reference note ``i`` is within ``onset_tolerance`` of the onset of\n    estimated note ``j``.\n\n    Every reference note is matched against at most one estimated note.\n\n    Note there are separate functions :func:`match_note_offsets` and\n    :func:`match_notes` for matching notes based on offsets only or based on\n    onset, offset, and pitch, respectively. This is because the rules for\n    matching note onsets and matching note offsets are different.\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    onset_tolerance : float > 0\n        The tolerance for an estimated note's onset deviating from the\n        reference note's onset, in seconds. Default is 0.05 (50 ms).\n    strict : bool\n        If ``strict=False`` (the default), threshold checks for onset matching\n        are performed using ``<=`` (less than or equal). If ``strict=True``,\n        the threshold checks are performed using ``<`` (less than).\n\n    Returns\n    -------\n    matching : list of tuples\n        A list of matched reference and estimated notes.\n        ``matching[i] == (i, j)`` where reference note ``i`` matches estimated\n        note ``j``.\n    \"\"\"\n    # set the comparison function\n    if strict:\n        cmp_func = np.less\n    else:\n        cmp_func = np.less_equal\n\n    # check for onset matches\n    onset_distances = np.abs(np.subtract.outer(ref_intervals[:, 0],\n                                               est_intervals[:, 0]))\n    # Round distances to a target precision to avoid the situation where\n    # if the distance is exactly 50ms (and strict=False) it erroneously\n    # doesn't match the notes because of precision issues.\n    onset_distances = np.around(onset_distances, decimals=N_DECIMALS)\n    onset_hit_matrix = cmp_func(onset_distances, onset_tolerance)\n\n    # find hits\n    hits = np.where(onset_hit_matrix)\n\n    # Construct the graph input\n    # Flip graph so that 'matching' is a list of tuples where the first item\n    # in each tuple is the reference note index, and the second item is the\n    # estimated note index.\n    G = {}\n    for ref_i, est_i in zip(*hits):\n        if est_i not in G:\n            G[est_i] = []\n        G[est_i].append(ref_i)\n\n    # Compute the maximum matching\n    matching = sorted(util._bipartite_match(G).items())\n\n    return matching", "response": "Compute a maximum matching between two reference and estimated notes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_notes(ref_intervals, ref_pitches, est_intervals, est_pitches,\n                onset_tolerance=0.05, pitch_tolerance=50.0, offset_ratio=0.2,\n                offset_min_tolerance=0.05, strict=False):\n    \"\"\"Compute a maximum matching between reference and estimated notes,\n    subject to onset, pitch and (optionally) offset constraints.\n\n    Given two note sequences represented by ``ref_intervals``, ``ref_pitches``,\n    ``est_intervals`` and ``est_pitches``\n    (see :func:`mir_eval.io.load_valued_intervals`), we seek the largest set\n    of correspondences ``(i, j)`` such that:\n\n    1. The onset of reference note ``i`` is within ``onset_tolerance`` of the\n       onset of estimated note ``j``.\n    2. The pitch of reference note ``i`` is within ``pitch_tolerance`` of the\n       pitch of estimated note ``j``.\n    3. If ``offset_ratio`` is not ``None``, the offset of reference note ``i``\n       has to be within ``offset_tolerance`` of the offset of estimated note\n       ``j``, where ``offset_tolerance`` is equal to ``offset_ratio`` times the\n       reference note's duration, i.e. ``offset_ratio * ref_duration[i]`` where\n       ``ref_duration[i] = ref_intervals[i, 1] - ref_intervals[i, 0]``.  If the\n       resulting ``offset_tolerance`` is less than 0.05 (50 ms), 0.05 is used\n       instead.\n    4. If ``offset_ratio`` is ``None``, note offsets are ignored, and only\n       criteria 1 and 2 are taken into consideration.\n\n    Every reference note is matched against at most one estimated note.\n\n    This is useful for computing precision/recall metrics for note\n    transcription.\n\n    Note there are separate functions :func:`match_note_onsets` and\n    :func:`match_note_offsets` for matching notes based on onsets only or based\n    on offsets only, respectively.\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    ref_pitches : np.ndarray, shape=(n,)\n        Array of reference pitch values in Hertz\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    est_pitches : np.ndarray, shape=(m,)\n        Array of estimated pitch values in Hertz\n    onset_tolerance : float > 0\n        The tolerance for an estimated note's onset deviating from the\n        reference note's onset, in seconds. Default is 0.05 (50 ms).\n    pitch_tolerance : float > 0\n        The tolerance for an estimated note's pitch deviating from the\n        reference note's pitch, in cents. Default is 50.0 (50 cents).\n    offset_ratio : float > 0 or None\n        The ratio of the reference note's duration used to define the\n        offset_tolerance. Default is 0.2 (20%), meaning the\n        ``offset_tolerance`` will equal the ``ref_duration * 0.2``, or 0.05 (50\n        ms), whichever is greater. If ``offset_ratio`` is set to ``None``,\n        offsets are ignored in the matching.\n    offset_min_tolerance : float > 0\n        The minimum tolerance for offset matching. See offset_ratio description\n        for an explanation of how the offset tolerance is determined. Note:\n        this parameter only influences the results if ``offset_ratio`` is not\n        ``None``.\n    strict : bool\n        If ``strict=False`` (the default), threshold checks for onset, offset,\n        and pitch matching are performed using ``<=`` (less than or equal). If\n        ``strict=True``, the threshold checks are performed using ``<`` (less\n        than).\n\n    Returns\n    -------\n    matching : list of tuples\n        A list of matched reference and estimated notes.\n        ``matching[i] == (i, j)`` where reference note ``i`` matches estimated\n        note ``j``.\n    \"\"\"\n    # set the comparison function\n    if strict:\n        cmp_func = np.less\n    else:\n        cmp_func = np.less_equal\n\n    # check for onset matches\n    onset_distances = np.abs(np.subtract.outer(ref_intervals[:, 0],\n                                               est_intervals[:, 0]))\n    # Round distances to a target precision to avoid the situation where\n    # if the distance is exactly 50ms (and strict=False) it erroneously\n    # doesn't match the notes because of precision issues.\n    onset_distances = np.around(onset_distances, decimals=N_DECIMALS)\n    onset_hit_matrix = cmp_func(onset_distances, onset_tolerance)\n\n    # check for pitch matches\n    pitch_distances = np.abs(1200*np.subtract.outer(np.log2(ref_pitches),\n                                                    np.log2(est_pitches)))\n    pitch_hit_matrix = cmp_func(pitch_distances, pitch_tolerance)\n\n    # check for offset matches if offset_ratio is not None\n    if offset_ratio is not None:\n        offset_distances = np.abs(np.subtract.outer(ref_intervals[:, 1],\n                                                    est_intervals[:, 1]))\n        # Round distances to a target precision to avoid the situation where\n        # if the distance is exactly 50ms (and strict=False) it erroneously\n        # doesn't match the notes because of precision issues.\n        offset_distances = np.around(offset_distances, decimals=N_DECIMALS)\n        ref_durations = util.intervals_to_durations(ref_intervals)\n        offset_tolerances = np.maximum(offset_ratio * ref_durations,\n                                       offset_min_tolerance)\n        offset_hit_matrix = (\n            cmp_func(offset_distances, offset_tolerances.reshape(-1, 1)))\n    else:\n        offset_hit_matrix = True\n\n    # check for overall matches\n    note_hit_matrix = onset_hit_matrix * pitch_hit_matrix * offset_hit_matrix\n    hits = np.where(note_hit_matrix)\n\n    # Construct the graph input\n    # Flip graph so that 'matching' is a list of tuples where the first item\n    # in each tuple is the reference note index, and the second item is the\n    # estimated note index.\n    G = {}\n    for ref_i, est_i in zip(*hits):\n        if est_i not in G:\n            G[est_i] = []\n        G[est_i].append(ref_i)\n\n    # Compute the maximum matching\n    matching = sorted(util._bipartite_match(G).items())\n\n    return matching", "response": "Compute a maximum matching between two reference notes and estimated notes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the Precision Recall and F - measure of correct vs incorrectly transcribed notes and the Average Overlap Ratio of correctly transcribed notes.", "response": "def precision_recall_f1_overlap(ref_intervals, ref_pitches, est_intervals,\n                                est_pitches, onset_tolerance=0.05,\n                                pitch_tolerance=50.0, offset_ratio=0.2,\n                                offset_min_tolerance=0.05, strict=False,\n                                beta=1.0):\n    \"\"\"Compute the Precision, Recall and F-measure of correct vs incorrectly\n    transcribed notes, and the Average Overlap Ratio for correctly transcribed\n    notes (see :func:`average_overlap_ratio`). \"Correctness\" is determined\n    based on note onset, pitch and (optionally) offset: an estimated note is\n    assumed correct if its onset is within +-50ms of a reference note and its\n    pitch (F0) is within +- quarter tone (50 cents) of the corresponding\n    reference note. If ``offset_ratio`` is ``None``, note offsets are ignored\n    in the comparison. Otherwise, on top of the above requirements, a correct\n    returned note is required to have an offset value within 20% (by default,\n    adjustable via the ``offset_ratio`` parameter) of the reference note's\n    duration around the reference note's offset, or within\n    ``offset_min_tolerance`` (50 ms by default), whichever is larger.\n\n    Examples\n    --------\n    >>> ref_intervals, ref_pitches = mir_eval.io.load_valued_intervals(\n    ...     'reference.txt')\n    >>> est_intervals, est_pitches = mir_eval.io.load_valued_intervals(\n    ...     'estimated.txt')\n    >>> (precision,\n    ...  recall,\n    ...  f_measure) = mir_eval.transcription.precision_recall_f1_overlap(\n    ...      ref_intervals, ref_pitches, est_intervals, est_pitches)\n    >>> (precision_no_offset,\n    ...  recall_no_offset,\n    ...  f_measure_no_offset) = (\n    ...      mir_eval.transcription.precision_recall_f1_overlap(\n    ...          ref_intervals, ref_pitches, est_intervals, est_pitches,\n    ...          offset_ratio=None))\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    ref_pitches : np.ndarray, shape=(n,)\n        Array of reference pitch values in Hertz\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    est_pitches : np.ndarray, shape=(m,)\n        Array of estimated pitch values in Hertz\n    onset_tolerance : float > 0\n        The tolerance for an estimated note's onset deviating from the\n        reference note's onset, in seconds. Default is 0.05 (50 ms).\n    pitch_tolerance : float > 0\n        The tolerance for an estimated note's pitch deviating from the\n        reference note's pitch, in cents. Default is 50.0 (50 cents).\n    offset_ratio : float > 0 or None\n        The ratio of the reference note's duration used to define the\n        offset_tolerance. Default is 0.2 (20%), meaning the\n        ``offset_tolerance`` will equal the ``ref_duration * 0.2``, or\n        ``offset_min_tolerance`` (0.05 by default, i.e. 50 ms), whichever is\n        greater. If ``offset_ratio`` is set to ``None``, offsets are ignored in\n        the evaluation.\n    offset_min_tolerance : float > 0\n        The minimum tolerance for offset matching. See ``offset_ratio``\n        description for an explanation of how the offset tolerance is\n        determined. Note: this parameter only influences the results if\n        ``offset_ratio`` is not ``None``.\n    strict : bool\n        If ``strict=False`` (the default), threshold checks for onset, offset,\n        and pitch matching are performed using ``<=`` (less than or equal). If\n        ``strict=True``, the threshold checks are performed using ``<`` (less\n        than).\n    beta : float > 0\n        Weighting factor for f-measure (default value = 1.0).\n\n    Returns\n    -------\n    precision : float\n        The computed precision score\n    recall : float\n        The computed recall score\n    f_measure : float\n        The computed F-measure score\n    avg_overlap_ratio : float\n        The computed Average Overlap Ratio score\n    \"\"\"\n    validate(ref_intervals, ref_pitches, est_intervals, est_pitches)\n    # When reference notes are empty, metrics are undefined, return 0's\n    if len(ref_pitches) == 0 or len(est_pitches) == 0:\n        return 0., 0., 0., 0.\n\n    matching = match_notes(ref_intervals, ref_pitches, est_intervals,\n                           est_pitches, onset_tolerance=onset_tolerance,\n                           pitch_tolerance=pitch_tolerance,\n                           offset_ratio=offset_ratio,\n                           offset_min_tolerance=offset_min_tolerance,\n                           strict=strict)\n\n    precision = float(len(matching))/len(est_pitches)\n    recall = float(len(matching))/len(ref_pitches)\n    f_measure = util.f_measure(precision, recall, beta=beta)\n\n    avg_overlap_ratio = average_overlap_ratio(ref_intervals, est_intervals,\n                                              matching)\n\n    return precision, recall, f_measure, avg_overlap_ratio"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the average Overlap Ratio between two reference and corresponding estimated note.", "response": "def average_overlap_ratio(ref_intervals, est_intervals, matching):\n    \"\"\"Compute the Average Overlap Ratio between a reference and estimated\n    note transcription. Given a reference and corresponding estimated note,\n    their overlap ratio (OR) is defined as the ratio between the duration of\n    the time segment in which the two notes overlap and the time segment\n    spanned by the two notes combined (earliest onset to latest offset):\n\n    >>> OR = ((min(ref_offset, est_offset) - max(ref_onset, est_onset)) /\n    ...     (max(ref_offset, est_offset) - min(ref_onset, est_onset)))\n\n    The Average Overlap Ratio (AOR) is given by the mean OR computed over all\n    matching reference and estimated notes. The metric goes from 0 (worst) to 1\n    (best).\n\n    Note: this function assumes the matching of reference and estimated notes\n    (see :func:`match_notes`) has already been performed and is provided by the\n    ``matching`` parameter. Furthermore, it is highly recommended to validate\n    the intervals (see :func:`validate_intervals`) before calling this\n    function, otherwise it is possible (though unlikely) for this function to\n    attempt a divide-by-zero operation.\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    matching : list of tuples\n        A list of matched reference and estimated notes.\n        ``matching[i] == (i, j)`` where reference note ``i`` matches estimated\n        note ``j``.\n\n    Returns\n    -------\n    avg_overlap_ratio : float\n        The computed Average Overlap Ratio score\n    \"\"\"\n    ratios = []\n    for match in matching:\n        ref_int = ref_intervals[match[0]]\n        est_int = est_intervals[match[1]]\n        overlap_ratio = (\n            (min(ref_int[1], est_int[1]) - max(ref_int[0], est_int[0])) /\n            (max(ref_int[1], est_int[1]) - min(ref_int[0], est_int[0])))\n        ratios.append(overlap_ratio)\n\n    if len(ratios) == 0:\n        return 0\n    else:\n        return np.mean(ratios)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef onset_precision_recall_f1(ref_intervals, est_intervals,\n                              onset_tolerance=0.05, strict=False, beta=1.0):\n    \"\"\"Compute the Precision, Recall and F-measure of note onsets: an estimated\n    onset is considered correct if it is within +-50ms of a reference onset.\n    Note that this metric completely ignores note offset and note pitch. This\n    means an estimated onset will be considered correct if it matches a\n    reference onset, even if the onsets come from notes with completely\n    different pitches (i.e. notes that would not match with\n    :func:`match_notes`).\n\n\n    Examples\n    --------\n    >>> ref_intervals, _ = mir_eval.io.load_valued_intervals(\n    ...     'reference.txt')\n    >>> est_intervals, _ = mir_eval.io.load_valued_intervals(\n    ...     'estimated.txt')\n    >>> (onset_precision,\n    ...  onset_recall,\n    ...  onset_f_measure) = mir_eval.transcription.onset_precision_recall_f1(\n    ...      ref_intervals, est_intervals)\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    onset_tolerance : float > 0\n        The tolerance for an estimated note's onset deviating from the\n        reference note's onset, in seconds. Default is 0.05 (50 ms).\n    strict : bool\n        If ``strict=False`` (the default), threshold checks for onset matching\n        are performed using ``<=`` (less than or equal). If ``strict=True``,\n        the threshold checks are performed using ``<`` (less than).\n    beta : float > 0\n        Weighting factor for f-measure (default value = 1.0).\n\n    Returns\n    -------\n    precision : float\n        The computed precision score\n    recall : float\n        The computed recall score\n    f_measure : float\n        The computed F-measure score\n    \"\"\"\n    validate_intervals(ref_intervals, est_intervals)\n    # When reference notes are empty, metrics are undefined, return 0's\n    if len(ref_intervals) == 0 or len(est_intervals) == 0:\n        return 0., 0., 0.\n\n    matching = match_note_onsets(ref_intervals, est_intervals,\n                                 onset_tolerance=onset_tolerance,\n                                 strict=strict)\n\n    onset_precision = float(len(matching))/len(est_intervals)\n    onset_recall = float(len(matching))/len(ref_intervals)\n    onset_f_measure = util.f_measure(onset_precision, onset_recall, beta=beta)\n    return onset_precision, onset_recall, onset_f_measure", "response": "Compute the Precision Recall and F - measure of note onsets."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef offset_precision_recall_f1(ref_intervals, est_intervals, offset_ratio=0.2,\n                               offset_min_tolerance=0.05, strict=False,\n                               beta=1.0):\n    \"\"\"Compute the Precision, Recall and F-measure of note offsets: an\n    estimated offset is considered correct if it is within +-50ms (or 20% of\n    the ref note duration, which ever is greater) of a reference offset. Note\n    that this metric completely ignores note onsets and note pitch. This means\n    an estimated offset will be considered correct if it matches a\n    reference offset, even if the offsets come from notes with completely\n    different pitches (i.e. notes that would not match with\n    :func:`match_notes`).\n\n\n    Examples\n    --------\n    >>> ref_intervals, _ = mir_eval.io.load_valued_intervals(\n    ...     'reference.txt')\n    >>> est_intervals, _ = mir_eval.io.load_valued_intervals(\n    ...     'estimated.txt')\n    >>> (offset_precision,\n    ...  offset_recall,\n    ...  offset_f_measure) = mir_eval.transcription.offset_precision_recall_f1(\n    ...      ref_intervals, est_intervals)\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    offset_ratio : float > 0 or None\n        The ratio of the reference note's duration used to define the\n        offset_tolerance. Default is 0.2 (20%), meaning the\n        ``offset_tolerance`` will equal the ``ref_duration * 0.2``, or\n        ``offset_min_tolerance`` (0.05 by default, i.e. 50 ms), whichever is\n        greater.\n    offset_min_tolerance : float > 0\n        The minimum tolerance for offset matching. See ``offset_ratio``\n        description for an explanation of how the offset tolerance is\n        determined.\n    strict : bool\n        If ``strict=False`` (the default), threshold checks for onset matching\n        are performed using ``<=`` (less than or equal). If ``strict=True``,\n        the threshold checks are performed using ``<`` (less than).\n    beta : float > 0\n        Weighting factor for f-measure (default value = 1.0).\n\n    Returns\n    -------\n    precision : float\n        The computed precision score\n    recall : float\n        The computed recall score\n    f_measure : float\n        The computed F-measure score\n    \"\"\"\n    validate_intervals(ref_intervals, est_intervals)\n    # When reference notes are empty, metrics are undefined, return 0's\n    if len(ref_intervals) == 0 or len(est_intervals) == 0:\n        return 0., 0., 0.\n\n    matching = match_note_offsets(ref_intervals, est_intervals,\n                                  offset_ratio=offset_ratio,\n                                  offset_min_tolerance=offset_min_tolerance,\n                                  strict=strict)\n\n    offset_precision = float(len(matching))/len(est_intervals)\n    offset_recall = float(len(matching))/len(ref_intervals)\n    offset_f_measure = util.f_measure(offset_precision, offset_recall,\n                                      beta=beta)\n    return offset_precision, offset_recall, offset_f_measure", "response": "Compute the Precision Recall and F - measure of note offsets."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes all metrics for the given reference and estimated annotations.", "response": "def evaluate(ref_intervals, ref_pitches, est_intervals, est_pitches, **kwargs):\n    \"\"\"Compute all metrics for the given reference and estimated annotations.\n\n    Examples\n    --------\n    >>> ref_intervals, ref_pitches = mir_eval.io.load_valued_intervals(\n    ...    'reference.txt')\n    >>> est_intervals, est_pitches = mir_eval.io.load_valued_intervals(\n    ...    'estimate.txt')\n    >>> scores = mir_eval.transcription.evaluate(ref_intervals, ref_pitches,\n    ...     est_intervals, est_pitches)\n\n    Parameters\n    ----------\n    ref_intervals : np.ndarray, shape=(n,2)\n        Array of reference notes time intervals (onset and offset times)\n    ref_pitches : np.ndarray, shape=(n,)\n        Array of reference pitch values in Hertz\n    est_intervals : np.ndarray, shape=(m,2)\n        Array of estimated notes time intervals (onset and offset times)\n    est_pitches : np.ndarray, shape=(m,)\n        Array of estimated pitch values in Hertz\n    kwargs\n        Additional keyword arguments which will be passed to the\n        appropriate metric or preprocessing functions.\n\n    Returns\n    -------\n    scores : dict\n        Dictionary of scores, where the key is the metric name (str) and\n        the value is the (float) score achieved.\n    \"\"\"\n    # Compute all the metrics\n    scores = collections.OrderedDict()\n\n    # Precision, recall and f-measure taking note offsets into account\n    kwargs.setdefault('offset_ratio', 0.2)\n    orig_offset_ratio = kwargs['offset_ratio']\n    if kwargs['offset_ratio'] is not None:\n        (scores['Precision'],\n         scores['Recall'],\n         scores['F-measure'],\n         scores['Average_Overlap_Ratio']) = util.filter_kwargs(\n            precision_recall_f1_overlap, ref_intervals, ref_pitches,\n            est_intervals, est_pitches, **kwargs)\n\n    # Precision, recall and f-measure NOT taking note offsets into account\n    kwargs['offset_ratio'] = None\n    (scores['Precision_no_offset'],\n     scores['Recall_no_offset'],\n     scores['F-measure_no_offset'],\n     scores['Average_Overlap_Ratio_no_offset']) = (\n        util.filter_kwargs(precision_recall_f1_overlap,\n                           ref_intervals, ref_pitches,\n                           est_intervals, est_pitches, **kwargs))\n\n    # onset-only metrics\n    (scores['Onset_Precision'],\n     scores['Onset_Recall'],\n     scores['Onset_F-measure']) = (\n        util.filter_kwargs(onset_precision_recall_f1,\n                           ref_intervals, est_intervals, **kwargs))\n\n    # offset-only metrics\n    kwargs['offset_ratio'] = orig_offset_ratio\n    if kwargs['offset_ratio'] is not None:\n        (scores['Offset_Precision'],\n         scores['Offset_Recall'],\n         scores['Offset_F-measure']) = (\n            util.filter_kwargs(offset_precision_recall_f1,\n                               ref_intervals, est_intervals, **kwargs))\n\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate that a key is well - formatted e. g. in the form C# major or minor.", "response": "def validate_key(key):\n    \"\"\"Checks that a key is well-formatted, e.g. in the form ``'C# major'``.\n\n    Parameters\n    ----------\n    key : str\n        Key to verify\n    \"\"\"\n    if len(key.split()) != 2:\n        raise ValueError(\"'{}' is not in the form '(key) (mode)'\".format(key))\n    key, mode = key.split()\n    if key.lower() not in KEY_TO_SEMITONE:\n        raise ValueError(\n            \"Key {} is invalid; should be e.g. D or C# or Eb\".format(key))\n    if mode not in ['major', 'minor']:\n        raise ValueError(\n            \"Mode '{}' is invalid; must be 'major' or 'minor'\".format(mode))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit a key string into a tuple of tuples where the first element is the semitone and the second is the mode.", "response": "def split_key_string(key):\n    \"\"\"Splits a key string (of the form, e.g. ``'C# major'``), into a tuple of\n    ``(key, mode)`` where ``key`` is is an integer representing the semitone\n    distance from C.\n\n    Parameters\n    ----------\n    key : str\n        String representing a key.\n\n    Returns\n    -------\n    key : int\n        Number of semitones above C.\n    mode : str\n        String representing the mode.\n    \"\"\"\n    key, mode = key.split()\n    return KEY_TO_SEMITONE[key.lower()], mode"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute a heuristic score which is weighted according to the relationship of the reference and estimated key.", "response": "def weighted_score(reference_key, estimated_key):\n    \"\"\"Computes a heuristic score which is weighted according to the\n    relationship of the reference and estimated key, as follows:\n\n    +------------------------------------------------------+-------+\n    | Relationship                                         | Score |\n    +------------------------------------------------------+-------+\n    | Same key                                             | 1.0   |\n    +------------------------------------------------------+-------+\n    | Estimated key is a perfect fifth above reference key | 0.5   |\n    +------------------------------------------------------+-------+\n    | Relative major/minor                                 | 0.3   |\n    +------------------------------------------------------+-------+\n    | Parallel major/minor                                 | 0.2   |\n    +------------------------------------------------------+-------+\n    | Other                                                | 0.0   |\n    +------------------------------------------------------+-------+\n\n    Examples\n    --------\n    >>> ref_key = mir_eval.io.load_key('ref.txt')\n    >>> est_key = mir_eval.io.load_key('est.txt')\n    >>> score = mir_eval.key.weighted_score(ref_key, est_key)\n\n    Parameters\n    ----------\n    reference_key : str\n        Reference key string.\n    estimated_key : str\n        Estimated key string.\n\n    Returns\n    -------\n    score : float\n        Score representing how closely related the keys are.\n    \"\"\"\n    validate(reference_key, estimated_key)\n    reference_key, reference_mode = split_key_string(reference_key)\n    estimated_key, estimated_mode = split_key_string(estimated_key)\n    # If keys are the same, return 1.\n    if reference_key == estimated_key and reference_mode == estimated_mode:\n        return 1.\n    # If keys are the same mode and a perfect fifth (differ by 7 semitones)\n    if (estimated_mode == reference_mode and\n            (estimated_key - reference_key) % 12 == 7):\n        return 0.5\n    # Estimated key is relative minor of reference key (9 semitones)\n    if (estimated_mode != reference_mode == 'major' and\n            (estimated_key - reference_key) % 12 == 9):\n        return 0.3\n    # Estimated key is relative major of reference key (3 semitones)\n    if (estimated_mode != reference_mode == 'minor' and\n            (estimated_key - reference_key) % 12 == 3):\n        return 0.3\n    # If keys are in different modes and parallel (same key name)\n    if estimated_mode != reference_mode and reference_key == estimated_key:\n        return 0.2\n    # Otherwise return 0\n    return 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes all metrics for the given reference and estimated annotations.", "response": "def evaluate(reference_key, estimated_key, **kwargs):\n    \"\"\"Compute all metrics for the given reference and estimated annotations.\n\n    Examples\n    --------\n    >>> ref_key = mir_eval.io.load_key('reference.txt')\n    >>> est_key = mir_eval.io.load_key('estimated.txt')\n    >>> scores = mir_eval.key.evaluate(ref_key, est_key)\n\n    Parameters\n    ----------\n    ref_key : str\n        Reference key string.\n\n    ref_key : str\n        Estimated key string.\n\n    kwargs\n        Additional keyword arguments which will be passed to the\n        appropriate metric or preprocessing functions.\n\n    Returns\n    -------\n    scores : dict\n        Dictionary of scores, where the key is the metric name (str) and\n        the value is the (float) score achieved.\n    \"\"\"\n    # Compute all metrics\n    scores = collections.OrderedDict()\n\n    scores['Weighted Score'] = util.filter_kwargs(\n            weighted_score, reference_key, estimated_key)\n\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that voicing inputs to a metric are in the correct format.", "response": "def validate_voicing(ref_voicing, est_voicing):\n    \"\"\"Checks that voicing inputs to a metric are in the correct format.\n\n    Parameters\n    ----------\n    ref_voicing : np.ndarray\n        Reference boolean voicing array\n    est_voicing : np.ndarray\n        Estimated boolean voicing array\n\n    \"\"\"\n    if ref_voicing.size == 0:\n        warnings.warn(\"Reference voicing array is empty.\")\n    if est_voicing.size == 0:\n        warnings.warn(\"Estimated voicing array is empty.\")\n    if ref_voicing.sum() == 0:\n        warnings.warn(\"Reference melody has no voiced frames.\")\n    if est_voicing.sum() == 0:\n        warnings.warn(\"Estimated melody has no voiced frames.\")\n    # Make sure they're the same length\n    if ref_voicing.shape[0] != est_voicing.shape[0]:\n        raise ValueError('Reference and estimated voicing arrays should '\n                         'be the same length.')\n    for voicing in [ref_voicing, est_voicing]:\n        # Make sure they're (effectively) boolean\n        if np.logical_and(voicing != 0, voicing != 1).any():\n            raise ValueError('Voicing arrays must be boolean.')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(ref_voicing, ref_cent, est_voicing, est_cent):\n    if ref_cent.size == 0:\n        warnings.warn(\"Reference frequency array is empty.\")\n    if est_cent.size == 0:\n        warnings.warn(\"Estimated frequency array is empty.\")\n    # Make sure they're the same length\n    if ref_voicing.shape[0] != ref_cent.shape[0] or \\\n       est_voicing.shape[0] != est_cent.shape[0] or \\\n       ref_cent.shape[0] != est_cent.shape[0]:\n        raise ValueError('All voicing and frequency arrays must have the '\n                         'same length.')", "response": "Checks that voicing and frequency arrays are well - formed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting an array of frequency values in Hz to cents.", "response": "def hz2cents(freq_hz, base_frequency=10.0):\n    \"\"\"Convert an array of frequency values in Hz to cents.\n    0 values are left in place.\n\n    Parameters\n    ----------\n    freq_hz : np.ndarray\n        Array of frequencies in Hz.\n    base_frequency : float\n        Base frequency for conversion.\n        (Default value = 10.0)\n\n    Returns\n    -------\n    cent : np.ndarray\n        Array of frequencies in cents, relative to base_frequency\n\n    \"\"\"\n    freq_cent = np.zeros(freq_hz.shape[0])\n    freq_nonz_ind = np.flatnonzero(freq_hz)\n    normalized_frequency = np.abs(freq_hz[freq_nonz_ind])/base_frequency\n    freq_cent[freq_nonz_ind] = 1200*np.log2(normalized_frequency)\n\n    return freq_cent"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef constant_hop_timebase(hop, end_time):\n    # Compute new timebase.  Rounding/linspace is to avoid float problems.\n    end_time = np.round(end_time, 10)\n    times = np.linspace(0, hop*int(np.floor(end_time/hop)),\n                        int(np.floor(end_time/hop)) + 1)\n    times = np.round(times, 10)\n    return times", "response": "Generates a time series from 0 to end_time with times spaced hop."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resample_melody_series(times, frequencies, voicing,\n                           times_new, kind='linear'):\n    \"\"\"Resamples frequency and voicing time series to a new timescale. Maintains\n    any zero (\"unvoiced\") values in frequencies.\n\n    If ``times`` and ``times_new`` are equivalent, no resampling will be\n    performed.\n\n    Parameters\n    ----------\n    times : np.ndarray\n        Times of each frequency value\n    frequencies : np.ndarray\n        Array of frequency values, >= 0\n    voicing : np.ndarray\n        Boolean array which indicates voiced or unvoiced\n    times_new : np.ndarray\n        Times to resample frequency and voicing sequences to\n    kind : str\n        kind parameter to pass to scipy.interpolate.interp1d.\n        (Default value = 'linear')\n\n    Returns\n    -------\n    frequencies_resampled : np.ndarray\n        Frequency array resampled to new timebase\n    voicing_resampled : np.ndarray, dtype=bool\n        Boolean voicing array resampled to new timebase\n\n    \"\"\"\n    # If the timebases are already the same, no need to interpolate\n    if times.shape == times_new.shape and np.allclose(times, times_new):\n        return frequencies, voicing.astype(np.bool)\n\n    # Warn when the delta between the original times is not constant,\n    # unless times[0] == 0. and frequencies[0] == frequencies[1] (see logic at\n    # the beginning of to_cent_voicing)\n    if not (np.allclose(np.diff(times), np.diff(times).mean()) or\n            (np.allclose(np.diff(times[1:]), np.diff(times[1:]).mean()) and\n             frequencies[0] == frequencies[1])):\n        warnings.warn(\n            \"Non-uniform timescale passed to resample_melody_series.  Pitch \"\n            \"will be linearly interpolated, which will result in undesirable \"\n            \"behavior if silences are indicated by missing values.  Silences \"\n            \"should be indicated by nonpositive frequency values.\")\n    # Round to avoid floating point problems\n    times = np.round(times, 10)\n    times_new = np.round(times_new, 10)\n    # Add in an additional sample if we'll be asking for a time too large\n    if times_new.max() > times.max():\n        times = np.append(times, times_new.max())\n        frequencies = np.append(frequencies, 0)\n        voicing = np.append(voicing, 0)\n    # We need to fix zero transitions if interpolation is not zero or nearest\n    if kind != 'zero' and kind != 'nearest':\n        # Fill in zero values with the last reported frequency\n        # to avoid erroneous values when resampling\n        frequencies_held = np.array(frequencies)\n        for n, frequency in enumerate(frequencies[1:]):\n            if frequency == 0:\n                frequencies_held[n + 1] = frequencies_held[n]\n        # Linearly interpolate frequencies\n        frequencies_resampled = scipy.interpolate.interp1d(times,\n                                                           frequencies_held,\n                                                           kind)(times_new)\n        # Retain zeros\n        frequency_mask = scipy.interpolate.interp1d(times,\n                                                    frequencies,\n                                                    'zero')(times_new)\n        frequencies_resampled *= (frequency_mask != 0)\n    else:\n        frequencies_resampled = scipy.interpolate.interp1d(times,\n                                                           frequencies,\n                                                           kind)(times_new)\n    # Use nearest-neighbor for voicing if it was used for frequencies\n    if kind == 'nearest':\n        voicing_resampled = scipy.interpolate.interp1d(times,\n                                                       voicing,\n                                                       kind)(times_new)\n    # otherwise, always use zeroth order\n    else:\n        voicing_resampled = scipy.interpolate.interp1d(times,\n                                                       voicing,\n                                                       'zero')(times_new)\n    return frequencies_resampled, voicing_resampled.astype(np.bool)", "response": "Resample the frequency and voicing time series to a new timescale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_cent_voicing(ref_time, ref_freq, est_time, est_freq, base_frequency=10.,\n                    hop=None, kind='linear'):\n    \"\"\"Converts reference and estimated time/frequency (Hz) annotations to sampled\n    frequency (cent)/voicing arrays.\n\n    A zero frequency indicates \"unvoiced\".\n\n    A negative frequency indicates \"Predicted as unvoiced, but if it's voiced,\n    this is the frequency estimate\".\n\n    Parameters\n    ----------\n    ref_time : np.ndarray\n        Time of each reference frequency value\n    ref_freq : np.ndarray\n        Array of reference frequency values\n    est_time : np.ndarray\n        Time of each estimated frequency value\n    est_freq : np.ndarray\n        Array of estimated frequency values\n    base_frequency : float\n        Base frequency in Hz for conversion to cents\n        (Default value = 10.)\n    hop : float\n        Hop size, in seconds, to resample,\n        default None which means use ref_time\n    kind : str\n        kind parameter to pass to scipy.interpolate.interp1d.\n        (Default value = 'linear')\n\n    Returns\n    -------\n    ref_voicing : np.ndarray, dtype=bool\n        Resampled reference boolean voicing array\n    ref_cent : np.ndarray\n        Resampled reference frequency (cent) array\n    est_voicing : np.ndarray, dtype=bool\n        Resampled estimated boolean voicing array\n    est_cent : np.ndarray\n        Resampled estimated frequency (cent) array\n\n    \"\"\"\n    # Check if missing sample at time 0 and if so add one\n    if ref_time[0] > 0:\n        ref_time = np.insert(ref_time, 0, 0)\n        ref_freq = np.insert(ref_freq, 0, ref_freq[0])\n    if est_time[0] > 0:\n        est_time = np.insert(est_time, 0, 0)\n        est_freq = np.insert(est_freq, 0, est_freq[0])\n    # Get separated frequency array and voicing boolean array\n    ref_freq, ref_voicing = freq_to_voicing(ref_freq)\n    est_freq, est_voicing = freq_to_voicing(est_freq)\n    # convert both sequences to cents\n    ref_cent = hz2cents(ref_freq, base_frequency)\n    est_cent = hz2cents(est_freq, base_frequency)\n    # If we received a hop, use it to resample both\n    if hop is not None:\n        # Resample to common time base\n        ref_cent, ref_voicing = resample_melody_series(\n            ref_time, ref_cent, ref_voicing,\n            constant_hop_timebase(hop, ref_time.max()), kind)\n        est_cent, est_voicing = resample_melody_series(\n            est_time, est_cent, est_voicing,\n            constant_hop_timebase(hop, est_time.max()), kind)\n    # Otherwise, only resample estimated to the reference time base\n    else:\n        est_cent, est_voicing = resample_melody_series(\n            est_time, est_cent, est_voicing, ref_time, kind)\n    # ensure the estimated sequence is the same length as the reference\n    len_diff = ref_cent.shape[0] - est_cent.shape[0]\n    if len_diff >= 0:\n        est_cent = np.append(est_cent, np.zeros(len_diff))\n        est_voicing = np.append(est_voicing, np.zeros(len_diff))\n    else:\n        est_cent = est_cent[:ref_cent.shape[0]]\n        est_voicing = est_voicing[:ref_voicing.shape[0]]\n\n    return (ref_voicing.astype(bool), ref_cent,\n            est_voicing.astype(bool), est_cent)", "response": "Converts reference and estimated time and frequency annotations to sampled frequency and voicing arrays."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef voicing_measures(ref_voicing, est_voicing):\n    validate_voicing(ref_voicing, est_voicing)\n    ref_voicing = ref_voicing.astype(bool)\n    est_voicing = est_voicing.astype(bool)\n    # When input arrays are empty, return 0 by special case\n    if ref_voicing.size == 0 or est_voicing.size == 0:\n        return 0.\n\n    # How voicing is computed\n    #        | ref_v | !ref_v |\n    # -------|-------|--------|\n    # est_v  |  TP   |   FP   |\n    # -------|-------|------- |\n    # !est_v |  FN   |   TN   |\n    # -------------------------\n\n    TP = (ref_voicing*est_voicing).sum()\n    FP = ((ref_voicing == 0)*est_voicing).sum()\n    FN = (ref_voicing*(est_voicing == 0)).sum()\n    TN = ((ref_voicing == 0)*(est_voicing == 0)).sum()\n\n    # Voicing recall = fraction of voiced frames according the reference that\n    # are declared as voiced by the estimate\n    if TP + FN == 0:\n        vx_recall = 0.\n    else:\n        vx_recall = TP/float(TP + FN)\n\n    # Voicing false alarm = fraction of unvoiced frames according to the\n    # reference that are declared as voiced by the estimate\n    if FP + TN == 0:\n        vx_false_alm = 0.\n    else:\n        vx_false_alm = FP/float(FP + TN)\n\n    return vx_recall, vx_false_alm", "response": "Compute the voicing recall and false alarm rates given two voicing indicator sequences one as reference and another as the estimate\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef raw_pitch_accuracy(ref_voicing, ref_cent, est_voicing, est_cent,\n                       cent_tolerance=50):\n    \"\"\"Compute the raw pitch accuracy given two pitch (frequency) sequences in\n    cents and matching voicing indicator sequences. The first pitch and voicing\n    arrays are treated as the reference (truth), and the second two as the\n    estimate (prediction).  All 4 sequences must be of the same length.\n\n    Examples\n    --------\n    >>> ref_time, ref_freq = mir_eval.io.load_time_series('ref.txt')\n    >>> est_time, est_freq = mir_eval.io.load_time_series('est.txt')\n    >>> (ref_v, ref_c,\n    ...  est_v, est_c) = mir_eval.melody.to_cent_voicing(ref_time,\n    ...                                                  ref_freq,\n    ...                                                  est_time,\n    ...                                                  est_freq)\n    >>> raw_pitch = mir_eval.melody.raw_pitch_accuracy(ref_v, ref_c,\n    ...                                                est_v, est_c)\n\n    Parameters\n    ----------\n    ref_voicing : np.ndarray\n        Reference boolean voicing array\n    ref_cent : np.ndarray\n        Reference pitch sequence in cents\n    est_voicing : np.ndarray\n        Estimated boolean voicing array\n    est_cent : np.ndarray\n        Estimate pitch sequence in cents\n    cent_tolerance : float\n        Maximum absolute deviation for a cent value to be considerd correct\n        (Default value = 50)\n\n    Returns\n    -------\n    raw_pitch : float\n        Raw pitch accuracy, the fraction of voiced frames in ref_cent for\n        which est_cent provides a correct frequency values\n        (within cent_tolerance cents).\n\n    \"\"\"\n\n    validate_voicing(ref_voicing, est_voicing)\n    validate(ref_voicing, ref_cent, est_voicing, est_cent)\n    ref_voicing = ref_voicing.astype(bool)\n    est_voicing = est_voicing.astype(bool)\n    # When input arrays are empty, return 0 by special case\n    if ref_voicing.size == 0 or est_voicing.size == 0 \\\n       or ref_cent.size == 0 or est_cent.size == 0:\n        return 0.\n    # If there are no voiced frames in reference, metric is 0\n    if ref_voicing.sum() == 0:\n        return 0.\n\n    # Raw pitch = the number of voiced frames in the reference for which the\n    # estimate provides a correct frequency value (within cent_tolerance cents)\n    # NB: voicing estimation is ignored in this measure\n    matching_voicing = ref_voicing * (est_cent > 0)\n    cent_diff = np.abs(ref_cent - est_cent)[matching_voicing]\n    frame_correct = (cent_diff < cent_tolerance)\n    raw_pitch = (frame_correct).sum()/float(ref_voicing.sum())\n\n    return raw_pitch", "response": "Compute the raw pitch accuracy given two pitch sequences in cents and matching voicing indicator sequences."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raw_chroma_accuracy(ref_voicing, ref_cent, est_voicing, est_cent,\n                        cent_tolerance=50):\n    \"\"\"Compute the raw chroma accuracy given two pitch (frequency) sequences\n    in cents and matching voicing indicator sequences. The first pitch and\n    voicing arrays are treated as the reference (truth), and the second two as\n    the estimate (prediction).  All 4 sequences must be of the same length.\n\n\n    Examples\n    --------\n    >>> ref_time, ref_freq = mir_eval.io.load_time_series('ref.txt')\n    >>> est_time, est_freq = mir_eval.io.load_time_series('est.txt')\n    >>> (ref_v, ref_c,\n    ...  est_v, est_c) = mir_eval.melody.to_cent_voicing(ref_time,\n    ...                                                  ref_freq,\n    ...                                                  est_time,\n    ...                                                  est_freq)\n    >>> raw_chroma = mir_eval.melody.raw_chroma_accuracy(ref_v, ref_c,\n    ...                                                  est_v, est_c)\n\n\n    Parameters\n    ----------\n    ref_voicing : np.ndarray\n        Reference boolean voicing array\n    ref_cent : np.ndarray\n        Reference pitch sequence in cents\n    est_voicing : np.ndarray\n        Estimated boolean voicing array\n    est_cent : np.ndarray\n        Estimate pitch sequence in cents\n    cent_tolerance : float\n        Maximum absolute deviation for a cent value to be considered correct\n        (Default value = 50)\n\n\n    Returns\n    -------\n    raw_chroma : float\n        Raw chroma accuracy, the fraction of voiced frames in ref_cent for\n        which est_cent provides a correct frequency values (within\n        cent_tolerance cents), ignoring octave errors\n\n\n    References\n    ----------\n    .. [#] J. Salamon, E. Gomez, D. P. W. Ellis and G. Richard, \"Melody\n        Extraction from Polyphonic Music Signals: Approaches, Applications\n        and Challenges\", IEEE Signal Processing Magazine, 31(2):118-134,\n        Mar. 2014.\n\n\n    .. [#] G. E. Poliner, D. P. W. Ellis, A. F. Ehmann, E. Gomez, S.\n        Streich, and B. Ong. \"Melody transcription from music audio:\n        Approaches and evaluation\", IEEE Transactions on Audio, Speech, and\n        Language Processing, 15(4):1247-1256, 2007.\n\n    \"\"\"\n    validate_voicing(ref_voicing, est_voicing)\n    validate(ref_voicing, ref_cent, est_voicing, est_cent)\n    ref_voicing = ref_voicing.astype(bool)\n    est_voicing = est_voicing.astype(bool)\n    # When input arrays are empty, return 0 by special case\n    if ref_voicing.size == 0 or est_voicing.size == 0 \\\n       or ref_cent.size == 0 or est_cent.size == 0:\n        return 0.\n\n    # If there are no voiced frames in reference, metric is 0\n    if ref_voicing.sum() == 0:\n        return 0.\n\n    # Raw chroma = same as raw pitch except that octave errors are ignored.\n    cent_diff = np.abs(ref_cent - est_cent)\n    octave = 1200*np.floor(cent_diff/1200.0 + 0.5)\n    matching_voicing = ref_voicing * (est_cent > 0)\n    cent_diff = np.abs(cent_diff - octave)[matching_voicing]\n    frame_correct = (cent_diff < cent_tolerance)\n    n_voiced = float(ref_voicing.sum())\n    raw_chroma = (frame_correct).sum()/n_voiced\n    return raw_chroma", "response": "Compute the raw chroma accuracy given two pitch sequences ref_voicing and est_voicing arrays and matching voicing indicator sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the overall accuracy given two pitch sequences in cents and matching voicing indicator sequences.", "response": "def overall_accuracy(ref_voicing, ref_cent, est_voicing, est_cent,\n                     cent_tolerance=50):\n    \"\"\"Compute the overall accuracy given two pitch (frequency) sequences in cents\n    and matching voicing indicator sequences. The first pitch and voicing\n    arrays are treated as the reference (truth), and the second two as the\n    estimate (prediction).  All 4 sequences must be of the same length.\n\n    Examples\n    --------\n    >>> ref_time, ref_freq = mir_eval.io.load_time_series('ref.txt')\n    >>> est_time, est_freq = mir_eval.io.load_time_series('est.txt')\n    >>> (ref_v, ref_c,\n    ...  est_v, est_c) = mir_eval.melody.to_cent_voicing(ref_time,\n    ...                                                  ref_freq,\n    ...                                                  est_time,\n    ...                                                  est_freq)\n    >>> overall_accuracy = mir_eval.melody.overall_accuracy(ref_v, ref_c,\n    ...                                                     est_v, est_c)\n\n    Parameters\n    ----------\n    ref_voicing : np.ndarray\n        Reference boolean voicing array\n    ref_cent : np.ndarray\n        Reference pitch sequence in cents\n    est_voicing : np.ndarray\n        Estimated boolean voicing array\n    est_cent : np.ndarray\n        Estimate pitch sequence in cents\n    cent_tolerance : float\n        Maximum absolute deviation for a cent value to be considered correct\n        (Default value = 50)\n\n    Returns\n    -------\n    overall_accuracy : float\n        Overall accuracy, the total fraction of correctly estimates frames,\n        where provides a correct frequency values (within cent_tolerance\n        cents).\n\n    \"\"\"\n    validate_voicing(ref_voicing, est_voicing)\n    validate(ref_voicing, ref_cent, est_voicing, est_cent)\n    ref_voicing = ref_voicing.astype(bool)\n    est_voicing = est_voicing.astype(bool)\n    # When input arrays are empty, return 0 by special case\n    if ref_voicing.size == 0 or est_voicing.size == 0 \\\n       or ref_cent.size == 0 or est_cent.size == 0:\n        return 0.\n\n    # True negatives = frames correctly estimates as unvoiced\n    TN = ((ref_voicing == 0)*(est_voicing == 0)).sum()\n\n    cent_diff = np.abs(ref_cent - est_cent)\n    frame_correct = (cent_diff[ref_voicing*est_voicing] < cent_tolerance)\n    accuracy = (frame_correct.sum() + TN)/float(ref_cent.shape[0])\n\n    return accuracy"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef evaluate(ref_time, ref_freq, est_time, est_freq, **kwargs):\n    # Convert to reference/estimated voicing/frequency (cent) arrays\n    (ref_voicing, ref_cent,\n     est_voicing, est_cent) = util.filter_kwargs(\n         to_cent_voicing, ref_time, ref_freq, est_time, est_freq, **kwargs)\n\n    # Compute metrics\n    scores = collections.OrderedDict()\n\n    (scores['Voicing Recall'],\n     scores['Voicing False Alarm']) = util.filter_kwargs(voicing_measures,\n                                                         ref_voicing,\n                                                         est_voicing, **kwargs)\n\n    scores['Raw Pitch Accuracy'] = util.filter_kwargs(raw_pitch_accuracy,\n                                                      ref_voicing, ref_cent,\n                                                      est_voicing, est_cent,\n                                                      **kwargs)\n\n    scores['Raw Chroma Accuracy'] = util.filter_kwargs(raw_chroma_accuracy,\n                                                       ref_voicing, ref_cent,\n                                                       est_voicing, est_cent,\n                                                       **kwargs)\n\n    scores['Overall Accuracy'] = util.filter_kwargs(overall_accuracy,\n                                                    ref_voicing, ref_cent,\n                                                    est_voicing, est_cent,\n                                                    **kwargs)\n    return scores", "response": "Evaluate two melody transcriptions where the first is the reference and the second is the estimated."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_boundary(reference_intervals, estimated_intervals, trim):\n\n    if trim:\n        # If we're trimming, then we need at least 2 intervals\n        min_size = 2\n    else:\n        # If we're not trimming, then we only need one interval\n        min_size = 1\n\n    if len(reference_intervals) < min_size:\n        warnings.warn(\"Reference intervals are empty.\")\n\n    if len(estimated_intervals) < min_size:\n        warnings.warn(\"Estimated intervals are empty.\")\n\n    for intervals in [reference_intervals, estimated_intervals]:\n        util.validate_intervals(intervals)", "response": "Validates that the input annotations to a segment boundary estimation\n    metric is valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that the input annotations to a structure estimation metric are well - formed.", "response": "def validate_structure(reference_intervals, reference_labels,\n                       estimated_intervals, estimated_labels):\n    \"\"\"Checks that the input annotations to a structure estimation metric (i.e.\n    one that takes in both segment boundaries and their labels) look like valid\n    segment times and labels, and throws helpful errors if not.\n\n    Parameters\n    ----------\n    reference_intervals : np.ndarray, shape=(n, 2)\n        reference segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    reference_labels : list, shape=(n,)\n        reference segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    estimated_intervals : np.ndarray, shape=(m, 2)\n        estimated segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    estimated_labels : list, shape=(m,)\n        estimated segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    \"\"\"\n    for (intervals, labels) in [(reference_intervals, reference_labels),\n                                (estimated_intervals, estimated_labels)]:\n\n        util.validate_intervals(intervals)\n        if intervals.shape[0] != len(labels):\n            raise ValueError('Number of intervals does not match number '\n                             'of labels')\n\n        # Check only when intervals are non-empty\n        if intervals.size > 0:\n            # Make sure intervals start at 0\n            if not np.allclose(intervals.min(), 0.0):\n                raise ValueError('Segment intervals do not start at 0')\n\n    if reference_intervals.size == 0:\n        warnings.warn(\"Reference intervals are empty.\")\n    if estimated_intervals.size == 0:\n        warnings.warn(\"Estimated intervals are empty.\")\n    # Check only when intervals are non-empty\n    if reference_intervals.size > 0 and estimated_intervals.size > 0:\n        if not np.allclose(reference_intervals.max(),\n                           estimated_intervals.max()):\n            raise ValueError('End times do not match')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef detection(reference_intervals, estimated_intervals,\n              window=0.5, beta=1.0, trim=False):\n    \"\"\"Boundary detection hit-rate.\n\n    A hit is counted whenever an reference boundary is within ``window`` of a\n    estimated boundary.  Note that each boundary is matched at most once: this\n    is achieved by computing the size of a maximal matching between reference\n    and estimated boundary points, subject to the window constraint.\n\n    Examples\n    --------\n    >>> ref_intervals, _ = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> est_intervals, _ = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> # With 0.5s windowing\n    >>> P05, R05, F05 = mir_eval.segment.detection(ref_intervals,\n    ...                                            est_intervals,\n    ...                                            window=0.5)\n    >>> # With 3s windowing\n    >>> P3, R3, F3 = mir_eval.segment.detection(ref_intervals,\n    ...                                         est_intervals,\n    ...                                         window=3)\n    >>> # Ignoring hits for the beginning and end of track\n    >>> P, R, F = mir_eval.segment.detection(ref_intervals,\n    ...                                      est_intervals,\n    ...                                      window=0.5,\n    ...                                      trim=True)\n\n    Parameters\n    ----------\n    reference_intervals : np.ndarray, shape=(n, 2)\n        reference segment intervals, in the format returned by\n        :func:`mir_eval.io.load_intervals` or\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_intervals : np.ndarray, shape=(m, 2)\n        estimated segment intervals, in the format returned by\n        :func:`mir_eval.io.load_intervals` or\n        :func:`mir_eval.io.load_labeled_intervals`.\n    window : float > 0\n        size of the window of 'correctness' around ground-truth beats\n        (in seconds)\n        (Default value = 0.5)\n    beta : float > 0\n        weighting constant for F-measure.\n        (Default value = 1.0)\n    trim : boolean\n        if ``True``, the first and last boundary times are ignored.\n        Typically, these denote start (0) and end-markers.\n        (Default value = False)\n\n    Returns\n    -------\n    precision : float\n        precision of estimated predictions\n    recall : float\n        recall of reference reference boundaries\n    f_measure : float\n        F-measure (weighted harmonic mean of ``precision`` and ``recall``)\n\n    \"\"\"\n\n    validate_boundary(reference_intervals, estimated_intervals, trim)\n\n    # Convert intervals to boundaries\n    reference_boundaries = util.intervals_to_boundaries(reference_intervals)\n    estimated_boundaries = util.intervals_to_boundaries(estimated_intervals)\n\n    # Suppress the first and last intervals\n    if trim:\n        reference_boundaries = reference_boundaries[1:-1]\n        estimated_boundaries = estimated_boundaries[1:-1]\n\n    # If we have no boundaries, we get no score.\n    if len(reference_boundaries) == 0 or len(estimated_boundaries) == 0:\n        return 0.0, 0.0, 0.0\n\n    matching = util.match_events(reference_boundaries,\n                                 estimated_boundaries,\n                                 window)\n\n    precision = float(len(matching)) / len(estimated_boundaries)\n    recall = float(len(matching)) / len(reference_boundaries)\n\n    f_measure = util.f_measure(precision, recall, beta=beta)\n\n    return precision, recall, f_measure", "response": "Boundary detection hit - rate."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deviation(reference_intervals, estimated_intervals, trim=False):\n\n    validate_boundary(reference_intervals, estimated_intervals, trim)\n\n    # Convert intervals to boundaries\n    reference_boundaries = util.intervals_to_boundaries(reference_intervals)\n    estimated_boundaries = util.intervals_to_boundaries(estimated_intervals)\n\n    # Suppress the first and last intervals\n    if trim:\n        reference_boundaries = reference_boundaries[1:-1]\n        estimated_boundaries = estimated_boundaries[1:-1]\n\n    # If we have no boundaries, we get no score.\n    if len(reference_boundaries) == 0 or len(estimated_boundaries) == 0:\n        return np.nan, np.nan\n\n    dist = np.abs(np.subtract.outer(reference_boundaries,\n                                    estimated_boundaries))\n\n    estimated_to_reference = np.median(dist.min(axis=0))\n    reference_to_estimated = np.median(dist.min(axis=1))\n\n    return reference_to_estimated, estimated_to_reference", "response": "Compute the median deviations between reference and estimated boundary times."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nframing - clustering segmentation evaluation by pair - wise agreement.", "response": "def pairwise(reference_intervals, reference_labels,\n             estimated_intervals, estimated_labels,\n             frame_size=0.1, beta=1.0):\n    \"\"\"Frame-clustering segmentation evaluation by pair-wise agreement.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> # Trim or pad the estimate to match reference timing\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.util.adjust_intervals(ref_intervals,\n    ...                                               ref_labels,\n    ...                                               t_min=0)\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, t_min=0, t_max=ref_intervals.max())\n    >>> precision, recall, f = mir_eval.structure.pairwise(ref_intervals,\n    ...                                                    ref_labels,\n    ...                                                    est_intervals,\n    ...                                                    est_labels)\n\n    Parameters\n    ----------\n    reference_intervals : np.ndarray, shape=(n, 2)\n        reference segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    reference_labels : list, shape=(n,)\n        reference segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_intervals : np.ndarray, shape=(m, 2)\n        estimated segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_labels : list, shape=(m,)\n        estimated segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    frame_size : float > 0\n        length (in seconds) of frames for clustering\n        (Default value = 0.1)\n    beta : float > 0\n        beta value for F-measure\n        (Default value = 1.0)\n\n    Returns\n    -------\n    precision : float > 0\n        Precision of detecting whether frames belong in the same cluster\n    recall : float > 0\n        Recall of detecting whether frames belong in the same cluster\n    f : float > 0\n        F-measure of detecting whether frames belong in the same cluster\n\n    \"\"\"\n    validate_structure(reference_intervals, reference_labels,\n                       estimated_intervals, estimated_labels)\n\n    # Check for empty annotations.  Don't need to check labels because\n    # validate_structure makes sure they're the same size as intervals\n    if reference_intervals.size == 0 or estimated_intervals.size == 0:\n        return 0., 0., 0.\n\n    # Generate the cluster labels\n    y_ref = util.intervals_to_samples(reference_intervals,\n                                      reference_labels,\n                                      sample_size=frame_size)[-1]\n\n    y_ref = util.index_labels(y_ref)[0]\n\n    # Map to index space\n    y_est = util.intervals_to_samples(estimated_intervals,\n                                      estimated_labels,\n                                      sample_size=frame_size)[-1]\n\n    y_est = util.index_labels(y_est)[0]\n\n    # Build the reference label agreement matrix\n    agree_ref = np.equal.outer(y_ref, y_ref)\n    # Count the unique pairs\n    n_agree_ref = (agree_ref.sum() - len(y_ref)) / 2.0\n\n    # Repeat for estimate\n    agree_est = np.equal.outer(y_est, y_est)\n    n_agree_est = (agree_est.sum() - len(y_est)) / 2.0\n\n    # Find where they agree\n    matches = np.logical_and(agree_ref, agree_est)\n    n_matches = (matches.sum() - len(y_ref)) / 2.0\n\n    precision = n_matches / n_agree_est\n    recall = n_matches / n_agree_ref\n    f_measure = util.f_measure(precision, recall, beta=beta)\n\n    return precision, recall, f_measure"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rand_index(reference_intervals, reference_labels,\n               estimated_intervals, estimated_labels,\n               frame_size=0.1, beta=1.0):\n    \"\"\"(Non-adjusted) Rand index.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> # Trim or pad the estimate to match reference timing\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.util.adjust_intervals(ref_intervals,\n    ...                                               ref_labels,\n    ...                                               t_min=0)\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, t_min=0, t_max=ref_intervals.max())\n    >>> rand_index = mir_eval.structure.rand_index(ref_intervals,\n    ...                                            ref_labels,\n    ...                                            est_intervals,\n    ...                                            est_labels)\n\n    Parameters\n    ----------\n    reference_intervals : np.ndarray, shape=(n, 2)\n        reference segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    reference_labels : list, shape=(n,)\n        reference segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_intervals : np.ndarray, shape=(m, 2)\n        estimated segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_labels : list, shape=(m,)\n        estimated segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    frame_size : float > 0\n        length (in seconds) of frames for clustering\n        (Default value = 0.1)\n    beta : float > 0\n        beta value for F-measure\n        (Default value = 1.0)\n\n    Returns\n    -------\n    rand_index : float > 0\n        Rand index\n\n    \"\"\"\n\n    validate_structure(reference_intervals, reference_labels,\n                       estimated_intervals, estimated_labels)\n\n    # Check for empty annotations.  Don't need to check labels because\n    # validate_structure makes sure they're the same size as intervals\n    if reference_intervals.size == 0 or estimated_intervals.size == 0:\n        return 0., 0., 0.\n\n    # Generate the cluster labels\n    y_ref = util.intervals_to_samples(reference_intervals,\n                                      reference_labels,\n                                      sample_size=frame_size)[-1]\n\n    y_ref = util.index_labels(y_ref)[0]\n\n    # Map to index space\n    y_est = util.intervals_to_samples(estimated_intervals,\n                                      estimated_labels,\n                                      sample_size=frame_size)[-1]\n\n    y_est = util.index_labels(y_est)[0]\n\n    # Build the reference label agreement matrix\n    agree_ref = np.equal.outer(y_ref, y_ref)\n\n    # Repeat for estimate\n    agree_est = np.equal.outer(y_est, y_est)\n\n    # Find where they agree\n    matches_pos = np.logical_and(agree_ref, agree_est)\n\n    # Find where they disagree\n    matches_neg = np.logical_and(~agree_ref, ~agree_est)\n\n    n_pairs = len(y_ref) * (len(y_ref) - 1) / 2.0\n\n    n_matches_pos = (matches_pos.sum() - len(y_ref)) / 2.0\n    n_matches_neg = matches_neg.sum() / 2.0\n    rand = (n_matches_pos + n_matches_neg) / n_pairs\n\n    return rand", "response": "Returns a random index for the given set of reference and estimated segments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _contingency_matrix(reference_indices, estimated_indices):\n    ref_classes, ref_class_idx = np.unique(reference_indices,\n                                           return_inverse=True)\n    est_classes, est_class_idx = np.unique(estimated_indices,\n                                           return_inverse=True)\n    n_ref_classes = ref_classes.shape[0]\n    n_est_classes = est_classes.shape[0]\n    # Using coo_matrix is faster than histogram2d\n    return scipy.sparse.coo_matrix((np.ones(ref_class_idx.shape[0]),\n                                    (ref_class_idx, est_class_idx)),\n                                   shape=(n_ref_classes, n_est_classes),\n                                   dtype=np.int).toarray()", "response": "Computes the contingency matrix of a true labeling vs an estimated one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _adjusted_rand_index(reference_indices, estimated_indices):\n    n_samples = len(reference_indices)\n    ref_classes = np.unique(reference_indices)\n    est_classes = np.unique(estimated_indices)\n    # Special limit cases: no clustering since the data is not split;\n    # or trivial clustering where each document is assigned a unique cluster.\n    # These are perfect matches hence return 1.0.\n    if (ref_classes.shape[0] == est_classes.shape[0] == 1 or\n        ref_classes.shape[0] == est_classes.shape[0] == 0 or\n        (ref_classes.shape[0] == est_classes.shape[0] ==\n         len(reference_indices))):\n        return 1.0\n\n    contingency = _contingency_matrix(reference_indices, estimated_indices)\n\n    # Compute the ARI using the contingency data\n    sum_comb_c = sum(scipy.special.comb(n_c, 2, exact=1) for n_c in\n                     contingency.sum(axis=1))\n    sum_comb_k = sum(scipy.special.comb(n_k, 2, exact=1) for n_k in\n                     contingency.sum(axis=0))\n\n    sum_comb = sum((scipy.special.comb(n_ij, 2, exact=1) for n_ij in\n                    contingency.flatten()))\n    prod_comb = (sum_comb_c * sum_comb_k)/float(scipy.special.comb(n_samples,\n                                                                   2))\n    mean_comb = (sum_comb_k + sum_comb_c)/2.\n    return (sum_comb - prod_comb)/(mean_comb - prod_comb)", "response": "Compute the Rand index adjusted for change."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ari(reference_intervals, reference_labels,\n        estimated_intervals, estimated_labels,\n        frame_size=0.1):\n    \"\"\"Adjusted Rand Index (ARI) for frame clustering segmentation evaluation.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> # Trim or pad the estimate to match reference timing\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.util.adjust_intervals(ref_intervals,\n    ...                                               ref_labels,\n    ...                                               t_min=0)\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, t_min=0, t_max=ref_intervals.max())\n    >>> ari_score = mir_eval.structure.ari(ref_intervals, ref_labels,\n    ...                                    est_intervals, est_labels)\n\n    Parameters\n    ----------\n    reference_intervals : np.ndarray, shape=(n, 2)\n        reference segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    reference_labels : list, shape=(n,)\n        reference segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_intervals : np.ndarray, shape=(m, 2)\n        estimated segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_labels : list, shape=(m,)\n        estimated segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    frame_size : float > 0\n        length (in seconds) of frames for clustering\n        (Default value = 0.1)\n\n    Returns\n    -------\n    ari_score : float > 0\n        Adjusted Rand index between segmentations.\n\n    \"\"\"\n    validate_structure(reference_intervals, reference_labels,\n                       estimated_intervals, estimated_labels)\n\n    # Check for empty annotations.  Don't need to check labels because\n    # validate_structure makes sure they're the same size as intervals\n    if reference_intervals.size == 0 or estimated_intervals.size == 0:\n        return 0., 0., 0.\n\n    # Generate the cluster labels\n    y_ref = util.intervals_to_samples(reference_intervals,\n                                      reference_labels,\n                                      sample_size=frame_size)[-1]\n\n    y_ref = util.index_labels(y_ref)[0]\n\n    # Map to index space\n    y_est = util.intervals_to_samples(estimated_intervals,\n                                      estimated_labels,\n                                      sample_size=frame_size)[-1]\n\n    y_est = util.index_labels(y_est)[0]\n\n    return _adjusted_rand_index(y_ref, y_est)", "response": "Adjusted Rand Index for frame clustering segmentation evaluation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _mutual_info_score(reference_indices, estimated_indices, contingency=None):\n    if contingency is None:\n        contingency = _contingency_matrix(reference_indices,\n                                          estimated_indices).astype(float)\n    contingency_sum = np.sum(contingency)\n    pi = np.sum(contingency, axis=1)\n    pj = np.sum(contingency, axis=0)\n    outer = np.outer(pi, pj)\n    nnz = contingency != 0.0\n    # normalized contingency\n    contingency_nm = contingency[nnz]\n    log_contingency_nm = np.log(contingency_nm)\n    contingency_nm /= contingency_sum\n    # log(a / b) should be calculated as log(a) - log(b) for\n    # possible loss of precision\n    log_outer = -np.log(outer[nnz]) + np.log(pi.sum()) + np.log(pj.sum())\n    mi = (contingency_nm * (log_contingency_nm - np.log(contingency_sum)) +\n          contingency_nm * log_outer)\n    return mi.sum()", "response": "Compute the mutual information between two sequence labelings."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the entropy of a cluster.", "response": "def _entropy(labels):\n    \"\"\"Calculates the entropy for a labeling.\n\n    Parameters\n    ----------\n    labels : list-like\n        List of labels.\n\n    Returns\n    -------\n    entropy : float\n        Entropy of the labeling.\n\n    .. note:: Based on sklearn.metrics.cluster.entropy\n\n    \"\"\"\n    if len(labels) == 0:\n        return 1.0\n    label_idx = np.unique(labels, return_inverse=True)[1]\n    pi = np.bincount(label_idx).astype(np.float)\n    pi = pi[pi > 0]\n    pi_sum = np.sum(pi)\n    # log(a / b) should be calculated as log(a) - log(b) for\n    # possible loss of precision\n    return -np.sum((pi / pi_sum) * (np.log(pi) - np.log(pi_sum)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the mutual information between two sequence labelings adjusted for chance.", "response": "def _adjusted_mutual_info_score(reference_indices, estimated_indices):\n    \"\"\"Compute the mutual information between two sequence labelings, adjusted for\n    chance.\n\n    Parameters\n    ----------\n    reference_indices : np.ndarray\n        Array of reference indices\n\n    estimated_indices : np.ndarray\n        Array of estimated indices\n\n    Returns\n    -------\n    ami : float <= 1.0\n        Mutual information\n\n    .. note:: Based on sklearn.metrics.cluster.adjusted_mutual_info_score\n        and sklearn.metrics.cluster.expected_mutual_info_score\n\n    \"\"\"\n    n_samples = len(reference_indices)\n    ref_classes = np.unique(reference_indices)\n    est_classes = np.unique(estimated_indices)\n    # Special limit cases: no clustering since the data is not split.\n    # This is a perfect match hence return 1.0.\n    if (ref_classes.shape[0] == est_classes.shape[0] == 1 or\n            ref_classes.shape[0] == est_classes.shape[0] == 0):\n        return 1.0\n    contingency = _contingency_matrix(reference_indices,\n                                      estimated_indices).astype(float)\n    # Calculate the MI for the two clusterings\n    mi = _mutual_info_score(reference_indices, estimated_indices,\n                            contingency=contingency)\n    # The following code is based on\n    # sklearn.metrics.cluster.expected_mutual_information\n    R, C = contingency.shape\n    N = float(n_samples)\n    a = np.sum(contingency, axis=1).astype(np.int32)\n    b = np.sum(contingency, axis=0).astype(np.int32)\n    # There are three major terms to the EMI equation, which are multiplied to\n    # and then summed over varying nij values.\n    # While nijs[0] will never be used, having it simplifies the indexing.\n    nijs = np.arange(0, max(np.max(a), np.max(b)) + 1, dtype='float')\n    # Stops divide by zero warnings. As its not used, no issue.\n    nijs[0] = 1\n    # term1 is nij / N\n    term1 = nijs / N\n    # term2 is log((N*nij) / (a * b)) == log(N * nij) - log(a * b)\n    # term2 uses the outer product\n    log_ab_outer = np.log(np.outer(a, b))\n    # term2 uses N * nij\n    log_Nnij = np.log(N * nijs)\n    # term3 is large, and involved many factorials. Calculate these in log\n    # space to stop overflows.\n    gln_a = scipy.special.gammaln(a + 1)\n    gln_b = scipy.special.gammaln(b + 1)\n    gln_Na = scipy.special.gammaln(N - a + 1)\n    gln_Nb = scipy.special.gammaln(N - b + 1)\n    gln_N = scipy.special.gammaln(N + 1)\n    gln_nij = scipy.special.gammaln(nijs + 1)\n    # start and end values for nij terms for each summation.\n    start = np.array([[v - N + w for w in b] for v in a], dtype='int')\n    start = np.maximum(start, 1)\n    end = np.minimum(np.resize(a, (C, R)).T, np.resize(b, (R, C))) + 1\n    # emi itself is a summation over the various values.\n    emi = 0\n    for i in range(R):\n        for j in range(C):\n            for nij in range(start[i, j], end[i, j]):\n                term2 = log_Nnij[nij] - log_ab_outer[i, j]\n                # Numerators are positive, denominators are negative.\n                gln = (gln_a[i] + gln_b[j] + gln_Na[i] + gln_Nb[j] -\n                       gln_N - gln_nij[nij] -\n                       scipy.special.gammaln(a[i] - nij + 1) -\n                       scipy.special.gammaln(b[j] - nij + 1) -\n                       scipy.special.gammaln(N - a[i] - b[j] + nij + 1))\n                term3 = np.exp(gln)\n                emi += (term1[nij] * term2 * term3)\n    # Calculate entropy for each labeling\n    h_true, h_pred = _entropy(reference_indices), _entropy(estimated_indices)\n    ami = (mi - emi) / (max(h_true, h_pred) - emi)\n    return ami"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the mutual information between two sequence labelings adjusted for chance.", "response": "def _normalized_mutual_info_score(reference_indices, estimated_indices):\n    \"\"\"Compute the mutual information between two sequence labelings, adjusted for\n    chance.\n\n    Parameters\n    ----------\n    reference_indices : np.ndarray\n        Array of reference indices\n\n    estimated_indices : np.ndarray\n        Array of estimated indices\n\n    Returns\n    -------\n    nmi : float <= 1.0\n        Normalized mutual information\n\n    .. note:: Based on sklearn.metrics.cluster.normalized_mutual_info_score\n\n    \"\"\"\n    ref_classes = np.unique(reference_indices)\n    est_classes = np.unique(estimated_indices)\n    # Special limit cases: no clustering since the data is not split.\n    # This is a perfect match hence return 1.0.\n    if (ref_classes.shape[0] == est_classes.shape[0] == 1 or\n            ref_classes.shape[0] == est_classes.shape[0] == 0):\n        return 1.0\n    contingency = _contingency_matrix(reference_indices,\n                                      estimated_indices).astype(float)\n    contingency = np.array(contingency, dtype='float')\n    # Calculate the MI for the two clusterings\n    mi = _mutual_info_score(reference_indices, estimated_indices,\n                            contingency=contingency)\n    # Calculate the expected value for the mutual information\n    # Calculate entropy for each labeling\n    h_true, h_pred = _entropy(reference_indices), _entropy(estimated_indices)\n    nmi = mi / max(np.sqrt(h_true * h_pred), 1e-10)\n    return nmi"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mutual_information(reference_intervals, reference_labels,\n                       estimated_intervals, estimated_labels,\n                       frame_size=0.1):\n    \"\"\"Frame-clustering segmentation: mutual information metrics.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> # Trim or pad the estimate to match reference timing\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.util.adjust_intervals(ref_intervals,\n    ...                                               ref_labels,\n    ...                                               t_min=0)\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, t_min=0, t_max=ref_intervals.max())\n    >>> mi, ami, nmi = mir_eval.structure.mutual_information(ref_intervals,\n    ...                                                      ref_labels,\n    ...                                                      est_intervals,\n    ...                                                      est_labels)\n\n    Parameters\n    ----------\n    reference_intervals : np.ndarray, shape=(n, 2)\n        reference segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    reference_labels : list, shape=(n,)\n        reference segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_intervals : np.ndarray, shape=(m, 2)\n        estimated segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_labels : list, shape=(m,)\n        estimated segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    frame_size : float > 0\n        length (in seconds) of frames for clustering\n        (Default value = 0.1)\n\n    Returns\n    -------\n    MI : float > 0\n        Mutual information between segmentations\n    AMI : float\n        Adjusted mutual information between segmentations.\n    NMI : float > 0\n        Normalize mutual information between segmentations\n\n    \"\"\"\n    validate_structure(reference_intervals, reference_labels,\n                       estimated_intervals, estimated_labels)\n\n    # Check for empty annotations.  Don't need to check labels because\n    # validate_structure makes sure they're the same size as intervals\n    if reference_intervals.size == 0 or estimated_intervals.size == 0:\n        return 0., 0., 0.\n\n    # Generate the cluster labels\n    y_ref = util.intervals_to_samples(reference_intervals,\n                                      reference_labels,\n                                      sample_size=frame_size)[-1]\n\n    y_ref = util.index_labels(y_ref)[0]\n\n    # Map to index space\n    y_est = util.intervals_to_samples(estimated_intervals,\n                                      estimated_labels,\n                                      sample_size=frame_size)[-1]\n\n    y_est = util.index_labels(y_est)[0]\n\n    # Mutual information\n    mutual_info = _mutual_info_score(y_ref, y_est)\n\n    # Adjusted mutual information\n    adj_mutual_info = _adjusted_mutual_info_score(y_ref, y_est)\n\n    # Normalized mutual information\n    norm_mutual_info = _normalized_mutual_info_score(y_ref, y_est)\n\n    return mutual_info, adj_mutual_info, norm_mutual_info", "response": "Frame - clustering segmentation of the mutual information metrics."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nframes - clustering segmentation : normalized conditional entropy Computes cross - entropy of cluster assignment normalized by the max - entropy of cluster assignment.", "response": "def nce(reference_intervals, reference_labels, estimated_intervals,\n        estimated_labels, frame_size=0.1, beta=1.0, marginal=False):\n    \"\"\"Frame-clustering segmentation: normalized conditional entropy\n\n    Computes cross-entropy of cluster assignment, normalized by the\n    max-entropy.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> # Trim or pad the estimate to match reference timing\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.util.adjust_intervals(ref_intervals,\n    ...                                               ref_labels,\n    ...                                               t_min=0)\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, t_min=0, t_max=ref_intervals.max())\n    >>> S_over, S_under, S_F = mir_eval.structure.nce(ref_intervals,\n    ...                                               ref_labels,\n    ...                                               est_intervals,\n    ...                                               est_labels)\n\n    Parameters\n    ----------\n    reference_intervals : np.ndarray, shape=(n, 2)\n        reference segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    reference_labels : list, shape=(n,)\n        reference segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_intervals : np.ndarray, shape=(m, 2)\n        estimated segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_labels : list, shape=(m,)\n        estimated segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    frame_size : float > 0\n        length (in seconds) of frames for clustering\n        (Default value = 0.1)\n    beta : float > 0\n        beta for F-measure\n        (Default value = 1.0)\n\n    marginal : bool\n        If `False`, normalize conditional entropy by uniform entropy.\n        If `True`, normalize conditional entropy by the marginal entropy.\n        (Default value = False)\n\n    Returns\n    -------\n    S_over\n        Over-clustering score:\n\n        - For `marginal=False`, ``1 - H(y_est | y_ref) / log(|y_est|)``\n\n        - For `marginal=True`, ``1 - H(y_est | y_ref) / H(y_est)``\n\n        If `|y_est|==1`, then `S_over` will be 0.\n\n    S_under\n        Under-clustering score:\n\n        - For `marginal=False`, ``1 - H(y_ref | y_est) / log(|y_ref|)``\n\n        - For `marginal=True`, ``1 - H(y_ref | y_est) / H(y_ref)``\n\n        If `|y_ref|==1`, then `S_under` will be 0.\n\n    S_F\n        F-measure for (S_over, S_under)\n\n    \"\"\"\n\n    validate_structure(reference_intervals, reference_labels,\n                       estimated_intervals, estimated_labels)\n\n    # Check for empty annotations.  Don't need to check labels because\n    # validate_structure makes sure they're the same size as intervals\n    if reference_intervals.size == 0 or estimated_intervals.size == 0:\n        return 0., 0., 0.\n\n    # Generate the cluster labels\n    y_ref = util.intervals_to_samples(reference_intervals,\n                                      reference_labels,\n                                      sample_size=frame_size)[-1]\n\n    y_ref = util.index_labels(y_ref)[0]\n\n    # Map to index space\n    y_est = util.intervals_to_samples(estimated_intervals,\n                                      estimated_labels,\n                                      sample_size=frame_size)[-1]\n\n    y_est = util.index_labels(y_est)[0]\n\n    # Make the contingency table: shape = (n_ref, n_est)\n    contingency = _contingency_matrix(y_ref, y_est).astype(float)\n\n    # Normalize by the number of frames\n    contingency = contingency / len(y_ref)\n\n    # Compute the marginals\n    p_est = contingency.sum(axis=0)\n    p_ref = contingency.sum(axis=1)\n\n    # H(true | prediction) = sum_j P[estimated = j] *\n    # sum_i P[true = i | estimated = j] log P[true = i | estimated = j]\n    # entropy sums over axis=0, which is true labels\n\n    true_given_est = p_est.dot(scipy.stats.entropy(contingency, base=2))\n    pred_given_ref = p_ref.dot(scipy.stats.entropy(contingency.T, base=2))\n\n    if marginal:\n        # Normalize conditional entropy by marginal entropy\n        z_ref = scipy.stats.entropy(p_ref, base=2)\n        z_est = scipy.stats.entropy(p_est, base=2)\n    else:\n        z_ref = np.log2(contingency.shape[0])\n        z_est = np.log2(contingency.shape[1])\n\n    score_under = 0.0\n    if z_ref > 0:\n        score_under = 1. - true_given_est / z_ref\n\n    score_over = 0.0\n    if z_est > 0:\n        score_over = 1. - pred_given_ref / z_est\n\n    f_measure = util.f_measure(score_over, score_under, beta=beta)\n\n    return score_over, score_under, f_measure"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nframe - clustering segmentation v - measure the cross - entropy of cluster assignment normalized by the v - measure.", "response": "def vmeasure(reference_intervals, reference_labels, estimated_intervals,\n             estimated_labels, frame_size=0.1, beta=1.0):\n    \"\"\"Frame-clustering segmentation: v-measure\n\n    Computes cross-entropy of cluster assignment, normalized by the\n    marginal-entropy.\n\n    This is equivalent to `nce(..., marginal=True)`.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> # Trim or pad the estimate to match reference timing\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.util.adjust_intervals(ref_intervals,\n    ...                                               ref_labels,\n    ...                                               t_min=0)\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, t_min=0, t_max=ref_intervals.max())\n    >>> V_precision, V_recall, V_F = mir_eval.structure.vmeasure(ref_intervals,\n    ...                                                          ref_labels,\n    ...                                                          est_intervals,\n    ...                                                          est_labels)\n\n    Parameters\n    ----------\n    reference_intervals : np.ndarray, shape=(n, 2)\n        reference segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    reference_labels : list, shape=(n,)\n        reference segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_intervals : np.ndarray, shape=(m, 2)\n        estimated segment intervals, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    estimated_labels : list, shape=(m,)\n        estimated segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n    frame_size : float > 0\n        length (in seconds) of frames for clustering\n        (Default value = 0.1)\n    beta : float > 0\n        beta for F-measure\n        (Default value = 1.0)\n\n    Returns\n    -------\n    V_precision\n        Over-clustering score:\n        ``1 - H(y_est | y_ref) / H(y_est)``\n\n        If `|y_est|==1`, then `V_precision` will be 0.\n\n    V_recall\n        Under-clustering score:\n        ``1 - H(y_ref | y_est) / H(y_ref)``\n\n        If `|y_ref|==1`, then `V_recall` will be 0.\n\n    V_F\n        F-measure for (V_precision, V_recall)\n\n    \"\"\"\n\n    return nce(reference_intervals, reference_labels,\n               estimated_intervals, estimated_labels,\n               frame_size=frame_size, beta=beta,\n               marginal=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evaluate(ref_intervals, ref_labels, est_intervals, est_labels, **kwargs):\n\n    # Adjust timespan of estimations relative to ground truth\n    ref_intervals, ref_labels = \\\n        util.adjust_intervals(ref_intervals, labels=ref_labels, t_min=0.0)\n\n    est_intervals, est_labels = \\\n        util.adjust_intervals(est_intervals, labels=est_labels, t_min=0.0,\n                              t_max=ref_intervals.max())\n\n    # Now compute all the metrics\n    scores = collections.OrderedDict()\n\n    # Boundary detection\n    # Force these values for window\n    kwargs['window'] = .5\n    scores['Precision@0.5'], scores['Recall@0.5'], scores['F-measure@0.5'] = \\\n        util.filter_kwargs(detection, ref_intervals, est_intervals, **kwargs)\n\n    kwargs['window'] = 3.0\n    scores['Precision@3.0'], scores['Recall@3.0'], scores['F-measure@3.0'] = \\\n        util.filter_kwargs(detection, ref_intervals, est_intervals, **kwargs)\n\n    # Boundary deviation\n    scores['Ref-to-est deviation'], scores['Est-to-ref deviation'] = \\\n        util.filter_kwargs(deviation, ref_intervals, est_intervals, **kwargs)\n\n    # Pairwise clustering\n    (scores['Pairwise Precision'],\n     scores['Pairwise Recall'],\n     scores['Pairwise F-measure']) = util.filter_kwargs(pairwise,\n                                                        ref_intervals,\n                                                        ref_labels,\n                                                        est_intervals,\n                                                        est_labels, **kwargs)\n\n    # Rand index\n    scores['Rand Index'] = util.filter_kwargs(rand_index, ref_intervals,\n                                              ref_labels, est_intervals,\n                                              est_labels, **kwargs)\n    # Adjusted rand index\n    scores['Adjusted Rand Index'] = util.filter_kwargs(ari, ref_intervals,\n                                                       ref_labels,\n                                                       est_intervals,\n                                                       est_labels, **kwargs)\n\n    # Mutual information metrics\n    (scores['Mutual Information'],\n     scores['Adjusted Mutual Information'],\n     scores['Normalized Mutual Information']) = \\\n        util.filter_kwargs(mutual_information, ref_intervals, ref_labels,\n                           est_intervals, est_labels, **kwargs)\n\n    # Conditional entropy metrics\n    scores['NCE Over'], scores['NCE Under'], scores['NCE F-measure'] = \\\n        util.filter_kwargs(nce, ref_intervals, ref_labels, est_intervals,\n                           est_labels, **kwargs)\n\n    # V-measure metrics\n    scores['V Precision'], scores['V Recall'], scores['V-measure'] = \\\n        util.filter_kwargs(vmeasure, ref_intervals, ref_labels, est_intervals,\n                           est_labels, **kwargs)\n\n    return scores", "response": "Compute all metrics for the given reference and estimated annotations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that there are two non - negative tempo values.", "response": "def validate_tempi(tempi, reference=True):\n    \"\"\"Checks that there are two non-negative tempi.\n    For a reference value, at least one tempo has to be greater than zero.\n\n    Parameters\n    ----------\n    tempi : np.ndarray\n        length-2 array of tempo, in bpm\n\n    reference : bool\n        indicates a reference value\n\n    \"\"\"\n\n    if tempi.size != 2:\n        raise ValueError('tempi must have exactly two values')\n\n    if not np.all(np.isfinite(tempi)) or np.any(tempi < 0):\n        raise ValueError('tempi={} must be non-negative numbers'.format(tempi))\n\n    if reference and np.all(tempi == 0):\n        raise ValueError('reference tempi={} must have one'\n                         ' value greater than zero'.format(tempi))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate(reference_tempi, reference_weight, estimated_tempi):\n    validate_tempi(reference_tempi, reference=True)\n    validate_tempi(estimated_tempi, reference=False)\n\n    if reference_weight < 0 or reference_weight > 1:\n        raise ValueError('Reference weight must lie in range [0, 1]')", "response": "Validates that the input annotations to a metric look like valid tempo\n    annotations."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the tempo detection accuracy metric.", "response": "def detection(reference_tempi, reference_weight, estimated_tempi, tol=0.08):\n    \"\"\"Compute the tempo detection accuracy metric.\n\n    Parameters\n    ----------\n    reference_tempi : np.ndarray, shape=(2,)\n        Two non-negative reference tempi\n\n    reference_weight : float > 0\n        The relative strength of ``reference_tempi[0]`` vs\n        ``reference_tempi[1]``.\n\n    estimated_tempi : np.ndarray, shape=(2,)\n        Two non-negative estimated tempi.\n\n    tol : float in [0, 1]:\n        The maximum allowable deviation from a reference tempo to\n        count as a hit.\n        ``|est_t - ref_t| <= tol * ref_t``\n        (Default value = 0.08)\n\n    Returns\n    -------\n    p_score : float in [0, 1]\n        Weighted average of recalls:\n        ``reference_weight * hits[0] + (1 - reference_weight) * hits[1]``\n\n    one_correct : bool\n        True if at least one reference tempo was correctly estimated\n\n    both_correct : bool\n        True if both reference tempi were correctly estimated\n\n    Raises\n    ------\n    ValueError\n        If the input tempi are ill-formed\n\n        If the reference weight is not in the range [0, 1]\n\n        If ``tol < 0`` or ``tol > 1``.\n    \"\"\"\n\n    validate(reference_tempi, reference_weight, estimated_tempi)\n\n    if tol < 0 or tol > 1:\n        raise ValueError('invalid tolerance {}: must lie in the range '\n                         '[0, 1]'.format(tol))\n    if tol == 0.:\n        warnings.warn('A tolerance of 0.0 may not '\n                      'lead to the results you expect.')\n\n    hits = [False, False]\n\n    for i, ref_t in enumerate(reference_tempi):\n        if ref_t > 0:\n            # Compute the relative error for this reference tempo\n            f_ref_t = float(ref_t)\n            relative_error = np.min(np.abs(ref_t - estimated_tempi) / f_ref_t)\n\n            # Count the hits\n            hits[i] = relative_error <= tol\n\n    p_score = reference_weight * hits[0] + (1.0-reference_weight) * hits[1]\n\n    one_correct = bool(np.max(hits))\n    both_correct = bool(np.min(hits))\n\n    return p_score, one_correct, both_correct"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute all metrics for the given reference and estimated annotations.", "response": "def evaluate(reference_tempi, reference_weight, estimated_tempi, **kwargs):\n    \"\"\"Compute all metrics for the given reference and estimated annotations.\n\n    Parameters\n    ----------\n    reference_tempi : np.ndarray, shape=(2,)\n        Two non-negative reference tempi\n\n    reference_weight : float > 0\n        The relative strength of ``reference_tempi[0]`` vs\n        ``reference_tempi[1]``.\n\n    estimated_tempi : np.ndarray, shape=(2,)\n        Two non-negative estimated tempi.\n\n    kwargs\n        Additional keyword arguments which will be passed to the\n        appropriate metric or preprocessing functions.\n\n    Returns\n    -------\n    scores : dict\n        Dictionary of scores, where the key is the metric name (str) and\n        the value is the (float) score achieved.\n    \"\"\"\n    # Compute all metrics\n    scores = collections.OrderedDict()\n\n    (scores['P-score'],\n     scores['One-correct'],\n     scores['Both-correct']) = util.filter_kwargs(detection, reference_tempi,\n                                                  reference_weight,\n                                                  estimated_tempi,\n                                                  **kwargs)\n\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(ref_time, ref_freqs, est_time, est_freqs):\n\n    util.validate_events(ref_time, max_time=MAX_TIME)\n    util.validate_events(est_time, max_time=MAX_TIME)\n\n    if ref_time.size == 0:\n        warnings.warn(\"Reference times are empty.\")\n    if ref_time.ndim != 1:\n        raise ValueError(\"Reference times have invalid dimension\")\n    if len(ref_freqs) == 0:\n        warnings.warn(\"Reference frequencies are empty.\")\n    if est_time.size == 0:\n        warnings.warn(\"Estimated times are empty.\")\n    if est_time.ndim != 1:\n        raise ValueError(\"Estimated times have invalid dimension\")\n    if len(est_freqs) == 0:\n        warnings.warn(\"Estimated frequencies are empty.\")\n    if ref_time.size != len(ref_freqs):\n        raise ValueError('Reference times and frequencies have unequal '\n                         'lengths.')\n    if est_time.size != len(est_freqs):\n        raise ValueError('Estimate times and frequencies have unequal '\n                         'lengths.')\n\n    for freq in ref_freqs:\n        util.validate_frequencies(freq, max_freq=MAX_FREQ, min_freq=MIN_FREQ,\n                                  allow_negatives=False)\n\n    for freq in est_freqs:\n        util.validate_frequencies(freq, max_freq=MAX_FREQ, min_freq=MIN_FREQ,\n                                  allow_negatives=False)", "response": "Checks that the time and frequency inputs are well - formed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_num_true_positives(ref_freqs, est_freqs, window=0.5, chroma=False):\n    n_frames = len(ref_freqs)\n    true_positives = np.zeros((n_frames, ))\n\n    for i, (ref_frame, est_frame) in enumerate(zip(ref_freqs, est_freqs)):\n        if chroma:\n            # match chroma-wrapped frequency events\n            matching = util.match_events(\n                ref_frame, est_frame, window,\n                distance=util._outer_distance_mod_n)\n        else:\n            # match frequency events within tolerance window in semitones\n            matching = util.match_events(ref_frame, est_frame, window)\n\n        true_positives[i] = len(matching)\n\n    return true_positives", "response": "Compute the number of true positives in an estimate given a reference."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_accuracy(true_positives, n_ref, n_est):\n    true_positive_sum = float(true_positives.sum())\n\n    n_est_sum = n_est.sum()\n    if n_est_sum > 0:\n        precision = true_positive_sum/n_est.sum()\n    else:\n        warnings.warn(\"Estimate frequencies are all empty.\")\n        precision = 0.0\n\n    n_ref_sum = n_ref.sum()\n    if n_ref_sum > 0:\n        recall = true_positive_sum/n_ref.sum()\n    else:\n        warnings.warn(\"Reference frequencies are all empty.\")\n        recall = 0.0\n\n    acc_denom = (n_est + n_ref - true_positives).sum()\n    if acc_denom > 0:\n        acc = true_positive_sum/acc_denom\n    else:\n        acc = 0.0\n\n    return precision, recall, acc", "response": "Compute the accuracy of a single node in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the error score of the current and current true positives at each time point.", "response": "def compute_err_score(true_positives, n_ref, n_est):\n    \"\"\"Compute error score metrics.\n\n    Parameters\n    ----------\n    true_positives : np.ndarray\n        Array containing the number of true positives at each time point.\n    n_ref : np.ndarray\n        Array containing the number of reference frequencies at each time\n        point.\n    n_est : np.ndarray\n        Array containing the number of estimate frequencies at each time point.\n\n    Returns\n    -------\n    e_sub : float\n        Substitution error\n    e_miss : float\n        Miss error\n    e_fa : float\n        False alarm error\n    e_tot : float\n        Total error\n\n    \"\"\"\n    n_ref_sum = float(n_ref.sum())\n\n    if n_ref_sum == 0:\n        warnings.warn(\"Reference frequencies are all empty.\")\n        return 0., 0., 0., 0.\n\n    # Substitution error\n    e_sub = (np.min([n_ref, n_est], axis=0) - true_positives).sum()/n_ref_sum\n\n    # compute the max of (n_ref - n_est) and 0\n    e_miss_numerator = n_ref - n_est\n    e_miss_numerator[e_miss_numerator < 0] = 0\n    # Miss error\n    e_miss = e_miss_numerator.sum()/n_ref_sum\n\n    # compute the max of (n_est - n_ref) and 0\n    e_fa_numerator = n_est - n_ref\n    e_fa_numerator[e_fa_numerator < 0] = 0\n    # False alarm error\n    e_fa = e_fa_numerator.sum()/n_ref_sum\n\n    # total error\n    e_tot = (np.max([n_ref, n_est], axis=0) - true_positives).sum()/n_ref_sum\n\n    return e_sub, e_miss, e_fa, e_tot"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute multipitch metrics at the macro level .", "response": "def metrics(ref_time, ref_freqs, est_time, est_freqs, **kwargs):\n    \"\"\"Compute multipitch metrics. All metrics are computed at the 'macro' level\n    such that the frame true positive/false positive/false negative rates are\n    summed across time and the metrics are computed on the combined values.\n\n    Examples\n    --------\n    >>> ref_time, ref_freqs = mir_eval.io.load_ragged_time_series(\n    ...     'reference.txt')\n    >>> est_time, est_freqs = mir_eval.io.load_ragged_time_series(\n    ...     'estimated.txt')\n    >>> metris_tuple = mir_eval.multipitch.metrics(\n    ...     ref_time, ref_freqs, est_time, est_freqs)\n\n    Parameters\n    ----------\n    ref_time : np.ndarray\n        Time of each reference frequency value\n    ref_freqs : list of np.ndarray\n        List of np.ndarrays of reference frequency values\n    est_time : np.ndarray\n        Time of each estimated frequency value\n    est_freqs : list of np.ndarray\n        List of np.ndarrays of estimate frequency values\n    kwargs\n        Additional keyword arguments which will be passed to the\n        appropriate metric or preprocessing functions.\n\n    Returns\n    -------\n    precision : float\n        Precision (TP/(TP + FP))\n    recall : float\n        Recall (TP/(TP + FN))\n    accuracy : float\n        Accuracy (TP/(TP + FP + FN))\n    e_sub : float\n        Substitution error\n    e_miss : float\n        Miss error\n    e_fa : float\n        False alarm error\n    e_tot : float\n        Total error\n    precision_chroma : float\n        Chroma precision\n    recall_chroma : float\n        Chroma recall\n    accuracy_chroma : float\n        Chroma accuracy\n    e_sub_chroma : float\n        Chroma substitution error\n    e_miss_chroma : float\n        Chroma miss error\n    e_fa_chroma : float\n        Chroma false alarm error\n    e_tot_chroma : float\n        Chroma total error\n\n    \"\"\"\n    validate(ref_time, ref_freqs, est_time, est_freqs)\n\n    # resample est_freqs if est_times is different from ref_times\n    if est_time.size != ref_time.size or not np.allclose(est_time, ref_time):\n        warnings.warn(\"Estimate times not equal to reference times. \"\n                      \"Resampling to common time base.\")\n        est_freqs = resample_multipitch(est_time, est_freqs, ref_time)\n\n    # convert frequencies from Hz to continuous midi note number\n    ref_freqs_midi = frequencies_to_midi(ref_freqs)\n    est_freqs_midi = frequencies_to_midi(est_freqs)\n\n    # compute chroma wrapped midi number\n    ref_freqs_chroma = midi_to_chroma(ref_freqs_midi)\n    est_freqs_chroma = midi_to_chroma(est_freqs_midi)\n\n    # count number of occurences\n    n_ref = compute_num_freqs(ref_freqs_midi)\n    n_est = compute_num_freqs(est_freqs_midi)\n\n    # compute the number of true positives\n    true_positives = util.filter_kwargs(\n        compute_num_true_positives, ref_freqs_midi, est_freqs_midi, **kwargs)\n\n    # compute the number of true positives ignoring octave mistakes\n    true_positives_chroma = util.filter_kwargs(\n        compute_num_true_positives, ref_freqs_chroma,\n        est_freqs_chroma, chroma=True, **kwargs)\n\n    # compute accuracy metrics\n    precision, recall, accuracy = compute_accuracy(\n        true_positives, n_ref, n_est)\n\n    # compute error metrics\n    e_sub, e_miss, e_fa, e_tot = compute_err_score(\n        true_positives, n_ref, n_est)\n\n    # compute accuracy metrics ignoring octave mistakes\n    precision_chroma, recall_chroma, accuracy_chroma = compute_accuracy(\n        true_positives_chroma, n_ref, n_est)\n\n    # compute error metrics ignoring octave mistakes\n    e_sub_chroma, e_miss_chroma, e_fa_chroma, e_tot_chroma = compute_err_score(\n        true_positives_chroma, n_ref, n_est)\n\n    return (precision, recall, accuracy, e_sub, e_miss, e_fa, e_tot,\n            precision_chroma, recall_chroma, accuracy_chroma, e_sub_chroma,\n            e_miss_chroma, e_fa_chroma, e_tot_chroma)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate(ref_time, ref_freqs, est_time, est_freqs, **kwargs):\n    scores = collections.OrderedDict()\n\n    (scores['Precision'],\n     scores['Recall'],\n     scores['Accuracy'],\n     scores['Substitution Error'],\n     scores['Miss Error'],\n     scores['False Alarm Error'],\n     scores['Total Error'],\n     scores['Chroma Precision'],\n     scores['Chroma Recall'],\n     scores['Chroma Accuracy'],\n     scores['Chroma Substitution Error'],\n     scores['Chroma Miss Error'],\n     scores['Chroma False Alarm Error'],\n     scores['Chroma Total Error']) = util.filter_kwargs(\n         metrics, ref_time, ref_freqs, est_time, est_freqs, **kwargs)\n\n    return scores", "response": "Evaluate two multipitch transcriptions where the first is the reference and the second is the estimated."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _hierarchy_bounds(intervals_hier):\n    '''Compute the covered time range of a hierarchical segmentation.\n\n    Parameters\n    ----------\n    intervals_hier : list of ndarray\n        A hierarchical segmentation, encoded as a list of arrays of segment\n        intervals.\n\n    Returns\n    -------\n    t_min : float\n    t_max : float\n        The minimum and maximum times spanned by the annotation\n    '''\n    boundaries = list(itertools.chain(*list(itertools.chain(*intervals_hier))))\n\n    return min(boundaries), max(boundaries)", "response": "Compute the covered time range of a hierarchical segmentation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\naligning a hierarchical segment annotation to span a fixed start and end time.", "response": "def _align_intervals(int_hier, lab_hier, t_min=0.0, t_max=None):\n    '''Align a hierarchical annotation to span a fixed start and end time.\n\n    Parameters\n    ----------\n    int_hier : list of list of intervals\n    lab_hier : list of list of str\n        Hierarchical segment annotations, encoded as a\n        list of list of intervals (int_hier) and list of\n        list of strings (lab_hier)\n\n    t_min : None or number >= 0\n        The minimum time value for the segmentation\n\n    t_max : None or number >= t_min\n        The maximum time value for the segmentation\n\n    Returns\n    -------\n    intervals_hier : list of list of intervals\n    labels_hier : list of list of str\n        `int_hier` `lab_hier` aligned to span `[t_min, t_max]`.\n    '''\n    return [list(_) for _ in zip(*[util.adjust_intervals(np.asarray(ival),\n                                                         labels=lab,\n                                                         t_min=t_min,\n                                                         t_max=t_max)\n                                   for ival, lab in zip(int_hier, lab_hier)])]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the least common ancestor matrix for a single segment at a specific level.", "response": "def _lca(intervals_hier, frame_size):\n    '''Compute the (sparse) least-common-ancestor (LCA) matrix for a\n    hierarchical segmentation.\n\n    For any pair of frames ``(s, t)``, the LCA is the deepest level in\n    the hierarchy such that ``(s, t)`` are contained within a single\n    segment at that level.\n\n    Parameters\n    ----------\n    intervals_hier : list of ndarray\n        An ordered list of segment interval arrays.\n        The list is assumed to be ordered by increasing specificity (depth).\n\n    frame_size : number\n        The length of the sample frames (in seconds)\n\n    Returns\n    -------\n    lca_matrix : scipy.sparse.csr_matrix\n        A sparse matrix such that ``lca_matrix[i, j]`` contains the depth\n        of the deepest segment containing frames ``i`` and ``j``.\n    '''\n\n    frame_size = float(frame_size)\n\n    # Figure out how many frames we need\n\n    n_start, n_end = _hierarchy_bounds(intervals_hier)\n\n    n = int((_round(n_end, frame_size) -\n             _round(n_start, frame_size)) / frame_size)\n\n    # Initialize the LCA matrix\n    lca_matrix = scipy.sparse.lil_matrix((n, n), dtype=np.uint8)\n\n    for level, intervals in enumerate(intervals_hier, 1):\n        for ival in (_round(np.asarray(intervals),\n                            frame_size) / frame_size).astype(int):\n            idx = slice(ival[0], ival[1])\n            lca_matrix[idx, idx] = level\n\n    return lca_matrix.tocsr()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the least common ancestor matrix for a single segment hierarchy.", "response": "def _meet(intervals_hier, labels_hier, frame_size):\n    '''Compute the (sparse) least-common-ancestor (LCA) matrix for a\n    hierarchical segmentation.\n\n    For any pair of frames ``(s, t)``, the LCA is the deepest level in\n    the hierarchy such that ``(s, t)`` are contained within a single\n    segment at that level.\n\n    Parameters\n    ----------\n    intervals_hier : list of ndarray\n        An ordered list of segment interval arrays.\n        The list is assumed to be ordered by increasing specificity (depth).\n\n    labels_hier : list of list of str\n        ``labels_hier[i]`` contains the segment labels for the\n        ``i``th layer of the annotations\n\n    frame_size : number\n        The length of the sample frames (in seconds)\n\n    Returns\n    -------\n    meet_matrix : scipy.sparse.csr_matrix\n        A sparse matrix such that ``meet_matrix[i, j]`` contains the depth\n        of the deepest segment label containing both ``i`` and ``j``.\n    '''\n\n    frame_size = float(frame_size)\n\n    # Figure out how many frames we need\n\n    n_start, n_end = _hierarchy_bounds(intervals_hier)\n\n    n = int((_round(n_end, frame_size) -\n             _round(n_start, frame_size)) / frame_size)\n\n    # Initialize the meet matrix\n    meet_matrix = scipy.sparse.lil_matrix((n, n), dtype=np.uint8)\n\n    for level, (intervals, labels) in enumerate(zip(intervals_hier,\n                                                    labels_hier), 1):\n\n        # Encode the labels at this level\n        lab_enc = util.index_labels(labels)[0]\n\n        # Find unique agreements\n        int_agree = np.triu(np.equal.outer(lab_enc, lab_enc))\n\n        # Map intervals to frame indices\n        int_frames = (_round(intervals, frame_size) / frame_size).astype(int)\n\n        # For each intervals i, j where labels agree, update the meet matrix\n        for (seg_i, seg_j) in zip(*np.where(int_agree)):\n            idx_i = slice(*list(int_frames[seg_i]))\n            idx_j = slice(*list(int_frames[seg_j]))\n            meet_matrix[idx_i, idx_j] = level\n            if seg_i != seg_j:\n                meet_matrix[idx_j, idx_i] = level\n\n    return scipy.sparse.csr_matrix(meet_matrix)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngeneralize area under the curve GAUC.", "response": "def _gauc(ref_lca, est_lca, transitive, window):\n    '''Generalized area under the curve (GAUC)\n\n    This function computes the normalized recall score for correctly\n    ordering triples ``(q, i, j)`` where frames ``(q, i)`` are closer than\n    ``(q, j)`` in the reference annotation.\n\n    Parameters\n    ----------\n    ref_lca : scipy.sparse\n    est_lca : scipy.sparse\n        The least common ancestor matrices for the reference and\n        estimated annotations\n\n    transitive : bool\n        If True, then transitive comparisons are counted, meaning that\n        ``(q, i)`` and ``(q, j)`` can differ by any number of levels.\n\n        If False, then ``(q, i)`` and ``(q, j)`` can differ by exactly one\n        level.\n\n    window : number or None\n        The maximum number of frames to consider for each query.\n        If `None`, then all frames are considered.\n\n    Returns\n    -------\n    score : number [0, 1]\n        The percentage of reference triples correctly ordered by\n        the estimation.\n\n    Raises\n    ------\n    ValueError\n        If ``ref_lca`` and ``est_lca`` have different shapes\n    '''\n    # Make sure we have the right number of frames\n\n    if ref_lca.shape != est_lca.shape:\n        raise ValueError('Estimated and reference hierarchies '\n                         'must have the same shape.')\n\n    # How many frames?\n    n = ref_lca.shape[0]\n\n    # By default, the window covers the entire track\n    if window is None:\n        window = n\n\n    # Initialize the score\n    score = 0.0\n\n    # Iterate over query frames\n    num_frames = 0\n\n    for query in range(n):\n\n        # Find all pairs i,j such that ref_lca[q, i] > ref_lca[q, j]\n        results = slice(max(0, query - window), min(n, query + window))\n\n        ref_score = ref_lca[query, results]\n        est_score = est_lca[query, results]\n\n        # Densify the results\n        ref_score = ref_score.toarray().squeeze()\n        est_score = est_score.toarray().squeeze()\n\n        # Don't count the query as a result\n        # when query < window, query itself is the index within the slice\n        # otherwise, query is located at the center of the slice, window\n        # (this also holds when the slice goes off the end of the array.)\n        idx = min(query, window)\n\n        ref_score = np.concatenate((ref_score[:idx], ref_score[idx+1:]))\n        est_score = np.concatenate((est_score[:idx], est_score[idx+1:]))\n\n        inversions, normalizer = _compare_frame_rankings(ref_score, est_score,\n                                                         transitive=transitive)\n\n        if normalizer:\n            score += 1.0 - inversions / float(normalizer)\n            num_frames += 1\n\n    # Normalize by the number of frames counted.\n    # If no frames are counted, take the convention 0/0 -> 0\n    if num_frames:\n        score /= float(num_frames)\n    else:\n        score = 0.0\n\n    return score"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _count_inversions(a, b):\n    '''Count the number of inversions in two numpy arrays:\n\n    # points i, j where a[i] >= b[j]\n\n    Parameters\n    ----------\n    a, b : np.ndarray, shape=(n,) (m,)\n        The arrays to be compared.\n\n        This implementation is optimized for arrays with many\n        repeated values.\n\n    Returns\n    -------\n    inversions : int\n        The number of detected inversions\n    '''\n\n    a, a_counts = np.unique(a, return_counts=True)\n    b, b_counts = np.unique(b, return_counts=True)\n\n    inversions = 0\n    i = 0\n    j = 0\n\n    while i < len(a) and j < len(b):\n        if a[i] < b[j]:\n            i += 1\n        elif a[i] >= b[j]:\n            inversions += np.sum(a_counts[i:]) * b_counts[j]\n            j += 1\n\n    return inversions", "response": "Count the number of inversions in two numpy arrays a and b."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the number of ranking disagreements in two lists.", "response": "def _compare_frame_rankings(ref, est, transitive=False):\n    '''Compute the number of ranking disagreements in two lists.\n\n    Parameters\n    ----------\n    ref : np.ndarray, shape=(n,)\n    est : np.ndarray, shape=(n,)\n        Reference and estimate ranked lists.\n        `ref[i]` is the relevance score for point `i`.\n\n    transitive : bool\n        If true, all pairs of reference levels are compared.\n        If false, only adjacent pairs of reference levels are compared.\n\n    Returns\n    -------\n    inversions : int\n        The number of pairs of indices `i, j` where\n        `ref[i] < ref[j]` but `est[i] >= est[j]`.\n\n    normalizer : float\n        The total number of pairs (i, j) under consideration.\n        If transitive=True, then this is |{(i,j) : ref[i] < ref[j]}|\n        If transitive=False, then this is |{i,j) : ref[i] +1 = ref[j]}|\n    '''\n\n    idx = np.argsort(ref)\n    ref_sorted = ref[idx]\n    est_sorted = est[idx]\n\n    # Find the break-points in ref_sorted\n    levels, positions, counts = np.unique(ref_sorted,\n                                          return_index=True,\n                                          return_counts=True)\n\n    positions = list(positions)\n    positions.append(len(ref_sorted))\n\n    index = collections.defaultdict(lambda: slice(0))\n    ref_map = collections.defaultdict(lambda: 0)\n\n    for level, cnt, start, end in zip(levels, counts,\n                                      positions[:-1], positions[1:]):\n        index[level] = slice(start, end)\n        ref_map[level] = cnt\n\n    # Now that we have values sorted, apply the inversion-counter to\n    # pairs of reference values\n    if transitive:\n        level_pairs = itertools.combinations(levels, 2)\n    else:\n        level_pairs = [(i, i+1) for i in levels]\n\n    level_pairs, lcounter = itertools.tee(level_pairs)\n\n    normalizer = float(sum([ref_map[i] * ref_map[j] for (i, j) in lcounter]))\n\n    if normalizer == 0:\n        return 0, 0.0\n\n    inversions = 0\n\n    for level_1, level_2 in level_pairs:\n        inversions += _count_inversions(est_sorted[index[level_1]],\n                                        est_sorted[index[level_2]])\n\n    return inversions, float(normalizer)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_hier_intervals(intervals_hier):\n    '''Validate a hierarchical segment annotation.\n\n    Parameters\n    ----------\n    intervals_hier : ordered list of segmentations\n\n    Raises\n    ------\n    ValueError\n        If any segmentation does not span the full duration of the top-level\n        segmentation.\n\n        If any segmentation does not start at 0.\n    '''\n\n    # Synthesize a label array for the top layer.\n    label_top = util.generate_labels(intervals_hier[0])\n\n    boundaries = set(util.intervals_to_boundaries(intervals_hier[0]))\n\n    for level, intervals in enumerate(intervals_hier[1:], 1):\n        # Make sure this level is consistent with the root\n        label_current = util.generate_labels(intervals)\n        validate_structure(intervals_hier[0], label_top,\n                           intervals, label_current)\n\n        # Make sure all previous boundaries are accounted for\n        new_bounds = set(util.intervals_to_boundaries(intervals))\n\n        if boundaries - new_bounds:\n            warnings.warn('Segment hierarchy is inconsistent '\n                          'at level {:d}'.format(level))\n        boundaries |= new_bounds", "response": "Validate a hierarchical segment annotation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tmeasure(reference_intervals_hier, estimated_intervals_hier,\n             transitive=False, window=15.0, frame_size=0.1, beta=1.0):\n    '''Computes the tree measures for hierarchical segment annotations.\n\n    Parameters\n    ----------\n    reference_intervals_hier : list of ndarray\n        ``reference_intervals_hier[i]`` contains the segment intervals\n        (in seconds) for the ``i`` th layer of the annotations.  Layers are\n        ordered from top to bottom, so that the last list of intervals should\n        be the most specific.\n\n    estimated_intervals_hier : list of ndarray\n        Like ``reference_intervals_hier`` but for the estimated annotation\n\n    transitive : bool\n        whether to compute the t-measures using transitivity or not.\n\n    window : float > 0\n        size of the window (in seconds).  For each query frame q,\n        result frames are only counted within q +- window.\n\n    frame_size : float > 0\n        length (in seconds) of frames.  The frame size cannot be longer than\n        the window.\n\n    beta : float > 0\n        beta parameter for the F-measure.\n\n    Returns\n    -------\n    t_precision : number [0, 1]\n        T-measure Precision\n\n    t_recall : number [0, 1]\n        T-measure Recall\n\n    t_measure : number [0, 1]\n        F-beta measure for ``(t_precision, t_recall)``\n\n    Raises\n    ------\n    ValueError\n        If either of the input hierarchies are inconsistent\n\n        If the input hierarchies have different time durations\n\n        If ``frame_size > window`` or ``frame_size <= 0``\n    '''\n\n    # Compute the number of frames in the window\n    if frame_size <= 0:\n        raise ValueError('frame_size ({:.2f}) must be a positive '\n                         'number.'.format(frame_size))\n\n    if window is None:\n        window_frames = None\n    else:\n        if frame_size > window:\n            raise ValueError('frame_size ({:.2f}) cannot exceed '\n                             'window ({:.2f})'.format(frame_size, window))\n\n        window_frames = int(_round(window, frame_size) / frame_size)\n\n    # Validate the hierarchical segmentations\n    validate_hier_intervals(reference_intervals_hier)\n    validate_hier_intervals(estimated_intervals_hier)\n\n    # Build the least common ancestor matrices\n    ref_lca = _lca(reference_intervals_hier, frame_size)\n    est_lca = _lca(estimated_intervals_hier, frame_size)\n\n    # Compute precision and recall\n    t_recall = _gauc(ref_lca, est_lca, transitive, window_frames)\n    t_precision = _gauc(est_lca, ref_lca, transitive, window_frames)\n\n    t_measure = util.f_measure(t_precision, t_recall, beta=beta)\n\n    return t_precision, t_recall, t_measure", "response": "Computes the tree measures for hierarchical segment annotations."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the tree measures for hierarchical segment annotations.", "response": "def lmeasure(reference_intervals_hier, reference_labels_hier,\n             estimated_intervals_hier, estimated_labels_hier,\n             frame_size=0.1, beta=1.0):\n    '''Computes the tree measures for hierarchical segment annotations.\n\n    Parameters\n    ----------\n    reference_intervals_hier : list of ndarray\n        ``reference_intervals_hier[i]`` contains the segment intervals\n        (in seconds) for the ``i`` th layer of the annotations.  Layers are\n        ordered from top to bottom, so that the last list of intervals should\n        be the most specific.\n\n    reference_labels_hier : list of list of str\n        ``reference_labels_hier[i]`` contains the segment labels for the\n        ``i``th layer of the annotations\n\n    estimated_intervals_hier : list of ndarray\n    estimated_labels_hier : list of ndarray\n        Like ``reference_intervals_hier`` and ``reference_labels_hier``\n        but for the estimated annotation\n\n    frame_size : float > 0\n        length (in seconds) of frames.  The frame size cannot be longer than\n        the window.\n\n    beta : float > 0\n        beta parameter for the F-measure.\n\n    Returns\n    -------\n    l_precision : number [0, 1]\n        L-measure Precision\n\n    l_recall : number [0, 1]\n        L-measure Recall\n\n    l_measure : number [0, 1]\n        F-beta measure for ``(l_precision, l_recall)``\n\n    Raises\n    ------\n    ValueError\n        If either of the input hierarchies are inconsistent\n\n        If the input hierarchies have different time durations\n\n        If ``frame_size > window`` or ``frame_size <= 0``\n    '''\n\n    # Compute the number of frames in the window\n    if frame_size <= 0:\n        raise ValueError('frame_size ({:.2f}) must be a positive '\n                         'number.'.format(frame_size))\n\n    # Validate the hierarchical segmentations\n    validate_hier_intervals(reference_intervals_hier)\n    validate_hier_intervals(estimated_intervals_hier)\n\n    # Build the least common ancestor matrices\n    ref_meet = _meet(reference_intervals_hier, reference_labels_hier,\n                     frame_size)\n    est_meet = _meet(estimated_intervals_hier, estimated_labels_hier,\n                     frame_size)\n\n    # Compute precision and recall\n    l_recall = _gauc(ref_meet, est_meet, True, None)\n    l_precision = _gauc(est_meet, ref_meet, True, None)\n\n    l_measure = util.f_measure(l_precision, l_recall, beta=beta)\n\n    return l_precision, l_recall, l_measure"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing all hierarchical structure metrics for the given reference and estimated annotations.", "response": "def evaluate(ref_intervals_hier, ref_labels_hier,\n             est_intervals_hier, est_labels_hier, **kwargs):\n    '''Compute all hierarchical structure metrics for the given reference and\n    estimated annotations.\n\n    Examples\n    --------\n    A toy example with two two-layer annotations\n\n    >>> ref_i = [[[0, 30], [30, 60]], [[0, 15], [15, 30], [30, 45], [45, 60]]]\n    >>> est_i = [[[0, 45], [45, 60]], [[0, 15], [15, 30], [30, 45], [45, 60]]]\n    >>> ref_l = [ ['A', 'B'], ['a', 'b', 'a', 'c'] ]\n    >>> est_l = [ ['A', 'B'], ['a', 'a', 'b', 'b'] ]\n    >>> scores = mir_eval.hierarchy.evaluate(ref_i, ref_l, est_i, est_l)\n    >>> dict(scores)\n    {'T-Measure full': 0.94822745804853459,\n     'T-Measure reduced': 0.8732458222764804,\n     'T-Precision full': 0.96569179094693058,\n     'T-Precision reduced': 0.89939075137018787,\n     'T-Recall full': 0.93138358189386117,\n     'T-Recall reduced': 0.84857799953694923}\n\n    A more realistic example, using SALAMI pre-parsed annotations\n\n    >>> def load_salami(filename):\n    ...     \"load SALAMI event format as labeled intervals\"\n    ...     events, labels = mir_eval.io.load_labeled_events(filename)\n    ...     intervals = mir_eval.util.boundaries_to_intervals(events)[0]\n    ...     return intervals, labels[:len(intervals)]\n    >>> ref_files = ['data/10/parsed/textfile1_uppercase.txt',\n    ...              'data/10/parsed/textfile1_lowercase.txt']\n    >>> est_files = ['data/10/parsed/textfile2_uppercase.txt',\n    ...              'data/10/parsed/textfile2_lowercase.txt']\n    >>> ref = [load_salami(fname) for fname in ref_files]\n    >>> ref_int = [seg[0] for seg in ref]\n    >>> ref_lab = [seg[1] for seg in ref]\n    >>> est = [load_salami(fname) for fname in est_files]\n    >>> est_int = [seg[0] for seg in est]\n    >>> est_lab = [seg[1] for seg in est]\n    >>> scores = mir_eval.hierarchy.evaluate(ref_int, ref_lab,\n    ...                                      est_hier, est_lab)\n    >>> dict(scores)\n    {'T-Measure full': 0.66029225561405358,\n     'T-Measure reduced': 0.62001868041578034,\n     'T-Precision full': 0.66844764668949885,\n     'T-Precision reduced': 0.63252297209957919,\n     'T-Recall full': 0.6523334654992341,\n     'T-Recall reduced': 0.60799919710921635}\n\n\n    Parameters\n    ----------\n    ref_intervals_hier : list of list-like\n    ref_labels_hier : list of list of str\n    est_intervals_hier : list of list-like\n    est_labels_hier : list of list of str\n        Hierarchical annotations are encoded as an ordered list\n        of segmentations.  Each segmentation itself is a list (or list-like)\n        of intervals (\\*_intervals_hier) and a list of lists of labels\n        (\\*_labels_hier).\n\n    kwargs\n        additional keyword arguments to the evaluation metrics.\n\n    Returns\n    -------\n    scores :  OrderedDict\n        Dictionary of scores, where the key is the metric name (str) and\n        the value is the (float) score achieved.\n\n        T-measures are computed in both the \"full\" (``transitive=True``) and\n        \"reduced\" (``transitive=False``) modes.\n\n    Raises\n    ------\n    ValueError\n        Thrown when the provided annotations are not valid.\n    '''\n\n    # First, find the maximum length of the reference\n    _, t_end = _hierarchy_bounds(ref_intervals_hier)\n\n    # Pre-process the intervals to match the range of the reference,\n    # and start at 0\n    ref_intervals_hier, ref_labels_hier = _align_intervals(ref_intervals_hier,\n                                                           ref_labels_hier,\n                                                           t_min=0.0,\n                                                           t_max=None)\n\n    est_intervals_hier, est_labels_hier = _align_intervals(est_intervals_hier,\n                                                           est_labels_hier,\n                                                           t_min=0.0,\n                                                           t_max=t_end)\n\n    scores = collections.OrderedDict()\n\n    # Force the transitivity setting\n    kwargs['transitive'] = False\n    (scores['T-Precision reduced'],\n     scores['T-Recall reduced'],\n     scores['T-Measure reduced']) = util.filter_kwargs(tmeasure,\n                                                       ref_intervals_hier,\n                                                       est_intervals_hier,\n                                                       **kwargs)\n\n    kwargs['transitive'] = True\n    (scores['T-Precision full'],\n     scores['T-Recall full'],\n     scores['T-Measure full']) = util.filter_kwargs(tmeasure,\n                                                    ref_intervals_hier,\n                                                    est_intervals_hier,\n                                                    **kwargs)\n\n    (scores['L-Precision'],\n     scores['L-Recall'],\n     scores['L-Measure']) = util.filter_kwargs(lmeasure,\n                                               ref_intervals_hier,\n                                               ref_labels_hier,\n                                               est_intervals_hier,\n                                               est_labels_hier,\n                                               **kwargs)\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __get_axes(ax=None, fig=None):\n    '''Get or construct the target axes object for a new plot.\n\n    Parameters\n    ----------\n    ax : matplotlib.pyplot.axes, optional\n        If provided, return this axes object directly.\n\n    fig : matplotlib.figure.Figure, optional\n        The figure to query for axes.\n\n        By default, uses the current figure `plt.gcf()`.\n\n    Returns\n    -------\n    ax : matplotlib.pyplot.axes\n        An axis handle on which to draw the segmentation.\n        If none is provided, a new set of axes is created.\n\n    new_axes : bool\n        If `True`, the axis object was newly constructed.\n        If `False`, the axis object already existed.\n\n    '''\n\n    new_axes = False\n\n    if ax is not None:\n        return ax, new_axes\n\n    if fig is None:\n        import matplotlib.pyplot as plt\n        fig = plt.gcf()\n\n    if not fig.get_axes():\n        new_axes = True\n\n    return fig.gca(), new_axes", "response": "Get or construct the target axes object for a new plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots a segmentation as a set of disjoint rectangles.", "response": "def segments(intervals, labels, base=None, height=None, text=False,\n             text_kw=None, ax=None, **kwargs):\n    '''Plot a segmentation as a set of disjoint rectangles.\n\n    Parameters\n    ----------\n    intervals : np.ndarray, shape=(n, 2)\n        segment intervals, in the format returned by\n        :func:`mir_eval.io.load_intervals` or\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    labels : list, shape=(n,)\n        reference segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    base : number\n        The vertical position of the base of the rectangles.\n        By default, this will be the bottom of the plot.\n\n    height : number\n        The height of the rectangles.\n        By default, this will be the top of the plot (minus ``base``).\n\n    text : bool\n        If true, each segment's label is displayed in its\n        upper-left corner\n\n    text_kw : dict\n        If ``text == True``, the properties of the text\n        object can be specified here.\n        See ``matplotlib.pyplot.Text`` for valid parameters\n\n    ax : matplotlib.pyplot.axes\n        An axis handle on which to draw the segmentation.\n        If none is provided, a new set of axes is created.\n\n    kwargs\n        Additional keyword arguments to pass to\n        ``matplotlib.patches.Rectangle``.\n\n    Returns\n    -------\n    ax : matplotlib.pyplot.axes._subplots.AxesSubplot\n        A handle to the (possibly constructed) plot axes\n    '''\n    if text_kw is None:\n        text_kw = dict()\n    text_kw.setdefault('va', 'top')\n    text_kw.setdefault('clip_on', True)\n    text_kw.setdefault('bbox', dict(boxstyle='round', facecolor='white'))\n\n    # Make sure we have a numpy array\n    intervals = np.atleast_2d(intervals)\n\n    seg_def_style = dict(linewidth=1)\n\n    ax, new_axes = __get_axes(ax=ax)\n\n    if new_axes:\n        ax.set_ylim([0, 1])\n\n    # Infer height\n    if base is None:\n        base = ax.get_ylim()[0]\n\n    if height is None:\n        height = ax.get_ylim()[1]\n\n    cycler = ax._get_patches_for_fill.prop_cycler\n\n    seg_map = dict()\n\n    for lab in labels:\n        if lab in seg_map:\n            continue\n\n        style = next(cycler)\n        seg_map[lab] = seg_def_style.copy()\n        seg_map[lab].update(style)\n        # Swap color -> facecolor here so we preserve edgecolor on rects\n        seg_map[lab]['facecolor'] = seg_map[lab].pop('color')\n        seg_map[lab].update(kwargs)\n        seg_map[lab]['label'] = lab\n\n    for ival, lab in zip(intervals, labels):\n        rect = Rectangle((ival[0], base), ival[1] - ival[0], height,\n                         **seg_map[lab])\n        ax.add_patch(rect)\n        seg_map[lab].pop('label', None)\n\n        if text:\n            ann = ax.annotate(lab,\n                              xy=(ival[0], height), xycoords='data',\n                              xytext=(8, -10), textcoords='offset points',\n                              **text_kw)\n            ann.set_clip_path(rect)\n\n    if new_axes:\n        ax.set_yticks([])\n\n    # Only expand if we have data\n    if intervals.size:\n        __expand_limits(ax, [intervals.min(), intervals.max()], which='x')\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef labeled_intervals(intervals, labels, label_set=None,\n                      base=None, height=None, extend_labels=True,\n                      ax=None, tick=True, **kwargs):\n    '''Plot labeled intervals with each label on its own row.\n\n    Parameters\n    ----------\n    intervals : np.ndarray, shape=(n, 2)\n        segment intervals, in the format returned by\n        :func:`mir_eval.io.load_intervals` or\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    labels : list, shape=(n,)\n        reference segment labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_intervals`.\n\n    label_set : list\n        An (ordered) list of labels to determine the plotting order.\n        If not provided, the labels will be inferred from\n        ``ax.get_yticklabels()``.\n        If no ``yticklabels`` exist, then the sorted set of unique values\n        in ``labels`` is taken as the label set.\n\n    base : np.ndarray, shape=(n,), optional\n        Vertical positions of each label.\n        By default, labels are positioned at integers\n        ``np.arange(len(labels))``.\n\n    height : scalar or np.ndarray, shape=(n,), optional\n        Height for each label.\n        If scalar, the same value is applied to all labels.\n        By default, each label has ``height=1``.\n\n    extend_labels : bool\n        If ``False``, only values of ``labels`` that also exist in\n        ``label_set`` will be shown.\n\n        If ``True``, all labels are shown, with those in `labels` but\n        not in `label_set` appended to the top of the plot.\n        A horizontal line is drawn to indicate the separation between\n        values in or out of ``label_set``.\n\n    ax : matplotlib.pyplot.axes\n        An axis handle on which to draw the intervals.\n        If none is provided, a new set of axes is created.\n\n    tick : bool\n        If ``True``, sets tick positions and labels on the y-axis.\n\n    kwargs\n        Additional keyword arguments to pass to\n        `matplotlib.collection.BrokenBarHCollection`.\n\n    Returns\n    -------\n    ax : matplotlib.pyplot.axes._subplots.AxesSubplot\n        A handle to the (possibly constructed) plot axes\n    '''\n\n    # Get the axes handle\n    ax, _ = __get_axes(ax=ax)\n\n    # Make sure we have a numpy array\n    intervals = np.atleast_2d(intervals)\n\n    if label_set is None:\n        # If we have non-empty pre-existing tick labels, use them\n        label_set = [_.get_text() for _ in ax.get_yticklabels()]\n        # If none of the label strings have content, treat it as empty\n        if not any(label_set):\n            label_set = []\n    else:\n        label_set = list(label_set)\n\n    # Put additional labels at the end, in order\n    if extend_labels:\n        ticks = label_set + sorted(set(labels) - set(label_set))\n    elif label_set:\n        ticks = label_set\n    else:\n        ticks = sorted(set(labels))\n\n    style = dict(linewidth=1)\n\n    style.update(next(ax._get_patches_for_fill.prop_cycler))\n    # Swap color -> facecolor here so we preserve edgecolor on rects\n    style['facecolor'] = style.pop('color')\n    style.update(kwargs)\n\n    if base is None:\n        base = np.arange(len(ticks))\n\n    if height is None:\n        height = 1\n\n    if np.isscalar(height):\n        height = height * np.ones_like(base)\n\n    seg_y = dict()\n    for ybase, yheight, lab in zip(base, height, ticks):\n        seg_y[lab] = (ybase, yheight)\n\n    xvals = defaultdict(list)\n    for ival, lab in zip(intervals, labels):\n        if lab not in seg_y:\n            continue\n        xvals[lab].append((ival[0], ival[1] - ival[0]))\n\n    for lab in seg_y:\n        ax.add_collection(BrokenBarHCollection(xvals[lab], seg_y[lab],\n                                               **style))\n        # Pop the label after the first time we see it, so we only get\n        # one legend entry\n        style.pop('label', None)\n\n    # Draw a line separating the new labels from pre-existing labels\n    if label_set != ticks:\n        ax.axhline(len(label_set), color='k', alpha=0.5)\n\n    if tick:\n        ax.grid(True, axis='y')\n        ax.set_yticks([])\n        ax.set_yticks(base)\n        ax.set_yticklabels(ticks, va='bottom')\n        ax.yaxis.set_major_formatter(IntervalFormatter(base, ticks))\n\n    if base.size:\n        __expand_limits(ax, [base.min(), (base + height).max()], which='y')\n    if intervals.size:\n        __expand_limits(ax, [intervals.min(), intervals.max()], which='x')\n\n    return ax", "response": "Plot labeled intervals with each label on its own row."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hierarchy(intervals_hier, labels_hier, levels=None, ax=None, **kwargs):\n    '''Plot a hierarchical segmentation\n\n    Parameters\n    ----------\n    intervals_hier : list of np.ndarray\n        A list of segmentation intervals.  Each element should be\n        an n-by-2 array of segment intervals, in the format returned by\n        :func:`mir_eval.io.load_intervals` or\n        :func:`mir_eval.io.load_labeled_intervals`.\n        Segmentations should be ordered by increasing specificity.\n\n    labels_hier : list of list-like\n        A list of segmentation labels.  Each element should\n        be a list of labels for the corresponding element in\n        `intervals_hier`.\n\n    levels : list of string\n        Each element ``levels[i]`` is a label for the ```i`` th segmentation.\n        This is used in the legend to denote the levels in a segment hierarchy.\n\n    kwargs\n        Additional keyword arguments to `labeled_intervals`.\n\n    Returns\n    -------\n    ax : matplotlib.pyplot.axes._subplots.AxesSubplot\n        A handle to the (possibly constructed) plot axes\n    '''\n\n    # This will break if a segment label exists in multiple levels\n    if levels is None:\n        levels = list(range(len(intervals_hier)))\n\n    # Get the axes handle\n    ax, _ = __get_axes(ax=ax)\n\n    # Count the pre-existing patches\n    n_patches = len(ax.patches)\n\n    for ints, labs, key in zip(intervals_hier[::-1],\n                               labels_hier[::-1],\n                               levels[::-1]):\n        labeled_intervals(ints, labs, label=key, ax=ax, **kwargs)\n\n    # Reverse the patch ordering for anything we've added.\n    # This way, intervals are listed in the legend from top to bottom\n    ax.patches[n_patches:] = ax.patches[n_patches:][::-1]\n    return ax", "response": "Plot a hierarchical segmentation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef events(times, labels=None, base=None, height=None, ax=None, text_kw=None,\n           **kwargs):\n    '''Plot event times as a set of vertical lines\n\n    Parameters\n    ----------\n    times : np.ndarray, shape=(n,)\n        event times, in the format returned by\n        :func:`mir_eval.io.load_events` or\n        :func:`mir_eval.io.load_labeled_events`.\n\n    labels : list, shape=(n,), optional\n        event labels, in the format returned by\n        :func:`mir_eval.io.load_labeled_events`.\n\n    base : number\n        The vertical position of the base of the line.\n        By default, this will be the bottom of the plot.\n\n    height : number\n        The height of the lines.\n        By default, this will be the top of the plot (minus `base`).\n\n    ax : matplotlib.pyplot.axes\n        An axis handle on which to draw the segmentation.\n        If none is provided, a new set of axes is created.\n\n    text_kw : dict\n        If `labels` is provided, the properties of the text\n        objects can be specified here.\n        See `matplotlib.pyplot.Text` for valid parameters\n\n    kwargs\n        Additional keyword arguments to pass to\n        `matplotlib.pyplot.vlines`.\n\n    Returns\n    -------\n    ax : matplotlib.pyplot.axes._subplots.AxesSubplot\n        A handle to the (possibly constructed) plot axes\n    '''\n    if text_kw is None:\n        text_kw = dict()\n    text_kw.setdefault('va', 'top')\n    text_kw.setdefault('clip_on', True)\n    text_kw.setdefault('bbox', dict(boxstyle='round', facecolor='white'))\n\n    # make sure we have an array for times\n    times = np.asarray(times)\n\n    # Get the axes handle\n    ax, new_axes = __get_axes(ax=ax)\n\n    # If we have fresh axes, set the limits\n\n    if new_axes:\n        # Infer base and height\n        if base is None:\n            base = 0\n        if height is None:\n            height = 1\n\n        ax.set_ylim([base, height])\n    else:\n        if base is None:\n            base = ax.get_ylim()[0]\n\n        if height is None:\n            height = ax.get_ylim()[1]\n\n    cycler = ax._get_patches_for_fill.prop_cycler\n\n    style = next(cycler).copy()\n    style.update(kwargs)\n    # If the user provided 'colors', don't override it with 'color'\n    if 'colors' in style:\n        style.pop('color', None)\n\n    lines = ax.vlines(times, base, base + height, **style)\n\n    if labels:\n        for path, lab in zip(lines.get_paths(), labels):\n            ax.annotate(lab,\n                        xy=(path.vertices[0][0], height),\n                        xycoords='data',\n                        xytext=(8, -10), textcoords='offset points',\n                        **text_kw)\n\n    if new_axes:\n        ax.set_yticks([])\n\n    __expand_limits(ax, [base, base + height], which='y')\n\n    if times.size:\n        __expand_limits(ax, [times.min(), times.max()], which='x')\n\n    return ax", "response": "Plot event times as a set of vertical lines."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pitch(times, frequencies, midi=False, unvoiced=False, ax=None, **kwargs):\n    '''Visualize pitch contours\n\n    Parameters\n    ----------\n    times : np.ndarray, shape=(n,)\n        Sample times of frequencies\n\n    frequencies : np.ndarray, shape=(n,)\n        frequencies (in Hz) of the pitch contours.\n        Voicing is indicated by sign (positive for voiced,\n        non-positive for non-voiced).\n\n    midi : bool\n        If `True`, plot on a MIDI-numbered vertical axis.\n        Otherwise, plot on a linear frequency axis.\n\n    unvoiced : bool\n        If `True`, unvoiced pitch contours are plotted and indicated\n        by transparency.\n\n        Otherwise, unvoiced pitch contours are omitted from the display.\n\n    ax : matplotlib.pyplot.axes\n        An axis handle on which to draw the pitch contours.\n        If none is provided, a new set of axes is created.\n\n    kwargs\n        Additional keyword arguments to `matplotlib.pyplot.plot`.\n\n    Returns\n    -------\n    ax : matplotlib.pyplot.axes._subplots.AxesSubplot\n        A handle to the (possibly constructed) plot axes\n    '''\n\n    ax, _ = __get_axes(ax=ax)\n\n    times = np.asarray(times)\n\n    # First, segment into contiguously voiced contours\n    frequencies, voicings = freq_to_voicing(np.asarray(frequencies,\n                                                       dtype=np.float))\n\n    # Here are all the change-points\n    v_changes = 1 + np.flatnonzero(voicings[1:] != voicings[:-1])\n    v_changes = np.unique(np.concatenate([[0], v_changes, [len(voicings)]]))\n\n    # Set up arrays of slices for voiced and unvoiced regions\n    v_slices, u_slices = [], []\n    for start, end in zip(v_changes, v_changes[1:]):\n        idx = slice(start, end)\n        # A region is voiced if its starting sample is voiced\n        # It's unvoiced if none of the samples in the region are voiced.\n        if voicings[start]:\n            v_slices.append(idx)\n        elif frequencies[idx].all():\n            u_slices.append(idx)\n\n    # Now we just need to plot the contour\n    style = dict()\n    style.update(next(ax._get_lines.prop_cycler))\n    style.update(kwargs)\n\n    if midi:\n        idx = frequencies > 0\n        frequencies[idx] = hz_to_midi(frequencies[idx])\n\n        # Tick at integer midi notes\n        ax.yaxis.set_minor_locator(MultipleLocator(1))\n\n    for idx in v_slices:\n        ax.plot(times[idx], frequencies[idx], **style)\n        style.pop('label', None)\n\n    # Plot the unvoiced portions\n    if unvoiced:\n        style['alpha'] = style.get('alpha', 1.0) * 0.5\n        for idx in u_slices:\n            ax.plot(times[idx], frequencies[idx], **style)\n\n    return ax", "response": "Visualize pitch contours on a set of times and frequencies."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvisualizing multiple f0 measurements.", "response": "def multipitch(times, frequencies, midi=False, unvoiced=False, ax=None,\n               **kwargs):\n    '''Visualize multiple f0 measurements\n\n    Parameters\n    ----------\n    times : np.ndarray, shape=(n,)\n        Sample times of frequencies\n\n    frequencies : list of np.ndarray\n        frequencies (in Hz) of the pitch measurements.\n        Voicing is indicated by sign (positive for voiced,\n        non-positive for non-voiced).\n\n        `times` and `frequencies` should be in the format produced by\n        :func:`mir_eval.io.load_ragged_time_series`\n\n    midi : bool\n        If `True`, plot on a MIDI-numbered vertical axis.\n        Otherwise, plot on a linear frequency axis.\n\n    unvoiced : bool\n        If `True`, unvoiced pitches are plotted and indicated\n        by transparency.\n\n        Otherwise, unvoiced pitches are omitted from the display.\n\n    ax : matplotlib.pyplot.axes\n        An axis handle on which to draw the pitch contours.\n        If none is provided, a new set of axes is created.\n\n    kwargs\n        Additional keyword arguments to `plt.scatter`.\n\n    Returns\n    -------\n    ax : matplotlib.pyplot.axes._subplots.AxesSubplot\n        A handle to the (possibly constructed) plot axes\n    '''\n\n    # Get the axes handle\n    ax, _ = __get_axes(ax=ax)\n\n    # Set up a style for the plot\n    style_voiced = dict()\n    style_voiced.update(next(ax._get_lines.prop_cycler))\n    style_voiced.update(kwargs)\n\n    style_unvoiced = style_voiced.copy()\n    style_unvoiced.pop('label', None)\n    style_unvoiced['alpha'] = style_unvoiced.get('alpha', 1.0) * 0.5\n\n    # We'll collect all times and frequencies first, then plot them\n    voiced_times = []\n    voiced_freqs = []\n\n    unvoiced_times = []\n    unvoiced_freqs = []\n\n    for t, freqs in zip(times, frequencies):\n        if not len(freqs):\n            continue\n\n        freqs, voicings = freq_to_voicing(np.asarray(freqs, dtype=np.float))\n\n        # Discard all 0-frequency measurements\n        idx = freqs > 0\n        freqs = freqs[idx]\n        voicings = voicings[idx]\n\n        if midi:\n            freqs = hz_to_midi(freqs)\n\n        n_voiced = sum(voicings)\n        voiced_times.extend([t] * n_voiced)\n        voiced_freqs.extend(freqs[voicings])\n        unvoiced_times.extend([t] * (len(freqs) - n_voiced))\n        unvoiced_freqs.extend(freqs[~voicings])\n\n    # Plot the voiced frequencies\n    ax.scatter(voiced_times, voiced_freqs, **style_voiced)\n\n    # Plot the unvoiced frequencies\n    if unvoiced:\n        ax.scatter(unvoiced_times, unvoiced_freqs, **style_unvoiced)\n\n    # Tick at integer midi notes\n    if midi:\n        ax.yaxis.set_minor_locator(MultipleLocator(1))\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots a quantized piano roll as intervals", "response": "def piano_roll(intervals, pitches=None, midi=None, ax=None, **kwargs):\n    '''Plot a quantized piano roll as intervals\n\n    Parameters\n    ----------\n    intervals : np.ndarray, shape=(n, 2)\n        timing intervals for notes\n\n    pitches : np.ndarray, shape=(n,), optional\n        pitches of notes (in Hz).\n\n    midi : np.ndarray, shape=(n,), optional\n        pitches of notes (in MIDI numbers).\n\n        At least one of ``pitches`` or ``midi`` must be provided.\n\n    ax : matplotlib.pyplot.axes\n        An axis handle on which to draw the intervals.\n        If none is provided, a new set of axes is created.\n\n    kwargs\n        Additional keyword arguments to :func:`labeled_intervals`.\n\n    Returns\n    -------\n    ax : matplotlib.pyplot.axes._subplots.AxesSubplot\n        A handle to the (possibly constructed) plot axes\n    '''\n\n    if midi is None:\n        if pitches is None:\n            raise ValueError('At least one of `midi` or `pitches` '\n                             'must be provided.')\n\n        midi = hz_to_midi(pitches)\n\n    scale = np.arange(128)\n    ax = labeled_intervals(intervals, np.round(midi).astype(int),\n                           label_set=scale,\n                           tick=False,\n                           ax=ax,\n                           **kwargs)\n\n    # Minor tick at each semitone\n    ax.yaxis.set_minor_locator(MultipleLocator(1))\n\n    ax.axis('auto')\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsources - separation visualization for a set of sources.", "response": "def separation(sources, fs=22050, labels=None, alpha=0.75, ax=None, **kwargs):\n    '''Source-separation visualization\n\n    Parameters\n    ----------\n    sources : np.ndarray, shape=(nsrc, nsampl)\n        A list of waveform buffers corresponding to each source\n\n    fs : number > 0\n        The sampling rate\n\n    labels : list of strings\n        An optional list of descriptors corresponding to each source\n\n    alpha : float in [0, 1]\n        Maximum alpha (opacity) of spectrogram values.\n\n    ax : matplotlib.pyplot.axes\n        An axis handle on which to draw the spectrograms.\n        If none is provided, a new set of axes is created.\n\n    kwargs\n        Additional keyword arguments to ``scipy.signal.spectrogram``\n\n    Returns\n    -------\n    ax\n        The axis handle for this plot\n    '''\n\n    # Get the axes handle\n    ax, new_axes = __get_axes(ax=ax)\n\n    # Make sure we have at least two dimensions\n    sources = np.atleast_2d(sources)\n\n    if labels is None:\n        labels = ['Source {:d}'.format(_) for _ in range(len(sources))]\n\n    kwargs.setdefault('scaling', 'spectrum')\n\n    # The cumulative spectrogram across sources\n    # is used to establish the reference power\n    # for each individual source\n    cumspec = None\n    specs = []\n    for i, src in enumerate(sources):\n        freqs, times, spec = spectrogram(src, fs=fs, **kwargs)\n\n        specs.append(spec)\n        if cumspec is None:\n            cumspec = spec.copy()\n        else:\n            cumspec += spec\n\n    ref_max = cumspec.max()\n    ref_min = ref_max * 1e-6\n\n    color_conv = ColorConverter()\n\n    for i, spec in enumerate(specs):\n\n        # For each source, grab a new color from the cycler\n        # Then construct a colormap that interpolates from\n        # [transparent white -> new color]\n        color = next(ax._get_lines.prop_cycler)['color']\n        color = color_conv.to_rgba(color, alpha=alpha)\n        cmap = LinearSegmentedColormap.from_list(labels[i],\n                                                 [(1.0, 1.0, 1.0, 0.0),\n                                                  color])\n\n        ax.pcolormesh(times, freqs, spec,\n                      cmap=cmap,\n                      norm=LogNorm(vmin=ref_min, vmax=ref_max),\n                      shading='gouraud',\n                      label=labels[i])\n\n        # Attach a 0x0 rect to the axis with the corresponding label\n        # This way, it will show up in the legend\n        ax.add_patch(Rectangle((0, 0), 0, 0, color=color, label=labels[i]))\n\n    if new_axes:\n        ax.axis('tight')\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __ticker_midi_note(x, pos):\n    '''A ticker function for midi notes.\n\n    Inputs x are interpreted as midi numbers, and converted\n    to [NOTE][OCTAVE]+[cents].\n    '''\n\n    NOTES = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n\n    cents = float(np.mod(x, 1.0))\n    if cents >= 0.5:\n        cents = cents - 1.0\n        x = x + 0.5\n\n    idx = int(x % 12)\n\n    octave = int(x / 12) - 1\n\n    if cents == 0:\n        return '{:s}{:2d}'.format(NOTES[idx], octave)\n    return '{:s}{:2d}{:+02d}'.format(NOTES[idx], octave, int(cents * 100))", "response": "A ticker function for midi notes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the y - axis of the given axes to MIDI notes", "response": "def ticker_notes(ax=None):\n    '''Set the y-axis of the given axes to MIDI notes\n\n    Parameters\n    ----------\n    ax : matplotlib.pyplot.axes\n        The axes handle to apply the ticker.\n        By default, uses the current axes handle.\n\n    '''\n    ax, _ = __get_axes(ax=ax)\n\n    ax.yaxis.set_major_formatter(FMT_MIDI_NOTE)\n    # Get the tick labels and reset the vertical alignment\n    for tick in ax.yaxis.get_ticklabels():\n        tick.set_verticalalignment('baseline')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the y - axis of the given axes to MIDI frequencies.", "response": "def ticker_pitch(ax=None):\n    '''Set the y-axis of the given axes to MIDI frequencies\n\n    Parameters\n    ----------\n    ax : matplotlib.pyplot.axes\n        The axes handle to apply the ticker.\n        By default, uses the current axes handle.\n    '''\n    ax, _ = __get_axes(ax=ax)\n\n    ax.yaxis.set_major_formatter(FMT_MIDI_HZ)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, **import_params):\n        if self.file:\n            import_params[\"url\"] = self.file\n\n        self.id_field = \"id\"\n\n        if \"connection\" in import_params:\n            self.fields.append(\"connector\")\n            self.update_from_dict(import_params[\"connection\"])\n            self.save(force_create=True)\n        else:\n            super(FileImportJob, self).run(params=import_params,\n                                           files=self.files)", "response": "Actually creates the file import job on the CARTO server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of file imports with only the id set", "response": "def filter(self):\n        \"\"\"\n        Get a filtered list of file imports\n\n        :return: A list of file imports, with only the id set (you need to\n                refresh them if you want all the attributes to be filled in)\n        :rtype: list of :class:`carto.file_import.FileImportJob`\n\n        :raise: CartoException\n        \"\"\"\n        try:\n            response = self.send(self.get_collection_endpoint(), \"get\")\n            if self.json_collection_attribute is not None:\n                resource_ids = self.client.get_response_data(\n                    response,\n                    self.Meta.parse_json)[self.json_collection_attribute]\n            else:\n                resource_ids = self.client.get_response_data(\n                    response, self.Meta.parse_json)\n        except Exception as e:\n            raise CartoException(e)\n\n        resources = []\n\n        for resource_id in resource_ids:\n            try:\n                resource = self.resource_class(self.client)\n            except (ValueError, TypeError):\n                continue\n            else:\n                setattr(resource, resource.Meta.id_field, resource_id)\n                resources.append(resource)\n\n        return resources"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake an API - key - authorized request to the specified URL.", "response": "def send(self, relative_path, http_method, **requests_args):\n        \"\"\"\n        Makes an API-key-authorized request\n\n        :param relative_path: URL path relative to self.base_url\n        :param http_method: HTTP method\n        :param requests_args: kwargs to be sent to requests\n        :type relative_path: str\n        :type http_method: str\n        :type requests_args: kwargs\n\n        :return:\n            A request response object\n        :raise:\n            CartoException\n        \"\"\"\n        try:\n            http_method, requests_args = self.prepare_send(http_method, **requests_args)\n\n            response = super(APIKeyAuthClient, self).send(relative_path, http_method, **requests_args)\n        except Exception as e:\n            raise CartoException(e)\n\n        if CartoRateLimitException.is_rate_limited(response):\n            raise CartoRateLimitException(response)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking validity. Right now an API key is considered valid if the current user can list user API keys and the result contains that API key.", "response": "def is_valid_api_key(self):\n        \"\"\"\n        Checks validity. Right now, an API key is considered valid if it\n        can list user API keys and the result contains that API key.\n        This might change in the future.\n\n        :return: True if the API key is considered valid for current user.\n        \"\"\"\n        res = self.send('api/v3/api_keys', 'get')\n        return \\\n            res.ok and \\\n            self.api_key in (ak['token'] for ak in res.json()['result'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, **export_params):\n        export_params[\"visualization_id\"] = self.visualization_id\n\n        return super(ExportJob, self).run(params=export_params)", "response": "This method is called by the export API to get the current set of items from the export queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend an API request to the API endpoint.", "response": "def send(self, url, http_method, **client_args):\n        \"\"\"\n        Sends an API request, taking into account that datasets are part of\n        the visualization endpoint.\n\n        :param url: Endpoint URL\n        :param http_method: The method used to make the request to the API\n        :param client_args: Arguments to be sent to the auth client\n        :type url: str\n        :type http_method: str\n        :type client_args: kwargs\n\n        :return: A request response object\n\n        :raise: CartoException\n        \"\"\"\n        try:\n            client_args = client_args or {}\n\n            if \"params\" not in client_args:\n                client_args[\"params\"] = {}\n            client_args[\"params\"].update({\"type\": \"table\",\n                                          \"exclude_shared\": \"true\"})\n\n            return super(DatasetManager, self).send(url,\n                                                    http_method,\n                                                    **client_args)\n        except Exception as e:\n            raise CartoException(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_sync_table(self, archive, interval, **import_args):\n        return (hasattr(archive, \"startswith\") and archive.startswith(\"http\")\n                or \"connection\" in import_args) \\\n            and interval is not None", "response": "Checks if this is a request for a sync dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, archive, interval=None, **import_args):\n        archive = archive.lower() if hasattr(archive, \"lower\") else archive\n\n        if self.is_sync_table(archive, interval, **import_args):\n            manager = SyncTableJobManager(self.client)\n        else:\n            manager = FileImportJobManager(self.client)\n\n        import_job = manager.create(archive) if interval is None \\\n            else manager.create(archive, interval)\n        import_job.run(**import_args)\n\n        if import_job.get_id() is None:\n            raise CartoException(_(\"Import API returned corrupt job details \\\n                                   when creating dataset\"))\n\n        import_job.refresh()\n\n        count = 0\n        while import_job.state in (\"enqueued\", \"queued\", \"pending\", \"uploading\",\n                                   \"unpacking\", \"importing\", \"guessing\") \\\n            or (isinstance(manager, SyncTableJobManager)\n                and import_job.state == \"created\"):\n            if count >= MAX_NUMBER_OF_RETRIES:\n                raise CartoException(_(\"Maximum number of retries exceeded \\\n                                       when polling the import API for \\\n                                       dataset creation\"))\n            time.sleep(INTERVAL_BETWEEN_RETRIES_S)\n            import_job.refresh()\n            count += 1\n\n        if import_job.state == \"failure\":\n            raise CartoException(_(\"Dataset creation was not successful \\\n                                   because of failed import (error: {error}\")\n                                 .format(error=json.dumps(\n                                     import_job.get_error_text)))\n\n        if (import_job.state != \"complete\" and import_job.state != \"created\"\n            and import_job.state != \"success\") \\\n                or import_job.success is False:\n            raise CartoException(_(\"Dataset creation was not successful \\\n                                   because of unknown import error\"))\n\n        if hasattr(import_job, \"visualization_id\") \\\n                and import_job.visualization_id is not None:\n            visualization_id = import_job.visualization_id\n        else:\n            table = TableManager(self.client).get(import_job.table_id)\n            visualization_id = table.table_visualization.get_id() \\\n                if table is not None else None\n\n        try:\n            return self.get(visualization_id) if visualization_id is not None \\\n                else None\n        except AttributeError:\n            raise CartoException(_(\"Dataset creation was not successful \\\n                                   because of unknown error\"))", "response": "Creates a new dataset object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self):\n        export_job = ExportJob(self.client, self.get_id())\n        export_job.run()\n\n        export_job.refresh()\n\n        count = 0\n        while export_job.state in (\"exporting\", \"enqueued\", \"pending\"):\n            if count >= MAX_NUMBER_OF_RETRIES:\n                raise CartoException(_(\"Maximum number of retries exceeded \\\n                                       when polling the import API for \\\n                                       visualization export\"))\n            time.sleep(INTERVAL_BETWEEN_RETRIES_S)\n            export_job.refresh()\n            count += 1\n\n        if export_job.state == \"failure\":\n            raise CartoException(_(\"Visualization export failed\"))\n\n        if (export_job.state != \"complete\" and export_job.state != \"created\"):\n            raise CartoException(_(\"Unexpected problem on visualization export \\\n                                   (state: {state})\").\n                                 format(state=export_job.state))\n\n        return export_job.url", "response": "This method executes the Export Job and returns the URL pointing to the. carto file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(self, url, http_method, **client_args):\n        try:\n            client_args.setdefault('params', {})\n            client_args[\"params\"].update({\"type\": \"derived\",\n                                          \"exclude_shared\": \"true\"})\n\n            return super(VisualizationManager, self).send(url,\n                                                          http_method,\n                                                          **client_args)\n        except Exception as e:\n            raise CartoException(e)", "response": "Sends API request to the API endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the response has been rate limited by CARTO APIs.", "response": "def is_rate_limited(response):\n        \"\"\"\n        Checks if the response has been rate limited by CARTO APIs\n\n        :param response: The response rate limited by CARTO APIs\n        :type response: requests.models.Response class\n\n        :return: Boolean\n        \"\"\"\n        if (response.status_code == codes.too_many_requests and 'Retry-After' in response.headers and\n                int(response.headers['Retry-After']) >= 0):\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tile_url(self, x, y, z, layer_id=None, feature_id=None,\n                     filter=None, extension=\"png\"):\n        \"\"\"\n        Prepares a URL to get data (raster or vector) from a NamedMap or\n        AnonymousMap\n\n        :param x: The x tile\n        :param y: The y tile\n        :param z: The zoom level\n        :param layer_id: Can be a number (referring to the # layer of your  \\\n                        map), all layers of your map, or a list of layers.\n                        To show just the basemap layer, enter the value 0\n                        To show the first layer, enter the value 1\n                        To show all layers, enter the value 'all'\n                        To show a list of layers, enter the comma separated \\\n                        layer value as '0,1,2'\n        :param feature_id: The id of the feature\n        :param filter: The filter to be applied to the layer\n        :param extension: The format of the data to be retrieved: png, mvt, ...\n        :type x: int\n        :type y: int\n        :type z: int\n        :type layer_id: str\n        :type feature_id: str\n        :type filter: str\n        :type extension: str\n\n        :return: A URL to download data\n        :rtype: str\n\n        :raise: CartoException\n        \"\"\"\n        base_url = self.client.base_url + self.Meta.collection_endpoint\n        template_id = self.template_id if hasattr(self, 'template_id') \\\n            else self.layergroupid\n        if layer_id is not None and feature_id is not None:\n            url = urljoin(base_url,\n                          \"{template_id}/{layer}/attributes/{feature_id}\"). \\\n                          format(template_id=template_id,\n                                 layer=layer_id,\n                                 feature_id=feature_id)\n        elif layer_id is not None and filter is not None:\n            url = urljoin(base_url,\n                          \"{template_id}/{filter}/{z}/{x}/{y}.{extension}\"). \\\n                          format(template_id=template_id,\n                                 filter=filter,\n                                 z=z, x=x, y=y,\n                                 extension=extension)\n        elif layer_id is not None:\n            url = urljoin(base_url,\n                          \"{template_id}/{layer}/{z}/{x}/{y}.{extension}\"). \\\n                          format(template_id=template_id,\n                                 layer=layer_id,\n                                 z=z, x=x, y=y,\n                                 extension=extension)\n        else:\n            url = urljoin(base_url, \"{template_id}/{z}/{x}/{y}.{extension}\"). \\\n                          format(\n                               template_id=template_id,\n                               z=z, x=x, y=y,\n                               extension=extension)\n\n        if hasattr(self, 'auth') and self.auth is not None \\\n           and len(self.auth['valid_tokens']) > 0:\n            url = urljoin(url, \"?auth_token={auth_token}\"). \\\n                format(auth_token=self.auth['valid_tokens'][0])\n\n        return url", "response": "Gets the tile URL for a specific feature and layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef instantiate(self, params, auth=None):\n        try:\n            endpoint = (self.Meta.collection_endpoint\n                        + \"{template_id}\"). \\\n                format(template_id=self.template_id)\n            if (auth is not None):\n                endpoint = (endpoint + \"?auth_token={auth_token}\"). \\\n                    format(auth_token=auth)\n\n            self.send(endpoint, \"POST\", json=params)\n        except CartoRateLimitException as e:\n            raise e\n        except Exception as e:\n            raise CartoException(e)", "response": "Creates a new map tile with the given json with the styling info for the named map tile."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_from_dict(self, attribute_dict):\n        if 'template' in attribute_dict:\n            self.update_from_dict(attribute_dict['template'])\n            setattr(self,\n                    self.Meta.id_field, attribute_dict['template']['name'])\n            return\n        try:\n            for k, v in attribute_dict.items():\n                setattr(self, k, v)\n        except Exception:\n            setattr(self, self.Meta.id_field, attribute_dict)", "response": "Method overriden from the base class to update the object attributes with the dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nallowing you to fetch the map tiles of a created map :param params: The json with the styling info for the named map :type params: dict :return: :raise: CartoException", "response": "def instantiate(self, params):\n        \"\"\"\n        Allows you to fetch the map tiles of a created map\n\n        :param params: The json with the styling info for the named map\n        :type params: dict\n\n        :return:\n\n        :raise: CartoException\n        \"\"\"\n        try:\n            self.send(self.Meta.collection_endpoint, \"POST\", json=params)\n        except CartoRateLimitException as e:\n            raise e\n        except Exception as e:\n            raise CartoException(e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a SQL query and returns the response data as json or as a regular object.", "response": "def send(self, sql, parse_json=True, do_post=True, format=None, **request_args):\n        \"\"\"\n        Executes SQL query in a CARTO server\n\n        :param sql: The SQL\n        :param parse_json: Set it to False if you want raw reponse\n        :param do_post: Set it to True to force post request\n        :param format: Any of the data export formats allowed by CARTO's\n                        SQL API\n        :param request_args: Additional parameters to send with the request\n        :type sql: str\n        :type parse_json: boolean\n        :type do_post: boolean\n        :type format: str\n        :type request_args: dictionary\n\n        :return: response data, either as json or as a regular\n                    response.content object\n        :rtype: object\n\n        :raise: CartoException\n        \"\"\"\n        try:\n            params = {'q': sql}\n            if format:\n                params['format'] = format\n                if format not in ['json', 'geojson']:\n                    parse_json = False\n\n            if request_args is not None:\n                for attr in request_args:\n                    params[attr] = request_args[attr]\n\n            if len(sql) < MAX_GET_QUERY_LEN and do_post is False:\n                resp = self.auth_client.send(self.api_url,\n                                             'GET',\n                                             params=params)\n            else:\n                resp = self.auth_client.send(self.api_url, 'POST', data=params)\n\n            return self.auth_client.get_response_data(resp, parse_json)\n        except CartoRateLimitException as e:\n            raise e\n        except Exception as e:\n            raise CartoException(e)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the attributes of the object from a dictionary.", "response": "def update_from_dict(self, data_dict):\n        \"\"\"\n        :param data_dict: Dictionary to be mapped into object attributes\n        :type data_dict: dict\n\n        :return:\n        \"\"\"\n        for k, v in data_dict.items():\n            setattr(self, k, v)\n        if \"item_queue_id\" in data_dict:\n            self.id = data_dict[\"item_queue_id\"]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute a batch SQL query in a CARTO server and returns the response data as json or regular response. content", "response": "def send(self, url, http_method, json_body=None, http_header=None):\n        \"\"\"\n        Executes Batch SQL query in a CARTO server\n\n        :param url: Endpoint url\n        :param http_method: The method used to make the request to the API\n        :param json_body: The information that needs to be sent, by default\n                            is set to None\n        :param http_header: The header used to make write requests to the API,\n                            by default is none\n        :type url: str\n        :type http_method: str\n        :type json_body: dict\n        :type http_header: str\n\n        :return: Response data, either as json or as a regular response.content\n                object\n        :rtype: object\n\n        :raise: CartoException\n        \"\"\"\n        try:\n            data = self.client.send(url,\n                                    http_method=http_method,\n                                    headers=http_header,\n                                    json=json_body)\n            data_json = self.client.get_response_data(data)\n        except CartoRateLimitException as e:\n            raise e\n        except Exception as e:\n            raise CartoException(e)\n        return data_json"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(self, sql_query):\n        header = {'content-type': 'application/json'}\n        data = self.send(self.api_url,\n                         http_method=\"POST\",\n                         json_body={\"query\": sql_query},\n                         http_header=header)\n        return data", "response": "Creates a new batch SQL query."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new batch SQL query and waits for its completion or failure.", "response": "def create_and_wait_for_completion(self, sql_query):\n        \"\"\"\n        Creates a new batch SQL query and waits for its completion or failure\n\n        Batch SQL jobs are asynchronous, once created this method\n        automatically queries the job status until it's one of 'done',\n        'failed', 'canceled', 'unknown'\n\n        :param sql_query: The SQL query to be used\n        :type sql_query: str or list of str\n\n        :return: Response data, either as json or as a regular response.content\n                    object\n        :rtype: object\n\n        :raise: CartoException when there's an exception in the BatchSQLJob execution or the batch job status is one of the BATCH_JOBS_FAILED_STATUSES ('failed', 'canceled', 'unknown')\n        \"\"\"\n        header = {'content-type': 'application/json'}\n        data = self.send(self.api_url,\n                         http_method=\"POST\",\n                         json_body={\"query\": sql_query},\n                         http_header=header)\n\n        warnings.warn('Batch SQL job created with job_id: {job_id}'.format(job_id=data['job_id']))\n\n        while data and data['status'] in BATCH_JOBS_PENDING_STATUSES:\n            time.sleep(BATCH_READ_STATUS_AFTER_SECONDS)\n            data = self.read(data['job_id'])\n\n        if data['status'] in BATCH_JOBS_FAILED_STATUSES:\n            raise CartoException(_(\"Batch SQL job failed with result: {data}\".format(data=data)))\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the information for a specific Batch API request", "response": "def read(self, job_id):\n        \"\"\"\n        Reads the information for a specific Batch API request\n\n        :param job_id: The id of the job to be read from\n        :type job_id: str\n\n        :return: Response data, either as json or as a regular response.content\n                    object\n        :rtype: object\n\n        :raise: CartoException\n        \"\"\"\n        data = self.send(self.api_url + job_id, http_method=\"GET\")\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the sql query of a specific job.", "response": "def update(self, job_id, sql_query):\n        \"\"\"\n        Updates the sql query of a specific job\n\n        :param job_id: The id of the job to be updated\n        :param sql_query: The new SQL query for the job\n        :type job_id: str\n        :type sql_query: str\n\n        :return: Response data, either as json or as a regular response.content\n                    object\n        :rtype: object\n\n        :raise: CartoException\n        \"\"\"\n        header = {'content-type': 'application/json'}\n        data = self.send(self.api_url + job_id,\n                         http_method=\"PUT\",\n                         json_body={\"query\": sql_query},\n                         http_header=header)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncancel a job in the carto database", "response": "def cancel(self, job_id):\n        \"\"\"\n        Cancels a job\n\n        :param job_id: The id of the job to be cancelled\n        :type job_id: str\n\n        :return: A status code depending on whether the cancel request was\n                    successful\n        :rtype: str\n\n        :raise CartoException:\n        \"\"\"\n        try:\n            confirmation = self.send(self.api_url + job_id, http_method=\"DELETE\")\n        except CartoException as e:\n            if 'Cannot set status from done to cancelled' in e.args[0].args[0]:\n                return 'done'\n            else:\n                raise e\n        return confirmation['status']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copyfrom(self, query, iterable_data, compress=True,\n                 compression_level=DEFAULT_COMPRESSION_LEVEL):\n        \"\"\"\n        Gets data from an iterable object into a table\n\n        :param query: The \"COPY table_name [(column_name[, ...])]\n                           FROM STDIN [WITH(option[,...])]\" query to execute\n        :type query: str\n\n        :param iterable_data: An object that can be iterated\n                              to retrieve the data\n        :type iterable_data: object\n\n        :return: Response data as json\n        :rtype: str\n\n        :raise CartoException:\n        \"\"\"\n        url = self.api_url + '/copyfrom'\n        headers = {\n            'Content-Type': 'application/octet-stream',\n            'Transfer-Encoding': 'chunked'\n        }\n        params = {'api_key': self.api_key, 'q': query}\n\n        if compress:\n            headers['Content-Encoding'] = 'gzip'\n            _iterable_data = self._compress_chunks(iterable_data,\n                                                   compression_level)\n        else:\n            _iterable_data = iterable_data\n\n        try:\n            response = self.client.send(url,\n                                        http_method='POST',\n                                        params=params,\n                                        data=_iterable_data,\n                                        headers=headers,\n                                        stream=True)\n            response_json = self.client.get_response_data(response)\n        except CartoRateLimitException as e:\n            raise e\n        except Exception as e:\n            raise CartoException(e)\n\n        return response_json", "response": "Executes a COPY query and returns the response data as json"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting data from a readable file - like object into a table_name", "response": "def copyfrom_file_object(self, query, file_object, compress=True,\n                             compression_level=DEFAULT_COMPRESSION_LEVEL):\n        \"\"\"\n        Gets data from a readable file object into a table\n\n        :param query: The \"COPY table_name [(column_name[, ...])]\n                           FROM STDIN [WITH(option[,...])]\" query to execute\n        :type query: str\n\n        :param file_object: A file-like object.\n                            Normally the return value of open('file.ext', 'rb')\n        :type file_object: file\n\n        :return: Response data as json\n        :rtype: str\n\n        :raise CartoException:\n        \"\"\"\n        chunk_generator = self._read_in_chunks(file_object)\n        return self.copyfrom(query, chunk_generator, compress,\n                             compression_level)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a COPY query on a file and return the response data as json", "response": "def copyfrom_file_path(self, query, path, compress=True,\n                           compression_level=DEFAULT_COMPRESSION_LEVEL):\n        \"\"\"\n        Gets data from a readable file into a table\n\n        :param query: The \"COPY table_name [(column_name[, ...])]\n                           FROM STDIN [WITH(option[,...])]\" query to execute\n        :type query: str\n\n        :param path: A path to a file\n        :type path: str\n\n        :return: Response data as json\n        :rtype: str\n\n        :raise CartoException:\n        \"\"\"\n        with open(path, 'rb') as f:\n            result = self.copyfrom_file_object(query, f, compress,\n                                               compression_level)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copyto(self, query):\n        url = self.api_url + '/copyto'\n        params = {'api_key': self.api_key, 'q': query}\n\n        try:\n            response = self.client.send(url,\n                                        http_method='GET',\n                                        params=params,\n                                        stream=True)\n            response.raise_for_status()\n        except CartoRateLimitException as e:\n            raise e\n        except HTTPError as e:\n            if 400 <= response.status_code < 500:\n                # Client error, provide better reason\n                reason = response.json()['error'][0]\n                error_msg = u'%s Client Error: %s' % (response.status_code,\n                                                      reason)\n                raise CartoException(error_msg)\n            else:\n                raise CartoException(e)\n        except Exception as e:\n            raise CartoException(e)\n\n        return response", "response": "Gets data from a table into a Response object that can be iterated over."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copyto_file_object(self, query, file_object):\n        response = self.copyto(query)\n        for block in response.iter_content(DEFAULT_CHUNK_SIZE):\n            file_object.write(block)", "response": "Copy data from a table into a writable file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy data from a table into a writable file", "response": "def copyto_file_path(self, query, path, append=False):\n        \"\"\"\n        Gets data from a table into a writable file\n\n        :param query: The \"COPY { table_name [(column_name[, ...])] | (query) }\n                           TO STDOUT [WITH(option[,...])]\" query to execute\n        :type query: str\n\n        :param path: A path to a writable file\n        :type path: str\n\n        :param append: Whether to append or not if the file already exists\n                       Default value is False\n        :type append: bool\n\n        :raise CartoException:\n        \"\"\"\n        file_mode = 'wb' if not append else 'ab'\n        with open(path, file_mode) as f:\n            self.copyto_file_object(query, f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef force_sync(self):\n        try:\n            self.send(self.get_resource_endpoint(), \"put\")\n        except Exception as e:\n            raise CartoException(e)", "response": "Force sync the SyncTableJob with the current version of the user s local cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads training and target dataset from CSV files.", "response": "def load_data(flist, drop_duplicates=False):\n    '''\n    Usage: set train, target, and test key and feature files.\n\n    FEATURE_LIST_stage2 = {\n                'train':(\n                         TEMP_PATH + 'v1_stage1_all_fold.csv',\n                         TEMP_PATH + 'v2_stage1_all_fold.csv',\n                         TEMP_PATH + 'v3_stage1_all_fold.csv',\n                        ),#target is not in 'train'\n\n                'target':(\n                         INPUT_PATH + 'target.csv',\n                        ),#target is in 'target'\n\n                'test':(\n                         TEMP_PATH + 'v1_stage1_test.csv',\n                         TEMP_PATH + 'v2_stage1_test.csv',\n                         TEMP_PATH + 'v3_stage1_test.csv',\n                        ),\n                }\n    '''\n    if (len(flist['train'])==0) or (len(flist['target'])==0) or (len(flist['test'])==0):\n        raise Exception('train, target, and test must be set at \\\n                                    least one file, respectively.')\n\n    X_train = pd.DataFrame()\n    test = pd.DataFrame()\n\n    print 'Reading train dataset'\n    for i in flist['train']:\n        X_train = pd.concat([X_train, paratext.load_csv_to_pandas(PATH+i, allow_quoted_newlines=True)],axis=1)\n\n    print 'train dataset is created'\n\n\n    print 'Reading target data'\n    y_train = paratext.load_csv_to_pandas(PATH+flist['target'][0], allow_quoted_newlines=True)['target']\n\n    print 'Reading train dataset'\n    for i in flist['test']:\n        test = pd.concat([test, paratext.load_csv_to_pandas(PATH+i, allow_quoted_newlines=True)],axis=1)\n\n    #del test['t_id']\n    #print X_train.columns\n    #print test.columns\n    assert( (False in X_train.columns == test.columns) == False)\n    print 'train shape :{}'.format(X_train.shape)\n    if drop_duplicates == True:\n        #delete identical columns\n        unique_col = X_train.T.drop_duplicates().T.columns\n        X_train = X_train[unique_col]\n        test = test[unique_col]\n        assert( all(X_train.columns == test.columns))\n        print 'train shape after concat and drop_duplicates :{}'.format(X_train.shape)\n\n    # drop constant features\n    #X_train = X_train.loc[:, (X_train != X_train.ix[0]).any()] \n    #test = test.loc[:, (test != test.ix[0]).any()] \n\n    #common_col = list(set(X_train.columns.tolist()) and set(test.columns.tolist()))\n    #X_train = X_train[common_col]\n    #test = test[common_col]\n    #print 'shape after dropping constant features: {}'.format(X_train.shape)\n    \n    return X_train, y_train, test"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes cols for multi - class predictions", "response": "def make_multi_cols(self, num_class, name):\n        '''make cols for multi-class predictions'''\n        cols = ['c' + str(i) + '_' for i in xrange(num_class)]\n        cols = map(lambda x: x + name, cols)\n        return cols"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self, data):\n        # type: (bytes) -> None\n        '''\n        A method to parse an ISO9660 Path Table Record out of a string.\n\n        Parameters:\n         data - The string to parse.\n        Returns:\n         Nothing.\n        '''\n        (self.len_di, self.xattr_length, self.extent_location,\n         self.parent_directory_num) = struct.unpack_from(self.FMT, data[:8], 0)\n\n        if self.len_di % 2 != 0:\n            self.directory_identifier = data[8:-1]\n        else:\n            self.directory_identifier = data[8:]\n        self.dirrecord = None\n        self._initialized = True", "response": "A method to parse an ISO9660 Path Table Record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _record(self, ext_loc, parent_dir_num):\n        # type: (int, int) -> bytes\n        '''\n        An internal method to generate a string representing this Path Table Record.\n\n        Parameters:\n         ext_loc - The extent location to place in this Path Table Record.\n         parent_dir_num - The parent directory number to place in this Path Table\n                          Record.\n        Returns:\n         A string representing this Path Table Record.\n        '''\n        return struct.pack(self.FMT, self.len_di, self.xattr_length,\n                           ext_loc, parent_dir_num) + self.directory_identifier + b'\\x00' * (self.len_di % 2)", "response": "This method generates a string representing this Path Table Record."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef record_little_endian(self):\n        # type: () -> bytes\n        '''\n        A method to generate a string representing the little endian version of\n        this Path Table Record.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing the little endian version of this Path Table Record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Path Table Record not yet initialized')\n\n        return self._record(self.extent_location, self.parent_directory_num)", "response": "A method to generate a string representing the little endian version of the current Path Table Record."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef record_big_endian(self):\n        # type: () -> bytes\n        '''\n        A method to generate a string representing the big endian version of\n        this Path Table Record.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing the big endian version of this Path Table Record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Path Table Record not yet initialized')\n\n        return self._record(utils.swab_32bit(self.extent_location),\n                            utils.swab_16bit(self.parent_directory_num))", "response": "A method to generate a string representing the big endian version of the path table record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _new(self, name, parent_dir_num):\n        # type: (bytes, int) -> None\n        '''\n        An internal method to create a new Path Table Record.\n\n        Parameters:\n         name - The name for this Path Table Record.\n         parent_dir_num - The directory number of the parent of this Path Table\n                          Record.\n        Returns:\n         Nothing.\n        '''\n        self.len_di = len(name)\n        self.xattr_length = 0  # FIXME: we don't support xattr for now\n        self.parent_directory_num = parent_dir_num\n        self.directory_identifier = name\n        self._initialized = True", "response": "Internal method to create a new Path Table Record."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_extent_location(self, extent_loc):\n        # type: (int) -> None\n        '''\n        A method to update the extent location for this Path Table Record.\n\n        Parameters:\n         extent_loc - The new extent location.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Path Table Record not yet initialized')\n\n        self.extent_location = extent_loc", "response": "A method to update the extent location for this Path Table Record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_data(data_length, blocksize, infp, outfp):\n    # type: (int, int, BinaryIO, BinaryIO) -> None\n    '''\n    A utility function to copy data from the input file object to the output\n    file object.  This function will use the most efficient copy method available,\n    which is often sendfile.\n\n    Parameters:\n     data_length - The amount of data to copy.\n     blocksize - How much data to copy per iteration.\n     infp - The file object to copy data from.\n     outfp - The file object to copy data to.\n    Returns:\n     Nothing.\n    '''\n    use_sendfile = False\n    if have_sendfile:\n        # Python 3 implements the fileno method for all file-like objects, so\n        # we can't just use the existence of the method to tell whether it is\n        # available.  Instead, we try to assign it, and if we fail, then we\n        # assume it is not available.\n        try:\n            x_unused = infp.fileno()  # NOQA\n            y_unused = outfp.fileno()  # NOQA\n            use_sendfile = True\n        except (AttributeError, io.UnsupportedOperation):\n            pass\n\n    if use_sendfile:\n        # This is one of those instances where using the file object and the\n        # file descriptor causes problems.  The sendfile() call actually updates\n        # the underlying file descriptor, but the file object does not know\n        # about it.  To get around this, we instead get the offset, allow\n        # sendfile() to update the offset, then manually seek the file object\n        # to the right location.  This ensures that the file object gets updated\n        # properly.\n        in_offset = infp.tell()\n        out_offset = outfp.tell()\n        sendfile(outfp.fileno(), infp.fileno(), in_offset, data_length)\n        infp.seek(in_offset + data_length)\n        outfp.seek(out_offset + data_length)\n    else:\n        left = data_length\n        readsize = blocksize\n        while left > 0:\n            if left < readsize:\n                readsize = left\n            data = infp.read(readsize)\n            # We have seen ISOs in the wild (Tribes Vengeance 1of4.iso) that\n            # lie about the size of their files, causing reads to fail (since\n            # we hit EOF before the supposed end of the file).  If we are using\n            # sendfile above, sendfile just silently returns as much data as it\n            # can, with no additional checking.  We should do the same here, so\n            # if we got less data than we asked for, abort the loop silently.\n            data_len = len(data)\n            if data_len != readsize:\n                data_len = left\n            outfp.write(data)\n            left -= data_len", "response": "This function copies data from one file object to another."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encode_space_pad(instr, length, encoding):\n    # type: (bytes, int, str) -> bytes\n    '''\n    A function to pad out an input string with spaces to the length specified.\n    The space is first encoded into the specified encoding, then appended to\n    the input string until the length is reached.\n\n    Parameters:\n     instr - The input string to encode and pad.\n     length - The length to pad the input string to.\n     encoding - The encoding to use.\n    Returns:\n     The input string encoded in the encoding and padded with encoded spaces.\n    '''\n    output = instr.decode('utf-8').encode(encoding)\n    if len(output) > length:\n        raise pycdlibexception.PyCdlibInvalidInput('Input string too long!')\n\n    encoded_space = ' '.encode(encoding)\n\n    left = length - len(output)\n    while left > 0:\n        output += encoded_space\n        left -= len(encoded_space)\n\n    if left < 0:\n        output = output[:left]\n\n    return output", "response": "A function to pad out an input string with spaces to the specified length."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normpath(path):\n    # type: (str) -> bytes\n    '''\n    A method to normalize the path, eliminating double slashes, etc.  This\n    method is a copy of the built-in python normpath, except we do *not* allow\n    double slashes at the start.\n\n    Parameters:\n     path - The path to normalize.\n    Returns:\n     The normalized path.\n    '''\n    sep = '/'\n    empty = ''\n    dot = '.'\n    dotdot = '..'\n\n    if path == empty:\n        return dot.encode('utf-8')\n\n    initial_slashes = path.startswith(sep)\n    comps = path.split(sep)\n    new_comps = []  # type: List[str]\n    for comp in comps:\n        if comp in (empty, dot):\n            continue\n        if comp != dotdot or (not initial_slashes and not new_comps) or (new_comps and new_comps[-1] == dotdot):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    newpath = sep * initial_slashes + sep.join(new_comps)\n    if sys.version_info >= (3, 0):\n        newpath_bytes = newpath.encode('utf-8')\n    else:\n        newpath_bytes = newpath.decode('utf-8').encode('utf-8')\n    return newpath_bytes or dot.encode('utf-8')", "response": "This method is a copy of the built - in python normpath method except we do not allow double slashes at the start of the path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef zero_pad(fp, data_size, pad_size):\n    # type: (BinaryIO, int, int) -> None\n    '''\n    A function to write padding out from data_size up to pad_size\n    efficiently.\n\n    Parameters:\n     fp - The file object to use to write padding out to.\n     data_size - The current size of the data.\n     pad_size - The boundary size of data to pad out to.\n    Returns:\n     Nothing.\n    '''\n    padbytes = pad_size - (data_size % pad_size)\n    if padbytes == pad_size:\n        # Nothing to pad, get out.\n        return\n\n    fp.seek(padbytes - 1, os.SEEK_CUR)\n    fp.write(b'\\x00')", "response": "A function to write padding out from data_size up to pad_size\n    efficiently."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef file_object_supports_binary(fp):\n    # type: (BinaryIO) -> bool\n    '''\n    A function to check whether a file-like object supports binary mode.\n\n    Parameters:\n     fp - The file-like object to check for binary mode support.\n    Returns:\n     True if the file-like object supports binary mode, False otherwise.\n    '''\n    if hasattr(fp, 'mode'):\n        return 'b' in fp.mode\n\n    # Python 3\n    if sys.version_info >= (3, 0):\n        return isinstance(fp, (io.RawIOBase, io.BufferedIOBase))\n\n    # Python 2\n    return isinstance(fp, (cStringIO.OutputType, cStringIO.InputType, io.RawIOBase, io.BufferedIOBase))", "response": "Returns a boolean indicating whether a file - like object supports binary mode."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, instr):\n        # type: (bytes) -> bool\n        '''\n        A method to parse ISO hybridization info out of an existing ISO.\n\n        Parameters:\n         instr - The data for the ISO hybridization.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This IsoHybrid object is already initialized')\n\n        if len(instr) != 512:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid size of the instr')\n\n        if instr[0:32] == self.ORIG_HEADER:\n            self.header = self.ORIG_HEADER\n        elif instr[0:32] == self.MAC_AFP:\n            self.header = self.MAC_AFP\n        else:\n            # If we didn't see anything that we expected, then this is not an\n            # IsoHybrid ISO, so just quietly return False\n            return False\n\n        (self.mbr, self.rba, unused1, self.mbr_id,\n         unused2) = struct.unpack_from(self.FMT, instr[:32 + struct.calcsize(self.FMT)], 32)\n\n        if unused1 != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid IsoHybrid section')\n\n        if unused2 != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid IsoHybrid section')\n\n        offset = 32 + struct.calcsize(self.FMT)\n        for i in range(1, 5):\n            if bytes(bytearray([instr[offset]])) == b'\\x80':\n                self.part_entry = i\n                (const_unused, self.bhead, self.bsect, self.bcyle, self.ptype,\n                 self.ehead, self.esect, self.ecyle, self.part_offset,\n                 self.psize) = struct.unpack_from('=BBBBBBBBLL', instr[:offset + 16], offset)\n                break\n            offset += 16\n        else:\n            raise pycdlibexception.PyCdlibInvalidISO('No valid partition found in IsoHybrid!')\n\n        if bytes(bytearray([instr[-2]])) != b'\\x55' or bytes(bytearray([instr[-1]])) != b'\\xaa':\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid tail on isohybrid section')\n\n        self.geometry_heads = self.ehead + 1\n        # FIXME: I can't see any way to compute the number of sectors from the\n        # available information.  For now, we just hard-code this at 32 and\n        # hope for the best.\n        self.geometry_sectors = 32\n\n        self._initialized = True\n\n        return True", "response": "A method to parse an ISO hybridization info out of an existing ISO."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new(self, mac, part_entry, mbr_id, part_offset,\n            geometry_sectors, geometry_heads, part_type):\n        # type: (bool, int, Optional[int], int, int, int, int) -> None\n        '''\n        A method to add ISO hybridization to an ISO.\n\n        Parameters:\n         mac - Whether this ISO should be made bootable for the Macintosh.\n         part_entry - The partition entry for the hybridization.\n         mbr_id - The mbr_id to use for the hybridization.\n         part_offset - The partition offset to use for the hybridization.\n         geometry_sectors - The number of sectors to use for the hybridization.\n         geometry_heads - The number of heads to use for the hybridization.\n         part_type - The partition type for the hybridization.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This IsoHybrid object is already initialized')\n\n        if mac:\n            self.header = self.MAC_AFP\n        else:\n            self.header = self.ORIG_HEADER\n\n        isohybrid_data_hd0 = b'\\x33\\xed\\xfa\\x8e\\xd5\\xbc\\x00\\x7c\\xfb\\xfc\\x66\\x31\\xdb\\x66\\x31\\xc9\\x66\\x53\\x66\\x51\\x06\\x57\\x8e\\xdd\\x8e\\xc5\\x52\\xbe\\x00\\x7c\\xbf\\x00\\x06\\xb9\\x00\\x01\\xf3\\xa5\\xea\\x4b\\x06\\x00\\x00\\x52\\xb4\\x41\\xbb\\xaa\\x55\\x31\\xc9\\x30\\xf6\\xf9\\xcd\\x13\\x72\\x16\\x81\\xfb\\x55\\xaa\\x75\\x10\\x83\\xe1\\x01\\x74\\x0b\\x66\\xc7\\x06\\xf1\\x06\\xb4\\x42\\xeb\\x15\\xeb\\x00\\x5a\\x51\\xb4\\x08\\xcd\\x13\\x83\\xe1\\x3f\\x5b\\x51\\x0f\\xb6\\xc6\\x40\\x50\\xf7\\xe1\\x53\\x52\\x50\\xbb\\x00\\x7c\\xb9\\x04\\x00\\x66\\xa1\\xb0\\x07\\xe8\\x44\\x00\\x0f\\x82\\x80\\x00\\x66\\x40\\x80\\xc7\\x02\\xe2\\xf2\\x66\\x81\\x3e\\x40\\x7c\\xfb\\xc0\\x78\\x70\\x75\\x09\\xfa\\xbc\\xec\\x7b\\xea\\x44\\x7c\\x00\\x00\\xe8\\x83\\x00\\x69\\x73\\x6f\\x6c\\x69\\x6e\\x75\\x78\\x2e\\x62\\x69\\x6e\\x20\\x6d\\x69\\x73\\x73\\x69\\x6e\\x67\\x20\\x6f\\x72\\x20\\x63\\x6f\\x72\\x72\\x75\\x70\\x74\\x2e\\x0d\\x0a\\x66\\x60\\x66\\x31\\xd2\\x66\\x03\\x06\\xf8\\x7b\\x66\\x13\\x16\\xfc\\x7b\\x66\\x52\\x66\\x50\\x06\\x53\\x6a\\x01\\x6a\\x10\\x89\\xe6\\x66\\xf7\\x36\\xe8\\x7b\\xc0\\xe4\\x06\\x88\\xe1\\x88\\xc5\\x92\\xf6\\x36\\xee\\x7b\\x88\\xc6\\x08\\xe1\\x41\\xb8\\x01\\x02\\x8a\\x16\\xf2\\x7b\\xcd\\x13\\x8d\\x64\\x10\\x66\\x61\\xc3\\xe8\\x1e\\x00\\x4f\\x70\\x65\\x72\\x61\\x74\\x69\\x6e\\x67\\x20\\x73\\x79\\x73\\x74\\x65\\x6d\\x20\\x6c\\x6f\\x61\\x64\\x20\\x65\\x72\\x72\\x6f\\x72\\x2e\\x0d\\x0a\\x5e\\xac\\xb4\\x0e\\x8a\\x3e\\x62\\x04\\xb3\\x07\\xcd\\x10\\x3c\\x0a\\x75\\xf1\\xcd\\x18\\xf4\\xeb\\xfd\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n\n        self.mbr = isohybrid_data_hd0\n        self.rba = 0  # This will be set later\n        self.mbr_id = mbr_id\n        if self.mbr_id is None:\n            self.mbr_id = random.getrandbits(32)\n\n        self.part_entry = part_entry\n        self.bhead = (part_offset // geometry_sectors) % geometry_heads\n        self.bsect = (part_offset % geometry_sectors) + 1\n        self.bcyle = part_offset // (geometry_heads * geometry_sectors)\n        self.bsect += (self.bcyle & 0x300) >> 2\n        self.bcyle &= 0xff\n        self.ptype = part_type\n        self.ehead = geometry_heads - 1\n        self.part_offset = part_offset\n        self.geometry_heads = geometry_heads\n        self.geometry_sectors = geometry_sectors\n\n        self._initialized = True", "response": "A method to add a new ISO hybridization."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _calc_cc(self, iso_size):\n        # type: (int) -> Tuple[int, int]\n        '''\n        A method to calculate the 'cc' and the 'padding' values for this\n        hybridization.\n\n        Parameters:\n         iso_size - The size of the ISO, excluding the hybridization.\n        Returns:\n         A tuple containing the cc value and the padding.\n        '''\n        cylsize = self.geometry_heads * self.geometry_sectors * 512\n        frac = iso_size % cylsize\n        padding = 0\n        if frac > 0:\n            padding = cylsize - frac\n        cc = (iso_size + padding) // cylsize\n        if cc > 1024:\n            cc = 1024\n\n        return (cc, padding)", "response": "Calculates the cc and padding values for this ISO."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef record_padding(self, iso_size):\n        # type: (int) -> bytes\n        '''\n        A method to record padding for the ISO hybridization.\n\n        Parameters:\n         iso_size - The size of the ISO, excluding the hybridization.\n        Returns:\n         A string of zeros the right size to pad the ISO.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This IsoHybrid object is not yet initialized')\n\n        return b'\\x00' * self._calc_cc(iso_size)[1]", "response": "A method to record padding for the ISO hybridization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse an Extended Attribute Record out of a string.", "response": "def parse(self, xastr):\n        # type: (bytes) -> None\n        '''\n        Parse an Extended Attribute Record out of a string.\n\n        Parameters:\n         xastr - The string to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This XARecord is already initialized!')\n\n        (self._group_id, self._user_id, self._attributes, signature, self._filenum,\n         unused) = struct.unpack_from(self.FMT, xastr, 0)\n\n        if signature != b'XA':\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid signature on the XARecord!')\n\n        if unused != b'\\x00\\x00\\x00\\x00\\x00':\n            raise pycdlibexception.PyCdlibInvalidISO('Unused fields should be 0')\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new Extended Attribute Record.", "response": "def new(self):\n        # type: () -> None\n        '''\n        Create a new Extended Attribute Record.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This XARecord is already initialized!')\n\n        # FIXME: we should allow the user to set these\n        self._group_id = 0\n        self._user_id = 0\n        self._attributes = 0\n        self._filenum = 0\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrecords this Extended Attribute Record. Parameters: None. Returns: A string representing this Extended Attribute Record.", "response": "def record(self):\n        # type: () -> bytes\n        '''\n        Record this Extended Attribute Record.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing this Extended Attribute Record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This XARecord is not yet initialized!')\n\n        return struct.pack(self.FMT, self._group_id, self._user_id,\n                           self._attributes, b'XA', self._filenum, b'\\x00' * 5)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a Rock Ridge Directory Record out of a string.", "response": "def parse(self, vd, record, parent):\n        # type: (headervd.PrimaryOrSupplementaryVD, bytes, Optional[DirectoryRecord]) -> str\n        '''\n        Parse a directory record out of a string.\n\n        Parameters:\n         vd - The Volume Descriptor this record is part of.\n         record - The string to parse for this record.\n         parent - The parent of this record.\n        Returns:\n         The Rock Ridge version as a string if this Directory Record has Rock\n         Ridge, None otherwise.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record already initialized')\n\n        if len(record) > 255:\n            # Since the length is supposed to be 8 bits, this should never\n            # happen.\n            raise pycdlibexception.PyCdlibInvalidISO('Directory record longer than 255 bytes!')\n\n        (self.dr_len, self.xattr_len, extent_location_le, extent_location_be,\n         data_length_le, data_length_be_unused, dr_date, self.file_flags,\n         self.file_unit_size, self.interleave_gap_size, seqnum_le, seqnum_be,\n         self.len_fi) = struct.unpack_from(self.FMT, record[:33], 0)\n\n        # In theory we should have a check here that checks to make sure that\n        # the length of the record we were passed in matches the data record\n        # length.  However, we have seen ISOs in the wild where this is\n        # incorrect, so we elide the check here.\n\n        if extent_location_le != utils.swab_32bit(extent_location_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Little-endian (%d) and big-endian (%d) extent location disagree' % (extent_location_le, utils.swab_32bit(extent_location_be)))\n        self.orig_extent_loc = extent_location_le\n\n        # Theoretically, we should check to make sure that the little endian\n        # data length is the same as the big endian data length.  In practice,\n        # though, we've seen ISOs where this is wrong.  Skip the check, and just\n        # pick the little-endian as the 'actual' size, and hope for the best.\n\n        self.data_length = data_length_le\n\n        if seqnum_le != utils.swab_16bit(seqnum_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Little-endian and big-endian seqnum disagree')\n        self.seqnum = seqnum_le\n\n        self.date = dates.DirectoryRecordDate()\n        self.date.parse(dr_date)\n\n        # OK, we've unpacked what we can from the beginning of the string.  Now\n        # we have to use the len_fi to get the rest.\n\n        self.parent = parent\n        self.vd = vd\n\n        if self.parent is None:\n            self.is_root = True\n\n            # A root directory entry should always be exactly 34 bytes.\n            # However, we have seen ISOs in the wild that get this wrong, so we\n            # elide a check for it.\n\n            self.file_ident = bytes(bytearray([record[33]]))\n\n            # A root directory entry should always have 0 as the identifier.\n            # However, we have seen ISOs in the wild that don't have this set\n            # properly to 0.  In that case, we override what we parsed out from\n            # the original with the correct value (\\x00), and hope for the best.\n            if self.file_ident != b'\\x00':\n                self.file_ident = b'\\x00'\n            self.isdir = True\n        else:\n            record_offset = 33\n            self.file_ident = record[record_offset:record_offset + self.len_fi]\n            record_offset += self.len_fi\n            if self.file_flags & (1 << self.FILE_FLAG_DIRECTORY_BIT):\n                self.isdir = True\n\n            if self.len_fi % 2 == 0:\n                record_offset += 1\n\n            if len(record[record_offset:]) >= XARecord.length():\n                xa_rec = XARecord()\n\n                try:\n                    xa_rec.parse(record[record_offset:record_offset + XARecord.length()])\n                    self.xa_record = xa_rec\n                    record_offset += XARecord.length()\n                except pycdlibexception.PyCdlibInvalidISO:\n                    # We've seen some ISOs in the wild (Windows 98 SE) that\n                    # put the XA record all the way at the back, with some\n                    # padding.  Try again from the back.\n                    try:\n                        xa_rec.parse(record[-XARecord.length():])\n                        self.xa_record = xa_rec\n                        self.xa_pad_size = len(record) - record_offset - XARecord.length()\n                        record_offset = len(record)\n                    except pycdlibexception.PyCdlibInvalidISO:\n                        pass\n\n            if len(record[record_offset:]) >= 2 and record[record_offset:record_offset + 2] in (b'SP', b'RR', b'CE', b'PX', b'ER', b'ES', b'PN', b'SL', b'NM', b'CL', b'PL', b'TF', b'SF', b'RE'):\n                self.rock_ridge = rockridge.RockRidge()\n\n                is_first_dir_record_of_root = False\n\n                if self.parent.is_root:\n                    if self.file_ident == b'\\x00':\n                        is_first_dir_record_of_root = True\n                        bytes_to_skip = 0\n                    else:\n                        if not self.parent.children:\n                            raise pycdlibexception.PyCdlibInvalidISO('Parent has no dot child')\n                        if self.parent.children[0].rock_ridge is None:\n                            raise pycdlibexception.PyCdlibInvalidISO('Dot child does not have Rock Ridge; ISO is corrupt')\n                        bytes_to_skip = self.parent.children[0].rock_ridge.bytes_to_skip\n                else:\n                    if self.parent.rock_ridge is None:\n                        raise pycdlibexception.PyCdlibInvalidISO('Parent does not have Rock Ridge; ISO is corrupt')\n                    bytes_to_skip = self.parent.rock_ridge.bytes_to_skip\n\n                self.rock_ridge.parse(record[record_offset:],\n                                      is_first_dir_record_of_root,\n                                      bytes_to_skip,\n                                      False)\n\n        if self.xattr_len != 0:\n            if self.file_flags & (1 << self.FILE_FLAG_RECORD_BIT):\n                raise pycdlibexception.PyCdlibInvalidISO('Record Bit not allowed with Extended Attributes')\n            if self.file_flags & (1 << self.FILE_FLAG_PROTECTION_BIT):\n                raise pycdlibexception.PyCdlibInvalidISO('Protection Bit not allowed with Extended Attributes')\n\n        if self.rock_ridge is None:\n            ret = ''\n        else:\n            ret = self.rock_ridge.rr_version\n\n        if self.is_root:\n            self._printable_name = b'/'\n        elif self.file_ident == b'\\x00':\n            self._printable_name = b'.'\n        elif self.file_ident == b'\\x01':\n            self._printable_name = b'..'\n        else:\n            self._printable_name = self.file_ident\n\n        self._initialized = True\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _new(self, vd, name, parent, seqnum, isdir, length, xa):\n        # type: (headervd.PrimaryOrSupplementaryVD, bytes, Optional[DirectoryRecord], int, bool, int, bool) -> None\n        '''\n        Internal method to create a new Directory Record.\n\n        Parameters:\n         vd - The Volume Descriptor this record is part of.\n         name - The name for this directory record.\n         parent - The parent of this directory record.\n         seqnum - The sequence number to associate with this directory record.\n         isdir - Whether this directory record represents a directory.\n         length - The length of the data for this directory record.\n         xa - True if this is an Extended Attribute record.\n        Returns:\n         Nothing.\n        '''\n\n        # Adding a new time should really be done when we are going to write\n        # the ISO (in record()).  Ecma-119 9.1.5 says:\n        #\n        # 'This field shall indicate the date and the time of the day at which\n        # the information in the Extent described by the Directory Record was\n        # recorded.'\n        #\n        # We create it here just to have something in the field, but we'll\n        # redo the whole thing when we are mastering.\n        self.date = dates.DirectoryRecordDate()\n        self.date.new()\n\n        if length > 2**32 - 1:\n            raise pycdlibexception.PyCdlibInvalidInput('Maximum supported file length is 2^32-1')\n\n        self.data_length = length\n\n        self.file_ident = name\n\n        self.isdir = isdir\n\n        self.seqnum = seqnum\n        # For a new directory record entry, there is no original_extent_loc,\n        # so we leave it at None.\n        self.orig_extent_loc = None\n        self.len_fi = len(self.file_ident)\n        self.dr_len = struct.calcsize(self.FMT) + self.len_fi\n\n        # From Ecma-119, 9.1.6, the file flag bits are:\n        #\n        # Bit 0 - Existence - 0 for existence known, 1 for hidden\n        # Bit 1 - Directory - 0 for file, 1 for directory\n        # Bit 2 - Associated File - 0 for not associated, 1 for associated\n        # Bit 3 - Record - 0=structure not in xattr, 1=structure in xattr\n        # Bit 4 - Protection - 0=no owner and group, 1=owner and group in xattr\n        # Bit 5 - Reserved\n        # Bit 6 - Reserved\n        # Bit 7 - Multi-extent - 0=final directory record, 1=not final directory record\n        self.file_flags = 0\n        if self.isdir:\n            self.file_flags |= (1 << self.FILE_FLAG_DIRECTORY_BIT)\n        self.file_unit_size = 0  # FIXME: we don't support setting file unit size for now\n        self.interleave_gap_size = 0  # FIXME: we don't support setting interleave gap size for now\n        self.xattr_len = 0  # FIXME: we don't support xattrs for now\n\n        self.parent = parent\n        if parent is None:\n            # If no parent, then this is the root\n            self.is_root = True\n\n        if xa:\n            self.xa_record = XARecord()\n            self.xa_record.new()\n            self.dr_len += XARecord.length()\n\n        self.dr_len += (self.dr_len % 2)\n\n        if self.is_root:\n            self._printable_name = b'/'\n        elif self.file_ident == b'\\x00':\n            self._printable_name = b'.'\n        elif self.file_ident == b'\\x01':\n            self._printable_name = b'..'\n        else:\n            self._printable_name = self.file_ident\n\n        self.vd = vd\n\n        self._initialized = True", "response": "A method to create a new Directory Record. This method is called by the new Directory Record class when it is created."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new symlink Directory Record. This implies that the new record will be Rock Ridge. Parameters: vd - The Volume Descriptor this record is part of. name - The name for this directory record. parent - The parent of this directory record. rr_target - The symlink target for this directory record. seqnum - The sequence number for this directory record. rock_ridge - The version of Rock Ridge to use for this directory record. rr_name - The Rock Ridge name for this directory record. xa - True if this is an Extended Attribute record. Returns: Nothing.", "response": "def new_symlink(self, vd, name, parent, rr_target, seqnum, rock_ridge,\n                    rr_name, xa):\n        # type: (headervd.PrimaryOrSupplementaryVD, bytes, DirectoryRecord, bytes, int, str, bytes, bool) -> None\n        '''\n        Create a new symlink Directory Record.  This implies that the new\n        record will be Rock Ridge.\n\n        Parameters:\n         vd - The Volume Descriptor this record is part of.\n         name - The name for this directory record.\n         parent - The parent of this directory record.\n         rr_target - The symlink target for this directory record.\n         seqnum - The sequence number for this directory record.\n         rock_ridge - The version of Rock Ridge to use for this directory record.\n         rr_name - The Rock Ridge name for this directory record.\n         xa - True if this is an Extended Attribute record.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record already initialized')\n\n        self._new(vd, name, parent, seqnum, False, 0, xa)\n        if rock_ridge:\n            self._rr_new(rock_ridge, rr_name, rr_target, False, False, False,\n                         0o0120555)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new_file(self, vd, length, isoname, parent, seqnum, rock_ridge, rr_name,\n                 xa, file_mode):\n        # type: (headervd.PrimaryOrSupplementaryVD, int, bytes, DirectoryRecord, int, str, bytes, bool, int) -> None\n        '''\n        Create a new file Directory Record.\n\n        Parameters:\n         vd - The Volume Descriptor this record is part of.\n         length - The length of the data.\n         isoname - The name for this directory record.\n         parent - The parent of this directory record.\n         seqnum - The sequence number for this directory record.\n         rock_ridge - Whether to make this a Rock Ridge directory record.\n         rr_name - The Rock Ridge name for this directory record.\n         xa - True if this is an Extended Attribute record.\n         file_mode - The POSIX file mode for this entry.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record already initialized')\n\n        self._new(vd, isoname, parent, seqnum, False, length, xa)\n        if rock_ridge:\n            self._rr_new(rock_ridge, rr_name, b'', False, False, False,\n                         file_mode)", "response": "A method to create a new file Directory Record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new root Directory Record. Parameters: vd - The Volume Descriptor this record is part of. seqnum - The sequence number for this directory record. log_block_size - The logical block size to use. Returns: Nothing.", "response": "def new_root(self, vd, seqnum, log_block_size):\n        # type: (headervd.PrimaryOrSupplementaryVD, int, int) -> None\n        '''\n        Create a new root Directory Record.\n\n        Parameters:\n         vd - The Volume Descriptor this record is part of.\n         seqnum - The sequence number for this directory record.\n         log_block_size - The logical block size to use.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record already initialized')\n\n        self._new(vd, b'\\x00', None, seqnum, True, log_block_size, False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_dot(self, vd, parent, seqnum, rock_ridge, log_block_size, xa,\n                file_mode):\n        # type: (headervd.PrimaryOrSupplementaryVD, DirectoryRecord, int, str, int, bool, int) -> None\n        '''\n        Create a new 'dot' Directory Record.\n\n        Parameters:\n         vd - The Volume Descriptor this record is part of.\n         parent - The parent of this directory record.\n         seqnum - The sequence number for this directory record.\n         rock_ridge - Whether to make this a Rock Ridge directory record.\n         log_block_size - The logical block size to use.\n         xa - True if this is an Extended Attribute record.\n         file_mode - The POSIX file mode to set for this directory.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record already initialized')\n\n        self._new(vd, b'\\x00', parent, seqnum, True, log_block_size, xa)\n        if rock_ridge:\n            self._rr_new(rock_ridge, b'', b'', False, False, False, file_mode)", "response": "A method to create a new DOT Directory Record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new_dotdot(self, vd, parent, seqnum, rock_ridge, log_block_size,\n                   rr_relocated_parent, xa, file_mode):\n        # type: (headervd.PrimaryOrSupplementaryVD, DirectoryRecord, int, str, int, bool, bool, int) -> None\n        '''\n        Create a new 'dotdot' Directory Record.\n\n        Parameters:\n         vd - The Volume Descriptor this record is part of.\n         parent - The parent of this directory record.\n         seqnum - The sequence number for this directory record.\n         rock_ridge - Whether to make this a Rock Ridge directory record.\n         log_block_size - The logical block size to use.\n         rr_relocated_parent - True if this is a Rock Ridge relocated parent.\n         xa - True if this is an Extended Attribute record.\n         file_mode - The POSIX file mode to set for this directory.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record already initialized')\n\n        self._new(vd, b'\\x01', parent, seqnum, True, log_block_size, xa)\n        if rock_ridge:\n            self._rr_new(rock_ridge, b'', b'', False, False, rr_relocated_parent, file_mode)", "response": "A method to create a new dotdot Directory Record."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef new_dir(self, vd, name, parent, seqnum, rock_ridge, rr_name, log_block_size,\n                rr_relocated_child, rr_relocated, xa, file_mode):\n        # type: (headervd.PrimaryOrSupplementaryVD, bytes, DirectoryRecord, int, str, bytes, int, bool, bool, bool, int) -> None\n        '''\n        Create a new directory Directory Record.\n\n        Parameters:\n         vd - The Volume Descriptor this record is part of.\n         name - The name for this directory record.\n         parent - The parent of this directory record.\n         seqnum - The sequence number for this directory record.\n         rock_ridge - Whether to make this a Rock Ridge directory record.\n         rr_name - The Rock Ridge name for this directory record.\n         log_block_size - The logical block size to use.\n         rr_relocated_child - True if this is a Rock Ridge relocated child.\n         rr_relocated - True if this is a Rock Ridge relocated entry.\n         xa - True if this is an Extended Attribute record.\n         file_mode - The POSIX file mode to set for this directory.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record already initialized')\n\n        self._new(vd, name, parent, seqnum, True, log_block_size, xa)\n        if rock_ridge:\n            self._rr_new(rock_ridge, rr_name, b'', rr_relocated_child,\n                         rr_relocated, False, file_mode)\n            if rr_relocated_child and self.rock_ridge:\n                # Relocated Rock Ridge entries are not exactly treated as directories, so\n                # fix things up here.\n                self.isdir = False\n                self.file_flags = 0\n                self.rock_ridge.add_to_file_links()", "response": "A method to create a new directory Directory Record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the ISO9660 existence flag of this Directory Record. Parameters: is_hidden - True if this Directory Record should be hidden, False otherwise. Returns: Nothing.", "response": "def change_existence(self, is_hidden):\n        # type: (bool) -> None\n        '''\n        Change the ISO9660 existence flag of this Directory Record.\n\n        Parameters:\n         is_hidden - True if this Directory Record should be hidden, False otherwise.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record not yet initialized')\n\n        if is_hidden:\n            self.file_flags |= (1 << self.FILE_FLAG_EXISTENCE_BIT)\n        else:\n            self.file_flags &= ~(1 << self.FILE_FLAG_EXISTENCE_BIT)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_child(self, child, logical_block_size, allow_duplicate=False):\n        # type: (DirectoryRecord, int, bool) -> bool\n        '''\n        A method to add a new child to this directory record.\n\n        Parameters:\n         child - The child directory record object to add.\n         logical_block_size - The size of a logical block for this volume descriptor.\n         allow_duplicate - Whether to allow duplicate names, as there are\n                           situations where duplicate children are allowed.\n        Returns:\n         True if adding this child caused the directory to overflow into another\n         extent, False otherwise.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record not yet initialized')\n\n        return self._add_child(child, logical_block_size, allow_duplicate, True)", "response": "A method to add a new child to this directory record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_child(self, child, index, logical_block_size):\n        # type: (DirectoryRecord, int, int) -> bool\n        '''\n        A method to remove a child from this Directory Record.\n\n        Parameters:\n         child - The child DirectoryRecord object to remove.\n         index - The index of the child into this DirectoryRecord children list.\n         logical_block_size - The size of a logical block on this volume descriptor.\n        Returns:\n         True if removing this child caused an underflow, False otherwise.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record not yet initialized')\n\n        if index < 0:\n            # This should never happen\n            raise pycdlibexception.PyCdlibInternalError('Invalid child index to remove')\n\n        # Unfortunately, Rock Ridge specifies that a CL 'directory' is replaced\n        # by a *file*, not another directory.  Thus, we can't just depend on\n        # whether this child is marked as a directory by the file flags during\n        # parse time.  Instead, we check if this is either a true directory,\n        # or a Rock Ridge CL entry, and in either case try to manipulate the\n        # file links.\n        if child.rock_ridge is not None:\n            if child.isdir or child.rock_ridge.child_link_record_exists():\n                if len(self.children) < 2:\n                    raise pycdlibexception.PyCdlibInvalidISO('Expected a dot and dotdot entry, but missing; ISO is corrupt')\n                if self.children[0].rock_ridge is None or self.children[1].rock_ridge is None:\n                    raise pycdlibexception.PyCdlibInvalidISO('Missing Rock Ridge entry on dot or dotdot; ISO is corrupt')\n\n                if self.parent is None:\n                    self.children[0].rock_ridge.remove_from_file_links()\n                    self.children[1].rock_ridge.remove_from_file_links()\n                else:\n                    if self.rock_ridge is None:\n                        raise pycdlibexception.PyCdlibInvalidISO('Child has Rock Ridge, but parent does not; ISO is corrupt')\n                    self.rock_ridge.remove_from_file_links()\n\n                    self.children[0].rock_ridge.remove_from_file_links()\n\n        del self.children[index]\n\n        # We now have to check if we need to remove a logical block.\n        # We have to iterate over the entire list again, because where we\n        # removed this last entry may rearrange the empty spaces in the blocks\n        # that we've already allocated.\n        num_extents, dirrecord_offset = self._recalculate_extents_and_offsets(index,\n                                                                              logical_block_size)\n\n        underflow = False\n        total_size = (num_extents - 1) * logical_block_size + dirrecord_offset\n        if (self.data_length - total_size) > logical_block_size:\n            self.data_length -= logical_block_size\n            # We also have to make sure to update the length of the dot child,\n            # as that should always reflect the length.\n            self.children[0].data_length = self.data_length\n            # We also have to update all of the dotdot entries.  If this is\n            # the root directory record (no parent), we first update the root\n            # dotdot entry.  In all cases, we update the dotdot entry of all\n            # children that are directories.\n            if self.parent is None:\n                self.children[1].data_length = self.data_length\n\n            for c in self.children:\n                if not c.is_dir():\n                    continue\n                if len(c.children) > 1:\n                    c.children[1].data_length = self.data_length\n            underflow = True\n\n        return underflow", "response": "A method to remove a child from this Directory Record."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef record(self):\n        # type: () -> bytes\n        '''\n        A method to generate the string representing this Directory Record.\n\n        Parameters:\n         None.\n        Returns:\n         String representing this Directory Record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record not yet initialized')\n\n        # Ecma-119 9.1.5 says the date should reflect the time when the\n        # record was written, so we make a new date now and use that to\n        # write out the record.\n        self.date = dates.DirectoryRecordDate()\n        self.date.new()\n\n        padlen = struct.calcsize(self.FMT) + self.len_fi\n        padstr = b'\\x00' * (padlen % 2)\n\n        extent_loc = self._extent_location()\n\n        xa_rec = b''\n        if self.xa_record is not None:\n            xa_rec = b'\\x00' * self.xa_pad_size + self.xa_record.record()\n        rr_rec = b''\n        if self.rock_ridge is not None:\n            rr_rec = self.rock_ridge.record_dr_entries()\n\n        outlist = [struct.pack(self.FMT, self.dr_len, self.xattr_len,\n                               extent_loc, utils.swab_32bit(extent_loc),\n                               self.data_length, utils.swab_32bit(self.data_length),\n                               self.date.record(), self.file_flags,\n                               self.file_unit_size, self.interleave_gap_size,\n                               self.seqnum, utils.swab_16bit(self.seqnum),\n                               self.len_fi) + self.file_ident + padstr + xa_rec + rr_rec]\n\n        outlist.append(b'\\x00' * (len(outlist[0]) % 2))\n\n        return b''.join(outlist)", "response": "A method to generate the string representing this Directory Record. This method is used to generate the string representing this Directory Record."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_ptr(self, ptr):\n        # type: (path_table_record.PathTableRecord) -> None\n        '''\n        A method to set the Path Table Record associated with this Directory\n        Record.\n\n        Parameters:\n         ptr - The path table record to associate with this Directory Record.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record not yet initialized')\n\n        self.ptr = ptr", "response": "A method to set the Path Table Record associated with this Directory Record. This method is only valid if the Path Table Record is already set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing a new Inode. Parameters: None. Returns: Nothing.", "response": "def new(self, length, fp, manage_fp, offset):\n        # type: (int, BinaryIO, bool, int) -> None\n        '''\n        Initialize a new Inode.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Inode is already initialized')\n\n        self.data_length = length\n\n        self.data_fp = fp\n        self.manage_fp = manage_fp\n        self.fp_offset = offset\n        self.original_data_location = self.DATA_IN_EXTERNAL_FP\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(self, extent, length, fp, log_block_size):\n        # type: (int, int, BinaryIO, int) -> None\n        '''\n        Parse an existing Inode.  This just saves off the extent for later use.\n\n        Parameters:\n         extent - The original extent that the data lives at.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Inode is already initialized')\n\n        self.orig_extent_loc = extent\n\n        self.data_length = length\n\n        self.data_fp = fp\n        self.manage_fp = False\n        self.fp_offset = extent * log_block_size\n        self.original_data_location = self.DATA_ON_ORIGINAL_ISO\n\n        self._initialized = True", "response": "A method to parse an existing Inode. This just saves off the extent for later use."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_boot_info_table(self, boot_info_table):\n        # type: (eltorito.EltoritoBootInfoTable) -> None\n        '''\n        A method to add a boot info table to this Inode.\n\n        Parameters:\n         boot_info_table - The Boot Info Table object to add to this Inode.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Inode is not yet initialized')\n\n        self.boot_info_table = boot_info_table", "response": "A method to add a boot info table to this Inode."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_fp(self, fp, length):\n        # type: (BinaryIO, int) -> None\n        '''\n        Update the Inode to use a different file object and length.\n\n        Parameters:\n         fp - A file object that contains the data for this Inode.\n         length - The length of the data.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Inode is not yet initialized')\n\n        self.original_data_location = self.DATA_IN_EXTERNAL_FP\n        self.data_fp = fp\n        self.data_length = length\n        self.fp_offset = 0", "response": "Update the Inode to use a different file object and length."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, datestr):\n        # type: (bytes) -> None\n        '''\n        Parse a Directory Record date out of a string.\n\n        Parameters:\n         datestr - The string to parse the date out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record Date already initialized')\n\n        (self.years_since_1900, self.month, self.day_of_month, self.hour,\n         self.minute, self.second,\n         self.gmtoffset) = struct.unpack_from(self.FMT, datestr, 0)\n\n        self._initialized = True", "response": "Parse a Directory Record date out of a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new(self):\n        # type: () -> None\n        '''\n        Create a new Directory Record date based on the current time.\n\n        Parameters:\n         tm - An optional argument that must be None\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record Date already initialized')\n\n        # This algorithm was ported from cdrkit, genisoimage.c:iso9660_date()\n        tm = time.time()\n        local = time.localtime(tm)\n        self.years_since_1900 = local.tm_year - 1900\n        self.month = local.tm_mon\n        self.day_of_month = local.tm_mday\n        self.hour = local.tm_hour\n        self.minute = local.tm_min\n        self.second = local.tm_sec\n        self.gmtoffset = utils.gmtoffset_from_tm(tm, local)\n        self._initialized = True", "response": "A method to create a new Directory Record date based on the current time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef record(self):\n        # type: () -> bytes\n        '''\n        Return a string representation of the Directory Record date.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing this Directory Record Date.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Directory Record Date not initialized')\n\n        return struct.pack(self.FMT, self.years_since_1900, self.month,\n                           self.day_of_month, self.hour, self.minute,\n                           self.second, self.gmtoffset)", "response": "A method to generate the string representation of this Directory Record date."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a Volume Descriptor Date out of a string.", "response": "def parse(self, datestr):\n        # type: (bytes) -> None\n        '''\n        Parse a Volume Descriptor Date out of a string.  A string of all zeros\n        is valid, which means that the date in this field was not specified.\n\n        Parameters:\n          datestr - string to be parsed\n        Returns:\n          Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Volume Descriptor Date object is already initialized')\n\n        if len(datestr) != 17:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid ISO9660 date string')\n\n        timestruct = string_to_timestruct(datestr[:-3])\n        self.year = timestruct.tm_year\n        self.month = timestruct.tm_mon\n        self.dayofmonth = timestruct.tm_mday\n        self.hour = timestruct.tm_hour\n        self.minute = timestruct.tm_min\n        self.second = timestruct.tm_sec\n        if timestruct.tm_year == 0 and timestruct.tm_mon == 0 and timestruct.tm_mday == 0 and timestruct.tm_hour == 0 and timestruct.tm_min == 0 and timestruct.tm_sec == 0:\n            self.hundredthsofsecond = 0\n            self.gmtoffset = 0\n            self.date_str = self.EMPTY_STRING\n        else:\n            self.hundredthsofsecond = int(datestr[14:15])\n            self.gmtoffset, = struct.unpack_from('=b', datestr, 16)\n            self.date_str = datestr\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new(self, tm=0.0):\n        # type: (float) -> None\n        '''\n        Create a new Volume Descriptor Date.  If tm is None, then this Volume\n        Descriptor Date will be full of zeros (meaning not specified).  If tm\n        is not None, it is expected to be a struct_time object, at which point\n        this Volume Descriptor Date object will be filled in with data from that\n        struct_time.\n\n        Parameters:\n          tm - struct_time object to base new VolumeDescriptorDate off of,\n               or 0.0 for an empty VolumeDescriptorDate.\n        Returns:\n          Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Volume Descriptor Date object is already initialized')\n\n        if tm != 0.0:\n            local = time.localtime(tm)\n            self.year = local.tm_year\n            self.month = local.tm_mon\n            self.dayofmonth = local.tm_mday\n            self.hour = local.tm_hour\n            self.minute = local.tm_min\n            self.second = local.tm_sec\n            self.hundredthsofsecond = 0\n            self.gmtoffset = utils.gmtoffset_from_tm(tm, local)\n            self.date_str = time.strftime(self.TIME_FMT, local).encode('utf-8') + '{:0<2}'.format(self.hundredthsofsecond).encode('utf-8') + struct.pack('=b', self.gmtoffset)\n        else:\n            self.year = 0\n            self.month = 0\n            self.dayofmonth = 0\n            self.hour = 0\n            self.minute = 0\n            self.second = 0\n            self.hundredthsofsecond = 0\n            self.gmtoffset = 0\n            self.date_str = self.EMPTY_STRING\n\n        self._initialized = True", "response": "A method to create a new Volume Descriptor Date object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge Sharing Protocol record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SP record already initialized!')\n\n        (su_len, su_entry_version_unused, check_byte1, check_byte2,\n         self.bytes_to_skip) = struct.unpack_from('=BBBBB', rrstr[:7], 2)\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        if su_len != RRSPRecord.length():\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid length on rock ridge extension')\n        if check_byte1 != 0xbe or check_byte2 != 0xef:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid check bytes on rock ridge extension')\n\n        self._initialized = True", "response": "Parse a Rock Ridge Sharing Protocol record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new Rock Ridge Sharing Protocol record.", "response": "def new(self, bytes_to_skip):\n        # type: (int) -> None\n        '''\n        Create a new Rock Ridge Sharing Protocol record.\n\n        Parameters:\n        bytes_to_skip - The number of bytes to skip.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SP record already initialized!')\n\n        self.bytes_to_skip = bytes_to_skip\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Sharing Protocol record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SP record not yet initialized!')\n\n        return b'SP' + struct.pack('=BBBBB', RRSPRecord.length(), SU_ENTRY_VERSION, 0xbe, 0xef, self.bytes_to_skip)", "response": "Generate a string representing the Rock Ridge Sharing Protocol record."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new(self):\n        # type: () -> None\n        '''\n        Create a new Rock Ridge Rock Ridge record.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('RR record already initialized!')\n\n        self.rr_flags = 0\n        self._initialized = True", "response": "Create a new Rock Ridge record."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend a field to the Rock Ridge record.", "response": "def append_field(self, fieldname):\n        # type: (str) -> None\n        '''\n        Mark a field as present in the Rock Ridge records.\n\n        Parameters:\n         fieldname - The name of the field to mark as present; should be one\n                     of 'PX', 'PN', 'SL', 'NM', 'CL', 'PL', 'RE', or 'TF'.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('RR record not yet initialized!')\n\n        if fieldname == 'PX':\n            bit = 0\n        elif fieldname == 'PN':\n            bit = 1\n        elif fieldname == 'SL':\n            bit = 2\n        elif fieldname == 'NM':\n            bit = 3\n        elif fieldname == 'CL':\n            bit = 4\n        elif fieldname == 'PL':\n            bit = 5\n        elif fieldname == 'RE':\n            bit = 6\n        elif fieldname == 'TF':\n            bit = 7\n        else:\n            raise pycdlibexception.PyCdlibInternalError('Unknown RR field name %s' % (fieldname))\n\n        self.rr_flags |= (1 << bit)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Rock Ridge record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('RR record not yet initialized!')\n\n        return b'RR' + struct.pack('=BBB', RRRRRecord.length(), SU_ENTRY_VERSION, self.rr_flags)", "response": "Generate a string representing the Rock Ridge record."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge Continuation Entry record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('CE record already initialized!')\n\n        (su_len, su_entry_version_unused, bl_cont_area_le, bl_cont_area_be,\n         offset_cont_area_le, offset_cont_area_be,\n         len_cont_area_le, len_cont_area_be) = struct.unpack_from('=BBLLLLLL', rrstr[:28], 2)\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        if su_len != RRCERecord.length():\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid length on rock ridge extension')\n\n        if bl_cont_area_le != utils.swab_32bit(bl_cont_area_be):\n            raise pycdlibexception.PyCdlibInvalidISO('CE record big and little endian continuation area do not agree')\n\n        if offset_cont_area_le != utils.swab_32bit(offset_cont_area_be):\n            raise pycdlibexception.PyCdlibInvalidISO('CE record big and little endian continuation area offset do not agree')\n\n        if len_cont_area_le != utils.swab_32bit(len_cont_area_be):\n            raise pycdlibexception.PyCdlibInvalidISO('CE record big and little endian continuation area length do not agree')\n\n        self.bl_cont_area = bl_cont_area_le\n        self.offset_cont_area = offset_cont_area_le\n        self.len_cont_area = len_cont_area_le\n\n        self._initialized = True", "response": "Parse a Rock Ridge Continuation Entry record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new Rock Ridge Continuation Entry record.", "response": "def new(self):\n        # type: () -> None\n        '''\n        Create a new Rock Ridge Continuation Entry record.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('CE record already initialized!')\n\n        self.bl_cont_area = 0  # This will get set during reshuffle_extents\n        self.offset_cont_area = 0  # This will get set during reshuffle_extents\n        self.len_cont_area = 0  # This will be calculated based on fields put in\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the extent for this CE record.", "response": "def update_extent(self, extent):\n        # type: (int) -> None\n        '''\n        Update the extent for this CE record.\n\n        Parameters:\n         extent - The new extent for this CE record.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('CE record not yet initialized!')\n\n        self.bl_cont_area = extent"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_offset(self, offset):\n        # type: (int) -> None\n        '''\n        Update the offset for this CE record.\n\n        Parameters:\n         extent - The new offset for this CE record.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('CE record not yet initialized!')\n\n        self.offset_cont_area = offset", "response": "Update the offset for this CE record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_record(self, length):\n        # type: (int) -> None\n        '''\n        Add some more length to this CE record.  Used when a new record is going\n        to get recorded into the CE (rather than the DR).\n\n        Parameters:\n         length - The length to add to this CE record.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('CE record not yet initialized!')\n\n        self.len_cont_area += length", "response": "Add some more length to this CE record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Continuation Entry record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('CE record not yet initialized!')\n\n        return b'CE' + struct.pack('=BBLLLLLL',\n                                   RRCERecord.length(),\n                                   SU_ENTRY_VERSION,\n                                   self.bl_cont_area,\n                                   utils.swab_32bit(self.bl_cont_area),\n                                   self.offset_cont_area,\n                                   utils.swab_32bit(self.offset_cont_area),\n                                   self.len_cont_area,\n                                   utils.swab_32bit(self.len_cont_area))", "response": "Generate a string representing the Rock Ridge Continuation Entry record."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a Rock Ridge POSIX File Attributes record out of a string.", "response": "def parse(self, rrstr):\n        # type: (bytes) -> int\n        '''\n        Parse a Rock Ridge POSIX File Attributes record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         A string representing the RR version, either 1.09 or 1.12.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PX record already initialized!')\n\n        (su_len, su_entry_version_unused, posix_file_mode_le, posix_file_mode_be,\n         posix_file_links_le, posix_file_links_be, posix_file_user_id_le,\n         posix_file_user_id_be, posix_file_group_id_le,\n         posix_file_group_id_be) = struct.unpack_from('=BBLLLLLLLL', rrstr[:38], 2)\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        if posix_file_mode_le != utils.swab_32bit(posix_file_mode_be):\n            raise pycdlibexception.PyCdlibInvalidISO('PX record big and little-endian file mode do not agree')\n\n        if posix_file_links_le != utils.swab_32bit(posix_file_links_be):\n            raise pycdlibexception.PyCdlibInvalidISO('PX record big and little-endian file links do not agree')\n\n        if posix_file_user_id_le != utils.swab_32bit(posix_file_user_id_be):\n            raise pycdlibexception.PyCdlibInvalidISO('PX record big and little-endian file user ID do not agree')\n\n        if posix_file_group_id_le != utils.swab_32bit(posix_file_group_id_be):\n            raise pycdlibexception.PyCdlibInvalidISO('PX record big and little-endian file group ID do not agree')\n\n        # In Rock Ridge 1.09 and 1.10, there is no serial number so the su_len\n        # is 36, while in Rock Ridge 1.12, there is an 8-byte serial number so\n        # su_len is 44.\n        if su_len == 36:\n            posix_file_serial_number_le = 0\n        elif su_len == 44:\n            (posix_file_serial_number_le,\n             posix_file_serial_number_be) = struct.unpack_from('=LL',\n                                                               rrstr[:44], 36)\n            if posix_file_serial_number_le != utils.swab_32bit(posix_file_serial_number_be):\n                raise pycdlibexception.PyCdlibInvalidISO('PX record big and little-endian file serial number do not agree')\n        else:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid length on Rock Ridge PX record')\n\n        self.posix_file_mode = posix_file_mode_le\n        self.posix_file_links = posix_file_links_le\n        self.posix_user_id = posix_file_user_id_le\n        self.posix_group_id = posix_file_group_id_le\n        self.posix_serial_number = posix_file_serial_number_le\n\n        self._initialized = True\n\n        return su_len"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef new(self, mode):\n        # type: (int) -> None\n        '''\n        Create a new Rock Ridge POSIX File Attributes record.\n\n        Parameters:\n         mode - The Unix file mode for this record.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PX record already initialized!')\n\n        self.posix_file_mode = mode\n        self.posix_file_links = 1\n        self.posix_user_id = 0\n        self.posix_group_id = 0\n        self.posix_serial_number = 0\n\n        self._initialized = True", "response": "Create a new Rock Ridge POSIX File Attributes record."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a string representing the Rock Ridge POSIX File Attributes record.", "response": "def record(self, rr_version):\n        # type: (str) -> bytes\n        '''\n        Generate a string representing the Rock Ridge POSIX File Attributes\n        record.\n\n        Parameters:\n         rr_version - The Rock Ridge version to use.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PX record not yet initialized!')\n\n        outlist = [b'PX', struct.pack('=BBLLLLLLLL', RRPXRecord.length(rr_version),\n                                      SU_ENTRY_VERSION, self.posix_file_mode,\n                                      utils.swab_32bit(self.posix_file_mode),\n                                      self.posix_file_links,\n                                      utils.swab_32bit(self.posix_file_links),\n                                      self.posix_user_id,\n                                      utils.swab_32bit(self.posix_user_id),\n                                      self.posix_group_id,\n                                      utils.swab_32bit(self.posix_group_id))]\n        if rr_version == '1.12':\n            outlist.append(struct.pack('=LL', self.posix_serial_number,\n                                       utils.swab_32bit(self.posix_serial_number)))\n        # The rr_version can never be \"wrong\" at this point; if it was, it would\n        # have thrown an exception earlier when calling length().  So just skip\n        # any potential checks here.\n\n        return b''.join(outlist)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge Extensions Reference record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('ER record already initialized!')\n\n        (su_len, su_entry_version_unused, len_id, len_des, len_src,\n         self.ext_ver) = struct.unpack_from('=BBBBBB', rrstr[:8], 2)\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        # Ensure that the length isn't crazy\n        if su_len > len(rrstr):\n            raise pycdlibexception.PyCdlibInvalidISO('Length of ER record much too long')\n\n        # Also ensure that the combination of len_id, len_des, and len_src\n        # doesn't overrun su_len; because of the check above, this means it\n        # can't overrun len(rrstr) either\n        total_length = len_id + len_des + len_src\n        if total_length > su_len:\n            raise pycdlibexception.PyCdlibInvalidISO('Combined length of ER ID, des, and src longer than record')\n\n        fmtstr = '=%ds%ds%ds' % (len_id, len_des, len_src)\n        (self.ext_id, self.ext_des, self.ext_src) = struct.unpack_from(fmtstr, rrstr, 8)\n\n        self._initialized = True", "response": "Parse a Rock Ridge Extensions Reference record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new Rock Ridge Extensions Reference record.", "response": "def new(self, ext_id, ext_des, ext_src):\n        # type: (bytes, bytes, bytes) -> None\n        '''\n        Create a new Rock Ridge Extensions Reference record.\n\n        Parameters:\n         ext_id - The extension identifier to use.\n         ext_des - The extension descriptor to use.\n         ext_src - The extension specification source to use.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('ER record already initialized!')\n\n        self.ext_id = ext_id\n        self.ext_des = ext_des\n        self.ext_src = ext_src\n        self.ext_ver = 1\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Extensions Reference\n        record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('ER record not yet initialized!')\n\n        return b'ER' + struct.pack('=BBBBBB', RRERRecord.length(self.ext_id, self.ext_des, self.ext_src), SU_ENTRY_VERSION, len(self.ext_id), len(self.ext_des), len(self.ext_src), self.ext_ver) + self.ext_id + self.ext_des + self.ext_src", "response": "Generate a string representing the Rock Ridge Extensions Reference\n        record."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new Rock Ridge Extension Selector record.", "response": "def new(self, extension_sequence):\n        # type: (int) -> None\n        '''\n        Create a new Rock Ridge Extension Selector record.\n\n        Parameters:\n         extension_sequence - The sequence number of this extension.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('ES record already initialized!')\n\n        self.extension_sequence = extension_sequence\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Extension Selector record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('ES record not yet initialized!')\n\n        return b'ES' + struct.pack('=BBB', RRESRecord.length(), SU_ENTRY_VERSION, self.extension_sequence)", "response": "Generate a string representing the Rock Ridge Extension Selector record."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a Rock Ridge POSIX Device Number record out of a string.", "response": "def parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge POSIX Device Number record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PN record already initialized!')\n\n        (su_len, su_entry_version_unused, dev_t_high_le, dev_t_high_be,\n         dev_t_low_le, dev_t_low_be) = struct.unpack_from('=BBLLLL', rrstr[:20], 2)\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        if su_len != RRPNRecord.length():\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid length on rock ridge extension')\n\n        if dev_t_high_le != utils.swab_32bit(dev_t_high_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Dev_t high little-endian does not match big-endian')\n\n        if dev_t_low_le != utils.swab_32bit(dev_t_low_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Dev_t low little-endian does not match big-endian')\n\n        self.dev_t_high = dev_t_high_le\n        self.dev_t_low = dev_t_low_le\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new Rock Ridge POSIX device number record. Parameters: dev_t_high - The high-order 32-bits of the device number. dev_t_low - The low-order 32-bits of the device number. Returns: Nothing.", "response": "def new(self, dev_t_high, dev_t_low):\n        # type: (int, int) -> None\n        '''\n        Create a new Rock Ridge POSIX device number record.\n\n        Parameters:\n         dev_t_high - The high-order 32-bits of the device number.\n         dev_t_low - The low-order 32-bits of the device number.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PN record already initialized!')\n\n        self.dev_t_high = dev_t_high\n        self.dev_t_low = dev_t_low\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge POSIX Device Number\n        record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PN record not yet initialized!')\n\n        return b'PN' + struct.pack('=BBLLLL', RRPNRecord.length(), SU_ENTRY_VERSION, self.dev_t_high, utils.swab_32bit(self.dev_t_high), self.dev_t_low, utils.swab_32bit(self.dev_t_low))", "response": "Generate a string representing the Rock Ridge POSIX Device Number record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge Symbolic Link record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SL record already initialized!')\n\n        (su_len, su_entry_version_unused, self.flags) = struct.unpack_from('=BBB', rrstr[:5], 2)\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        cr_offset = 5\n        data_len = su_len - 5\n        while data_len > 0:\n            (cr_flags, len_cp) = struct.unpack_from('=BB', rrstr[:cr_offset + 2], cr_offset)\n\n            data_len -= 2\n            cr_offset += 2\n\n            self.symlink_components.append(self.Component(cr_flags, len_cp,\n                                                          rrstr[cr_offset:cr_offset + len_cp]))\n\n            # FIXME: if this is the last component in this SL record,\n            # but the component continues on in the next SL record, we will\n            # fail to record this bit.  We should fix that.\n\n            cr_offset += len_cp\n            data_len -= len_cp\n\n        self._initialized = True", "response": "Parse a Rock Ridge Symbolic Link record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a new component to this symlink record. Parameters: symlink_comp - The string to add to this symlink record. Returns: Nothing.", "response": "def add_component(self, symlink_comp):\n        # type: (bytes) -> None\n        '''\n        Add a new component to this symlink record.\n\n        Parameters:\n         symlink_comp - The string to add to this symlink record.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SL record not yet initialized!')\n\n        if (self.current_length() + RRSLRecord.Component.length(symlink_comp)) > 255:\n            raise pycdlibexception.PyCdlibInvalidInput('Symlink would be longer than 255')\n\n        self.symlink_components.append(self.Component.factory(symlink_comp))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef current_length(self):\n        # type: () -> int\n        '''\n        Calculate the current length of this symlink record.\n\n        Parameters:\n         None.\n        Returns:\n         Length of this symlink record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SL record not yet initialized!')\n\n        strlist = []\n        for comp in self.symlink_components:\n            strlist.append(comp.name())\n\n        return RRSLRecord.length(strlist)", "response": "Calculate the current length of this symlink record."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Symbolic Link record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SL record not yet initialized!')\n\n        outlist = [b'SL', struct.pack('=BBB', self.current_length(), SU_ENTRY_VERSION, self.flags)]\n        for comp in self.symlink_components:\n            outlist.append(comp.record())\n\n        return b''.join(outlist)", "response": "Generate a string representing the Rock Ridge Symbolic Link record."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a string that contains all components of the symlink.", "response": "def name(self):\n        # type: () -> bytes\n        '''\n        Generate a string that contains all components of the symlink.\n\n        Parameters:\n         None\n        Returns:\n         String containing all components of the symlink.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SL record not yet initialized!')\n\n        outlist = []  # type: List[bytes]\n        continued = False\n        for comp in self.symlink_components:\n            name = comp.name()\n            if name == b'/':\n                outlist = []\n                continued = False\n                name = b''\n\n            if not continued:\n                outlist.append(name)\n            else:\n                outlist[-1] += name\n\n            continued = comp.is_continued()\n\n        return b'/'.join(outlist)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the previous component of this SL record to continued. Parameters: None. Returns: Nothing.", "response": "def set_last_component_continued(self):\n        # type: () -> None\n        '''\n        Set the previous component of this SL record to continued.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SL record not yet initialized!')\n\n        if not self.symlink_components:\n            raise pycdlibexception.PyCdlibInternalError('Trying to set continued on a non-existent component!')\n\n        self.symlink_components[-1].set_continued()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines whether the previous component of this SL record is a continued one or not. Parameters: None. Returns: True if the previous component of this SL record is continued, False otherwise.", "response": "def last_component_continued(self):\n        # type: () -> bool\n        '''\n        Determines whether the previous component of this SL record is a\n        continued one or not.\n\n        Parameters:\n         None.\n        Returns:\n         True if the previous component of this SL record is continued, False otherwise.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SL record not yet initialized!')\n\n        if not self.symlink_components:\n            raise pycdlibexception.PyCdlibInternalError('Trying to get continued on a non-existent component!')\n\n        return self.symlink_components[-1].is_continued()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef length(symlink_components):\n        # type: (List[bytes]) -> int\n        '''\n        Static method to return the length of the Rock Ridge Symbolic Link\n        record.\n\n        Parameters:\n         symlink_components - A list containing a string for each of the\n                              symbolic link components.\n        Returns:\n         The length of this record in bytes.\n        '''\n        length = RRSLRecord.header_length()\n        for comp in symlink_components:\n            length += RRSLRecord.Component.length(comp)\n        return length", "response": "Returns the length of the Rock Ridge Symbolic Link record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge Alternate Name record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('NM record already initialized!')\n\n        (su_len, su_entry_version_unused, self.posix_name_flags) = struct.unpack_from('=BBB', rrstr[:5], 2)\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        name_len = su_len - 5\n        if (self.posix_name_flags & 0x7) not in (0, 1, 2, 4):\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid Rock Ridge NM flags')\n\n        if name_len != 0:\n            if (self.posix_name_flags & (1 << 1)) or (self.posix_name_flags & (1 << 2)) or (self.posix_name_flags & (1 << 5)):\n                raise pycdlibexception.PyCdlibInvalidISO('Invalid name in Rock Ridge NM entry (0x%x %d)' % (self.posix_name_flags, name_len))\n            self.posix_name += rrstr[5:5 + name_len]\n\n        self._initialized = True", "response": "Parse a Rock Ridge Alternate Name record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new NM record.", "response": "def new(self, rr_name):\n        # type: (bytes) -> None\n        '''\n        Create a new Rock Ridge Alternate Name record.\n\n        Parameters:\n         rr_name - The name for the new record.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('NM record already initialized!')\n\n        self.posix_name = rr_name\n        self.posix_name_flags = 0\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Alternate Name record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('NM record not yet initialized!')\n\n        return b'NM' + struct.pack(b'=BBB', RRNMRecord.length(self.posix_name), SU_ENTRY_VERSION, self.posix_name_flags) + self.posix_name", "response": "Generate a string representing the Rock Ridge Alternate Name record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a Rock Ridge Child Link record out of a string.", "response": "def parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge Child Link record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('CL record already initialized!')\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        (su_len, su_entry_version_unused, child_log_block_num_le, child_log_block_num_be) = struct.unpack_from('=BBLL', rrstr[:12], 2)\n        if su_len != RRCLRecord.length():\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid length on rock ridge extension')\n\n        if child_log_block_num_le != utils.swab_32bit(child_log_block_num_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Little endian block num does not equal big endian; corrupt ISO')\n        self.child_log_block_num = child_log_block_num_le\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new Rock Ridge Child Link record.", "response": "def new(self):\n        # type: () -> None\n        '''\n        Create a new Rock Ridge Child Link record.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('CL record already initialized!')\n\n        self.child_log_block_num = 0  # This gets set later\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Child Link record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('CL record not yet initialized!')\n\n        return b'CL' + struct.pack('=BBLL', RRCLRecord.length(), SU_ENTRY_VERSION, self.child_log_block_num, utils.swab_32bit(self.child_log_block_num))", "response": "Generate a string representing the Rock Ridge Child Link record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_log_block_num(self, bl):\n        # type: (int) -> None\n        '''\n        Set the logical block number for the child.\n\n        Parameters:\n         bl - Logical block number of the child.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('CL record not yet initialized!')\n\n        self.child_log_block_num = bl", "response": "A method to set the logical block number for the child."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge Parent Link record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PL record already initialized!')\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        (su_len, su_entry_version_unused, parent_log_block_num_le, parent_log_block_num_be) = struct.unpack_from('=BBLL', rrstr[:12], 2)\n        if su_len != RRPLRecord.length():\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid length on rock ridge extension')\n        if parent_log_block_num_le != utils.swab_32bit(parent_log_block_num_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Little endian block num does not equal big endian; corrupt ISO')\n        self.parent_log_block_num = parent_log_block_num_le\n\n        self._initialized = True", "response": "Parse a Rock Ridge Parent Link record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new(self):\n        # type: () -> None\n        '''\n        Generate a string representing the Rock Ridge Parent Link record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PL record already initialized!')\n\n        self.parent_log_block_num = 0  # This will get set later\n\n        self._initialized = True", "response": "Generate a string representing the Rock Ridge Parent Link record."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a string representing the Rock Ridge Child Link record.", "response": "def record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Child Link record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PL record not yet initialized!')\n\n        return b'PL' + struct.pack('=BBLL', RRPLRecord.length(), SU_ENTRY_VERSION, self.parent_log_block_num, utils.swab_32bit(self.parent_log_block_num))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_log_block_num(self, bl):\n        # type: (int) -> None\n        '''\n        Set the logical block number for the parent.\n\n        Parameters:\n         bl - Logical block number of the parent.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PL record not yet initialized!')\n\n        self.parent_log_block_num = bl", "response": "A method to set the logical block number for the parent record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge Time Stamp record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('TF record already initialized!')\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        (su_len, su_entry_version_unused, self.time_flags,) = struct.unpack_from('=BBB', rrstr[:5], 2)\n        if su_len < 5:\n            raise pycdlibexception.PyCdlibInvalidISO('Not enough bytes in the TF record')\n\n        tflen = 7\n        if self.time_flags & (1 << 7):\n            tflen = 17\n\n        offset = 5\n        for index, fieldname in enumerate(self.FIELDNAMES):\n            if self.time_flags & (1 << index):\n                if tflen == 7:\n                    setattr(self, fieldname, dates.DirectoryRecordDate())\n                elif tflen == 17:\n                    setattr(self, fieldname, dates.VolumeDescriptorDate())\n                getattr(self, fieldname).parse(rrstr[offset:offset + tflen])\n                offset += tflen\n\n        self._initialized = True", "response": "Parse a Rock Ridge Time Stamp record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new Rock Ridge Time Stamp record. Parameters: time_flags - The flags to use for this time stamp record. Returns: Nothing.", "response": "def new(self, time_flags):\n        # type: (int) -> None\n        '''\n        Create a new Rock Ridge Time Stamp record.\n\n        Parameters:\n         time_flags - The flags to use for this time stamp record.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('TF record already initialized!')\n\n        self.time_flags = time_flags\n\n        tflen = 7\n        if self.time_flags & (1 << 7):\n            tflen = 17\n\n        for index, fieldname in enumerate(self.FIELDNAMES):\n            if self.time_flags & (1 << index):\n                if tflen == 7:\n                    setattr(self, fieldname, dates.DirectoryRecordDate())\n                elif tflen == 17:\n                    setattr(self, fieldname, dates.VolumeDescriptorDate())\n                getattr(self, fieldname).new()\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a string representing the Rock Ridge Time Stamp record.", "response": "def record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Time Stamp record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('TF record not yet initialized!')\n\n        outlist = [b'TF', struct.pack('=BBB', RRTFRecord.length(self.time_flags), SU_ENTRY_VERSION, self.time_flags)]\n        for fieldname in self.FIELDNAMES:\n            field = getattr(self, fieldname)\n            if field is not None:\n                outlist.append(field.record())\n\n        return b''.join(outlist)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef length(time_flags):\n        # type: (int) -> int\n        '''\n        Static method to return the length of the Rock Ridge Time Stamp\n        record.\n\n        Parameters:\n         time_flags - Integer representing the flags to use.\n        Returns:\n         The length of this record in bytes.\n        '''\n        tf_each_size = 7\n        if time_flags & (1 << 7):\n            tf_each_size = 17\n        time_flags &= 0x7f\n        tf_num = 0\n        while time_flags:\n            time_flags &= time_flags - 1\n            tf_num += 1\n\n        return 5 + tf_each_size * tf_num", "response": "Returns the length of the Rock Ridge Time Stamp Entry record."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge Sparse File record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SF record already initialized!')\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        (su_len, su_entry_version_unused,) = struct.unpack_from('=BB', rrstr[:4], 2)\n\n        if su_len == 12:\n            # This is a Rock Ridge version 1.10 SF Record, which is 12 bytes.\n            (virtual_file_size_le, virtual_file_size_be) = struct.unpack_from('=LL', rrstr[:12], 4)\n            if virtual_file_size_le != utils.swab_32bit(virtual_file_size_be):\n                raise pycdlibexception.PyCdlibInvalidISO('Virtual file size little-endian does not match big-endian')\n            self.virtual_file_size_low = virtual_file_size_le\n        elif su_len == 21:\n            # This is a Rock Ridge version 1.12 SF Record, which is 21 bytes.\n            (virtual_file_size_high_le, virtual_file_size_high_be, virtual_file_size_low_le,\n             virtual_file_size_low_be, self.table_depth) = struct.unpack_from('=LLLLB', rrstr[:21], 4)\n            if virtual_file_size_high_le != utils.swab_32bit(virtual_file_size_high_be):\n                raise pycdlibexception.PyCdlibInvalidISO('Virtual file size high little-endian does not match big-endian')\n\n            if virtual_file_size_low_le != utils.swab_32bit(virtual_file_size_low_be):\n                raise pycdlibexception.PyCdlibInvalidISO('Virtual file size low little-endian does not match big-endian')\n            self.virtual_file_size_low = virtual_file_size_low_le\n            self.virtual_file_size_high = virtual_file_size_high_le\n        else:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid length on Rock Ridge SF record (expected 12 or 21)')\n\n        self._initialized = True", "response": "Parse a Rock Ridge Sparse File record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new(self, file_size_high, file_size_low, table_depth):\n        # type: (Optional[int], int, Optional[int]) -> None\n        '''\n        Create a new Rock Ridge Sparse File record.\n\n        Parameters:\n         file_size_high - The high-order 32-bits of the file size.\n         file_size_low - The low-order 32-bits of the file size.\n         table_depth - The maximum virtual file size.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SF record already initialized!')\n\n        self.virtual_file_size_high = file_size_high\n        self.virtual_file_size_low = file_size_low\n        self.table_depth = table_depth\n\n        self._initialized = True", "response": "A method to create a new Rock Ridge Sparse File record."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a string representing the Rock Ridge Sparse File record.", "response": "def record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Sparse File record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('SF record not yet initialized!')\n\n        length = 12\n        if self.virtual_file_size_high is not None:\n            length = 21\n        ret = b'SF' + struct.pack('=BB', length, SU_ENTRY_VERSION)\n        if self.virtual_file_size_high is not None and self.table_depth is not None:\n            ret += struct.pack('=LLLLB', self.virtual_file_size_high, utils.swab_32bit(self.virtual_file_size_high), self.virtual_file_size_low, utils.swab_32bit(self.virtual_file_size_low), self.table_depth)\n        else:\n            ret += struct.pack('=LL', self.virtual_file_size_low, utils.swab_32bit(self.virtual_file_size_low))\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Relocated Directory\n        record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('RE record not yet initialized!')\n\n        return b'RE' + struct.pack('=BB', RRRERecord.length(), SU_ENTRY_VERSION)", "response": "Generate a string representing the Rock Ridge Relocated Directory\n        record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge System Terminator record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('ST record already initialized!')\n\n        (su_len, su_entry_version_unused) = struct.unpack_from('=BB', rrstr[:4], 2)\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        if su_len != 4:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid length on rock ridge extension')\n\n        self._initialized = True", "response": "Parse a Rock Ridge System Terminator record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge System Terminator\n        record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('ST record not yet initialized!')\n\n        return b'ST' + struct.pack('=BB', RRSTRecord.length(), SU_ENTRY_VERSION)", "response": "Generate a string representing the Rock Ridge System Terminator\n        record."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(self, rrstr):\n        # type: (bytes) -> None\n        '''\n        Parse a Rock Ridge Platform Dependent record out of a string.\n\n        Parameters:\n         rrstr - The string to parse the record out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PD record already initialized!')\n\n        (su_len_unused, su_entry_version_unused) = struct.unpack_from('=BB', rrstr[:4], 2)\n\n        self.padding = rrstr[4:]\n\n        # We assume that the caller has already checked the su_entry_version,\n        # so we don't bother.\n\n        self._initialized = True", "response": "Parse a Rock Ridge Platform Dependent record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new(self):\n        # type: () -> None\n        '''\n        Create a new Rock Ridge Platform Dependent record.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PD record already initialized!')\n\n        self._initialized = True\n        self.padding = b''", "response": "Create a new Rock Ridge Platform Dependent record."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a string representing the Rock Ridge Platform Dependent record.", "response": "def record(self):\n        # type: () -> bytes\n        '''\n        Generate a string representing the Rock Ridge Platform Dependent\n        record.\n\n        Parameters:\n         None.\n        Returns:\n         String containing the Rock Ridge record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('PD record not yet initialized!')\n\n        return b'PD' + struct.pack('=BB', RRPDRecord.length(self.padding),\n                                   SU_ENTRY_VERSION) + self.padding"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_entry(self, name):\n        # type: (str) -> bool\n        '''\n        An internal method to tell if we have already parsed an entry of the\n        named type.\n\n        Parameters:\n         name - The name of the entry to check.\n        Returns:\n         True if we have already parsed an entry of the named type, False otherwise.\n        '''\n        return getattr(self.dr_entries, name) or getattr(self.ce_entries, name)", "response": "Returns True if we have already parsed an entry of the named type False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a Rock Ridge Entry record into a list of entries.", "response": "def parse(self, record, is_first_dir_record_of_root, bytes_to_skip, continuation):\n        # type: (bytes, bool, int, bool) -> None\n        '''\n        Method to parse a rock ridge record.\n\n        Parameters:\n         record - The record to parse.\n         is_first_dir_record_of_root - Whether this is the first directory\n                                       record of the root directory record;\n                                       certain Rock Ridge entries are only\n                                       valid there.\n         bytes_to_skip - The number of bytes to skip at the beginning of the\n                         record.\n         continuation - Whether the new entries should go in the continuation\n                        list or in the DR list.\n        Returns:\n         Nothing.\n        '''\n        # Note that we very explicitly do not check if self._initialized is True\n        # here; this can be called multiple times in the case where there is\n        # a continuation entry.\n\n        if continuation:\n            entry_list = self.ce_entries\n        else:\n            entry_list = self.dr_entries\n\n        self.bytes_to_skip = bytes_to_skip\n        offset = bytes_to_skip\n        left = len(record)\n        px_record_length = None\n        has_es_record = False\n        sf_record_length = None\n        er_id = None\n        while True:\n            if left == 0:\n                break\n            elif left == 1:\n                # There may be a padding byte on the end.\n                if bytes(bytearray([record[offset]])) != b'\\x00':\n                    raise pycdlibexception.PyCdlibInvalidISO('Invalid pad byte')\n                break\n            elif left < 4:\n                raise pycdlibexception.PyCdlibInvalidISO('Not enough bytes left in the System Use field')\n\n            (rtype, su_len, su_entry_version) = struct.unpack_from('=2sBB', record[:offset + 4], offset)\n            if su_entry_version != SU_ENTRY_VERSION:\n                raise pycdlibexception.PyCdlibInvalidISO('Invalid RR version %d!' % su_entry_version)\n\n            recslice = record[offset:]\n\n            if rtype in (b'SP', b'RR', b'CE', b'PX', b'ST', b'ER',\n                         b'PN', b'CL', b'PL', b'RE', b'TF', b'SF'):\n                recname = rtype.decode('utf-8').lower() + '_record'\n                if self.has_entry(recname):\n                    raise pycdlibexception.PyCdlibInvalidISO('Only single SP record supported')\n\n            if rtype == b'SP':\n                if left < 7 or not is_first_dir_record_of_root:\n                    raise pycdlibexception.PyCdlibInvalidISO('Invalid SUSP SP record')\n\n                # OK, this is the first Directory Record of the root\n                # directory, which means we should check it for the SUSP/RR\n                # extension, which is exactly 7 bytes and starts with 'SP'.\n\n                entry_list.sp_record = RRSPRecord()\n                entry_list.sp_record.parse(recslice)\n            elif rtype == b'RR':\n                entry_list.rr_record = RRRRRecord()\n                entry_list.rr_record.parse(recslice)\n            elif rtype == b'CE':\n                if self.has_entry('ce_record'):\n                    raise pycdlibexception.PyCdlibInvalidISO('Only single CE record supported')\n\n                entry_list.ce_record = RRCERecord()\n                entry_list.ce_record.parse(recslice)\n            elif rtype == b'PX':\n                entry_list.px_record = RRPXRecord()\n                px_record_length = entry_list.px_record.parse(recslice)\n            elif rtype == b'PD':\n                pd = RRPDRecord()\n                pd.parse(recslice)\n                entry_list.pd_records.append(pd)\n            elif rtype == b'ST':\n                if entry_list.st_record is not None:\n                    raise pycdlibexception.PyCdlibInvalidISO('Only one ST record per SUSP area supported')\n                if su_len != 4:\n                    raise pycdlibexception.PyCdlibInvalidISO('Invalid length on rock ridge extension')\n                entry_list.st_record = RRSTRecord()\n                entry_list.st_record.parse(recslice)\n            elif rtype == b'ER':\n                entry_list.er_record = RRERRecord()\n                entry_list.er_record.parse(recslice)\n                er_id = entry_list.er_record.ext_id\n            elif rtype == b'ES':\n                es = RRESRecord()\n                es.parse(recslice)\n                entry_list.es_records.append(es)\n                has_es_record = True\n            elif rtype == b'PN':\n                entry_list.pn_record = RRPNRecord()\n                entry_list.pn_record.parse(recslice)\n            elif rtype == b'SL':\n                new_sl_record = RRSLRecord()\n                new_sl_record.parse(recslice)\n                entry_list.sl_records.append(new_sl_record)\n            elif rtype == b'NM':\n                new_nm_record = RRNMRecord()\n                new_nm_record.parse(recslice)\n                entry_list.nm_records.append(new_nm_record)\n            elif rtype == b'CL':\n                entry_list.cl_record = RRCLRecord()\n                entry_list.cl_record.parse(recslice)\n            elif rtype == b'PL':\n                entry_list.pl_record = RRPLRecord()\n                entry_list.pl_record.parse(recslice)\n            elif rtype == b'RE':\n                entry_list.re_record = RRRERecord()\n                entry_list.re_record.parse(recslice)\n            elif rtype == b'TF':\n                entry_list.tf_record = RRTFRecord()\n                entry_list.tf_record.parse(recslice)\n            elif rtype == b'SF':\n                entry_list.sf_record = RRSFRecord()\n                entry_list.sf_record.parse(recslice)\n                sf_record_length = len(recslice)\n            else:\n                raise pycdlibexception.PyCdlibInvalidISO('Unknown SUSP record')\n            offset += su_len\n            left -= su_len\n\n        # Now let's determine the version of Rock Ridge that we have (1.09,\n        # 1.10, or 1.12).  Unfortunately, there is no direct information from\n        # Rock Ridge, so we infer it from what is present.  In an ideal world,\n        # the following table would tell us:\n        #\n        # | Feature/Rock Ridge version |      1.09     |      1.10     |      1.12     |\n        # +----------------------------+---------------+---------------+---------------+\n        # |    Has RR Record?          | True or False |     False     |     False     |\n        # |    Has ES Record?          |     False     |     False     | True or False |\n        # |    Has SF Record?          |     False     | True or False | True or False |\n        # |    PX Record length        |      36       |      36       |      44       |\n        # |    SF Record length        |     N/A       |      12       |      21       |\n        # |    ER Desc string          |  RRIP_1991A   |  RRIP_1991A   |  IEEE_P1282   |\n        # +----------------------------+---------------+---------------+---------------+\n        #\n        # While that is a good start, we don't live in an ideal world.  In\n        # particular, we've seen ISOs in the wild (OpenSolaris 2008) that put an\n        # RR record into an otherwise 1.12 Rock Ridge entry.  So we'll use the\n        # above as a hint, and allow for some wiggle room.\n\n        if px_record_length == 44 or sf_record_length == 21 or has_es_record or er_id == EXT_ID_112:\n            self.rr_version = '1.12'\n        else:\n            # Not 1.12, so either 1.09 or 1.10.\n            if sf_record_length == 12:\n                self.rr_version = '1.10'\n            else:\n                self.rr_version = '1.09'\n\n        namelist = [nm.posix_name for nm in self.dr_entries.nm_records]\n        namelist.extend([nm.posix_name for nm in self.ce_entries.nm_records])\n        self._full_name = b''.join(namelist)\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _record(self, entries):\n        # type: (RockRidgeEntries) -> bytes\n        '''\n        Return a string representing the Rock Ridge entry.\n\n        Parameters:\n         entries - The dr_entries or ce_entries to generate a record for.\n        Returns:\n         A string representing the Rock Ridge entry.\n        '''\n\n        outlist = []\n        if entries.sp_record is not None:\n            outlist.append(entries.sp_record.record())\n\n        if entries.rr_record is not None:\n            outlist.append(entries.rr_record.record())\n\n        for nm_record in entries.nm_records:\n            outlist.append(nm_record.record())\n\n        if entries.px_record is not None:\n            outlist.append(entries.px_record.record(self.rr_version))\n\n        for sl_record in entries.sl_records:\n            outlist.append(sl_record.record())\n\n        if entries.tf_record is not None:\n            outlist.append(entries.tf_record.record())\n\n        if entries.cl_record is not None:\n            outlist.append(entries.cl_record.record())\n\n        if entries.pl_record is not None:\n            outlist.append(entries.pl_record.record())\n\n        if entries.re_record is not None:\n            outlist.append(entries.re_record.record())\n\n        for es_record in entries.es_records:\n            outlist.append(es_record.record())\n\n        if entries.er_record is not None:\n            outlist.append(entries.er_record.record())\n\n        if entries.ce_record is not None:\n            outlist.append(entries.ce_record.record())\n\n        for pd_record in entries.pd_records:\n            outlist.append(pd_record.record())\n\n        if entries.st_record is not None:\n            outlist.append(entries.st_record.record())\n\n        if entries.sf_record is not None:\n            outlist.append(entries.sf_record.record())\n\n        return b''.join(outlist)", "response": "Generates a string representing the Rock Ridge entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string representing the Rock Ridge entries in the Directory Record. Parameters: None. Returns: A string representing the Rock Ridge entry.", "response": "def record_dr_entries(self):\n        # type: () -> bytes\n        '''\n        Return a string representing the Rock Ridge entries in the Directory Record.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing the Rock Ridge entry.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        return self._record(self.dr_entries)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string representing the Rock Ridge entries in the Continuation Entry. Parameters: None. Returns: A string representing the Rock Ridge entry.", "response": "def record_ce_entries(self):\n        # type: () -> bytes\n        '''\n        Return a string representing the Rock Ridge entries in the Continuation Entry.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing the Rock Ridge entry.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        return self._record(self.ce_entries)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_ce_record(self, curr_dr_len, thislen):\n        # type: (int, int) -> int\n        '''\n        An internal method to add a new length to a Continuation Entry.  If the\n        Continuation Entry does not yet exist, this method creates it.\n\n        Parameters:\n         curr_dr_len - The current Directory Record length.\n         thislen - The new length to add to the Continuation Entry.\n        Returns:\n         An integer representing the current directory record length after\n         adding the Continuation Entry.\n        '''\n        if self.dr_entries.ce_record is None:\n            self.dr_entries.ce_record = RRCERecord()\n            self.dr_entries.ce_record.new()\n            curr_dr_len += RRCERecord.length()\n        self.dr_entries.ce_record.add_record(thislen)\n        return curr_dr_len", "response": "Internal method to add a new length to a Continuation Entry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_name(self, rr_name, curr_dr_len):\n        # type: (bytes, int) -> int\n        '''\n        An internal method to add the appropriate name records to the ISO.\n\n        Parameters:\n         rr_name - The Rock Ridge name to add to the ISO.\n         curr_dr_len - The current directory record length.\n        Returns:\n         The new directory record length.\n        '''\n        # The length we are putting in this object (as opposed to the\n        # continuation entry) is the maximum, minus how much is already in the\n        # DR, minus 5 for the NM metadata.  We know that at least part of the\n        # NM record will always fit in this DR.  That's because the DR is a\n        # maximum size of 255, and the ISO9660 fields uses a maximum of 34 bytes\n        # for metadata and 8+1+3+1+5 (8 for name, 1 for dot, 3 for extension,\n        # 1 for semicolon, and 5 for version number, allowed up to 32767), which\n        # leaves the System Use entry with 255 - 34 - 18 = 203 bytes.  Before\n        # this record, the only records we ever put in place could be the SP or\n        # the RR record, and the combination of them is never > 203, so we will\n        # always put some NM data in here.\n        len_here = ALLOWED_DR_SIZE - curr_dr_len - 5\n        if len_here < len(rr_name):\n            curr_dr_len = self._add_ce_record(curr_dr_len, 0)\n            len_here = ALLOWED_DR_SIZE - curr_dr_len - 5\n        curr_nm = RRNMRecord()\n        curr_nm.new(rr_name[:len_here])\n        self.dr_entries.nm_records.append(curr_nm)\n        curr_dr_len += RRNMRecord.length(rr_name[:len_here])\n\n        offset = len_here\n        while offset < len(rr_name):\n            curr_nm.set_continued()\n\n            # We clip the length for this NM entry to 250, as that is\n            # the maximum possible size for an NM entry.\n            length = min(len(rr_name[offset:]), 250)\n\n            curr_nm = RRNMRecord()\n            curr_nm.new(rr_name[offset:offset + length])\n            self.ce_entries.nm_records.append(curr_nm)\n            if self.dr_entries.ce_record is not None:\n                self.dr_entries.ce_record.add_record(RRNMRecord.length(rr_name[offset:offset + length]))\n\n            offset += length\n\n        return curr_dr_len", "response": "Internal method to add the appropriate name records to the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Rock Ridge record. Parameters: is_first_dir_record_of_root - Whether this is the first directory record of the root directory record; certain Rock Ridge entries are only valid there. rr_name - The alternate name for this Rock Ridge entry. file_mode - The Unix file mode for this Rock Ridge entry. symlink_path - The path to the target of the symlink, or None if this is not a symlink. rr_version - The version of Rock Ridge to use; must be '1.09', '1.10', or '1.12'. rr_relocated_child - Whether this is a relocated child entry. rr_relocated - Whether this is a relocated entry. rr_relocated_parent - Whether this is a relocated parent entry. bytes_to_skip - The number of bytes to skip for the record. curr_dr_len - The current length of the directory record; this is used when figuring out whether a continuation entry is needed. Returns: The length of the directory record after the Rock Ridge extension has been added.", "response": "def new(self, is_first_dir_record_of_root, rr_name, file_mode,\n            symlink_path, rr_version, rr_relocated_child, rr_relocated,\n            rr_relocated_parent, bytes_to_skip, curr_dr_len):\n        # type: (bool, bytes, int, bytes, str, bool, bool, bool, int, int) -> int\n        '''\n        Create a new Rock Ridge record.\n\n        Parameters:\n         is_first_dir_record_of_root - Whether this is the first directory\n                                       record of the root directory record;\n                                       certain Rock Ridge entries are only\n                                       valid there.\n         rr_name - The alternate name for this Rock Ridge entry.\n         file_mode - The Unix file mode for this Rock Ridge entry.\n         symlink_path - The path to the target of the symlink, or None if this\n                        is not a symlink.\n         rr_version - The version of Rock Ridge to use; must be '1.09', '1.10',\n                      or '1.12'.\n         rr_relocated_child - Whether this is a relocated child entry.\n         rr_relocated - Whether this is a relocated entry.\n         rr_relocated_parent - Whether this is a relocated parent entry.\n         bytes_to_skip - The number of bytes to skip for the record.\n         curr_dr_len - The current length of the directory record; this is used\n                       when figuring out whether a continuation entry is needed.\n        Returns:\n         The length of the directory record after the Rock Ridge extension has\n         been added.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension already initialized')\n\n        if rr_version not in ['1.09', '1.10', '1.12']:\n            raise pycdlibexception.PyCdlibInvalidInput('Only Rock Ridge versions 1.09, 1.10, and 1.12 are implemented')\n\n        self.rr_version = rr_version\n\n        # For SP record\n        if is_first_dir_record_of_root:\n            new_sp = RRSPRecord()\n            new_sp.new(bytes_to_skip)\n            thislen = RRSPRecord.length()\n            if curr_dr_len + thislen > ALLOWED_DR_SIZE:\n                curr_dr_len = self._add_ce_record(curr_dr_len, thislen)\n                self.ce_entries.sp_record = new_sp\n            else:\n                curr_dr_len += thislen\n                self.dr_entries.sp_record = new_sp\n\n        # For RR record\n        rr_record = None\n        if rr_version == '1.09':\n            rr_record = RRRRRecord()\n            rr_record.new()\n            thislen = RRRRRecord.length()\n            if curr_dr_len + thislen > ALLOWED_DR_SIZE:\n                curr_dr_len = self._add_ce_record(curr_dr_len, thislen)\n                self.ce_entries.rr_record = rr_record\n            else:\n                curr_dr_len += thislen\n                self.dr_entries.rr_record = rr_record\n\n        # For NM record\n        if rr_name:\n            curr_dr_len = self._add_name(rr_name, curr_dr_len)\n            if rr_record is not None:\n                rr_record.append_field('NM')\n\n        # For PX record\n        new_px = RRPXRecord()\n        new_px.new(file_mode)\n        thislen = RRPXRecord.length(self.rr_version)\n        if curr_dr_len + thislen > ALLOWED_DR_SIZE:\n            curr_dr_len = self._add_ce_record(curr_dr_len, thislen)\n            self.ce_entries.px_record = new_px\n        else:\n            curr_dr_len += thislen\n            self.dr_entries.px_record = new_px\n\n        if rr_record is not None:\n            rr_record.append_field('PX')\n\n        # For SL record\n        if symlink_path:\n            curr_dr_len = self._new_symlink(symlink_path, curr_dr_len)\n            if rr_record is not None:\n                rr_record.append_field('SL')\n\n        # For TF record\n        new_tf = RRTFRecord()\n        new_tf.new(TF_FLAGS)\n        thislen = RRTFRecord.length(TF_FLAGS)\n        if curr_dr_len + thislen > ALLOWED_DR_SIZE:\n            curr_dr_len = self._add_ce_record(curr_dr_len, thislen)\n            self.ce_entries.tf_record = new_tf\n        else:\n            curr_dr_len += thislen\n            self.dr_entries.tf_record = new_tf\n\n        if rr_record is not None:\n            rr_record.append_field('TF')\n\n        # For CL record\n        if rr_relocated_child:\n            new_cl = RRCLRecord()\n            new_cl.new()\n            thislen = RRCLRecord.length()\n            if curr_dr_len + thislen > ALLOWED_DR_SIZE:\n                curr_dr_len = self._add_ce_record(curr_dr_len, thislen)\n                self.ce_entries.cl_record = new_cl\n            else:\n                curr_dr_len += thislen\n                self.dr_entries.cl_record = new_cl\n\n            if rr_record is not None:\n                rr_record.append_field('CL')\n\n        # For RE record\n        if rr_relocated:\n            new_re = RRRERecord()\n            new_re.new()\n            thislen = RRRERecord.length()\n            if curr_dr_len + thislen > ALLOWED_DR_SIZE:\n                curr_dr_len = self._add_ce_record(curr_dr_len, thislen)\n                self.ce_entries.re_record = new_re\n            else:\n                curr_dr_len += thislen\n                self.dr_entries.re_record = new_re\n\n            if rr_record is not None:\n                rr_record.append_field('RE')\n\n        # For PL record\n        if rr_relocated_parent:\n            new_pl = RRPLRecord()\n            new_pl.new()\n            thislen = RRPLRecord.length()\n            if curr_dr_len + thislen > ALLOWED_DR_SIZE:\n                curr_dr_len = self._add_ce_record(curr_dr_len, thislen)\n                self.ce_entries.pl_record = new_pl\n            else:\n                curr_dr_len += thislen\n                self.dr_entries.pl_record = new_pl\n\n            if rr_record is not None:\n                rr_record.append_field('PL')\n\n        # For ER record\n        if is_first_dir_record_of_root:\n            new_er = RRERRecord()\n            if rr_version in ['1.09', '1.10']:\n                new_er.new(EXT_ID_109, EXT_DES_109, EXT_SRC_109)\n                thislen = RRERRecord.length(EXT_ID_109, EXT_DES_109, EXT_SRC_109)\n            else:\n                # Assume 1.12\n                new_er.new(EXT_ID_112, EXT_DES_112, EXT_SRC_112)\n                thislen = RRERRecord.length(EXT_ID_112, EXT_DES_112, EXT_SRC_112)\n\n            if curr_dr_len + thislen > ALLOWED_DR_SIZE:\n                curr_dr_len = self._add_ce_record(curr_dr_len, thislen)\n                self.ce_entries.er_record = new_er\n            else:\n                curr_dr_len += thislen\n                self.dr_entries.er_record = new_er\n\n        curr_dr_len += (curr_dr_len % 2)\n\n        if curr_dr_len > 255:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge entry increased DR length too far')\n\n        namelist = [nm.posix_name for nm in self.dr_entries.nm_records]\n        namelist.extend([nm.posix_name for nm in self.ce_entries.nm_records])\n        self._full_name = b''.join(namelist)\n\n        self._initialized = True\n\n        return curr_dr_len"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nincrements the number of POSIX file links on this entry by one. Parameters: None. Returns: Nothing.", "response": "def add_to_file_links(self):\n        # type: () -> None\n        '''\n        Increment the number of POSIX file links on this entry by one.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        if self.dr_entries.px_record is None:\n            if self.ce_entries.px_record is None:\n                raise pycdlibexception.PyCdlibInvalidInput('No Rock Ridge file links')\n            self.ce_entries.px_record.posix_file_links += 1\n        else:\n            self.dr_entries.px_record.posix_file_links += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies the number of file links from the source Rock Ridge entry into this Rock Ridge entry. Parameters: src - The source Rock Ridge entry to copy from. Returns: Nothing.", "response": "def copy_file_links(self, src):\n        # type: (RockRidge) -> None\n        '''\n        Copy the number of file links from the source Rock Ridge entry into\n        this Rock Ridge entry.\n\n        Parameters:\n         src - The source Rock Ridge entry to copy from.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        # First, get the src data\n        if src.dr_entries.px_record is None:\n            if src.ce_entries.px_record is None:\n                raise pycdlibexception.PyCdlibInvalidInput('No Rock Ridge file links')\n            num_links = src.ce_entries.px_record.posix_file_links\n        else:\n            num_links = src.dr_entries.px_record.posix_file_links\n\n        # Now apply it to this record.\n        if self.dr_entries.px_record is None:\n            if self.ce_entries.px_record is None:\n                raise pycdlibexception.PyCdlibInvalidInput('No Rock Ridge file links')\n            self.ce_entries.px_record.posix_file_links = num_links\n        else:\n            self.dr_entries.px_record.posix_file_links = num_links"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the POSIX file mode bits for this Rock Ridge entry. Parameters: None. Returns: The POSIX file mode bits for this Rock Ridge entry.", "response": "def get_file_mode(self):\n        # type: () -> int\n        '''\n        Get the POSIX file mode bits for this Rock Ridge entry.\n\n        Parameters:\n         None.\n        Returns:\n         The POSIX file mode bits for this Rock Ridge entry.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        if self.dr_entries.px_record is None:\n            if self.ce_entries.px_record is None:\n                raise pycdlibexception.PyCdlibInvalidInput('No Rock Ridge file mode')\n            return self.ce_entries.px_record.posix_file_mode\n\n        return self.dr_entries.px_record.posix_file_mode"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _is_symlink(self):\n        # type: () -> bool\n        '''\n        Internal method to determine whether this Rock Ridge entry is a symlink.\n        '''\n        return len(self.dr_entries.sl_records) > 0 or len(self.ce_entries.sl_records) > 0", "response": "Internal method to determine whether this Rock Ridge entry is a symlink."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the path as a string of the symlink target of this Rock Ridge entry (if this is a symlink). Parameters: None. Returns: Symlink path as a string.", "response": "def symlink_path(self):\n        # type: () -> bytes\n        '''\n        Get the path as a string of the symlink target of this Rock Ridge entry\n        (if this is a symlink).\n\n        Parameters:\n         None.\n        Returns:\n         Symlink path as a string.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        if not self.is_symlink():\n            raise pycdlibexception.PyCdlibInvalidInput('Entry is not a symlink!')\n\n        outlist = []\n        saved = b''\n        for rec in self.dr_entries.sl_records + self.ce_entries.sl_records:\n            if rec.last_component_continued():\n                saved += rec.name()\n            else:\n                saved += rec.name()\n                outlist.append(saved)\n                saved = b''\n\n        if saved != b'':\n            raise pycdlibexception.PyCdlibInvalidISO('Saw a continued symlink record with no end; ISO is probably malformed')\n\n        return b'/'.join(outlist)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining whether this Rock Ridge entry has a child link record (used for relocating deep directory records). Parameters: None. Returns: True if this Rock Ridge entry has a child link record, False otherwise.", "response": "def child_link_record_exists(self):\n        # type: () -> bool\n        '''\n        Determine whether this Rock Ridge entry has a child link record (used\n        for relocating deep directory records).\n\n        Parameters:\n         None.\n        Returns:\n         True if this Rock Ridge entry has a child link record, False otherwise.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        return self.dr_entries.cl_record is not None or self.ce_entries.cl_record is not None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the logical extent number stored in the child link record (if there is one), from the directory record entry that was stored in the child_link member. This is used at the end of reshuffling extents to properly update the child link records. Parameters: None. Returns: Nothing.", "response": "def child_link_update_from_dirrecord(self):\n        # type: () -> None\n        '''\n        Update the logical extent number stored in the child link record (if\n        there is one), from the directory record entry that was stored in\n        the child_link member.  This is used at the end of reshuffling extents\n        to properly update the child link records.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        if self.cl_to_moved_dr is None:\n            raise pycdlibexception.PyCdlibInvalidInput('No child link found!')\n\n        if self.dr_entries.cl_record is not None:\n            self.dr_entries.cl_record.set_log_block_num(self.cl_to_moved_dr.extent_location())\n        elif self.ce_entries.cl_record is not None:\n            self.ce_entries.cl_record.set_log_block_num(self.cl_to_moved_dr.extent_location())\n        else:\n            raise pycdlibexception.PyCdlibInvalidInput('Could not find child link record!')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef child_link_extent(self):\n        # type: () -> int\n        '''\n        Get the extent of the child of this entry if it has one.\n\n        Parameters:\n         None.\n        Returns:\n         The logical block number of the child if it exists.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        if self.dr_entries.cl_record is not None:\n            return self.dr_entries.cl_record.child_log_block_num\n        if self.ce_entries.cl_record is not None:\n            return self.ce_entries.cl_record.child_log_block_num\n\n        raise pycdlibexception.PyCdlibInternalError('Asked for child extent for non-existent parent record')", "response": "A method to get the extent of the child of this entry if it has one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parent_link_record_exists(self):\n        # type: () -> bool\n        '''\n        Determine whether this Rock Ridge entry has a parent link record (used\n        for relocating deep directory records).\n\n        Parameters:\n         None:\n        Returns:\n         True if this Rock Ridge entry has a parent link record, False otherwise.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        return self.dr_entries.pl_record is not None or self.ce_entries.pl_record is not None", "response": "A method to determine whether this entry has a parent link record."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the logical extent number stored in the parent link record (if there is one), from the directory record entry that was stored in the parent_link member. This is used at the end of reshuffling extents to properly update the parent link records. Parameters: None. Returns: Nothing.", "response": "def parent_link_update_from_dirrecord(self):\n        # type: () -> None\n        '''\n        Update the logical extent number stored in the parent link record (if\n        there is one), from the directory record entry that was stored in\n        the parent_link member.  This is used at the end of reshuffling extents\n        to properly update the parent link records.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        if self.parent_link is None:\n            raise pycdlibexception.PyCdlibInvalidInput('No parent link found!')\n\n        if self.dr_entries.pl_record is not None:\n            self.dr_entries.pl_record.set_log_block_num(self.parent_link.extent_location())\n        elif self.ce_entries.pl_record is not None:\n            self.ce_entries.pl_record.set_log_block_num(self.parent_link.extent_location())\n        else:\n            raise pycdlibexception.PyCdlibInvalidInput('Could not find parent link record!')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the extent of the parent of this entry if it has one. Parameters: None. Returns: The logical block number of the parent if it exists.", "response": "def parent_link_extent(self):\n        # type: () -> int\n        '''\n        Get the extent of the parent of this entry if it has one.\n\n        Parameters:\n         None.\n        Returns:\n         The logical block number of the parent if it exists.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        if self.dr_entries.pl_record is not None:\n            return self.dr_entries.pl_record.parent_log_block_num\n        if self.ce_entries.pl_record is not None:\n            return self.ce_entries.pl_record.parent_log_block_num\n\n        raise pycdlibexception.PyCdlibInternalError('Asked for parent extent for non-existent parent record')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines whether this Rock Ridge entry has a relocated record.", "response": "def relocated_record(self):\n        # type: () -> bool\n        '''\n        Determine whether this Rock Ridge entry has a relocated record (used for\n        relocating deep directory records).\n\n        Parameters:\n         None.\n        Returns:\n         True if this Rock Ridge entry has a relocated record, False otherwise.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        return self.dr_entries.re_record is not None or self.ce_entries.re_record is not None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the Continuation Entry block object used by this Rock Ridge Record.", "response": "def update_ce_block(self, block):\n        # type: (RockRidgeContinuationBlock) -> None\n        '''\n        Update the Continuation Entry block object used by this Rock Ridge Record.\n\n        Parameters:\n         block - The new block object.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Rock Ridge extension not yet initialized')\n\n        self.ce_block = block"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntracking an already allocated entry in this Rock Ridge Continuation Block.", "response": "def track_entry(self, offset, length):\n        # type: (int, int) -> None\n        '''\n        Track an already allocated entry in this Rock Ridge Continuation Block.\n\n        Parameters:\n         offset - The offset at which to place the entry.\n         length - The length of the entry to track.\n        Returns:\n         Nothing.\n        '''\n        newlen = offset + length - 1\n        for entry in self._entries:\n            thislen = entry.offset + entry.length - 1\n            overlap = range(max(entry.offset, offset), min(thislen, newlen) + 1)\n            if overlap:\n                raise pycdlibexception.PyCdlibInvalidISO('Overlapping CE regions on the ISO')\n\n        # OK, there were no overlaps with existing entries.  Let's see if\n        # the new entry fits at the end.\n        if offset + length > self._max_block_size:\n            raise pycdlibexception.PyCdlibInvalidISO('No room in continuation block to track entry')\n\n        # We passed all of the checks; add the new entry to track in.\n        bisect.insort_left(self._entries, RockRidgeContinuationEntry(offset, length))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a new entry to this Rock Ridge Continuation Block. This method attempts to find a gap that fits the new length anywhere within this Continuation Block. If successful, it returns the offset at which it placed this entry. If unsuccessful, it returns None. Parameters: length - The length of the entry to find a gap for. Returns: The offset the entry was placed at, or None if no gap was found.", "response": "def add_entry(self, length):\n        # type: (int) -> int\n        '''\n        Add a new entry to this Rock Ridge Continuation Block.  This method\n        attempts to find a gap that fits the new length anywhere within this\n        Continuation Block.  If successful, it returns the offset at which\n        it placed this entry.  If unsuccessful, it returns None.\n\n        Parameters:\n         length - The length of the entry to find a gap for.\n        Returns:\n         The offset the entry was placed at, or None if no gap was found.\n        '''\n        offset = -1\n        # Need to find a gap\n        for index, entry in enumerate(self._entries):\n            if index == 0:\n                if entry.offset != 0 and length <= entry.offset:\n                    # We can put it at the beginning!\n                    offset = 0\n                    break\n            else:\n                lastentry = self._entries[index - 1]\n                lastend = lastentry.offset + lastentry.length - 1\n                gapsize = entry.offset - lastend - 1\n                if gapsize >= length:\n                    # We found a spot for it!\n                    offset = lastend + 1\n                    break\n        else:\n            # We reached the end without finding a gap for it.  Look at the last\n            # entry and see if there is room at the end.\n            if self._entries:\n                lastentry = self._entries[-1]\n                lastend = lastentry.offset + lastentry.length - 1\n                left = self._max_block_size - lastend - 1\n                if left >= length:\n                    offset = lastend + 1\n            else:\n                if self._max_block_size >= length:\n                    offset = 0\n\n        if offset >= 0:\n            bisect.insort_left(self._entries,\n                               RockRidgeContinuationEntry(offset, length))\n\n        return offset"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives an offset and length, find and remove the entry in this block that corresponds. Parameters: offset - The offset of the entry to look for. length - The length of the entry to look for. Returns: Nothing.", "response": "def remove_entry(self, offset, length):\n        # type: (int, int) -> None\n        '''\n        Given an offset and length, find and remove the entry in this block\n        that corresponds.\n\n        Parameters:\n         offset - The offset of the entry to look for.\n         length - The length of the entry to look for.\n        Returns:\n         Nothing.\n        '''\n        for index, entry in enumerate(self._entries):\n            if entry.offset == offset and entry.length == length:\n                del self._entries[index]\n                break\n        else:\n            raise pycdlibexception.PyCdlibInternalError('Could not find an entry for the RR CE entry in the CE block!')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef crc_ccitt(data):\n    # type: (bytes) -> int\n    '''\n    Calculate the CRC over a range of bytes using the CCITT polynomial.\n\n    Parameters:\n     data - The array of bytes to calculate the CRC over.\n    Returns:\n     The CCITT CRC of the data.\n    '''\n    crc = 0\n    if not have_py_3:\n        for x in data:\n            crc = crc_ccitt_table[ord(x) ^ ((crc >> 8) & 0xFF)] ^ ((crc << 8) & 0xFF00)  # type: ignore\n    else:\n        mv = memoryview(data)\n        for x in mv.tobytes():\n            crc = crc_ccitt_table[x ^ ((crc >> 8) & 0xFF)] ^ ((crc << 8) & 0xFF00)\n\n    return crc", "response": "Calculate the CRC over a range of bytes using the CCITT polynomial."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ostaunicode(src):\n    # type: (str) -> bytes\n    '''\n    Internal function to create an OSTA byte string from a source string.\n    '''\n    if have_py_3:\n        bytename = src\n    else:\n        bytename = src.decode('utf-8')  # type: ignore\n\n    try:\n        enc = bytename.encode('latin-1')\n        encbyte = b'\\x08'\n    except (UnicodeEncodeError, UnicodeDecodeError):\n        enc = bytename.encode('utf-16_be')\n        encbyte = b'\\x10'\n    return encbyte + enc", "response": "Internal function to create an OSTA byte string from a source string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_csum(data):\n    # type: (bytes) -> int\n    '''\n    A method to compute a simple checksum over the given data.\n\n    Parameters:\n     data - The data to compute the checksum over.\n    Returns:\n     The checksum.\n    '''\n    def identity(x):\n        # type: (int) -> int\n        '''\n        The identity function so we can use a function for python2/3\n        compatibility.\n        '''\n        return x\n\n    if isinstance(data, str):\n        myord = ord\n    elif isinstance(data, bytes):\n        myord = identity\n    elif isinstance(data, bytearray):\n        myord = identity\n    csum = 0\n    for byte in data:\n        csum += myord(byte)\n    csum -= myord(data[4])\n    csum %= 256\n\n    return csum", "response": "Compute a simple checksum over the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(self, data, extent):\n        # type: (bytes, int) -> None\n        '''\n        Parse the passed in data into a UDF NSR Volume Structure.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF NSR Volume Structure already initialized')\n\n        (structure_type, self.standard_ident, structure_version,\n         reserved_unused) = struct.unpack_from(self.FMT, data, 0)\n\n        if structure_type != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid structure type')\n\n        if self.standard_ident not in [b'NSR02', b'NSR03']:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid standard identifier')\n\n        if structure_version != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid structure version')\n\n        self.orig_extent_loc = extent\n\n        self._initialized = True", "response": "Parse the passed in data into a UDF NSR Volume Structure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extent_location(self):\n        # type: () -> int\n        '''\n        A method to get the extent location of this UDF NSR Volume Structure.\n\n        Parameters:\n         None.\n        Returns:\n         Integer extent location of this UDF NSR Volume Structure.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF NSR Volume Structure not yet initialized')\n\n        if self.new_extent_loc < 0:\n            return self.orig_extent_loc\n        return self.new_extent_loc", "response": "A method to get the extent location of this UDF NSR Volume Structure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, data, extent):\n        # type: (bytes, int) -> None\n        '''\n        Parse the passed in data into a UDF Descriptor tag.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent to compare against for the tag location.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Tag already initialized')\n\n        (self.tag_ident, self.desc_version, tag_checksum, reserved,\n         self.tag_serial_number, desc_crc, self.desc_crc_length,\n         self.tag_location) = struct.unpack_from(self.FMT, data, 0)\n\n        if reserved != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Reserved data not 0!')\n\n        if _compute_csum(data[:16]) != tag_checksum:\n            raise pycdlibexception.PyCdlibInvalidISO('Tag checksum does not match!')\n\n        if self.tag_location != extent:\n            # In theory, we should abort (throw an exception) if we see that a\n            # tag location that doesn't match an actual location.  However, we\n            # have seen UDF ISOs in the wild (most notably PS2 GT4 ISOs) that\n            # have an invalid tag location for the second anchor and File Set\n            # Terminator.  So that we can support those ISOs, just silently\n            # fix it up.  We lose a little bit of detection of whether this is\n            # \"truly\" a UDFTag, but it is really not a big risk.\n            self.tag_location = extent\n\n        if self.desc_version not in (2, 3):\n            raise pycdlibexception.PyCdlibInvalidISO('Tag version not 2 or 3')\n\n        if (len(data) - 16) < self.desc_crc_length:\n            raise pycdlibexception.PyCdlibInternalError('Not enough CRC bytes to compute (expected at least %d, got %d)' % (self.desc_crc_length, len(data) - 16))\n\n        if desc_crc != crc_ccitt(data[16:16 + self.desc_crc_length]):\n            raise pycdlibexception.PyCdlibInvalidISO('Tag CRC does not match!')\n\n        self._initialized = True", "response": "Parse the passed in data into a UDF Descriptor tag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef record(self, crc_bytes):\n        # type: (bytes) -> bytes\n        '''\n        A method to generate the string representing this UDF Descriptor Tag.\n\n        Parameters:\n         crc_bytes - The string to compute the CRC over.\n        Returns:\n         A string representing this UDF Descriptor Tag.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Descriptor Tag not initialized')\n\n        crc_byte_len = len(crc_bytes)\n        if self.desc_crc_length >= 0:\n            crc_byte_len = self.desc_crc_length\n\n        # We need to compute the checksum, but we'll do that by first creating\n        # the output buffer with the csum field set to 0, computing the csum,\n        # and then setting that record back as usual.\n        rec = bytearray(struct.pack(self.FMT, self.tag_ident, self.desc_version,\n                                    0, 0, self.tag_serial_number,\n                                    crc_ccitt(crc_bytes[:crc_byte_len]),\n                                    crc_byte_len, self.tag_location))\n\n        rec[4] = _compute_csum(rec)\n\n        return bytes(rec)", "response": "A method to generate the string representing this UDF Descriptor Tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new(self, tag_ident, tag_serial=0):\n        # type: (int, int) -> None\n        '''\n        A method to create a new UDF Descriptor Tag.\n\n        Parameters:\n         tag_ident - The tag identifier number for this tag.\n         tag_serial - The tag serial number for this tag.\n        Returns:\n         Nothing\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Tag already initialized')\n\n        self.tag_ident = tag_ident\n        self.desc_version = 2\n        self.tag_serial_number = tag_serial\n        self.tag_location = 0  # This will be set later.\n\n        self._initialized = True", "response": "A method to create a new UDF Descriptor Tag. This method is called by the new UDF Descriptor Tag class to create a new UDF Descriptor Tag."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the passed in data into a UDF Anchor Volume Structure.", "response": "def parse(self, data, extent, desc_tag):\n        # type: (bytes, int, UDFTag) -> None\n        '''\n        Parse the passed in data into a UDF Anchor Volume Structure.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Anchor Volume Structure already initialized')\n\n        (tag_unused, self.main_vd_length, self.main_vd_extent,\n         self.reserve_vd_length,\n         self.reserve_vd_extent) = struct.unpack_from(self.FMT, data, 0)\n\n        self.desc_tag = desc_tag\n\n        self.orig_extent_loc = extent\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef record(self):\n        # type: () -> bytes\n        '''\n        A method to generate the string representing this UDF Anchor Volume\n        Structure.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing this UDF Anchor Volume Structure.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Anchor Volume Descriptor not initialized')\n\n        rec = struct.pack(self.FMT, b'\\x00' * 16, self.main_vd_length,\n                          self.main_vd_extent, self.reserve_vd_length,\n                          self.reserve_vd_extent)[16:] + b'\\x00' * 480\n\n        return self.desc_tag.record(rec) + rec", "response": "A method to generate the string representing this UDF Anchor Volume Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the passed in data into a UDF Timestamp.", "response": "def parse(self, data):\n        # type: (bytes) -> None\n        '''\n        Parse the passed in data into a UDF Timestamp.\n\n        Parameters:\n         data - The data to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Timestamp already initialized')\n\n        (tz, timetype, self.year, self.month, self.day, self.hour, self.minute,\n         self.second, self.centiseconds, self.hundreds_microseconds,\n         self.microseconds) = struct.unpack_from(self.FMT, data, 0)\n\n        self.timetype = timetype >> 4\n\n        def twos_comp(val, bits):\n            # type: (int, int) -> int\n            '''\n            Compute the 2's complement of int value val\n            '''\n            if (val & (1 << (bits - 1))) != 0:  # if sign bit is set e.g., 8bit: 128-255\n                val = val - (1 << bits)         # compute negative value\n            return val                          # return positive value as is\n        self.tz = twos_comp(((timetype & 0xf) << 8) | tz, 12)\n        if self.tz < -1440 or self.tz > 1440:\n            if self.tz != -2047:\n                raise pycdlibexception.PyCdlibInvalidISO('Invalid UDF timezone')\n\n        if self.year < 1 or self.year > 9999:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid UDF year')\n        if self.month < 1 or self.month > 12:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid UDF month')\n        if self.day < 1 or self.day > 31:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid UDF day')\n        if self.hour < 0 or self.hour > 23:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid UDF hour')\n        if self.minute < 0 or self.minute > 59:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid UDF minute')\n        if self.second < 0 or self.second > 59:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid UDF second')\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new(self):\n        # type: () -> None\n        '''\n        A method to create a new UDF Timestamp.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Timestamp already initialized')\n\n        tm = time.time()\n        local = time.localtime(tm)\n\n        self.tz = utils.gmtoffset_from_tm(tm, local)\n        # FIXME: for the timetype, 0 is UTC, 1 is local, 2 is 'agreement'.\n        # We should let the user set this.\n        self.timetype = 1\n        self.year = local.tm_year\n        self.month = local.tm_mon\n        self.day = local.tm_mon\n        self.hour = local.tm_hour\n        self.minute = local.tm_min\n        self.second = local.tm_sec\n        self.centiseconds = 0\n        self.hundreds_microseconds = 0\n        self.microseconds = 0\n\n        self._initialized = True", "response": "A method to create a new UDF Timestamp. This method is used to create a new UDF Timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, data):\n        # type: (bytes) -> None\n        '''\n        Parse the passed in data into a UDF Entity ID.\n\n        Parameters:\n         data - The data to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Entity ID already initialized')\n\n        (self.flags, self.identifier, self.suffix) = struct.unpack_from(self.FMT, data, 0)\n\n        self._initialized = True", "response": "Parse the passed in data into a UDF Entity ID."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef record(self):\n        # type: () -> bytes\n        '''\n        A method to generate the string representing this UDF Entity ID.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing this UDF Entity ID.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Entity ID not initialized')\n\n        return struct.pack(self.FMT, self.flags, self.identifier, self.suffix)", "response": "A method to generate the string representing this UDF Entity ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the passed in data into a UDF Primary Volume Descriptor.", "response": "def parse(self, data, extent, desc_tag):\n        # type: (bytes, int, UDFTag) -> None\n        '''\n        Parse the passed in data into a UDF Primary Volume Descriptor.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Primary Volume Descriptor already initialized')\n\n        (tag_unused, self.vol_desc_seqnum, self.desc_num, self.vol_ident,\n         vol_seqnum, max_vol_seqnum, interchange_level,\n         self.max_interchange_level, char_set_list,\n         max_char_set_list, self.vol_set_ident, self.desc_char_set,\n         self.explanatory_char_set, self.vol_abstract_length, self.vol_abstract_extent,\n         self.vol_copyright_length, self.vol_copyright_extent, app_ident,\n         recording_date, impl_ident, self.implementation_use,\n         self.predecessor_vol_desc_location, flags,\n         reserved) = struct.unpack_from(self.FMT, data, 0)\n\n        self.desc_tag = desc_tag\n\n        if vol_seqnum != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n        if max_vol_seqnum != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n        if interchange_level != 2:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n        if char_set_list != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n        if max_char_set_list != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n        if flags != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n\n        if reserved != b'\\x00' * 22:\n            raise pycdlibexception.PyCdlibInvalidISO('UDF Primary Volume Descriptor reserved data not 0')\n\n        self.recording_date = UDFTimestamp()\n        self.recording_date.parse(recording_date)\n\n        self.app_ident = UDFEntityID()\n        self.app_ident.parse(app_ident)\n\n        self.impl_ident = UDFEntityID()\n        self.impl_ident.parse(impl_ident)\n\n        self.orig_extent_loc = extent\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new(self):\n        # type: () -> None\n        '''\n        A method to create a new UDF Primary Volume Descriptor.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Primary Volume Descriptor already initialized')\n\n        self.desc_tag = UDFTag()\n        self.desc_tag.new(1)  # FIXME: we should let the user set serial_number\n\n        self.vol_desc_seqnum = 0  # FIXME: we should let the user set this\n        self.desc_num = 0  # FIXME: we should let the user set this\n        self.vol_ident = _ostaunicode_zero_pad('CDROM', 32)\n        # According to UDF 2.60, 2.2.2.5, the VolumeSetIdentifier should have\n        # at least the first 16 characters be a unique value.  Further, the\n        # first 8 bytes of that should be a time value in ASCII hexadecimal\n        # representation.  To make it truly unique, we use that time plus a\n        # random value, all ASCII encoded.\n        unique = format(int(time.time()), '08x') + format(random.getrandbits(26), '08x')\n        self.vol_set_ident = _ostaunicode_zero_pad(unique, 128)\n        self.desc_char_set = _unicodecharset()\n        self.explanatory_char_set = _unicodecharset()\n        self.vol_abstract_length = 0  # FIXME: we should let the user set this\n        self.vol_abstract_extent = 0  # FIXME: we should let the user set this\n        self.vol_copyright_length = 0  # FIXME: we should let the user set this\n        self.vol_copyright_extent = 0  # FIXME: we should let the user set this\n        self.app_ident = UDFEntityID()\n        self.app_ident.new()\n        self.recording_date = UDFTimestamp()\n        self.recording_date.new()\n        self.impl_ident = UDFEntityID()\n        self.impl_ident.new(0, b'*pycdlib')\n        self.implementation_use = b'\\x00' * 64  # FIXME: we should let the user set this\n        self.predecessor_vol_desc_location = 0  # FIXME: we should let the user set this\n        self.max_interchange_level = 2\n\n        self._initialized = True", "response": "A method to create a new UDF Primary Volume Descriptor. This method is used to create a new UDF Primary Volume Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, data):\n        # type: (bytes) -> None\n        '''\n        Parse the passed in data into a UDF Implementation Use Volume\n        Descriptor Implementation Use field.\n\n        Parameters:\n         data - The data to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Implementation Use Volume Descriptor Implementation Use field already initialized')\n\n        (self.char_set, self.log_vol_ident, self.lv_info1, self.lv_info2,\n         self.lv_info3, impl_ident,\n         self.impl_use) = struct.unpack_from(self.FMT, data, 0)\n\n        self.impl_ident = UDFEntityID()\n        self.impl_ident.parse(impl_ident)\n\n        self._initialized = True", "response": "Parse the passed in data into a UDF Implementation Use Volume Descriptor Implementation Use field."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef record(self):\n        # type: () -> bytes\n        '''\n        A method to generate the string representing this UDF Implementation Use\n        Volume Descriptor Implementation Use field.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing this UDF Implementation Use Volume Descriptor.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Implementation Use Volume Descriptor Implementation Use field not initialized')\n\n        return struct.pack(self.FMT, self.char_set, self.log_vol_ident,\n                           self.lv_info1, self.lv_info2, self.lv_info3,\n                           self.impl_ident.record(), self.impl_use)", "response": "A method to generate the string representing this UDF Implementation Use Volume Descriptor Implementation Use field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef new(self):\n        # type: () -> None\n        '''\n        A method to create a new UDF Implementation Use Volume Descriptor Implementation Use field.\n\n        Parameters:\n         None:\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Implementation Use Volume Descriptor Implementation Use field already initialized')\n\n        self.char_set = _unicodecharset()\n        self.log_vol_ident = _ostaunicode_zero_pad('CDROM', 128)\n        self.lv_info1 = b'\\x00' * 36\n        self.lv_info2 = b'\\x00' * 36\n        self.lv_info3 = b'\\x00' * 36\n        self.impl_ident = UDFEntityID()\n        self.impl_ident.new(0, b'*pycdlib', b'')\n        self.impl_use = b'\\x00' * 128\n\n        self._initialized = True", "response": "A method to create a new UDF Implementation Use Volume Descriptor Implementation Use field."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the passed in data into a UDF Implementation Use Volume Descriptor.", "response": "def parse(self, data, extent, desc_tag):\n        # type: (bytes, int, UDFTag) -> None\n        '''\n        Parse the passed in data into a UDF Implementation Use Volume\n        Descriptor.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Implementation Use Volume Descriptor already initialized')\n\n        (tag_unused, self.vol_desc_seqnum, impl_ident,\n         impl_use) = struct.unpack_from(self.FMT, data, 0)\n\n        self.desc_tag = desc_tag\n\n        self.impl_ident = UDFEntityID()\n        self.impl_ident.parse(impl_ident)\n        if self.impl_ident.identifier[:12] != b'*UDF LV Info':\n            raise pycdlibexception.PyCdlibInvalidISO(\"Implementation Use Identifier not '*UDF LV Info'\")\n\n        self.impl_use = UDFImplementationUseVolumeDescriptorImplementationUse()\n        self.impl_use.parse(impl_use)\n\n        self.orig_extent_loc = extent\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the passed in data into a UDF Partition Header Descriptor.", "response": "def parse(self, data):\n        # type: (bytes) -> None\n        '''\n        Parse the passed in data into a UDF Partition Header Descriptor.\n\n        Parameters:\n         data - The data to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Partition Header Descriptor already initialized')\n        (unalloc_table_length, unalloc_table_pos, unalloc_bitmap_length,\n         unalloc_bitmap_pos, part_integrity_table_length,\n         part_integrity_table_pos, freed_table_length, freed_table_pos,\n         freed_bitmap_length, freed_bitmap_pos,\n         reserved_unused) = struct.unpack_from(self.FMT, data, 0)\n\n        if unalloc_table_length != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Partition Header unallocated table length not 0')\n        if unalloc_table_pos != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Partition Header unallocated table position not 0')\n        if unalloc_bitmap_length != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Partition Header unallocated bitmap length not 0')\n        if unalloc_bitmap_pos != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Partition Header unallocated bitmap position not 0')\n        if part_integrity_table_length != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Partition Header partition integrity length not 0')\n        if part_integrity_table_pos != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Partition Header partition integrity position not 0')\n        if freed_table_length != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Partition Header freed table length not 0')\n        if freed_table_pos != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Partition Header freed table position not 0')\n        if freed_bitmap_length != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Partition Header freed bitmap length not 0')\n        if freed_bitmap_pos != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Partition Header freed bitmap position not 0')\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(self, data, extent, desc_tag):\n        # type: (bytes, int, UDFTag) -> None\n        '''\n        Parse the passed in data into a UDF Partition Volume Descriptor.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Partition Volume Descriptor already initialized')\n\n        (tag_unused, self.vol_desc_seqnum, self.part_flags, self.part_num,\n         part_contents, part_contents_use, self.access_type,\n         self.part_start_location, self.part_length, impl_ident,\n         self.implementation_use, reserved_unused) = struct.unpack_from(self.FMT, data, 0)\n\n        self.desc_tag = desc_tag\n\n        self.part_contents = UDFEntityID()\n        self.part_contents.parse(part_contents)\n        if self.part_contents.identifier[:6] != b'+NSR02':\n            raise pycdlibexception.PyCdlibInvalidISO(\"Partition Contents Identifier not '+NSR02'\")\n\n        self.impl_ident = UDFEntityID()\n        self.impl_ident.parse(impl_ident)\n\n        self.part_contents_use = UDFPartitionHeaderDescriptor()\n        self.part_contents_use.parse(part_contents_use)\n\n        self.orig_extent_loc = extent\n\n        self._initialized = True", "response": "Parse the passed in data into a UDF Partition Volume Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(self, data):\n        # type: (bytes) -> None\n        '''\n        Parse the passed in data into a UDF Partition Map.\n\n        Parameters:\n         data - The data to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Partition Map already initialized')\n\n        (map_type, map_length, vol_seqnum,\n         self.part_num) = struct.unpack_from(self.FMT, data, 0)\n\n        if map_type != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('UDF Partition Map type is not 1')\n        if map_length != 6:\n            raise pycdlibexception.PyCdlibInvalidISO('UDF Partition Map length is not 6')\n        if vol_seqnum != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('UDF Partition Volume Sequence Number is not 1')\n\n        self._initialized = True", "response": "Parse the passed in data into a UDF Partition Map."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new(self):\n        # type: () -> None\n        '''\n        A method to create a new UDF Partition Map.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Partition Map already initialized')\n\n        self.part_num = 0  # FIXME: we should let the user set this\n\n        self._initialized = True", "response": "A method to create a new UDF Partition Map."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the passed in data into a UDF Long AD.", "response": "def parse(self, data):\n        # type: (bytes) -> None\n        '''\n        Parse the passed in data into a UDF Long AD.\n\n        Parameters:\n         data - The data to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Long Allocation descriptor already initialized')\n        (self.extent_length, self.log_block_num, self.part_ref_num,\n         self.impl_use) = struct.unpack_from(self.FMT, data, 0)\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new(self, length, blocknum):\n        # type: (int, int) -> None\n        '''\n        A method to create a new UDF Long AD.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Long AD already initialized')\n\n        self.extent_length = length\n        self.log_block_num = blocknum\n        self.part_ref_num = 0  # FIXME: we should let the user set this\n        self.impl_use = b'\\x00' * 6\n\n        self._initialized = True", "response": "A method to create a new UDF Long AD. This method is called by the new UDF Long AD module when it is created."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_extent_location(self, new_location, tag_location):\n        # type: (int, int) -> None\n        '''\n        A method to set the location fields of this UDF Long AD.\n\n        Parameters:\n         new_location - The new relative extent that this UDF Long AD references.\n         tag_location - The new absolute extent that this UDF Long AD references.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Long AD not initialized')\n\n        self.log_block_num = tag_location\n        self.impl_use = b'\\x00\\x00' + struct.pack('=L', new_location)", "response": "A method to set the location fields of this UDF Long AD."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the passed in data into a UDF Logical Volume Descriptor.", "response": "def parse(self, data, extent, desc_tag):\n        # type: (bytes, int, UDFTag) -> None\n        '''\n        Parse the passed in data into a UDF Logical Volume Descriptor.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Logical Volume Descriptor already initialized')\n\n        (tag_unused, self.vol_desc_seqnum, self.desc_char_set,\n         self.logical_vol_ident, logical_block_size, domain_ident,\n         logical_volume_contents_use, map_table_length, num_partition_maps,\n         impl_ident, self.implementation_use, self.integrity_sequence_length,\n         self.integrity_sequence_extent, partition_map,\n         end_unused) = struct.unpack_from(self.FMT, data, 0)\n\n        self.desc_tag = desc_tag\n\n        if logical_block_size != 2048:\n            raise pycdlibexception.PyCdlibInvalidISO('Volume Descriptor block size is not 2048')\n\n        self.domain_ident = UDFEntityID()\n        self.domain_ident.parse(domain_ident)\n        if self.domain_ident.identifier[:19] != b'*OSTA UDF Compliant':\n            raise pycdlibexception.PyCdlibInvalidISO(\"Volume Descriptor Identifier not '*OSTA UDF Compliant'\")\n\n        if map_table_length != 6:\n            raise pycdlibexception.PyCdlibInvalidISO('Volume Descriptor map table length not 6')\n\n        if num_partition_maps != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Volume Descriptor number of partition maps not 1')\n\n        self.impl_ident = UDFEntityID()\n        self.impl_ident.parse(impl_ident)\n\n        self.partition_map = UDFPartitionMap()\n        self.partition_map.parse(partition_map)\n\n        self.logical_volume_contents_use = UDFLongAD()\n        self.logical_volume_contents_use.parse(logical_volume_contents_use)\n\n        self.orig_extent_loc = extent\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the passed in data into a UDF Unallocated Space Descriptor.", "response": "def parse(self, data, extent, desc_tag):\n        # type: (bytes, int, UDFTag) -> None\n        '''\n        Parse the passed in data into a UDF Unallocated Space Descriptor.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Unallocated Space Descriptor already initialized')\n\n        (tag_unused, self.vol_desc_seqnum,\n         num_alloc_descriptors, end_unused) = struct.unpack_from(self.FMT, data, 0)\n\n        self.desc_tag = desc_tag\n\n        if num_alloc_descriptors != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('UDF Unallocated Space Descriptor allocated descriptors is not 0')\n\n        self.orig_extent_loc = extent\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, extent, desc_tag):\n        # type: (int, UDFTag) -> None\n        '''\n        Parse the passed in data into a UDF Terminating Descriptor.\n\n        Parameters:\n         extent - The extent that this descriptor currently lives at.\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Terminating Descriptor already initialized')\n\n        self.desc_tag = desc_tag\n\n        self.orig_extent_loc = extent\n\n        self._initialized = True", "response": "Parse the passed in data into a UDF Terminating Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new(self):\n        # type: () -> None\n        '''\n        A method to create a new UDF Terminating Descriptor.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Terminating Descriptor already initialized')\n\n        self.desc_tag = UDFTag()\n        self.desc_tag.new(8)  # FIXME: we should let the user set serial_number\n\n        self._initialized = True", "response": "A method to create a new UDF Terminating Descriptor. This method is called by the new UDF Terminating Descriptor when it is created."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_extent_location(self, new_location, tag_location=None):\n        # type: (int, int) -> None\n        '''\n        A method to set the location of this UDF Terminating Descriptor.\n\n        Parameters:\n         new_location - The new extent this UDF Terminating Descriptor should be located at.\n         tag_location - The tag location to set for this UDF Terminator Descriptor.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Terminating Descriptor not initialized')\n\n        self.new_extent_loc = new_location\n        if tag_location is None:\n            tag_location = new_location\n        self.desc_tag.tag_location = tag_location", "response": "A method to set the location of this UDF Terminating Descriptor to new_location."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the passed in data into a UDF Logical Volume Header Descriptor.", "response": "def parse(self, data):\n        # type: (bytes) -> None\n        '''\n        Parse the passed in data into a UDF Logical Volume Header Descriptor.\n\n        Parameters:\n         data - The data to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Logical Volume Header Descriptor already initialized')\n        (self.unique_id, reserved_unused) = struct.unpack_from(self.FMT, data, 0)\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new(self):\n        # type: () -> None\n        '''\n        A method to create a new UDF Logical Volume Header Descriptor.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Logical Volume Header Descriptor already initialized')\n\n        self.unique_id = 261\n\n        self._initialized = True", "response": "A method to create a new UDF Logical Volume Header Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the passed in data into a UDF Logical Volume Implementation Use.", "response": "def parse(self, data):\n        # type: (bytes) -> None\n        '''\n        Parse the passed in data into a UDF Logical Volume Implementation Use.\n\n        Parameters:\n         data - The data to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Logical Volume Implementation Use already initialized')\n\n        (impl_id, self.num_files, self.num_dirs, self.min_udf_read_revision,\n         self.min_udf_write_revision,\n         self.max_udf_write_revision) = struct.unpack_from(self.FMT, data, 0)\n\n        self.impl_id = UDFEntityID()\n        self.impl_id.parse(impl_id)\n\n        self.impl_use = data[46:]\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new(self):\n        # type: () -> None\n        '''\n        A method to create a new UDF Logical Volume Implementation Use.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Logical Volume Implementation Use already initialized')\n\n        self.impl_id = UDFEntityID()\n        self.impl_id.new(0, b'*pycdlib')\n\n        self.num_files = 0\n        self.num_dirs = 1\n        self.min_udf_read_revision = 258\n        self.min_udf_write_revision = 258\n        self.max_udf_write_revision = 258\n\n        self.impl_use = b'\\x00' * 378  # FIXME: let the user set this\n\n        self._initialized = True", "response": "A method to create a new UDF Logical Volume Implementation Use."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, data, extent, desc_tag):\n        # type: (bytes, int, UDFTag) -> None\n        '''\n        Parse the passed in data into a UDF Logical Volume Integrity Descriptor.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Logical Volume Integrity Descriptor already initialized')\n\n        (tag_unused, recording_date, integrity_type,\n         next_integrity_extent_length, next_integrity_extent_extent,\n         logical_volume_contents_use, num_partitions,\n         self.length_impl_use, self.free_space_table,\n         self.size_table, impl_use) = struct.unpack_from(self.FMT, data, 0)\n\n        self.desc_tag = desc_tag\n\n        self.recording_date = UDFTimestamp()\n        self.recording_date.parse(recording_date)\n\n        if integrity_type != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Logical Volume Integrity Type not 1')\n        if next_integrity_extent_length != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Logical Volume Integrity Extent length not 1')\n        if next_integrity_extent_extent != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Logical Volume Integrity Extent extent not 1')\n        if num_partitions != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Logical Volume Integrity number partitions not 1')\n        # For now, we only support an implementation use field of up to 424\n        # bytes (the 'rest' of the 512 byte sector we get here).  If we run\n        # across ones that are larger, we can go up to 2048, but anything\n        # larger than that is invalid (I'm not quite sure why UDF defines\n        # this as a 32-bit quantity, since there are no situations in which\n        # this can be larger than 2048 minus 88).\n        if self.length_impl_use > 424:\n            raise pycdlibexception.PyCdlibInvalidISO('Logical Volume Integrity implementation use length too large')\n        if self.free_space_table != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Logical Volume Integrity free space table not 0')\n\n        self.logical_volume_contents_use = UDFLogicalVolumeHeaderDescriptor()\n        self.logical_volume_contents_use.parse(logical_volume_contents_use)\n\n        self.logical_volume_impl_use = UDFLogicalVolumeImplementationUse()\n        self.logical_volume_impl_use.parse(impl_use)\n\n        self.orig_extent_loc = extent\n\n        self._initialized = True", "response": "Parse the passed in data into a UDF Logical Volume Integrity Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new(self):\n        # type: () -> None\n        '''\n        A method to create a new UDF Logical Volume Integrity Descriptor.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Logical Volume Integrity Descriptor already initialized')\n\n        self.desc_tag = UDFTag()\n        self.desc_tag.new(9)  # FIXME: we should let the user set serial_number\n\n        self.recording_date = UDFTimestamp()\n        self.recording_date.new()\n\n        self.length_impl_use = 46\n        self.free_space_table = 0  # FIXME: let the user set this\n        self.size_table = 3\n\n        self.logical_volume_contents_use = UDFLogicalVolumeHeaderDescriptor()\n        self.logical_volume_contents_use.new()\n\n        self.logical_volume_impl_use = UDFLogicalVolumeImplementationUse()\n        self.logical_volume_impl_use.new()\n\n        self._initialized = True", "response": "A method to create a new UDF Logical Volume Integrity Descriptor. This method is called by the new UDF Logical Volume Integrity Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, data, extent, desc_tag):\n        # type: (bytes, int, UDFTag) -> None\n        '''\n        Parse the passed in data into a UDF File Set Descriptor.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF File Set Descriptor already initialized')\n\n        (tag_unused, recording_date, interchange_level, max_interchange_level,\n         char_set_list, max_char_set_list, self.file_set_num, file_set_desc_num,\n         self.log_vol_char_set, self.log_vol_ident,\n         self.file_set_char_set, self.file_set_ident, self.copyright_file_ident,\n         self.abstract_file_ident, root_dir_icb, domain_ident, next_extent,\n         reserved_unused) = struct.unpack_from(self.FMT, data, 0)\n\n        self.desc_tag = desc_tag\n\n        self.recording_date = UDFTimestamp()\n        self.recording_date.parse(recording_date)\n\n        if interchange_level != 3:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n        if max_interchange_level != 3:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n        if char_set_list != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n        if max_char_set_list != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n        if file_set_desc_num != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n\n        self.domain_ident = UDFEntityID()\n        self.domain_ident.parse(domain_ident)\n        if self.domain_ident.identifier[:19] != b'*OSTA UDF Compliant':\n            raise pycdlibexception.PyCdlibInvalidISO(\"File Set Descriptor Identifier not '*OSTA UDF Compliant'\")\n\n        self.root_dir_icb = UDFLongAD()\n        self.root_dir_icb.parse(root_dir_icb)\n\n        if next_extent != b'\\x00' * 16:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-Only disks are supported')\n\n        self.orig_extent_loc = extent\n\n        self._initialized = True", "response": "Parse the passed in data into a UDF File Set Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new(self):\n        # type: () -> None\n        '''\n        A method to create a new UDF File Set Descriptor.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF File Set Descriptor already initialized')\n\n        self.desc_tag = UDFTag()\n        self.desc_tag.new(256)  # FIXME: we should let the user set serial_number\n\n        self.recording_date = UDFTimestamp()\n        self.recording_date.new()\n\n        self.domain_ident = UDFEntityID()\n        self.domain_ident.new(0, b'*OSTA UDF Compliant', b'\\x02\\x01\\x03')\n\n        self.root_dir_icb = UDFLongAD()\n        self.root_dir_icb.new(2048, 2)\n\n        self.file_set_num = 0\n        self.log_vol_char_set = _unicodecharset()\n        self.log_vol_ident = _ostaunicode_zero_pad('CDROM', 128)\n        self.file_set_char_set = _unicodecharset()\n        self.file_set_ident = _ostaunicode_zero_pad('CDROM', 32)\n        self.copyright_file_ident = b'\\x00' * 32  # FIXME: let the user set this\n        self.abstract_file_ident = b'\\x00' * 32  # FIXME: let the user set this\n\n        self._initialized = True", "response": "A method to create a new UDF File Set Descriptor. This method is used to create a new UDF File Set Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the passed in data into a UDF ICB Tag.", "response": "def parse(self, data):\n        # type: (bytes) -> None\n        '''\n        Parse the passed in data into a UDF ICB Tag.\n\n        Parameters:\n         data - The data to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF ICB Tag already initialized')\n\n        (self.prior_num_direct_entries, self.strategy_type, self.strategy_param,\n         self.max_num_entries, reserved, self.file_type,\n         self.parent_icb_log_block_num, self.parent_icb_part_ref_num,\n         self.flags) = struct.unpack_from(self.FMT, data, 0)\n\n        if self.strategy_type not in (4, 4096):\n            raise pycdlibexception.PyCdlibInvalidISO('UDF ICB Tag invalid strategy type')\n\n        if reserved != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('UDF ICB Tag reserved not 0')\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new(self, file_type):\n        # type: (str) -> None\n        '''\n        A method to create a new UDF ICB Tag.\n\n        Parameters:\n         file_type - What file type this represents, one of 'dir', 'file', or 'symlink'.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF ICB Tag already initialized')\n\n        self.prior_num_direct_entries = 0  # FIXME: let the user set this\n        self.strategy_type = 4\n        self.strategy_param = 0  # FIXME: let the user set this\n        self.max_num_entries = 1\n        if file_type == 'dir':\n            self.file_type = 4\n        elif file_type == 'file':\n            self.file_type = 5\n        elif file_type == 'symlink':\n            self.file_type = 12\n        else:\n            raise pycdlibexception.PyCdlibInternalError(\"Invalid file type for ICB; must be one of 'dir', 'file', or 'symlink'\")\n\n        self.parent_icb_log_block_num = 0  # FIXME: let the user set this\n        self.parent_icb_part_ref_num = 0  # FIXME: let the user set this\n        self.flags = 560  # hex 0x230 == binary 0010 0011 0000\n\n        self._initialized = True", "response": "A method to create a new UDF ICB Tag."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the passed in data into a UDF File Entry.", "response": "def parse(self, data, extent, parent, desc_tag):\n        # type: (bytes, int, Optional[UDFFileEntry], UDFTag) -> None\n        '''\n        Parse the passed in data into a UDF File Entry.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n         parent - The parent File Entry for this file (may be None).\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF File Entry already initialized')\n\n        (tag_unused, icb_tag, self.uid, self.gid, self.perms, self.file_link_count,\n         record_format, record_display_attrs, record_len,\n         self.info_len, self.log_block_recorded, access_time, mod_time,\n         attr_time, checkpoint, extended_attr_icb, impl_ident, self.unique_id,\n         self.len_extended_attrs, len_alloc_descs) = struct.unpack_from(self.FMT, data, 0)\n\n        self.desc_tag = desc_tag\n\n        self.icb_tag = UDFICBTag()\n        self.icb_tag.parse(icb_tag)\n\n        if record_format != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('File Entry record format is not 0')\n\n        if record_display_attrs != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('File Entry record display attributes is not 0')\n\n        if record_len != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('File Entry record length is not 0')\n\n        self.access_time = UDFTimestamp()\n        self.access_time.parse(access_time)\n\n        self.mod_time = UDFTimestamp()\n        self.mod_time.parse(mod_time)\n\n        self.attr_time = UDFTimestamp()\n        self.attr_time.parse(attr_time)\n\n        if checkpoint != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Only DVD Read-only disks supported')\n\n        self.extended_attr_icb = UDFLongAD()\n        self.extended_attr_icb.parse(extended_attr_icb)\n\n        self.impl_ident = UDFEntityID()\n        self.impl_ident.parse(impl_ident)\n\n        offset = struct.calcsize(self.FMT)\n        self.extended_attrs = data[offset:offset + self.len_extended_attrs]\n\n        offset += self.len_extended_attrs\n        num_alloc_descs = len_alloc_descs // 8  # a short_ad is 8 bytes\n        for i_unused in range(0, num_alloc_descs):\n            (length, pos) = struct.unpack('=LL', data[offset:offset + 8])\n            self.alloc_descs.append([length, pos])\n            offset += 8\n\n        self.orig_extent_loc = extent\n\n        self.parent = parent\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_file_ident_desc(self, new_fi_desc, logical_block_size):\n        # type: (UDFFileIdentifierDescriptor, int) -> int\n        '''\n        A method to add a new UDF File Identifier Descriptor to this UDF File\n        Entry.\n\n        Parameters:\n         new_fi_desc - The new UDF File Identifier Descriptor to add.\n         logical_block_size - The logical block size to use.\n        Returns:\n         The number of extents added due to adding this File Identifier Descriptor.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF File Entry not initialized')\n\n        if self.icb_tag.file_type != 4:\n            raise pycdlibexception.PyCdlibInvalidInput('Can only add a UDF File Identifier to a directory')\n\n        self.fi_descs.append(new_fi_desc)\n\n        num_bytes_to_add = UDFFileIdentifierDescriptor.length(len(new_fi_desc.fi))\n\n        old_num_extents = 0\n        # If info_len is 0, then this is a brand-new File Entry, and thus the\n        # number of extents it is using is 0.\n        if self.info_len > 0:\n            old_num_extents = utils.ceiling_div(self.info_len, logical_block_size)\n\n        self.info_len += num_bytes_to_add\n        new_num_extents = utils.ceiling_div(self.info_len, logical_block_size)\n\n        self.log_block_recorded = new_num_extents\n\n        self.alloc_descs[0][0] = self.info_len\n        if new_fi_desc.is_dir():\n            self.file_link_count += 1\n\n        return new_num_extents - old_num_extents", "response": "A method to add a new UDF File Identifier Descriptor to this UDF File Entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_data_location(self, current_extent, start_extent):  # pylint: disable=unused-argument\n        # type: (int, int) -> None\n        '''\n        A method to set the location of the data that this UDF File Entry\n        points to.\n\n        Parameters:\n         current_extent - Unused\n         start_extent - The starting extent for this data location.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF File Entry not initialized')\n\n        current_assignment = start_extent\n        for index, desc_unused in enumerate(self.alloc_descs):\n            self.alloc_descs[index][1] = current_assignment\n            current_assignment += 1", "response": "A method to set the location of the data that this UDF File Entry points to."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the passed in data into a UDF File Identifier Descriptor.", "response": "def parse(self, data, extent, desc_tag, parent):\n        # type: (bytes, int, UDFTag, UDFFileEntry) -> int\n        '''\n        Parse the passed in data into a UDF File Identifier Descriptor.\n\n        Parameters:\n         data - The data to parse.\n         extent - The extent that this descriptor currently lives at.\n         desc_tag - A UDFTag object that represents the Descriptor Tag.\n         parent - The UDF File Entry representing the parent.\n        Returns:\n         The number of bytes this descriptor consumed.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF File Identifier Descriptor already initialized')\n\n        (tag_unused, file_version_num, self.file_characteristics,\n         self.len_fi, icb, self.len_impl_use) = struct.unpack_from(self.FMT, data, 0)\n\n        self.desc_tag = desc_tag\n\n        if file_version_num != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('File Identifier Descriptor file version number not 1')\n\n        if self.file_characteristics & 0x2:\n            self.isdir = True\n\n        if self.file_characteristics & 0x8:\n            self.isparent = True\n\n        self.icb = UDFLongAD()\n        self.icb.parse(icb)\n\n        start = struct.calcsize(self.FMT)\n        end = start + self.len_impl_use\n        self.impl_use = data[start:end]\n\n        start = end\n        end = start + self.len_fi\n        # The very first byte of the File Identifier describes whether this is\n        # an 8-bit or 16-bit encoded string; this corresponds to whether we\n        # encode with 'latin-1' or with 'utf-16_be'.  We save that off because we have\n        # to write the correct thing out when we record.\n        if not self.isparent:\n            encoding = bytes(bytearray([data[start]]))\n            if encoding == b'\\x08':\n                self.encoding = 'latin-1'\n            elif encoding == b'\\x10':\n                self.encoding = 'utf-16_be'\n            else:\n                raise pycdlibexception.PyCdlibInvalidISO('Only UDF File Identifier Descriptor Encodings 8 or 16 are supported')\n\n            start += 1\n\n            self.fi = data[start:end]\n\n        self.orig_extent_loc = extent\n\n        self.parent = parent\n\n        self._initialized = True\n\n        return end + UDFFileIdentifierDescriptor.pad(end)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_icb(self, new_location, tag_location):\n        # type: (int, int) -> None\n        '''\n        A method to set the location of the data that this UDF File Identifier\n        Descriptor points at.  The data can either be for a directory or for a\n        file.\n\n        Parameters:\n         new_location - The new extent this UDF File Identifier Descriptor data lives at.\n         tag_location - The new relative extent this UDF File Identifier Descriptor data lives at.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF File Identifier not initialized')\n\n        self.icb.set_extent_location(new_location, tag_location)", "response": "A method to set the location of the data that this UDF File Identifier Descriptor points at."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a string containing a Volume Descriptor and return a new object.", "response": "def parse(self, vd, extent_loc):\n        # type: (bytes, int) -> None\n        '''\n        Parse a Volume Descriptor out of a string.\n\n        Parameters:\n         vd - The string containing the Volume Descriptor.\n         extent_loc - The location on the ISO of this Volume Descriptor.\n        Returns:\n         Nothing.\n        '''\n        ################ PVD VERSION ######################\n        (descriptor_type, identifier, self.version, self.flags,\n         self.system_identifier, self.volume_identifier, unused1,\n         space_size_le, space_size_be, self.escape_sequences, set_size_le,\n         set_size_be, seqnum_le, seqnum_be, logical_block_size_le,\n         logical_block_size_be, path_table_size_le, path_table_size_be,\n         self.path_table_location_le, self.optional_path_table_location_le,\n         self.path_table_location_be, self.optional_path_table_location_be,\n         root_dir_record, self.volume_set_identifier, pub_ident_str,\n         prepare_ident_str, app_ident_str, self.copyright_file_identifier,\n         self.abstract_file_identifier, self.bibliographic_file_identifier,\n         vol_create_date_str, vol_mod_date_str, vol_expire_date_str,\n         vol_effective_date_str, self.file_structure_version, unused2,\n         self.application_use, zero_unused) = struct.unpack_from(self.FMT, vd, 0)\n\n        # According to Ecma-119, 8.4.1, the primary volume descriptor type\n        # should be 1.\n        if descriptor_type != self._vd_type:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid volume descriptor')\n        # According to Ecma-119, 8.4.2, the identifier should be 'CD001'.\n        if identifier != b'CD001':\n            raise pycdlibexception.PyCdlibInvalidISO('invalid CD isoIdentification')\n        # According to Ecma-119, 8.4.3, the version should be 1 (or 2 for\n        # ISO9660:1999)\n        expected_versions = [1]\n        if self._vd_type == VOLUME_DESCRIPTOR_TYPE_SUPPLEMENTARY:\n            expected_versions.append(2)\n        if self.version not in expected_versions:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid volume descriptor version %d' % (self.version))\n        # According to Ecma-119, 8.4.4, the first flags field should be 0 for a Primary.\n        if self._vd_type == VOLUME_DESCRIPTOR_TYPE_PRIMARY and self.flags != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('PVD flags field is not zero')\n        # According to Ecma-119, 8.4.5, the first unused field (after the\n        # system identifier and volume identifier) should be 0.\n        if unused1 != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('data in 2nd unused field not zero')\n        # According to Ecma-119, 8.4.9, the escape sequences for a PVD should\n        # be 32 zero-bytes.  However, we have seen ISOs in the wild (Fantastic\n        # Night Dreams - Cotton Original (Japan).cue from the psx redump\n        # collection) that don't have this set to 0, so allow anything here.\n\n        # According to Ecma-119, 8.4.30, the file structure version should be 1.\n        # However, we have seen ISOs in the wild that that don't have this\n        # properly set to one.  In those cases, forcibly set it to one and let\n        # it pass.\n        if self._vd_type == VOLUME_DESCRIPTOR_TYPE_PRIMARY:\n            if self.file_structure_version != 1:\n                self.file_structure_version = 1\n        elif self._vd_type == VOLUME_DESCRIPTOR_TYPE_SUPPLEMENTARY:\n            if self.file_structure_version not in (1, 2):\n                raise pycdlibexception.PyCdlibInvalidISO('File structure version expected to be 1')\n        # According to Ecma-119, 8.4.31, the second unused field should be 0.\n        if unused2 != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('data in 2nd unused field not zero')\n        # According to Ecma-119, the last 653 bytes of the VD should be all 0.\n        # However, we have seen ISOs in the wild that do not follow this, so\n        # relax the check.\n\n        # Check to make sure that the little-endian and big-endian versions\n        # of the parsed data agree with each other.\n        if space_size_le != utils.swab_32bit(space_size_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Little-endian and big-endian space size disagree')\n        self.space_size = space_size_le\n\n        if set_size_le != utils.swab_16bit(set_size_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Little-endian and big-endian set size disagree')\n        self.set_size = set_size_le\n\n        if seqnum_le != utils.swab_16bit(seqnum_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Little-endian and big-endian seqnum disagree')\n        self.seqnum = seqnum_le\n\n        if logical_block_size_le != utils.swab_16bit(logical_block_size_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Little-endian and big-endian logical block size disagree')\n        self.log_block_size = logical_block_size_le\n\n        if path_table_size_le != utils.swab_32bit(path_table_size_be):\n            raise pycdlibexception.PyCdlibInvalidISO('Little-endian and big-endian path table size disagree')\n        self.path_tbl_size = path_table_size_le\n        self.path_table_num_extents = utils.ceiling_div(self.path_tbl_size, 4096) * 2\n\n        self.path_table_location_be = utils.swab_32bit(self.path_table_location_be)\n\n        self.publisher_identifier = FileOrTextIdentifier()\n        self.publisher_identifier.parse(pub_ident_str)\n        self.preparer_identifier = FileOrTextIdentifier()\n        self.preparer_identifier.parse(prepare_ident_str)\n        self.application_identifier = FileOrTextIdentifier()\n        self.application_identifier.parse(app_ident_str)\n        self.volume_creation_date = dates.VolumeDescriptorDate()\n        self.volume_creation_date.parse(vol_create_date_str)\n        self.volume_modification_date = dates.VolumeDescriptorDate()\n        self.volume_modification_date.parse(vol_mod_date_str)\n        self.volume_expiration_date = dates.VolumeDescriptorDate()\n        self.volume_expiration_date.parse(vol_expire_date_str)\n        self.volume_effective_date = dates.VolumeDescriptorDate()\n        self.volume_effective_date.parse(vol_effective_date_str)\n        self.root_dir_record.parse(self, root_dir_record, None)\n\n        self.orig_extent_loc = extent_loc\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new(self, flags, sys_ident, vol_ident, set_size, seqnum, log_block_size,\n            vol_set_ident, pub_ident_str, preparer_ident_str, app_ident_str,\n            copyright_file, abstract_file, bibli_file, vol_expire_date,\n            app_use, xa, version, escape_sequence):\n        # type: (int, bytes, bytes, int, int, int, bytes, bytes, bytes, bytes, bytes, bytes, bytes, float, bytes, bool, int, bytes) -> None\n        '''\n        Create a new Volume Descriptor.\n\n        Parameters:\n         flags - The flags to set for this Volume Descriptor (must be 0 for a\n                 Primay Volume Descriptor).\n         sys_ident - The system identification string to use on the new ISO.\n         vol_ident - The volume identification string to use on the new ISO.\n         set_size - The size of the set of ISOs this ISO is a part of.\n         seqnum - The sequence number of the set of this ISO.\n         log_block_size - The logical block size to use for the ISO.  While\n                          ISO9660 technically supports sizes other than 2048\n                          (the default), this almost certainly doesn't work.\n         vol_set_ident - The volume set identification string to use on the\n                         new ISO.\n         pub_ident_str - The publisher identification string to use on the new ISO.\n         preparer_ident_str - The preparer identification string to use on the new\n                              ISO.\n         app_ident_str - The application identification string to use on the new\n                         ISO.\n         copyright_file - The name of a file at the root of the ISO to use as\n                          the copyright file.\n         abstract_file - The name of a file at the root of the ISO to use as the\n                         abstract file.\n         bibli_file - The name of a file at the root of the ISO to use as the\n                      bibliographic file.\n         vol_expire_date - The date that this ISO will expire at.\n         app_use - Arbitrary data that the application can stuff into the\n                   primary volume descriptor of this ISO.\n         xa - Whether to embed XA data into the volume descriptor.\n         version - What version to assign to the header (ignored).\n         escape_sequence - The escape sequence to assign to this volume\n                           descriptor (must be empty for a PVD, or empty or a\n                           valid Joliet escape sequence for an SVD).\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Primary Volume Descriptor is already initialized')\n\n        encoding = 'ascii'\n        if self._vd_type == VOLUME_DESCRIPTOR_TYPE_PRIMARY:\n            if flags != 0:\n                raise pycdlibexception.PyCdlibInvalidInput('Non-zero flags not allowed for a PVD')\n            if escape_sequence != b'':\n                raise pycdlibexception.PyCdlibInvalidInput('Non-empty escape sequence not allowed for a PVD')\n            if version != 1:\n                raise pycdlibexception.PyCdlibInvalidInput('Only version 1 supported for a PVD')\n            self.escape_sequences = b'\\x00' * 32\n        elif self._vd_type == VOLUME_DESCRIPTOR_TYPE_SUPPLEMENTARY:\n            if version not in (1, 2):\n                raise pycdlibexception.PyCdlibInvalidInput('Only version 1 and version 2 supported for a Supplementary Volume Descriptor')\n            if escape_sequence in (b'%/@', b'%/C', b'%/E'):\n                encoding = 'utf-16_be'\n            self.escape_sequences = escape_sequence.ljust(32, b'\\x00')\n\n        self.file_structure_version = version\n        self.version = version\n        self.flags = 0\n\n        if len(sys_ident) > 32:\n            raise pycdlibexception.PyCdlibInvalidInput('The system identifer has a maximum length of 32')\n        self.system_identifier = utils.encode_space_pad(sys_ident, 32, encoding)\n\n        if len(vol_ident) > 32:\n            raise pycdlibexception.PyCdlibInvalidInput('The volume identifier has a maximum length of 32')\n        self.volume_identifier = utils.encode_space_pad(vol_ident, 32, encoding)\n\n        # The space_size is the number of extents (2048-byte blocks) in the\n        # ISO.  We know we will at least have the system area (16 extents),\n        # and this VD (1 extent) to start with; the rest will be added later.\n        self.space_size = 17\n        self.set_size = set_size\n        if seqnum > set_size:\n            raise pycdlibexception.PyCdlibInvalidInput('Sequence number must be less than or equal to set size')\n        self.seqnum = seqnum\n        self.log_block_size = log_block_size\n        # The path table size is in bytes, and is always at least 10 bytes\n        # (for the root directory record).\n        self.path_tbl_size = 10\n        self.path_table_num_extents = utils.ceiling_div(self.path_tbl_size, 4096) * 2\n        # By default the Little Endian Path Table record starts at extent 19\n        # (right after the Volume Terminator).\n        self.path_table_location_le = 19\n        # By default the Big Endian Path Table record starts at extent 21\n        # (two extents after the Little Endian Path Table Record).\n        self.path_table_location_be = 21\n        # FIXME: we don't support the optional path table location right now\n        self.optional_path_table_location_le = 0\n        self.optional_path_table_location_be = 0\n        self.root_dir_record.new_root(self, seqnum, self.log_block_size)\n\n        if len(vol_set_ident) > 128:\n            raise pycdlibexception.PyCdlibInvalidInput('The maximum length for the volume set identifier is 128')\n        self.volume_set_identifier = utils.encode_space_pad(vol_set_ident, 128, encoding)\n\n        self.publisher_identifier = FileOrTextIdentifier()\n        self.publisher_identifier.new(utils.encode_space_pad(pub_ident_str, 128, encoding))\n\n        self.preparer_identifier = FileOrTextIdentifier()\n        self.preparer_identifier.new(utils.encode_space_pad(preparer_ident_str, 128, encoding))\n\n        self.application_identifier = FileOrTextIdentifier()\n        self.application_identifier.new(utils.encode_space_pad(app_ident_str, 128, encoding))\n\n        self.copyright_file_identifier = utils.encode_space_pad(copyright_file, 37, encoding)\n        self.abstract_file_identifier = utils.encode_space_pad(abstract_file, 37, encoding)\n        self.bibliographic_file_identifier = utils.encode_space_pad(bibli_file, 37, encoding)\n\n        now = time.time()\n        self.volume_creation_date = dates.VolumeDescriptorDate()\n        self.volume_creation_date.new(now)\n        # We make a valid volume modification date here, but it will get\n        # overwritten during record().\n        self.volume_modification_date = dates.VolumeDescriptorDate()\n        self.volume_modification_date.new(now)\n        self.volume_expiration_date = dates.VolumeDescriptorDate()\n        self.volume_expiration_date.new(vol_expire_date)\n        self.volume_effective_date = dates.VolumeDescriptorDate()\n        self.volume_effective_date.new(now)\n\n        if xa:\n            if len(app_use) > 141:\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot have XA and an app_use of > 140 bytes')\n            self.application_use = app_use.ljust(141, b' ')\n            self.application_use += b'CD-XA001' + b'\\x00' * 18\n            self.application_use = self.application_use.ljust(512, b' ')\n        else:\n            if len(app_use) > 512:\n                raise pycdlibexception.PyCdlibInvalidInput('The maximum length for the application use is 512')\n            self.application_use = app_use.ljust(512, b' ')\n\n        self._initialized = True", "response": "This function creates a new Volume Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self, orig):\n        # type: (PrimaryOrSupplementaryVD) -> None\n        '''\n        A method to populate and initialize this VD object from the contents\n        of an old VD.\n\n        Parameters:\n         orig_pvd - The original VD to copy data from.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Volume Descriptor is already initialized')\n\n        self.version = orig.version\n        self.flags = orig.flags\n        self.system_identifier = orig.system_identifier\n        self.volume_identifier = orig.volume_identifier\n        self.escape_sequences = orig.escape_sequences\n        self.space_size = orig.space_size\n        self.set_size = orig.set_size\n        self.seqnum = orig.seqnum\n        self.log_block_size = orig.log_block_size\n        self.path_tbl_size = orig.path_tbl_size\n        self.path_table_location_le = orig.path_table_location_le\n        self.optional_path_table_location_le = orig.optional_path_table_location_le\n        self.path_table_location_be = orig.path_table_location_be\n        self.optional_path_table_location_be = orig.optional_path_table_location_be\n        # Root dir record is a DirectoryRecord object, and we want this copy to hold\n        # onto exactly the same reference as the original\n        self.root_dir_record = orig.root_dir_record\n        self.volume_set_identifier = orig.volume_set_identifier\n        # publisher_identifier is a FileOrTextIdentifier object, and we want this copy to\n        # hold onto exactly the same reference as the original\n        self.publisher_identifier = orig.publisher_identifier\n        # preparer_identifier is a FileOrTextIdentifier object, and we want this copy to\n        # hold onto exactly the same reference as the original\n        self.preparer_identifier = orig.preparer_identifier\n        # application_identifier is a FileOrTextIdentifier object, and we want this copy to\n        # hold onto exactly the same reference as the original\n        self.application_identifier = orig.application_identifier\n        self.copyright_file_identifier = orig.copyright_file_identifier\n        self.abstract_file_identifier = orig.abstract_file_identifier\n        self.bibliographic_file_identifier = orig.bibliographic_file_identifier\n        # volume_creation_date is a VolumeDescriptorDate object, and we want this copy to\n        # hold onto exactly the same reference as the original\n        self.volume_creation_date = orig.volume_creation_date\n        # volume_expiration_date is a VolumeDescriptorDate object, and we want this copy to\n        # hold onto exactly the same reference as the original\n        self.volume_expiration_date = orig.volume_expiration_date\n        # volume_effective_date is a VolumeDescriptorDate object, and we want this copy to\n        # hold onto exactly the same reference as the original\n        self.volume_effective_date = orig.volume_effective_date\n        self.file_structure_version = orig.file_structure_version\n        self.application_use = orig.application_use\n\n        self._initialized = True", "response": "A method to populate and initialize this VD object from the contents of the original VD."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef track_rr_ce_entry(self, extent, offset, length):\n        # type: (int, int, int) -> rockridge.RockRidgeContinuationBlock\n        '''\n        Start tracking a new Rock Ridge Continuation Entry entry in this Volume\n        Descriptor, at the extent, offset, and length provided.  Since Rock\n        Ridge Continuation Blocks are shared across multiple Rock Ridge\n        Directory Records, the most logical place to track them is in the PVD.\n        This method is expected to be used during parse time, when an extent,\n        offset and length are already assigned to the entry.\n\n        Parameters:\n         extent - The extent that this Continuation Entry lives at.\n         offset - The offset within the extent that this Continuation Entry\n                  lives at.\n         length - The length of this Continuation Entry.\n        Returns:\n         The object representing the block in which the Continuation Entry was\n         placed in.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Primary Volume Descriptor is not yet initialized')\n\n        for block in self.rr_ce_blocks:\n            if block.extent_location() == extent:\n                break\n        else:\n            # We didn't find it in the list, add it\n            block = rockridge.RockRidgeContinuationBlock(extent, self.log_block_size)\n            self.rr_ce_blocks.append(block)\n\n        block.track_entry(offset, length)\n\n        return block", "response": "A method to track a new Rock Ridge Continuation Entry entry in this Volume Descriptor at the extent offset and length."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_rr_ce_entry(self, length):\n        # type: (int) -> Tuple[bool, rockridge.RockRidgeContinuationBlock, int]\n        '''\n        Add a new Rock Ridge Continuation Entry to this PVD; see\n        track_rr_ce_entry() above for why we track these in the PVD.  This\n        method is used to add a new Continuation Entry anywhere it fits in the\n        list of Continuation Blocks.  If it doesn't fit in any of the existing\n        blocks, a new block for it is allocated.\n\n        Parameters:\n         length - The length of the Continuation Entry that should be added.\n        Returns:\n         A 3-tuple consisting of whether we added a new block, the object\n         representing the block that this entry was added to, and the offset\n         within the block that the entry was added to.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Primary Volume Descriptor is not yet initialized')\n\n        added_block = False\n        for block in self.rr_ce_blocks:\n            offset = block.add_entry(length)\n            if offset is not None:\n                break\n        else:\n            # We didn't find a block this would fit in; add one.\n            block = rockridge.RockRidgeContinuationBlock(0, self.log_block_size)\n            self.rr_ce_blocks.append(block)\n            offset = block.add_entry(length)\n            added_block = True\n\n        return (added_block, block, offset)", "response": "A method to add a new Rock Ridge Continuation Entry to this PVD."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_from_space_size(self, removal_bytes):\n        # type: (int) -> None\n        '''\n        Remove bytes from the volume descriptor.\n\n        Parameters:\n         removal_bytes - The number of bytes to remove.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Volume Descriptor is not yet initialized')\n        # The 'removal' parameter is expected to be in bytes, but the space\n        # size we track is in extents.  Round up to the next extent.\n        self.space_size -= utils.ceiling_div(removal_bytes, self.log_block_size)", "response": "Removes the specified number of bytes from the space of the volume descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the space for a path table record to the volume descriptor. Parameters: ptr_size - The length of the Path Table Record being added to this Volume Descriptor. Returns: True if extents need to be added to the Volume Descriptor, False otherwise.", "response": "def add_to_ptr_size(self, ptr_size):\n        # type: (int) -> bool\n        '''\n        Add the space for a path table record to the volume descriptor.\n\n        Parameters:\n         ptr_size - The length of the Path Table Record being added to this Volume Descriptor.\n        Returns:\n         True if extents need to be added to the Volume Descriptor, False otherwise.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Volume Descriptor is not yet initialized')\n\n        # First add to the path table size.\n        self.path_tbl_size += ptr_size\n        if (utils.ceiling_div(self.path_tbl_size, 4096) * 2) > self.path_table_num_extents:\n            # If we overflowed the path table size, then we need to update the\n            # space size.  Since we always add two extents for the little and\n            # two for the big, add four total extents.  The locations will be\n            # fixed up during reshuffle_extents.\n            self.path_table_num_extents += 2\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the space for a path table record from the volume descriptor. Parameters: ptr_size - The length of the Path Table Record being removed from this Volume Descriptor. Returns: True if extents need to be removed from the Volume Descriptor, False otherwise.", "response": "def remove_from_ptr_size(self, ptr_size):\n        # type: (int) -> bool\n        '''\n        Remove the space for a path table record from the volume descriptor.\n\n        Parameters:\n         ptr_size - The length of the Path Table Record being removed from this Volume Descriptor.\n        Returns:\n         True if extents need to be removed from the Volume Descriptor, False otherwise.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Volume Descriptor is not yet initialized')\n\n        # Next remove from the Path Table Record size.\n        self.path_tbl_size -= ptr_size\n        new_extents = utils.ceiling_div(self.path_tbl_size, 4096) * 2\n\n        need_remove_extents = False\n        if new_extents > self.path_table_num_extents:\n            # This should never happen.\n            raise pycdlibexception.PyCdlibInvalidInput('This should never happen')\n        elif new_extents < self.path_table_num_extents:\n            self.path_table_num_extents -= 2\n            need_remove_extents = True\n\n        return need_remove_extents"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy_sizes(self, othervd):\n        # type: (PrimaryOrSupplementaryVD) -> None\n        '''\n        Copy the path_tbl_size, path_table_num_extents, and space_size from\n        another volume descriptor.\n\n        Parameters:\n         othervd - The other volume descriptor to copy from.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Volume Descriptor is not yet initialized')\n\n        self.space_size = othervd.space_size\n        self.path_tbl_size = othervd.path_tbl_size\n        self.path_table_num_extents = othervd.path_table_num_extents", "response": "A method to copy the path_tbl_size path_table_num_extents and space_size from another volume descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a file or text identifier out of a string.", "response": "def parse(self, ident_str):\n        # type: (bytes) -> None\n        '''\n        Parse a file or text identifier out of a string.\n\n        Parameters:\n          ident_str  - The string to parse the file or text identifier from.\n        Returns:\n          Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This File or Text identifier is already initialized')\n        self.text = ident_str\n\n        # FIXME: we do not support a file identifier here.  In the future, we\n        # might want to implement this.\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new file or text identifier. Parameters: text - The text to store into the identifier. Returns: Nothing.", "response": "def new(self, text):\n        # type: (bytes) -> None\n        '''\n        Create a new file or text identifier.\n\n        Parameters:\n          text   - The text to store into the identifier.\n        Returns:\n          Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This File or Text identifier is already initialized')\n\n        if len(text) != 128:\n            raise pycdlibexception.PyCdlibInvalidInput('Length of text must be 128')\n\n        self.text = text\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, vd, extent_loc):\n        # type: (bytes, int) -> None\n        '''\n        A method to parse a Boot Record out of a string.\n\n        Parameters:\n         vd - The string to parse the Boot Record out of.\n         extent_loc - The extent location this Boot Record is current at.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Boot Record already initialized')\n\n        (descriptor_type, identifier, version,\n         self.boot_system_identifier, self.boot_identifier,\n         self.boot_system_use) = struct.unpack_from(self.FMT, vd, 0)\n\n        # According to Ecma-119, 8.2.1, the boot record type should be 0\n        if descriptor_type != VOLUME_DESCRIPTOR_TYPE_BOOT_RECORD:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid boot record descriptor type')\n        # According to Ecma-119, 8.2.2, the identifier should be 'CD001'\n        if identifier != b'CD001':\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid boot record identifier')\n        # According to Ecma-119, 8.2.3, the version should be 1\n        if version != 1:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid boot record version')\n\n        self.orig_extent_loc = extent_loc\n\n        self._initialized = True", "response": "A method to parse a Boot Record out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef record(self):\n        # type: () -> bytes\n        '''\n        A method to generate a string representing this Boot Record.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing this Boot Record.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('Boot Record not yet initialized')\n\n        return struct.pack(self.FMT, VOLUME_DESCRIPTOR_TYPE_BOOT_RECORD,\n                           b'CD001', 1, self.boot_system_identifier,\n                           self.boot_identifier, self.boot_system_use)", "response": "A method to generate a string representing this Boot Record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndoing a parse of a Version Volume Descriptor. This consists of seeing whether the data is either all zero or starts with 'MKI', and if so, setting the extent location of the Version Volume Descriptor properly. Parameters: data - The potential version data. extent - The location of the extent on the original ISO of this Version Volume Descriptor. Returns: True if the data passed in is a Version Descriptor, False otherwise.", "response": "def parse(self, data, extent):\n        # type: (bytes, int) -> bool\n        '''\n        Do a parse of a Version Volume Descriptor.  This consists of seeing\n        whether the data is either all zero or starts with 'MKI', and if so,\n        setting the extent location of the Version Volume Descriptor properly.\n\n        Parameters:\n         data - The potential version data.\n         extent - The location of the extent on the original ISO of this\n                  Version Volume Descriptor.\n        Returns:\n         True if the data passed in is a Version Descriptor, False otherwise.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Version Volume Descriptor is already initialized')\n\n        if data[:3] == b'MKI' or data == allzero:\n            # OK, we have a version descriptor.\n            self._data = data\n            self.orig_extent_loc = extent\n            self._initialized = True\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new(self, log_block_size):\n        # type: (int) -> None\n        '''\n        Create a new Version Volume Descriptor.\n\n        Parameters:\n         log_block_size - The size of one extent.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Version Volume Descriptor is already initialized')\n\n        self._data = b'\\x00' * log_block_size\n        self._initialized = True", "response": "A method to create a new Version Volume Descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hdmbrcheck(disk_mbr, sector_count, bootable):\n    # type: (bytes, int, bool) -> int\n    '''\n    A function to sanity check an El Torito Hard Drive Master Boot Record (HDMBR).\n    On success, it returns the system_type (also known as the partition type) that\n    should be fed into the rest of the El Torito methods.  On failure, it raises\n    an exception.\n\n    Parameters:\n     disk_mbr - The data to look in.\n     sector_count - The number of sectors expected in the MBR.\n     bootable - Whether this MBR is bootable.\n    Returns:\n     The system (or partition) type the should be fed into the rest of El Torito.\n    '''\n    # The MBR that we want to see to do hd emulation boot for El Torito is a standard\n    # x86 MBR, documented here:\n    # https://en.wikipedia.org/wiki/Master_boot_record#Sector_layout\n    #\n    # In brief, it should consist of 512 bytes laid out like:\n    # Offset 0x0 - 0x1BD:   Bootstrap code area\n    # Offset 0x1BE - 0x1CD: Partition entry 1\n    # Offset 0x1CE - 0x1DD: Partition entry 2\n    # Offset 0x1DE - 0x1ED: Partition entry 3\n    # Offset 0x1EE - 0x1FD: Partition entry 4\n    # Offset 0x1FE:         0x55\n    # Offset 0x1FF:         0xAA\n    #\n    # Each partition entry above should consist of:\n    # Offset 0x0: Active (bit 7 set) or inactive (all zeros)\n    # Offset 0x1 - 0x3: CHS address of first sector in partition\n    #   Offset 0x1: Head\n    #   Offset 0x2: Sector in bits 0-5, bits 6-7 are high bits of of cylinder\n    #   Offset 0x3: bits 0-7 of cylinder\n    # Offset 0x4: Partition type (almost all of these are valid, see https://en.wikipedia.org/wiki/Partition_type)\n    # Offset 0x5 - 0x7: CHS address of last sector in partition (same format as first sector)\n    # Offset 0x8 - 0xB: LBA of first sector in partition\n    # Offset 0xC - 0xF: number of sectors in partition\n\n    PARTITION_TYPE_UNUSED = 0x0\n\n    PARTITION_STATUS_ACTIVE = 0x80\n\n    (bootstrap_unused, part1, part2, part3, part4, keybyte1,\n     keybyte2) = struct.unpack_from('=446s16s16s16s16sBB', disk_mbr, 0)\n\n    if keybyte1 != 0x55 or keybyte2 != 0xAA:\n        raise pycdlibexception.PyCdlibInvalidInput('Invalid magic on HD MBR')\n\n    parts = [part1, part2, part3, part4]\n    system_type = PARTITION_TYPE_UNUSED\n    for part in parts:\n        (status, s_head, s_seccyl, s_cyl, parttype, e_head, e_seccyl, e_cyl,\n         lba_unused, num_sectors_unused) = struct.unpack('=BBBBBBBBLL', part)\n\n        if parttype == PARTITION_TYPE_UNUSED:\n            continue\n\n        if system_type != PARTITION_TYPE_UNUSED:\n            raise pycdlibexception.PyCdlibInvalidInput('Boot image has multiple partitions')\n\n        if bootable and status != PARTITION_STATUS_ACTIVE:\n            # genisoimage prints a warning in this case, but we have no other\n            # warning prints in the whole codebase, and an exception will probably\n            # make us too fragile.  So we leave the code but don't do anything.\n            with open(os.devnull, 'w') as devnull:\n                print('Warning: partition not marked active', file=devnull)\n\n        cyl = ((s_seccyl & 0xC0) << 10) | s_cyl\n        sec = s_seccyl & 0x3f\n        if cyl != 0 or s_head != 1 or sec != 1:\n            # genisoimage prints a warning in this case, but we have no other\n            # warning prints in the whole codebase, and an exception will probably\n            # make us too fragile.  So we leave the code but don't do anything.\n            with open(os.devnull, 'w') as devnull:\n                print('Warning: partition does not start at 0/1/1', file=devnull)\n\n        cyl = ((e_seccyl & 0xC0) << 10) | e_cyl\n        sec = e_seccyl & 0x3f\n        geometry_sectors = (cyl + 1) * (e_head + 1) * sec\n\n        if sector_count != geometry_sectors:\n            # genisoimage prints a warning in this case, but we have no other\n            # warning prints in the whole codebase, and an exception will probably\n            # make us too fragile.  So we leave the code but don't do anything.\n            with open(os.devnull, 'w') as devnull:\n                print('Warning: image size does not match geometry', file=devnull)\n\n        system_type = parttype\n\n    if system_type == PARTITION_TYPE_UNUSED:\n        raise pycdlibexception.PyCdlibInvalidInput('Boot image has no partitions')\n\n    return system_type", "response": "This function checks an El Torito Hard Drive Master Boot Record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new(self, vd, ino, orig_len, csum):\n        # type: (headervd.PrimaryOrSupplementaryVD, inode.Inode, int, int) -> None\n        '''\n        A method to create a new boot info table.\n\n        Parameters:\n         vd - The volume descriptor to associate with this boot info table.\n         ino - The Inode associated with this Boot Info Table.\n         orig_len - The original length of the file before the boot info table was patched into it.\n         csum - The checksum for the boot file, starting at the byte after the boot info table.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Eltorito Boot Info Table is already initialized')\n        self.vd = vd\n        self.orig_len = orig_len\n        self.csum = csum\n        self.inode = ino\n        self._initialized = True", "response": "A method to create a new boot info table. This method is called by the base class to create a new boot info table."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the checksum of the data.", "response": "def _checksum(data):\n        # type: (bytes) -> int\n        '''\n        A static method to compute the checksum on the ISO.  Note that this is\n        *not* a 1's complement checksum; when an addition overflows, the carry\n        bit is discarded, not added to the end.\n\n        Parameters:\n         data - The data to compute the checksum over.\n        Returns:\n         The checksum of the data.\n        '''\n        def identity(x):\n            # type: (int) -> int\n            '''\n            The identity function so we can use a function for python2/3\n            compatibility.\n            '''\n            return x\n\n        if isinstance(data, str):\n            myord = ord\n        elif isinstance(data, bytes):\n            myord = identity\n        s = 0\n        for i in range(0, len(data), 2):\n            w = myord(data[i]) + (myord(data[i + 1]) << 8)\n            s = (s + w) & 0xffff\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, valstr):\n        # type: (bytes) -> None\n        '''\n        A method to parse an El Torito Entry out of a string.\n\n        Parameters:\n         valstr - The string to parse the El Torito Entry out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Entry already initialized')\n\n        (self.boot_indicator, self.boot_media_type, self.load_segment,\n         self.system_type, unused1, self.sector_count, self.load_rba,\n         self.selection_criteria_type,\n         self.selection_criteria) = struct.unpack_from(self.FMT, valstr, 0)\n\n        if self.boot_indicator not in (0x88, 0x00):\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid El Torito initial entry boot indicator')\n        if self.boot_media_type > 4:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid El Torito boot media type')\n\n        # FIXME: check that the system type matches the partition table\n\n        if unused1 != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('El Torito unused field must be 0')\n\n        # According to the specification, the El Torito unused end field (bytes\n        # 0xc - 0x1f, unused2 field) should be all zero.  However, we have found\n        # ISOs in the wild where that is not the case, so skip that particular\n        # check here.\n\n        self._initialized = True", "response": "A method to parse an El Torito Entry out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef new(self, sector_count, load_seg, media_name, system_type, bootable):\n        # type: (int, int, str, int, bool) -> None\n        '''\n        A method to create a new El Torito Entry.\n\n        Parameters:\n         sector_count - The number of sectors to assign to this El Torito Entry.\n         load_seg - The load segment address of the boot image.\n         media_name - The name of the media type, one of 'noemul', 'floppy', or 'hdemul'.\n         system_type - The partition type to assign to the entry.\n         bootable - Whether this entry is bootable.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Entry already initialized')\n\n        if media_name == 'noemul':\n            media_type = self.MEDIA_NO_EMUL\n        elif media_name == 'floppy':\n            if sector_count == 2400:\n                media_type = self.MEDIA_12FLOPPY\n            elif sector_count == 2880:\n                media_type = self.MEDIA_144FLOPPY\n            elif sector_count == 5760:\n                media_type = self.MEDIA_288FLOPPY\n            else:\n                raise pycdlibexception.PyCdlibInvalidInput('Invalid sector count for floppy media type; must be 2400, 2880, or 5760')\n            # With floppy booting, the sector_count always ends up being 1\n            sector_count = 1\n        elif media_name == 'hdemul':\n            media_type = self.MEDIA_HD_EMUL\n            # With HD emul booting, the sector_count always ends up being 1\n            sector_count = 1\n        else:\n            raise pycdlibexception.PyCdlibInvalidInput(\"Invalid media name '%s'\" % (media_name))\n\n        if bootable:\n            self.boot_indicator = 0x88\n        else:\n            self.boot_indicator = 0\n        self.boot_media_type = media_type\n        self.load_segment = load_seg\n        self.system_type = system_type\n        self.sector_count = sector_count\n        self.load_rba = 0  # This will get set later\n        self.selection_criteria_type = 0  # FIXME: allow the user to set this\n        self.selection_criteria = b''.ljust(19, b'\\x00')\n\n        self._initialized = True", "response": "A method to create a new El Torito Entry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_data_length(self, length):\n        # type: (int) -> None\n        '''\n        A method to set the length of data for this El Torito Entry.\n\n        Parameters:\n         length - The new length for the El Torito Entry.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Entry not initialized')\n        self.sector_count = utils.ceiling_div(length, 512)", "response": "A method to set the length of data for this El Torito Entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an El Torito section header from a string.", "response": "def parse(self, valstr):\n        # type: (bytes) -> None\n        '''\n        Parse an El Torito section header from a string.\n\n        Parameters:\n         valstr - The string to parse.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Section Header already initialized')\n\n        (self.header_indicator, self.platform_id, self.num_section_entries,\n         self.id_string) = struct.unpack_from(self.FMT, valstr, 0)\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new El Torito section header. Parameters: id_string - The ID to use for this section header. platform_id - The platform ID for this section header. Returns: Nothing.", "response": "def new(self, id_string, platform_id):\n        # type: (bytes, int) -> None\n        '''\n        Create a new El Torito section header.\n\n        Parameters:\n         id_string - The ID to use for this section header.\n         platform_id - The platform ID for this section header.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Section Header already initialized')\n\n        # We always assume this is the last section, until we are told otherwise\n        # via set_record_not_last.\n        self.header_indicator = 0x91\n        self.platform_id = platform_id\n        self.num_section_entries = 0\n        self.id_string = id_string\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a string representing this El Torito section header. Parameters: None. Returns: A string representing this El Torito section header.", "response": "def record(self):\n        # type: () -> bytes\n        '''\n        Get a string representing this El Torito section header.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing this El Torito section header.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Section Header not yet initialized')\n\n        outlist = [struct.pack(self.FMT, self.header_indicator,\n                               self.platform_id, self.num_section_entries,\n                               self.id_string)]\n\n        for entry in self.section_entries:\n            outlist.append(entry.record())\n\n        return b''.join(outlist)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self, valstr):\n        # type: (bytes) -> bool\n        '''\n        A method to parse an El Torito Boot Catalog out of a string.\n\n        Parameters:\n         valstr - The string to parse the El Torito Boot Catalog out of.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Boot Catalog already initialized')\n\n        if self.state == self.EXPECTING_VALIDATION_ENTRY:\n            # The first entry in an El Torito boot catalog is the Validation\n            # Entry.  A Validation entry consists of 32 bytes (described in\n            # detail in the parse_eltorito_validation_entry() method).\n            self.validation_entry.parse(valstr)\n            self.state = self.EXPECTING_INITIAL_ENTRY\n        elif self.state == self.EXPECTING_INITIAL_ENTRY:\n            # The next entry is the Initial/Default entry.  An Initial/Default\n            # entry consists of 32 bytes (described in detail in the\n            # parse_eltorito_initial_entry() method).\n            self.initial_entry.parse(valstr)\n            self.state = self.EXPECTING_SECTION_HEADER_OR_DONE\n        else:\n            val = bytes(bytearray([valstr[0]]))\n            if val == b'\\x00':\n                # An empty entry tells us we are done parsing El Torito.  Do\n                # some sanity checks.\n                last_section_index = len(self.sections) - 1\n                for index, sec in enumerate(self.sections):\n                    if sec.num_section_entries != len(sec.section_entries):\n                        raise pycdlibexception.PyCdlibInvalidISO('El Torito section header specified %d entries, only saw %d' % (sec.num_section_entries, len(sec.section_entries)))\n                    if index != last_section_index:\n                        if sec.header_indicator != 0x90:\n                            raise pycdlibexception.PyCdlibInvalidISO('Intermediate El Torito section header not properly specified')\n                    # In theory, we should also make sure that the very last\n                    # section has a header_indicator of 0x91.  However, we\n                    # have seen ISOs in the wild (FreeBSD 11.0 amd64) in which\n                    # this is not the case, so we skip that check.\n                self._initialized = True\n            elif val in (b'\\x90', b'\\x91'):\n                # A Section Header Entry\n                section_header = EltoritoSectionHeader()\n                section_header.parse(valstr)\n                self.sections.append(section_header)\n            elif val in (b'\\x88', b'\\x00'):\n                # A Section Entry. According to El Torito 2.4, a Section Entry\n                # must follow a Section Header, but we have seen ISOs in the\n                # wild that do not follow this (Mageia 4 ISOs, for instance).\n                # To deal with this, we get a little complicated here.  If there\n                # is a previous section header, and the length of the entries\n                # attached to it is less than the number of entries it should\n                # have, then we attach this entry to that header.  If there is\n                # no previous section header, or if the previous section header\n                # is already 'full', then we make this a standalone entry.\n                secentry = EltoritoEntry()\n                secentry.parse(valstr)\n                if self.sections and len(self.sections[-1].section_entries) < self.sections[-1].num_section_entries:\n                    self.sections[-1].add_parsed_entry(secentry)\n                else:\n                    self.standalone_entries.append(secentry)\n            elif val == b'\\x44':\n                # A Section Entry Extension\n                self.sections[-1].section_entries[-1].selection_criteria += valstr[2:]\n            else:\n                raise pycdlibexception.PyCdlibInvalidISO('Invalid El Torito Boot Catalog entry')\n\n        return self._initialized", "response": "A method to parse an El Torito Boot Catalog out of a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new(self, br, ino, sector_count, load_seg, media_name, system_type,\n            platform_id, bootable):\n        # type: (headervd.BootRecord, inode.Inode, int, int, str, int, int, bool) -> None\n        '''\n        A method to create a new El Torito Boot Catalog.\n\n        Parameters:\n         br - The boot record that this El Torito Boot Catalog is associated with.\n         ino - The Inode to associate with the initial entry.\n         sector_count - The number of sectors for the initial entry.\n         load_seg - The load segment address of the boot image.\n         media_name - The name of the media type, one of 'noemul', 'floppy', or 'hdemul'.\n         system_type - The partition type the entry should be.\n         platform_id - The platform id to set in the validation entry.\n         bootable - Whether this entry should be bootable.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Boot Catalog already initialized')\n\n        # Create the El Torito validation entry\n        self.validation_entry.new(platform_id)\n\n        self.initial_entry.new(sector_count, load_seg, media_name, system_type,\n                               bootable)\n        self.initial_entry.set_inode(ino)\n        ino.linked_records.append(self.initial_entry)\n\n        self.br = br\n\n        self._initialized = True", "response": "A method to create a new El Torito Boot Catalog."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef record(self):\n        # type: () -> bytes\n        '''\n        A method to generate a string representing this El Torito Boot Catalog.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing this El Torito Boot Catalog.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Boot Catalog not yet initialized')\n\n        outlist = [self.validation_entry.record(), self.initial_entry.record()]\n\n        for sec in self.sections:\n            outlist.append(sec.record())\n\n        for entry in self.standalone_entries:\n            outlist.append(entry.record())\n\n        return b''.join(outlist)", "response": "A method to generate a string representing this El Torito Boot Catalog."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_catalog_extent(self, current_extent):\n        # type: (int) -> None\n        '''\n        A method to update the extent associated with this Boot Catalog.\n\n        Parameters:\n         current_extent - New extent to associate with this Boot Catalog\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Boot Catalog not yet initialized')\n\n        self.br.update_boot_system_use(struct.pack('=L', current_extent))", "response": "A method to update the extent associated with this Boot Catalog."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_d1_characters(name):\n    # type: (bytes) -> None\n    '''\n    A function to check that a name only uses d1 characters as defined by ISO9660.\n\n    Parameters:\n     name - The name to check.\n    Returns:\n     Nothing.\n    '''\n    bytename = bytearray(name)\n    for char in bytename:\n        if char not in _allowed_d1_characters:\n            raise pycdlibexception.PyCdlibInvalidInput('ISO9660 filenames must consist of characters A-Z, 0-9, and _')", "response": "A function to check that a name only uses d1 characters as defined by ISO9660."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_iso9660_directory(fullname, interchange_level):\n    # type: (bytes, int) -> None\n    '''\n    A function to check that an directory identifier conforms to the ISO9660 rules\n    for a particular interchange level.\n\n    Parameters:\n     fullname - The name to check.\n     interchange_level - The interchange level to check against.\n    Returns:\n     Nothing.\n    '''\n    # Check to ensure the directory name is valid for the ISO according to\n    # Ecma-119 7.6.\n\n    # Ecma-119 section 7.6.1 says that a directory identifier needs at least one\n    # character\n    if not fullname:\n        raise pycdlibexception.PyCdlibInvalidInput('ISO9660 directory names must be at least 1 character long')\n\n    maxlen = float('inf')\n    if interchange_level == 1:\n        # Ecma-119 section 10.1 says that directory identifiers lengths cannot\n        # exceed 8 at interchange level 1.\n        maxlen = 8\n    elif interchange_level in (2, 3):\n        # Ecma-119 section 7.6.3 says that directory identifiers lengths cannot\n        # exceed 207.\n        maxlen = 207\n    # for interchange_level 4, we allow any length\n\n    if len(fullname) > maxlen:\n        raise pycdlibexception.PyCdlibInvalidInput('ISO9660 directory names at interchange level %d cannot exceed %d characters' % (interchange_level, maxlen))\n\n    # Ecma-119 section 7.6.1 says that directory names consist of one or more\n    # d-characters or d1-characters.  While the definition of d-characters and\n    # d1-characters is not specified in Ecma-119,\n    # http://wiki.osdev.org/ISO_9660 suggests that this consists of A-Z, 0-9, _\n    # which seems to correlate with empirical evidence.  Thus we check for that\n    # here.\n    if interchange_level < 4:\n        _check_d1_characters(fullname)", "response": "A function to check that a directory identifier conforms to the ISO9660 rules."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _interchange_level_from_filename(fullname):\n    # type: (bytes) -> int\n    '''\n    A function to determine the ISO interchange level from the filename.\n    In theory, there are 3 levels, but in practice we only deal with level 1\n    and level 3.\n\n    Parameters:\n     name - The name to use to determine the interchange level.\n    Returns:\n     The interchange level determined from this filename.\n    '''\n    (name, extension, version) = _split_iso9660_filename(fullname)\n\n    interchange_level = 1\n\n    if version != b'' and (int(version) < 1 or int(version) > 32767):\n        interchange_level = 3\n\n    if b';' in name or b';' in extension:\n        interchange_level = 3\n\n    if len(name) > 8 or len(extension) > 3:\n        interchange_level = 3\n\n    try:\n        _check_d1_characters(name)\n        _check_d1_characters(extension)\n    except pycdlibexception.PyCdlibInvalidInput:\n        interchange_level = 3\n\n    return interchange_level", "response": "A function to determine the ISO interchange level from a filename."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _reassign_vd_dirrecord_extents(vd, current_extent):\n    # type: (headervd.PrimaryOrSupplementaryVD, int) -> Tuple[int, List[inode.Inode]]\n    '''\n    An internal helper method for reassign_extents that assigns extents to\n    directory records for the passed in Volume Descriptor.  The current\n    extent is passed in, and this function returns the extent after the\n    last one it assigned.\n\n    Parameters:\n     vd - The volume descriptor on which to operate.\n     current_extent - The current extent before assigning extents to the\n                      volume descriptor directory records.\n    Returns:\n     The current extent after assigning extents to the volume descriptor\n     directory records.\n    '''\n    log_block_size = vd.logical_block_size()\n\n    # Here we re-walk the entire tree, re-assigning extents as necessary.\n    root_dir_record = vd.root_directory_record()\n    root_dir_record.set_data_location(current_extent, 0)\n    current_extent += utils.ceiling_div(root_dir_record.data_length, log_block_size)\n\n    # Walk through the list, assigning extents to all of the directories.\n    child_link_recs = []  # type: List[dr.DirectoryRecord]\n    parent_link_recs = []  # type: List[dr.DirectoryRecord]\n    file_list = []\n    ptr_index = 1\n    dirs = collections.deque([root_dir_record])\n    while dirs:\n        dir_record = dirs.popleft()\n\n        if dir_record.is_root:\n            # The root directory record doesn't need an extent assigned,\n            # so just add its children to the list and continue on\n            for child in dir_record.children:\n                if child.ptr is not None:\n                    child.ptr.update_parent_directory_number(ptr_index)\n            ptr_index += 1\n            dirs.extend(dir_record.children)\n            continue\n\n        dir_record_parent = dir_record.parent\n\n        if dir_record_parent is None:\n            raise pycdlibexception.PyCdlibInternalError('Parent of record is empty, this should never happen')\n\n        if dir_record.is_dot():\n            dir_record.set_data_location(dir_record_parent.extent_location(), 0)\n            continue\n\n        dir_record_rock_ridge = dir_record.rock_ridge\n\n        if dir_record.is_dotdot():\n            if dir_record_parent.is_root:\n                # Special case of the root directory record.  In this case, we\n                # set the dotdot extent location to the same as the root.\n                dir_record.set_data_location(dir_record_parent.extent_location(), 0)\n                continue\n\n            if dir_record_parent.parent is None:\n                raise pycdlibexception.PyCdlibInternalError('Grandparent of record is empty, this should never happen')\n            dir_record.set_data_location(dir_record_parent.parent.extent_location(), 0)\n\n            # Now that we've set the data location, move around the Rock Ridge\n            # links if necessary.\n            if dir_record_rock_ridge is not None:\n                if dir_record_rock_ridge.parent_link is not None:\n                    parent_link_recs.append(dir_record)\n\n                if dir_record_parent.rock_ridge is not None:\n                    if dir_record_parent.parent is not None:\n                        if dir_record_parent.parent.is_root:\n                            source_dr = dir_record_parent.parent.children[0]\n                        else:\n                            source_dr = dir_record_parent.parent\n\n                        if source_dr is None or source_dr.rock_ridge is None:\n                            raise pycdlibexception.PyCdlibInternalError('Expected directory record to have Rock Ridge')\n                        dir_record_rock_ridge.copy_file_links(source_dr.rock_ridge)\n            continue\n\n        if dir_record.is_dir():\n            dir_record.set_data_location(current_extent, current_extent)\n            for child in dir_record.children:\n                if child.ptr is not None:\n                    child.ptr.update_parent_directory_number(ptr_index)\n            ptr_index += 1\n            if dir_record_rock_ridge is None or not dir_record_rock_ridge.child_link_record_exists():\n                current_extent += utils.ceiling_div(dir_record.data_length, log_block_size)\n            dirs.extend(dir_record.children)\n        else:\n            if dir_record.data_length == 0 or (dir_record_rock_ridge is not None and (dir_record_rock_ridge.child_link_record_exists() or dir_record_rock_ridge.is_symlink())):\n                # If this is a child link record, the extent location really\n                # doesn't matter, since it is fake.  We set it to zero.\n                dir_record.set_data_location(0, 0)\n            else:\n                if dir_record.inode is not None:\n                    file_list.append(dir_record.inode)\n\n        if dir_record_rock_ridge is not None:\n            if dir_record_rock_ridge.dr_entries.ce_record is not None and dir_record_rock_ridge.ce_block is not None:\n                if dir_record_rock_ridge.ce_block.extent_location() < 0:\n                    dir_record_rock_ridge.ce_block.set_extent_location(current_extent)\n                    current_extent += 1\n                dir_record_rock_ridge.dr_entries.ce_record.update_extent(dir_record_rock_ridge.ce_block.extent_location())\n            if dir_record_rock_ridge.cl_to_moved_dr is not None:\n                child_link_recs.append(dir_record)\n\n    # After we have reshuffled the extents, we need to update the rock ridge\n    # links.\n    for ch in child_link_recs:\n        if ch.rock_ridge is not None:\n            ch.rock_ridge.child_link_update_from_dirrecord()\n\n    for p in parent_link_recs:\n        if p.rock_ridge is not None:\n            p.rock_ridge.parent_link_update_from_dirrecord()\n\n    return current_extent, file_list", "response": "Internal method that reassigns extents to the current extent of the passed in Volume Descriptor directory record."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _assign_udf_desc_extents(descs, start_extent):\n    # type: (PyCdlib._UDFDescriptors, int) -> None\n    '''\n    An internal function to assign a consecutive sequence of extents for the\n    given set of UDF Descriptors, starting at the given extent.\n\n    Parameters:\n     descs - The PyCdlib._UDFDescriptors object to assign extents for.\n     start_extent - The starting extent to assign from.\n    Returns:\n     Nothing.\n    '''\n    current_extent = start_extent\n\n    descs.pvd.set_extent_location(current_extent)\n    current_extent += 1\n\n    descs.impl_use.set_extent_location(current_extent)\n    current_extent += 1\n\n    descs.partition.set_extent_location(current_extent)\n    current_extent += 1\n\n    descs.logical_volume.set_extent_location(current_extent)\n    current_extent += 1\n\n    descs.unallocated_space.set_extent_location(current_extent)\n    current_extent += 1\n\n    descs.terminator.set_extent_location(current_extent)\n    current_extent += 1", "response": "A function to assign extents for the given set of UDF Descriptors to the current extent."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_dr_record_by_name(vd, path, encoding):\n    # type: (headervd.PrimaryOrSupplementaryVD, bytes, str) -> dr.DirectoryRecord\n    '''\n    An internal function to find an directory record on the ISO given an ISO\n    or Joliet path.  If the entry is found, it returns the directory record\n    object corresponding to that entry.  If the entry could not be found,\n    a pycdlibexception.PyCdlibInvalidInput exception is raised.\n\n    Parameters:\n     vd - The Volume Descriptor to look in for the Directory Record.\n     path - The ISO or Joliet entry to find the Directory Record for.\n     encoding - The string encoding used for the path.\n    Returns:\n     The directory record entry representing the entry on the ISO.\n    '''\n    if not utils.starts_with_slash(path):\n        raise pycdlibexception.PyCdlibInvalidInput('Must be a path starting with /')\n\n    root_dir_record = vd.root_directory_record()\n\n    # If the path is just the slash, we just want the root directory, so\n    # get the child there and quit.\n    if path == b'/':\n        return root_dir_record\n\n    # Split the path along the slashes\n    splitpath = utils.split_path(path)\n\n    currpath = splitpath.pop(0).decode('utf-8').encode(encoding)\n\n    entry = root_dir_record\n\n    tmpdr = dr.DirectoryRecord()\n\n    while True:\n        child = None\n\n        thelist = entry.children\n        lo = 2\n        hi = len(thelist)\n        while lo < hi:\n            mid = (lo + hi) // 2\n            tmpdr.file_ident = currpath\n            if thelist[mid] < tmpdr:\n                lo = mid + 1\n            else:\n                hi = mid\n        index = lo\n        if index != len(thelist) and thelist[index].file_ident == currpath:\n            child = thelist[index]\n\n        if child is None:\n            # We failed to find this component of the path, so break out of the\n            # loop and fail\n            break\n\n        if child.rock_ridge is not None and child.rock_ridge.child_link_record_exists():\n            # Here, the rock ridge extension has a child link, so we\n            # need to follow it.\n            child = child.rock_ridge.cl_to_moved_dr\n            if child is None:\n                break\n\n        # We found the child, and it is the last one we are looking for;\n        # return it.\n        if not splitpath:\n            return child\n\n        if not child.is_dir():\n            break\n        entry = child\n        currpath = splitpath.pop(0).decode('utf-8').encode(encoding)\n\n    raise pycdlibexception.PyCdlibInvalidInput('Could not find path')", "response": "An internal function to find a Directory Record by name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, size=None):\n        # type: (Optional[int]) -> bytes\n        '''\n        A method to read and return up to size bytes.\n\n        Parameters:\n         size - Optional parameter to read size number of bytes; if None or\n                negative, all remaining bytes in the file will be read\n        Returns:\n         The number of bytes requested or the rest of the data left in the file,\n         whichever is smaller.  If the file is at or past EOF, returns an empty\n         bytestring.\n        '''\n        if not self._open:\n            raise pycdlibexception.PyCdlibInvalidInput('I/O operation on closed file.')\n\n        if self._offset >= self._length:\n            return b''\n\n        if size is None or size < 0:\n            data = self.readall()\n        else:\n            readsize = min(self._length - self._offset, size)\n            data = self._fp.read(readsize)\n            self._offset += readsize\n\n        return data", "response": "A method to read and return up to size bytes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readall(self):\n        # type: () -> bytes\n        '''\n        A method to read and return the remaining bytes in the file.\n\n        Parameters:\n         None.\n        Returns:\n         The rest of the data left in the file.  If the file is at or past EOF,\n         returns an empty bytestring.\n        '''\n        if not self._open:\n            raise pycdlibexception.PyCdlibInvalidInput('I/O operation on closed file.')\n\n        readsize = self._length - self._offset\n        if readsize > 0:\n            data = self._fp.read(readsize)\n            self._offset += readsize\n        else:\n            data = b''\n\n        return data", "response": "A method to read and return the remaining bytes in the file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef seek(self, offset, whence=0):\n        # type: (int, int) -> int\n        '''\n        A method to change the stream position to byte offset offset.  The\n        offset is interpreted relative to the position indicated by whence.\n        Valid values for whence are:\n\n        * 0 -- start of stream (the default); offset should be zero or positive\n        * 1 -- current stream position; offset may be negative\n        * 2 -- end of stream; offset is usually negative\n\n        Parameters:\n         offset - The byte offset to seek to.\n         whence - The position in the file to start from (0 for start, 1 for\n                  current, 2 for end)\n        Returns:\n         The new absolute position.\n        '''\n        if not self._open:\n            raise pycdlibexception.PyCdlibInvalidInput('I/O operation on closed file.')\n\n        if isinstance(offset, float):\n            raise pycdlibexception.PyCdlibInvalidInput('an integer is required')\n\n        if whence == 0:\n            # From beginning of file\n            if offset < 0:\n                raise pycdlibexception.PyCdlibInvalidInput('Invalid offset value (must be positive)')\n\n            if offset < self._length:\n                self._fp.seek(self._startpos + offset, 0)\n\n            self._offset = offset\n        elif whence == 1:\n            # From current file position\n            if self._offset + offset < 0:\n                raise pycdlibexception.PyCdlibInvalidInput('Invalid offset value (cannot seek before start of file)')\n\n            if self._offset + offset < self._length:\n                self._fp.seek(self._startpos + self._offset + offset, 0)\n\n            self._offset += offset\n        elif whence == 2:\n            # From end of file\n            if offset < 0 and abs(offset) > self._length:\n                raise pycdlibexception.PyCdlibInvalidInput('Invalid offset value (cannot seek before start of file)')\n\n            if self._length + offset < self._length:\n                self._fp.seek(self._length + offset, 0)\n\n            self._offset = self._length + offset\n        else:\n            raise pycdlibexception.PyCdlibInvalidInput('Invalid value for whence (options are 0, 1, and 2)')\n\n        return self._offset", "response": "A method to change the stream position to a specific offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _seek_to_extent(self, extent):\n        # type: (int) -> None\n        '''\n        An internal method to seek to a particular extent on the input ISO.\n\n        Parameters:\n         extent - The extent to seek to.\n        Returns:\n         Nothing.\n        '''\n        self._cdfp.seek(extent * self.pvd.logical_block_size())", "response": "Internal method to seek to a particular extent on the input ISO."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_rr_record(self, rr_path):\n        # type: (bytes) -> dr.DirectoryRecord\n        '''\n        An internal method to find an directory record on the ISO given a Rock\n        Ridge path.  If the entry is found, it returns the directory record\n        object corresponding to that entry.  If the entry could not be found, a\n        pycdlibexception.PyCdlibInvalidInput is raised.\n\n        Parameters:\n         rr_path - The Rock Ridge path to lookup.\n        Returns:\n         The directory record entry representing the entry on the ISO.\n        '''\n        if not utils.starts_with_slash(rr_path):\n            raise pycdlibexception.PyCdlibInvalidInput('Must be a path starting with /')\n\n        root_dir_record = self.pvd.root_directory_record()\n\n        # If the path is just the slash, we just want the root directory, so\n        # get the child there and quit.\n        if rr_path == b'/':\n            return root_dir_record\n\n        # Split the path along the slashes\n        splitpath = utils.split_path(rr_path)\n\n        currpath = splitpath.pop(0).decode('utf-8').encode('utf-8')\n\n        entry = root_dir_record\n\n        while True:\n            child = None\n\n            thelist = entry.rr_children\n            lo = 0\n            hi = len(thelist)\n            while lo < hi:\n                mid = (lo + hi) // 2\n\n                tmpchild = thelist[mid]\n\n                if tmpchild.rock_ridge is None:\n                    raise pycdlibexception.PyCdlibInvalidInput('Record without Rock Ridge entry on Rock Ridge ISO')\n\n                if tmpchild.rock_ridge.name() < currpath:\n                    lo = mid + 1\n                else:\n                    hi = mid\n            index = lo\n            tmpchild = thelist[index]\n            if index != len(thelist) and tmpchild.rock_ridge is not None and tmpchild.rock_ridge.name() == currpath:\n                child = thelist[index]\n\n            if child is None:\n                # We failed to find this component of the path, so break out of the\n                # loop and fail\n                break\n\n            if child.rock_ridge is not None and child.rock_ridge.child_link_record_exists():\n                # Here, the rock ridge extension has a child link, so we\n                # need to follow it.\n                child = child.rock_ridge.cl_to_moved_dr\n                if child is None:\n                    break\n\n            # We found the child, and it is the last one we are looking for;\n            # return it.\n            if not splitpath:\n                return child\n\n            if not child.is_dir():\n                break\n            entry = child\n            currpath = splitpath.pop(0).decode('utf-8').encode('utf-8')\n\n        raise pycdlibexception.PyCdlibInvalidInput('Could not find path')", "response": "An internal method to find a directory record corresponding to a Rock Ridge path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_udf_record(self, udf_path):\n        # type: (bytes) -> Tuple[Optional[udfmod.UDFFileIdentifierDescriptor], udfmod.UDFFileEntry]\n        '''\n        An internal method to find an directory record on the ISO given a UDF\n        path.  If the entry is found, it returns the directory record object\n        corresponding to that entry.  If the entry could not be found, a\n        pycdlibexception.PyCdlibInvalidInput is raised.\n\n        Parameters:\n         udf_path - The UDF path to lookup.\n        Returns:\n         The UDF File Entry representing the entry on the ISO.\n        '''\n        # If the path is just the slash, we just want the root directory, so\n        # get the child there and quit.\n        if udf_path == b'/':\n            return None, self.udf_root  # type: ignore\n\n        # Split the path along the slashes\n        splitpath = utils.split_path(udf_path)\n\n        currpath = splitpath.pop(0)\n\n        entry = self.udf_root\n\n        while entry is not None:\n            child = entry.find_file_ident_desc_by_name(currpath)\n\n            # We found the child, and it is the last one we are looking for;\n            # return it.\n            if not splitpath:\n                return child, child.file_entry  # type: ignore\n\n            if not child.is_dir():\n                break\n            entry = child.file_entry\n            currpath = splitpath.pop(0)\n\n        raise pycdlibexception.PyCdlibInvalidInput('Could not find path')", "response": "An internal method to find an object record corresponding to a UDF path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _udf_name_and_parent_from_path(self, udf_path):\n        # type: (bytes) -> Tuple[bytes, udfmod.UDFFileEntry]\n        '''\n        An internal method to find the parent directory record and name given a\n        UDF path.  If the parent is found, return a tuple containing the basename\n        of the path and the parent UDF File Entry object.\n\n        Parameters:\n         udf_path - The absolute UDF path to the entry on the ISO.\n        Returns:\n         A tuple containing just the name of the entry and a UDF File Entry\n         object representing the parent of the entry.\n        '''\n        splitpath = utils.split_path(udf_path)\n        name = splitpath.pop()\n        (parent_ident_unused, parent) = self._find_udf_record(b'/' + b'/'.join(splitpath))\n\n        return (name.decode('utf-8').encode('utf-8'), parent)", "response": "Internal method to find the parent directory record and name given a UDF path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _walk_directories(self, vd, extent_to_ptr, extent_to_inode, path_table_records):\n        # type: (headervd.PrimaryOrSupplementaryVD, Dict[int, path_table_record.PathTableRecord], Dict[int, inode.Inode], List[path_table_record.PathTableRecord]) -> Tuple[int, int]\n        '''\n        An internal method to walk the directory records in a volume descriptor,\n        starting with the root.  For each child in the directory record,\n        we create a new dr.DirectoryRecord object and append it to the parent.\n\n        Parameters:\n         vd - The volume descriptor to walk.\n         extent_to_ptr - A dictionary mapping extents to PTRs.\n         extent_to_inode - A dictionary mapping extents to Inodes.\n         path_table_records - The list of path table records.\n        Returns:\n         The interchange level that this ISO conforms to.\n        '''\n        cdfp = self._cdfp\n        old_loc = cdfp.tell()\n        cdfp.seek(0, os.SEEK_END)\n        iso_file_length = cdfp.tell()\n        cdfp.seek(old_loc)\n\n        all_extent_to_dr = {}  # type: Dict[int, dr.DirectoryRecord]\n        is_pvd = vd.is_pvd()\n        root_dir_record = vd.root_directory_record()\n        root_dir_record.set_ptr(path_table_records[0])\n        interchange_level = 1\n        block_size = vd.logical_block_size()\n        parent_links = []\n        child_links = []\n        lastbyte = 0\n        dirs = collections.deque([root_dir_record])\n        while dirs:\n            dir_record = dirs.popleft()\n\n            self._seek_to_extent(dir_record.extent_location())\n            length = dir_record.get_data_length()\n            offset = 0\n            last_record = None  # type: Optional[dr.DirectoryRecord]\n            data = cdfp.read(length)\n            while offset < length:\n                if offset > (len(data) - 1):\n                    # The data we read off of the ISO was shorter than what we\n                    # expected.  The ISO is corrupt, throw an error.\n                    raise pycdlibexception.PyCdlibInvalidISO('Invalid directory record')\n                lenbyte = bytearray([data[offset]])[0]\n                if lenbyte == 0:\n                    # If we saw a zero length, this is probably the padding for\n                    # the end of this extent.  Move the offset to the start of\n                    # the next extent.\n                    padsize = block_size - (offset % block_size)\n                    if data[offset:offset + padsize] != b'\\x00' * padsize:\n                        # For now we are pedantic, and if the padding bytes\n                        # are not all zero we throw an Exception.  Depending\n                        # one what we see in the wild, we may have to loosen\n                        # this check.\n                        raise pycdlibexception.PyCdlibInvalidISO('Invalid padding on ISO')\n\n                    offset = offset + padsize\n                    continue\n\n                new_record = dr.DirectoryRecord()\n                rr = new_record.parse(vd, data[offset:offset + lenbyte],\n                                      dir_record)\n                offset += lenbyte\n\n                # The parse method of dr.DirectoryRecord returns '' if this\n                # record doesn't have Rock Ridge extensions, or the version of\n                # the Rock Ridge extension (as detected for this directory record).\n                self._set_rock_ridge(rr)\n\n                # Cache some properties of this record for later use.\n                is_symlink = new_record.rock_ridge is not None and new_record.rock_ridge.is_symlink()\n                dots = new_record.is_dot() or new_record.is_dotdot()\n                rr_cl = new_record.rock_ridge is not None and new_record.rock_ridge.child_link_record_exists()\n                is_dir = new_record.is_dir()\n                data_length = new_record.get_data_length()\n                new_extent_loc = new_record.extent_location()\n\n                if is_pvd and not dots and not rr_cl and not is_symlink and new_extent_loc not in all_extent_to_dr:\n                    all_extent_to_dr[new_extent_loc] = new_record\n\n                # ISO generation programs sometimes use random extent locations\n                # for zero-length files.  Thus, it is not valid for us to link\n                # zero-length files to other files, as the linkage will be\n                # essentially random.  Make sure we ignore zero-length files\n                # (which includes symlinks) for linkage.  Similarly, we don't\n                # do the lastbyte calculation on zero-length files for the same\n                # reason.\n                if not is_dir:\n                    len_to_use = data_length\n                    extent_to_use = new_extent_loc\n                    # An important side-effect of this is that zero-length files\n                    # or symlinks get an inode, but it is always set to length 0\n                    # and location 0 and not actually written out.  This is so\n                    # that we can 'link' everything through the Inode.\n                    if len_to_use == 0 or is_symlink:\n                        len_to_use = 0\n                        extent_to_use = 0\n\n                    # Directory Records that point to the El Torito Boot Catalog\n                    # do not get Inodes since all of that is handled in-memory.\n                    if self.eltorito_boot_catalog is not None and extent_to_use == self.eltorito_boot_catalog.extent_location():\n                        self.eltorito_boot_catalog.add_dirrecord(new_record)\n                    else:\n                        # For all real files, we create an inode that points to\n                        # the location on disk.\n                        if extent_to_use in extent_to_inode:\n                            ino = extent_to_inode[extent_to_use]\n                        else:\n                            ino = inode.Inode()\n                            ino.parse(extent_to_use, len_to_use, cdfp,\n                                      block_size)\n                            extent_to_inode[extent_to_use] = ino\n                            self.inodes.append(ino)\n\n                        ino.linked_records.append(new_record)\n                        new_record.inode = ino\n\n                    new_end = extent_to_use * block_size + len_to_use\n                    if new_end > iso_file_length:\n                        # In this case, the end of the file is beyond the size\n                        # of the file.  Since this can't possibly work, truncate\n                        # the file size.\n                        if new_record.inode is not None:\n                            new_record.inode.data_length = iso_file_length - extent_to_use * block_size\n                            for rec in new_record.inode.linked_records:\n                                rec.set_data_length(new_end)\n                    else:\n                        # In this case, the new end is still within the file\n                        # size, but the PVD size is wrong.  Set the lastbyte\n                        # appropriately, which will eventually be used to fix\n                        # the PVD size.\n                        lastbyte = max(lastbyte, new_end)\n\n                if new_record.rock_ridge is not None and new_record.rock_ridge.dr_entries.ce_record is not None:\n                    ce_record = new_record.rock_ridge.dr_entries.ce_record\n                    orig_pos = cdfp.tell()\n                    self._seek_to_extent(ce_record.bl_cont_area)\n                    cdfp.seek(ce_record.offset_cont_area, os.SEEK_CUR)\n                    con_block = cdfp.read(ce_record.len_cont_area)\n                    new_record.rock_ridge.parse(con_block, False,\n                                                new_record.rock_ridge.bytes_to_skip,\n                                                True)\n                    cdfp.seek(orig_pos)\n                    block = self.pvd.track_rr_ce_entry(ce_record.bl_cont_area,\n                                                       ce_record.offset_cont_area,\n                                                       ce_record.len_cont_area)\n                    new_record.rock_ridge.update_ce_block(block)\n\n                if rr_cl:\n                    child_links.append(new_record)\n\n                if is_dir:\n                    if new_record.rock_ridge is not None and new_record.rock_ridge.relocated_record():\n                        self._rr_moved_record = new_record\n\n                    if new_record.is_dotdot() and new_record.rock_ridge is not None and new_record.rock_ridge.parent_link_record_exists():\n                        # If this is the dotdot record, and it has a parent\n                        # link record, make sure to link up the parent link\n                        # directory record.\n                        parent_links.append(new_record)\n                    if not dots and not rr_cl:\n                        dirs.append(new_record)\n                        new_record.set_ptr(extent_to_ptr[new_extent_loc])\n\n                if new_record.parent is None:\n                    raise pycdlibexception.PyCdlibInternalError('Trying to track child with no parent')\n                try_long_entry = False\n                try:\n                    new_record.parent.track_child(new_record, block_size)\n                except pycdlibexception.PyCdlibInvalidInput:\n                    # dir_record.track_child() may throw a PyCdlibInvalidInput if it\n                    # saw a duplicate child.  However, we allow duplicate children\n                    # iff this record is a file and the last child has the same name;\n                    # this means we have a very long entry.  If that is not the case,\n                    # re-raise the error, otherwise pass through to try with the\n                    # allow_duplicates flag set to True.\n                    if new_record.is_dir() or last_record is None or last_record.file_identifier() != new_record.file_identifier():\n                        raise\n                    else:\n                        try_long_entry = True\n\n                if try_long_entry:\n                    new_record.parent.track_child(new_record, block_size, True)\n\n                if is_pvd:\n                    if new_record.is_dir():\n                        new_level = _interchange_level_from_directory(new_record.file_identifier())\n                    else:\n                        new_level = _interchange_level_from_filename(new_record.file_identifier())\n                    interchange_level = max(interchange_level, new_level)\n\n                last_record = new_record\n\n        for pl in parent_links:\n            if pl.rock_ridge is not None:\n                pl.rock_ridge.parent_link = all_extent_to_dr[pl.rock_ridge.parent_link_extent()]\n\n        for cl in child_links:\n            if cl.rock_ridge is not None:\n                cl.rock_ridge.cl_to_moved_dr = all_extent_to_dr[cl.rock_ridge.child_link_extent()]\n                if cl.rock_ridge.cl_to_moved_dr.rock_ridge is not None:\n                    cl.rock_ridge.cl_to_moved_dr.rock_ridge.moved_to_cl_dr = cl\n\n        return interchange_level, lastbyte", "response": "Internal method that walks the directory records in a volume descriptor and creates a new directory record object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _initialize(self):\n        # type: () -> None\n        '''\n        An internal method to re-initialize the object.  Called from\n        both __init__ and close.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        self._cdfp = BytesIO()\n        self.svds = []  # type: List[headervd.PrimaryOrSupplementaryVD]\n        self.brs = []  # type: List[headervd.BootRecord]\n        self.vdsts = []  # type: List[headervd.VolumeDescriptorSetTerminator]\n        self.eltorito_boot_catalog = None  # type: Optional[eltorito.EltoritoBootCatalog]\n        self._initialized = False\n        self.rock_ridge = ''\n        self.isohybrid_mbr = None  # type: Optional[isohybrid.IsoHybrid]\n        self.xa = False\n        self._managing_fp = False\n        self.pvds = []  # type: List[headervd.PrimaryOrSupplementaryVD]\n        self._has_udf = False\n        self.udf_bea = udfmod.BEAVolumeStructure()  # type: udfmod.BEAVolumeStructure\n        self.udf_nsr = udfmod.NSRVolumeStructure()  # type: udfmod.NSRVolumeStructure\n        self.udf_tea = udfmod.TEAVolumeStructure()  # type: udfmod.TEAVolumeStructure\n        self.udf_anchors = []  # type: List[udfmod.UDFAnchorVolumeStructure]\n        self.udf_main_descs = self._UDFDescriptors()\n        self.udf_reserve_descs = self._UDFDescriptors()\n        self.udf_logical_volume_integrity = udfmod.UDFLogicalVolumeIntegrityDescriptor()\n        self.udf_logical_volume_integrity_terminator = udfmod.UDFTerminatingDescriptor()\n        self.udf_root = None  # type: Optional[udfmod.UDFFileEntry]\n        self.udf_file_set = udfmod.UDFFileSetDescriptor()\n        self.udf_file_set_terminator = udfmod.UDFTerminatingDescriptor()\n        self._needs_reshuffle = False\n        self._rr_moved_record = None  # type: ignore\n        self._rr_moved_name = None  # type: Optional[bytes]\n        self._rr_moved_rr_name = None  # type: Optional[bytes]\n        self.enhanced_vd = None  # type: Optional[headervd.PrimaryOrSupplementaryVD]\n        self.joliet_vd = None  # type: Optional[headervd.PrimaryOrSupplementaryVD]\n        self._find_iso_record.cache_clear()  # pylint: disable=no-member\n        self._find_rr_record.cache_clear()  # pylint: disable=no-member\n        self._find_joliet_record.cache_clear()  # pylint: disable=no-member\n        self._find_udf_record.cache_clear()  # pylint: disable=no-member\n        self._write_check_list = []  # type: List[PyCdlib._WriteRange]\n        self.version_vd = None  # type: Optional[headervd.VersionVolumeDescriptor]\n        self.inodes = []", "response": "Internal method to re - initialize the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_path_table(self, ptr_size, extent):\n        # type: (int, int) -> Tuple[List[path_table_record.PathTableRecord], Dict[int, path_table_record.PathTableRecord]]\n        '''\n        An internal method to parse a path table on an ISO.  For each path\n        table entry found, a Path Table Record object is created, and the\n        callback is called.\n\n        Parameters:\n         vd - The volume descriptor that these path table records correspond to.\n         extent - The extent at which this path table record starts.\n         callback - The callback to call for each path table record.\n        Returns:\n         A tuple consisting of the list of path table record entries and a\n         dictionary of the extent locations to the path table record entries.\n        '''\n        self._seek_to_extent(extent)\n        data = self._cdfp.read(ptr_size)\n        offset = 0\n        out = []\n        extent_to_ptr = {}\n        while offset < ptr_size:\n            ptr = path_table_record.PathTableRecord()\n            len_di_byte = bytearray([data[offset]])[0]\n            read_len = path_table_record.PathTableRecord.record_length(len_di_byte)\n\n            ptr.parse(data[offset:offset + read_len])\n            out.append(ptr)\n            extent_to_ptr[ptr.extent_location] = ptr\n            offset += read_len\n\n        return out, extent_to_ptr", "response": "Internal method to parse a path table on an ISO."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _remove_child_from_dr(self, child, index, logical_block_size):\n        # type: (dr.DirectoryRecord, int, int) -> int\n        '''\n        An internal method to remove a child from a directory record, shrinking\n        the space in the Volume Descriptor if necessary.\n\n        Parameters:\n         child - The child to remove.\n         index - The index of the child into the parent's child array.\n         logical_block_size - The size of one logical block.\n        Returns:\n         The number of bytes to remove for this directory record (this may be zero).\n        '''\n\n        if child.parent is None:\n            raise pycdlibexception.PyCdlibInternalError('Trying to remove child from non-existent parent')\n\n        self._find_iso_record.cache_clear()  # pylint: disable=no-member\n        self._find_rr_record.cache_clear()  # pylint: disable=no-member\n        self._find_joliet_record.cache_clear()  # pylint: disable=no-member\n\n        # The remove_child() method returns True if the parent no longer needs\n        # the extent that the directory record for this child was on.  Remove\n        # the extent as appropriate here.\n        if child.parent.remove_child(child, index, logical_block_size):\n            return self.pvd.logical_block_size()\n\n        return 0", "response": "An internal method to remove a child from a directory record. This method is used by the remove_child method of the Directory Record."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_to_ptr_size(self, ptr):\n        # type: (path_table_record.PathTableRecord) -> int\n        '''\n        An internal method to add a PTR to a VD, adding space to the VD if\n        necessary.\n\n        Parameters:\n         ptr - The PTR to add to the vd.\n        Returns:\n         The number of additional bytes that are needed to fit the new PTR\n         (this may be zero).\n        '''\n        num_bytes_to_add = 0\n        for pvd in self.pvds:\n            # The add_to_ptr_size() method returns True if the PVD needs\n            # additional space in the PTR to store this directory.  We always\n            # add 4 additional extents for that (2 for LE, 2 for BE).\n            if pvd.add_to_ptr_size(path_table_record.PathTableRecord.record_length(ptr.len_di)):\n                num_bytes_to_add += 4 * self.pvd.logical_block_size()\n\n        return num_bytes_to_add", "response": "Internal method to add a PTR to a VD adding space to the VD if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_or_create_rr_moved(self):\n        # type: () -> int\n        '''\n        An internal method to find the /RR_MOVED directory on the ISO.  If it\n        already exists, the directory record to it is returned.  If it doesn't\n        yet exist, it is created and the directory record to it is returned.\n\n        Parameters:\n         None.\n        Returns:\n         The number of additional bytes needed for the rr_moved directory (this\n         may be zero).\n        '''\n\n        if self._rr_moved_record is not None:\n            return 0\n\n        if self._rr_moved_name is None:\n            self._rr_moved_name = b'RR_MOVED'\n        if self._rr_moved_rr_name is None:\n            self._rr_moved_rr_name = b'rr_moved'\n\n        # No rr_moved found, so we have to create it.\n        rec = dr.DirectoryRecord()\n        rec.new_dir(self.pvd, self._rr_moved_name,\n                    self.pvd.root_directory_record(),\n                    self.pvd.sequence_number(), self.rock_ridge,\n                    self._rr_moved_rr_name, self.pvd.logical_block_size(),\n                    False, False, self.xa, 0o040555)\n        num_bytes_to_add = self._add_child_to_dr(rec,\n                                                 self.pvd.logical_block_size())\n\n        self._create_dot(self.pvd, rec, self.rock_ridge, self.xa, 0o040555)\n        self._create_dotdot(self.pvd, rec, self.rock_ridge, False, self.xa,\n                            0o040555)\n\n        # We always need to add an entry to the path table record\n        ptr = path_table_record.PathTableRecord()\n        ptr.new_dir(self._rr_moved_name)\n        num_bytes_to_add += self.pvd.logical_block_size() + self._add_to_ptr_size(ptr)\n\n        rec.set_ptr(ptr)\n\n        self._rr_moved_record = rec\n\n        return num_bytes_to_add", "response": "Internal method to find the rr_moved directory on the ISO and create it if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_rr_name(self, rr_name):\n        # type: (Optional[str]) -> bytes\n        '''\n        An internal method to check whether this ISO requires or does not\n        require a Rock Ridge path.\n\n        Parameters:\n         rr_name - The Rock Ridge name.\n        Returns:\n         The Rock Ridge name in bytes if this is a Rock Ridge ISO, None otherwise.\n        '''\n        if self.rock_ridge:\n            if not rr_name:\n                raise pycdlibexception.PyCdlibInvalidInput('A rock ridge name must be passed for a rock-ridge ISO')\n\n            if rr_name.count('/') != 0:\n                raise pycdlibexception.PyCdlibInvalidInput('A rock ridge name must be relative')\n\n            return rr_name.encode('utf-8')\n\n        if rr_name:\n            raise pycdlibexception.PyCdlibInvalidInput('A rock ridge name can only be specified for a rock-ridge ISO')\n\n        return b''", "response": "An internal method to check whether this ISO requires or does not require a Rock Ridge path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _normalize_joliet_path(self, joliet_path):\n        # type: (str) -> bytes\n        '''\n        An internal method to check whether this ISO does or does not require\n        a Joliet path.  If a Joliet path is required, the path is normalized\n        and returned.\n\n        Parameters:\n         joliet_path - The joliet_path to normalize (if necessary).\n        Returns:\n         The normalized joliet_path if this ISO has Joliet, None otherwise.\n        '''\n        tmp_path = b''\n        if self.joliet_vd is not None:\n            if not joliet_path:\n                raise pycdlibexception.PyCdlibInvalidInput('A Joliet path must be passed for a Joliet ISO')\n            tmp_path = utils.normpath(joliet_path)\n        else:\n            if joliet_path:\n                raise pycdlibexception.PyCdlibInvalidInput('A Joliet path can only be specified for a Joliet ISO')\n\n        return tmp_path", "response": "An internal method to normalize a Joliet path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_udf_vol_descs(self, extent, length, descs):\n        # type: (int, int, PyCdlib._UDFDescriptors) -> None\n        '''\n        An internal method to parse a set of UDF Volume Descriptors.\n\n        Parameters:\n         extent - The extent at which to start parsing.\n         length - The number of bytes to read from the incoming ISO.\n         descs - The _UDFDescriptors object to store parsed objects into.\n        Returns:\n         Nothing.\n        '''\n        # Read in the Volume Descriptor Sequence\n        self._seek_to_extent(extent)\n        vd_data = self._cdfp.read(length)\n\n        # And parse it.  Since the sequence doesn't have to be in any set order,\n        # and since some of the entries may be missing, we parse the Descriptor\n        # Tag (the first 16 bytes) to find out what kind of descriptor it is,\n        # then construct the correct type based on that.  We keep going until we\n        # see a Terminating Descriptor.\n\n        block_size = self.pvd.logical_block_size()\n        offset = 0\n        current_extent = extent\n        done = False\n        while not done:\n            desc_tag = udfmod.UDFTag()\n            desc_tag.parse(vd_data[offset:], current_extent)\n            if desc_tag.tag_ident == 1:\n                descs.pvd.parse(vd_data[offset:offset + 512], current_extent, desc_tag)\n            elif desc_tag.tag_ident == 4:\n                descs.impl_use.parse(vd_data[offset:offset + 512], current_extent, desc_tag)\n            elif desc_tag.tag_ident == 5:\n                descs.partition.parse(vd_data[offset:offset + 512], current_extent, desc_tag)\n            elif desc_tag.tag_ident == 6:\n                descs.logical_volume.parse(vd_data[offset:offset + 512], current_extent, desc_tag)\n            elif desc_tag.tag_ident == 7:\n                descs.unallocated_space.parse(vd_data[offset:offset + 512], current_extent, desc_tag)\n            elif desc_tag.tag_ident == 8:\n                descs.terminator.parse(current_extent, desc_tag)\n                done = True\n            else:\n                raise pycdlibexception.PyCdlibInvalidISO('UDF Tag identifier not %d' % (desc_tag.tag_ident))\n\n            offset += block_size\n            current_extent += 1", "response": "An internal method to parse a set of UDF Volume Descriptors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _open_fp(self, fp):\n        # type: (BinaryIO) -> None\n        '''\n        An internal method to open an existing ISO for inspection and\n        modification.  Note that the file object passed in here must stay open\n        for the lifetime of this object, as the PyCdlib class uses it internally\n        to do writing and reading operations.\n\n        Parameters:\n         fp - The file object containing the ISO to open up.\n        Returns:\n         Nothing.\n        '''\n        if hasattr(fp, 'mode') and 'b' not in fp.mode:\n            raise pycdlibexception.PyCdlibInvalidInput(\"The file to open must be in binary mode (add 'b' to the open flags)\")\n\n        self._cdfp = fp\n\n        # Get the Primary Volume Descriptor (pvd), the set of Supplementary\n        # Volume Descriptors (svds), the set of Volume Partition\n        # Descriptors (vpds), the set of Boot Records (brs), and the set of\n        # Volume Descriptor Set Terminators (vdsts)\n        self._parse_volume_descriptors()\n\n        old = self._cdfp.tell()\n        self._cdfp.seek(0)\n        tmp_mbr = isohybrid.IsoHybrid()\n        if tmp_mbr.parse(self._cdfp.read(512)):\n            # We only save the object if it turns out to be a valid IsoHybrid\n            self.isohybrid_mbr = tmp_mbr\n        self._cdfp.seek(old)\n\n        if self.pvd.application_use[141:149] == b'CD-XA001':\n            self.xa = True\n\n        for br in self.brs:\n            self._check_and_parse_eltorito(br)\n\n        # Now that we have the PVD, parse the Path Tables according to Ecma-119\n        # section 9.4.  We want to ensure that the big endian versions agree\n        # with the little endian ones (to make sure it is a valid ISO).\n\n        # Little Endian first\n        le_ptrs, extent_to_ptr = self._parse_path_table(self.pvd.path_table_size(),\n                                                        self.pvd.path_table_location_le)\n\n        # Big Endian next.\n        tmp_be_ptrs, e_unused = self._parse_path_table(self.pvd.path_table_size(),\n                                                       self.pvd.path_table_location_be)\n\n        for index, ptr in enumerate(le_ptrs):\n            if not ptr.equal_to_be(tmp_be_ptrs[index]):\n                raise pycdlibexception.PyCdlibInvalidISO('Little-endian and big-endian path table records do not agree')\n\n        self.interchange_level = 1\n        for svd in self.svds:\n            if svd.version == 2 and svd.file_structure_version == 2:\n                self.interchange_level = 4\n                break\n\n        extent_to_inode = {}  # type: Dict[int, inode.Inode]\n\n        # OK, so now that we have the PVD, we start at its root directory\n        # record and find all of the files\n        ic_level, lastbyte = self._walk_directories(self.pvd, extent_to_ptr,\n                                                    extent_to_inode, le_ptrs)\n\n        self.interchange_level = max(self.interchange_level, ic_level)\n\n        # On El Torito ISOs, after we have walked the directories we look\n        # to see if all of the entries in El Torito have corresponding\n        # directory records.  If they don't, then it may be the case that\n        # the El Torito bits of the system are 'hidden' or 'unlinked',\n        # meaning that they take up space but have no corresponding directory\n        # record in the ISO filesystem.  In order to accommodate the rest\n        # of the system, which really expects these things to have directory\n        # records, we use fake directory records that don't get written out.\n        #\n        # Note that we specifically do *not* add these to any sort of parent;\n        # that way, we don't run afoul of any checks that adding a child to a\n        # parent might have.  This means that if we do ever want to unhide this\n        # entry, we'll have to do some additional work to give it a real name\n        # and link it to the appropriate parent.\n        if self.eltorito_boot_catalog is not None:\n            self._link_eltorito(extent_to_inode)\n\n            # Now that everything has a dirrecord, see if we have a boot\n            # info table.\n            self._check_for_eltorito_boot_info_table(self.eltorito_boot_catalog.initial_entry.inode)\n            for sec in self.eltorito_boot_catalog.sections:\n                for entry in sec.section_entries:\n                    self._check_for_eltorito_boot_info_table(entry.inode)\n\n        # The PVD is finished.  Now look to see if we need to parse the SVD.\n        for svd in self.svds:\n            if (svd.flags & 0x1) == 0 and svd.escape_sequences[:3] in (b'%/@', b'%/C', b'%/E'):\n                if self.joliet_vd is not None:\n                    raise pycdlibexception.PyCdlibInvalidISO('Only a single Joliet SVD is supported')\n\n                self.joliet_vd = svd\n\n                le_ptrs, joliet_extent_to_ptr = self._parse_path_table(svd.path_table_size(),\n                                                                       svd.path_table_location_le)\n\n                tmp_be_ptrs, j_unused = self._parse_path_table(svd.path_table_size(),\n                                                               svd.path_table_location_be)\n\n                for index, ptr in enumerate(le_ptrs):\n                    if not ptr.equal_to_be(tmp_be_ptrs[index]):\n                        raise pycdlibexception.PyCdlibInvalidISO('Joliet little-endian and big-endian path table records do not agree')\n\n                self._walk_directories(svd, joliet_extent_to_ptr,\n                                       extent_to_inode, le_ptrs)\n            elif svd.version == 2 and svd.file_structure_version == 2:\n                if self.enhanced_vd is not None:\n                    raise pycdlibexception.PyCdlibInvalidISO('Only a single enhanced VD is supported')\n                self.enhanced_vd = svd\n\n        # We've seen ISOs in the wild (Office XP) that have a PVD space size\n        # that is smaller than the location of the last directory record\n        # extent + length.  If we see this, automatically update the size in the\n        # PVD (and any SVDs) so that subsequent operations will be correct.\n        log_block_size = self.pvd.logical_block_size()\n        if lastbyte > self.pvd.space_size * log_block_size:\n            new_pvd_size = utils.ceiling_div(lastbyte, log_block_size)\n            for pvd in self.pvds:\n                pvd.space_size = new_pvd_size\n            if self.joliet_vd is not None:\n                self.joliet_vd.space_size = new_pvd_size\n            if self.enhanced_vd is not None:\n                self.enhanced_vd.space_size = new_pvd_size\n\n        # Look to see if this is a UDF volume.  It is one if we have a UDF BEA,\n        # UDF NSR, and UDF TEA, in which case we parse the UDF descriptors and\n        # walk the filesystem.\n        if self._has_udf:\n            self._parse_udf_descriptors()\n            self._walk_udf_directories(extent_to_inode)\n\n        # Now we look for the 'version' volume descriptor, common on ISOs made\n        # with genisoimage or mkisofs.  This volume descriptor doesn't have any\n        # specification, but from code inspection, it is either a completely\n        # zero extent, or starts with 'MKI'.  Further, it starts directly after\n        # the VDST, or directly after the UDF recognition sequence (if this is\n        # a UDF ISO).  Thus, we go looking for it at those places, and add it\n        # if we find it there.\n        version_vd_extent = self.vdsts[0].extent_location() + 1\n        if self._has_udf:\n            version_vd_extent = self.udf_tea.extent_location() + 1\n\n        version_vd = headervd.VersionVolumeDescriptor()\n        self._cdfp.seek(version_vd_extent * log_block_size)\n        if version_vd.parse(self._cdfp.read(log_block_size), version_vd_extent):\n            self.version_vd = version_vd\n\n        self._initialized = True", "response": "An internal method to open an existing ISO for inspection and modification."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _udf_get_file_from_iso_fp(self, outfp, blocksize, udf_path):\n        # type: (BinaryIO, int, bytes) -> None\n        '''\n        An internal method to fetch a single UDF file from the ISO and write it\n        out to the file object.\n\n        Parameters:\n         outfp - The file object to write data to.\n         blocksize - The number of bytes in each transfer.\n         udf_path - The absolute UDF path to lookup on the ISO.\n        Returns:\n         Nothing.\n        '''\n        if self.udf_root is None:\n            raise pycdlibexception.PyCdlibInvalidInput('Cannot fetch a udf_path from a non-UDF ISO')\n\n        (ident_unused, found_file_entry) = self._find_udf_record(udf_path)\n        if found_file_entry is None:\n            raise pycdlibexception.PyCdlibInvalidInput('Cannot get the contents of an empty UDF File Entry')\n\n        if not found_file_entry.is_file():\n            raise pycdlibexception.PyCdlibInvalidInput('Can only write out a file')\n\n        if found_file_entry.inode is None:\n            raise pycdlibexception.PyCdlibInvalidInput('Cannot write out an entry without data')\n\n        if found_file_entry.get_data_length() > 0:\n            with inode.InodeOpenData(found_file_entry.inode, self.pvd.logical_block_size()) as (data_fp, data_len):\n                utils.copy_data(data_len, blocksize, data_fp, outfp)", "response": "An internal method to fetch a single UDF file from the ISO and write it out to the file object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_file_from_iso_fp(self, outfp, blocksize, iso_path, rr_path, joliet_path):\n        # type: (BinaryIO, int, Optional[bytes], Optional[bytes], Optional[bytes]) -> None\n        '''\n        An internal method to fetch a single file from the ISO and write it out\n        to the file object.\n\n        Parameters:\n         outfp - The file object to write data to.\n         blocksize - The number of bytes in each transfer.\n         iso_path - The absolute ISO9660 path to lookup on the ISO (exclusive\n                    with rr_path and joliet_path).\n         rr_path - The absolute Rock Ridge path to lookup on the ISO (exclusive\n                   with iso_path and joliet_path).\n         joliet_path - The absolute Joliet path to lookup on the ISO (exclusive\n                       with iso_path and rr_path).\n        Returns:\n         Nothing.\n        '''\n        if joliet_path is not None:\n            if self.joliet_vd is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot fetch a joliet_path from a non-Joliet ISO')\n            found_record = self._find_joliet_record(joliet_path)\n        elif rr_path is not None:\n            if not self.rock_ridge:\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot fetch a rr_path from a non-Rock Ridge ISO')\n            found_record = self._find_rr_record(rr_path)\n        elif iso_path is not None:\n            found_record = self._find_iso_record(iso_path)\n        else:\n            raise pycdlibexception.PyCdlibInternalError('Invalid path passed to get_file_from_iso_fp')\n\n        if found_record.is_dir():\n            raise pycdlibexception.PyCdlibInvalidInput('Cannot write out a directory')\n\n        if rr_path is not None or iso_path is not None:\n            if found_record.rock_ridge is not None and found_record.rock_ridge.is_symlink():\n                # If this Rock Ridge record is a symlink, it has no data\n                # associated with it, so it makes no sense to try and get the\n                # data.  In theory, we could follow the symlink to the\n                # appropriate place and get the data of the thing it points to.\n                # However, Rock Ridge symlinks are allowed to point *outside*\n                # of this ISO, so it is really not clear that this is something\n                # we want to do.  For now we make the user follow the symlink\n                # themselves if they want to get the data.  We can revisit this\n                # decision in the future if we need to.\n                raise pycdlibexception.PyCdlibInvalidInput('Symlinks have no data associated with them')\n\n        if found_record.inode is None:\n            raise pycdlibexception.PyCdlibInvalidInput('Cannot write out a file without data')\n\n        while found_record.get_data_length() > 0:\n            with inode.InodeOpenData(found_record.inode, self.pvd.logical_block_size()) as (data_fp, data_len):\n                # Here we copy the data into the output file descriptor.  If a boot\n                # info table is present, we overlay the table over bytes 8-64 of the\n                # file.  Note, however, that we never return more bytes than the length\n                # of the file, so the boot info table may get truncated.\n                if found_record.inode.boot_info_table is not None:\n                    header_len = min(data_len, 8)\n                    outfp.write(data_fp.read(header_len))\n                    data_len -= header_len\n                    if data_len > 0:\n                        rec = found_record.inode.boot_info_table.record()\n                        table_len = min(data_len, len(rec))\n                        outfp.write(rec[:table_len])\n                        data_len -= table_len\n                        if data_len > 0:\n                            data_fp.seek(len(rec), os.SEEK_CUR)\n                            utils.copy_data(data_len, blocksize, data_fp, outfp)\n                else:\n                    utils.copy_data(data_len, blocksize, data_fp, outfp)\n\n            if found_record.data_continuation is not None:\n                found_record = found_record.data_continuation\n            else:\n                break", "response": "An internal method to fetch a single file from the ISO and write it out to the file object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _output_file_data(self, outfp, blocksize, ino):\n        # type: (BinaryIO, int, inode.Inode) -> int\n        '''\n        Internal method to write a directory record entry out.\n\n        Parameters:\n         outfp - The file object to write the data to.\n         blocksize - The blocksize to use when writing the data out.\n         ino - The Inode to write.\n        Returns:\n         The total number of bytes written out.\n        '''\n        log_block_size = self.pvd.logical_block_size()\n\n        outfp.seek(ino.extent_location() * log_block_size)\n        tmp_start = outfp.tell()\n        with inode.InodeOpenData(ino, log_block_size) as (data_fp, data_len):\n            utils.copy_data(data_len, blocksize, data_fp, outfp)\n            utils.zero_pad(outfp, data_len, log_block_size)\n\n        if self._track_writes:\n            end = outfp.tell()\n            bisect.insort_left(self._write_check_list, self._WriteRange(tmp_start, end - 1))\n\n        # If this file is being used as a bootfile, and the user\n        # requested that the boot info table be patched into it,\n        # we patch the boot info table at offset 8 here.\n        if ino.boot_info_table is not None:\n            old = outfp.tell()\n            outfp.seek(tmp_start + 8)\n            self._outfp_write_with_check(outfp, ino.boot_info_table.record(),\n                                         enable_overwrite_check=False)\n            outfp.seek(old)\n        return outfp.tell() - tmp_start", "response": "Internal method to write a directory record entry out."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _write_fp(self, outfp, blocksize, progress_cb, progress_opaque):\n        # type: (BinaryIO, int, Optional[Callable[[int, int, Any], None]], Optional[Any]) -> None\n        '''\n        Write a properly formatted ISO out to the file object passed in.  This\n        also goes by the name of 'mastering'.\n\n        Parameters:\n         outfp - The file object to write the data to.\n         blocksize - The blocksize to use when copying data.\n         progress_cb - If not None, a function to call as the write call does its\n                       work.  The callback function must have a signature of:\n                       def func(done, total).\n         progress_opaque - User data to be passed to the progress callback.\n        Returns:\n         Nothing.\n        '''\n        if hasattr(outfp, 'mode') and 'b' not in outfp.mode:\n            raise pycdlibexception.PyCdlibInvalidInput(\"The file to write out must be in binary mode (add 'b' to the open flags)\")\n\n        if self._needs_reshuffle:\n            self._reshuffle_extents()\n\n        self._write_check_list = []\n        outfp.seek(0)\n\n        log_block_size = self.pvd.logical_block_size()\n\n        progress = self._Progress(self.pvd.space_size * log_block_size, progress_cb, progress_opaque)\n        progress.call(0)\n\n        if self.isohybrid_mbr is not None:\n            self._outfp_write_with_check(outfp,\n                                         self.isohybrid_mbr.record(self.pvd.space_size * log_block_size))\n\n        # Ecma-119, 6.2.1 says that the Volume Space is divided into a System\n        # Area and a Data Area, where the System Area is in logical sectors 0\n        # to 15, and whose contents is not specified by the standard.  Thus\n        # we skip the first 16 sectors.\n        outfp.seek(self.pvd.extent_location() * log_block_size)\n\n        # First write out the PVD.\n        for pvd in self.pvds:\n            rec = pvd.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n        # Next write out the boot records.\n        for br in self.brs:\n            outfp.seek(br.extent_location() * log_block_size)\n            rec = br.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n        # Next we write out the SVDs.\n        for svd in self.svds:\n            outfp.seek(svd.extent_location() * log_block_size)\n            rec = svd.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n        # Next we write out the Volume Descriptor Terminators.\n        for vdst in self.vdsts:\n            outfp.seek(vdst.extent_location() * log_block_size)\n            rec = vdst.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n        # Next we write out the UDF Volume Recognition sequence (if we are a\n        # UDF ISO).\n        if self._has_udf:\n            outfp.seek(self.udf_bea.extent_location() * log_block_size)\n            rec = self.udf_bea.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n            outfp.seek(self.udf_nsr.extent_location() * log_block_size)\n            rec = self.udf_nsr.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n            outfp.seek(self.udf_tea.extent_location() * log_block_size)\n            rec = self.udf_tea.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n        # Next we write out the version block if it exists.\n        if self.version_vd is not None:\n            outfp.seek(self.version_vd.extent_location() * log_block_size)\n            rec = self.version_vd.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n        if self._has_udf:\n            # Now the UDF Main and Reserved Volume Descriptor Sequence\n            self._write_udf_descs(self.udf_main_descs, outfp, progress)\n            self._write_udf_descs(self.udf_reserve_descs, outfp, progress)\n\n            # Now the UDF Logical Volume Integrity Sequence (if there is one).\n            outfp.seek(self.udf_logical_volume_integrity.extent_location() * log_block_size)\n            rec = self.udf_logical_volume_integrity.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n            outfp.seek(self.udf_logical_volume_integrity_terminator.extent_location() * log_block_size)\n            rec = self.udf_logical_volume_integrity_terminator.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n        # Now the UDF Anchor Points (if there are any).\n        for anchor in self.udf_anchors:\n            outfp.seek(anchor.extent_location() * log_block_size)\n            rec = anchor.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n        # In theory, the Path Table Records (for both the PVD and SVD) get\n        # written out next.  Since we store them along with the Directory\n        # Records, however, we will write them out along with the directory\n        # records instead.\n\n        # Now write out the El Torito Boot Catalog if it exists.\n        if self.eltorito_boot_catalog is not None:\n            outfp.seek(self.eltorito_boot_catalog.extent_location() * log_block_size)\n            rec = self.eltorito_boot_catalog.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n        # Now write out the ISO9660 directory records.\n        self._write_directory_records(self.pvd, outfp, progress)\n\n        # Now write out the Joliet directory records, if they exist.\n        if self.joliet_vd is not None:\n            self._write_directory_records(self.joliet_vd, outfp, progress)\n\n        # Now write out the UDF directory records, if they exist.\n        if self.udf_root is not None:\n            # Write out the UDF File Sets\n            outfp.seek(self.udf_file_set.extent_location() * log_block_size)\n            rec = self.udf_file_set.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n            outfp.seek(self.udf_file_set_terminator.extent_location() * log_block_size)\n            rec = self.udf_file_set_terminator.record()\n            self._outfp_write_with_check(outfp, rec)\n            progress.call(len(rec))\n\n            written_file_entry_inodes = {}  # type: Dict[int, bool]\n            udf_file_entries = collections.deque([(self.udf_root, True)])  # type: Deque[Tuple[Optional[udfmod.UDFFileEntry], bool]]\n            while udf_file_entries:\n                udf_file_entry, isdir = udf_file_entries.popleft()\n\n                if udf_file_entry is None:\n                    continue\n\n                if udf_file_entry.inode is None or not id(udf_file_entry.inode) in written_file_entry_inodes:\n                    outfp.seek(udf_file_entry.extent_location() * log_block_size)\n                    rec = udf_file_entry.record()\n                    self._outfp_write_with_check(outfp, rec)\n                    progress.call(len(rec))\n                    written_file_entry_inodes[id(udf_file_entry.inode)] = True\n\n                if isdir:\n                    outfp.seek(udf_file_entry.fi_descs[0].extent_location() * log_block_size)\n                    # FIXME: for larger directories, we'll actually need to\n                    # iterate over the alloc_descs and write them\n                    for fi_desc in udf_file_entry.fi_descs:\n                        rec = fi_desc.record()\n                        self._outfp_write_with_check(outfp, rec)\n                        progress.call(len(rec))\n                        if not fi_desc.is_parent():\n                            udf_file_entries.append((fi_desc.file_entry, fi_desc.is_dir()))\n\n        # Now we need to write out the actual files.  Note that in many cases,\n        # we haven't yet read the file out of the original, so we need to do\n        # that here.\n        for ino in self.inodes:\n            if ino.get_data_length() > 0:\n                progress.call(self._output_file_data(outfp, blocksize, ino))\n\n        # We need to pad out to the total size of the disk, in the case that\n        # the last thing we wrote is shorter than a full block size.  It turns\n        # out that not all file-like objects allow you to use truncate() to\n        # grow the file, so we do it the old-fashioned way by seeking to the\n        # end - 1 and writing a padding '\\x00' byte.\n        outfp.seek(0, os.SEEK_END)\n        total_size = self.pvd.space_size * log_block_size\n        if outfp.tell() != total_size:\n            outfp.seek(total_size - 1)\n            outfp.write(b'\\x00')\n\n        if self.isohybrid_mbr is not None:\n            outfp.seek(0, os.SEEK_END)\n            # Note that we very specifically do not call\n            # self._outfp_write_with_check here because this writes outside\n            # the PVD boundaries.\n            outfp.write(self.isohybrid_mbr.record_padding(self.pvd.space_size * log_block_size))\n\n        progress.finish()", "response": "An internal method to write out a properly formatted ISO out to the file object passed in."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_hard_link_to_rec(self, old_rec, boot_catalog_old, **kwargs):\n        # type: (Any, bool, str) -> int\n        '''\n        Add a hard link to the ISO.  Hard links are alternate names for the\n        same file contents that don't take up any additional space on the ISO.\n        This API can be used to create hard links between two files on the\n        ISO9660 filesystem, between two files on the Joliet filesystem, or\n        between a file on the ISO9660 filesystem and the Joliet filesystem.\n        In all cases, exactly one old path must be specified, and exactly one\n        new path must be specified.\n\n        Parameters:\n         old_rec - The old record to link against.\n         boot_catalog_old - Whether this is a link to an old boot catalog.\n         iso_new_path - The new path on the ISO9660 filesystem to link to.\n         joliet_new_path - The new path on the Joliet filesystem to link to.\n         rr_name - The Rock Ridge name to use for the new file if this is a\n                   Rock Ridge ISO and the new path is on the ISO9660 filesystem.\n         udf_new_path - The new path on the UDF filesystem to link to.\n        Returns:\n         The number of bytes to add to the descriptors.\n        '''\n        num_new = 0\n        iso_new_path = None\n        joliet_new_path = None\n        rr_name = b''\n        udf_new_path = None\n        new_rec = None  # type: Optional[Union[dr.DirectoryRecord, udfmod.UDFFileEntry]]\n        for key in kwargs:\n            if key == 'iso_new_path' and kwargs[key] is not None:\n                num_new += 1\n                iso_new_path = utils.normpath(kwargs[key])\n                if not self.rock_ridge:\n                    _check_path_depth(iso_new_path)\n            elif key == 'joliet_new_path' and kwargs[key] is not None:\n                num_new += 1\n                joliet_new_path = self._normalize_joliet_path(kwargs[key])\n            elif key == 'rr_name' and kwargs[key] is not None:\n                rr_name = self._check_rr_name(kwargs[key])\n            elif key == 'udf_new_path' and kwargs[key] is not None:\n                num_new += 1\n                udf_new_path = utils.normpath(kwargs[key])\n            else:\n                raise pycdlibexception.PyCdlibInvalidInput('Unknown keyword %s' % (key))\n\n        if num_new != 1:\n            raise pycdlibexception.PyCdlibInvalidInput('Exactly one new path must be specified')\n        if self.rock_ridge and iso_new_path is not None and not rr_name:\n            raise pycdlibexception.PyCdlibInvalidInput('Rock Ridge name must be supplied for a Rock Ridge new path')\n\n        data_ino = old_rec.inode\n\n        num_bytes_to_add = 0\n        if udf_new_path is None:\n            file_mode = -1\n            if iso_new_path is not None:\n                # ... to another file on the ISO9660 filesystem.\n                (new_name, new_parent) = self._iso_name_and_parent_from_path(iso_new_path)\n                vd = self.pvd\n                rr = self.rock_ridge\n                xa = self.xa\n                if self.rock_ridge:\n                    file_mode = old_rec.rock_ridge.get_file_mode()\n            elif joliet_new_path is not None:\n                if self.joliet_vd is None:\n                    raise pycdlibexception.PyCdlibInternalError('Tried to link to Joliet record on non-Joliet ISO')\n                # ... to a file on the Joliet filesystem.\n                (new_name, new_parent) = self._joliet_name_and_parent_from_path(joliet_new_path)\n                vd = self.joliet_vd\n                rr = ''\n                xa = False\n            # Above we checked to make sure we got at least one new path, so we\n            # don't need to worry about the else situation here.\n\n            new_rec = dr.DirectoryRecord()\n            new_rec.new_file(vd, old_rec.get_data_length(), new_name,\n                             new_parent, vd.sequence_number(), rr, rr_name, xa,\n                             file_mode)\n\n            num_bytes_to_add += self._add_child_to_dr(new_rec,\n                                                      vd.logical_block_size())\n        else:\n            if self.udf_root is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Can only specify a udf_path for a UDF ISO')\n\n            log_block_size = self.pvd.logical_block_size()\n\n            # UDF new path\n            (udf_name, udf_parent) = self._udf_name_and_parent_from_path(udf_new_path)\n\n            file_ident = udfmod.UDFFileIdentifierDescriptor()\n            file_ident.new(False, False, udf_name, udf_parent)\n            num_new_extents = udf_parent.add_file_ident_desc(file_ident, log_block_size)\n            num_bytes_to_add += num_new_extents * log_block_size\n\n            file_entry = udfmod.UDFFileEntry()\n            file_entry.new(old_rec.get_data_length(), 'file', udf_parent,\n                           log_block_size)\n            file_ident.file_entry = file_entry\n            file_entry.file_ident = file_ident\n            if data_ino is None or data_ino.num_udf == 0:\n                num_bytes_to_add += log_block_size\n\n            if data_ino is not None:\n                data_ino.num_udf += 1\n\n            new_rec = file_entry\n\n            self.udf_logical_volume_integrity.logical_volume_impl_use.num_files += 1\n\n        if data_ino is not None and new_rec is not None:\n            data_ino.linked_records.append(new_rec)\n            new_rec.inode = data_ino\n\n        if boot_catalog_old and new_rec is not None:\n            if self.eltorito_boot_catalog is None:\n                raise pycdlibexception.PyCdlibInternalError('Tried to link to El Torito on non-El Torito ISO')\n            self.eltorito_boot_catalog.add_dirrecord(new_rec)\n\n        return num_bytes_to_add", "response": "Adds a hard link to the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_fp(self, fp, length, manage_fp, old_iso_path, orig_rr_name, joliet_path,\n                udf_path, file_mode, eltorito_catalog):\n        # type: (Optional[BinaryIO], int, bool, str, Optional[str], Optional[str], Optional[str], Optional[int], bool) -> int\n        '''\n        An internal method to add a file to the ISO.  If the ISO contains Rock\n        Ridge, then a Rock Ridge name must be provided.  If the ISO contains\n        Joliet, then a Joliet path is not required but is highly recommended.\n        Note that the caller must ensure that the file remains open for the\n        lifetime of the ISO object, as the PyCdlib class uses the file\n        descriptor internally when writing (mastering) the ISO.\n\n        Parameters:\n         fp - The file object to use for the contents of the new file.\n         length - The length of the data for the new file.\n         manage_fp - Whether or not pycdlib should internally manage the file\n                     pointer.  It is faster to manage the file pointer\n                     externally, but it is more convenient to have pycdlib do it\n                     internally.\n         old_iso_path - The ISO9660 absolute path to the file destination on the ISO.\n         orig_rr_name - The Rock Ridge name of the file destination on the ISO.\n         joliet_path - The Joliet absolute path to the file destination on the ISO.\n         udf_path - The UDF absolute path to the file destination on the ISO.\n         file_mode - The POSIX file_mode to apply to this file.  This only\n                     applies if this is a Rock Ridge ISO.  If this is None (the\n                     default), the permissions from the original file are used.\n         eltorito_catalog - Whether this entry represents an El Torito Boot\n                            Catalog.\n        Returns:\n         The number of bytes to add to the descriptors.\n        '''\n\n        iso_path = utils.normpath(old_iso_path)\n\n        rr_name = self._check_rr_name(orig_rr_name)\n\n        # We call _normalize_joliet_path here even though we aren't going to\n        # use the result.  This is to ensure that we throw an exception when\n        # a joliet_path is passed for a non-Joliet ISO.\n        if joliet_path:\n            self._normalize_joliet_path(joliet_path)\n\n        if udf_path and self.udf_root is None:\n            raise pycdlibexception.PyCdlibInvalidInput('Can only specify a UDF path for a UDF ISO')\n\n        if not self.rock_ridge:\n            _check_path_depth(iso_path)\n        (name, parent) = self._iso_name_and_parent_from_path(iso_path)\n\n        _check_iso9660_filename(name, self.interchange_level)\n\n        fmode = 0\n        if file_mode is not None:\n            if not self.rock_ridge:\n                raise pycdlibexception.PyCdlibInvalidInput('Can only specify a file mode for Rock Ridge ISOs')\n            fmode = file_mode\n        else:\n            if self.rock_ridge:\n                if fp is not None:\n                    # Python 3 implements the fileno method for all file-like objects, so\n                    # we can't just use the existence of the method to tell whether it is\n                    # available.  Instead, we try to assign it, and if we fail, then we\n                    # assume it is not available.\n                    try:\n                        fileno = fp.fileno()\n                        fmode = os.fstat(fileno).st_mode\n                    except (AttributeError, io.UnsupportedOperation):\n                        # We couldn't get the actual file mode of the file, so just assume\n                        # a conservative 444\n                        fmode = 0o0100444\n                else:\n                    fmode = 0o0100444\n\n        left = length\n        offset = 0\n        done = False\n        num_bytes_to_add = 0\n        first_rec = None\n        while not done:\n            # The maximum length we allow in one directory record is 0xfffff800\n            # (this is taken from xorriso, though I don't really know why).\n            thislen = min(left, 0xfffff800)\n\n            rec = dr.DirectoryRecord()\n            rec.new_file(self.pvd, thislen, name, parent,\n                         self.pvd.sequence_number(), self.rock_ridge, rr_name,\n                         self.xa, fmode)\n            num_bytes_to_add += self._add_child_to_dr(rec,\n                                                      self.pvd.logical_block_size())\n            # El Torito Boot Catalogs have no inode, so only add it if this is\n            # not a boot catalog.\n            if eltorito_catalog:\n                if self.eltorito_boot_catalog is None:\n                    raise pycdlibexception.PyCdlibInternalError('Tried to add to a non-existent boot catalog')\n                if offset == 0:\n                    self.eltorito_boot_catalog.add_dirrecord(rec)\n            else:\n                # Zero-length files get a directory record but no Inode (there\n                # is nothing to write out).\n                if fp is not None:\n                    ino = inode.Inode()\n                    ino.new(thislen, fp, manage_fp, offset)\n                    ino.linked_records.append(rec)\n                    rec.inode = ino\n                    self.inodes.append(ino)\n\n            num_bytes_to_add += thislen\n            if first_rec is None:\n                first_rec = rec\n            left -= thislen\n            offset += thislen\n            if left == 0:\n                done = True\n\n            num_bytes_to_add += self._update_rr_ce_entry(rec)\n\n        if self.joliet_vd is not None and joliet_path:\n            # If this is a Joliet ISO, then we can re-use add_hard_link to do\n            # most of the work.\n            num_bytes_to_add += self._add_hard_link_to_rec(first_rec, eltorito_catalog,\n                                                           joliet_new_path=joliet_path)\n\n        if udf_path:\n            num_bytes_to_add += self._add_hard_link_to_rec(first_rec, eltorito_catalog,\n                                                           udf_new_path=udf_path)\n\n        return num_bytes_to_add", "response": "Internal method to add a file to the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _rm_udf_file_ident(self, parent, fi):\n        # type: (udfmod.UDFFileEntry, bytes) -> int\n        '''\n        An internal method to remove a UDF File Identifier from the parent\n        and remove any space from the Logical Volume as necessary.\n\n        Parameters:\n         parent - The parent entry to remove the UDF File Identifier from.\n         fi - The file identifier to remove.\n        Returns:\n         The number of bytes to remove from the ISO.\n        '''\n        logical_block_size = self.pvd.logical_block_size()\n        num_extents_to_remove = parent.remove_file_ident_desc_by_name(fi,\n                                                                      logical_block_size)\n        self.udf_logical_volume_integrity.logical_volume_impl_use.num_files -= 1\n\n        self._find_udf_record.cache_clear()  # pylint: disable=no-member\n\n        return num_extents_to_remove * logical_block_size", "response": "Internal method to remove a UDF File Identifier from the parent entry and remove any space from the Logical Volume as necessary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_joliet_dir(self, joliet_path):\n        # type: (bytes) -> int\n        '''\n        An internal method to add a joliet directory to the ISO.\n\n        Parameters:\n         joliet_path - The path to add to the Joliet portion of the ISO.\n        Returns:\n         The number of additional bytes needed on the ISO to fit this directory.\n        '''\n\n        if self.joliet_vd is None:\n            raise pycdlibexception.PyCdlibInternalError('Tried to add joliet dir to non-Joliet ISO')\n\n        (joliet_name, joliet_parent) = self._joliet_name_and_parent_from_path(joliet_path)\n\n        log_block_size = self.joliet_vd.logical_block_size()\n\n        rec = dr.DirectoryRecord()\n        rec.new_dir(self.joliet_vd, joliet_name, joliet_parent,\n                    self.joliet_vd.sequence_number(), '', b'',\n                    log_block_size, False, False,\n                    False, -1)\n        num_bytes_to_add = self._add_child_to_dr(rec, log_block_size)\n\n        self._create_dot(self.joliet_vd, rec, '', False, -1)\n        self._create_dotdot(self.joliet_vd, rec, '', False, False, -1)\n\n        num_bytes_to_add += log_block_size\n        if self.joliet_vd.add_to_ptr_size(path_table_record.PathTableRecord.record_length(len(joliet_name))):\n            num_bytes_to_add += 4 * log_block_size\n\n        # We always need to add an entry to the path table record\n        ptr = path_table_record.PathTableRecord()\n        ptr.new_dir(joliet_name)\n        rec.set_ptr(ptr)\n\n        return num_bytes_to_add", "response": "An internal method to add a joliet directory to the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_dotdot(self, vd, parent, rock_ridge, relocated, xa, file_mode):\n        # type: (headervd.PrimaryOrSupplementaryVD, dr.DirectoryRecord, str, bool, bool, int) -> dr.DirectoryRecord\n        '''\n        An internal method to create a new 'dotdot' Directory Record.\n\n        Parameters:\n         vd - The volume descriptor to attach the 'dotdot' Directory Record to.\n         parent - The parent Directory Record for new Directory Record.\n         rock_ridge - The Rock Ridge version to use for this entry (if any).\n         relocated - Whether this Directory Record is a Rock Ridge relocated entry.\n         xa - Whether this Directory Record should have extended attributes.\n         file_mode - The mode to assign to the dot directory (only applies to Rock Ridge).\n        Returns:\n         Nothing.\n        '''\n        dotdot = dr.DirectoryRecord()\n        dotdot.new_dotdot(vd, parent, vd.sequence_number(), rock_ridge,\n                          vd.logical_block_size(), relocated, xa, file_mode)\n        self._add_child_to_dr(dotdot, vd.logical_block_size())\n        return dotdot", "response": "Internal method to create a new dotdot Directory Record."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new ISO from scratch.", "response": "def new(self, interchange_level=1, sys_ident='', vol_ident='', set_size=1,\n            seqnum=1, log_block_size=2048, vol_set_ident=' ', pub_ident_str='',\n            preparer_ident_str='', app_ident_str='', copyright_file='',\n            abstract_file='', bibli_file='', vol_expire_date=None, app_use='',\n            joliet=None, rock_ridge=None, xa=False, udf=None):\n        # type: (int, str, str, int, int, int, str, str, str, str, str, str, str, Optional[float], str, Optional[int], Optional[str], bool, Optional[str]) -> None\n        '''\n        Create a new ISO from scratch.\n\n        Parameters:\n         interchange_level - The ISO9660 interchange level to use; this dictates\n                             the rules on the names of files.  Levels 1, 2, 3,\n                             and 4 are supported.  Level 1 is the most\n                             conservative, and is the default, but level 3 is\n                             recommended.\n         sys_ident - The system identification string to use on the new ISO.\n         vol_ident - The volume identification string to use on the new ISO.\n         set_size - The size of the set of ISOs this ISO is a part of.\n         seqnum - The sequence number of the set of this ISO.\n         log_block_size - The logical block size to use for the ISO.  While ISO9660\n                          technically supports sizes other than 2048 (the default),\n                          this almost certainly doesn't work.\n         vol_set_ident - The volume set identification string to use on the new ISO.\n         pub_ident_str - The publisher identification string to use on the new ISO.\n         preparer_ident_str - The preparer identification string to use on the new ISO.\n         app_ident_str - The application identification string to use on the new ISO.\n         copyright_file - The name of a file at the root of the ISO to use as the\n                          copyright file.\n         abstract_file - The name of a file at the root of the ISO to use as the\n                         abstract file.\n         bibli_file - The name of a file at the root of the ISO to use as the\n                      bibliographic file.\n         vol_expire_date - The date that this ISO will expire at.\n         app_use - Arbitrary data that the application can stuff into the primary\n                   volume descriptor of this ISO.\n         joliet - A integer that can have the value 1, 2, or 3 for Joliet\n                  levels 1, 2, or 3 (3 is by far the most common), or None for\n                  no Joliet support (the default).  For legacy reasons, this\n                  parameter also accepts a boolean, where the value of 'False'\n                  means no Joliet and a value of 'True' means level 3.\n         rock_ridge - Whether to make this ISO have the Rock Ridge extensions or\n                      not.  The default value of None does not add Rock Ridge\n                      extensions.  A string value of '1.09', '1.10', or '1.12'\n                      adds the specified Rock Ridge version to the ISO.  If\n                      unsure, pass '1.09' to ensure maximum compatibility.\n         xa - Whether to add the ISO9660 Extended Attribute extensions to this\n              ISO.  The default is False.\n         udf - Whether to add UDF support to this ISO.  If it is None (the\n               default), no UDF support is added.  If it is \"2.60\", version 2.60\n               of the UDF spec is used.  All other values are disallowed.\n        Returns:\n         Nothing.\n        '''\n        # Start out with argument checking.\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object already has an ISO; either close it or create a new object')\n\n        if interchange_level < 1 or interchange_level > 4:\n            raise pycdlibexception.PyCdlibInvalidInput('Invalid interchange level (must be between 1 and 4)')\n\n        if rock_ridge and rock_ridge not in ['1.09', '1.10', '1.12']:\n            raise pycdlibexception.PyCdlibInvalidInput('Rock Ridge value must be None (no Rock Ridge), 1.09, 1.10, or 1.12')\n\n        if udf and udf != '2.60':\n            raise pycdlibexception.PyCdlibInvalidInput('UDF value must be empty (no UDF), or 2.60')\n\n        # Now save off the arguments we need to keep around.\n        if not app_ident_str:\n            app_ident_str = 'PyCdlib (C) 2015-2018 Chris Lalancette'\n\n        self.interchange_level = interchange_level\n\n        self.xa = xa\n\n        if isinstance(joliet, bool):\n            if joliet:\n                joliet = 3\n            else:\n                joliet = None\n\n        if rock_ridge:\n            self.rock_ridge = rock_ridge\n\n        sys_ident_bytes = sys_ident.encode('utf-8')\n        vol_ident_bytes = vol_ident.encode('utf-8')\n        vol_set_ident_bytes = vol_set_ident.encode('utf-8')\n        pub_ident_bytes = pub_ident_str.encode('utf-8')\n        preparer_ident_bytes = preparer_ident_str.encode('utf-8')\n        app_ident_bytes = app_ident_str.encode('utf-8')\n        copyright_file_bytes = copyright_file.encode('utf-8')\n        abstract_file_bytes = abstract_file.encode('utf-8')\n        bibli_file_bytes = bibli_file.encode('utf-8')\n        app_use_bytes = app_use.encode('utf-8')\n\n        if vol_expire_date is None:\n            real_vol_expire_date = 0.0\n        else:\n            real_vol_expire_date = vol_expire_date\n\n        # Now start creating the ISO.\n        self.pvd = headervd.pvd_factory(sys_ident_bytes, vol_ident_bytes,\n                                        set_size, seqnum, log_block_size,\n                                        vol_set_ident_bytes, pub_ident_bytes,\n                                        preparer_ident_bytes, app_ident_bytes,\n                                        copyright_file_bytes,\n                                        abstract_file_bytes, bibli_file_bytes,\n                                        real_vol_expire_date, app_use_bytes, xa)\n        self.pvds.append(self.pvd)\n\n        pvd_log_block_size = self.pvd.logical_block_size()\n\n        num_bytes_to_add = 0\n        if self.interchange_level == 4:\n            self.enhanced_vd = headervd.enhanced_vd_factory(sys_ident_bytes,\n                                                            vol_ident_bytes,\n                                                            set_size, seqnum,\n                                                            log_block_size,\n                                                            vol_set_ident_bytes,\n                                                            pub_ident_bytes,\n                                                            preparer_ident_bytes,\n                                                            app_ident_bytes,\n                                                            copyright_file_bytes,\n                                                            abstract_file_bytes,\n                                                            bibli_file_bytes,\n                                                            real_vol_expire_date,\n                                                            app_use_bytes, xa)\n            self.svds.append(self.enhanced_vd)\n\n            num_bytes_to_add += self.enhanced_vd.logical_block_size()\n\n        if joliet is not None:\n            self.joliet_vd = headervd.joliet_vd_factory(joliet, sys_ident_bytes,\n                                                        vol_ident_bytes, set_size,\n                                                        seqnum, log_block_size,\n                                                        vol_set_ident_bytes,\n                                                        pub_ident_bytes,\n                                                        preparer_ident_bytes,\n                                                        app_ident_bytes,\n                                                        copyright_file_bytes,\n                                                        abstract_file_bytes,\n                                                        bibli_file_bytes,\n                                                        real_vol_expire_date,\n                                                        app_use_bytes, xa)\n            self.svds.append(self.joliet_vd)\n\n            # Now that we have added joliet, we need to add the new space to the\n            # PVD for the VD itself.\n            num_bytes_to_add += self.joliet_vd.logical_block_size()\n\n        self.vdsts.append(headervd.vdst_factory())\n        num_bytes_to_add += pvd_log_block_size\n\n        if udf:\n            self._has_udf = True\n            # Create the Bridge Recognition Volume Sequence\n            self.udf_bea.new()\n            self.udf_nsr.new(2)\n            self.udf_tea.new()\n\n            num_bytes_to_add += 3 * pvd_log_block_size\n\n        # We always create an empty version volume descriptor\n        self.version_vd = headervd.version_vd_factory(pvd_log_block_size)\n        num_bytes_to_add += pvd_log_block_size\n\n        if udf:\n            # We need to pad out to extent 32.  The padding should be the\n            # distance between the current PVD space size and 32.\n            additional_extents = 32 - (self.pvd.space_size + (num_bytes_to_add // pvd_log_block_size))\n            num_bytes_to_add += additional_extents * pvd_log_block_size\n\n            # Create the Main Volume Descriptor Sequence\n            self.udf_main_descs.pvd.new()\n\n            self.udf_main_descs.impl_use.new()\n\n            self.udf_main_descs.partition.new()\n\n            self.udf_main_descs.logical_volume.new()\n\n            self.udf_main_descs.unallocated_space.new()\n\n            self.udf_main_descs.terminator.new()\n\n            num_bytes_to_add += 16 * pvd_log_block_size\n\n            # Create the Reserve Volume Descriptor Sequence\n            self.udf_reserve_descs.pvd.new()\n\n            self.udf_reserve_descs.impl_use.new()\n\n            self.udf_reserve_descs.partition.new()\n\n            self.udf_reserve_descs.logical_volume.new()\n\n            self.udf_reserve_descs.unallocated_space.new()\n\n            self.udf_reserve_descs.terminator.new()\n\n            num_bytes_to_add += 16 * pvd_log_block_size\n\n            # Create the Logical Volume Integrity Sequence\n            self.udf_logical_volume_integrity.new()\n\n            self.udf_logical_volume_integrity_terminator.new()\n\n            num_bytes_to_add += 192 * pvd_log_block_size\n\n            # Create the Anchor\n            anchor1 = udfmod.UDFAnchorVolumeStructure()\n            anchor1.new()\n            self.udf_anchors.append(anchor1)\n\n            num_bytes_to_add += pvd_log_block_size\n\n            # Create the File Set\n            self.udf_file_set.new()\n\n            self.udf_file_set_terminator.new()\n\n            num_bytes_to_add += 2 * pvd_log_block_size\n\n            # Create the root directory, and the 'parent' entry inside.\n            self.udf_root = udfmod.UDFFileEntry()\n            self.udf_root.new(0, 'dir', None, pvd_log_block_size)\n            num_bytes_to_add += pvd_log_block_size\n\n            parent = udfmod.UDFFileIdentifierDescriptor()\n            parent.new(True, True, b'', None)\n            num_new_extents = self.udf_root.add_file_ident_desc(parent, pvd_log_block_size)\n            num_bytes_to_add += num_new_extents * pvd_log_block_size\n\n        num_partition_bytes_to_add = 0\n        # Create the PTR, and add the 4 extents that comprise of the LE PTR and\n        # BE PTR to the number of bytes to add.\n        ptr = path_table_record.PathTableRecord()\n        ptr.new_root()\n        self.pvd.root_directory_record().set_ptr(ptr)\n        num_partition_bytes_to_add += 4 * pvd_log_block_size\n\n        # Also add one extent to the size for the root directory record.\n        num_partition_bytes_to_add += pvd_log_block_size\n\n        self._create_dot(self.pvd, self.pvd.root_directory_record(),\n                         self.rock_ridge, self.xa, 0o040555)\n        self._create_dotdot(self.pvd, self.pvd.root_directory_record(),\n                            self.rock_ridge, False, self.xa, 0o040555)\n\n        if self.joliet_vd is not None:\n            # Create the PTR, and add the 4 extents that comprise of the LE PTR and\n            # BE PTR to the number of bytes to add.\n            ptr = path_table_record.PathTableRecord()\n            ptr.new_root()\n            self.joliet_vd.root_directory_record().set_ptr(ptr)\n            num_partition_bytes_to_add += 4 * pvd_log_block_size\n\n            # Also add one extent to the size for the root directory record.\n            num_partition_bytes_to_add += pvd_log_block_size\n\n            self._create_dot(self.joliet_vd,\n                             self.joliet_vd.root_directory_record(), '',\n                             False, -1)\n            self._create_dotdot(self.joliet_vd,\n                                self.joliet_vd.root_directory_record(), '',\n                                False, False, -1)\n\n        if self.rock_ridge:\n            num_partition_bytes_to_add += pvd_log_block_size\n\n        if udf:\n            anchor2 = udfmod.UDFAnchorVolumeStructure()\n            anchor2.new()\n            self.udf_anchors.append(anchor2)\n\n            num_partition_bytes_to_add += pvd_log_block_size\n\n        self._finish_add(num_bytes_to_add, num_partition_bytes_to_add)\n\n        self._initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open(self, filename):\n        # type: (str) -> None\n        '''\n        Open up an existing ISO for inspection and modification.\n\n        Parameters:\n         filename - The filename containing the ISO to open up.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object already has an ISO; either close it or create a new object')\n\n        fp = open(filename, 'r+b')\n        self._managing_fp = True\n        try:\n            self._open_fp(fp)\n        except Exception:\n            fp.close()\n            raise", "response": "Open up an existing ISO for inspection and modification."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open_fp(self, fp):\n        # type: (BinaryIO) -> None\n        '''\n        Open up an existing ISO for inspection and modification.  Note that the\n        file object passed in here must stay open for the lifetime of this\n        object, as the PyCdlib class uses it internally to do writing and reading\n        operations.  If you want PyCdlib to manage this for you, use 'open'\n        instead.\n\n        Parameters:\n         fp - The file object containing the ISO to open up.\n        Returns:\n         Nothing.\n        '''\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object already has an ISO; either close it or create a new object')\n\n        self._open_fp(fp)", "response": "Open up an existing ISO for inspection and modification."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_and_write(self, iso_path, local_path, blocksize=8192):\n        # type: (str, str, int) -> None\n        '''\n        (deprecated) Fetch a single file from the ISO and write it out to the\n        specified file.  Note that this will overwrite the contents of the local\n        file if it already exists.  Also note that 'iso_path' must be an\n        absolute path to the file.  Finally, the 'iso_path' can be an ISO9660\n        path, a Rock Ridge path, or a Joliet path.  In the case of ambiguity,\n        the Joliet path is tried first, followed by the ISO9660 path, followed\n        by the Rock Ridge path.  It is recommended to use the get_file_from_iso\n        API instead to resolve this ambiguity.\n\n        Parameters:\n         iso_path - The absolute path to the file to get data from.\n         local_path - The local filename to write the contents to.\n         blocksize - The blocksize to use when copying data; the default is 8192.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        with open(local_path, 'wb') as fp:\n            self._get_and_write_fp(utils.normpath(iso_path), fp, blocksize)", "response": "A method to read a single file from the ISO and write it out to the local file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, filename, blocksize=32768, progress_cb=None, progress_opaque=None):\n        # type: (str, int, Optional[Callable[[int, int, Any], None]], Optional[Any]) -> None\n        '''\n        Write a properly formatted ISO out to the filename passed in.  This\n        also goes by the name of 'mastering'.\n\n        Parameters:\n         filename - The filename to write the data to.\n         blocksize - The blocksize to use when copying data; set to 32768 by default.\n         progress_cb - If not None, a function to call as the write call does its\n                       work.  The callback function must have a signature of:\n                       def func(done, total, opaque).\n         progress_opaque - User data to be passed to the progress callback.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        with open(filename, 'wb') as fp:\n            self._write_fp(fp, blocksize, progress_cb, progress_opaque)", "response": "Write a properly formatted ISO out to the filename passed in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a properly formatted ISO out to the file object passed in. This also goes by the name of 'mastering'. Parameters: outfp - The file object to write the data to. blocksize - The blocksize to use when copying data; set to 32768 by default. progress_cb - If not None, a function to call as the write call does its work. The callback function must have a signature of: def func(done, total, opaque). progress_opaque - User data to be passed to the progress callback. Returns: Nothing.", "response": "def write_fp(self, outfp, blocksize=32768, progress_cb=None, progress_opaque=None):\n        # type: (BinaryIO, int, Optional[Callable[[int, int, Any], None]], Optional[Any]) -> None\n        '''\n        Write a properly formatted ISO out to the file object passed in.  This\n        also goes by the name of 'mastering'.\n\n        Parameters:\n         outfp - The file object to write the data to.\n         blocksize - The blocksize to use when copying data; set to 32768 by default.\n         progress_cb - If not None, a function to call as the write call does its\n                       work.  The callback function must have a signature of:\n                       def func(done, total, opaque).\n         progress_opaque - User data to be passed to the progress callback.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        self._write_fp(outfp, blocksize, progress_cb, progress_opaque)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a file to the ISO. If the ISO is a Rock Ridge one, then a Rock Ridge name must also be provided. If the ISO is a Joliet one, then a Joliet path may also be provided; while it is optional to do so, it is highly recommended. Note that the caller must ensure that 'fp' remains open for the lifetime of the PyCdlib object, as the PyCdlib class uses the file descriptor internally when writing (mastering) the ISO. If you want PyCdlib to manage this for you, use 'add_file' instead. Parameters: fp - The file object to use for the contents of the new file. length - The length of the data for the new file. iso_path - The ISO9660 absolute path to the file destination on the ISO. rr_name - The Rock Ridge name of the file destination on the ISO. joliet_path - The Joliet absolute path to the file destination on the ISO. file_mode - The POSIX file_mode to apply to this file. This only applies if this is a Rock Ridge ISO. If this is None (the default), the permissions from the original file are used. udf_path - The UDF name of the file destination on the ISO. Returns: Nothing.", "response": "def add_fp(self, fp, length, iso_path, rr_name=None, joliet_path=None,\n               file_mode=None, udf_path=None):\n        # type: (BinaryIO, int, str, Optional[str], Optional[str], Optional[int], Optional[str]) -> None\n        '''\n        Add a file to the ISO.  If the ISO is a Rock Ridge one, then a Rock\n        Ridge name must also be provided.  If the ISO is a Joliet one, then a\n        Joliet path may also be provided; while it is optional to do so, it is\n        highly recommended.  Note that the caller must ensure that 'fp' remains\n        open for the lifetime of the PyCdlib object, as the PyCdlib class uses\n        the file descriptor internally when writing (mastering) the ISO.  If\n        you want PyCdlib to manage this for you, use 'add_file' instead.\n\n        Parameters:\n         fp - The file object to use for the contents of the new file.\n         length - The length of the data for the new file.\n         iso_path - The ISO9660 absolute path to the file destination on the ISO.\n         rr_name - The Rock Ridge name of the file destination on the ISO.\n         joliet_path - The Joliet absolute path to the file destination on the ISO.\n         file_mode - The POSIX file_mode to apply to this file.  This only\n                     applies if this is a Rock Ridge ISO.  If this is None (the\n                     default), the permissions from the original file are used.\n         udf_path - The UDF name of the file destination on the ISO.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if not utils.file_object_supports_binary(fp):\n            raise pycdlibexception.PyCdlibInvalidInput('The fp argument must be in binary mode')\n\n        num_bytes_to_add = self._add_fp(fp, length, False, iso_path, rr_name,\n                                        joliet_path, udf_path, file_mode, False)\n\n        self._finish_add(0, num_bytes_to_add)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a file to the ISO. If the ISO is a Rock Ridge one, then a Rock Ridge name must also be provided. If the ISO is a Joliet one, then a Joliet path may also be provided; while it is optional to do so, it is highly recommended. Parameters: filename - The filename to use for the data contents for the new file. iso_path - The ISO9660 absolute path to the file destination on the ISO. rr_name - The Rock Ridge name of the file destination on the ISO. joliet_path - The Joliet absolute path to the file destination on the ISO. file_mode - The POSIX file_mode to apply to this file. This only applies if this is a Rock Ridge ISO. If this is None (the default), the permissions from the original file are used. udf_path - The UDF name of the file destination on the ISO. Returns: Nothing.", "response": "def add_file(self, filename, iso_path, rr_name=None, joliet_path=None,\n                 file_mode=None, udf_path=None):\n        # type: (Any, str, Optional[str], str, Optional[int], Optional[str]) -> None\n        '''\n        Add a file to the ISO.  If the ISO is a Rock Ridge one, then a Rock\n        Ridge name must also be provided.  If the ISO is a Joliet one, then a\n        Joliet path may also be provided; while it is optional to do so, it is\n        highly recommended.\n\n        Parameters:\n         filename - The filename to use for the data contents for the new file.\n         iso_path - The ISO9660 absolute path to the file destination on the ISO.\n         rr_name - The Rock Ridge name of the file destination on the ISO.\n         joliet_path - The Joliet absolute path to the file destination on the ISO.\n         file_mode - The POSIX file_mode to apply to this file.  This only\n                     applies if this is a Rock Ridge ISO.  If this is None (the\n                     default), the permissions from the original file are used.\n         udf_path - The UDF name of the file destination on the ISO.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        num_bytes_to_add = self._add_fp(filename, os.stat(filename).st_size,\n                                        True, iso_path, rr_name, joliet_path,\n                                        udf_path, file_mode, False)\n\n        self._finish_add(0, num_bytes_to_add)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef modify_file_in_place(self, fp, length, iso_path, rr_name=None,  # pylint: disable=unused-argument\n                             joliet_path=None, udf_path=None):          # pylint: disable=unused-argument\n        # type: (BinaryIO, int, str, Optional[str], Optional[str], Optional[str]) -> None\n        '''\n        An API to modify a file in place on the ISO.  This can be extremely fast\n        (much faster than calling the write method), but has many restrictions.\n\n        1.  The original ISO file pointer must have been opened for reading\n            and writing.\n        2.  Only an existing *file* can be modified; directories cannot be\n            changed.\n        3.  Only an existing file can be *modified*; no new files can be added\n            or removed.\n        4.  The new file contents must use the same number of extents (typically\n            2048 bytes) as the old file contents.  If using this API to shrink\n            a file, this is usually easy since the new contents can be padded\n            out with zeros or newlines to meet the requirement.  If using this\n            API to grow a file, the new contents can only grow up to the next\n            extent boundary.\n\n        Unlike all other APIs in PyCdlib, this API actually modifies the\n        originally opened on-disk file, so use it with caution.\n\n        Parameters:\n         fp - The file object to use for the contents of the new file.\n         length - The length of the new data for the file.\n         iso_path - The ISO9660 absolute path to the file destination on the ISO.\n         rr_name - The Rock Ridge name of the file destination on the ISO.\n         joliet_path - The Joliet absolute path to the file destination on the ISO.\n         udf_path - The UDF absolute path to the file destination on the ISO.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if hasattr(self._cdfp, 'mode') and not self._cdfp.mode.startswith(('r+', 'w', 'a', 'rb+')):\n            raise pycdlibexception.PyCdlibInvalidInput('To modify a file in place, the original ISO must have been opened in a write mode (r+, w, or a)')\n\n        log_block_size = self.pvd.logical_block_size()\n\n        child = self._find_iso_record(utils.normpath(iso_path))\n\n        old_num_extents = utils.ceiling_div(child.get_data_length(), log_block_size)\n        new_num_extents = utils.ceiling_div(length, log_block_size)\n\n        if old_num_extents != new_num_extents:\n            raise pycdlibexception.PyCdlibInvalidInput('When modifying a file in-place, the number of extents for a file cannot change!')\n\n        if not child.is_file():\n            raise pycdlibexception.PyCdlibInvalidInput('Cannot modify a directory with modify_file_in_place')\n\n        if child.inode is None:\n            raise pycdlibexception.PyCdlibInternalError('Child file found without inode')\n\n        child.inode.update_fp(fp, length)\n\n        # Remove the old size from the PVD size\n        for pvd in self.pvds:\n            pvd.remove_from_space_size(child.get_data_length())\n        # And add the new size to the PVD size\n        for pvd in self.pvds:\n            pvd.add_to_space_size(length)\n\n        if self.enhanced_vd is not None:\n            self.enhanced_vd.copy_sizes(self.pvd)\n\n        # If we made it here, we have successfully updated all of the in-memory\n        # metadata.  Now we can go and modify the on-disk file.\n\n        self._cdfp.seek(self.pvd.extent_location() * log_block_size)\n\n        # First write out the PVD.\n        rec = self.pvd.record()\n        self._cdfp.write(rec)\n\n        # Write out the joliet VD\n        if self.joliet_vd is not None:\n            self._cdfp.seek(self.joliet_vd.extent_location() * log_block_size)\n            rec = self.joliet_vd.record()\n            self._cdfp.write(rec)\n\n        # Write out the enhanced VD\n        if self.enhanced_vd is not None:\n            self._cdfp.seek(self.enhanced_vd.extent_location() * log_block_size)\n            rec = self.enhanced_vd.record()\n            self._cdfp.write(rec)\n\n        # We don't have to write anything out for UDF since it only tracks\n        # extents, and we know we aren't changing the number of extents.\n\n        # Write out the actual file contents\n        self._cdfp.seek(child.extent_location() * log_block_size)\n        with inode.InodeOpenData(child.inode, log_block_size) as (data_fp, data_len):\n            utils.copy_data(data_len, log_block_size, data_fp, self._cdfp)\n            utils.zero_pad(self._cdfp, data_len, log_block_size)\n\n        # Finally write out the directory record entry.\n        # This is a little tricky because of what things mean.  First of all,\n        # child.extents_to_here represents the total number of extents up to\n        # this child in the parent.  Thus, to get the absolute extent offset,\n        # we start with the parent's extent location, add on the number of\n        # extents to here, and remove 1 (since our offset will be zero-based).\n        # Second, child.offset_to_here is the *last* byte that the child uses,\n        # so to get the start of it we subtract off the length of the child.\n        # Then we can multiple the extent location by the logical block size,\n        # add on the offset, and get to the absolute location in the file.\n        first_joliet = True\n        for record in child.inode.linked_records:\n            if isinstance(record, dr.DirectoryRecord):\n                if self.joliet_vd is not None and id(record.vd) == id(self.joliet_vd) and first_joliet:\n                    first_joliet = False\n                    self.joliet_vd.remove_from_space_size(record.get_data_length())\n                    self.joliet_vd.add_to_space_size(length)\n                if record.parent is None:\n                    raise pycdlibexception.PyCdlibInternalError('Modifying file with empty parent')\n                abs_extent_loc = record.parent.extent_location() + record.extents_to_here - 1\n                offset = record.offset_to_here - record.dr_len\n                abs_offset = abs_extent_loc * log_block_size + offset\n            elif isinstance(record, udfmod.UDFFileEntry):\n                abs_offset = record.extent_location() * log_block_size\n\n            record.set_data_length(length)\n            self._cdfp.seek(abs_offset)\n            self._cdfp.write(record.record())", "response": "A method to modify a file in place on the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_hard_link(self, **kwargs):\n        # type: (Any) -> None\n        '''\n        Add a hard link to the ISO.  Hard links are alternate names for the\n        same file contents that don't take up any additional space on the the\n        ISO.  This API can be used to create hard links between two files on\n        the ISO9660 filesystem, between two files on the Joliet filesystem, or\n        between a file on the ISO9660 filesystem and the Joliet filesystem.\n        In all cases, exactly one old path must be specified, and exactly one\n        new path must be specified.\n        Note that this is an advanced API, so using it in combination with the\n        higher-level APIs (like rm_file) may result in unexpected behavior.\n        Once this API has been used, this API and rm_hard_link() should be\n        preferred over add_file() and rm_file(), respectively.\n\n        Parameters:\n         iso_old_path - The old path on the ISO9660 filesystem to link from.\n         iso_new_path - The new path on the ISO9660 filesystem to link to.\n         joliet_old_path - The old path on the Joliet filesystem to link from.\n         joliet_new_path - The new path on the Joliet filesystem to link to.\n         rr_name - The Rock Ridge name to use for the new file if this is a Rock\n                   Ridge ISO and the new path is on the ISO9660 filesystem.\n         boot_catalog_old - Use the El Torito boot catalog as the old path.\n         udf_old_path - The old path on the UDF filesystem to link from.\n         udf_new_path - The new path on the UDF filesystem to link to.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        num_old = 0\n        iso_old_path = None\n        joliet_old_path = None\n        boot_catalog_old = False\n        udf_old_path = None\n        keys_to_remove = []\n        for key in kwargs:\n            if key == 'iso_old_path' and kwargs[key] is not None:\n                num_old += 1\n                iso_old_path = utils.normpath(kwargs[key])\n                keys_to_remove.append(key)\n            elif key == 'joliet_old_path' and kwargs[key] is not None:\n                num_old += 1\n                joliet_old_path = self._normalize_joliet_path(kwargs[key])\n                keys_to_remove.append(key)\n            elif key == 'boot_catalog_old' and kwargs[key] is not None:\n                num_old += 1\n                boot_catalog_old = True\n                if self.eltorito_boot_catalog is None:\n                    raise pycdlibexception.PyCdlibInvalidInput('Attempting to make link to non-existent El Torito boot catalog')\n                keys_to_remove.append(key)\n            elif key == 'udf_old_path' and kwargs[key] is not None:\n                num_old += 1\n                udf_old_path = utils.normpath(kwargs[key])\n                keys_to_remove.append(key)\n\n        if num_old != 1:\n            raise pycdlibexception.PyCdlibInvalidInput('Exactly one old path must be specified')\n\n        # Once we've iterated over the keys we know about, remove them from\n        # the map so that _add_hard_link_to_rec() can parse the rest.\n        for key in keys_to_remove:\n            del kwargs[key]\n\n        # It would be nice to allow the addition of a link to the El Torito\n        # Initial/Default Entry.  Unfortunately, the information we need for\n        # a 'hidden' Initial entry just doesn't exist on the ISO.  In\n        # particular, we don't know the real size that the file should be, we\n        # only know the number of emulated sectors (512 bytes) that it will be\n        # loaded into.  Since the true length and the number of sectors are not\n        # the same thing, we can't actually add a hard link.\n\n        old_rec = None  # type: Optional[Union[dr.DirectoryRecord, udfmod.UDFFileEntry]]\n        if iso_old_path is not None:\n            # A link from a file on the ISO9660 filesystem...\n            old_rec = self._find_iso_record(iso_old_path)\n        elif joliet_old_path is not None:\n            # A link from a file on the Joliet filesystem...\n            old_rec = self._find_joliet_record(joliet_old_path)\n        elif boot_catalog_old:\n            # A link from the El Torito boot catalog...\n            if self.eltorito_boot_catalog is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Attempting to make link to non-existent El Torito boot catalog')\n            old_rec = self.eltorito_boot_catalog.dirrecords[0]\n        elif udf_old_path is not None:\n            # A link from a file on the UDF filesystem...\n            (old_ident_unused, old_rec) = self._find_udf_record(udf_old_path)\n            if old_rec is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot make hard link to a UDF file with an empty UDF File Entry')\n\n        # Above we checked to make sure we got at least one old path, so we\n        # don't need to worry about the else situation here.\n\n        num_bytes_to_add = self._add_hard_link_to_rec(old_rec, boot_catalog_old,\n                                                      **kwargs)\n\n        self._finish_add(0, num_bytes_to_add)", "response": "Add a hard link to the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves a hard link from the ISO. If the number of links to a piece of data drops to zero, then the contents will be removed from the ISO. Thus, this can be thought of as a lower-level interface to rm_file. Either an ISO9660 path or a Joliet path must be passed to this API, but not both. Thus, this interface can be used to hide files from either the ISO9660 filesystem, the Joliet filesystem, or both (if there is another reference to the data on the ISO, such as in El Torito). Note that this is an advanced API, so using it in combination with the higher-level APIs (like rm_file) may result in unexpected behavior. Once this API has been used, this API and add_hard_link() should be preferred over rm_file() and add_file(), respectively. Parameters: iso_path - The ISO link path to remove. joliet_path - The Joliet link path to remove. udf_path - The UDF link path to remove. Returns: Nothing.", "response": "def rm_hard_link(self, iso_path=None, joliet_path=None, udf_path=None):\n        # type: (Optional[str], Optional[str], Optional[str]) -> None\n        '''\n        Remove a hard link from the ISO.  If the number of links to a piece of\n        data drops to zero, then the contents will be removed from the ISO.\n        Thus, this can be thought of as a lower-level interface to rm_file.\n        Either an ISO9660 path or a Joliet path must be passed to this API, but\n        not both.  Thus, this interface can be used to hide files from either\n        the ISO9660 filesystem, the Joliet filesystem, or both (if there is\n        another reference to the data on the ISO, such as in El Torito).\n        Note that this is an advanced API, so using it in combination with the\n        higher-level APIs (like rm_file) may result in unexpected behavior.\n        Once this API has been used, this API and add_hard_link() should be\n        preferred over rm_file() and add_file(), respectively.\n\n        Parameters:\n         iso_path - The ISO link path to remove.\n         joliet_path - The Joliet link path to remove.\n         udf_path - The UDF link path to remove.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if len([x for x in (iso_path, joliet_path, udf_path) if x]) != 1:\n            raise pycdlibexception.PyCdlibInvalidInput('Must provide exactly one of iso_path, joliet_path, or udf_path')\n\n        num_bytes_to_remove = 0\n        rec = None  # type: Optional[Union[dr.DirectoryRecord, udfmod.UDFFileEntry]]\n\n        if iso_path is not None:\n            rec = self._find_iso_record(utils.normpath(iso_path))\n            num_bytes_to_remove += self._rm_dr_link(rec)\n        elif joliet_path is not None:\n            if self.joliet_vd is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot remove Joliet link from non-Joliet ISO')\n            joliet_path_bytes = self._normalize_joliet_path(joliet_path)\n            rec = self._find_joliet_record(joliet_path_bytes)\n            num_bytes_to_remove += self._rm_dr_link(rec)\n        elif udf_path is not None:\n            # UDF hard link removal\n            if self.udf_root is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Can only specify a udf_path for a UDF ISO')\n\n            (ident, rec) = self._find_udf_record(utils.normpath(udf_path))\n            if rec is None:\n                # If the rec is None, that means that this pointed to an 'empty'\n                # UDF File Entry.  Just remove the UDF File Identifier, which is\n                # as much as we can do.\n                if ident is not None and ident.parent is not None:\n                    num_bytes_to_remove += self._rm_udf_file_ident(ident.parent, ident.fi)\n                # We also have to remove the \"zero\" UDF File Entry, since nothing\n                # else will.\n                num_bytes_to_remove += self.pvd.logical_block_size()\n            else:\n                num_bytes_to_remove += self._rm_udf_link(rec)\n        else:\n            raise pycdlibexception.PyCdlibInvalidInput(\"One of 'iso_path', 'joliet_path', or 'udf_path' must be specified\")\n\n        self._finish_remove(num_bytes_to_remove, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_directory(self, iso_path=None, rr_name=None, joliet_path=None,\n                      file_mode=None, udf_path=None):\n        # type: (Optional[str], Optional[str], Optional[str], int, Optional[str]) -> None\n        '''\n        Add a directory to the ISO.  At least one of an iso_path, joliet_path,\n        or udf_path must be provided.  Providing joliet_path on a non-Joliet\n        ISO, or udf_path on a non-UDF ISO, is an error.  If the ISO contains\n        Rock Ridge, then a Rock Ridge name must be provided.\n\n        Parameters:\n         iso_path - The ISO9660 absolute path to use for the directory.\n         rr_name - The Rock Ridge name to use for the directory.\n         joliet_path - The Joliet absolute path to use for the directory.\n         file_mode - The POSIX file mode to use for the directory.  This only\n                     applies for Rock Ridge ISOs.\n         udf_path - The UDF absolute path to use for the directory.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if iso_path is None and joliet_path is None and udf_path is None:\n            raise pycdlibexception.PyCdlibInvalidInput('Either iso_path or joliet_path must be passed')\n\n        if file_mode is not None and not self.rock_ridge:\n            raise pycdlibexception.PyCdlibInvalidInput('A file mode can only be specified for Rock Ridge ISOs')\n\n        # For backwards-compatibility reasons, if the mode was not specified we\n        # just assume 555.  We should probably eventually make file_mode\n        # required for Rock Ridge and remove this assumption.\n        if file_mode is None:\n            file_mode = 0o040555\n\n        num_bytes_to_add = 0\n        if iso_path is not None:\n            iso_path_bytes = utils.normpath(iso_path)\n\n            new_rr_name = self._check_rr_name(rr_name)\n\n            depth = len(utils.split_path(iso_path_bytes))\n\n            if not self.rock_ridge and self.enhanced_vd is None:\n                _check_path_depth(iso_path_bytes)\n            (name, parent) = self._iso_name_and_parent_from_path(iso_path_bytes)\n\n            _check_iso9660_directory(name, self.interchange_level)\n\n            relocated = False\n            fake_dir_rec = None\n            orig_parent = None\n            iso9660_name = name\n            if self.rock_ridge and (depth % 8) == 0 and self.enhanced_vd is None:\n                # If the depth was a multiple of 8, then we are going to have to\n                # make a relocated entry for this record.\n\n                num_bytes_to_add += self._find_or_create_rr_moved()\n\n                # With a depth of 8, we have to add the directory both to the\n                # original parent with a CL link, and to the new parent with an\n                # RE link.  Here we make the 'fake' record, as a child of the\n                # original place; the real one will be done below.\n                fake_dir_rec = dr.DirectoryRecord()\n                fake_dir_rec.new_dir(self.pvd, name, parent,\n                                     self.pvd.sequence_number(),\n                                     self.rock_ridge, new_rr_name,\n                                     self.pvd.logical_block_size(), True, False,\n                                     self.xa, file_mode)\n                num_bytes_to_add += self._add_child_to_dr(fake_dir_rec,\n                                                          self.pvd.logical_block_size())\n\n                # The fake dir record doesn't get an entry in the path table record.\n\n                relocated = True\n                orig_parent = parent\n                parent = self._rr_moved_record\n\n                # Since we are moving the entry underneath the RR_MOVED\n                # directory, there is now the chance of a name collision (this\n                # can't happen without relocation since _add_child_to_dr() below\n                # won't allow duplicate names).  Check for that here and\n                # generate a new name.\n                index = 0\n                while True:\n                    for child in self._rr_moved_record.children:\n                        if child.file_ident == iso9660_name:\n                            # Python 3.4 doesn't support substitution with a byte\n                            # array, so we do it as a string and encode to bytes.\n                            iso9660_name = name + ('%03d' % (index)).encode()\n                            index += 1\n                            break\n                    else:\n                        break\n\n            rec = dr.DirectoryRecord()\n            rec.new_dir(self.pvd, iso9660_name, parent,\n                        self.pvd.sequence_number(), self.rock_ridge, new_rr_name,\n                        self.pvd.logical_block_size(), False, relocated,\n                        self.xa, file_mode)\n            num_bytes_to_add += self._add_child_to_dr(rec, self.pvd.logical_block_size())\n            if rec.rock_ridge is not None:\n                if relocated:\n                    fake_dir_rec.rock_ridge.cl_to_moved_dr = rec  # type: ignore\n                    rec.rock_ridge.moved_to_cl_dr = fake_dir_rec  # type: ignore\n                num_bytes_to_add += self._update_rr_ce_entry(rec)\n\n            self._create_dot(self.pvd, rec, self.rock_ridge, self.xa, file_mode)\n\n            parent_file_mode = -1\n            if parent.rock_ridge is not None:\n                parent_file_mode = parent.rock_ridge.get_file_mode()\n            else:\n                if parent.is_root:\n                    parent_file_mode = file_mode\n\n            dotdot = self._create_dotdot(self.pvd, rec, self.rock_ridge,\n                                         relocated, self.xa, parent_file_mode)\n            if dotdot.rock_ridge is not None and relocated:\n                dotdot.rock_ridge.parent_link = orig_parent\n\n            # We always need to add an entry to the path table record\n            ptr = path_table_record.PathTableRecord()\n            ptr.new_dir(iso9660_name)\n\n            num_bytes_to_add += self._add_to_ptr_size(ptr) + self.pvd.logical_block_size()\n\n            rec.set_ptr(ptr)\n\n        if joliet_path is not None:\n            num_bytes_to_add += self._add_joliet_dir(self._normalize_joliet_path(joliet_path))\n\n        if udf_path is not None:\n            if self.udf_root is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Can only specify a udf_path for a UDF ISO')\n\n            log_block_size = self.pvd.logical_block_size()\n\n            udf_path_bytes = utils.normpath(udf_path)\n            (udf_name, udf_parent) = self._udf_name_and_parent_from_path(udf_path_bytes)\n\n            file_ident = udfmod.UDFFileIdentifierDescriptor()\n            file_ident.new(True, False, udf_name, udf_parent)\n            num_new_extents = udf_parent.add_file_ident_desc(file_ident, log_block_size)\n            num_bytes_to_add += num_new_extents * log_block_size\n\n            file_entry = udfmod.UDFFileEntry()\n            file_entry.new(0, 'dir', udf_parent, log_block_size)\n            file_ident.file_entry = file_entry\n            file_entry.file_ident = file_ident\n            num_bytes_to_add += log_block_size\n\n            udf_dotdot = udfmod.UDFFileIdentifierDescriptor()\n            udf_dotdot.new(True, True, b'', udf_parent)\n            num_new_extents = file_ident.file_entry.add_file_ident_desc(udf_dotdot, log_block_size)\n            num_bytes_to_add += num_new_extents * log_block_size\n\n            self.udf_logical_volume_integrity.logical_volume_impl_use.num_dirs += 1\n\n        self._finish_add(0, num_bytes_to_add)", "response": "A method to add a directory to the Rock Ridge ISO."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rm_file(self, iso_path, rr_name=None, joliet_path=None, udf_path=None):  # pylint: disable=unused-argument\n        # type: (str, Optional[str], Optional[str], Optional[str]) -> None\n        '''\n        Remove a file from the ISO.\n\n        Parameters:\n         iso_path - The path to the file to remove.\n         rr_name - The Rock Ridge name of the file to remove.\n         joliet_path - The Joliet path to the file to remove.\n         udf_path - The UDF path to the file to remove.\n        Returns:\n         Nothing.\n        '''\n\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        iso_path_bytes = utils.normpath(iso_path)\n\n        if not utils.starts_with_slash(iso_path_bytes):\n            raise pycdlibexception.PyCdlibInvalidInput('Must be a path starting with /')\n\n        child = self._find_iso_record(iso_path_bytes)\n\n        if not child.is_file():\n            raise pycdlibexception.PyCdlibInvalidInput('Cannot remove a directory with rm_file (try rm_directory instead)')\n\n        # We also want to check to see if this Directory Record is currently\n        # being used as an El Torito Boot Catalog, Initial Entry, or Section\n        # Entry.  If it is, we throw an exception; we don't know if the user\n        # meant to remove El Torito from this ISO, or if they meant to 'hide'\n        # the entry, but we need them to call the correct API to let us know.\n        if self.eltorito_boot_catalog is not None:\n            if any([id(child) == id(rec) for rec in self.eltorito_boot_catalog.dirrecords]):\n                raise pycdlibexception.PyCdlibInvalidInput(\"Cannot remove a file that is referenced by El Torito; either use 'rm_eltorito' to remove El Torito first, or use 'rm_hard_link' to hide the entry\")\n\n            eltorito_entries = {}\n            eltorito_entries[id(self.eltorito_boot_catalog.initial_entry.inode)] = True\n            for sec in self.eltorito_boot_catalog.sections:\n                for entry in sec.section_entries:\n                    eltorito_entries[id(entry.inode)] = True\n\n            if id(child.inode) in eltorito_entries:\n                raise pycdlibexception.PyCdlibInvalidInput(\"Cannot remove a file that is referenced by El Torito; either use 'rm_eltorito' to remove El Torito first, or use 'rm_hard_link' to hide the entry\")\n\n        num_bytes_to_remove = 0\n\n        udf_file_ident = None\n        udf_file_entry = None\n        if udf_path is not None:\n            # Find the UDF record if the udf_path was specified; this may be\n            # used later on.\n            if self.udf_root is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Can only specify a udf_path for a UDF ISO')\n\n            udf_path_bytes = utils.normpath(udf_path)\n            (udf_file_ident, udf_file_entry) = self._find_udf_record(udf_path_bytes)\n\n        # If the child is a Rock Ridge symlink, then it has no inode since\n        # there is no data attached to it.\n        if child.inode is None:\n            num_bytes_to_remove += self._remove_child_from_dr(child,\n                                                              child.index_in_parent,\n                                                              self.pvd.logical_block_size())\n        else:\n            while child.inode.linked_records:\n                rec = child.inode.linked_records[0]\n\n                if isinstance(rec, dr.DirectoryRecord):\n                    num_bytes_to_remove += self._rm_dr_link(rec)\n                elif isinstance(rec, udfmod.UDFFileEntry):\n                    num_bytes_to_remove += self._rm_udf_link(rec)\n                else:\n                    # This should never happen\n                    raise pycdlibexception.PyCdlibInternalError('Saw a linked record that was neither ISO or UDF')\n\n        if udf_file_ident is not None and udf_file_entry is None and udf_file_ident.parent is not None:\n            # If the udf_path was specified, go looking for the UDF File Ident\n            # that corresponds to this record.  If the UDF File Ident exists,\n            # and the File Entry is None, this means that it is an \"zeroed\"\n            # UDF File Entry and we have to remove it by hand.\n            self._rm_udf_file_ident(udf_file_ident.parent, udf_file_ident.fi)\n            # We also have to remove the \"zero\" UDF File Entry, since nothing\n            # else will.\n            num_bytes_to_remove += self.pvd.logical_block_size()\n\n        self._finish_remove(num_bytes_to_remove, True)", "response": "A method to remove a file from the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rm_directory(self, iso_path=None, rr_name=None, joliet_path=None, udf_path=None):\n        # type: (Optional[str], Optional[str], Optional[str], Optional[str]) -> None\n        '''\n        Remove a directory from the ISO.\n\n        Parameters:\n         iso_path - The path to the directory to remove.\n         rr_name - The Rock Ridge name of the directory to remove.\n         joliet_path - The Joliet path to the directory to remove.\n         udf_path - The UDF absolute path to the directory to remove.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if iso_path is None and joliet_path is None and udf_path is None:\n            raise pycdlibexception.PyCdlibInvalidInput('Either iso_path or joliet_path must be passed')\n\n        num_bytes_to_remove = 0\n\n        if iso_path is not None:\n            iso_path_bytes = utils.normpath(iso_path)\n\n            if iso_path_bytes == b'/':\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot remove base directory')\n\n            self._check_rr_name(rr_name)\n\n            child = self._find_iso_record(iso_path_bytes)\n\n            if not child.is_dir():\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot remove a file with rm_directory (try rm_file instead)')\n\n            if len(child.children) > 2:\n                raise pycdlibexception.PyCdlibInvalidInput('Directory must be empty to use rm_directory')\n\n            num_bytes_to_remove += self._remove_child_from_dr(child,\n                                                              child.index_in_parent,\n                                                              self.pvd.logical_block_size())\n            if child.ptr is not None:\n                num_bytes_to_remove += self._remove_from_ptr_size(child.ptr)\n\n            # Remove space for the directory itself.\n            num_bytes_to_remove += child.get_data_length()\n\n            if child.rock_ridge is not None and child.rock_ridge.relocated_record():\n                # OK, this child was relocated.  If the parent of this relocated\n                # record is empty (only . and ..), we can remove it.\n                parent = child.parent\n                if parent is None:\n                    raise pycdlibexception.PyCdlibInternalError('Relocated child has empty parent; this should not be')\n                if len(parent.children) == 2:\n                    if parent.parent is None:\n                        raise pycdlibexception.PyCdlibInternalError('Tried to remove a directory that has no parent; this should not happen')\n                    for index, c in enumerate(parent.parent.children):\n                        if c.file_ident == parent.file_ident:\n                            parent_index = index\n                            break\n                    else:\n                        raise pycdlibexception.PyCdlibInvalidISO('Could not find parent in its own parent!')\n\n                    num_bytes_to_remove += self._remove_child_from_dr(parent,\n                                                                      parent_index,\n                                                                      self.pvd.logical_block_size())\n                    num_bytes_to_remove += parent.get_data_length()\n                    if parent.ptr is not None:\n                        num_bytes_to_remove += self._remove_from_ptr_size(parent.ptr)\n\n                cl = child.rock_ridge.moved_to_cl_dr\n                if cl is None:\n                    raise pycdlibexception.PyCdlibInternalError('Invalid child link record')\n                if cl.parent is None:\n                    raise pycdlibexception.PyCdlibInternalError('Invalid parent to child link record; this should not be')\n                for index, c in enumerate(cl.parent.children):\n                    if cl.file_ident == c.file_ident:\n                        clindex = index\n                        break\n                else:\n                    raise pycdlibexception.PyCdlibInvalidISO('CL record does not exist')\n\n                if cl.children:\n                    raise pycdlibexception.PyCdlibInvalidISO('Parent link should have no children!')\n                num_bytes_to_remove += self._remove_child_from_dr(cl, clindex,\n                                                                  self.pvd.logical_block_size())\n                # Note that we do not remove additional space from the PVD for the child_link\n                # record because it is a 'fake' record that has no real size.\n\n            if child.rock_ridge is not None and child.rock_ridge.dr_entries.ce_record is not None and child.rock_ridge.ce_block is not None:\n                child.rock_ridge.ce_block.remove_entry(child.rock_ridge.dr_entries.ce_record.offset_cont_area,\n                                                       child.rock_ridge.dr_entries.ce_record.len_cont_area)\n\n        if joliet_path is not None:\n            num_bytes_to_remove += self._rm_joliet_dir(self._normalize_joliet_path(joliet_path))\n\n        if udf_path is not None:\n            if self.udf_root is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Can only specify a udf_path for a UDF ISO')\n\n            udf_path_bytes = utils.normpath(udf_path)\n\n            if udf_path_bytes == b'/':\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot remove base directory')\n\n            (udf_name, udf_parent) = self._udf_name_and_parent_from_path(udf_path_bytes)\n\n            num_extents_to_remove = udf_parent.remove_file_ident_desc_by_name(udf_name,\n                                                                              self.pvd.logical_block_size())\n            # Remove space (if necessary) in the parent File Identifier\n            # Descriptor area.\n            num_bytes_to_remove += num_extents_to_remove * self.pvd.logical_block_size()\n            # Remove space for the File Entry.\n            num_bytes_to_remove += self.pvd.logical_block_size()\n            # Remove space for the list of File Identifier Descriptors.\n            num_bytes_to_remove += self.pvd.logical_block_size()\n\n            self.udf_logical_volume_integrity.logical_volume_impl_use.num_dirs -= 1\n\n            self._find_udf_record.cache_clear()  # pylint: disable=no-member\n\n        self._finish_remove(num_bytes_to_remove, True)", "response": "A method to remove a directory from the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an El Torito Boot Record, and associated files, to the ISO. The file that will be used as the bootfile must be passed into this function and must already be present on the ISO. Parameters: bootfile_path - The file to use as the boot file; it must already exist on this ISO. bootcatfile - The fake file to use as the boot catalog entry; set to BOOT.CAT;1 by default. rr_bootcatname - The Rock Ridge name for the fake file to use as the boot catalog entry; set to 'boot.cat' by default. joliet_bootcatfile - The Joliet name for the fake file to use as the boot catalog entry; set to 'boot.cat' by default. boot_load_size - The number of sectors to use for the boot entry; if set to None (the default), the number of sectors will be calculated. platform_id - The platform ID to set for the El Torito entry; 0 is for x86, 1 is for Power PC, and 2 is for Mac. 0 is the default. boot_info_table - Whether to add a boot info table to the ISO. The default is False. efi - Whether this is an EFI entry for El Torito. The default is False. media_name - The name of the media type, one of 'noemul', 'floppy', or 'hdemul'. bootable - Whether the boot media is bootable. The default is True. boot_load_seg - The load segment address of the boot image. Returns: Nothing.", "response": "def add_eltorito(self, bootfile_path, bootcatfile=None,\n                     rr_bootcatname=None, joliet_bootcatfile=None,\n                     boot_load_size=None, platform_id=0, boot_info_table=False,\n                     efi=False, media_name='noemul', bootable=True,\n                     boot_load_seg=0, udf_bootcatfile=None):\n        # type: (str, Optional[str], Optional[str], Optional[str], int, int, bool, bool, str, bool, int, Optional[str]) -> None\n        '''\n        Add an El Torito Boot Record, and associated files, to the ISO.  The\n        file that will be used as the bootfile must be passed into this function\n        and must already be present on the ISO.\n\n        Parameters:\n         bootfile_path - The file to use as the boot file; it must already\n                         exist on this ISO.\n         bootcatfile - The fake file to use as the boot catalog entry; set to\n                       BOOT.CAT;1 by default.\n         rr_bootcatname - The Rock Ridge name for the fake file to use as the\n                          boot catalog entry; set to 'boot.cat' by default.\n         joliet_bootcatfile - The Joliet name for the fake file to use as the\n                              boot catalog entry; set to 'boot.cat' by default.\n         boot_load_size - The number of sectors to use for the boot entry; if\n                          set to None (the default), the number of sectors will\n                          be calculated.\n         platform_id - The platform ID to set for the El Torito entry; 0 is for\n                       x86, 1 is for Power PC, and 2 is for Mac.  0 is the\n                       default.\n         boot_info_table - Whether to add a boot info table to the ISO.  The\n                           default is False.\n         efi - Whether this is an EFI entry for El Torito.  The default is False.\n         media_name - The name of the media type, one of 'noemul', 'floppy', or 'hdemul'.\n         bootable - Whether the boot media is bootable.  The default is True.\n         boot_load_seg - The load segment address of the boot image.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        # In order to add an El Torito boot, we need to do the following:\n        # 1.  Find the boot file record (which must already exist).\n        # 2.  Construct a BootRecord.\n        # 3.  Construct a BootCatalog, and add it to the filesystem.\n        # 4.  Add the boot record to the ISO.\n\n        if not bootcatfile:\n            bootcatfile = '/BOOT.CAT;1'\n\n        bootfile_path_bytes = utils.normpath(bootfile_path)\n\n        if self.joliet_vd is not None:\n            if not joliet_bootcatfile:\n                joliet_bootcatfile = '/boot.cat'\n        else:\n            if joliet_bootcatfile:\n                raise pycdlibexception.PyCdlibInvalidInput('A joliet path must not be passed when adding El Torito to a non-Joliet ISO')\n\n        if self.udf_root is not None:\n            if not udf_bootcatfile:\n                udf_bootcatfile = '/boot.cat'\n        else:\n            if udf_bootcatfile:\n                raise pycdlibexception.PyCdlibInvalidInput('A UDF path must not be passed when adding El Torito to a non-UDF ISO')\n\n        log_block_size = self.pvd.logical_block_size()\n\n        # Step 1.\n        boot_dirrecord = self._find_iso_record(bootfile_path_bytes)\n\n        if boot_load_size is None:\n            sector_count = utils.ceiling_div(boot_dirrecord.get_data_length(),\n                                             log_block_size) * log_block_size // 512\n        else:\n            sector_count = boot_load_size\n\n        if boot_dirrecord.inode is None:\n            raise pycdlibexception.PyCdlibInternalError('Tried to add an empty boot dirrecord inode to the El Torito boot catalog')\n\n        if boot_info_table:\n            orig_len = boot_dirrecord.get_data_length()\n            bi_table = eltorito.EltoritoBootInfoTable()\n            with inode.InodeOpenData(boot_dirrecord.inode, log_block_size) as (data_fp, data_len):\n                bi_table.new(self.pvd, boot_dirrecord.inode, orig_len,\n                             self._calculate_eltorito_boot_info_table_csum(data_fp, data_len))\n\n            boot_dirrecord.inode.add_boot_info_table(bi_table)\n\n        system_type = 0\n        if media_name == 'hdemul':\n            with inode.InodeOpenData(boot_dirrecord.inode, log_block_size) as (data_fp, data_len):\n                disk_mbr = data_fp.read(512)\n                if len(disk_mbr) != 512:\n                    raise pycdlibexception.PyCdlibInvalidInput('Could not read entire HD MBR, must be at least 512 bytes')\n                system_type = eltorito.hdmbrcheck(disk_mbr, sector_count, bootable)\n\n        num_bytes_to_add = 0\n        if self.eltorito_boot_catalog is not None:\n            # All right, we already created the boot catalog.  Add a new section\n            # to the boot catalog\n            self.eltorito_boot_catalog.add_section(boot_dirrecord.inode,\n                                                   sector_count, boot_load_seg,\n                                                   media_name, system_type, efi,\n                                                   bootable)\n        else:\n            # Step 2.\n            br = headervd.BootRecord()\n            br.new(b'EL TORITO SPECIFICATION')\n            self.brs.append(br)\n            # On a UDF ISO, adding a new Boot Record doesn't actually increase\n            # the size, since there are a bunch of gaps at the beginning.\n            if not self._has_udf:\n                num_bytes_to_add += log_block_size\n\n            # Step 3.\n            self.eltorito_boot_catalog = eltorito.EltoritoBootCatalog(br)\n            self.eltorito_boot_catalog.new(br, boot_dirrecord.inode, sector_count,\n                                           boot_load_seg, media_name, system_type,\n                                           platform_id, bootable)\n\n            # Step 4.\n            rrname = ''\n            if self.rock_ridge:\n                if rr_bootcatname is None:\n                    rrname = 'boot.cat'\n                else:\n                    rrname = rr_bootcatname\n\n            num_bytes_to_add += self._add_fp(None, log_block_size, False, bootcatfile,\n                                             rrname, joliet_bootcatfile,\n                                             udf_bootcatfile, None, True)\n\n        self._finish_add(0, num_bytes_to_add)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rm_eltorito(self):\n        # type: () -> None\n        '''\n        Remove the El Torito boot record (and Boot Catalog) from the ISO.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if self.eltorito_boot_catalog is None:\n            raise pycdlibexception.PyCdlibInvalidInput('This ISO does not have an El Torito Boot Record')\n\n        for brindex, br in enumerate(self.brs):\n            if br.boot_system_identifier == b'EL TORITO SPECIFICATION'.ljust(32, b'\\x00'):\n                eltorito_index = brindex\n                break\n        else:\n            # There was a boot catalog, but no corresponding boot record.  This\n            # should never happen.\n            raise pycdlibexception.PyCdlibInternalError('El Torito boot catalog found with no corresponding boot record')\n\n        del self.brs[eltorito_index]\n\n        num_bytes_to_remove = 0\n\n        # On a UDF ISO, removing the Boot Record doesn't actually decrease\n        # the size, since there are a bunch of gaps at the beginning.\n        if not self._has_udf:\n            num_bytes_to_remove += self.pvd.logical_block_size()\n\n        # Remove all of the DirectoryRecord/UDFFileEntries associated with\n        # the Boot Catalog\n        for rec in self.eltorito_boot_catalog.dirrecords:\n            if isinstance(rec, dr.DirectoryRecord):\n                num_bytes_to_remove += self._rm_dr_link(rec)\n            elif isinstance(rec, udfmod.UDFFileEntry):\n                num_bytes_to_remove += self._rm_udf_link(rec)\n            else:\n                # This should never happen\n                raise pycdlibexception.PyCdlibInternalError('Saw an El Torito record that was neither ISO nor UDF')\n\n        # Remove the linkage from the El Torito Entries to the inodes\n        entries_to_remove = [self.eltorito_boot_catalog.initial_entry]\n        for sec in self.eltorito_boot_catalog.sections:\n            for entry in sec.section_entries:\n                entries_to_remove.append(entry)\n\n        for entry in entries_to_remove:\n            if entry.inode is not None:\n                new_list = []\n                for linkrec in entry.inode.linked_records:\n                    if id(linkrec) != id(entry):\n                        new_list.append(linkrec)\n                entry.inode.linked_records = new_list\n\n        num_bytes_to_remove += len(self.eltorito_boot_catalog.record())\n\n        self.eltorito_boot_catalog = None\n\n        self._finish_remove(num_bytes_to_remove, True)", "response": "A method to remove an El Torito Boot Record from the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_children(self, **kwargs):\n        # type: (str) -> Generator\n        '''\n        Generate a list of all of the file/directory objects in the\n        specified location on the ISO.\n\n        Parameters:\n         iso_path - The absolute path on the ISO to list the children for.\n         rr_path - The absolute Rock Ridge path on the ISO to list the children for.\n         joliet_path - The absolute Joliet path on the ISO to list the children for.\n         udf_path - The absolute UDF path on the ISO to list the children for.\n        Yields:\n         Children of this path.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        num_paths = 0\n        for key in kwargs:\n            if key in ['joliet_path', 'rr_path', 'iso_path', 'udf_path']:\n                if kwargs[key] is not None:\n                    num_paths += 1\n            else:\n                raise pycdlibexception.PyCdlibInvalidInput(\"Invalid keyword, must be one of 'iso_path', 'rr_path', 'joliet_path', or 'udf_path'\")\n\n        if num_paths != 1:\n            raise pycdlibexception.PyCdlibInvalidInput(\"Must specify one, and only one of 'iso_path', 'rr_path', 'joliet_path', or 'udf_path'\")\n\n        if 'udf_path' in kwargs:\n            udf_rec = self._get_udf_entry(kwargs['udf_path'])\n\n            if not udf_rec.is_dir():\n                raise pycdlibexception.PyCdlibInvalidInput('UDF File Entry is not a directory!')\n\n            for fi_desc in udf_rec.fi_descs:\n                yield fi_desc.file_entry\n        else:\n            if 'joliet_path' in kwargs:\n                rec = self._get_entry(None, None, self._normalize_joliet_path(kwargs['joliet_path']))\n            elif 'rr_path' in kwargs:\n                rec = self._get_entry(None, utils.normpath(kwargs['rr_path']), None)\n            else:\n                rec = self._get_entry(utils.normpath(kwargs['iso_path']), None, None)\n\n            for c in _yield_children(rec):\n                yield c", "response": "A method to list all of the file or directory objects in the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_entry(self, iso_path, joliet=False):\n        # type: (str, bool) -> dr.DirectoryRecord\n        '''\n        (deprecated) Get the directory record for a particular path.  It is\n        recommended to use the 'get_record' API instead.\n\n        Parameters:\n         iso_path - The path on the ISO to look up information for.\n         joliet - Whether to look for the path in the Joliet portion of the ISO.\n        Returns:\n         A dr.DirectoryRecord object representing the path.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if joliet:\n            return self._get_entry(None, None, self._normalize_joliet_path(iso_path))\n        return self._get_entry(utils.normpath(iso_path), None, None)", "response": "A method to get the directory record for a particular path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_record(self, **kwargs):\n        # type: (str) -> Union[dr.DirectoryRecord, udfmod.UDFFileEntry]\n        '''\n        Get the directory record for a particular path.\n\n        Parameters:\n         iso_path - The absolute path on the ISO9660 filesystem to get the\n                    record for.\n         rr_path - The absolute path on the Rock Ridge filesystem to get the\n                   record for.\n         joliet_path - The absolute path on the Joliet filesystem to get the\n                       record for.\n         udf_path - The absolute path on the UDF filesystem to get the record\n                    for.\n        Returns:\n         An object that represents the path.  This may be a dr.DirectoryRecord\n         object (in the cases of iso_path, rr_path, or joliet_path), or a\n         udf.UDFFileEntry object (in the case of udf_path).\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        num_paths = 0\n        for key in kwargs:\n            if key in ['joliet_path', 'rr_path', 'iso_path', 'udf_path']:\n                if kwargs[key] is not None:\n                    num_paths += 1\n            else:\n                raise pycdlibexception.PyCdlibInvalidInput(\"Invalid keyword, must be one of 'iso_path', 'rr_path', 'joliet_path', or 'udf_path'\")\n\n        if num_paths != 1:\n            raise pycdlibexception.PyCdlibInvalidInput(\"Must specify one, and only one of 'iso_path', 'rr_path', 'joliet_path', or 'udf_path'\")\n\n        if 'joliet_path' in kwargs:\n            return self._get_entry(None, None, self._normalize_joliet_path(kwargs['joliet_path']))\n        if 'rr_path' in kwargs:\n            return self._get_entry(None, utils.normpath(kwargs['rr_path']), None)\n        if 'udf_path' in kwargs:\n            return self._get_udf_entry(kwargs['udf_path'])\n        return self._get_entry(utils.normpath(kwargs['iso_path']), None, None)", "response": "A method to get the directory record for a particular path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_isohybrid(self, part_entry=1, mbr_id=None, part_offset=0,\n                      geometry_sectors=32, geometry_heads=64, part_type=0x17,\n                      mac=False):\n        # type: (int, Optional[int], int, int, int, int, bool) -> None\n        '''\n        Make an ISO a 'hybrid', which means that it can be booted either from a\n        CD or from more traditional media (like a USB stick).  This requires\n        that the ISO already have El Torito, and will use the El Torito boot\n        file as a bootable image.  That image must contain a certain signature\n        in order to work as a hybrid (if using syslinux, this generally means\n        the isohdpfx.bin files).\n\n        Parameters:\n         part_entry - The partition entry to use; one by default.\n         mbr_id - The mbr_id to use.  If set to None (the default), a random one\n                  will be generated.\n         part_offset - The partition offset to use; zero by default.\n         geometry_sectors - The number of sectors to assign; thirty-two by default.\n         geometry_heads - The number of heads to assign; sixty-four by default.\n         part_type - The partition type to assign; twenty-three by default.\n         mac - Add support for Mac; False by default.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if self.eltorito_boot_catalog is None:\n            raise pycdlibexception.PyCdlibInvalidInput('The ISO must have an El Torito Boot Record to add isohybrid support')\n\n        if self.eltorito_boot_catalog.initial_entry.sector_count != 4:\n            raise pycdlibexception.PyCdlibInvalidInput('El Torito Boot Catalog sector count must be 4 (was actually 0x%x)' % (self.eltorito_boot_catalog.initial_entry.sector_count))\n\n        # Now check that the eltorito boot file contains the appropriate\n        # signature (offset 0x40, '\\xFB\\xC0\\x78\\x70')\n        with inode.InodeOpenData(self.eltorito_boot_catalog.initial_entry.inode, self.pvd.logical_block_size()) as (data_fp, data_len_unused):\n            data_fp.seek(0x40, os.SEEK_CUR)\n            signature = data_fp.read(4)\n\n        if signature != b'\\xfb\\xc0\\x78\\x70':\n            raise pycdlibexception.PyCdlibInvalidInput('Invalid signature on boot file for iso hybrid')\n\n        self.isohybrid_mbr = isohybrid.IsoHybrid()\n        self.isohybrid_mbr.new(mac, part_entry, mbr_id, part_offset,\n                               geometry_sectors, geometry_heads, part_type)", "response": "A method to add an ISO hybridization to the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef full_path_from_dirrecord(self, rec, rockridge=False):\n        # type: (Union[dr.DirectoryRecord, udfmod.UDFFileEntry], bool) -> str\n        '''\n        A method to get the absolute path of a directory record.\n\n        Parameters:\n         rec - The directory record to get the full path for.\n         rockridge - Whether to get the rock ridge full path.\n        Returns:\n         A string representing the absolute path to the file on the ISO.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        ret = b''\n        if isinstance(rec, dr.DirectoryRecord):\n            encoding = 'utf-8'\n            if self.joliet_vd is not None and id(rec.vd) == id(self.joliet_vd):\n                encoding = 'utf-16_be'\n            slash = '/'.encode(encoding)\n\n            # A root entry has no Rock Ridge entry, even on a Rock Ridge ISO.  Just\n            # always return / here.\n            if rec.is_root:\n                return '/'\n\n            if rockridge and rec.rock_ridge is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot generate a Rock Ridge path on a non-Rock Ridge ISO')\n\n            parent = rec  # type: Optional[dr.DirectoryRecord]\n            while parent is not None:\n                if not parent.is_root:\n                    if rockridge and parent.rock_ridge is not None:\n                        ret = slash + parent.rock_ridge.name() + ret\n                    else:\n                        ret = slash + parent.file_identifier() + ret\n                parent = parent.parent\n        else:\n            if rec.parent is None:\n                return '/'\n            if rec.file_ident is not None:\n                encoding = rec.file_ident.encoding\n            else:\n                encoding = 'utf-8'\n            slash = '/'.encode(encoding)\n            udfparent = rec  # type: Optional[udfmod.UDFFileEntry]\n            while udfparent is not None:\n                ident = udfparent.file_identifier()\n                if ident != b'/':\n                    ret = slash + ident + ret\n                udfparent = udfparent.parent\n\n        if sys.version_info >= (3, 0):\n            # Python 3, just return the encoded version\n            return ret.decode(encoding)\n\n        # Python 2.\n        return ret.decode(encoding).encode('utf-8')", "response": "A method to get the absolute path of a directory record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclearing the ISO9660 hidden attribute on a file or directory. This will cause the file or directory to show up when listing entries on the ISO. Exactly one of iso_path, rr_path, or joliet_path must be specified. Parameters: iso_path - The path on the ISO to clear the hidden bit from. rr_path - The Rock Ridge path on the ISO to clear the hidden bit from. joliet_path - The Joliet path on the ISO to clear the hidden bit from. Returns: Nothing.", "response": "def clear_hidden(self, iso_path=None, rr_path=None, joliet_path=None):\n        # type: (Optional[str], Optional[str], Optional[str]) -> None\n        '''\n        Clear the ISO9660 hidden attribute on a file or directory.  This will\n        cause the file or directory to show up when listing entries on the ISO.\n        Exactly one of iso_path, rr_path, or joliet_path must be specified.\n\n        Parameters:\n         iso_path - The path on the ISO to clear the hidden bit from.\n         rr_path - The Rock Ridge path on the ISO to clear the hidden bit from.\n         joliet_path - The Joliet path on the ISO to clear the hidden bit from.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if len([x for x in (iso_path, rr_path, joliet_path) if x is not None]) != 1:\n            raise pycdlibexception.PyCdlibInvalidInput('Must provide exactly one of iso_path, rr_path, or joliet_path')\n\n        if iso_path is not None:\n            rec = self._find_iso_record(utils.normpath(iso_path))\n        elif rr_path is not None:\n            rec = self._find_rr_record(utils.normpath(rr_path))\n        elif joliet_path is not None:\n            joliet_path_bytes = self._normalize_joliet_path(joliet_path)\n            rec = self._find_joliet_record(joliet_path_bytes)\n\n        rec.change_existence(False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the name of the relocated directory on a Rock Ridge ISO. The ISO must be a Rock Ridge one, and must not have previously had the relocated name set. Parameters: name - The name for a relocated directory. rr_name - The Rock Ridge name for a relocated directory. Returns: Nothing.", "response": "def set_relocated_name(self, name, rr_name):\n        # type: (str, str) -> None\n        '''\n        Set the name of the relocated directory on a Rock Ridge ISO.  The ISO\n        must be a Rock Ridge one, and must not have previously had the relocated\n        name set.\n\n        Parameters:\n         name - The name for a relocated directory.\n         rr_name - The Rock Ridge name for a relocated directory.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if not self.rock_ridge:\n            raise pycdlibexception.PyCdlibInvalidInput('Can only set the relocated name on a Rock Ridge ISO')\n\n        encoded_name = name.encode('utf-8')\n        encoded_rr_name = rr_name.encode('utf-8')\n        if self._rr_moved_name is not None:\n            if self._rr_moved_name == encoded_name and self._rr_moved_rr_name == encoded_rr_name:\n                return\n            raise pycdlibexception.PyCdlibInvalidInput('Changing the existing rr_moved name is not allowed')\n\n        _check_iso9660_directory(encoded_name, self.interchange_level)\n        self._rr_moved_name = encoded_name\n        self._rr_moved_rr_name = encoded_rr_name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef walk(self, **kwargs):\n        # type: (str) -> Generator\n        '''\n        Walk the entries on the ISO, starting at the given path.  One, and only\n        one, of iso_path, rr_path, joliet_path, and udf_path is allowed.\n        Similar to os.walk(), yield a 3-tuple of (path-to-here, dirlist, filelist)\n        for each directory level.\n\n        Parameters:\n         iso_path - The absolute ISO path to the starting entry on the ISO.\n         rr_path - The absolute Rock Ridge path to the starting entry on the ISO.\n         joliet_path - The absolute Joliet path to the starting entry on the ISO.\n         udf_path - The absolute UDF path to the starting entry on the ISO.\n        Yields:\n         3-tuples of (path-to-here, dirlist, filelist)\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        num_paths = 0\n        for key in kwargs:\n            if key in ['joliet_path', 'rr_path', 'iso_path', 'udf_path']:\n                if kwargs[key] is not None:\n                    num_paths += 1\n            else:\n                raise pycdlibexception.PyCdlibInvalidInput(\"Invalid keyword, must be one of 'iso_path', 'rr_path', 'joliet_path', or 'udf_path'\")\n\n        if num_paths != 1:\n            raise pycdlibexception.PyCdlibInvalidInput(\"Must specify one, and only one of 'iso_path', 'rr_path', 'joliet_path', or 'udf_path'\")\n\n        rec = None  # type: Optional[Union[dr.DirectoryRecord, udfmod.UDFFileEntry]]\n        if 'joliet_path' in kwargs:\n            joliet_path = self._normalize_joliet_path(kwargs['joliet_path'])\n            rec = self._find_joliet_record(joliet_path)\n            path_type = 'joliet_path'\n            encoding = 'utf-16_be'\n        elif 'udf_path' in kwargs:\n            if self.udf_root is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Can only specify a UDF path for a UDF ISO')\n            (ident_unused, rec) = self._find_udf_record(utils.normpath(kwargs['udf_path']))\n            if rec is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot get entry for empty UDF File Entry')\n            path_type = 'udf_path'\n            encoding = ''\n        elif 'rr_path' in kwargs:\n            if not self.rock_ridge:\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot fetch a rr_path from a non-Rock Ridge ISO')\n            rec = self._find_rr_record(utils.normpath(kwargs['rr_path']))\n            path_type = 'rr_path'\n            encoding = 'utf-8'\n        else:\n            rec = self._find_iso_record(utils.normpath(kwargs['iso_path']))\n            path_type = 'iso_path'\n            encoding = 'utf-8'\n\n        dirs = collections.deque([rec])\n        while dirs:\n            dir_record = dirs.popleft()\n\n            relpath = self.full_path_from_dirrecord(dir_record,\n                                                    rockridge=path_type == 'rr_path')\n            dirlist = []\n            filelist = []\n            dirdict = {}\n\n            for child in reversed(list(self.list_children(**{path_type: relpath}))):\n                if child is None or child.is_dot() or child.is_dotdot():\n                    continue\n\n                if isinstance(child, udfmod.UDFFileEntry) and child.file_ident is not None:\n                    encoding = child.file_ident.encoding\n\n                if path_type == 'rr_path':\n                    name = child.rock_ridge.name()\n                else:\n                    name = child.file_identifier()\n\n                if sys.version_info >= (3, 0):\n                    # Python 3, just return the encoded version\n                    encoded = name.decode(encoding)\n                else:\n                    # Python 2.\n                    encoded = name.decode(encoding).encode('utf-8')\n\n                if child.is_dir():\n                    dirlist.append(encoded)\n                    dirdict[encoded] = child\n                else:\n                    filelist.append(encoded)\n\n            yield relpath, dirlist, filelist\n\n            # We allow the user to modify dirlist along the way, so we\n            # add the children to dirs *after* yield returns.\n            for name in dirlist:\n                dirs.appendleft(dirdict[name])", "response": "A method to walk the ISO and return a list of the ISO entries."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen a file for reading in a context manager.", "response": "def open_file_from_iso(self, **kwargs):\n        # type: (str) -> PyCdlibIO\n        '''\n        Open a file for reading in a context manager.  This allows the user to\n        operate on the file in user-defined chunks (utilizing the read() method\n        of the returned context manager).\n\n        Parameters:\n         iso_path - The absolute ISO path to the file on the ISO.\n         rr_path - The absolute Rock Ridge path to the file on the ISO.\n         joliet_path - The absolute Joliet path to the file on the ISO.\n         udf_path - The absolute UDF path to the file on the ISO.\n        Returns:\n         A PyCdlibIO object allowing access to the file.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        num_paths = 0\n        rec = None  # type: Optional[Union[dr.DirectoryRecord, udfmod.UDFFileEntry]]\n        for key in kwargs:\n            if key in ['joliet_path', 'rr_path', 'iso_path', 'udf_path']:\n                if kwargs[key] is not None:\n                    num_paths += 1\n            else:\n                raise pycdlibexception.PyCdlibInvalidInput(\"Invalid keyword, must be one of 'iso_path', 'rr_path', 'joliet_path', or 'udf_path'\")\n\n        if num_paths != 1:\n            raise pycdlibexception.PyCdlibInvalidInput(\"Must specify one, and only one of 'iso_path', 'rr_path', 'joliet_path', or 'udf_path'\")\n\n        if 'joliet_path' in kwargs:\n            joliet_path = self._normalize_joliet_path(kwargs['joliet_path'])\n            rec = self._find_joliet_record(joliet_path)\n        elif 'udf_path' in kwargs:\n            if self.udf_root is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Can only specify a UDF path for a UDF ISO')\n            (ident_unused, rec) = self._find_udf_record(utils.normpath(kwargs['udf_path']))\n            if rec is None:\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot get entry for empty UDF File Entry')\n        elif 'rr_path' in kwargs:\n            if not self.rock_ridge:\n                raise pycdlibexception.PyCdlibInvalidInput('Cannot fetch a rr_path from a non-Rock Ridge ISO')\n            rec = self._find_rr_record(utils.normpath(kwargs['rr_path']))\n        else:\n            rec = self._find_iso_record(utils.normpath(kwargs['iso_path']))\n\n        if not rec.is_file():\n            raise pycdlibexception.PyCdlibInvalidInput('Path to open must be a file')\n\n        if rec.inode is None:\n            raise pycdlibexception.PyCdlibInvalidInput('File has no data')\n\n        return PyCdlibIO(rec.inode, self.pvd.logical_block_size())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n        # type: () -> None\n        '''\n        Close the PyCdlib object, and re-initialize the object to the defaults.\n        The object can then be re-used for manipulation of another ISO.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n        if not self._initialized:\n            raise pycdlibexception.PyCdlibInvalidInput('This object is not yet initialized; call either open() or new() to create an ISO')\n\n        if self._managing_fp:\n            # In this case, we are managing self._cdfp, so we need to close it\n            self._cdfp.close()\n\n        self._initialize()", "response": "Close the PyCdlib object and re - initialize the object to the defaults."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an argument parser for the specified command.", "response": "def make_arg_parser():\n    \"\"\"Return a standard ArgumentParser object.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawTextHelpFormatter,\n        usage=\"vex [OPTIONS] VIRTUALENV_NAME COMMAND_TO_RUN ...\",\n    )\n\n    make = parser.add_argument_group(title='To make a new virtualenv')\n    make.add_argument(\n        '-m', '--make',\n        action=\"store_true\",\n        help=\"make named virtualenv before running command\"\n    )\n    make.add_argument(\n        '--python',\n        help=\"specify which python for virtualenv to be made\",\n        action=\"store\",\n        default=None,\n    )\n    make.add_argument(\n        '--site-packages',\n        help=\"allow site package imports from new virtualenv\",\n        action=\"store_true\",\n    )\n    make.add_argument(\n        '--always-copy',\n        help=\"use copies instead of symlinks in new virtualenv\",\n        action=\"store_true\",\n    )\n\n    remove = parser.add_argument_group(title='To remove a virtualenv')\n    remove.add_argument(\n        '-r', '--remove',\n        action=\"store_true\",\n        help=\"remove the named virtualenv after running command\"\n    )\n\n    parser.add_argument(\n        \"--path\",\n        metavar=\"DIR\",\n        help=\"absolute path to virtualenv to use\",\n        action=\"store\"\n    )\n    parser.add_argument(\n        '--cwd',\n        metavar=\"DIR\",\n        action=\"store\",\n        default='.',\n        help=\"path to run command in (default: '.' aka $PWD)\",\n    )\n    parser.add_argument(\n        \"--config\",\n        metavar=\"FILE\",\n        default=None,\n        action=\"store\",\n        help=\"path to config file to read (default: '~/.vexrc')\"\n    )\n    parser.add_argument(\n        '--shell-config',\n        metavar=\"SHELL\",\n        dest=\"shell_to_configure\",\n        action=\"store\",\n        default=None,\n        help=\"print optional config for the specified shell\"\n    )\n    parser.add_argument(\n        '--list',\n        metavar=\"PREFIX\",\n        nargs=\"?\",\n        const=\"\",\n        default=None,\n        help=\"print a list of available virtualenvs [matching PREFIX]\",\n        action=\"store\"\n    )\n    parser.add_argument(\n        '--version',\n        help=\"print the version of vex that is being run\",\n        action=\"store_true\"\n    )\n    parser.add_argument(\n        \"rest\",\n        nargs=argparse.REMAINDER,\n        help=argparse.SUPPRESS)\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls to parse the given list as command - line arguments.", "response": "def get_options(argv):\n    \"\"\"Called to parse the given list as command-line arguments.\n\n    :returns:\n        an options object as returned by argparse.\n    \"\"\"\n    arg_parser = make_arg_parser()\n    options, unknown = arg_parser.parse_known_args(argv)\n    if unknown:\n        arg_parser.print_help()\n        raise exceptions.UnknownArguments(\n            \"unknown args: {0!r}\".format(unknown))\n    options.print_help = arg_parser.print_help\n    return options"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_bird_conf_file(self, operation):\n        conf_updated = False\n        prefixes = []\n        ip_version = operation.ip_version\n        config_file = self.bird_configuration[ip_version]['config_file']\n        variable_name = self.bird_configuration[ip_version]['variable_name']\n        changes_counter =\\\n            self.bird_configuration[ip_version]['changes_counter']\n        dummy_ip_prefix =\\\n            self.bird_configuration[ip_version]['dummy_ip_prefix']\n\n        try:\n            prefixes = get_ip_prefixes_from_bird(config_file)\n        except OSError as error:\n            self.log.error(\"failed to open Bird configuration %s, this is a \"\n                           \"FATAL error, thus exiting main program\", error)\n            sys.exit(1)\n\n        if not prefixes:\n            self.log.error(\"found empty bird configuration %s, this is a FATAL\"\n                           \" error, thus exiting main program\", config_file)\n            sys.exit(1)\n\n        if dummy_ip_prefix not in prefixes:\n            self.log.warning(\"dummy IP prefix %s wasn't found in bird \"\n                             \"configuration, adding it. This shouldn't have \"\n                             \"happened!\", dummy_ip_prefix)\n            prefixes.insert(0, dummy_ip_prefix)\n            conf_updated = True\n\n        ip_prefixes_without_check = set(prefixes).difference(\n            self.ip_prefixes[ip_version])\n        if ip_prefixes_without_check:\n            self.log.warning(\"found %s IP prefixes in Bird configuration but \"\n                             \"we aren't configured to run health checks on \"\n                             \"them. Either someone modified the configuration \"\n                             \"manually or something went horrible wrong. We \"\n                             \"remove them from Bird configuration\",\n                             ','.join(ip_prefixes_without_check))\n            # This is faster than using lambda and filter.\n            # NOTE: We don't use remove method as we want to remove more than\n            # occurrences of the IP prefixes without check.\n            prefixes[:] = (ip for ip in prefixes\n                           if ip not in ip_prefixes_without_check)\n            conf_updated = True\n\n        # Update the list of IP prefixes based on the status of health check.\n        if operation.update(prefixes):\n            conf_updated = True\n\n        if not conf_updated:\n            self.log.info('no updates for bird configuration')\n            return conf_updated\n\n        if self.bird_configuration[ip_version]['keep_changes']:\n            archive_bird_conf(config_file, changes_counter)\n\n        # some IP prefixes are either removed or added, create\n        # configuration with new data.\n        tempname = write_temp_bird_conf(\n            dummy_ip_prefix,\n            config_file,\n            variable_name,\n            prefixes\n        )\n        try:\n            os.rename(tempname, config_file)\n        except OSError as error:\n            self.log.critical(\"failed to create Bird configuration %s, this \"\n                              \"is a FATAL error, thus exiting main program\",\n                              error)\n            sys.exit(1)\n        else:\n            self.log.info(\"Bird configuration for IPv%s is updated\",\n                          ip_version)\n\n        # dummy_ip_prefix is always there\n        if len(prefixes) == 1:\n            self.log.warning(\"Bird configuration doesn't have IP prefixes for \"\n                             \"any of the services we monitor! It means local \"\n                             \"node doesn't receive any traffic\")\n\n        return conf_updated", "response": "Update the BIRD configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlunch checks and triggers updates on BIRD configuration.", "response": "def run(self):\n        \"\"\"Lunch checks and triggers updates on BIRD configuration.\"\"\"\n        # Lunch a thread for each configuration\n        if not self.services:\n            self.log.warning(\"no service checks are configured\")\n        else:\n            self.log.info(\"going to lunch %s threads\", len(self.services))\n            if self.config.has_option('daemon', 'splay_startup'):\n                splay_startup = self.config.getfloat('daemon', 'splay_startup')\n            else:\n                splay_startup = None\n\n            for service in self.services:\n                self.log.debug(\"lunching thread for %s\", service)\n                _config = {}\n                for option, getter in SERVICE_OPTIONS_TYPE.items():\n                    try:\n                        _config[option] = getattr(self.config, getter)(service,\n                                                                       option)\n                    except NoOptionError:\n                        pass  # for optional settings\n\n                _thread = ServiceCheck(service, _config, self.action,\n                                       splay_startup)\n                _thread.start()\n\n        # Stay running until we are stopped\n        while True:\n            # Fetch items from action queue\n            operation = self.action.get(block=True)\n\n            if isinstance(operation, ServiceCheckDiedError):\n                self.log.critical(operation)\n                self.log.critical(\"This is a fatal error and the only way to \"\n                                  \"recover is to restart, thus exiting with a \"\n                                  \"non-zero code and let systemd act by \"\n                                  \"triggering a restart\")\n                sys.exit(1)\n\n            self.log.info(\"returned an item from the queue for %s with IP \"\n                          \"prefix %s and action to %s Bird configuration\",\n                          operation.name,\n                          operation.ip_prefix,\n                          operation)\n            bird_updated = self._update_bird_conf_file(operation)\n            self.action.task_done()\n            if bird_updated:\n                ip_version = operation.ip_version\n                if operation.bird_reconfigure_cmd is None:\n                    reconfigure_bird(\n                        self.bird_configuration[ip_version]['reconfigure_cmd'])\n                else:\n                    run_custom_bird_reconfigure(operation)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the given IP - Prefix is a valid DNS record.", "response": "def valid_ip_prefix(ip_prefix):\n    \"\"\"Perform a sanity check on ip_prefix.\n\n    Arguments:\n        ip_prefix (str): The IP-Prefix to validate\n\n    Returns:\n        True if ip_prefix is a valid IPv4 address with prefix length 32 or a\n        valid IPv6 address with prefix length 128, otherwise False\n\n    \"\"\"\n    try:\n        ip_prefix = ipaddress.ip_network(ip_prefix)\n    except ValueError:\n        return False\n    else:\n        if ip_prefix.version == 4 and ip_prefix.max_prefixlen != 32:\n            return False\n        if ip_prefix.version == 6 and ip_prefix.max_prefixlen != 128:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a set of IP prefixes found in service configuration files.", "response": "def get_ip_prefixes_from_config(config, services, ip_version):\n    \"\"\"Build a set of IP prefixes found in service configuration files.\n\n    Arguments:\n        config (obg): A configparser object which holds our configuration.\n        services (list): A list of section names which are the name of the\n        service checks.\n        ip_version (int): IP protocol version\n\n    Returns:\n        A set of IP prefixes.\n\n    \"\"\"\n    ip_prefixes = set()\n\n    for service in services:\n        ip_prefix = ipaddress.ip_network(config.get(service, 'ip_prefix'))\n        if ip_prefix.version == ip_version:\n            ip_prefixes.add(ip_prefix.with_prefixlen)\n\n    return ip_prefixes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ip_prefixes_sanity_check(config, bird_configuration):\n    for ip_version in bird_configuration:\n        modify_ip_prefixes(config,\n                           bird_configuration[ip_version]['config_file'],\n                           bird_configuration[ip_version]['variable_name'],\n                           bird_configuration[ip_version]['dummy_ip_prefix'],\n                           bird_configuration[ip_version]['reconfigure_cmd'],\n                           bird_configuration[ip_version]['keep_changes'],\n                           bird_configuration[ip_version]['changes_counter'],\n                           ip_version)", "response": "Sanity check on IP prefixes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmodifying the IP prefixes in Bird configuration.", "response": "def modify_ip_prefixes(\n        config,\n        config_file,\n        variable_name,\n        dummy_ip_prefix,\n        reconfigure_cmd,\n        keep_changes,\n        changes_counter,\n        ip_version):\n    \"\"\"Modify IP prefixes in Bird configuration.\n\n    Depending on the configuration either removes or reports IP prefixes found\n    in Bird configuration for which we don't have a service check associated\n    with them. Moreover, it adds the dummy IP prefix if it isn't present and\n    ensures that the correct variable name is set.\n\n    Arguments:\n        config (obg): A configparser object which holds our configuration.\n        config_file (str): The file name of bird configuration\n        variable_name (str): The name of the variable set in bird configuration\n        dummy_ip_prefix (str): The dummy IP prefix, which must be always\n        reconfigure_cmd (str): The command to run to trigger a reconfiguration\n        on Bird daemon upon successful configuration update\n        keep_changes (boolean): To enable keeping a history of changes applied\n        to bird configuration\n        changes_counter (int): The number of configuration changes to keep\n        ip_version (int): IP protocol version of Bird configuration\n\n    \"\"\"\n    log = logging.getLogger(PROGRAM_NAME)\n    services = config.sections()\n    services.remove('daemon')  # not needed during sanity check for IP-Prefixes\n    update_bird_conf = False\n    try:\n        ip_prefixes_in_bird = get_ip_prefixes_from_bird(config_file)\n    except OSError as error:\n        log.error(\"failed to open Bird configuration %s, this is a FATAL \"\n                  \"error, thus exiting main program\", error)\n        sys.exit(1)\n\n    _name = get_variable_name_from_bird(config_file)\n    if _name is None:\n        log.warning(\"failed to find variable name in %s, going to add it\",\n                    config_file)\n        update_bird_conf = True\n    elif _name != variable_name:\n        log.warning(\"found incorrect variable name in %s, going to add the \"\n                    \"correct one %s\", _name, variable_name)\n        update_bird_conf = True\n\n    if dummy_ip_prefix not in ip_prefixes_in_bird:\n        log.warning(\"dummy IP prefix %s is missing from bird configuration \"\n                    \"%s, adding it\", dummy_ip_prefix, config_file)\n        ip_prefixes_in_bird.insert(0, dummy_ip_prefix)\n        update_bird_conf = True\n\n    # Find IP prefixes in Bird configuration without a check.\n    ip_prefixes_with_check = get_ip_prefixes_from_config(\n        config,\n        services,\n        ip_version)\n    # dummy_ip_prefix doesn't have a config by design\n    ip_prefixes_with_check.add(dummy_ip_prefix)\n\n    ip_prefixes_without_check = set(ip_prefixes_in_bird).difference(\n        ip_prefixes_with_check)\n\n    if ip_prefixes_without_check:\n        if config.getboolean('daemon', 'purge_ip_prefixes'):\n            log.warning(\"removing IP prefix(es) %s from %s because they don't \"\n                        \"have a service check configured\",\n                        ','.join(ip_prefixes_without_check),\n                        config_file)\n            ip_prefixes_in_bird[:] = (ip for ip in ip_prefixes_in_bird\n                                      if ip not in ip_prefixes_without_check)\n            update_bird_conf = True\n        else:\n            log.warning(\"found IP prefixes %s in %s without a service \"\n                        \"check configured\",\n                        ','.join(ip_prefixes_without_check),\n                        config_file)\n\n    if update_bird_conf:\n        if keep_changes:\n            archive_bird_conf(config_file, changes_counter)\n        tempname = write_temp_bird_conf(\n            dummy_ip_prefix,\n            config_file,\n            variable_name,\n            ip_prefixes_in_bird\n        )\n        try:\n            os.rename(tempname, config_file)\n        except OSError as error:\n            msg = (\"CRITICAL: failed to create Bird configuration {e}, \"\n                   \"this is FATAL error, thus exiting main program\"\n                   .format(e=error))\n            sys.exit(\"{m}\".format(m=msg))\n        else:\n            log.info(\"Bird configuration for IPv%s is updated\", ip_version)\n            reconfigure_bird(reconfigure_cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a configuration file and create a bird configuration dictionary.", "response": "def load_configuration(config_file, config_dir, service_file):\n    \"\"\"Build configuration objects.\n\n    If all sanity checks against daemon and service check settings are passed\n    then it builds a ConfigParser object which holds all our configuration\n    and a dictionary data structure which holds Bird configuration per IP\n    protocol version.\n\n    Arguments:\n        config_file (str): The file name which holds daemon settings\n        config_dir (str): The directory name which has configuration files\n        for each service check\n        service_file (str): A file which contains configuration for a single\n        service check\n\n    Returns:\n        A tuple with 1st element a ConfigParser object and 2nd element\n        a dictionary.\n    Raises:\n        ValueError if a sanity check fails.\n\n    \"\"\"\n    config_files = [config_file]\n    config = configparser.ConfigParser()\n    config.read_dict(DEFAULT_OPTIONS)\n\n    if not os.path.isfile(config_file):\n        raise ValueError(\"{f} configuration file either isn't readable or \"\n                         \"doesn't exist\".format(f=config_file))\n    if service_file is not None:\n        if not os.path.isfile(service_file):\n            raise ValueError(\"{f} configuration file for a service check \"\n                             \"doesn't exist\".format(f=service_file))\n        else:\n            config_files.append(service_file)\n    elif config_dir is not None:\n        if not os.path.isdir(config_dir):\n            raise ValueError(\"{d} directory with configuration files for \"\n                             \"service checks doesn't exist\"\n                             .format(d=config_dir))\n        else:\n            config_files.extend(glob.glob(os.path.join(config_dir, '*.conf')))\n\n    try:\n        config.read(config_files)\n    except configparser.Error as exc:\n        raise ValueError(exc)\n\n    configuration_check(config)\n    bird_configuration = build_bird_configuration(config)\n    create_bird_config_files(bird_configuration)\n\n    return config, bird_configuration"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef configuration_check(config):\n    log_level = config.get('daemon', 'loglevel')\n    num_level = getattr(logging, log_level.upper(), None)\n    pidfile = config.get('daemon', 'pidfile')\n\n    # Catch the case where the directory, under which we store the pid file, is\n    # missing.\n    if not os.path.isdir(os.path.dirname(pidfile)):\n        raise ValueError(\"{d} doesn't exit\".format(d=os.path.dirname(pidfile)))\n\n    if not isinstance(num_level, int):\n        raise ValueError('Invalid log level: {}'.format(log_level))\n\n    for _file in 'log_file', 'stderr_file':\n        if config.has_option('daemon', _file):\n            try:\n                touch(config.get('daemon', _file))\n            except OSError as exc:\n                raise ValueError(exc)\n\n    for option, getter in DAEMON_OPTIONS_TYPE.items():\n        try:\n            getattr(config, getter)('daemon', option)\n        except configparser.NoOptionError as error:\n            if option not in DAEMON_OPTIONAL_OPTIONS:\n                raise ValueError(error)\n        except configparser.Error as error:\n            raise ValueError(error)\n        except ValueError as exc:\n            msg = (\"invalid data for '{opt}' option in daemon section: {err}\"\n                   .format(opt=option, err=exc))\n            raise ValueError(msg)\n\n    service_configuration_check(config)", "response": "Perform a sanity check on the configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef service_configuration_check(config):\n    ipv4_enabled = config.getboolean('daemon', 'ipv4')\n    ipv6_enabled = config.getboolean('daemon', 'ipv6')\n    services = config.sections()\n    # we don't need it during sanity check for services check\n    services.remove('daemon')\n    ip_prefixes = []\n\n    for service in services:\n        for option, getter in SERVICE_OPTIONS_TYPE.items():\n            try:\n                getattr(config, getter)(service, option)\n            except configparser.NoOptionError as error:\n                if option not in SERVICE_OPTIONAL_OPTIONS:\n                    raise ValueError(error)\n            except configparser.Error as error:\n                raise ValueError(error)\n            except ValueError as exc:\n                msg = (\"invalid data for '{opt}' option in service check \"\n                       \"{name}: {err}\"\n                       .format(opt=option, name=service, err=exc))\n                raise ValueError(msg)\n\n        if (config.get(service, 'on_disabled') != 'withdraw' and\n                config.get(service, 'on_disabled') != 'advertise'):\n            msg = (\"'on_disabled' option has invalid value ({val}) for \"\n                   \"service check {name}, 'on_disabled option should be set \"\n                   \"either to 'withdraw' or to 'advertise'\"\n                   .format(name=service,\n                           val=config.get(service, 'on_disabled')))\n            raise ValueError(msg)\n\n        ip_prefixes.append(config.get(service, 'ip_prefix'))\n\n        if not valid_ip_prefix(config.get(service, 'ip_prefix')):\n            msg = (\"invalid value ({val}) for 'ip_prefix' option in service \"\n                   \"check {name}. It should be an IP PREFIX in form of \"\n                   \"ip/prefixlen.\"\n                   .format(name=service, val=config.get(service, 'ip_prefix')))\n            raise ValueError(msg)\n\n        _ip_prefix = ipaddress.ip_network(config.get(service, 'ip_prefix'))\n        if not ipv6_enabled and _ip_prefix.version == 6:\n            raise ValueError(\"IPv6 support is disabled in \"\n                             \"anycast-healthchecker while there is an IPv6 \"\n                             \"prefix configured for {name} service check\"\n                             .format(name=service))\n        if not ipv4_enabled and _ip_prefix.version == 4:\n            raise ValueError(\"IPv4 support is disabled in \"\n                             \"anycast-healthchecker while there is an IPv4 \"\n                             \"prefix configured for {name} service check\"\n                             .format(name=service))\n\n        cmd = shlex.split(config.get(service, 'check_cmd'))\n        try:\n            proc = subprocess.Popen(cmd)\n            proc.kill()\n        except (OSError, subprocess.SubprocessError) as exc:\n            msg = (\"failed to run check command '{cmd}' for service check \"\n                   \"{name}: {err}\"\n                   .format(name=service,\n                           cmd=config.get(service, 'check_cmd'),\n                           err=exc))\n            raise ValueError(msg)\n\n    occurrences_of_ip_prefixes = Counter(ip_prefixes)\n    for ip_prefix, counter in occurrences_of_ip_prefixes.items():\n        if counter > 1:\n            raise ValueError(\"{ip} is used by {c} service checks\"\n                             .format(ip=ip_prefix, c=counter))", "response": "Perform a sanity check against the options for each service check."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a bird configuration structure.", "response": "def build_bird_configuration(config):\n    \"\"\"Build bird configuration structure.\n\n    First it performs a sanity check against bird settings and then builds a\n    dictionary structure with bird configuration per IP version.\n\n    Arguments:\n        config (obj): A configparser object which holds our configuration.\n\n    Returns:\n        A dictionary\n\n    Raises:\n        ValueError if sanity check fails.\n\n    \"\"\"\n    bird_configuration = {}\n\n    if config.getboolean('daemon', 'ipv4'):\n        if os.path.islink(config.get('daemon', 'bird_conf')):\n            config_file = os.path.realpath(config.get('daemon', 'bird_conf'))\n            print(\"'bird_conf' is set to a symbolic link ({s} -> {d}, but we \"\n                  \"will use the canonical path of that link\"\n                  .format(s=config.get('daemon', 'bird_conf'), d=config_file))\n        else:\n            config_file = config.get('daemon', 'bird_conf')\n\n        dummy_ip_prefix = config.get('daemon', 'dummy_ip_prefix')\n        if not valid_ip_prefix(dummy_ip_prefix):\n            raise ValueError(\"invalid dummy IPv4 prefix: {i}\"\n                             .format(i=dummy_ip_prefix))\n\n        bird_configuration[4] = {\n            'config_file': config_file,\n            'variable_name': config.get('daemon', 'bird_variable'),\n            'dummy_ip_prefix': dummy_ip_prefix,\n            'reconfigure_cmd': config.get('daemon', 'bird_reconfigure_cmd'),\n            'keep_changes': config.getboolean('daemon', 'bird_keep_changes'),\n            'changes_counter': config.getint('daemon', 'bird_changes_counter')\n        }\n    if config.getboolean('daemon', 'ipv6'):\n        if os.path.islink(config.get('daemon', 'bird6_conf')):\n            config_file = os.path.realpath(config.get('daemon', 'bird6_conf'))\n            print(\"'bird6_conf' is set to a symbolic link ({s} -> {d}, but we \"\n                  \"will use the canonical path of that link\"\n                  .format(s=config.get('daemon', 'bird6_conf'), d=config_file))\n        else:\n            config_file = config.get('daemon', 'bird6_conf')\n\n        dummy_ip_prefix = config.get('daemon', 'dummy_ip6_prefix')\n        if not valid_ip_prefix(dummy_ip_prefix):\n            raise ValueError(\"invalid dummy IPv6 prefix: {i}\"\n                             .format(i=dummy_ip_prefix))\n        bird_configuration[6] = {\n            'config_file': config_file,\n            'variable_name': config.get('daemon', 'bird6_variable'),\n            'dummy_ip_prefix': dummy_ip_prefix,\n            'reconfigure_cmd': config.get('daemon', 'bird6_reconfigure_cmd'),\n            'keep_changes': config.getboolean('daemon', 'bird6_keep_changes'),\n            'changes_counter': config.getint('daemon', 'bird6_changes_counter')\n        }\n\n    return bird_configuration"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_variable_name_from_bird(bird_conf):\n    bird_variable_pattern = re.compile(\n        r'''\n        ^\\s*\n        define\\s+\n        (?P<name>\\S+\\b)\n        \\s+\n        =\n        ''', re.VERBOSE\n    )\n\n    with open(bird_conf, 'r') as content:\n        for line in content.readlines():\n            variable_match = bird_variable_pattern.search(line)\n            if variable_match:\n                return variable_match.group('name')\n\n    return None", "response": "Return the variable name set in Bird configuration."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_bird_config_files(bird_configuration):\n    for ip_version in bird_configuration:\n        # This creates the file if it doesn't exist.\n        config_file = bird_configuration[ip_version]['config_file']\n        try:\n            touch(config_file)\n        except OSError as exc:\n            raise ValueError(\"failed to create {f}:{e}\"\n                             .format(f=config_file, e=exc))\n        if bird_configuration[ip_version]['keep_changes']:\n            history_dir = os.path.join(os.path.dirname(config_file), 'history')\n            try:\n                os.mkdir(history_dir)\n            except FileExistsError:\n                pass\n            except OSError as exc:\n                raise ValueError(\"failed to make directory {d} for keeping a \"\n                                 \"history of changes for {b}:{e}\"\n                                 .format(d=history_dir, b=config_file, e=exc))\n            else:\n                print(\"{d} is created\".format(d=history_dir))", "response": "Create bird configuration files per IP version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the validity of a process ID.", "response": "def running(processid):\n    \"\"\"Check the validity of a process ID.\n\n    Arguments:\n        processid (int): Process ID number.\n\n    Returns:\n        True if process ID is found otherwise False.\n\n    \"\"\"\n    try:\n        # From kill(2)\n        #   If sig is 0 (the null signal), error checking is performed but no\n        #   signal is actually sent. The null signal can be used to check the\n        #   validity of pid\n        os.kill(processid, 0)\n    except OverflowError as exc:\n        print(\"checking validity of pid ({p}) failed with: {e}\"\n              .format(p=processid, e=exc))\n        sys.exit(1)\n    except OSError:\n        return False\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ip_prefixes_from_bird(filename):\n    prefixes = []\n    with open(filename, 'r') as bird_conf:\n        lines = bird_conf.read()\n\n    for line in lines.splitlines():\n        line = line.strip(', ')\n        if valid_ip_prefix(line):\n            prefixes.append(line)\n\n    return prefixes", "response": "Build a list of IP prefixes found in Bird configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reconfigure_bird(cmd):\n    log = logging.getLogger(PROGRAM_NAME)\n    cmd = shlex.split(cmd)\n    log.info(\"reconfiguring BIRD by running %s\", ' '.join(cmd))\n    try:\n        output = subprocess.check_output(\n            cmd,\n            timeout=2,\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n            )\n    except subprocess.TimeoutExpired:\n        log.error(\"reconfiguring bird timed out\")\n        return\n    except subprocess.CalledProcessError as error:\n        # birdc returns 0 even when it fails due to invalid config,\n        # but it returns 1 when BIRD is down.\n        log.error(\"reconfiguring BIRD failed, either BIRD daemon is down or \"\n                  \"we don't have privileges to reconfigure it (sudo problems?)\"\n                  \":%s\", error.output.strip())\n        return\n    except FileNotFoundError as error:\n        log.error(\"reconfiguring BIRD failed with: %s\", error)\n        return\n\n    # 'Reconfigured' string will be in the output if and only if conf is valid.\n    pattern = re.compile('^Reconfigured$', re.MULTILINE)\n    if pattern.search(str(output)):\n        log.info('reconfigured BIRD daemon')\n    else:\n        # We will end up here only if we generated an invalid conf\n        # or someone broke bird.conf.\n        log.error(\"reconfiguring BIRD returned error, most likely we generated\"\n                  \" an invalid configuration file or Bird configuration in is \"\n                  \"broken:%s\", output)", "response": "Reconfigure the internal BIRD structure."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_temp_bird_conf(dummy_ip_prefix,\n                         config_file,\n                         variable_name,\n                         prefixes):\n    \"\"\"Write in a temporary file the list of IP-Prefixes.\n\n    A failure to create and write the temporary file will exit main program.\n\n    Arguments:\n        dummy_ip_prefix (str): The dummy IP prefix, which must be always\n        config_file (str): The file name of bird configuration\n        variable_name (str): The name of the variable set in bird configuration\n        prefixes (list): The list of IP-Prefixes to write\n\n    Returns:\n        The filename of the temporary file\n\n    \"\"\"\n    log = logging.getLogger(PROGRAM_NAME)\n    comment = (\"# {i} is a dummy IP Prefix. It should NOT be used and \"\n               \"REMOVED from the constant.\".format(i=dummy_ip_prefix))\n\n    # the temporary file must be on the same filesystem as the bird config\n    # as we use os.rename to perform an atomic update on the bird config.\n    # Thus, we create it in the same directory that bird config is stored.\n    tm_file = os.path.join(os.path.dirname(config_file), str(time.time()))\n    log.debug(\"going to write to %s\", tm_file)\n\n    try:\n        with open(tm_file, 'w') as tmpf:\n            tmpf.write(\"# Generated {t} by {n} (pid={p})\\n\"\n                       .format(t=datetime.datetime.now(),\n                               n=PROGRAM_NAME,\n                               p=os.getpid()))\n            tmpf.write(\"{c}\\n\".format(c=comment))\n            tmpf.write(\"define {n} =\\n\".format(n=variable_name))\n            tmpf.write(\"{s}[\\n\".format(s=4 * ' '))\n            # all entries of the array need a trailing comma except the last\n            # one. A single element array doesn't need a trailing comma.\n            tmpf.write(',\\n'.join([' '*8 + n for n in prefixes]))\n            tmpf.write(\"\\n{s}];\\n\".format(s=4 * ' '))\n    except OSError as error:\n        log.critical(\"failed to write temporary file %s: %s. This is a FATAL \"\n                     \"error, this exiting main program\", tm_file, error)\n        sys.exit(1)\n    else:\n        return tm_file", "response": "Write in a temporary file the list of IP - prefixes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef archive_bird_conf(config_file, changes_counter):\n    log = logging.getLogger(PROGRAM_NAME)\n    history_dir = os.path.join(os.path.dirname(config_file), 'history')\n    dst = os.path.join(history_dir, str(time.time()))\n    log.debug(\"coping %s to %s\", config_file, dst)\n\n    history = [x for x in os.listdir(history_dir)\n               if os.path.isfile(os.path.join(history_dir, x))]\n\n    if len(history) > changes_counter:\n        log.info(\"threshold of %s is reached, removing old files\",\n                 changes_counter)\n        for _file in sorted(history, reverse=True)[changes_counter - 1:]:\n            _path = os.path.join(history_dir, _file)\n            try:\n                os.remove(_path)\n            except OSError as exc:\n                log.warning(\"failed to remove %s: %s\", _file, exc)\n            else:\n                log.info(\"removed %s\", _path)\n\n    try:\n        shutil.copy2(config_file, dst)\n    except OSError as exc:\n        log.warning(\"failed to copy %s to %s: %s\", config_file, dst, exc)", "response": "Archive the Bird configuration files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_pidfile(pidfile):\n    try:\n        with open(pidfile, mode='r') as _file:\n            pid = _file.read(1024).rstrip()\n\n        try:\n            pid = int(pid)\n        except ValueError:\n            print(\"cleaning stale pidfile with invalid data:'{}'\".format(pid))\n            write_pid(pidfile)\n        else:\n            if running(pid):\n                # This is to catch migration issues from 0.7.x to 0.8.x\n                # version, where old process is still around as it failed to\n                # be stopped. Since newer version has a different locking\n                # mechanism, we can end up with both versions running.\n                # In order to avoid this situation we refuse to startup.\n                sys.exit(\"process {} is already running\".format(pid))\n            else:\n                # pidfile exists with a PID for a process that is not running.\n                # Let's update PID.\n                print(\"updating stale processID({}) in pidfile\".format(pid))\n                write_pid(pidfile)\n    except FileNotFoundError:\n        # Either it's 1st time we run or previous run was terminated\n        # successfully.\n        print(\"creating pidfile {f}\".format(f=pidfile))\n        write_pid(pidfile)\n    except OSError as exc:\n        sys.exit(\"failed to update pidfile:{e}\".format(e=exc))", "response": "Update the pidfile with the contents of the specified file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite processID to the pidfile", "response": "def write_pid(pidfile):\n    \"\"\"Write processID to the pidfile.\n\n    Notice:\n        It exits main program if it fails to write pidfile.\n\n    Arguments:\n        pidfile (str): pidfile to update\n\n    \"\"\"\n    pid = str(os.getpid())\n    try:\n        with open(pidfile, mode='w') as _file:\n            print(\"writing processID {p} to pidfile\".format(p=pid))\n            _file.write(pid)\n    except OSError as exc:\n        sys.exit(\"failed to write pidfile:{e}\".format(e=exc))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shutdown(pidfile, signalnb=None, frame=None):\n    log = logging.getLogger(PROGRAM_NAME)\n    log.info(\"received %s at %s\", signalnb, frame)\n    log.info(\"going to remove pidfile %s\", pidfile)\n    # no point to catch possible errors when we delete the pid file\n    os.unlink(pidfile)\n    log.info('shutdown is complete')\n    sys.exit(0)", "response": "Clean up pidfile upon shutdown."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconfiguring the logging environment.", "response": "def setup_logger(config):\n    \"\"\"Configure the logging environment.\n\n    Notice:\n        By default logging will go to STDOUT and messages for unhandled\n        exceptions or crashes will go to STDERR. If log_file and/or log_server\n        is set then we don't log to STDOUT. Messages for unhandled exceptions\n        or crashes can only go to either STDERR or to stderr_file or to\n        stderr_log_server.\n\n    Arguments:\n        config (obj): A configparser object which holds our configuration.\n\n    Returns:\n        A logger with all possible handlers configured.\n\n    \"\"\"\n    logger = logging.getLogger(PROGRAM_NAME)\n    num_level = getattr(\n        logging,\n        config.get('daemon', 'loglevel').upper(),  # pylint: disable=no-member\n        None\n    )\n    logger.setLevel(num_level)\n    lengths = []\n    for section in config:\n        lengths.append(len(section))\n\n    width = sorted(lengths)[-1] + 1\n\n    def log_format():\n        \"\"\"Produce a log format line.\"\"\"\n        supported_keys = [\n            'asctime',\n            'levelname',\n            'process',\n            # 'funcName',\n            # 'lineno',\n            'threadName',\n            'message',\n        ]\n\n        return ' '.join(['%({0:s})'.format(i) for i in supported_keys])\n\n    custom_format = log_format()\n    json_formatter = CustomJsonFormatter(custom_format,\n                                         prefix=PROGRAM_NAME + ': ')\n    formatter = logging.Formatter(\n        '%(asctime)s {program}[%(process)d] %(levelname)-8s '\n        '%(threadName)-{width}s %(message)s'\n        .format(program=PROGRAM_NAME, width=width)\n    )\n\n    # Register logging handlers based on configuration.\n    if config.has_option('daemon', 'log_file'):\n        file_handler = logging.handlers.RotatingFileHandler(\n            config.get('daemon', 'log_file'),\n            maxBytes=config.getint('daemon', 'log_maxbytes'),\n            backupCount=config.getint('daemon', 'log_backups')\n        )\n\n        if config.getboolean('daemon', 'json_log_file'):\n            file_handler.setFormatter(json_formatter)\n        else:\n            file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    if config.has_option('daemon', 'log_server'):\n        udp_handler = logging.handlers.SysLogHandler(\n            (\n                config.get('daemon', 'log_server'),\n                config.getint('daemon', 'log_server_port')\n            )\n        )\n\n        if config.getboolean('daemon', 'json_log_server'):\n            udp_handler.setFormatter(json_formatter)\n        else:\n            udp_handler.setFormatter(formatter)\n        logger.addHandler(udp_handler)\n\n    # Log to STDOUT if and only if log_file and log_server aren't enabled\n    if (not config.has_option('daemon', 'log_file')\n            and not config.has_option('daemon', 'log_server')):\n        stream_handler = logging.StreamHandler()\n        if config.getboolean('daemon', 'json_stdout'):\n            stream_handler.setFormatter(json_formatter)\n        else:\n            stream_handler.setFormatter(formatter)\n        logger.addHandler(stream_handler)\n\n    # We can redirect STDERR only to one destination.\n    if config.has_option('daemon', 'stderr_file'):\n        sys.stderr = CustomRotatingFileLogger(\n            filepath=config.get('daemon', 'stderr_file'),\n            maxbytes=config.getint('daemon', 'log_maxbytes'),\n            backupcount=config.getint('daemon', 'log_backups')\n        )\n    elif (config.has_option('daemon', 'stderr_log_server')\n          and not config.has_option('daemon', 'stderr_file')):\n        sys.stderr = CustomUdpLogger(\n            server=config.get('daemon', 'log_server'),\n            port=config.getint('daemon', 'log_server_port')\n        )\n    else:\n        print('messages for unhandled exceptions will go to STDERR')\n\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreconfigure BIRD daemon by running a custom command.", "response": "def run_custom_bird_reconfigure(operation):\n    \"\"\"Reconfigure BIRD daemon by running a custom command.\n\n    It adds one argument to the command, either \"up\" or \"down\".\n    If command times out then we kill it. In order to avoid leaving any orphan\n    processes, that may have been started by the command, we start a new\n    session when we invoke the command and then we kill process group of that\n    session.\n\n    Arguments:\n        operation (obj): Either a AddOperation or DeleteOperation object.\n\n    \"\"\"\n    log = logging.getLogger(PROGRAM_NAME)\n    if isinstance(operation, AddOperation):\n        status = 'up'\n    else:\n        status = 'down'\n    cmd = shlex.split(operation.bird_reconfigure_cmd + \" \" + status)\n    log.info(\"reconfiguring BIRD by running custom command %s\", ' '.join(cmd))\n    try:\n        proc = subprocess.Popen(cmd,\n                                start_new_session=True,\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE)\n        _, errs = proc.communicate(\n            timeout=operation.bird_reconfigure_timeout\n        )\n    except OSError as exc:\n        log.error(\"reconfiguring BIRD failed with: %s\", exc)\n    except subprocess.TimeoutExpired as exc:\n        log.error(\"reconfiguring bird timed out\")\n        if proc.poll() is None:  # if process is still alive\n            try:\n                os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n            except PermissionError as exc:\n                log.error(\"failed to terminate custom bird command: %s\", exc)\n    else:\n        if proc.returncode != 0:\n            log.error(\"reconfiguring BIRD failed with return code: %s and \"\n                      \"stderr: %s\", proc.returncode, errs)\n        else:\n            log.info(\"custom command successfully reconfigured Bird\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, prefixes):\n        if self.ip_prefix not in prefixes:\n            prefixes.append(self.ip_prefix)\n            self.log.info(\"announcing %s for %s\", self.ip_prefix, self.name)\n            return True\n\n        return False", "response": "Adds a value to the list. Returns True if the value was added False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, string):\n        string = string.rstrip()\n        if string:  # Don't log empty lines\n            self.logger.critical(string)", "response": "Erase newline from a string and write to the logger."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd customer record keys and rename threadName key.", "response": "def process_log_record(self, log_record):\n        \"\"\"Add customer record keys and rename threadName key.\"\"\"\n        log_record[\"version\"] = __version__\n        log_record[\"program\"] = PROGRAM_NAME\n        log_record[\"service_name\"] = log_record.pop('threadName', None)\n        # return jsonlogger.JsonFormatter.process_log_record(self, log_record)\n\n        return log_record"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a representation of the contents of the config file.", "response": "def get_vexrc(options, environ):\n    \"\"\"Get a representation of the contents of the config file.\n\n    :returns:\n        a Vexrc instance.\n    \"\"\"\n    # Complain if user specified nonexistent file with --config.\n    # But we don't want to complain just because ~/.vexrc doesn't exist.\n    if options.config and not os.path.exists(options.config):\n        raise exceptions.InvalidVexrc(\"nonexistent config: {0!r}\".format(options.config))\n    filename = options.config or os.path.expanduser('~/.vexrc')\n    vexrc = config.Vexrc.from_file(filename, environ)\n    return vexrc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndiscovering what directory the command should run in.", "response": "def get_cwd(options):\n    \"\"\"Discover what directory the command should run in.\n    \"\"\"\n    if not options.cwd:\n        return None\n    if not os.path.exists(options.cwd):\n        raise exceptions.InvalidCwd(\n            \"can't --cwd to invalid path {0!r}\".format(options.cwd))\n    return options.cwd"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking a virtualenv path and return it.", "response": "def get_virtualenv_path(ve_base, ve_name):\n    \"\"\"Check a virtualenv path, raising exceptions to explain problems.\n    \"\"\"\n    if not ve_base:\n        raise exceptions.NoVirtualenvsDirectory(\n            \"could not figure out a virtualenvs directory. \"\n            \"make sure $HOME is set, or $WORKON_HOME,\"\n            \" or set virtualenvs=something in your .vexrc\")\n\n    # Using this requires get_ve_base to pass through nonexistent dirs\n    if not os.path.exists(ve_base):\n        message = (\n            \"virtualenvs directory {0!r} not found. \"\n            \"Create it or use vex --make to get started.\"\n        ).format(ve_base)\n        raise exceptions.NoVirtualenvsDirectory(message)\n\n    if not ve_name:\n        raise exceptions.InvalidVirtualenv(\"no virtualenv name\")\n\n    # n.b.: if ve_name is absolute, ve_base is discarded by os.path.join,\n    # and an absolute path will be accepted as first arg.\n    # So we check if they gave an absolute path as ve_name.\n    # But we don't want this error if $PWD == $WORKON_HOME,\n    # in which case 'foo' is a valid relative path to virtualenv foo.\n    ve_path = os.path.join(ve_base, ve_name)\n    if ve_path == ve_name and os.path.basename(ve_name) != ve_name:\n        raise exceptions.InvalidVirtualenv(\n            'To run in a virtualenv by its path, '\n            'use \"vex --path {0}\"'.format(ve_path))\n\n    ve_path = os.path.abspath(ve_path)\n    if not os.path.exists(ve_path):\n        raise exceptions.InvalidVirtualenv(\n            \"no virtualenv found at {0!r}.\".format(ve_path))\n    return ve_path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a command to run.", "response": "def get_command(options, vexrc, environ):\n    \"\"\"Get a command to run.\n\n    :returns:\n        a list of strings representing a command to be passed to Popen.\n    \"\"\"\n    command = options.rest\n    if not command:\n        command = vexrc.get_shell(environ)\n    if command and command[0].startswith('--'):\n        raise exceptions.InvalidCommand(\n            \"don't put flags like '%s' after the virtualenv name.\"\n            % command[0])\n    if not command:\n        raise exceptions.InvalidCommand(\"no command given\")\n    return command"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _main(environ, argv):\n    options = get_options(argv)\n    if options.version:\n        return handle_version()\n    vexrc = get_vexrc(options, environ)\n    # Handle --shell-config as soon as its arguments are available.\n    if options.shell_to_configure:\n        return handle_shell_config(options.shell_to_configure, vexrc, environ)\n    if options.list is not None:\n        return handle_list(vexrc.get_ve_base(environ), options.list)\n\n    # Do as much as possible before a possible make, so errors can raise\n    # without leaving behind an unused virtualenv.\n    # get_virtualenv_name is destructive and must happen before get_command\n    cwd = get_cwd(options)\n    ve_base = vexrc.get_ve_base(environ)\n    ve_name = get_virtualenv_name(options)\n    command = get_command(options, vexrc, environ)\n    # Either we create ve_path, get it from options.path or find it\n    # in ve_base.\n    if options.make:\n        if options.path:\n            make_path = os.path.abspath(options.path)\n        else:\n            make_path = os.path.abspath(os.path.join(ve_base, ve_name))\n        handle_make(environ, options, make_path)\n        ve_path = make_path\n    elif options.path:\n        ve_path = os.path.abspath(options.path)\n        if not os.path.exists(ve_path) or not os.path.isdir(ve_path):\n            raise exceptions.InvalidVirtualenv(\n                \"argument for --path is not a directory\")\n    else:\n        try:\n            ve_path = get_virtualenv_path(ve_base, ve_name)\n        except exceptions.NoVirtualenvName:\n            options.print_help()\n            raise\n    # get_environ has to wait until ve_path is defined, which might\n    # be after a make; of course we can't run until we have env.\n    env = get_environ(environ, vexrc['env'], ve_path)\n    returncode = run(command, env=env, cwd=cwd)\n    if options.remove:\n        handle_remove(ve_path)\n    if returncode is None:\n        raise exceptions.InvalidCommand(\n            \"command not found: {0!r}\".format(command[0]))\n    return returncode", "response": "Main entry point for the command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_processid(config):\n    pidfile = config.get('daemon', 'pidfile', fallback=None)\n    if pidfile is None:\n        raise ValueError(\"Configuration doesn't have pidfile option!\")\n\n    try:\n        with open(pidfile, 'r') as _file:\n            pid = _file.read().rstrip()\n            try:\n                pid = int(pid)\n            except ValueError:\n                raise ValueError(\"stale pid file with invalid data:{}\"\n                                 .format(pid))\n            else:\n                if pid in [-1, 1]:\n                    raise ValueError(\"invalid PID ({})\".format(pid))\n                else:\n                    return pid\n    except OSError as exc:\n        if exc.errno == 2:\n            print(\"CRITICAL: anycast-healthchecker could be down as pid file \"\n                  \"{} doesn't exist\".format(pidfile))\n            sys.exit(2)\n        else:\n            raise ValueError(\"error while reading pid file:{}\".format(exc))", "response": "Returns the process id of the anycast - healthchecker."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_services(config, services):\n    enabled = 0\n    for service in services:\n        check_disabled = config.getboolean(service, 'check_disabled')\n        if not check_disabled:\n            enabled += 1\n\n    return enabled", "response": "Parse configuration to return number of enabled service checks."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns check. anycast-healthchecker is a multi-threaded software and for each service check it holds a thread. If a thread dies then the service is not monitored anymore and the route for the IP associated with service it wont be withdrawn in case service goes down in the meantime.", "response": "def main():\n    \"\"\"Run check.\n\n    anycast-healthchecker is a multi-threaded software and for each\n    service check it holds a thread. If a thread dies then the service\n    is not monitored anymore and the route for the IP associated with service\n    it wont be withdrawn in case service goes down in the meantime.\n    \"\"\"\n    arguments = docopt(__doc__)\n    config_file = '/etc/anycast-healthchecker.conf'\n    config_dir = '/etc/anycast-healthchecker.d'\n    config = configparser.ConfigParser()\n    config_files = [config_file]\n    config_files.extend(glob.glob(os.path.join(config_dir, '*.conf')))\n    config.read(config_files)\n\n    try:\n        pid = get_processid(config)\n    except ValueError as exc:\n        print(\"UNKNOWN: {e}\".format(e=exc))\n        sys.exit(3)\n    else:\n        process_up = running(pid)\n\n    if not process_up:\n        print(\"CRITICAL: anycast-healthchecker with pid ({p}) isn't running\"\n              .format(p=pid))\n        sys.exit(3)\n\n    services = config.sections()\n    services.remove('daemon')\n    if not services:\n        print(\"UNKNOWN: No service checks are configured\")\n        sys.exit(3)\n\n    enabled_service_checks = parse_services(config, services)\n    if enabled_service_checks == 0:\n        print(\"OK: Number of service checks is zero, no threads are running\")\n        sys.exit(0)\n    else:\n        # parent process plus nummber of threads for each service check\n        configured_threads = enabled_service_checks + 1\n\n    cmd = ['/bin/ps', 'h', '-T', '-p', '{n}'.format(n=pid)]\n    try:\n        if arguments['-v']:\n            print(\"running {}\".format(' '.join(cmd)))\n        out = subprocess.check_output(cmd, timeout=1)\n    except subprocess.CalledProcessError as exc:\n        print(\"UNKNOWN: running '{c}' failed with return code: {r}\"\n              .format(c=' '.join(cmd), r=exc.returncode))\n        sys.exit(3)\n    except subprocess.TimeoutExpired:\n        print(\"UNKNOWN: running '{}' timed out\".format(' '.join(cmd)))\n        sys.exit(3)\n    else:\n        output_lines = out.splitlines()\n        if arguments['-v']:\n            for line in output_lines:\n                print(line)\n        running_threads = len(output_lines)\n        if running_threads == configured_threads:\n            print(\"OK: UP (pid={p}) and all threads ({t}) are running\"\n                  .format(p=pid, t=configured_threads - 1))\n            sys.exit(0)\n        elif running_threads - 1 == 0:  # minus parent process\n            print(\"CRITICAL: No threads are running OpDocs ANYCAST-03\")\n            sys.exit(2)\n        else:\n            print(\"CRITICAL: Found {n} running threads while configured \"\n                  \"number of threads is {c} OpDocs ANYCAST-03\"\n                  .format(n=running_threads - 1, c=configured_threads - 1))\n            sys.exit(2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if path is a valid scary path.", "response": "def scary_path(path):\n    \"\"\"Whitelist the WORKON_HOME strings we're willing to substitute in\n    to strings that we provide for user's shell to evaluate.\n\n    If it smells at all bad, return True.\n    \"\"\"\n    if not path:\n        return True\n    assert isinstance(path, bytes)\n    return not NOT_SCARY.match(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shell_config_for(shell, vexrc, environ):\n    here = os.path.dirname(os.path.abspath(__file__))\n    path = os.path.join(here, 'shell_configs', shell)\n    try:\n        with open(path, 'rb') as inp:\n            data = inp.read()\n    except FileNotFoundError as error:\n        if error.errno != 2:\n            raise\n        return b''\n    ve_base = vexrc.get_ve_base(environ).encode('ascii')\n    if ve_base and not scary_path(ve_base) and os.path.exists(ve_base):\n        data = data.replace(b'$WORKON_HOME', ve_base)\n    return data", "response": "return completion config for the named shell"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling the shell - config option.", "response": "def handle_shell_config(shell, vexrc, environ):\n    \"\"\"Carry out the logic of the --shell-config option.\n    \"\"\"\n    from vex import shell_config\n    data = shell_config.shell_config_for(shell, vexrc, environ)\n    if not data:\n        raise exceptions.OtherShell(\"unknown shell: {0!r}\".format(shell))\n    if hasattr(sys.stdout, 'buffer'):\n        sys.stdout.buffer.write(data)\n    else:\n        sys.stdout.write(data)\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_check(self):\n        cmd = shlex.split(self.config['check_cmd'])\n        self.log.info(\"running %s\", ' '.join(cmd))\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE)\n\n        start_time = time.time()\n        try:\n            outs, errs = proc.communicate(timeout=self.config['check_timeout'])\n        except subprocess.TimeoutExpired:\n            self.log.error(\"check timed out\")\n            if proc.poll() is None:\n                try:\n                    proc.kill()\n                except PermissionError:\n                    self.log.warning(\"failed to kill check due to adequate \"\n                                     \"access rights, check could be running \"\n                                     \"under another user(root) via sudo\")\n\n            return False\n        else:\n            msg = \"check duration {t:.3f}ms\".format(\n                t=(time.time() - start_time) * 1000)\n            self.log.info(msg)\n\n            if proc.returncode != 0:\n                self.log.info(\"stderr from the check %s\", errs)\n                self.log.info(\"stdout from the check %s\", outs)\n\n            return proc.returncode == 0", "response": "Execute a check command and return the exit code of the command is 0 otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if an IP prefix is assigned to loopback interface.", "response": "def _ip_assigned(self):\n        \"\"\"Check if IP prefix is assigned to loopback interface.\n\n        Returns:\n            True if IP prefix found assigned otherwise False.\n\n        \"\"\"\n        output = []\n        cmd = [\n            '/sbin/ip',\n            'address',\n            'show',\n            'dev',\n            self.config['interface'],\n            'to',\n            self.ip_with_prefixlen,\n        ]\n\n        if self.ip_check_disabled:\n            self.log.info(\"checking for IP assignment on interface %s is \"\n                          \"disabled\", self.config['interface'])\n            return True\n\n        self.log.debug(\"running %s\", ' '.join(cmd))\n        try:\n            output = subprocess.check_output(\n                cmd,\n                universal_newlines=True,\n                timeout=1)\n        except subprocess.CalledProcessError as error:\n            self.log.error(\"error checking IP-PREFIX %s: %s\",\n                           cmd, error.output)\n            # Because it is unlikely to ever get an error we return True\n            return True\n        except subprocess.TimeoutExpired:\n            self.log.error(\"timeout running %s\", ' '.join(cmd))\n            # Because it is unlikely to ever get a timeout we return True\n            return True\n        except ValueError as error:\n            # We have been getting intermittent ValueErrors, see here\n            # gist.github.com/unixsurfer/67db620d87f667423f6f6e3a04e0bff5\n            # It has happened ~5 times and this code is executed from multiple\n            # threads and every ~10secs on several (~40) production servers for\n            # more than 18months.\n            # It could be a bug in Python or system returns corrupted data.\n            # As a consequence of the raised exception thread dies and the\n            # service isn't monitored anymore!. So, we now catch the exception.\n            # While checking if an IP is assigned, we get an error unrelated to\n            # that prevents us from knowing if it's assigned. We simply don't\n            # know. A retry logic could be a more proper solution.\n            self.log.error(\"running %s raised ValueError exception:%s\",\n                           ' '.join(cmd), error)\n            return True\n        else:\n            if self.ip_with_prefixlen in output:  # pylint: disable=E1135,R1705\n                msg = \"{i} assigned to loopback interface\".format(\n                    i=self.ip_with_prefixlen)\n                self.log.debug(msg)\n                return True\n            else:\n                msg = (\"{i} isn't assigned to {d} interface\"\n                       .format(i=self.ip_with_prefixlen,\n                               d=self.config['interface']))\n                self.log.warning(msg)\n                return False\n\n        self.log.debug(\"I shouldn't land here!, it is a BUG\")\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_disabled(self):\n        if self.config['check_disabled']:\n            if self.config['on_disabled'] == 'withdraw':\n                self.log.info(\"Check is disabled and ip_prefix will be \"\n                              \"withdrawn\")\n                self.log.info(\"adding %s in the queue\", self.ip_with_prefixlen)\n                self.action.put(self.del_operation)\n                self.log.info(\"Check is now permanently disabled\")\n            elif self.config['on_disabled'] == 'advertise':\n                self.log.info(\"check is disabled, ip_prefix wont be withdrawn\")\n                self.log.info(\"adding %s in the queue\", self.ip_with_prefixlen)\n                self.action.put(self.add_operation)\n                self.log.info('check is now permanently disabled')\n\n            return True\n\n        return False", "response": "Checks if health check is disabled and adds an item to the action queue based on on_disabled setting."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _run(self):\n        up_cnt = 0\n        down_cnt = 0\n        # The current established state of the service check, it can be\n        # either UP or DOWN but only after a number of consecutive successful\n        # or unsuccessful health checks.\n        check_state = 'Unknown'\n\n        for key, value in self.config.items():\n            self.log.debug(\"%s=%s:%s\", key, value, type(value))\n\n        # Service check will abort if it is disabled.\n        if self._check_disabled():\n            return\n\n        if self.splay_startup is not None:\n            sleep_time = float(\"%.3f\" % random.uniform(0, self.splay_startup))\n            self.log.info(\"delaying startup for %ssecs\", sleep_time)\n            time.sleep(sleep_time)\n\n        interval = self.config['check_interval']\n        start_offset = time.time() % interval\n        # Go in a loop until we are told to stop\n        while True:\n            timestamp = time.time()\n            if not self._ip_assigned():\n                up_cnt = 0\n                self.extra['status'] = 'down'\n                self.log.warning(\"status DOWN because %s isn't assigned to \"\n                                 \"loopback interface.\",\n                                 self.ip_with_prefixlen,\n                                 extra=self.extra)\n                if check_state != 'DOWN':\n                    check_state = 'DOWN'\n                    self.log.info(\"adding %s in the queue\",\n                                  self.ip_with_prefixlen,\n                                  extra=self.extra)\n                    self.action.put(self.del_operation)\n            elif self._run_check():\n                if up_cnt == (self.config['check_rise'] - 1):\n                    self.extra['status'] = 'up'\n                    self.log.info(\"status UP\", extra=self.extra)\n                    # Service exceeded all consecutive checks. Set its state\n                    # accordingly and put an item in queue. But do it only if\n                    # previous state was different, to prevent unnecessary bird\n                    # reloads when a service flaps between states.\n                    if check_state != 'UP':\n                        check_state = 'UP'\n                        self.log.info(\"adding %s in the queue\",\n                                      self.ip_with_prefixlen,\n                                      extra=self.extra)\n                        self.action.put(self.add_operation)\n                elif up_cnt < self.config['check_rise']:\n                    up_cnt += 1\n                    self.log.info(\"going up %s\", up_cnt, extra=self.extra)\n                else:\n                    self.log.error(\"up_cnt is higher %s, it's a BUG!\",\n                                   up_cnt,\n                                   extra=self.extra)\n                down_cnt = 0\n            else:\n                if down_cnt == (self.config['check_fail'] - 1):\n                    self.extra['status'] = 'down'\n                    self.log.info(\"status DOWN\", extra=self.extra)\n                    # Service exceeded all consecutive checks.\n                    # Set its state accordingly and put an item in queue.\n                    # But do it only if previous state was different, to\n                    # prevent unnecessary bird reloads when a service flaps\n                    # between states\n                    if check_state != 'DOWN':\n                        check_state = 'DOWN'\n                        self.log.info(\"adding %s in the queue\",\n                                      self.ip_with_prefixlen,\n                                      extra=self.extra)\n                        self.action.put(self.del_operation)\n                elif down_cnt < self.config['check_fail']:\n                    down_cnt += 1\n                    self.log.info(\"going down %s\", down_cnt, extra=self.extra)\n                else:\n                    self.log.error(\"up_cnt is higher %s, it's a BUG!\",\n                                   up_cnt,\n                                   extra=self.extra)\n                up_cnt = 0\n\n            self.log.info(\"wall clock time %.3fms\",\n                          (time.time() - timestamp) * 1000,\n                          extra=self.extra)\n\n            # calculate sleep time\n            sleep = start_offset - time.time() % interval\n            if sleep < 0:\n                sleep += interval\n            self.log.debug(\"sleeping for %.3fsecs\", sleep, extra=self.extra)\n            time.sleep(sleep)", "response": "Runs the health checks for a specific service."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    args = docopt(__doc__, version=__version__)\n    if args['--print']:\n        for section in DEFAULT_OPTIONS:\n            print(\"[{}]\".format(section))\n            for key, value in DEFAULT_OPTIONS[section].items():\n                print(\"{k} = {v}\".format(k=key, v=value))\n            print()\n        sys.exit(0)\n\n    try:\n        config, bird_configuration = load_configuration(args['--file'],\n                                                        args['--dir'],\n                                                        args['--service-file'])\n    except ValueError as exc:\n        sys.exit('Invalid configuration: ' + str(exc))\n\n    if args['--check']:\n        print(\"OK\")\n        sys.exit(0)\n\n    if args['--print-conf']:\n        for section in config:\n            print(\"[{}]\".format(section))\n            for key, value in config[section].items():\n                print(\"{k} = {v}\".format(k=key, v=value))\n            print()\n        sys.exit(0)\n\n    try:\n        lock_socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)\n        lock_socket.bind('\\0' + \"{}\".format(PROGRAM_NAME))\n    except socket.error as exc:\n        sys.exit(\"failed to acquire a lock by creating an abstract namespace\"\n                 \" socket: {}\".format(exc))\n    else:\n        print(\"acquired a lock by creating an abstract namespace socket: {}\"\n              .format(lock_socket))\n\n    # Clean old pidfile, if it exists, and write PID to it.\n    pidfile = config.get('daemon', 'pidfile')\n    update_pidfile(pidfile)\n\n    # Register our shutdown handler to various termination signals.\n    shutdown_handler = partial(shutdown, pidfile)\n    signal.signal(signal.SIGHUP, shutdown_handler)\n    signal.signal(signal.SIGTERM, shutdown_handler)\n    signal.signal(signal.SIGABRT, shutdown_handler)\n    signal.signal(signal.SIGINT, shutdown_handler)\n\n    # Set up loggers.\n    logger = setup_logger(config)\n\n    # Perform a sanity check on IP-Prefixes\n    ip_prefixes_sanity_check(config, bird_configuration)\n\n    # Create our master process.\n    checker = healthchecker.HealthChecker(config, bird_configuration)\n    logger.info(\"starting %s version %s\", PROGRAM_NAME, __version__)\n    checker.run()", "response": "Parse CLI and starts main program."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_environ(environ, defaults, ve_path):\n    # Copy the parent environment, add in defaults from .vexrc.\n    env = environ.copy()\n    env.update(defaults)\n\n    # Leaving in existing PYTHONHOME can cause some errors\n    if 'PYTHONHOME' in env:\n        del env['PYTHONHOME']\n\n    # Now we have to adjust PATH to find scripts for the virtualenv...\n    # PATH being unset/empty is OK, but ve_path must be set\n    # or there is nothing for us to do here and it's bad.\n    if not ve_path:\n        raise exceptions.BadConfig('ve_path must be set')\n    if platform.system() == 'Windows':\n        ve_bin = os.path.join(ve_path, 'Scripts')\n    else:\n        ve_bin = os.path.join(ve_path, 'bin')\n\n    # If user is currently in a virtualenv, DON'T just prepend\n    # to its path (vex foo; echo $PATH -> \" /foo/bin:/bar/bin\")\n    # but don't incur this cost unless we're already in one.\n    # activate handles this by running 'deactivate' first, we don't\n    # have that so we have to use other ways.\n    # This would not be necessary and things would be simpler if vex\n    # did not have to interoperate with a ubiquitous existing tool.\n    # virtualenv doesn't...\n    current_ve = env.get('VIRTUAL_ENV', '')\n    system_path = environ.get('PATH', '')\n    segments = system_path.split(os.pathsep)\n    if current_ve:\n        # Since activate doesn't export _OLD_VIRTUAL_PATH, we are going to\n        # manually remove the virtualenv's bin.\n        # A virtualenv's bin should not normally be on PATH except\n        # via activate or similar, so I'm OK with this solution.\n        current_ve_bin = os.path.join(current_ve, 'bin')\n        try:\n            segments.remove(current_ve_bin)\n        except ValueError:\n            raise exceptions.BadConfig(\n                \"something set VIRTUAL_ENV prior to this vex execution, \"\n                \"implying that a virtualenv is already activated \"\n                \"and PATH should contain the virtualenv's bin directory. \"\n                \"Unfortunately, it doesn't: it's {0!r}. \"\n                \"You might want to check that PATH is not \"\n                \"getting clobbered somewhere, e.g. in your shell's configs.\"\n                .format(system_path)\n            )\n\n    segments.insert(0, ve_bin)\n    env['PATH'] = os.pathsep.join(segments)\n    env['VIRTUAL_ENV'] = ve_path\n    return env", "response": "Make a new environment that can be used to run with."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the given command.", "response": "def run(command, env, cwd):\n    \"\"\"Run the given command.\n    \"\"\"\n    assert command\n    if cwd:\n        assert os.path.exists(cwd)\n    if platform.system() == \"Windows\":\n        exe = distutils.spawn.find_executable(command[0], path=env['PATH'])\n        if exe:\n            command[0] = exe\n    _, command_name = os.path.split(command[0])\n    if (command_name in ('bash', 'zsh')\n    and 'VIRTUALENVWRAPPER_PYTHON' not in env):\n        env['VIRTUALENVWRAPPER_PYTHON'] = ':'\n    try:\n        process = subprocess.Popen(command, env=env, cwd=cwd)\n        process.wait()\n    except exceptions.CommandNotFoundError as error:\n        if error.errno != 2:\n            raise\n        return None\n    return process.returncode"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_key_value(line, environ):\n    segments = line.split(\"=\", 1)\n    if len(segments) < 2:\n        return None\n    key, value = segments\n    # foo passes through as-is (with spaces stripped)\n    # '{foo}' passes through literally\n    # \"{foo}\" substitutes from environ's foo\n    value = value.strip()\n    if value[0] == \"'\" and _SQUOTE_RE.match(value):\n        value = value[1:-1]\n    elif value[0] == '\"' and _DQUOTE_RE.match(value):\n        template = value[1:-1]\n        value = template.format(**environ)\n    key = key.strip()\n    value = value.strip()\n    return key, value", "response": "Extract key value from given line."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_file(cls, path, environ):\n        instance = cls()\n        instance.read(path, environ)\n        return instance", "response": "Make a Vexrc instance from given file in given environ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, path, environ):\n        try:\n            inp = open(path, 'rb')\n        except FileNotFoundError as error:\n            if error.errno != 2:\n                raise\n            return None\n        parsing = parse_vexrc(inp, environ)\n        for heading, key, value in parsing:\n            heading = self.default_heading if heading is None else heading\n            if heading not in self.headings:\n                self.headings[heading] = OrderedDict()\n            self.headings[heading][key] = value\n        parsing.close()", "response": "Read data from file into this vexrc instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds a directory to look for virtualenvs in.", "response": "def get_ve_base(self, environ):\n        \"\"\"Find a directory to look for virtualenvs in.\n        \"\"\"\n        # set ve_base to a path we can look for virtualenvs:\n        # 1. .vexrc\n        # 2. WORKON_HOME (as defined for virtualenvwrapper's benefit)\n        # 3. $HOME/.virtualenvs\n        # (unless we got --path, then we don't need it)\n        ve_base_value = self.headings[self.default_heading].get('virtualenvs')\n        if ve_base_value:\n            ve_base = os.path.expanduser(ve_base_value)\n        else:\n            ve_base = environ.get('WORKON_HOME', '')\n        if not ve_base:\n            # On Cygwin os.name == 'posix' and we want $HOME.\n            if platform.system() == 'Windows' and os.name == 'nt':\n                _win_drive = environ.get('HOMEDRIVE')\n                home = environ.get('HOMEPATH', '')\n                if home:\n                    home = os.path.join(_win_drive, home)\n            else:\n                home = environ.get('HOME', '')\n            if not home:\n                home = os.path.expanduser('~')\n            if not home:\n                return ''\n            ve_base = os.path.join(home, '.virtualenvs')\n        # pass through invalid paths so messages can be generated\n        # if not os.path.exists(ve_base) or os.path.isfile(ve_base):\n            # return ''\n        return ve_base or ''"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind a command to run.", "response": "def get_shell(self, environ):\n        \"\"\"Find a command to run.\n        \"\"\"\n        command = self.headings[self.default_heading].get('shell')\n        if not command and os.name != 'nt':\n            command = environ.get('SHELL', '')\n        command = shlex.split(command) if command else None\n        return command"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nyield relative file paths specified in metainfo.", "response": "def files(self):\n        \"\"\"\n        Yield relative file paths specified in :attr:`metainfo`\n\n        Each paths starts with :attr:`name`.\n\n        Note that the paths may not exist. See :attr:`filepaths` for existing\n        files.\n        \"\"\"\n        info = self.metainfo['info']\n        if 'length' in info:    # Singlefile\n            yield info['name']\n        elif 'files' in info:   # Multifile torrent\n            rootdir = self.name\n            for fileinfo in info['files']:\n                yield os.path.join(rootdir, os.path.join(*fileinfo['path']))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield absolute paths to existing files in self. path", "response": "def filepaths(self):\n        \"\"\"\n        Yield absolute paths to existing files in :attr:`path`\n\n        Any files that match patterns in :attr:`exclude` as well as hidden and\n        empty files are not included.\n        \"\"\"\n        if self.path is not None:\n            yield from utils.filepaths(self.path, exclude=self.exclude,\n                                       hidden=False, empty=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary that maps directory and file names to child nodes.", "response": "def filetree(self):\n        \"\"\"\n        :attr:`files` as a dictionary tree\n\n        Each node is a ``dict`` that maps directory/file names to child nodes.\n        Each child node is a ``dict`` for directories and ``None`` for files.\n\n        If :attr:`path` is ``None``, this is an empty ``dict``.\n        \"\"\"\n        tree = {}   # Complete directory tree\n        prefix = []\n        paths = (f.split(os.sep) for f in self.files)\n        for path in paths:\n            dirpath = path[:-1]  # Path without filename\n            filename = path[-1]\n            subtree = tree\n            for item in dirpath:\n                if item not in subtree:\n                    subtree[item] = {}\n                subtree = subtree[item]\n            subtree[filename] = None\n        return tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the size of the content in bytes or None if the content is not present.", "response": "def size(self):\n        \"\"\"\n        Total size of content in bytes or ``None`` if :attr:`path` is ``None``\n        \"\"\"\n        if 'length' in self.metainfo['info']:   # Singlefile\n            return self.metainfo['info']['length']\n        elif 'files' in self.metainfo['info']:  # Multifile torrent\n            return sum(fileinfo['length']\n                       for fileinfo in self.metainfo['info']['files'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the piece length of the entry in the metainfo.", "response": "def piece_size(self):\n        \"\"\"\n        Piece size/length or ``None``\n\n        If set to ``None``, :attr:`calculate_piece_size` is called.\n\n        If :attr:`size` returns ``None``, this also returns ``None``.\n\n        Setting this property sets ``piece length`` in :attr:`metainfo`\\\n        ``['info']``.\n        \"\"\"\n        if 'piece length' not in self.metainfo['info']:\n            if self.size is None:\n                return None\n            else:\n                self.calculate_piece_size()\n        return self.metainfo['info']['piece length']"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_piece_size(self):\n        size = self.size\n        if not size:\n            raise RuntimeError(f'Cannot calculate piece size with no \"path\" specified')\n        else:\n            self.metainfo['info']['piece length'] = utils.calc_piece_size(\n                size, self.MAX_PIECES, self.MIN_PIECE_SIZE, self.MAX_PIECE_SIZE)", "response": "Calculates and adds the piece length to the metainfo."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the number of pieces the content is split into or None if the content is not split into.", "response": "def pieces(self):\n        \"\"\"\n        Number of pieces the content is split into or ``None`` if :attr:`piece_size`\n        returns ``None``\n        \"\"\"\n        if self.piece_size is None:\n            return None\n        else:\n            return math.ceil(self.size / self.piece_size)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef name(self):\n        if 'name' not in self.metainfo['info'] and self.path is not None:\n            self.metainfo['info']['name'] = os.path.basename(self.path)\n        return self.metainfo['info'].get('name', None)", "response": "Return the name of the torrent."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of tiers of announce URLs or None for no trackers.", "response": "def trackers(self):\n        \"\"\"\n        List of tiers of announce URLs or ``None`` for no trackers\n\n        A tier is either a single announce URL (:class:`str`) or an\n        :class:`~collections.abc.Iterable` (e.g. :class:`list`) of announce\n        URLs.\n\n        Setting this property sets or removes ``announce`` and ``announce-list``\n        in :attr:`metainfo`. ``announce`` is set to the first tracker of the\n        first tier.\n\n        :raises URLError: if any of the announce URLs is invalid\n        \"\"\"\n        announce_list = self.metainfo.get('announce-list', None)\n        if not announce_list:\n            announce = self.metainfo.get('announce', None)\n            if announce:\n                return [[announce]]\n        else:\n            return announce_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating all the pieces in the file and send status information to callback.", "response": "def generate(self, callback=None, interval=0):\n        \"\"\"\n        Hash pieces and report progress to `callback`\n\n        This method sets ``pieces`` in :attr:`metainfo`\\ ``['info']`` when all\n        pieces are hashed successfully.\n\n        :param callable callback: Callable with signature ``(torrent, filepath,\n            pieces_done, pieces_total)``; if `callback` returns anything else\n            than None, hashing is canceled\n\n        :param float interval: Minimum number of seconds between calls to\n            `callback` (if 0, `callback` is called once per piece)\n        :raises PathEmptyError: if :attr:`path` contains only empty\n            files/directories\n        :raises PathNotFoundError: if :attr:`path` does not exist\n        :raises ReadError: if :attr:`path` or any file beneath it is not\n            readable\n\n        :return: ``True`` if all pieces were successfully hashed, ``False``\n            otherwise\n        \"\"\"\n        if self.path is None:\n            raise RuntimeError('generate() called with no path specified')\n        elif self.size <= 0:\n            raise error.PathEmptyError(self.path)\n        elif not os.path.exists(self.path):\n            raise error.PathNotFoundError(self.path)\n\n        if callback is not None:\n            cancel = lambda *status: callback(*status) is not None\n        else:\n            cancel = lambda *status: False\n\n        if os.path.isfile(self.path):\n            pieces = self._set_pieces_singlefile()\n        elif os.path.isdir(self.path):\n            pieces = self._set_pieces_multifile()\n\n        # Iterate over hashed pieces and send status information\n        last_cb_call = 0\n        for filepath,pieces_done,pieces_total in pieces:\n            now = time.time()\n            if now - last_cb_call >= interval or \\\n               pieces_done >= pieces_total:\n                last_cb_call = now\n                if cancel(self, filepath, pieces_done, pieces_total):\n                    return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert(self):\n        try:\n            return utils.encode_dict(self.metainfo)\n        except ValueError as e:\n            raise error.MetainfoError(str(e))", "response": "Converts the object to a dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate(self):\n        md = self.metainfo\n        info = md['info']\n\n        # Check values shared by singlefile and multifile torrents\n        utils.assert_type(md, ('info', 'name'), (str,), must_exist=True)\n        utils.assert_type(md, ('info', 'piece length'), (int,), must_exist=True)\n        utils.assert_type(md, ('info', 'pieces'), (bytes, bytearray), must_exist=True)\n\n        if 'length' in info and 'files' in info:\n            raise error.MetainfoError(\"['info'] includes both 'length' and 'files'\")\n\n        elif 'length' in info:\n            # Validate info as singlefile torrent\n            utils.assert_type(md, ('info', 'length'), (int, float), must_exist=True)\n            utils.assert_type(md, ('info', 'md5sum'), (str,), must_exist=False, check=utils.is_md5sum)\n\n            if self.path is not None:\n                # Check if filepath actually points to a file\n                if not os.path.isfile(self.path):\n                    raise error.MetainfoError(f\"Metainfo includes {self.path} as file, but it is not a file\")\n\n                # Check if size matches\n                if os.path.getsize(self.path) != info['length']:\n                    raise error.MetainfoError(f\"Mismatching file sizes in metainfo ({info['length']})\"\n                                              f\" and local file system ({os.path.getsize(self.path)}): \"\n                                              f\"{self.path!r}\")\n\n        elif 'files' in info:\n            # Validate info as multifile torrent\n            utils.assert_type(md, ('info', 'files'), (list,), must_exist=True)\n\n            for i,fileinfo in enumerate(info['files']):\n                utils.assert_type(md, ('info', 'files', i, 'length'), (int, float), must_exist=True)\n                utils.assert_type(md, ('info', 'files', i, 'path'), (list,), must_exist=True)\n                utils.assert_type(md, ('info', 'files', i, 'md5sum'), (str,), must_exist=False,\n                            check=utils.is_md5sum)\n\n            if self.path is not None:\n                # Check if filepath actually points to a directory\n                if not os.path.isdir(self.path):\n                    raise error.MetainfoError(f\"Metainfo includes {self.path} as directory, but it is not a directory\")\n\n                for i,fileinfo in enumerate(info['files']):\n                    for j,item in enumerate(fileinfo['path']):\n                        utils.assert_type(md, ('info', 'files', i, 'path', j), (str,))\n\n                    filepath = os.path.join(self.path, os.path.join(*fileinfo['path']))\n\n                    # Check if filepath exists and is a file\n                    if not os.path.exists(filepath):\n                        raise error.MetainfoError(f\"Metainfo inclues file that doesn't exist: {filepath!r}\")\n                    if not os.path.isfile(filepath):\n                        raise error.MetainfoError(f\"Metainfo inclues non-file: {filepath!r}\")\n\n                    # Check if sizes match\n                    if os.path.getsize(filepath) != fileinfo['length']:\n                        raise error.MetainfoError(f\"Mismatching file sizes in metainfo ({fileinfo['length']})\"\n                                                  f\" and local file system ({os.path.getsize(filepath)}): \"\n                                                  f\"{filepath!r}\")\n\n        else:\n            raise error.MetainfoError(\"Missing 'length' or 'files' in metainfo\")", "response": "Validate that all mandatory keys exist in the metainfo and are of expected types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump(self, validate=True):\n        if validate:\n            self.validate()\n        return bencode(self.convert())", "response": "Create bencoded :attr:`metainfo` (i.e. the content of a torrent file)\n\n        :param bool validate: Whether to run :meth:`validate` first\n\n        :return: :attr:`metainfo` as bencoded :class:`bytes`"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_stream(self, stream, validate=True):\n        content = self.dump(validate=validate)\n        try:\n            # Remove existing data from stream *after* dump() didn't raise\n            # anything so we don't destroy it prematurely.\n            if stream.seekable():\n                stream.seek(0)\n                stream.truncate(0)\n            stream.write(content)\n        except OSError as e:\n            raise error.WriteError(e.errno)", "response": "Writes the metainfo to a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the metainfo to a file.", "response": "def write(self, filepath, validate=True, overwrite=False, mode=0o666):\n        \"\"\"\n        Write :attr:`metainfo` to torrent file\n\n        This method is essentially equivalent to:\n\n        >>> with open('my.torrent', 'wb') as f:\n        ...     f.write(torrent.dump())\n\n        :param filepath: Path of the torrent file\n        :param bool validate: Whether to run :meth:`validate` first\n        :param bool overwrite: Whether to silently overwrite `filepath` (only\n            if all pieces were hashed successfully)\n        :param mode: File permissions of `filepath`\n\n        :raises WriteError: if writing to `filepath` fails\n        :raises MetainfoError: if `validate` is `True` and :attr:`metainfo`\n            contains invalid data\n        \"\"\"\n        if not overwrite and os.path.exists(filepath):\n            raise error.WriteError(errno.EEXIST, filepath)\n\n        # Get file content before opening the file in case there are errors like\n        # incomplete metainfo\n        content = io.BytesIO()\n        self.write_stream(content, validate=validate)\n        content.seek(0)\n        try:\n            with open(filepath, 'wb') as f:\n                f.write(content.read())\n        except OSError as e:\n            raise error.WriteError(e.errno, filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef magnet(self, name=True, size=True, trackers=True, tracker=False, validate=True):\n        if validate:\n            self.validate()\n\n        parts = [f'xt=urn:btih:{self.infohash}']\n        if name:\n            parts.append(f'dn={utils.urlquote(self.name)}')\n        if size:\n            parts.append(f'xl={self.size}')\n\n        if self.trackers is not None:\n            if tracker:\n                parts.append(f'tr={utils.urlquote(self.trackers[0][0])}')\n            elif trackers:\n                for tier in self.trackers:\n                    for url in tier:\n                        parts.append(f'tr={utils.urlquote(url)}')\n\n        return 'magnet:?' + '&'.join(parts)", "response": "Returns the BTIH Magnet URI for this object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_stream(cls, stream, validate=True):\n        try:\n            content = stream.read(cls.MAX_TORRENT_FILE_SIZE)\n        except OSError as e:\n            raise error.ReadError(e.errno)\n        else:\n            try:\n                metainfo_enc = bdecode(content)\n            except BTFailure as e:\n                raise error.ParseError()\n\n            if validate:\n                if b'info' not in metainfo_enc:\n                    raise error.MetainfoError(\"Missing 'info'\")\n                elif not isinstance(metainfo_enc[b'info'], abc.Mapping):\n                    raise error.MetainfoError(\"'info' is not a dictionary\")\n                elif b'pieces' not in metainfo_enc[b'info']:\n                    raise error.MetainfoError(\"Missing 'pieces' in ['info']\")\n\n            # Extract 'pieces' from metainfo because it's the only byte string\n            # that isn't supposed to be decoded to unicode.\n            if b'info' in metainfo_enc and b'pieces' in metainfo_enc[b'info']:\n                pieces = metainfo_enc[b'info'].pop(b'pieces')\n                metainfo = utils.decode_dict(metainfo_enc)\n                metainfo['info']['pieces'] = pieces\n            else:\n                metainfo = utils.decode_dict(metainfo_enc)\n\n            torrent = cls()\n            torrent._metainfo = metainfo\n\n            # Convert some values from official types to something nicer\n            # (e.g. int -> datetime)\n            for attr in ('creation_date', 'private'):\n                setattr(torrent, attr, getattr(torrent, attr))\n\n            # Auto-set 'include_md5'\n            info = torrent.metainfo['info']\n            torrent.include_md5 = ('length' in info and 'md5sum' in info) or \\\n                                  ('files' in info and all('md5sum' in fileinfo\n                                                           for fileinfo in info['files']))\n\n            if validate:\n                torrent.validate()\n\n            return torrent", "response": "Reads a Torrent from a stream."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(cls, filepath, validate=True):\n        try:\n            with open(filepath, 'rb') as fh:\n                return cls.read_stream(fh)\n        except (OSError, error.ReadError) as e:\n            raise error.ReadError(e.errno, filepath)\n        except error.ParseError:\n            raise error.ParseError(filepath)", "response": "Reads a Torrent from a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new object with the same metainfo as this one.", "response": "def copy(self):\n        \"\"\"\n        Return a new object with the same metainfo\n\n        Internally, this simply copies the internal metainfo dictionary with\n        :func:`copy.deepcopy` and gives it to the new instance.\n        \"\"\"\n        from copy import deepcopy\n        cp = type(self)()\n        cp._metainfo = deepcopy(self._metainfo)\n        return cp"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validated_url(url):\n    try:\n        u = urlparse(url)\n        u.port  # Trigger 'invalid port' exception\n    except Exception:\n        raise error.URLError(url)\n    else:\n        if not u.scheme or not u.netloc:\n            raise error.URLError(url)\n        return url", "response": "Return url if valid raise URLError otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning whether num is a power of two.", "response": "def is_power_of_2(num):\n    \"\"\"Return whether `num` is a power of two\"\"\"\n    log = math.log2(num)\n    return int(log) == float(log)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_hidden(path):\n    for name in path.split(os.sep):\n        if name != '.' and name != '..' and name and name[0] == '.':\n            return True\n    return False", "response": "Whether file or directory is hidden"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning list of absolute file paths in a directory.", "response": "def filepaths(path, exclude=(), hidden=True, empty=True):\n    \"\"\"\n    Return list of absolute, sorted file paths\n\n    path: Path to file or directory\n    exclude: List of file name patterns to exclude\n    hidden: Whether to include hidden files\n    empty: Whether to include empty files\n\n    Raise PathNotFoundError if path doesn't exist.\n    \"\"\"\n    if not os.path.exists(path):\n        raise error.PathNotFoundError(path)\n    elif not os.access(path, os.R_OK,\n                       effective_ids=os.access in os.supports_effective_ids):\n        raise error.ReadError(errno.EACCES, path)\n\n    if os.path.isfile(path):\n        return [path]\n    else:\n        filepaths = []\n        for dirpath, dirnames, filenames in os.walk(path):\n            # Ignore hidden directory\n            if not hidden and is_hidden(dirpath):\n                continue\n\n            for filename in filenames:\n                # Ignore hidden file\n                if not hidden and is_hidden(filename):\n                    continue\n\n                filepath = os.path.join(dirpath, filename)\n                # Ignore excluded file\n                if any(is_match(filepath, pattern) for pattern in exclude):\n                    continue\n                else:\n                    # Ignore empty file\n                    if empty or os.path.getsize(os.path.realpath(filepath)) > 0:\n                        filepaths.append(filepath)\n\n        return sorted(filepaths, key=lambda fp: fp.casefold())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef key_exists_in_list_or_dict(key, lst_or_dct):\n    if isinstance(lst_or_dct, dict) and key in lst_or_dct:\n        return True\n    elif isinstance(lst_or_dct, list):\n        min_i, max_i = 0, len(lst_or_dct)\n        if min_i <= key < max_i:\n            return True\n    return False", "response": "True if key exists in lst_or_dct False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises MetainfoError is not of a particular type", "response": "def assert_type(lst_or_dct, keys, exp_types, must_exist=True, check=None):\n    \"\"\"\n    Raise MetainfoError is not of a particular type\n\n    lst_or_dct: list or dict instance\n    keys: Sequence of keys so that `lst_or_dct[key[0]][key[1]]...` resolves to a\n          value\n    exp_types: Sequence of types that the value specified by `keys` must be an\n               instance of\n    must_exist: Whether to raise MetainfoError if `keys` does not resolve to a\n                value\n    check: Callable that gets the value specified by `keys` and returns True if\n           it OK, False otherwise\n    \"\"\"\n    keys = list(keys)\n    keychain = []\n    while len(keys[:-1]) > 0:\n        key = keys.pop(0)\n        try:\n            lst_or_dct = lst_or_dct[key]\n        except (KeyError, IndexError):\n            break\n        keychain.append(key)\n\n    keychain_str = ''.join(f'[{key!r}]' for key in keychain)\n    key = keys.pop(0)\n\n    if not key_exists_in_list_or_dict(key, lst_or_dct):\n        if not must_exist:\n            return\n        raise error.MetainfoError(f\"Missing {key!r} in {keychain_str}\")\n\n    elif not isinstance(lst_or_dct[key], exp_types):\n        exp_types_str = ' or '.join(t.__name__ for t in exp_types)\n        type_str = type(lst_or_dct[key]).__name__\n        raise error.MetainfoError(f\"{keychain_str}[{key!r}] must be {exp_types_str}, \"\n                                  f\"not {type_str}: {lst_or_dct[key]!r}\")\n\n    elif check is not None and not check(lst_or_dct[key]):\n        raise error.MetainfoError(f\"{keychain_str}[{key!r}] is invalid: {lst_or_dct[key]!r}\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef error_message_and_exit(message, error_result):\n    if message:\n        error_message(message)\n    puts(json.dumps(error_result, indent=2))\n    sys.exit(1)", "response": "Prints error messages in blue the failed task result and quits."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a prompt title and choices with a bit of formatting.", "response": "def print_prompt_values(values, message=None, sub_attr=None):\n    \"\"\"Prints prompt title and choices with a bit of formatting.\"\"\"\n    if message:\n        prompt_message(message)\n\n    for index, entry in enumerate(values):\n        if sub_attr:\n            line = '{:2d}: {}'.format(index, getattr(utf8(entry), sub_attr))\n        else:\n            line = '{:2d}: {}'.format(index, utf8(entry))\n\n        with indent(3):\n            print_message(line)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting prompt instruction and does basic input parsing.", "response": "def prompt_for_input(message, input_type=None):\n    \"\"\"Prints prompt instruction and does basic input parsing.\"\"\"\n    while True:\n        output = prompt.query(message)\n\n        if input_type:\n            try:\n                output = input_type(output)\n            except ValueError:\n                error_message('Invalid input type')\n                continue\n\n        break\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting prompt with a list of choices to choose from.", "response": "def prompt_for_choice(values, message, input_type=int, output_type=None):\n    \"\"\"Prints prompt with a list of choices to choose from.\"\"\"\n    output = None\n    while not output:\n        index = prompt_for_input(message, input_type=input_type)\n\n        try:\n            output = utf8(values[index])\n        except IndexError:\n            error_message('Selection out of range')\n            continue\n\n    if output_type:\n        output = output_type(output)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _retrieve_result(endpoints, token_header):\n        request_list = [\n            (url, token_header)\n            for (task_id, url) in endpoints\n        ]\n\n        responses = concurrent_get(request_list)\n\n        # Quick sanity check\n        assert len(endpoints) == len(responses)\n\n        responses_dic = {\n            task_id: r.content\n            for (task_id, _), r in zip(endpoints, responses)\n        }\n        return responses_dic", "response": "Prepare the request list and execute them concurrently."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_endpoint(self, endpoint_name):\n        endpoint_relative = settings.get('asmaster_endpoints', endpoint_name)\n        return '%s%s' % (self.host, endpoint_relative)", "response": "Build an enpoint url from a setting name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_allowed_services_and_actions(self, services):\n        for service in services:\n            self.services[service['name']] = {}\n\n            for action in service['actions']:\n                name = action.pop('name')\n                self.services[service['name']][name] = action", "response": "Expects services to be a list of service dictionaries each with name and actions keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nasks for a list of all subscribed accounts and devices along with their statuses.", "response": "def list_subscriptions(self, service):\n        \"\"\"Asks for a list of all subscribed accounts and devices, along with their statuses.\"\"\"\n        data = {\n            'service': service,\n        }\n        return self._perform_post_request(self.list_subscriptions_endpoint, data, self.token_header)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsubscribes an account for a service.", "response": "def subscribe_account(self, username, password, service):\n        \"\"\"Subscribe an account for a service.\n        \"\"\"\n        data = {\n            'service': service,\n            'username': username,\n            'password': password,\n        }\n\n        return self._perform_post_request(self.subscribe_account_endpoint, data, self.token_header)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntreating the response from ASApi. is and return the json data.", "response": "def _parse_response(response, post_request=False):\n        \"\"\"Treat the response from ASApi.\n\n        The json is dumped before checking the status as even if the response is\n        not properly formed we are in trouble.\n        \"\"\"\n        try:\n            data = response.json()\n        except:\n            msg = 'Unhandled HTTP %s response, shown truncated below:\\n%s...' % (\n                response.status_code, response.text[:50]\n            )\n            raise ValueError(msg)\n\n        if not response.ok:\n            utils.error_message_and_exit(None, data)\n\n        if post_request and not data['success']:\n            raise Exception('Asmaster Api Error: [%s]' % data['error'])\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a file id to a file name.", "response": "def file_id_to_file_name(file_id):\n        \"\"\"Sometimes file ids are not the file names on the device, but are instead generated\n        by the API. These are not guaranteed to be valid file names so need hashing.\n        \"\"\"\n        if len(file_id) == 40 and re.match(\"^[a-f0-9]+$\", file_id):\n            return file_id\n        # prefix with \"re_\" to avoid name collision with real fileids\n        return \"re_{}\".format(hashlib.sha1(file_id).hexdigest())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sync(func):\n    sync_timeout = 3600  # Match standard synchronous timeout.\n\n    def wraps(*args, **kwargs):\n        task = func(*args, **kwargs)\n        task.wait_for_result(timeout=sync_timeout)\n        result = json.loads(task.result)\n        return result\n\n    return wraps", "response": "Decorator to make a task synchronous."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nask for a data type choice and execute the fetch_data task.", "response": "def fetch_data(self):\n        \"\"\"Prompt for a data type choice and execute the `fetch_data` task.\n        The results are saved to a file in json format.\n        \"\"\"\n        choices = self.available_data\n        choices.insert(0, 'All')\n\n        selected_data_type = utils.select_item(\n            choices,\n            'Please select what data to fetch:',\n            'Available data:',\n        )\n\n        if selected_data_type == 'All':\n            selected_data_type = ','.join(self.available_data)\n\n        utils.pending_message('Performing fetch data task...')\n\n        fetch_data_task = self.client.data(\n            account=self.account,\n            data=selected_data_type,\n        )\n\n        # Wait here for result as rest of sample app relies on it.\n        fetch_data_task.wait_for_result(timeout=self.timeout)\n        fetch_data_result = json.loads(fetch_data_task.result)\n\n        # Write the result to file.\n        task_id = fetch_data_task.uuid\n        filepath = utils.get_or_create_filepath('%s.json' % task_id)\n\n        with open(filepath, 'w') as out:\n            json.dump(fetch_data_result, out, indent=2)\n\n        utils.info_message('Fetch data successful. Output file: %s.json' % task_id)\n\n        return fetch_data_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef log_in(self):\n        if not self.password:\n            # Password wasn't give, ask for it now\n            self.password = getpass.getpass('Password: ')\n\n        utils.pending_message('Performing login...')\n\n        login_result = self.client.login(\n            account=self.account,\n            password=self.password\n        )\n\n        if 'error' in login_result:\n            self.handle_failed_login(login_result)\n\n        utils.info_message('Login successful')", "response": "Perform the log_in task to setup the API session for future data requests."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles a failed login.", "response": "def handle_failed_login(self, login_result):\n        \"\"\"If Two Factor Authentication (2FA/2SV) is enabled, the initial\n        login will fail with a predictable error. Catching this error allows us\n        to begin the authentication process.\n\n        Other types of errors can be treated in a similar way.\n        \"\"\"\n        error_code = login_result.get('error')\n        if '2fa-required' in error_code:\n            utils.error_message('Login Failed: 2FA or 2SV is active!')\n            self.trigger_two_step_login(login_result)\n            self.finish_two_step_login()\n        else:\n            utils.error_message_and_exit('\\nLogin Failed', login_result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_devices(self):\n        utils.pending_message('Fetching device list...')\n\n        get_devices_task = self.client.devices(\n            account=self.account\n        )\n\n        # We wait for device list info as this sample relies on it next.\n        get_devices_task.wait_for_result(timeout=self.timeout)\n\n        get_devices_result = json.loads(get_devices_task.result)\n        self.devices = get_devices_result['devices']\n\n        utils.info_message('Get devices successful')", "response": "Execute the get_devices task and store the results in self. devices."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_files(self, files):\n        utils.pending_message(\n            \"Downloading {nfiles} file{plural}...\".format(\n                nfiles=len(files),\n                plural='s' if len(files) > 1 else ''\n            ))\n\n        for file in files:\n            if 'file_id' not in file:\n                continue\n\n            def build_callback(file):\n                \"\"\"Callback to save a download file result to a file on disk.\"\"\"\n                def file_callback(task):\n                    device_name = self.devices[self.device_id]['device_name']\n                    path_chunks = file['file_path'].split('/')\n\n                    directory = os.path.join('files', device_name, *path_chunks[:-1])\n\n                    filepath = utils.get_or_create_filepath(file['filename'], directory)\n\n                    with open(filepath, 'wb') as out:\n                        out.write(task.result)\n\n                    if settings.getboolean('logging', 'time_profile'):\n                        filepath = utils.append_profile_info(filepath, task.timer)\n\n                    with indent(4):\n                        utils.print_message(filepath)\n\n                return file_callback\n\n            self.client.download_file(\n                account=self.account,\n                device=self.device_id,\n                file=file['file_id'],\n                callback=build_callback(file)\n            )", "response": "This method downloads binary files from the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_account(self, username, service):\n        data = {\n            'service': service,\n            'username': username,\n        }\n\n        return self._perform_post_request(self.register_account_endpoint, data, self.token_header)", "response": "Register an account against a service."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef perform_task(self, service, task_name, account, payload, callback=None):\n        data = {\n            'service': service,\n            'action': task_name,\n            'account': account,\n        }\n        data.update(payload)\n\n        response = self._perform_post_request(self.submit_endpoint, data, self.token_header)\n\n        task = Task(uuid=response['task_id'], callback=callback)\n        self._pending_tasks[task.uuid] = task\n\n        return task", "response": "Submit a task to the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the status of a task.", "response": "def task_status(self, task_id):\n        \"\"\"Find the status of a task.\"\"\"\n        data = {\n            'task_ids': task_id,\n        }\n        return self._perform_post_request(self.task_status_endpoint, data, self.token_header)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef result_consumed(self, task_id):\n        logger.debug('Sending result consumed message.')\n        data = {\n            'task_ids': task_id,\n        }\n        return self._perform_post_request(self.results_consumed_endpoint, data, self.token_header)", "response": "Report the result as successfully consumed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntreating the response from ASApi. is", "response": "def _parse_response(response, post_request=False):\n        \"\"\"Treat the response from ASApi.\n\n        The json is dumped before checking the status as even if the response is\n        not properly formed we are in trouble.\n\n        TODO: Streamline error checking.\n        \"\"\"\n        data = response.json()\n\n        if not response.ok:\n            utils.error_message_and_exit('Push Api Error:', data)\n\n        if post_request and not data['success']:\n            raise Exception('Push Api Error: [%s]' % data['error'])\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove old FederateSLO objects for which the session do not exists anymore", "response": "def clean_deleted_sessions(cls):\n        \"\"\"remove old :class:`FederateSLO` object for which the session do not exists anymore\"\"\"\n        for federate_slo in cls.objects.all():\n            if not SessionStore(session_key=federate_slo.session_key).get('authenticated'):\n                federate_slo.delete()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends emails to ADMINS if the current django - cas - server version is not up to date.", "response": "def send_mails(cls):\n        \"\"\"\n            For each new django-cas-server version, if the current instance is not up to date\n            send one mail to ``settings.ADMINS``.\n        \"\"\"\n        if settings.CAS_NEW_VERSION_EMAIL_WARNING and settings.ADMINS:\n            try:\n                obj = cls.objects.get()\n            except cls.DoesNotExist:\n                obj = NewVersionWarning.objects.create(version=VERSION)\n            LAST_VERSION = utils.last_version()\n            if LAST_VERSION is not None and LAST_VERSION != obj.version:\n                if utils.decode_version(VERSION) < utils.decode_version(LAST_VERSION):\n                    try:\n                        send_mail(\n                            (\n                                '%sA new version of django-cas-server is available'\n                            ) % settings.EMAIL_SUBJECT_PREFIX,\n                            u'''\nA new version of the django-cas-server is available.\n\nYour version: %s\nNew version: %s\n\nUpgrade using:\n    * pip install -U django-cas-server\n    * fetching the last release on\n      https://github.com/nitmir/django-cas-server/ or on\n      https://pypi.org/project/django-cas-server/\n\nAfter upgrade, do not forget to run:\n    * ./manage.py migrate\n    * ./manage.py collectstatic\nand to reload your wsgi server (apache2, uwsgi, gunicord, etc\u2026)\n\n--\\u0020\ndjango-cas-server\n'''.strip() % (VERSION, LAST_VERSION),\n                            settings.SERVER_EMAIL,\n                            [\"%s <%s>\" % admin for admin in settings.ADMINS],\n                            fail_silently=False,\n                        )\n                        obj.version = LAST_VERSION\n                        obj.save()\n                    except smtplib.SMTPException as error:  # pragma: no cover (should not happen)\n                        logger.error(\"Unable to send new version mail: %s\" % error)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_login_url(self):\n        params = {'service': self.service_url}\n        if self.renew:\n            params.update({'renew': 'true'})\n\n        params.update(self.extra_login_params)\n        url = urllib_parse.urljoin(self.server_url, 'login')\n        query = urllib_parse.urlencode(params)\n        return url + '?' + query", "response": "Generates CAS login URL"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate CAS logout URL", "response": "def get_logout_url(self, redirect_url=None):\n        \"\"\"Generates CAS logout URL\"\"\"\n        url = urllib_parse.urljoin(self.server_url, 'logout')\n        if redirect_url:\n            params = {self.logout_redirect_param_name: redirect_url}\n            url += '?' + urllib_parse.urlencode(params)\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_proxy_url(self, pgt):\n        params = urllib_parse.urlencode({'pgt': pgt, 'targetService': self.service_url})\n        return \"%s/proxy?%s\" % (self.server_url, params)", "response": "Returns proxy url given the proxy granting ticket"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn proxy ticket given the proxy granting ticket", "response": "def get_proxy_ticket(self, pgt):\n        \"\"\"Returns proxy ticket given the proxy granting ticket\"\"\"\n        response = urllib_request.urlopen(self.get_proxy_url(pgt))\n        if response.code == 200:\n            from lxml import etree\n            root = etree.fromstring(response.read())\n            tickets = root.xpath(\n                \"//cas:proxyTicket\",\n                namespaces={\"cas\": \"http://www.yale.edu/tp/cas\"}\n            )\n            if len(tickets) == 1:\n                return tickets[0].text\n            errors = root.xpath(\n                \"//cas:authenticationFailure\",\n                namespaces={\"cas\": \"http://www.yale.edu/tp/cas\"}\n            )\n            if len(errors) == 1:\n                raise CASError(errors[0].attrib['code'], errors[0].text)\n        raise CASError(\"Bad http code %s\" % response.code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies CAS 1. 0 authentication ticket.", "response": "def verify_ticket(self, ticket):\n        \"\"\"Verifies CAS 1.0 authentication ticket.\n\n        Returns username on success and None on failure.\n        \"\"\"\n        params = [('ticket', ticket), ('service', self.service_url)]\n        if self.renew:\n            params.append(('renew', 'true'))\n        url = (urllib_parse.urljoin(self.server_url, 'validate') + '?' +\n               urllib_parse.urlencode(params))\n        page = urllib_request.urlopen(url)\n        try:\n            verified = page.readline().strip()\n            if verified == b'yes':\n                charset = self.get_page_charset(page, default=\"ascii\")\n                user = self.u(page.readline().strip(), charset)\n                return user, None, None\n            else:\n                return None, None, None\n        finally:\n            page.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nverifying CAS 2. 0 + 3. 0 + XML - based authentication ticket and returns extended attributes", "response": "def verify_ticket(self, ticket):\n        \"\"\"Verifies CAS 2.0+/3.0+ XML-based authentication ticket and returns extended attributes\"\"\"\n        (response, charset) = self.get_verification_response(ticket)\n        return self.verify_response(response, charset)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the SAML assertion for the CAS Service Ticket.", "response": "def get_saml_assertion(cls, ticket):\n        \"\"\"\n        http://www.jasig.org/cas/protocol#samlvalidate-cas-3.0\n\n        SAML request values:\n\n        RequestID [REQUIRED]:\n            unique identifier for the request\n        IssueInstant [REQUIRED]:\n            timestamp of the request\n        samlp:AssertionArtifact [REQUIRED]:\n            the valid CAS Service Ticket obtained as a response parameter at login.\n        \"\"\"\n        # RequestID [REQUIRED] - unique identifier for the request\n        request_id = uuid4()\n\n        # e.g. 2014-06-02T09:21:03.071189\n        timestamp = datetime.datetime.now().isoformat()\n\n        return SAML_ASSERTION_TEMPLATE.format(\n            request_id=request_id,\n            timestamp=timestamp,\n            ticket=ticket,\n        ).encode('utf8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a connection object to the ldap database", "response": "def get_conn(cls):\n        \"\"\"Return a connection object to the ldap database\"\"\"\n        conn = cls._conn\n        if conn is None or conn.closed:\n            conn = ldap3.Connection(\n                settings.CAS_LDAP_SERVER,\n                settings.CAS_LDAP_USER,\n                settings.CAS_LDAP_PASSWORD,\n                client_strategy=\"RESTARTABLE\",\n                auto_bind=True\n            )\n            cls._conn = conn\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dict with the user attributes defined as the fields on the user object.", "response": "def attributs(self):\n        \"\"\"\n            The user attributes, defined as the fields on the :attr:`user` object.\n\n            :return: a :class:`dict` with the :attr:`user` object fields. Attributes may be\n                If the user do not exists, the returned :class:`dict` is empty.\n            :rtype: dict\n        \"\"\"\n        if self.user:\n            attr = {}\n            # _meta.get_fields() is from the new documented _meta interface in django 1.8\n            try:\n                field_names = [\n                    field.attname for field in self.user._meta.get_fields()\n                    if hasattr(field, \"attname\")\n                ]\n            # backward compatibility with django 1.7\n            except AttributeError:  # pragma: no cover (only used by django 1.7)\n                field_names = self.user._meta.get_all_field_names()\n            for name in field_names:\n                attr[name] = getattr(self.user, name)\n\n            # unfold user_permissions many to many relation\n            if 'user_permissions' in attr:\n                attr['user_permissions'] = [\n                    (\n                        u\"%s.%s\" % (\n                            perm.content_type.model_class().__module__,\n                            perm.content_type.model_class().__name__\n                        ),\n                        perm.codename\n                    ) for perm in attr['user_permissions'].filter()\n                ]\n\n            # unfold group many to many relation\n            if 'groups' in attr:\n                attr['groups'] = [group.name for group in attr['groups'].filter()]\n\n            return attr\n        else:\n            return {}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode a python object to json", "response": "def json_encode(obj):\n    \"\"\"Encode a python object to json\"\"\"\n    try:\n        return json_encode.encoder.encode(obj)\n    except AttributeError:\n        json_encode.encoder = DjangoJSONEncoder(default=six.text_type)\n        return json_encode(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef context(params):\n    params[\"settings\"] = settings\n    params[\"message_levels\"] = DEFAULT_MESSAGE_LEVELS\n\n    if settings.CAS_NEW_VERSION_HTML_WARNING:\n        LAST_VERSION = last_version()\n        params[\"VERSION\"] = VERSION\n        params[\"LAST_VERSION\"] = LAST_VERSION\n        if LAST_VERSION is not None:\n            params[\"upgrade_available\"] = decode_version(VERSION) < decode_version(LAST_VERSION)\n        else:\n            params[\"upgrade_available\"] = False\n\n    if settings.CAS_INFO_MESSAGES_ORDER:\n        params[\"CAS_INFO_RENDER\"] = []\n        for msg_name in settings.CAS_INFO_MESSAGES_ORDER:\n            if msg_name in settings.CAS_INFO_MESSAGES:\n                if not isinstance(settings.CAS_INFO_MESSAGES[msg_name], dict):\n                    continue\n                msg = settings.CAS_INFO_MESSAGES[msg_name].copy()\n                if \"message\" in msg:\n                    msg[\"name\"] = msg_name\n                    # use info as default infox type\n                    msg[\"type\"] = msg.get(\"type\", \"info\")\n                    # make box discardable by default\n                    msg[\"discardable\"] = msg.get(\"discardable\", True)\n                    msg_hash = (\n                        six.text_type(msg[\"message\"]).encode(\"utf-8\") +\n                        msg[\"type\"].encode(\"utf-8\")\n                    )\n                    # hash depend of the rendering language\n                    msg[\"hash\"] = hashlib.md5(msg_hash).hexdigest()\n                    params[\"CAS_INFO_RENDER\"].append(msg)\n    return params", "response": "Function that adds somes variable to the context dictionary that will be used to render templates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef json_response(request, data):\n    data[\"messages\"] = []\n    for msg in messages.get_messages(request):\n        data[\"messages\"].append({'message': msg.message, 'level': msg.level_tag})\n    return HttpResponse(json.dumps(data), content_type=\"application/json\")", "response": "Returns a json response with the data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_attr(path):\n    # if we got a str, decode it to unicode (normally it should only contain ascii)\n    if isinstance(path, six.binary_type):\n        path = path.decode(\"utf-8\")\n    # if path is not an unicode, return it unchanged (may be it is already the attribute to import)\n    if not isinstance(path, six.text_type):\n        return path\n    if u\".\" not in path:\n        ValueError(\"%r should be of the form `module.attr` and we just got `attr`\" % path)\n    module, attr = path.rsplit(u'.', 1)\n    try:\n        return getattr(import_module(module), attr)\n    except ImportError:\n        raise ImportError(\"Module %r not found\" % module)\n    except AttributeError:\n        raise AttributeError(\"Module %r has not attribut %r\" % (module, attr))", "response": "Transform a dotted path to a python object or unchanged"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef redirect_params(url_name, params=None):\n    url = reverse(url_name)\n    params = urlencode(params if params else {})\n    return HttpResponseRedirect(url + \"?%s\" % params)", "response": "Redirect to url_name with params as querystring."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reverse_params(url_name, params=None, **kwargs):\n    url = reverse(url_name, **kwargs)\n    params = urlencode(params if params else {})\n    if params:\n        return u\"%s?%s\" % (url, params)\n    else:\n        return url", "response": "Compute the reverse URL of url_name with possible querystring from params as querystring"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy_params(get_or_post_params, ignore=None):\n    if ignore is None:\n        ignore = set()\n    params = {}\n    for key in get_or_post_params:\n        if key not in ignore and get_or_post_params[key]:\n            params[key] = get_or_post_params[key]\n    return params", "response": "Copy a GET or POST parameters into a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the cookie key on response with value valid for max_age seconds.", "response": "def set_cookie(response, key, value, max_age):\n    \"\"\"\n        Set the cookie ``key`` on ``response`` with value ``value`` valid for ``max_age`` secondes\n\n        :param django.http.HttpResponse response: a django response where to set the cookie\n        :param unicode key: the cookie key\n        :param unicode value: the cookie value\n        :param int max_age: the maximum validity age of the cookie\n    \"\"\"\n    expires = datetime.strftime(\n        datetime.utcnow() + timedelta(seconds=max_age),\n        \"%a, %d-%b-%Y %H:%M:%S GMT\"\n    )\n    response.set_cookie(\n        key,\n        value,\n        max_age=max_age,\n        expires=expires,\n        domain=settings.SESSION_COOKIE_DOMAIN,\n        secure=settings.SESSION_COOKIE_SECURE or None\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the current url of the current page.", "response": "def get_current_url(request, ignore_params=None):\n    \"\"\"\n        Giving a django request, return the current http url, possibly ignoring some GET parameters\n\n        :param django.http.HttpRequest request: The current request object.\n        :param set ignore_params: An optional set of GET parameters to ignore\n        :return: The URL of the current page, possibly omitting some parameters from\n            ``ignore_params`` in the querystring.\n        :rtype: unicode\n    \"\"\"\n    if ignore_params is None:\n        ignore_params = set()\n    protocol = u'https' if request.is_secure() else u\"http\"\n    service_url = u\"%s://%s%s\" % (protocol, request.get_host(), request.path)\n    if request.GET:\n        params = copy_params(request.GET, ignore_params)\n        if params:\n            service_url += u\"?%s\" % urlencode(params)\n    return service_url"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_url(url, params):\n    if not isinstance(url, bytes):\n        url = url.encode('utf-8')\n    for key, value in list(params.items()):\n        if not isinstance(key, bytes):\n            del params[key]\n            key = key.encode('utf-8')\n        if not isinstance(value, bytes):\n            value = value.encode('utf-8')\n        params[key] = value\n    url_parts = list(urlparse(url))\n    query = dict(parse_qsl(url_parts[4]))\n    query.update(params)\n    # make the params order deterministic\n    query = list(query.items())\n    query.sort()\n    url_query = urlencode(query)\n    if not isinstance(url_query, bytes):  # pragma: no cover in python3 urlencode return an unicode\n        url_query = url_query.encode(\"utf-8\")\n    url_parts[4] = url_query\n    return urlunparse(url_parts).decode('utf-8')", "response": "Update the url with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unpack_nested_exception(error):\n    i = 0\n    while True:\n        if error.args[i:]:\n            if isinstance(error.args[i], Exception):\n                error = error.args[i]\n                i = 0\n            else:\n                i += 1\n        else:\n            break\n    return error", "response": "Unpacks an exception into a nested exception."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a random ticket with the given prefix and length lg.", "response": "def _gen_ticket(prefix=None, lg=settings.CAS_TICKET_LEN):\n    \"\"\"\n        Generate a ticket with prefix ``prefix`` and length ``lg``\n\n        :param unicode prefix: An optional prefix (probably ST, PT, PGT or PGTIOU)\n        :param int lg: The length of the generated ticket (with the prefix)\n        :return: A randomlly generated ticket of length ``lg``\n        :rtype: unicode\n    \"\"\"\n    random_part = u''.join(\n        random.choice(\n            string.ascii_letters + string.digits\n        ) for _ in range(lg - len(prefix or \"\") - 1)\n    )\n    if prefix is not None:\n        return u'%s-%s' % (prefix, random_part)\n    else:\n        return random_part"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the value of the item in the nuplet at the specified index.", "response": "def get_tuple(nuplet, index, default=None):\n    \"\"\"\n        :param tuple nuplet: A tuple\n        :param int index: An index\n        :param default: An optional default value\n        :return: ``nuplet[index]`` if defined, else ``default`` (possibly ``None``)\n    \"\"\"\n    if nuplet is None:\n        return default\n    try:\n        return nuplet[index]\n    except IndexError:\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef crypt_salt_is_valid(salt):\n    if len(salt) < 2:\n        return False\n    else:\n        if salt[0] == '$':\n            if salt[1] == '$':\n                return False\n            else:\n                if '$' not in salt[1:]:\n                    return False\n                else:\n                    hashed = crypt.crypt(\"\", salt)\n                    if not hashed or '$' not in hashed[1:]:\n                        return False\n                    else:\n                        return True\n        else:\n            return True", "response": "Validate a salt as crypt"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_password(method, password, hashed_password, charset):\n    if not isinstance(password, six.binary_type):\n        password = password.encode(charset)\n    if not isinstance(hashed_password, six.binary_type):\n        hashed_password = hashed_password.encode(charset)\n    if method == \"plain\":\n        return password == hashed_password\n    elif method == \"crypt\":\n        if hashed_password.startswith(b'$'):\n            salt = b'$'.join(hashed_password.split(b'$', 3)[:-1])\n        elif hashed_password.startswith(b'_'):  # pragma: no cover old BSD format not supported\n            salt = hashed_password[:9]\n        else:\n            salt = hashed_password[:2]\n        if six.PY3:\n            password = password.decode(charset)\n            salt = salt.decode(charset)\n            hashed_password = hashed_password.decode(charset)\n        if not crypt_salt_is_valid(salt):\n            raise ValueError(\"System crypt implementation do not support the salt %r\" % salt)\n        crypted_password = crypt.crypt(password, salt)\n        return crypted_password == hashed_password\n    elif method == \"ldap\":\n        scheme = LdapHashUserPassword.get_scheme(hashed_password)\n        salt = LdapHashUserPassword.get_salt(hashed_password)\n        return LdapHashUserPassword.hash(scheme, password, salt, charset=charset) == hashed_password\n    elif (\n       method.startswith(\"hex_\") and\n       method[4:] in {\"md5\", \"sha1\", \"sha224\", \"sha256\", \"sha384\", \"sha512\"}\n    ):\n        return getattr(\n            hashlib,\n            method[4:]\n        )(password).hexdigest().encode(\"ascii\") == hashed_password.lower()\n    else:\n        raise ValueError(\"Unknown password method check %r\" % method)", "response": "Check that the password matches the hashed password using the method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef last_version():\n    try:\n        last_update, version, success = last_version._cache\n    except AttributeError:\n        last_update = 0\n        version = None\n        success = False\n    cache_delta = 24 * 3600 if success else 600\n    if (time.time() - last_update) < cache_delta:\n        return version\n    else:\n        try:\n            req = requests.get(settings.CAS_NEW_VERSION_JSON_URL)\n            data = json.loads(req.text)\n            version = data[\"info\"][\"version\"]\n            last_version._cache = (time.time(), version, True)\n            return version\n        except (\n            KeyError,\n            ValueError,\n            requests.exceptions.RequestException\n        ) as error:  # pragma: no cover (should not happen unless pypi is not available)\n            logger.error(\n                \"Unable to fetch %s: %s\" % (settings.CAS_NEW_VERSION_JSON_URL, error)\n            )\n            last_version._cache = (time.time(), version, False)", "response": "Fetch the last version from pypi and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntests that value is a valid regular expression.", "response": "def regexpr_validator(value):\n    \"\"\"\n        Test that ``value`` is a valid regular expression\n\n        :param unicode value: A regular expression to test\n        :raises ValidationError: if ``value`` is not a valid regular expression\n    \"\"\"\n    try:\n        re.compile(value)\n    except re.error:\n        raise ValidationError(\n            _('\"%(value)s\" is not a valid regular expression'),\n            params={'value': value}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nraise an error for a bad scheme.", "response": "def _raise_bad_scheme(cls, scheme, valid, msg):\n        \"\"\"\n            Raise :attr:`BadScheme` error for ``scheme``, possible valid scheme are\n            in ``valid``, the error message is ``msg``\n\n            :param bytes scheme: A bad scheme\n            :param list valid: A list a valid scheme\n            :param str msg: The error template message\n            :raises LdapHashUserPassword.BadScheme: always\n        \"\"\"\n        valid_schemes = [s.decode() for s in valid]\n        valid_schemes.sort()\n        raise cls.BadScheme(msg % (scheme, u\", \".join(valid_schemes)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhash a password with a given scheme using a salt.", "response": "def hash(cls, scheme, password, salt=None, charset=\"utf8\"):\n        \"\"\"\n           Hash ``password`` with ``scheme`` using ``salt``.\n           This three variable beeing encoded in ``charset``.\n\n           :param bytes scheme: A valid scheme\n           :param bytes password: A byte string to hash using ``scheme``\n           :param bytes salt: An optional salt to use if ``scheme`` requires any\n           :param str charset: The encoding of ``scheme``, ``password`` and ``salt``\n           :return: The hashed password encoded with ``charset``\n           :rtype: bytes\n        \"\"\"\n        scheme = scheme.upper()\n        cls._test_scheme(scheme)\n        if salt is None or salt == b\"\":\n            salt = b\"\"\n            cls._test_scheme_nosalt(scheme)\n        else:\n            cls._test_scheme_salt(scheme)\n        try:\n            return scheme + base64.b64encode(\n                cls._schemes_to_hash[scheme](password + salt).digest() + salt\n            )\n        except KeyError:\n            if six.PY3:\n                password = password.decode(charset)\n                salt = salt.decode(charset)\n            if not crypt_salt_is_valid(salt):\n                raise cls.BadSalt(\"System crypt implementation do not support the salt %r\" % salt)\n            hashed_password = crypt.crypt(password, salt)\n            if six.PY3:\n                hashed_password = hashed_password.encode(charset)\n            return scheme + hashed_password"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_scheme(cls, hashed_passord):\n        if not hashed_passord[0] == b'{'[0] or b'}' not in hashed_passord:\n            raise cls.BadHash(\"%r should start with the scheme enclosed with { }\" % hashed_passord)\n        scheme = hashed_passord.split(b'}', 1)[0]\n        scheme = scheme.upper() + b\"}\"\n        return scheme", "response": "Returns the scheme used by the hashed password\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_salt(cls, hashed_passord):\n        scheme = cls.get_scheme(hashed_passord)\n        cls._test_scheme(scheme)\n        if scheme in cls.schemes_nosalt:\n            return b\"\"\n        elif scheme == b'{CRYPT}':\n            return b'$'.join(hashed_passord.split(b'$', 3)[:-1])[len(scheme):]\n        else:\n            try:\n                hashed_passord = base64.b64decode(hashed_passord[len(scheme):])\n            except (TypeError, binascii.Error) as error:\n                raise cls.BadHash(\"Bad base64: %s\" % error)\n            if len(hashed_passord) < cls._schemes_to_len[scheme]:\n                raise cls.BadHash(\"Hash too short for the scheme %s\" % scheme)\n            return hashed_passord[cls._schemes_to_len[scheme]:]", "response": "Returns the salt used by the hashed password."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify_ticket(self, ticket):\n        try:\n            username, attributs = self.client.verify_ticket(ticket)[:2]\n        except urllib.error.URLError:\n            return False\n        if username is not None:\n            if attributs is None:\n                attributs = {}\n            attributs[\"provider\"] = self.provider.suffix\n            self.username = username\n            self.attributs = attributs\n            user = FederatedUser.objects.update_or_create(\n                username=username,\n                provider=self.provider,\n                defaults=dict(attributs=attributs, ticket=ticket)\n            )[0]\n            user.save()\n            self.federated_username = user.federated_username\n            return True\n        else:\n            return False", "response": "Verify the ticket against the CAS provider."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_slo(username, session_key, ticket):\n        try:\n            FederateSLO.objects.create(\n                username=username,\n                session_key=session_key,\n                ticket=ticket\n            )\n        except IntegrityError:  # pragma: no cover (ignore if the FederateSLO already exists)\n            pass", "response": "Registers a new FederateSLO object with a username session_key and ticket."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans the session store for a single log out request.", "response": "def clean_sessions(self, logout_request):\n        \"\"\"\n            process a SLO request: Search for ticket values in ``logout_request``. For each\n            ticket value matching a :class:`cas_server.models.FederateSLO`, disconnect the\n            corresponding user.\n\n            :param unicode logout_request: An XML document contening one or more Single Log Out\n                requests.\n        \"\"\"\n        try:\n            slos = self.client.get_saml_slos(logout_request) or []\n        except NameError:  # pragma: no cover (should not happen)\n            slos = []\n        for slo in slos:\n            for federate_slo in FederateSLO.objects.filter(ticket=slo.text):\n                logger.info(\n                    \"Got an SLO requests for ticket %s, logging out user %s\" % (\n                        federate_slo.username,\n                        federate_slo.ticket\n                    )\n                )\n                session = SessionStore(session_key=federate_slo.session_key)\n                session.flush()\n                try:\n                    user = User.objects.get(\n                        username=federate_slo.username,\n                        session_key=federate_slo.session_key\n                    )\n                    user.logout()\n                    user.delete()\n                except User.DoesNotExist:  # pragma: no cover (should not happen)\n                    pass\n                federate_slo.delete()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvisits handler for snippet node", "response": "def visit_snippet(self, node):\n    \"\"\"\n    HTML document generator visit handler\n    \"\"\"\n    lang = self.highlightlang\n    linenos = node.rawsource.count('\\n') >= self.highlightlinenothreshold - 1\n    fname = node['filename']\n    highlight_args = node.get('highlight_args', {})\n    if 'language' in node:\n        # code-block directives\n        lang = node['language']\n        highlight_args['force'] = True\n    if 'linenos' in node:\n        linenos = node['linenos']\n\n    def warner(msg):\n        self.builder.warn(msg, (self.builder.current_docname, node.line))\n\n    highlighted = self.highlighter.highlight_block(node.rawsource, lang,\n                                                   warn=warner,\n                                                   linenos=linenos,\n                                                   **highlight_args)\n    starttag = self.starttag(node, 'div', suffix='',\n                             CLASS='highlight-%s snippet' % lang)\n    self.body.append(starttag)\n    self.body.append('<div class=\"snippet-filename\">%s</div>\\n''' % (fname,))\n    self.body.append(highlighted)\n    self.body.append('</div>\\n')\n    raise nodes.SkipNode"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef active_link(context, viewnames, css_class=None, strict=None, *args, **kwargs):\n    if css_class is None:\n        css_class = getattr(settings, 'ACTIVE_LINK_CSS_CLASS', 'active')\n\n    if strict is None:\n        strict = getattr(settings, 'ACTIVE_LINK_STRICT', False)\n\n    request = context.get('request')\n    if request is None:\n        # Can't work without the request object.\n        return ''\n    active = False\n    views = viewnames.split('||')\n    for viewname in views:\n        path = reverse(viewname.strip(), args=args, kwargs=kwargs)\n        request_path = escape_uri_path(request.path)\n        if strict:\n            active = request_path == path\n        else:\n            active = request_path.find(path) == 0\n        if active:\n            break\n\n    if active:\n        return css_class\n\n    return ''", "response": "Renders the given CSS class if the request path matches the path of the view."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean(self):\n        cleaned_data = super(UserCredential, self).clean()\n        if \"username\" in cleaned_data and \"password\" in cleaned_data:\n            auth = utils.import_attr(settings.CAS_AUTH_CLASS)(cleaned_data[\"username\"])\n            if auth.test_password(cleaned_data[\"password\"]):\n                cleaned_data[\"username\"] = auth.username\n            else:\n                raise forms.ValidationError(\n                    _(u\"The credentials you provided cannot be determined to be authentic.\")\n                )\n        return cleaned_data", "response": "Validate that the username and password are valid and that the username and password are not already provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates that the username and password are valid using the CASFederateAuth class.", "response": "def clean(self):\n        \"\"\"\n            Validate that the submited :attr:`username` and :attr:`password` are valid using\n            the :class:`CASFederateAuth<cas_server.auth.CASFederateAuth>` auth class.\n\n            :raises django.forms.ValidationError: if the :attr:`username` and :attr:`password`\n                do not correspond to a :class:`FederatedUser<cas_server.models.FederatedUser>`.\n            :return: The cleaned POST data\n            :rtype: dict\n        \"\"\"\n        cleaned_data = super(FederateUserCredential, self).clean()\n        try:\n            user = models.FederatedUser.get_from_federated_username(cleaned_data[\"username\"])\n            user.ticket = \"\"\n            user.save()\n        # should not happed as if the FederatedUser do not exists, super should\n        # raise before a ValidationError(\"bad user\")\n        except models.FederatedUser.DoesNotExist:  # pragma: no cover (should not happend)\n            raise forms.ValidationError(\n                _(u\"User not found in the temporary database, please try to reconnect\")\n            )\n        return cleaned_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs out a user from the CAS session store.", "response": "def logout(self, all_session=False):\n        \"\"\"\n            effectively destroy a CAS session\n\n            :param boolean all_session: If ``True`` destroy all the user sessions, otherwise\n                destroy the current user session.\n            :return: The number of destroyed sessions\n            :rtype: int\n        \"\"\"\n        # initialize the counter of the number of destroyed sesisons\n        session_nb = 0\n        # save the current user username before flushing the session\n        username = self.request.session.get(\"username\")\n        if username:\n            if all_session:\n                logger.info(\"Logging out user %s from all sessions.\" % username)\n            else:\n                logger.info(\"Logging out user %s.\" % username)\n        users = []\n        # try to get the user from the current session\n        try:\n            users.append(\n                models.User.objects.get(\n                    username=username,\n                    session_key=self.request.session.session_key\n                )\n            )\n        except models.User.DoesNotExist:\n            # if user not found in database, flush the session anyway\n            self.request.session.flush()\n\n        # If all_session is set, search all of the user sessions\n        if all_session:\n            users.extend(\n                models.User.objects.filter(\n                    username=username\n                ).exclude(\n                    session_key=self.request.session.session_key\n                )\n            )\n\n        # Iterate over all user sessions that have to be logged out\n        for user in users:\n            # get the user session\n            session = SessionStore(session_key=user.session_key)\n            # flush the session\n            session.flush()\n            # send SLO requests\n            user.logout(self.request)\n            # delete the user\n            user.delete()\n            # increment the destroyed session counter\n            session_nb += 1\n        if username:\n            logger.info(\"User %s logged out\" % username)\n        return session_nb"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_get(self, request):\n        self.request = request\n        self.service = request.GET.get('service')\n        self.url = request.GET.get('url')\n        self.ajax = settings.CAS_ENABLE_AJAX_AUTH and 'HTTP_X_AJAX' in request.META", "response": "Initialize the object attributes on GET request"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cas_client(self, request, provider, renew=False):\n        # compute the current url, ignoring ticket dans provider GET parameters\n        service_url = utils.get_current_url(request, {\"ticket\", \"provider\"})\n        self.service_url = service_url\n        return CASFederateValidateUser(provider, service_url, renew=renew)", "response": "Returns a CAS client object matching the user identity provider and the current request object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(self, request, provider=None):\n        # if settings.CAS_FEDERATE is not True redirect to the login page\n        if not settings.CAS_FEDERATE:\n            logger.warning(\"CAS_FEDERATE is False, set it to True to use federation\")\n            return redirect(\"cas_server:login\")\n        # POST with a provider suffix, this is probably an SLO request. csrf is disabled for\n        # allowing SLO requests reception\n        try:\n            provider = FederatedIendityProvider.objects.get(suffix=provider)\n            auth = self.get_cas_client(request, provider)\n            try:\n                auth.clean_sessions(request.POST['logoutRequest'])\n            except (KeyError, AttributeError):\n                pass\n            return HttpResponse(\"ok\")\n        # else, a User is trying to log in using an identity provider\n        except FederatedIendityProvider.DoesNotExist:\n            # Manually checking for csrf to protect the code below\n            reason = CsrfViewMiddleware().process_view(request, None, (), {})\n            if reason is not None:  # pragma: no cover (csrf checks are disabled during tests)\n                return reason  # Failed the test, stop here.\n            form = forms.FederateSelect(request.POST)\n            if form.is_valid():\n                params = utils.copy_params(\n                    request.POST,\n                    ignore={\"provider\", \"csrfmiddlewaretoken\", \"ticket\", \"lt\"}\n                )\n                if params.get(\"renew\") == \"False\":\n                    del params[\"renew\"]\n                url = utils.reverse_params(\n                    \"cas_server:federateAuth\",\n                    kwargs=dict(provider=form.cleaned_data[\"provider\"].suffix),\n                    params=params\n                )\n                return HttpResponseRedirect(url)\n            else:\n                return redirect(\"cas_server:login\")", "response": "method called on POST request"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the object with the POST received parameters.", "response": "def init_post(self, request):\n        \"\"\"\n            Initialize POST received parameters\n\n            :param django.http.HttpRequest request: The current request object\n        \"\"\"\n        self.request = request\n        self.service = request.POST.get('service')\n        self.renew = bool(request.POST.get('renew') and request.POST['renew'] != \"False\")\n        self.gateway = request.POST.get('gateway')\n        self.method = request.POST.get('method')\n        self.ajax = settings.CAS_ENABLE_AJAX_AUTH and 'HTTP_X_AJAX' in request.META\n        if request.POST.get('warned') and request.POST['warned'] != \"False\":\n            self.warned = True\n        self.warn = request.POST.get('warn')\n        if settings.CAS_FEDERATE:\n            self.username = request.POST.get('username')\n            # in federated mode, the valdated indentity provider CAS ticket is used as password\n            self.ticket = request.POST.get('password')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_lt(self):\n        self.request.session['lt'] = self.request.session.get('lt', []) + [utils.gen_lt()]\n        if len(self.request.session['lt']) > 100:\n            self.request.session['lt'] = self.request.session['lt'][-100:]", "response": "Generate a new LoginTicket and add it to the list of valid LT for the user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_lt(self):\n        # save LT for later check\n        lt_valid = self.request.session.get('lt', [])\n        lt_send = self.request.POST.get('lt')\n        # generate a new LT (by posting the LT has been consumed)\n        self.gen_lt()\n        # check if send LT is valid\n        if lt_send not in lt_valid:\n            return False\n        else:\n            self.request.session['lt'].remove(lt_send)\n            # we need to redo the affectation for django to detect that the list has changed\n            # and for its new value to be store in the session\n            self.request.session['lt'] = self.request.session['lt']\n            return True", "response": "Checks if the POSTed LoginTicket is valid and if yes invalide it\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess the POST request and return the ID of the user that was logged in.", "response": "def process_post(self):\n        \"\"\"\n            Analyse the POST request:\n\n                * check that the LoginTicket is valid\n                * check that the user sumited credentials are valid\n\n            :return:\n                * :attr:`INVALID_LOGIN_TICKET` if the POSTed LoginTicket is not valid\n                * :attr:`USER_ALREADY_LOGGED` if the user is already logged and do no request\n                  reauthentication.\n                * :attr:`USER_LOGIN_FAILURE` if the user is not logged or request for\n                  reauthentication and his credentials are not valid\n                * :attr:`USER_LOGIN_OK` if the user is not logged or request for\n                  reauthentication and his credentials are valid\n            :rtype: int\n        \"\"\"\n        if not self.check_lt():\n            self.init_form(self.request.POST)\n            logger.warning(\"Received an invalid login ticket\")\n            return self.INVALID_LOGIN_TICKET\n        elif not self.request.session.get(\"authenticated\") or self.renew:\n            # authentication request receive, initialize the form to use\n            self.init_form(self.request.POST)\n            if self.form.is_valid():\n                self.request.session.set_expiry(0)\n                self.request.session[\"username\"] = self.form.cleaned_data['username']\n                self.request.session[\"warn\"] = True if self.form.cleaned_data.get(\"warn\") else False\n                self.request.session[\"authenticated\"] = True\n                self.renewed = True\n                self.warned = True\n                logger.info(\"User %s successfully authenticated\" % self.request.session[\"username\"])\n                return self.USER_LOGIN_OK\n            else:\n                logger.warning(\"A login attempt failed\")\n                return self.USER_LOGIN_FAILURE\n        else:\n            logger.warning(\"Received a login attempt for an already-active user\")\n            return self.USER_ALREADY_LOGGED"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the object properties based on the GET received parameters.", "response": "def init_get(self, request):\n        \"\"\"\n            Initialize GET received parameters\n\n            :param django.http.HttpRequest request: The current request object\n        \"\"\"\n        self.request = request\n        self.service = request.GET.get('service')\n        self.renew = bool(request.GET.get('renew') and request.GET['renew'] != \"False\")\n        self.gateway = request.GET.get('gateway')\n        self.method = request.GET.get('method')\n        self.ajax = settings.CAS_ENABLE_AJAX_AUTH and 'HTTP_X_AJAX' in request.META\n        self.warn = request.GET.get('warn')\n        if settings.CAS_FEDERATE:\n            # here username and ticket are fetch from the session after a redirection from\n            # FederateAuth.get\n            self.username = request.session.get(\"federate_username\")\n            self.ticket = request.session.get(\"federate_ticket\")\n            if self.username:\n                del request.session[\"federate_username\"]\n            if self.ticket:\n                del request.session[\"federate_ticket\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, request, *args, **kwargs):\n        # initialize class parameters\n        self.init_get(request)\n        # process the GET request\n        self.process_get()\n        # call the GET/POST common part\n        return self.common()", "response": "This method is called by the get method of the view object that is being used to get the current object s attributes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_get(self):\n        # generate a new LT\n        self.gen_lt()\n        if not self.request.session.get(\"authenticated\") or self.renew:\n            # authentication will be needed, initialize the form to use\n            self.init_form()\n            return self.USER_NOT_AUTHENTICATED\n        return self.USER_AUTHENTICATED", "response": "Analyse the GET request and return the LT object that is used to store the LT object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the good form depending of POST and GET parameters.", "response": "def init_form(self, values=None):\n        \"\"\"\n            Initialization of the good form depending of POST and GET parameters\n\n            :param django.http.QueryDict values: A POST or GET QueryDict\n        \"\"\"\n        if values:\n            values = values.copy()\n            values['lt'] = self.request.session['lt'][-1]\n        form_initial = {\n            'service': self.service,\n            'method': self.method,\n            'warn': (\n                self.warn or self.request.session.get(\"warn\") or self.request.COOKIES.get('warn')\n            ),\n            'lt': self.request.session['lt'][-1],\n            'renew': self.renew\n        }\n        if settings.CAS_FEDERATE:\n            if self.username and self.ticket:\n                form_initial['username'] = self.username\n                form_initial['password'] = self.ticket\n                form_initial['ticket'] = self.ticket\n                self.form = forms.FederateUserCredential(\n                    values,\n                    initial=form_initial\n                )\n            else:\n                self.form = forms.FederateSelect(values, initial=form_initial)\n        else:\n            self.form = forms.UserCredential(\n                values,\n                initial=form_initial\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming login against a service.", "response": "def service_login(self):\n        \"\"\"\n            Perform login against a service\n\n            :return:\n                * The rendering of the ``settings.CAS_WARN_TEMPLATE`` if the user asked to be\n                  warned before ticket emission and has not yep been warned.\n                * The redirection to the service URL with a ticket GET parameter\n                * The redirection to the service URL without a ticket if ticket generation failed\n                  and the :attr:`gateway` attribute is set\n                * The rendering of the ``settings.CAS_LOGGED_TEMPLATE`` template with some error\n                  messages if the ticket generation failed (e.g: user not allowed).\n            :rtype: django.http.HttpResponse\n        \"\"\"\n        try:\n            # is the service allowed\n            service_pattern = ServicePattern.validate(self.service)\n            # is the current user allowed on this service\n            service_pattern.check_user(self.user)\n            # if the user has asked to be warned before any login to a service\n            if self.request.session.get(\"warn\", True) and not self.warned:\n                messages.add_message(\n                    self.request,\n                    messages.WARNING,\n                    _(u\"Authentication has been required by service %(name)s (%(url)s)\") %\n                    {'name': service_pattern.name, 'url': self.service}\n                )\n                if self.ajax:\n                    data = {\"status\": \"error\", \"detail\": \"confirmation needed\"}\n                    return json_response(self.request, data)\n                else:\n                    warn_form = forms.WarnForm(initial={\n                        'service': self.service,\n                        'renew': self.renew,\n                        'gateway': self.gateway,\n                        'method': self.method,\n                        'warned': True,\n                        'lt': self.request.session['lt'][-1]\n                    })\n                    return render(\n                        self.request,\n                        settings.CAS_WARN_TEMPLATE,\n                        utils.context({'form': warn_form})\n                    )\n            else:\n                # redirect, using method ?\n                list(messages.get_messages(self.request))  # clean messages before leaving django\n                redirect_url = self.user.get_service_url(\n                    self.service,\n                    service_pattern,\n                    renew=self.renewed\n                )\n                if not self.ajax:\n                    return HttpResponseRedirect(redirect_url)\n                else:\n                    data = {\"status\": \"success\", \"detail\": \"auth\", \"url\": redirect_url}\n                    return json_response(self.request, data)\n        except ServicePattern.DoesNotExist:\n            error = 1\n            messages.add_message(\n                self.request,\n                messages.ERROR,\n                _(u'Service %(url)s not allowed.') % {'url': self.service}\n            )\n        except models.BadUsername:\n            error = 2\n            messages.add_message(\n                self.request,\n                messages.ERROR,\n                _(u\"Username not allowed\")\n            )\n        except models.BadFilter:\n            error = 3\n            messages.add_message(\n                self.request,\n                messages.ERROR,\n                _(u\"User characteristics not allowed\")\n            )\n        except models.UserFieldNotDefined:\n            error = 4\n            messages.add_message(\n                self.request,\n                messages.ERROR,\n                _(u\"The attribute %(field)s is needed to use\"\n                  u\" that service\") % {'field': service_pattern.user_field}\n            )\n\n        # if gateway is set and auth failed redirect to the service without authentication\n        if self.gateway and not self.ajax:\n            list(messages.get_messages(self.request))  # clean messages before leaving django\n            return HttpResponseRedirect(self.service)\n\n        if not self.ajax:\n            return render(\n                self.request,\n                settings.CAS_LOGGED_TEMPLATE,\n                utils.context({'session': self.request.session})\n            )\n        else:\n            data = {\"status\": \"error\", \"detail\": \"auth\", \"code\": error}\n            return json_response(self.request, data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef authenticated(self):\n        # Try to get the current :class:`models.User<cas_server.models.User>` object for the current\n        # session\n        try:\n            self.user = models.User.objects.get(\n                username=self.request.session.get(\"username\"),\n                session_key=self.request.session.session_key\n            )\n        # if not found, flush the session and redirect to the login page\n        except models.User.DoesNotExist:\n            logger.warning(\n                \"User %s seems authenticated but is not found in the database.\" % (\n                    self.request.session.get(\"username\"),\n                )\n            )\n            self.logout()\n            if self.ajax:\n                data = {\n                    \"status\": \"error\",\n                    \"detail\": \"login required\",\n                    \"url\": utils.reverse_params(\"cas_server:login\", params=self.request.GET)\n                }\n                return json_response(self.request, data)\n            else:\n                return utils.redirect_params(\"cas_server:login\", params=self.request.GET)\n\n        # if login against a service\n        if self.service:\n            return self.service_login()\n        # else display the logged template\n        else:\n            if self.ajax:\n                data = {\"status\": \"success\", \"detail\": \"logged\"}\n                return json_response(self.request, data)\n            else:\n                return render(\n                    self.request,\n                    settings.CAS_LOGGED_TEMPLATE,\n                    utils.context({'session': self.request.session})\n                )", "response": "Handles authenticated users and returns a response of a user object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles non authenticated users and returns a response to the user.", "response": "def not_authenticated(self):\n        \"\"\"\n            Processing non authenticated users\n\n            :return:\n                * The rendering of ``settings.CAS_LOGIN_TEMPLATE`` with various messages\n                  depending of GET/POST parameters\n                * The redirection to :class:`FederateAuth` if ``settings.CAS_FEDERATE`` is ``True``\n                  and the \"remember my identity provider\" cookie is found\n            :rtype: django.http.HttpResponse\n        \"\"\"\n        if self.service:\n            try:\n                service_pattern = ServicePattern.validate(self.service)\n                if self.gateway and not self.ajax:\n                    # clean messages before leaving django\n                    list(messages.get_messages(self.request))\n                    return HttpResponseRedirect(self.service)\n\n                if settings.CAS_SHOW_SERVICE_MESSAGES:\n                    if self.request.session.get(\"authenticated\") and self.renew:\n                        messages.add_message(\n                            self.request,\n                            messages.WARNING,\n                            _(u\"Authentication renewal required by service %(name)s (%(url)s).\") %\n                            {'name': service_pattern.name, 'url': self.service}\n                        )\n                    else:\n                        messages.add_message(\n                            self.request,\n                            messages.WARNING,\n                            _(u\"Authentication required by service %(name)s (%(url)s).\") %\n                            {'name': service_pattern.name, 'url': self.service}\n                        )\n            except ServicePattern.DoesNotExist:\n                if settings.CAS_SHOW_SERVICE_MESSAGES:\n                    messages.add_message(\n                        self.request,\n                        messages.ERROR,\n                        _(u'Service %s not allowed') % self.service\n                    )\n        if self.ajax:\n            data = {\n                \"status\": \"error\",\n                \"detail\": \"login required\",\n                \"url\": utils.reverse_params(\"cas_server:login\",  params=self.request.GET)\n            }\n            return json_response(self.request, data)\n        else:\n            if settings.CAS_FEDERATE:\n                if self.username and self.ticket:\n                    return render(\n                        self.request,\n                        settings.CAS_LOGIN_TEMPLATE,\n                        utils.context({\n                            'form': self.form,\n                            'auto_submit': True,\n                            'post_url': reverse(\"cas_server:login\")\n                        })\n                    )\n                else:\n                    if (\n                        self.request.COOKIES.get('remember_provider') and\n                        FederatedIendityProvider.objects.filter(\n                            suffix=self.request.COOKIES['remember_provider']\n                        )\n                    ):\n                        params = utils.copy_params(self.request.GET)\n                        url = utils.reverse_params(\n                            \"cas_server:federateAuth\",\n                            params=params,\n                            kwargs=dict(provider=self.request.COOKIES['remember_provider'])\n                        )\n                        return HttpResponseRedirect(url)\n                    else:\n                        # if user is authenticated and auth renewal is requested, redirect directly\n                        # to the user identity provider\n                        if self.renew and self.request.session.get(\"authenticated\"):\n                            try:\n                                user = FederatedUser.get_from_federated_username(\n                                    self.request.session.get(\"username\")\n                                )\n                                params = utils.copy_params(self.request.GET)\n                                url = utils.reverse_params(\n                                    \"cas_server:federateAuth\",\n                                    params=params,\n                                    kwargs=dict(provider=user.provider.suffix)\n                                )\n                                return HttpResponseRedirect(url)\n                            # Should normally not happen: if the user is logged, it exists in the\n                            # database.\n                            except FederatedUser.DoesNotExist:  # pragma: no cover\n                                pass\n                        return render(\n                            self.request,\n                            settings.CAS_LOGIN_TEMPLATE,\n                            utils.context({\n                                'form': self.form,\n                                'post_url': reverse(\"cas_server:federateAuth\")\n                            })\n                        )\n            else:\n                return render(\n                    self.request,\n                    settings.CAS_LOGIN_TEMPLATE,\n                    utils.context({'form': self.form})\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post(request):\n        username = request.POST.get('username')\n        password = request.POST.get('password')\n        service = request.POST.get('service')\n        secret = request.POST.get('secret')\n\n        if not settings.CAS_AUTH_SHARED_SECRET:\n            return HttpResponse(\n                \"no\\nplease set CAS_AUTH_SHARED_SECRET\",\n                content_type=\"text/plain; charset=utf-8\"\n            )\n        if secret != settings.CAS_AUTH_SHARED_SECRET:\n            return HttpResponse(u\"no\\n\", content_type=\"text/plain; charset=utf-8\")\n        if not username or not password or not service:\n            return HttpResponse(u\"no\\n\", content_type=\"text/plain; charset=utf-8\")\n        form = forms.UserCredential(\n            request.POST,\n            initial={\n                'service': service,\n                'method': 'POST',\n                'warn': False\n            }\n        )\n        if form.is_valid():\n            try:\n                user = models.User.objects.get_or_create(\n                    username=form.cleaned_data['username'],\n                    session_key=request.session.session_key\n                )[0]\n                user.save()\n                # is the service allowed\n                service_pattern = ServicePattern.validate(service)\n                # is the current user allowed on this service\n                service_pattern.check_user(user)\n                if not request.session.get(\"authenticated\"):\n                    user.delete()\n                return HttpResponse(u\"yes\\n\", content_type=\"text/plain; charset=utf-8\")\n            except (ServicePattern.DoesNotExist, models.ServicePatternException):\n                return HttpResponse(u\"no\\n\", content_type=\"text/plain; charset=utf-8\")\n        else:\n            return HttpResponse(u\"no\\n\", content_type=\"text/plain; charset=utf-8\")", "response": "method called on POST request on this view"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_ticket(self):\n        try:\n            proxies = []\n            if self.allow_proxy_ticket:\n                ticket = models.Ticket.get(self.ticket, self.renew)\n            else:\n                ticket = models.ServiceTicket.get(self.ticket, self.renew)\n            try:\n                for prox in ticket.proxies.all():\n                    proxies.append(prox.url)\n            except AttributeError:\n                pass\n            if ticket.service != self.service:\n                raise ValidateError(u'INVALID_SERVICE', self.service)\n            return ticket, proxies\n        except Ticket.DoesNotExist:\n            raise ValidateError(u'INVALID_TICKET', self.ticket)\n        except (ServiceTicket.DoesNotExist, ProxyTicket.DoesNotExist):\n            raise ValidateError(u'INVALID_TICKET', 'ticket not found')", "response": "Process the ticket against the database and check its validity."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_pgturl(self, params):\n        try:\n            pattern = ServicePattern.validate(self.pgt_url)\n            if pattern.proxy_callback:\n                proxyid = utils.gen_pgtiou()\n                pticket = ProxyGrantingTicket.objects.create(\n                    user=self.ticket.user,\n                    service=self.pgt_url,\n                    service_pattern=pattern,\n                    single_log_out=pattern.single_log_out\n                )\n                url = utils.update_url(self.pgt_url, {'pgtIou': proxyid, 'pgtId': pticket.value})\n                try:\n                    ret = requests.get(url, verify=settings.CAS_PROXY_CA_CERTIFICATE_PATH)\n                    if ret.status_code == 200:\n                        params['proxyGrantingTicket'] = proxyid\n                    else:\n                        pticket.delete()\n                    logger.info(\n                        (\n                            \"ValidateService: ticket %s validated for user %s on service %s. \"\n                            \"Proxy Granting Ticket transmited to %s.\"\n                        ) % (\n                            self.ticket.value,\n                            self.ticket.user.username,\n                            self.ticket.service,\n                            self.pgt_url\n                        )\n                    )\n                    logger.debug(\n                        \"ValidateService: User attributs are:\\n%s\" % (\n                            pprint.pformat(self.ticket.attributs),\n                        )\n                    )\n                    return render(\n                        self.request,\n                        \"cas_server/serviceValidate.xml\",\n                        params,\n                        content_type=\"text/xml; charset=utf-8\"\n                    )\n                except requests.exceptions.RequestException as error:\n                    error = utils.unpack_nested_exception(error)\n                    raise ValidateError(\n                        u'INVALID_PROXY_CALLBACK',\n                        u\"%s: %s\" % (type(error), str(error))\n                    )\n            else:\n                raise ValidateError(\n                    u'INVALID_PROXY_CALLBACK',\n                    u\"callback url not allowed by configuration\"\n                )\n        except ServicePattern.DoesNotExist:\n            raise ValidateError(\n                u'INVALID_PROXY_CALLBACK',\n                u'callback url not allowed by configuration'\n            )", "response": "Process a PGT URL and return the XML response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles PT request and return the proxy. xml", "response": "def process_proxy(self):\n        \"\"\"\n            handle PT request\n\n            :raises ValidateError: if the PGT is not found, or the target service not allowed or\n                the user not allowed on the tardet service.\n            :return: The rendering of ``cas_server/proxy.xml``\n            :rtype: django.http.HttpResponse\n        \"\"\"\n        try:\n            # is the target service allowed\n            pattern = ServicePattern.validate(self.target_service)\n            # to get a proxy ticket require that the service allow it\n            if not pattern.proxy:\n                raise ValidateError(\n                    u'UNAUTHORIZED_SERVICE',\n                    u'the service %s does not allow proxy tickets' % self.target_service\n                )\n            # is the proxy granting ticket valid\n            ticket = ProxyGrantingTicket.get(self.pgt)\n            # is the pgt user allowed on the target service\n            pattern.check_user(ticket.user)\n            pticket = ticket.user.get_ticket(\n                ProxyTicket,\n                self.target_service,\n                pattern,\n                renew=False\n            )\n            models.Proxy.objects.create(proxy_ticket=pticket, url=ticket.service)\n            logger.info(\n                \"Proxy ticket created for user %s on service %s.\" % (\n                    ticket.user.username,\n                    self.target_service\n                )\n            )\n            return render(\n                self.request,\n                \"cas_server/proxy.xml\",\n                {'ticket': pticket.value},\n                content_type=\"text/xml; charset=utf-8\"\n            )\n        except (Ticket.DoesNotExist, ProxyGrantingTicket.DoesNotExist):\n            raise ValidateError(u'INVALID_TICKET', u'PGT %s not found' % self.pgt)\n        except ServicePattern.DoesNotExist:\n            raise ValidateError(u'UNAUTHORIZED_SERVICE', self.target_service)\n        except (models.BadUsername, models.BadFilter, models.UserFieldNotDefined):\n            raise ValidateError(\n                u'UNAUTHORIZED_USER',\n                u'User %s not allowed on %s' % (ticket.user.username, self.target_service)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary that can be used to render a log entry.", "response": "def context(self):\n        \"\"\"\n            :return: A dictionary to contextualize :attr:`template`\n            :rtype: dict\n        \"\"\"\n        return {\n            'code': self.code,\n            'msg': self.msg,\n            'IssueInstant': timezone.now().isoformat(),\n            'ResponseID': utils.gen_saml_id()\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(self, request):\n        self.request = request\n        self.target = request.GET.get('TARGET')\n        self.root = etree.fromstring(request.body)\n        try:\n            self.ticket = self.process_ticket()\n            expire_instant = (self.ticket.creation +\n                              timedelta(seconds=self.ticket.VALIDITY)).isoformat()\n            params = {\n                'IssueInstant': timezone.now().isoformat(),\n                'expireInstant': expire_instant,\n                'Recipient': self.target,\n                'ResponseID': utils.gen_saml_id(),\n                'username': self.ticket.username(),\n                'attributes': self.ticket.attributs_flat(),\n                'auth_date': self.ticket.user.last_login.replace(microsecond=0).isoformat(),\n                'is_new_login': 'true' if self.ticket.renew else 'false'\n\n            }\n            logger.info(\n                \"SamlValidate: ticket %s validated for user %s on service %s.\" % (\n                    self.ticket.value,\n                    self.ticket.user.username,\n                    self.ticket.service\n                )\n            )\n            logger.debug(\n                \"SamlValidate: User attributes are:\\n%s\" % pprint.pformat(self.ticket.attributs)\n            )\n\n            return render(\n                request,\n                \"cas_server/samlValidate.xml\",\n                params,\n                content_type=\"text/xml; charset=utf-8\"\n            )\n        except SamlValidateError as error:\n            logger.warning(\"SamlValidate: validation error: %s %s\" % (error.code, error.msg))\n            return error.render(request)", "response": "This method is called by the SAMLValidate class when a POST request is received."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing the ticket from SAML XML body.", "response": "def process_ticket(self):\n        \"\"\"\n            validate ticket from SAML XML body\n\n            :raises: SamlValidateError: if the ticket is not found or not valid, or if we fail\n                to parse the posted XML.\n            :return: a ticket object\n            :rtype: :class:`models.Ticket<cas_server.models.Ticket>`\n        \"\"\"\n        try:\n            auth_req = self.root.getchildren()[1].getchildren()[0]\n            ticket = auth_req.getchildren()[0].text\n            ticket = models.Ticket.get(ticket)\n            if ticket.service != self.target:\n                raise SamlValidateError(\n                    u'AuthnFailed',\n                    u'TARGET %s does not match ticket service' % self.target\n                )\n            return ticket\n        except (IndexError, KeyError):\n            raise SamlValidateError(u'VersionMismatch')\n        except Ticket.DoesNotExist:\n            raise SamlValidateError(\n                    u'AuthnFailed',\n                    u'ticket %s should begin with PT- or ST-' % ticket\n            )\n        except (ServiceTicket.DoesNotExist, ProxyTicket.DoesNotExist):\n            raise SamlValidateError(u'AuthnFailed', u'ticket %s not found' % ticket)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main(source):\n    if source is None:\n        click.echo(\n            \"You need to supply a file or url to a schema to a swagger schema, for\"\n            \"the validator to work.\"\n        )\n        return 1\n    try:\n        load(source)\n        click.echo(\"Validation passed\")\n        return 0\n    except ValidationError as e:\n        raise click.ClickException(str(e))", "response": "This function is used to load a single resource from a file or url to a swagger schema."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef host_validator(value, **kwargs):\n    scheme, hostname, port, path = decompose_hostname(value)\n\n    if len(hostname) > 255:\n        return False\n    if hostname[-1] == \".\":\n        hostname = hostname[:-1]  # strip exactly one dot from the right, if present\n    allowed = re.compile(\"(?!-)[A-Z\\d-]{1,63}(?<!-)$\", re.IGNORECASE)\n\n    with ErrorDict() as errors:\n        if not all(allowed.match(x) for x in hostname.split(\".\")):\n            errors.add_error(\n                'invalid',\n                MESSAGES['host']['invalid'].format(value),\n            )\n\n        if path:\n            errors.add_error(\n                'path',\n                MESSAGES['host']['may_not_include_path'].format(value),\n            )\n\n        if scheme:\n            errors.add_error(\n                'scheme',\n                MESSAGES['host']['may_not_include_scheme'].format(value),\n            )", "response": "Validate a hostname string and return a dict of errors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_source(source):\n    if isinstance(source, collections.Mapping):\n        return deepcopy(source)\n    elif hasattr(source, 'read') and callable(source.read):\n        raw_source = source.read()\n    elif os.path.exists(os.path.expanduser(str(source))):\n        with open(os.path.expanduser(str(source)), 'r') as source_file:\n            raw_source = source_file.read()\n    elif isinstance(source, six.string_types):\n        parts = urlparse.urlparse(source)\n        if parts.scheme and parts.netloc:\n            response = requests.get(source)\n            if isinstance(response.content, six.binary_type):\n                raw_source = six.text_type(response.content, encoding='utf-8')\n            else:\n                raw_source = response.content\n        else:\n            raw_source = source\n\n    try:\n        try:\n            return json.loads(raw_source)\n        except ValueError:\n            pass\n\n        try:\n            return yaml.safe_load(raw_source)\n        except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n            pass\n    except NameError:\n        pass\n\n    raise ValueError(\n        \"Unable to parse `{0}`.  Tried yaml and json.\".format(source),\n    )", "response": "Load a single source file into a dictionary of items."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(raw_schema, target=None, **kwargs):\n    schema = schema_validator(raw_schema, **kwargs)\n    if target is not None:\n        validate_object(target, schema=schema, **kwargs)", "response": "Validate that the given JSONschema complies with the spec."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the response of an api call against a swagger schema.", "response": "def validate_api_response(schema, raw_response, request_method='get', raw_request=None):\n    \"\"\"\n    Validate the response of an api call against a swagger schema.\n    \"\"\"\n    request = None\n    if raw_request is not None:\n        request = normalize_request(raw_request)\n\n    response = None\n    if raw_response is not None:\n        response = normalize_response(raw_response, request=request)\n\n    if response is not None:\n        validate_response(\n            response=response,\n            request_method=request_method,\n            schema=schema\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_api_call(schema, raw_request, raw_response):\n    request = normalize_request(raw_request)\n\n    with ErrorDict() as errors:\n        try:\n            validate_request(\n                request=request,\n                schema=schema,\n            )\n        except ValidationError as err:\n            errors['request'].add_error(err.messages or getattr(err, 'detail'))\n            return\n\n        response = normalize_response(raw_response, raw_request)\n\n        try:\n            validate_response(\n                response=response,\n                request_method=request.method,\n                schema=schema\n            )\n        except ValidationError as err:\n            errors['response'].add_error(err.messages or getattr(err, 'detail'))", "response": "Validate the request and response cycle of an api call against a swagger - compatible schema."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating an email address.", "response": "def validate_email(email, check_mx=False, verify=False, debug=False, smtp_timeout=10):\n    \"\"\"Indicate whether the given string is a valid email address\n    according to the 'addr-spec' portion of RFC 2822 (see section\n    3.4.1).  Parts of the spec that are marked obsolete are *not*\n    included in this test, and certain arcane constructions that\n    depend on circular definitions in the spec may not pass, but in\n    general this should correctly identify any email address likely\n    to be in use as of 2011.\"\"\"\n    if debug:\n        logger = logging.getLogger('validate_email')\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger = None\n\n    try:\n        assert re.match(VALID_ADDRESS_REGEXP, email) is not None\n        check_mx |= verify\n        if check_mx:\n            if not DNS:\n                raise Exception('For check the mx records or check if the email exists you must '\n                                'have installed pyDNS python package')\n            hostname = email[email.find('@') + 1:]\n            mx_hosts = get_mx_ip(hostname)\n            if mx_hosts is None:\n                return False\n            for mx in mx_hosts:\n                try:\n                    if not verify and mx[1] in MX_CHECK_CACHE:\n                        return MX_CHECK_CACHE[mx[1]]\n                    smtp = smtplib.SMTP(timeout=smtp_timeout)\n                    smtp.connect(mx[1])\n                    MX_CHECK_CACHE[mx[1]] = True\n                    if not verify:\n                        try:\n                            smtp.quit()\n                        except smtplib.SMTPServerDisconnected:\n                            pass\n                        return True\n                    status, _ = smtp.helo()\n                    if status != 250:\n                        smtp.quit()\n                        if debug:\n                            logger.debug(u'%s answer: %s - %s', mx[1], status, _)\n                        continue\n                    smtp.mail('')\n                    status, _ = smtp.rcpt(email)\n                    if status == 250:\n                        smtp.quit()\n                        return True\n                    if debug:\n                        logger.debug(u'%s answer: %s - %s', mx[1], status, _)\n                    smtp.quit()\n                except smtplib.SMTPServerDisconnected:  # Server not permits verify user\n                    if debug:\n                        logger.debug(u'%s disconected.', mx[1])\n                except smtplib.SMTPConnectError:\n                    if debug:\n                        logger.debug(u'Unable to connect to %s.', mx[1])\n            return None\n    except AssertionError:\n        return False\n    except (ServerError, socket.error) as e:\n        if debug:\n            logger.debug('ServerError or socket.error exception raised (%s).', e)\n        return None\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a list of parameters find the first parameter with the given name.", "response": "def find_parameter(parameters, **kwargs):\n    \"\"\"\n    Given a list of parameters, find the one with the given name.\n    \"\"\"\n    matching_parameters = filter_parameters(parameters, **kwargs)\n    if len(matching_parameters) == 1:\n        return matching_parameters[0]\n    elif len(matching_parameters) > 1:\n        raise MultipleParametersFound()\n    raise NoParameterFound()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_parameter_lists(*parameter_definitions):\n    merged_parameters = {}\n    for parameter_list in parameter_definitions:\n        for parameter in parameter_list:\n            key = (parameter['name'], parameter['in'])\n            merged_parameters[key] = parameter\n    return merged_parameters.values()", "response": "Merge multiple lists of parameters into a single list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_status_code_to_response_definition(response, operation_definition):\n    status_code = response.status_code\n    operation_responses = {str(code): val for code, val\n                           in operation_definition['responses'].items()}\n\n    key = status_code\n    if key not in operation_responses:\n        key = 'default'\n\n    try:\n        response_definition = operation_responses[key]\n    except KeyError:\n        raise ValidationError(\n            MESSAGES['response']['invalid_status_code'].format(\n                status_code, ', '.join(operation_responses.keys()),\n            ),\n        )\n    return response_definition", "response": "Given a response validate that the status code is in the accepted status codes defined by this endpoint and return the response definition that corresponds to the status code."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a callable for validating the parameters in a response object.", "response": "def generate_path_validator(api_path, path_definition, parameters,\n                            context, **kwargs):\n    \"\"\"\n    Generates a callable for validating the parameters in a response object.\n    \"\"\"\n    path_level_parameters = dereference_parameter_list(\n        path_definition.get('parameters', []),\n        context,\n    )\n    operation_level_parameters = dereference_parameter_list(\n        parameters,\n        context,\n    )\n\n    all_parameters = merge_parameter_lists(\n        path_level_parameters,\n        operation_level_parameters,\n    )\n\n    # PATH\n    in_path_parameters = filter_parameters(all_parameters, in_=PATH)\n    return chain_reduce_partial(\n        attrgetter('path'),\n        generate_path_parameters_validator(api_path, in_path_parameters, context),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate the response against the schema.", "response": "def validate_response(response, request_method, schema):\n    \"\"\"\n    Response validation involves the following steps.\n       4. validate that the response status_code is in the allowed responses for\n          the request method.\n       5. validate that the response content validates against any provided\n          schemas for the responses.\n       6. headers, content-types, etc..., ???\n    \"\"\"\n    with ErrorDict() as errors:\n        # 1\n        # TODO: tests\n        try:\n            api_path = validate_path_to_api_path(\n                path=response.path,\n                context=schema,\n                **schema\n            )\n        except ValidationError as err:\n            errors['path'].extend(list(err.messages))\n            return  # this causes an exception to be raised since errors is no longer falsy.\n\n        path_definition = schema['paths'][api_path] or {}\n\n        # TODO: tests\n        try:\n            operation_definition = validate_request_method_to_operation(\n                request_method=request_method,\n                path_definition=path_definition,\n            )\n        except ValidationError as err:\n            errors['method'].add_error(err.detail)\n            return\n\n        # 4\n        try:\n            response_definition = validate_status_code_to_response_definition(\n                response=response,\n                operation_definition=operation_definition,\n            )\n        except ValidationError as err:\n            errors['status_code'].add_error(err.detail)\n        else:\n            # 5\n            response_validator = generate_response_validator(\n                api_path,\n                operation_definition=operation_definition,\n                path_definition=path_definition,\n                response_definition=response_definition,\n                context=schema,\n            )\n            try:\n                response_validator(response, context=schema)\n            except ValidationError as err:\n                errors['body'].add_error(err.detail)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef construct_schema_validators(schema, context):\n    validators = ValidationDict()\n    if '$ref' in schema:\n        validators.add_validator(\n            '$ref', SchemaReferenceValidator(schema['$ref'], context),\n        )\n    if 'properties' in schema:\n        for property_, property_schema in schema['properties'].items():\n            property_validator = generate_object_validator(\n                schema=property_schema,\n                context=context,\n            )\n            validators.add_property_validator(property_, property_validator)\n    if schema.get('additionalProperties') is False:\n        validators.add_validator(\n            'additionalProperties',\n            generate_additional_properties_validator(context=context, **schema),\n        )\n    assert 'context' not in schema\n    for key in schema:\n        if key in validator_mapping:\n            validators.add_validator(key, validator_mapping[key](context=context, **schema))\n    return validators", "response": "Given a schema object construct a dictionary of validators needed to validate a response matching the given schema."}
